-        counts_mat[i, input_mat[j, i]] += 1
+        counts_mat[i, int(input_mat[j, i])] += 1
-        benchmark_log_dir=flags.benchmark_log_dir)
+  train_hooks = hooks_helper.get_train_hooks(
-    print('Starting a training cycle.')
+  def input_fn_train():
-                            flags.num_parallel_calls, flags.multi_gpu)
+  def input_fn_eval():
-
+    tf.logging.info('Starting to evaluate.')
-flags.DEFINE_string('counts_file', '', 'Counts file.')
+flags.DEFINE_string('counts_file', None, 'Counts file.')
-    fig.set_figheight(4.7)
+    fig.set_figheight(5)
-        linestyle='dashed',
+        linestyle='dotted',
-        color='b',
+        color='g',
-  ax.tick_params(labelsize=14)
+  ax.tick_params(labelsize=14, bottom=True, top=True, left=True, right=True)
-flags.DEFINE_string('counts_file', '', 'Counts file.')
+flags.DEFINE_string('counts_file', None, 'Counts file.')
-  answered = np.zeros(n)
+  partition = [None] * n
-  for i in xrange(n):
+  for i in range(n):
-      rdp_sqrd = rdp_query**2
+      rdp_sqrd = rdp_query ** 2
-      rdp_sqrd = rdp_query**2
+      rdp_sqrd = rdp_query ** 2
-                                          2**.5 * params['sigma1'], orders)
+                                          2 ** .5 * params['sigma1'], orders)
-          q_step1 * rdp_gnmax_step2**2)
+          rdp_gnmax_step1 ** 2 + 2 * rdp_gnmax_step1 * q_step1 * rdp_gnmax_step2
-          rdp_cum / i)**2  # Ignore Bessel's correction.
+          rdp_cum / i) ** 2  # Ignore Bessel's correction.
-      eps_std = ((i + 1) * rdp_var[order_opt_idx])**.5  # Std of the sum.
+      eps_std = ((i + 1) * rdp_var[order_opt_idx]) ** .5  # Std of the sum.
-  y_gnmax = np.full(len(x_axis), None, dtype=float)
+  y_lap = np.zeros(len(x_axis), dtype=float)
-  for i in xrange(len(x_axis)):
+  for i in range(len(x_axis)):
-                     eps_gnmax2, partition_gnmax2, answered_gnmax2):
+    eps_gnmax2, partition_gnmax2, answered_gnmax2):
-  for i in xrange(lenx):
+  for i in range(lenx):
-  })
+                        't': 1000,
-         votes, lambda_laplace, gnmax_parameters, sigma2)
+        votes, lambda_laplace, gnmax_parameters, sigma2)
-                       answered_gnmax[2], orders_opt_gnmax[2])
+from __future__ import absolute_import
-  logub = min(-(1 + 1. / sigma)**2, -((order - 1) / sigma)**2, -1 / sigma**2)
+  logub = min(-(1 + 1. / sigma)**2, -((order - .99) / sigma)**2, -1 / sigma**2)
-        parsers.ExportParser(),
+    export_dir: Create a flag to specify where a SavedModel should be exported.
-               hooks=True):
+               hooks=True, export_dir=True):
-
+def export_model(model, model_type, export_dir):
-                  output_node, batch_size=128, workspace_size=1<<30):
+                  output_node, batch_size=128, workspace_size=2<<10):
-    workspace_size: long, size in bytes that can be used during conversion.
+    workspace_size: int, size in megabytes that can be used during conversion.
-      max_workspace_size_bytes=workspace_size,
+      max_workspace_size_bytes=workspace_size<<20,
-        help="[default: %(default)s] Workspace size in bytes.",
+        "--workspace_size", "-ws", type=int, default=2<<10,
-    return dataset
+
-  train = automobile_data.make_dataset(args.batch_size, train_x, train_y, True, 1000)
+  # Provide the training input dataset.
-  test = automobile_data.make_dataset(args.batch_size, test_x, test_y)
+  test_input_fn = automobile_data.make_dataset(args.batch_size, test_x, test_y)
-  model.train(input_fn=automobile_data.from_dataset(train), steps=args.train_steps)
+  model.train(input_fn=train_input_fn, steps=args.train_steps)
-  eval_result = model.evaluate(input_fn=automobile_data.from_dataset(test))
+  eval_result = model.evaluate(input_fn=test_input_fn)
-  train = automobile_data.make_dataset(args.batch_size, train_x, train_y, True, 1000)
+  # Provide the training input dataset.
-  test = automobile_data.make_dataset(args.batch_size, test_x, test_y)
+  # Provide the validation input dataset.
-  model.train(input_fn=automobile_data.from_dataset(train), steps=args.train_steps)
+  model.train(input_fn=train_input_fn, steps=args.train_steps)
-  eval_result = model.evaluate(input_fn=automobile_data.from_dataset(test))
+  eval_result = model.evaluate(input_fn=test_input_fn)
-  train = automobile_data.make_dataset(args.batch_size, train_x, train_y, True, 1000)
+  # Provide the training input dataset.
-  test = automobile_data.make_dataset(args.batch_size, test_x, test_y)
+  # Provide the validation input dataset.
-  model.train(input_fn=automobile_data.from_dataset(train), steps=args.train_steps)
+  model.train(input_fn=train_input_fn, steps=args.train_steps)
-  eval_result = model.evaluate(input_fn=automobile_data.from_dataset(test))
+  eval_result = model.evaluate(input_fn=test_input_fn)
-  predict_results = model.predict(input_fn=automobile_data.from_dataset(predict))
+  # Provide the predict input dataset.
-  train = automobile_data.make_dataset(args.batch_size, train_x, train_y, True, 1000)
+  # Provide the training input dataset.
-  test = automobile_data.make_dataset(args.batch_size, test_x, test_y)
+  # Provide the validation input dataset.
-  model.train(input_fn=automobile_data.from_dataset(train), steps=args.train_steps)
+  model.train(input_fn=train_input_fn, steps=args.train_steps)
-  eval_result = model.evaluate(input_fn=automobile_data.from_dataset(test))
+  eval_result = model.evaluate(input_fn=test_input_fn)
-REQUIRED_PACKAGES = ['Pillow>=1.0']
+REQUIRED_PACKAGES = ['Pillow>=1.0', 'Matplotlib>=2.1', 'Cython>=0.28.1']
-        x[key] = np.array(x[key])
+def from_dataset(dataset): return lambda: dataset.make_one_shot_iterator().get_next()
-    return tf.data.Dataset.from_tensor_slices(tuple(items))
+def make_dataset(batch_sz, x, y=None, shuffle=False, shuffle_buffer_size=1000):
-
+
-      .repeat())
+  train = automobile_data.make_dataset(args.batch_size, train_x, train_y, True, 1000)
-  test = automobile_data.make_dataset(test_x, test_y).batch(args.batch_size)
+  test = automobile_data.make_dataset(args.batch_size, test_x, test_y)
-  model.train(input_fn=from_dataset(train), steps=args.train_steps)
+  model.train(input_fn=automobile_data.from_dataset(train), steps=args.train_steps)
-  eval_result = model.evaluate(input_fn=from_dataset(test))
+  eval_result = model.evaluate(input_fn=automobile_data.from_dataset(test))
-      .repeat())
+  train = automobile_data.make_dataset(args.batch_size, train_x, train_y, True, 1000)
-  test = automobile_data.make_dataset(test_x, test_y).batch(args.batch_size)
+  test = automobile_data.make_dataset(args.batch_size, test_x, test_y)
-  model.train(input_fn=from_dataset(train), steps=args.train_steps)
+  model.train(input_fn=automobile_data.from_dataset(train), steps=args.train_steps)
-  eval_result = model.evaluate(input_fn=from_dataset(test))
+  eval_result = model.evaluate(input_fn=automobile_data.from_dataset(test))
-      .repeat())
+  train = automobile_data.make_dataset(args.batch_size, train_x, train_y, True, 1000)
-  test = automobile_data.make_dataset(test_x, test_y).batch(args.batch_size)
+  test = automobile_data.make_dataset(args.batch_size, test_x, test_y)
-  model.train(input_fn=from_dataset(train), steps=args.train_steps)
+  model.train(input_fn=automobile_data.from_dataset(train), steps=args.train_steps)
-  eval_result = model.evaluate(input_fn=from_dataset(test))
+  eval_result = model.evaluate(input_fn=automobile_data.from_dataset(test))
-  predict_results = model.predict(input_fn=from_dataset(predict))
+
-      .repeat())
+  train = automobile_data.make_dataset(args.batch_size, train_x, train_y, True, 1000)
-  test = automobile_data.make_dataset(test_x, test_y).batch(args.batch_size)
+  test = automobile_data.make_dataset(args.batch_size, test_x, test_y)
-  model.train(input_fn=from_dataset(train), steps=args.train_steps)
+  model.train(input_fn=automobile_data.from_dataset(train), steps=args.train_steps)
-  eval_result = model.evaluate(input_fn=from_dataset(test))
+  eval_result = model.evaluate(input_fn=automobile_data.from_dataset(test))
-    tpu_grpc_url = tpu_cluster_resolver.get_master()
+  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
-      evaluation_master=tpu_grpc_url,
+      cluster=tpu_cluster_resolver,
-  """
+  """Wrapper for a go.Position which replays its history."""
-  _, diagonals = get_neighbors_diagonals[c]
+  _, all_diagonals = get_neighbors_diagonals(board_size)
-    if not board[d] in (color, EMPTY):
+    if board[d] not in (color, EMPTY):
-  """
+  """Group class.
-        np.ones([board_size, board_size], dtype=np.int32)
+    self.group_index = (group_index if group_index is not None else
-        np.zeros([board_size, board_size], dtype=np.uint8)
+    self.liberty_cache = (
-  def __deepcopy__(self, memodict={}):
+  def __deepcopy__(self, memodict=None):
-    if len(self.groups[new_group.id].liberties) == 0:
+    if self.groups[new_group.id].liberties is None:
-    assert type(recent) is tuple
+    if not isinstance(recent, tuple):
-        np.zeros([board_size, board_size], dtype=np.int8)
+    self.board = (board if board is not None else
-        np.zeros([0, board_size, board_size], dtype=np.int8)
+    self.board_deltas = (board_deltas if board_deltas is not None else
-  def __deepcopy__(self, memodict={}):
+  def __deepcopy__(self, memodict=None):
-    # Positional superko (this is very crudely approximate at the moment.)
+    """Obeys CGOS Rules of Play.
-          coords.to_kgs(c), self))
+          coords.to_kgs(self.board_size, c), self))
-    opp_color = color * -1
+    opp_color = -1 * color
-      O_border = WHITE in border_colors
+      X_border = BLACK in border_colors   # pylint: disable=invalid-name
-# Licensed under the Apache License, Version 2.0 (the "License");
+# Licensed under the Apache License, Version 2.0 (the 'License');
-# distributed under the License is distributed on an "AS IS" BASIS,
+# distributed under the License is distributed on an 'AS IS' BASIS,
-import sys
+import sys
-  first, rest = (message.split(" ", 1) + [None])[:2]
+  first, rest = (message.split(' ', 1) + [None])[:2]
-      command, arguments = (rest.split(" ", 1) + [None])[:2]
+      command, arguments = (rest.split(' ', 1) + [None])[:2]
-  command = command.replace("-", "_")  # for kgs extensions.
+  command = command.replace('-', '_')  # for kgs extensions.
-               version="0.1"):
+  def __init__(self, game_obj, name='gtp (python, kgs-chat extensions)',
-    self.known_commands += ["kgs-chat"]
+    self.known_commands += ['kgs-chat']
-        retval = getattr(self, "cmd_" + command)(arguments)
+        retval = getattr(self, 'cmd_' + command)(arguments)
-      return gtp.format_error(message_id, "unknown command: " + command)
+      return gtp.format_error(message_id, 'unknown command: ' + command)
-      text = " ".join(text)
+      text = ' '.join(text)
-      return "Unparseable message, args: %r" % arguments
+      return 'Unparseable message, args: %r' % arguments
-      print("movenum =", movenum, file=sys.stderr)
+      print('movenum =', movenum, file=sys.stderr)
-      raise ValueError("Unreadable file: " + file_)
+      raise ValueError('Unreadable file: ' + file_)
-        print("playing #", idx, p.next_move, file=sys.stderr)
+        print('playing #', idx, p.next_move, file=sys.stderr)
-  """ GTP extensions of 'analysis commands' for gogui.
+  """GTP extensions of 'analysis commands' for gogui.
-  violate encapsulation a bit... Suggestions welcome :) """
+  violate encapsulation a bit.
-               version="0.1"):
+  def __init__(self, game_obj, name='gtp (python, gogui extensions)',
-    self.known_commands += ["gogui-analyze_commands"]
+    self.known_commands += ['gogui-analyze_commands']
-              "pspairs/Q Heatmap/q_heatmap"])
+    return '\n'.join(['var/Most Read Variation/nextplay',
-      key=lambda i: self._game.root.child_Q[i], reverse=reverse)
+        key=lambda i: self._game.root.child_Q[i], reverse=reverse)
-      for key in sort_order if node.child_N[key] > 0][:20])
+    return '\n'.join(['{!s:6} {}'.format(
-      for j in range(100):
+    for _ in range(50):
-      print("gogui-gfx: VAR", moves_cols, file=sys.stderr, flush=True)
+      colors = 'bw' if self._game.root.position.to_play is go.BLACK else 'wb'
-          "Restart with env var BOARD_SIZE={n}").format(n=n))
+      raise ValueError((
-            "%Y-%m-%d-%H:%M.sgf"), 'w') as f:
+            '%Y-%m-%d-%H:%M.sgf'), 'w') as f:
-        print("Error saving sgf", file=sys.stderr, flush=True)
+        print('Error saving sgf', file=sys.stderr, flush=True)
-    if not translate_gtp_colors(color) == self.position.to_play:
+    if translate_gtp_colors(color) != self.position.to_play:
-                verbosity=verbosity, two_player_mode=True)
+                          verbosity=verbosity, two_player_mode=True)
-  name = "Somebot-" + os.path.basename(read_file)
+  name = 'Somebot-' + os.path.basename(read_file)
-c_PUCT = 1.38
+c_PUCT = 1.38  # pylint: disable=invalid-name
-def D_NOISE_ALPHA(board_size): return 0.03 * 361 / (board_size ** 2)
+def D_NOISE_ALPHA(board_size):  # pylint: disable=invalid-name
-  simpler."""
+  simpler.
-    return "<MCTSNode move=%s, N=%s, to_play=%s>" % (
+    return '<MCTSNode move={}, N={}, to_play={}>'.format(
-    "Return value of position, from perspective of player to play."
+    """Return value of position, from perspective of player to play."""
-    revert = -1 * self.position.to_play
+    revert = -self.position.to_play
-    """
+    """Revert visit increments."""
-    '''
+    # True if the last two moves were Pass or if the position is at a move
-    """
+    """Returns the child visit counts as a probability distribution, pi."""
-      probs = probs ** .95
+      probs **= .95
-        output.append("GAME END")
+        output.append('GAME END')
-    output.append("Q: {:.5f}\n".format(node.Q))
+      output.append('{} ({}) ==> '.format(
-          self.board_size, coords.from_flat(self.board_size, node.fmove)))
+      output.append('{}'.format(coords.to_kgs(
-    output.append("{q:.4f}\n".format(q=self.Q))
+    output.append('{q:.4f}\n'.format(q=self.Q))
-        "  p-delta  p-rel\n")
+        '''move:  action      Q      U      P    P-Dir    N  soft-N
-    return "".join(output)
+        '\n'.join([
-  """
+  """Make tf examples.
-  """
+  """Write tf.Example to files.
-  """
+  """Parse tf examples.
-  """
+  """Read tf records.
-    shuffle_buffer_size: how big of a buffer to fill before shuffling.
+
-      tf.random_uniform([1]), filter_amount)[0])
+  dataset = dataset.filter(
-  `read_tf_records` for parameter documentation.
+  """Read tf.Records and prepare them for ingestion by dualnet.
-  Returns a dict of tensors (see return value of batch_parse_tf_example)
+  Returns:
-  Returns an iterable of tf.Examples.
+  Returns:
-  pi = _one_hot(board_size, coords.to_flat(position_w_context.next_move))
+  pi = _one_hot(board_size, coords.to_flat(
-  Returns:
+  Yields:
-SGF_TEMPLATE = """(;GM[1]FF[4]CA[UTF-8]AP[Minigo_sgfgenerator]RU[{ruleset}]
+SGF_TEMPLATE = '''(;GM[1]FF[4]CA[UTF-8]AP[Minigo_sgfgenerator]RU[{ruleset}]
-{game_moves})"""
+{game_moves})'''
-PROGRAM_IDENTIFIER = "Minigo"
+PROGRAM_IDENTIFIER = 'Minigo'
-    move=translate_sgf_move(player_move), q=q)
+  return '{move}C[{q:.4f}]'.format(
-    raise ValueError("Can't translate color %s to sgf" % player_move.color)
+    raise ValueError(
-    comment_node = "C[{}]".format(comment)
+    comment_node = 'C[{}]'.format(comment)
-):
+    comment_node = ''
-    move_history: iterable of PlayerMoves
+    board_size: the go board size.
-             for z in zip_longest(move_history, comments))
+  game_moves = ''.join(translate_sgf_move(*z) for z in zip_longest(
-  'Converts raw sgf library output to sensible value'
+  """Converts raw sgf library output to sensible value."""
-  'A node can either add B+W stones, play as B, or play as W.'
+  """A node can either add B+W stones, play as B, or play as W."""
-    c) for c in props.get('AW', [])]
+  black_stones_added = [coords.from_sgf(c) for c in props.get('AB', [])]
-    return add_stones(pos, black_stones_added, white_stones_added)
+    return add_stones(board_size, pos, black_stones_added, white_stones_added)
-    black_move = coords.from_sgf(board_size, props.get('B', [''])[0])
+    black_move = coords.from_sgf(props.get('B', [''])[0])
-    white_move = coords.from_sgf(board_size, props.get('W', [''])[0])
+    white_move = coords.from_sgf(props.get('W', [''])[0])
-def add_stones(pos, black_stones_added, white_stones_added):
+def add_stones(board_size, pos, black_stones_added, white_stones_added):
-              caps=pos.caps, ko=pos.ko, recent=pos.recent, to_play=pos.to_play)
+  new_position = Position(
-def get_next_move(board_size, node):
+def get_next_move(node):
-    return coords.from_sgf(board_size, props['W'][0])
+    return coords.from_sgf(props['W'][0])
-    return coords.from_sgf(board_size, props['B'][0])
+    return coords.from_sgf(props['B'][0])
-      ('W' in next_node.properties and not pos.to_play == go.WHITE)):
+  if (('B' in next_node.properties and pos.to_play != go.BLACK) or
-  Wrapper for sgf files, returning go.PositionWithContext instances.
+  """Wrapper for sgf files.
-
+
-  assert int(sgf_prop(props.get('GM', ['1']))) == 1, "Not a Go SGF!"
+  assert int(sgf_prop(props.get('GM', ['1']))) == 1, 'Not a Go SGF!'
-  if props.get('KM') != None:
+  if props.get('KM') is not None:
-  pos = Position(komi=komi)
+  pos = Position(board_size, komi=komi)
-    next_move = get_next_move(board_size, current_node)
+    next_move = get_next_move(current_node)
-  for CGOS time controls, which are absolute 15 minute time.
+  """Compute the time can be used."""
-  """
+  # Given current move number and "desired" seconds per move,
-    """ Used for playing a single game.
+    """ Used for playing a single game."""
-        print("%d: Searched %d times in %s seconds\n\n" % (
+        print('%d: Searched %d times in %s seconds\n\n' % (
-    """
+    """Play a move."""
-    a move weighted by visit count; later on, pick the absolute max."""
+    a move weighted by visit count; later on, pick the absolute max.
-    MAX_DEPTH = (self.board_size ** 2) * 1.4  # 505 moves for 19x19, 113 for 9x9
+    max_depth = (self.board_size ** 2) * 1.4  # 505 moves for 19x19, 113 for 9x9
-    if len(pos.recent) == 0:
+    if pos.recent is None:
-      return "{}-{}".format('b' if move.color == 1 else 'w',
+      return '{}-{}'.format('b' if move.color == 1 else 'w',
-      path += " (depth cutoff reached) %0.1f" % node.position.score()
+    path = ' '.join(fmt(move) for move in pos.recent[-diff:])
-      path += " (game over) %0.1f" % node.position.score()
+      path += ' (game over) %0.1f' % node.position.score()
-      string = "B+R" if winner == go.BLACK else "W+R"
+      string = 'B+R' if winner == go.BLACK else 'W+R'
-      comments[0] = ("Resign Threshold: %0.3f\n" %
+      comments[0] = ('Resign Threshold: %0.3f\n' %
-        black_name=os.path.basename(self.network.save_file) or "Unknown",
+        white_name=os.path.basename(self.network.save_file) or 'Unknown',
-      return "{:s} {:.2f}%".format(color, wr * 100.0)
+      color = 'Black' if self.root.Q > 0 else 'White'
-          "timestamp": datetime.datetime.now().strftime(
+          "timestamp": datetime.datetime.utcnow().strftime(
-      "run_date": datetime.datetime.now().strftime(_DATE_TIME_FORMAT_PATTERN)}
+      "run_date": datetime.datetime.utcnow().strftime(
-    benchmark_logger = None
+  benchmark_logger = logger.config_benchmark_logger(flags.benchmark_log_dir)
-      benchmark_logger.log_estimator_evaluation_result(eval_results)
+    benchmark_logger.log_evaluation_result(eval_results)
-    raise ValueError("metric_log_dir should be provided to use metric logger")
+  logger.config_benchmark_logger(benchmark_log_dir)
-      log_dir=benchmark_log_dir,
+      metric_logger=logger.get_benchmark_logger(),
-  """Class to log the benchmark information to local disk."""
+# Don't use it directly. Use get_benchmark_logger to access a logger.
-    """Log the evaluation result for a estimator.
+def config_benchmark_logger(logging_dir):
-    The evaluate result is a directory that contains metrics defined in
+    The evaluate result is a dictionary that contains metrics defined in
-      eval_results: dict, the result of evaluate() from a estimator.
+      eval_results: dict, the result of evaluate.
-                         type(eval_results))
+      tf.logging.warning("eval_results should be dictionary for logging. "
-      extras = []
+    extras = _convert_to_json_dict(extras)
-    _collect_memory_info(run_info)
+    run_info = _gather_run_info(model_name)
-    super(BenchmarkLoggerTest, self).setUp()
+    super(BenchmarkFileLoggerTest, self).setUp()
-    super(BenchmarkLoggerTest, self).tearDown()
+    super(BenchmarkFileLoggerTest, self).tearDown()
-    logger.BenchmarkLogger(non_exist_temp_dir)
+    logger.BenchmarkFileLogger(non_exist_temp_dir)
-    log = logger.BenchmarkLogger(log_dir)
+    log = logger.BenchmarkFileLogger(log_dir)
-    log = logger.BenchmarkLogger(log_dir)
+    log = logger.BenchmarkFileLogger(log_dir)
-  def test_log_non_nubmer_value(self):
+  def test_log_non_number_value(self):
-    log = logger.BenchmarkLogger(log_dir)
+    log = logger.BenchmarkFileLogger(log_dir)
-    log.log_estimator_evaluation_result(eval_result)
+    log = logger.BenchmarkFileLogger(log_dir)
-    log.log_estimator_evaluation_result(eval_result)
+    log = logger.BenchmarkFileLogger(log_dir)
-  def __init__(self, tensors, log_dir=None, metric_logger=None,
+  def __init__(self, tensors, metric_logger=None,
-          `metric_logger` should be provided.
+          hook should use to write the log.
-      self._logger = metric_logger
+    if metric_logger is None:
-    with self.assertRaisesRegexp(ValueError, 'log_dir and metric_logger'):
+    with self.assertRaisesRegexp(ValueError, 'metric_logger'):
-        parsers.BaseParser(multi_gpu=True, num_gpu=False),
+        parsers.BaseParser(),
-def input_fn(is_training, data_dir, batch_size, num_epochs=1):
+def input_fn(is_training, data_dir, batch_size, num_epochs=1,
-  )
+      parse_record, num_epochs, num_parallel_calls,
-      self.assertEqual(eval_metric_ops['accuracy'][1].dtype, tf.float32)
+  def _cifar10_model_fn_helper(self, mode, version, dtype, multi_gpu=False):
-                                 dtype=tf.float32)
+    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.TRAIN, version=1)
-                                 dtype=tf.float32)
+                                 multi_gpu=True)
-                                 dtype=tf.float32)
+    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=1)
-                                 dtype=tf.float32)
+    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=2)
-                                 dtype=tf.float32)
+    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=1)
-                                 dtype=tf.float32)
+    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=2)
-def input_fn(is_training, data_dir, batch_size, num_epochs=1):
+def input_fn(is_training, data_dir, batch_size, num_epochs=1,
-  )
+      num_epochs, num_parallel_calls, examples_per_epoch=num_images,
-  def resnet_model_fn_helper(self, mode, version, dtype):
+  def _resnet_model_fn_helper(self, mode, version, dtype, multi_gpu):
-      self.assertEqual(eval_metric_ops['accuracy'][1].dtype, tf.float32)
+    with tf.Graph().as_default() as g:
-                                dtype=tf.float32)
+    self.resnet_model_fn_helper(tf.estimator.ModeKeys.TRAIN, version=1)
-                                dtype=tf.float32)
+                                multi_gpu=True)
-                                dtype=tf.float32)
+    self.resnet_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=1)
-                                dtype=tf.float32)
+    self.resnet_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=2)
-                                dtype=tf.float32)
+    self.resnet_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=1)
-                                dtype=tf.float32)
+    self.resnet_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=2)
-                           parse_record_fn, num_epochs=1):
+                           parse_record_fn, num_epochs=1, num_parallel_calls=1,
-          num_parallel_batches=1))
+  # Currently, if we are using multiple GPUs, we can't pass in uneven batches.
-  dataset.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)
+  # critical training path.
-  def input_fn(is_training, data_dir, batch_size, *args, **kwargs):  # pylint: disable=unused-argument
+  def input_fn(is_training, data_dir, batch_size, *args):  # pylint: disable=unused-argument
-                    data_format, version, loss_scale, loss_filter_fn=None,
+                    data_format, version, loss_scale,
-    )
+        momentum=momentum)
-
+  accuracy = tf.metrics.accuracy(
-def per_device_batch_size(batch_size, num_gpus):
+def validate_batch_size_for_multi_gpu(batch_size):
-  Note that this should eventually be handled by DistributionStrategies
+  Note that this should eventually be handled by replicate_model_fn
-    Batch size per device.
+    batch_size: the number of examples processed in each training batch.
-    ValueError: if batch_size is not divisible by number of devices
+    ValueError: if no GPUs are found, or selected batch_size is invalid.
-    return batch_size
+  from tensorflow.python.client import device_lib  # pylint: disable=g-import-not-at-top
-           'GPUs with a batch size of {}; try --batch_size={} instead.'
+           'must be a multiple of the number of available GPUs. '
-  return int(batch_size / num_gpus)
+  if flags.multi_gpu:
-
+  # Set up a RunConfig to save checkpoint and set session config.
-      )
+      return input_function(True, flags.data_dir, flags.batch_size,
-      )
+      return input_function(False, flags.data_dir, flags.batch_size,
-        parsers.PerformanceParser(num_parallel_calls=False),
+        parsers.BaseParser(),
-    batch_size: Create a flag to specify the global batch size.
+    batch_size: Create a flag to specify the batch size.
-               multi_gpu=False, num_gpu=True, hooks=True):
+               stop_threshold=True, batch_size=True, multi_gpu=True,
-               "evaluation.",
+          help="[default: %(default)s] Batch size for training and evaluation.",
-        parsers.BaseParser(multi_gpu=True, num_gpu=False),
+        parsers.BaseParser(),
-        parsers.BaseParser(multi_gpu=False, num_gpu=False)])
+    super(WideDeepArgParser, self).__init__(parents=[parsers.BaseParser()])
-_WARMUP_NUM_LOOPS = 50
+_WARMUP_NUM_LOOPS = 5
-            b = tf.Variable(tf.zeros([self.n_layers[layer+1]], dtype=tf.float32))
+                initializer((self.n_layers[layer], self.n_layers[layer + 1]),
-            b = tf.Variable(tf.zeros([self.n_layers[layer-1]], dtype=tf.float32))
+                initializer((self.n_layers[layer], self.n_layers[layer - 1]),
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-        sync_optimizer=optimizer if FLAGS.sync_replicas else None)
+        sync_optimizer=optimizer if FLAGS.sync_replicas else None,
-def get_inputs(input_queue, num_classes, merge_multiple_label_boxes=False):
+def get_inputs(input_queue,
-       train_config.merge_multiple_label_boxes)
+       train_config.merge_multiple_label_boxes,
-          is_chief, train_dir, graph_hook_fn=None):
+def train(create_tensor_dict_fn,
-      fields.InputDataFields.groundtruth_boxes: box_label
+      fields.InputDataFields.groundtruth_boxes: box_label,
-def build(model_config, is_training, add_summaries=True):
+def build(model_config, is_training, add_summaries=True,
-
+    add_background_class: Whether to add an implicit background class to one-hot
-    return _build_ssd_model(model_config.ssd, is_training, add_summaries)
+    return _build_ssd_model(model_config.ssd, is_training, add_summaries,
-def _build_ssd_model(ssd_config, is_training, add_summaries):
+def _build_ssd_model(ssd_config, is_training, add_summaries,
-
+    add_background_class: Whether to add an implicit background class to one-hot
-      inplace_batchnorm_update=ssd_config.inplace_batchnorm_update)
+      inplace_batchnorm_update=ssd_config.inplace_batchnorm_update,
-      background categories that might be implicitly be predicted in various
+      background categories that might be implicitly predicted in various
-               inplace_batchnorm_update=False):
+               inplace_batchnorm_update=False,
-    ]
+    if self._add_background_class:
-                    normalize_loc_loss_by_codesize=False):
+  def _create_model(self,
-        normalize_loss_by_num_matches, hard_example_miner, add_summaries=False,
+        is_training,
-        freeze_batchnorm=False, inplace_batchnorm_update=False)
+        freeze_batchnorm=False,
-    'must specify either this flag or --tpu_name.')
+    help='Name of the Cloud TPU for Cluster Resolvers.')
-                    'Mode to run: train, eval, train_and_eval')
+flags.DEFINE_string('mode', 'train',
-    tpu_grpc_url = tpu_cluster_resolver.get_master()
+  tpu_cluster_resolver = (
-  if FLAGS.mode in ['train', 'train_and_eval']:
+  if FLAGS.mode == 'train':
-            'Checkpoint %s no longer exists, skipping checkpoint' % ckpt)
+    if FLAGS.eval_training_data:
-  if data.has_key('object'):
+  if 'object' in data:
-      masks.append(mask_remapped)
+  if 'object' in data:
-          eval_metric_ops[str(var.op.name)] = (var, tf.no_op())
+          eval_metric_ops[var.op.name] = (var, tf.no_op())
-          mobilenet_v1.mobilenet_v1_arg_scope(is_training=None)):
+          mobilenet_v1.mobilenet_v1_arg_scope(
-        predict_auxiliary_outputs=predict_auxiliary_outputs)
+        predict_boxes_and_classes=True)
-    are only calculated for the top scored boxes.
+    For training, masks as predicted directly on the box_classifier_features,
-        1) mask_predictions: (optional) a 4-D tensor with shape
+        1) mask_predictions: a 4-D tensor with shape
-        predict_auxiliary_outputs=True)
+    if self._is_training:
-    if box_predictor.MASK_PREDICTIONS in box_predictions:
+
-          number_of_stages=2,
+          number_of_stages=3,
-            name='eval_on_train', input_fn=train_input_fn, steps=eval_steps))
+            name='eval_on_train', input_fn=eval_on_train_input_fn,
-      if mode == tf.estimator.ModeKeys.TRAIN:
+      if mode == 'train':
-      else:
+      elif mode == 'eval':
-      estimator_spec = model_fn(features, labels, mode)
+      estimator_spec = model_fn(features, labels, model_mode)
-      if mode == tf.estimator.ModeKeys.TRAIN:
+      if model_mode == tf.estimator.ModeKeys.TRAIN:
-    self._assert_model_fn_for_train_eval(configs, tf.estimator.ModeKeys.TRAIN)
+    self._assert_model_fn_for_train_eval(configs, 'train')
-    self._assert_model_fn_for_train_eval(configs, tf.estimator.ModeKeys.EVAL)
+    self._assert_model_fn_for_train_eval(configs, 'eval')
-
+    self.assertIn('train_input_fn', train_and_eval_dict)
-        input_fn = train_input_fn
+        input_fn = eval_on_train_input_fn
-        training_optimizer = tpu_optimizer.CrossShardOptimizer(
+        training_optimizer = tf.contrib.tpu.CrossShardOptimizer(
-    estimator = tpu_estimator.TPUEstimator(
+    estimator = tf.contrib.tpu.TPUEstimator(
-           multiclass_scores, distorted_multiclass_scores
+      (boxes_rank_, distorted_boxes_, distorted_boxes_rank_, images_rank_,
-      self.assertAllEqual(multiclass_scores_, distorted_multiclass_scores_)
+      self.assertAllEqual(multiclass_scores_rank_,
-    poses.append(obj['pose'].encode('utf8'))
+  if data.has_key('object'):
-              **params)
+    preprocessed_inputs = shape_utils.check_min_image_dim(
-                                     num_outputs=self.num_classes,
+                                     num_outputs=num_masks,
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
+        box_predictions[box_predictor.BOX_ENCODINGS],
-      mask_width = tf.shape(detection_masks)[2]
+      _, num_classes, mask_height, mask_width = (
-                     tf.range(k) * num_mask_classes + tf.to_int32(classes))
+    instance_masks = tf.reshape(instance_masks, [-1, height, width])
-            tf.greater(one_hot_flat_cls_targets_with_background, 0))
+        if prediction_masks.get_shape().as_list()[1] == 1:
-    faster_rcnn_meta_arch_test_lib.FasterRCNNMetaArchTestBase):
+    faster_rcnn_meta_arch_test_lib.FasterRCNNMetaArchTestBase,
-      self):
+      self, masks_are_class_agnostic):
-            predict_masks=True)
+            predict_masks=True,
-      self):
+      self, masks_are_class_agnostic):
-
+          predict_masks=True,
-          'mask_predictions': (2 * max_num_proposals, 2, 14, 14)
+          'mask_predictions': (2 * max_num_proposals, mask_shape_1, 14, 14)
-  def _add_mask_to_second_stage_box_predictor_text_proto(self):
+  def _add_mask_to_second_stage_box_predictor_text_proto(
-                                      predict_masks):
+                                      predict_masks, masks_are_class_agnostic):
-          self._add_mask_to_second_stage_box_predictor_text_proto(),
+          self._add_mask_to_second_stage_box_predictor_text_proto(
-                   pad_to_max_dimension=None):
+                   pad_to_max_dimension=None,
-            predict_masks=predict_masks), **common_kwargs)
+            predict_masks=predict_masks,
-    if mode == tf.estimator.ModeKeys.TRAIN:
+    if mode in [tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL]:
-"""
+r"""Constructs model, inputs, and training environment."""
-from tensorflow.contrib.learn.python.learn import learn_runner
+from tensorflow.contrib.tpu.python.tpu import tpu_estimator
-  """Populates an `Experiment` object.
+def create_estimator_and_inputs(run_config,
-    export.
+    A dictionary with the following fields:
-      'create_pipeline_proto_from_configs']
+  create_pipeline_proto_from_configs = MODEL_BUILD_UTIL_MAP[
-    train_steps = train_config.num_steps
+  if train_steps is None:
-    eval_steps = eval_config.num_examples
+  if eval_steps is None:
-  # Create the input functions for TRAIN/EVAL.
+  # Create the input functions for TRAIN/EVAL/PREDICT.
-
+  # Write the as-run pipeline config to disk.
-      f.write(config_text)
+    config_util.save_pipeline_config(pipeline_config_final, estimator.model_dir)
-  return tf.contrib.learn.Experiment(
+  return dict(
-      eval_delay_secs=120,)
+      eval_steps=eval_steps)
-      hparams=model_hparams.create_hparams())
+def populate_experiment(run_config,
-  tf.app.run()
+  Args:
-"""Tests for object detection model."""
+"""Tests for object detection model library."""
-from object_detection import model_test_util
+from object_detection import model_lib
-MODEL_NAME_FOR_TEST = model_test_util.SSD_INCEPTION_MODEL_NAME
+# Model for test. Options are:
-  filename = model_test_util.GetPipelineConfigPath(model_name)
+  filename = get_pipeline_config_path(model_name)
-class ModelTflearnTest(tf.test.TestCase):
+class ModelLibTest(tf.test.TestCase):
-  def _assert_outputs_for_train_eval(self, configs, mode, class_agnostic=False):
+  def _assert_model_fn_for_train_eval(self, configs, mode,
-      model_fn = model.create_model_fn(detection_model_fn, configs, hparams)
+      model_fn = model_lib.create_model_fn(detection_model_fn, configs, hparams)
-  def _assert_outputs_for_predict(self, configs):
+  def _assert_model_fn_for_predict(self, configs):
-      model_fn = model.create_model_fn(detection_model_fn, configs, hparams)
+      model_fn = model_lib.create_model_fn(detection_model_fn, configs, hparams)
-  def testModelFnInTrainMode(self):
+  def test_model_fn_in_train_mode(self):
-    self._assert_outputs_for_train_eval(configs, tf.estimator.ModeKeys.TRAIN)
+    self._assert_model_fn_for_train_eval(configs, tf.estimator.ModeKeys.TRAIN)
-  def testModelFnInEvalMode(self):
+  def test_model_fn_in_eval_mode(self):
-    self._assert_outputs_for_train_eval(configs, tf.estimator.ModeKeys.EVAL)
+    self._assert_model_fn_for_train_eval(configs, tf.estimator.ModeKeys.EVAL)
-  def testModelFnInPredictMode(self):
+  def test_model_fn_in_predict_mode(self):
-  def testExperiment(self):
+    self._assert_model_fn_for_predict(configs)
-    self.assertTrue(tf.gfile.Exists(pipeline_config_path))
+    run_config = tf.estimator.RunConfig()
-    unbatched_tensor_dict = model.unstack_batch(
+    unbatched_tensor_dict = model_lib.unstack_batch(
-    unbatched_tensor_dict = model.unstack_batch(
+    unbatched_tensor_dict = model_lib.unstack_batch(
-  tf.app.run()
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-          classification_loss.op.name: classification_loss
+          str(localization_loss.op.name): localization_loss,
-            tf.summary.scalar('regularization_loss', regularization_loss)
+          losses_dict['Loss/regularization_loss'] = regularization_loss
-               min_depth=16):
+               min_depth=16,
-              scope=scope)
+              scope=scope,
-      return sc
+    with (slim.arg_scope([slim.batch_norm], **batch_norm_params)
-class HyperparamsBuilderTest(tf.test.TestCase):
+def _get_scope_key(op):
-    return getattr(op, '_key_op', str(op))
+class HyperparamsBuilderTest(tf.test.TestCase):
-    self.assertTrue(self._get_scope_key(slim.conv2d) in scope)
+    self.assertTrue(_get_scope_key(slim.conv2d) in scope)
-    self.assertTrue(self._get_scope_key(slim.separable_conv2d) in scope)
+    self.assertTrue(_get_scope_key(slim.separable_conv2d) in scope)
-    self.assertTrue(self._get_scope_key(slim.conv2d_transpose) in scope)
+    self.assertTrue(_get_scope_key(slim.conv2d_transpose) in scope)
-    self.assertTrue(self._get_scope_key(slim.fully_connected) in scope)
+    self.assertTrue(_get_scope_key(slim.fully_connected) in scope)
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    batch_norm_params = conv_scope_arguments['normalizer_params']
+    batch_norm_params = scope[_get_scope_key(slim.batch_norm)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    batch_norm_params = conv_scope_arguments['normalizer_params']
+    batch_norm_params = scope[_get_scope_key(slim.batch_norm)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    batch_norm_params = conv_scope_arguments['normalizer_params']
+    batch_norm_params = scope[_get_scope_key(slim.batch_norm)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-    conv_scope_arguments = scope.values()[0]
+    conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
-      conv_hyperparams, reuse_weights, use_explicit_padding, use_depthwise)
+      conv_hyperparams, reuse_weights, use_explicit_padding, use_depthwise,
-               use_depthwise=False):
+               use_depthwise=False,
-        and separable_conv2d ops.
+        and separable_conv2d ops in the layers that are added on top of the
-
+      override_base_feature_extractor_hyperparams: Whether to override
-from object_detection.models import ssd_mobilenet_v1_feature_extractor
+from object_detection.utils import context_manager
-    ssd_mobilenet_v1_feature_extractor.SSDMobileNetV1FeatureExtractor):
+class EmbeddedSSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
-               use_depthwise=False):
+               use_depthwise=False,
-        and separable_conv2d ops.
+        and separable_conv2d ops in the layers that are added on top of the
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
-              scope=scope)
+    with tf.variable_scope('MobilenetV1',
-                self.conv_hyperparams_fn))
+                self.conv_hyperparams_fn,
-               use_depthwise=False):
+               use_depthwise=False,
-        and separable_conv2d ops.
+        and separable_conv2d ops in the layers that are added on top of the
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
-        self.conv_hyperparams_fn)
+        self.conv_hyperparams_fn,
-               use_depthwise=False):
+               use_depthwise=False,
-        and separable_conv2d ops.
+        and separable_conv2d ops in the layers that are added on top of the
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
-        self.conv_hyperparams_fn)
+        self.conv_hyperparams_fn,
-               use_depthwise=False):
+               use_depthwise=False,
-        and separable_conv2d ops.
+        and separable_conv2d ops in the layers that are added on top of the
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
-              scope=scope)
+          with slim.arg_scope([slim.batch_norm], fused=False):
-               use_depthwise=False):
+               use_depthwise=False,
-        and separable_conv2d ops.
+        and separable_conv2d ops in the layers that are added on top of the
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
-              scope=scope)
+        with (slim.arg_scope(self._conv_hyperparams_fn())
-               use_depthwise=False):
+               use_depthwise=False,
-        and separable_conv2d ops.
+        and separable_conv2d ops in the layers that are added on top of the
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding,
-            scope=scope)
+        with (slim.arg_scope(self._conv_hyperparams_fn())
-               conv_hyperparams,
+               conv_hyperparams_fn,
-    """Resnet50 v1 FPN Feature Extractor for SSD Models.
+               use_depthwise=False,
-      min_depth: minimum feature extractor depth.
+        UNUSED currently.
-      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        reuse_weights, use_explicit_padding)
+        conv_hyperparams_fn, resnet_v1.resnet_v1_50, 'resnet_v1_50', 'fpn',
-               conv_hyperparams,
+               conv_hyperparams_fn,
-    """Resnet101 v1 FPN Feature Extractor for SSD Models.
+               use_depthwise=False,
-      min_depth: minimum feature extractor depth.
+        UNUSED currently.
-      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        reuse_weights, use_explicit_padding)
+        conv_hyperparams_fn, resnet_v1.resnet_v1_101, 'resnet_v1_101', 'fpn',
-               conv_hyperparams,
+               conv_hyperparams_fn,
-    """Resnet152 v1 FPN Feature Extractor for SSD Models.
+               use_depthwise=False,
-      min_depth: minimum feature extractor depth.
+        UNUSED currently.
-      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        reuse_weights, use_explicit_padding)
+        conv_hyperparams_fn, resnet_v1.resnet_v1_152, 'resnet_v1_152', 'fpn',
-      poses.append(obj['pose'].encode('utf8'))
+  for obj in data['object']:
-        parsers.BaseParser(),
+        parsers.BaseParser(multi_gpu=True, num_gpu=False),
-             num_parallel_calls=1, multi_gpu=False):
+def input_fn(is_training, data_dir, batch_size, num_epochs=1):
-      examples_per_epoch=num_images, multi_gpu=multi_gpu)
+      parse_record, num_epochs,
-                                  multi_gpu=multi_gpu)
+  def cifar10_model_fn_helper(self, mode, version, dtype):
-                                 multi_gpu=True)
+                                 dtype=tf.float32)
-  def test_cifar10_model_fn_train_mode_multi_gpu_v2(self):
+  def test_cifar10_model_fn_trainmode__v2(self):
-                                 multi_gpu=True)
+                                 dtype=tf.float32)
-    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=1)
+    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=1,
-    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=2)
+    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=2,
-    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=1)
+    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=1,
-    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=2)
+    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=2,
-             num_parallel_calls=1, multi_gpu=False):
+def input_fn(is_training, data_dir, batch_size, num_epochs=1):
-      multi_gpu=multi_gpu)
+      num_epochs
-  def _resnet_model_fn_helper(self, mode, version, dtype, multi_gpu):
+  def resnet_model_fn_helper(self, mode, version, dtype):
-                                 multi_gpu=multi_gpu)
+    tf.train.create_global_step()
-                                multi_gpu=True)
+                                dtype=tf.float32)
-  def test_resnet_model_fn_train_mode_multi_gpu_v2(self):
+  def test_resnet_model_fn_train_mode_v2(self):
-                                multi_gpu=True)
+                                dtype=tf.float32)
-    self.resnet_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=1)
+    self.resnet_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=1,
-    self.resnet_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=2)
+    self.resnet_model_fn_helper(tf.estimator.ModeKeys.EVAL, version=2,
-    self.resnet_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=1)
+    self.resnet_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=1,
-    self.resnet_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=2)
+    self.resnet_model_fn_helper(tf.estimator.ModeKeys.PREDICT, version=2,
-                           examples_per_epoch=0, multi_gpu=False):
+                           parse_record_fn, num_epochs=1):
-      when that is handled directly by Estimator.
+
-  dataset = dataset.batch(batch_size)
+  # Parse the raw records into images and labels. Testing has shown that setting
-  dataset = dataset.prefetch(1)
+  # critical training path. Setting buffer_size to tf.contrib.data.AUTOTUNE
-  def input_fn(is_training, data_dir, batch_size, *args):  # pylint: disable=unused-argument
+  def input_fn(is_training, data_dir, batch_size, *args, **kwargs):  # pylint: disable=unused-argument
-                    loss_filter_fn=None, multi_gpu=False,
+                    data_format, version, loss_scale, loss_filter_fn=None,
-      optimizer = tf.contrib.estimator.TowerOptimizer(optimizer)
+        momentum=momentum
-      tf.argmax(labels, axis=1), predictions['classes'])
+  if not tf.contrib.distribute.has_distribution_strategy():
-def validate_batch_size_for_multi_gpu(batch_size):
+def per_device_batch_size(batch_size, num_gpus):
-  Note that this should eventually be handled by replicate_model_fn
+  Note that this should eventually be handled by DistributionStrategies
-    batch_size: the number of examples processed in each training batch.
+    batch_size: Global batch size to be divided among devices. This should be
-    ValueError: if no GPUs are found, or selected batch_size is invalid.
+    ValueError: if batch_size is not divisible by number of devices
-                     'were found. To use CPU, run without --multi_gpu.')
+  if num_gpus <= 1:
-           'Found {} GPUs with a batch size of {}; try --batch_size={} instead.'
+           'must be a multiple of the number of available GPUs. Found {} '
-                                                session_config=session_config)
+  if flags.num_gpus == 0:
-                            flags.num_parallel_calls, flags.multi_gpu)
+      return input_function(
-                            1, flags.num_parallel_calls, flags.multi_gpu)
+      return input_function(
-        parsers.PerformanceParser(),
+        parsers.BaseParser(multi_gpu=False),
-    batch_size: Create a flag to specify the batch size.
+    batch_size: Create a flag to specify the global batch size.
-               hooks=True):
+               stop_threshold=True, batch_size=True,
-          help="[default: %(default)s] Batch size for training and evaluation.",
+          help="[default: %(default)s] Global batch size for training and "
-        parsers.BaseParser(),
+        parsers.BaseParser(multi_gpu=True, num_gpu=False),
-    super(WideDeepArgParser, self).__init__(parents=[parsers.BaseParser()])
+    super(WideDeepArgParser, self).__init__(parents=[
-           with pixel values varying between [0, 1].
+           with pixel values varying between [0, 255].
-    image = tf.clip_by_value(image, 0.0, 1.0)
+    image = tf.clip_by_value(image, 0.0, 255.0)
-           with pixel values varying between [0, 1].
+           with pixel values varying between [0, 255].
-  Makes sure the output image is still between 0 and 1.
+  Makes sure the output image is still between 0 and 255.
-           with pixel values varying between [0, 1].
+           with pixel values varying between [0, 255].
-    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
+    image = tf.image.adjust_brightness(image / 255, delta) * 255
-  Makes sure the output image is still between 0 and 1.
+  Makes sure the output image is still between 0 and 255.
-           with pixel values varying between [0, 1].
+           with pixel values varying between [0, 255].
-    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
+    image = tf.image.adjust_contrast(image / 255, contrast_factor) * 255
-  Makes sure the output image is still between 0 and 1.
+  Makes sure the output image is still between 0 and 255.
-           with pixel values varying between [0, 1].
+           with pixel values varying between [0, 255].
-    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
+    image = tf.image.adjust_hue(image / 255, delta) * 255
-  Makes sure the output image is still between 0 and 1.
+  Makes sure the output image is still between 0 and 255.
-           with pixel values varying between [0, 1].
+           with pixel values varying between [0, 255].
-    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
+    image = tf.image.adjust_saturation(image / 255, saturation_factor) * 255
-  and saturation changes. Makes sure the output image is still between 0 and 1.
+  and saturation changes. Makes sure the output image is still between 0 and 255.
-           with pixel values varying between [0, 1].
+           with pixel values varying between [0, 255].
-  variables_averages_op = variable_averages.apply(tf.trainable_variables())
+  with tf.control_dependencies([apply_gradient_op]):
-  return train_op
+  return variables_averages_op
-    with open(expected_file, "wb") as f:
+    with tf.gfile.Open(expected_file, "wb") as f:
-        with open(os.path.join(data_dir, "results.json"), "wt") as f:
+        with tf.gfile.Open(os.path.join(data_dir, "results.json"), "w") as f:
-      with open(os.path.join(data_dir, "tf_version.json"), "wt") as f:
+      with tf.gfile.Open(os.path.join(data_dir, "tf_version.json"), "w") as f:
-    with open(expected_file, "rb") as f:
+    with tf.gfile.Open(expected_file, "rb") as f:
-    with open(os.path.join(data_dir, "tf_version.json"), "rt") as f:
+    with tf.gfile.Open(os.path.join(data_dir, "tf_version.json"), "r") as f:
-        with open(os.path.join(data_dir, "results.json"), "rt") as f:
+        with tf.gfile.Open(os.path.join(data_dir, "results.json"), "r") as f:
-              activation=tf.nn.relu), max_pool,
+              activation=tf.nn.relu),
-              activation=tf.nn.relu), max_pool,
+              activation=tf.nn.relu),
-      typically faster on CPUs. See
+    data_format: Either 'channels_first' or 'channels_last'. 'channels_first' is
-      L.Dense(10)])
+  l = tf.keras.layers
-                    predictions=tf.argmax(logits, axis=1)),
+                    labels=labels, predictions=tf.argmax(logits, axis=1)),
-        flags.stop_threshold, eval_results['accuracy']):
+    if model_helpers.past_stop_threshold(flags.stop_threshold,
-class Model(tf.keras.Model):
+def create_model(data_format):
-  """
+  But uses the tf.keras API.
-    return self.fc2(y)
+  Returns:
-  model = Model(params['data_format'])
+  model = create_model(params['data_format'])
-  model = mnist.Model(data_format)
+  model = mnist.create_model(data_format)
-  model = mnist.Model(data_format())
+  model = mnist.create_model(data_format())
-  model = mnist.Model(data_format())
+  model = mnist.create_model(data_format())
-  model = mnist.Model("channels_last")
+  model = mnist.create_model("channels_last")
-      return value.encode() if isinstance(value, str) else value
+      return value.encode() if isinstance(value, str) and six.PY3 else value
-            epochs_between_evals=False, multi_gpu=False, hooks=False),
+        parsers.EagerParser(),
-               multi_gpu=True, hooks=True):
+               train_epochs=True, epochs_between_evals=True,
-               version=resnet_model.DEFAULT_VERSION):
+               version=resnet_model.DEFAULT_VERSION,
-        data_format=data_format)
+        data_format=data_format,
-                                         multi_gpu=params['multi_gpu'])
+  return resnet_run_loop.resnet_model_fn(
-      self.assertEqual(eval_metric_ops['accuracy'][1].dtype, tf.float32)
+    self._cifar10_model_fn_helper(mode=mode, version=version, dtype=tf.float32,
-  def test_cifar10model_shape(self):
+  def _test_cifar10model_shape(self, version):
-      output = model(fake_input, training=True)
+    model = cifar10_main.Cifar10Model(32, data_format='channels_last',
-      self.assertAllEqual(output.shape, (batch_size, num_classes))
+  def test_cifar10model_shape_v2(self):
-               version=resnet_model.DEFAULT_VERSION):
+               version=resnet_model.DEFAULT_VERSION,
-        data_format=data_format)
+        data_format=data_format,
-                                         multi_gpu=params['multi_gpu'])
+  return resnet_run_loop.resnet_model_fn(
-  def tensor_shapes_helper(self, resnet_size, version, with_gpu=False):
+  def _tensor_shapes_helper(self, resnet_size, version, dtype, with_gpu):
-        use_gpu=with_gpu, force_gpu=with_gpu):
+        graph=graph, use_gpu=with_gpu, force_gpu=with_gpu):
-          resnet_size,
+          resnet_size=resnet_size,
-          version=version)
+          version=version,
-      dense = graph.get_tensor_by_name('final_dense:0')
+      initial_conv = graph.get_tensor_by_name('resnet_model/initial_conv:0')
-  def resnet_model_fn_helper(self, mode, version, multi_gpu=False):
+  def _resnet_model_fn_helper(self, mode, version, dtype, multi_gpu):
-      self.assertEqual(eval_metric_ops['accuracy'][1].dtype, tf.float32)
+    with tf.Graph().as_default() as g:
-  def test_imagenetmodel_shape(self):
+  def _test_imagenetmodel_shape(self, version):
-      output = model(fake_input, training=True)
+    model = imagenet_main.ImagenetModel(
-      self.assertAllEqual(output.shape, (batch_size, num_classes))
+  def test_imagenetmodel_shape_v2(self):
-               final_size, version=DEFAULT_VERSION, data_format=None):
+               final_size, version=DEFAULT_VERSION, data_format=None,
-    return inputs
+    with self._model_variable_scope():
-                    data_format, version, loss_filter_fn=None, multi_gpu=False):
+                    data_format, version, loss_scale,
-  model = model_class(resnet_size, data_format, version=version)
+  features = tf.cast(features, dtype)
-      [tf.nn.l2_loss(v) for v in tf.trainable_variables()
+      # loss is computed using fp32 for numerical stability.
-    train_op = tf.group(optimizer.minimize(loss, global_step), update_ops)
+    train_op = tf.group(minimize_op, update_ops)
-    benchmark_logger.log_run_info("resnet")
+    benchmark_logger.log_run_info('resnet')
-
+import tensorflow as tf
-               intra_op=True, use_synthetic_data=True, max_train_steps=True):
+               intra_op=True, use_synthetic_data=True, max_train_steps=True,
-  return np.random.random_sample(shape)
+  # Make sure we return float32, as float64 will not get cast automatically.
-    # range of labels = [0, dataset.num_classes). Note the ignore_lable regions
+    # range of labels = [0, dataset.num_classes). Note the ignore_label regions
-  zipped_filepath = filepath + '.gz'
+  _, zipped_filepath = tempfile.mkstemp(suffix='.gz')
-  with gzip.open(zipped_filepath, 'rb') as f_in, open(filepath, 'wb') as f_out:
+  with gzip.open(zipped_filepath, 'rb') as f_in, \
-  """ Convert the ADE20k dataset into into tfrecord format (SSTable).
+  """ Converts the ADE20k dataset into into tfrecord format (SSTable).
-    dataset_label_dir: dir in which the annotations locates
+    dataset_split: Dataset split (e.g., train, val).
-  img_names = glob.glob(os.path.join(dataset_dir, '*.jpg'))
+  img_names = tf.gfile.Glob(os.path.join(dataset_dir, '*.jpg'))
-  num_per_shard = int(math.ceil(num_images) / float(_NUM_SHARDS))
+  num_per_shard = int(math.ceil(num_images / float(_NUM_SHARDS)))
-  dataset_splits = glob.glob(os.path.join(FLAGS.list_folder, '*.txt'))
+  dataset_splits = tf.gfile.Glob(os.path.join(FLAGS.list_folder, '*.txt'))
-def get_extra_layer_scopes():
+def get_extra_layer_scopes(last_layers_contain_logits_only=False):
-  ]
+  if last_layers_contain_logits_only:
-      last_layers = model.get_extra_layer_scopes()
+      last_layers = model.get_extra_layer_scopes(FLAGS.last_layers_contain_logits_only)
-  exclude_list = ['global_step', 'logits']
+  exclude_list = ['global_step']
-  """Extracts features by the parituclar model_variant.
+  """Extracts features by the particular model_variant.
-                                   is_training)
+    conv_hyperparams_fn = argscope_fn(conv_box_predictor.conv_hyperparams,
-        conv_hyperparams=conv_hyperparams,
+        conv_hyperparams_fn=conv_hyperparams_fn,
-                                   is_training)
+    conv_hyperparams_fn = argscope_fn(conv_box_predictor.conv_hyperparams,
-        conv_hyperparams=conv_hyperparams,
+        conv_hyperparams_fn=conv_hyperparams_fn,
-    conv_hyperparams = None
+    fc_hyperparams_fn = argscope_fn(mask_rcnn_box_predictor.fc_hyperparams,
-                                     is_training)
+      conv_hyperparams_fn = argscope_fn(
-        fc_hyperparams=fc_hyperparams,
+        fc_hyperparams_fn=fc_hyperparams_fn,
-        conv_hyperparams=conv_hyperparams,
+        conv_hyperparams_fn=conv_hyperparams_fn,
-                                   is_training)
+    conv_hyperparams_fn = argscope_fn(rfcn_box_predictor.conv_hyperparams,
-        conv_hyperparams=conv_hyperparams,
+        conv_hyperparams_fn=conv_hyperparams_fn,
-    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams
+    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams_fn
-    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams
+    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams_fn
-    self.assertEqual(box_predictor._fc_hyperparams, 'arg_scope')
+    self.assertEqual(box_predictor._fc_hyperparams_fn, 'arg_scope')
-    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams
+    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams_fn
-      fields.InputDataFields.true_image_shape: [3]
+      fields.InputDataFields.true_image_shape: [3],
-    arg_scope: tf-slim arg_scope containing hyperparameters for ops.
+    arg_scope_fn: A function to construct tf-slim arg_scope containing
-    return sc
+  def scope_fn():
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=False)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    scope_fn = hyperparams_builder.build(conv_hyperparams_proto,
-                                 inplace_batchnorm_update=False):
+                                 reuse_weights=None):
-                                 inplace_batchnorm_update)
+  return feature_extractor_class(
-      inplace_batchnorm_update=ssd_config.inplace_batchnorm_update)
+      is_training=is_training)
-      normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize)
+      normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize,
-  first_stage_box_predictor_arg_scope = hyperparams_builder.build(
+  first_stage_box_predictor_arg_scope_fn = hyperparams_builder.build(
-      first_stage_box_predictor_arg_scope,
+      'first_stage_box_predictor_arg_scope_fn':
-          batch_norm_trainable: true
+        freeze_batchnorm: true
-    self.assertTrue(model._feature_extractor._batch_norm_trainable)
+    self.assertTrue(model._freeze_batchnorm)
-               conv_hyperparams,
+               conv_hyperparams_fn,
-        layers.
+      conv_hyperparams_fn: A function to construct tf-slim arg_scope with
-    self._conv_hyperparams = conv_hyperparams
+    self._conv_hyperparams_fn = conv_hyperparams_fn
-    with slim.arg_scope(self._conv_hyperparams):
+    with slim.arg_scope(self._conv_hyperparams_fn()):
-               fc_hyperparams,
+               fc_hyperparams_fn,
-               conv_hyperparams=None,
+               conv_hyperparams_fn=None,
-        connected ops.
+      fc_hyperparams_fn: A function to generate tf-slim arg_scope with
-        ops.
+      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
-    self._fc_hyperparams = fc_hyperparams
+    self._fc_hyperparams_fn = fc_hyperparams_fn
-    self._conv_hyperparams = conv_hyperparams
+    self._conv_hyperparams_fn = conv_hyperparams_fn
-        self._conv_hyperparams is None):
+        self._conv_hyperparams_fn is None):
-    with slim.arg_scope(self._fc_hyperparams):
+    with slim.arg_scope(self._fc_hyperparams_fn()):
-    with slim.arg_scope(self._conv_hyperparams):
+    with slim.arg_scope(self._conv_hyperparams_fn()):
-               conv_hyperparams,
+               conv_hyperparams_fn,
-      min_depth: Minumum feature depth prior to predicting box encodings
+      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
-    self._conv_hyperparams = conv_hyperparams
+    self._conv_hyperparams_fn = conv_hyperparams_fn
-        with slim.arg_scope(self._conv_hyperparams), \
+        with slim.arg_scope(self._conv_hyperparams_fn()), \
-               conv_hyperparams,
+               conv_hyperparams_fn,
-      conv_hyperparams: Slim arg_scope with hyperparameters for convolution ops.
+      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
-    self._conv_hyperparams = conv_hyperparams
+    self._conv_hyperparams_fn = conv_hyperparams_fn
-        with slim.arg_scope(self._conv_hyperparams):
+        with slim.arg_scope(self._conv_hyperparams_fn()):
-        fc_hyperparams=self._build_arg_scope_with_hyperparams(),
+        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-          fc_hyperparams=self._build_arg_scope_with_hyperparams(),
+          fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-        fc_hyperparams=self._build_arg_scope_with_hyperparams(),
+        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-        conv_hyperparams=self._build_arg_scope_with_hyperparams(
+        conv_hyperparams_fn=self._build_arg_scope_with_hyperparams(
-        fc_hyperparams=self._build_arg_scope_with_hyperparams(),
+        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-          fc_hyperparams=self._build_arg_scope_with_hyperparams(),
+          fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+        conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+        conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+        conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+        conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-    If masks, or keypoints are not None, the function also returns:
+    If multiclass_scores, masks, or keypoints are not None, the function also
-    If label_scores, masks, or keypoints is not None, the function also returns:
+    If label_scores, multiclass_scores, masks, or keypoints is not None, the
-    label_scores: new scores.
+    If label_scores, multiclass_scores, masks, or keypoints is not None, the
-    cropped_label_scores = result[3]
+    cropped_label_scores = result[index]
-    label_scores: new label scores.
+    If label_scores, masks, keypoints, or multiclass_scores is not None, the
-    label_scores: new label scores.
+    If masks, or keypoints is not None, the function also returns:
-    label_scores: new label scores.
+    If label_scores, multiclass_scores, masks, or keypoints  is not None, the
-          if t is not None),
+          t for t in (image, boxes, labels, label_scores, multiclass_scores,
-        selected_label_scores,
+        label_scores=selected_label_scores,
-      tuple(t for t in (image, boxes, labels, label_scores) if t is not None),
+      tuple(t for t in (image, boxes, labels, label_scores, multiclass_scores)
-    If masks or keypoints is not None, the function also returns:
+    If mulitclass_scores, masks, or keypoints is not None, the function also
-      preprocess_vars_cache)
+      image,
-      new_keypoints,
+      label_scores=new_label_scores,
-    If masks or keypoints is not None, the function also returns:
+    If multiclass_scores, masks, or keypoints is not None, the function also
-      preprocess_vars_cache)
+      image,
-      new_keypoints,
+      masks=new_masks,
-    result.insert(2, new_label_scores)
+  i = 3
-          groundtruth_keypoints,),
+          groundtruth_keypoints,
-          groundtruth_keypoints,),
+          groundtruth_keypoints,
-          groundtruth_keypoints,),
+          groundtruth_keypoints,
-          groundtruth_instance_masks,),
+          groundtruth_instance_masks,
-          groundtruth_keypoints,),
+      random_crop_image: (fields.InputDataFields.image,
-                              groundtruth_label_scores),
+                              groundtruth_label_scores,
-          groundtruth_keypoints,),
+          groundtruth_keypoints,
-          groundtruth_keypoints,),
+          groundtruth_keypoints,
-          groundtruth_keypoints,),
+          groundtruth_keypoints,
-          groundtruth_instance_masks,),
+          groundtruth_instance_masks,
-          groundtruth_instance_masks,),
+          groundtruth_instance_masks,
-          groundtruth_keypoints,),
+          groundtruth_keypoints,
-          groundtruth_instance_masks,),
+          groundtruth_instance_masks,
-          groundtruth_keypoints,),
+          groundtruth_keypoints
-                            groundtruth_label_scores),
+                            groundtruth_label_scores,
-          groundtruth_keypoints,),
+          fields.InputDataFields.groundtruth_classes, groundtruth_label_scores,
-          groundtruth_keypoints,),
+          groundtruth_keypoints,
-        (preprocessor.ssd_random_crop_fixed_aspect_ratio, {})]
+    preprocessing_options = [(preprocessor.normalize_image, {
-        fields.InputDataFields.groundtruth_classes: labels
+        fields.InputDataFields.groundtruth_classes: labels,
-               first_stage_box_predictor_arg_scope,
+               first_stage_box_predictor_arg_scope_fn,
-        separable_conv2d and fully_connected ops for the RPN box predictor.
+      first_stage_box_predictor_arg_scope_fn: A function to construct tf-slim
-        first_stage_box_predictor_arg_scope)
+    self._first_stage_box_predictor_arg_scope_fn = (
-        conv_hyperparams=self._first_stage_box_predictor_arg_scope,
+        conv_hyperparams_fn=self._first_stage_box_predictor_arg_scope_fn,
-    with slim.arg_scope(self._first_stage_box_predictor_arg_scope):
+    with slim.arg_scope(self._first_stage_box_predictor_arg_scope_fn()):
-    first_stage_box_predictor_arg_scope = (
+    first_stage_box_predictor_arg_scope_fn = (
-        first_stage_box_predictor_arg_scope,
+        'first_stage_box_predictor_arg_scope_fn':
-               first_stage_box_predictor_arg_scope,
+               first_stage_box_predictor_arg_scope_fn,
-        separable_conv2d and fully_connected ops for the RPN box predictor.
+      first_stage_box_predictor_arg_scope_fn: A function to generate tf-slim
-        first_stage_box_predictor_arg_scope,
+        first_stage_box_predictor_arg_scope_fn,
-               batch_norm_trainable=True,
+               conv_hyperparams_fn,
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-        pretrained batch norm params.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        batch norm statistics.
+
-    self._inplace_batchnorm_update = inplace_batchnorm_update
+    self._conv_hyperparams_fn = conv_hyperparams_fn
-               normalize_loc_loss_by_codesize=False):
+               normalize_loc_loss_by_codesize=False,
-      feature_maps = self._feature_extractor.extract_features(
+    batchnorm_updates_collections = (None if self._inplace_batchnorm_update
-    return predictions_dict
+      self._anchors = box_list_ops.concatenate(
-        conv_hyperparams=None)
+        conv_hyperparams_fn=None)
-        normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize)
+        normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize,
-               batch_norm_trainable=True,
+               conv_hyperparams_fn,
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-        pretrained batch norm params.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
-  def _extract_features(self, preprocessed_inputs):
+  def extract_features(self, preprocessed_inputs):
-    with slim.arg_scope(self._conv_hyperparams):
+    with slim.arg_scope(self._conv_hyperparams_fn()):
-                                is_training=True, batch_norm_trainable=True):
+                                is_training=True):
-                conv_hyperparams, batch_norm_trainable))
+                self.conv_hyperparams_fn))
-               batch_norm_trainable=True,
+               conv_hyperparams_fn,
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-        pretrained batch norm params.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
-  def _extract_features(self, preprocessed_inputs):
+  def extract_features(self, preprocessed_inputs):
-    with slim.arg_scope(self._conv_hyperparams):
+    with slim.arg_scope(self._conv_hyperparams_fn()):
-                                is_training=True, batch_norm_trainable=True):
+                                is_training=True):
-        training or not
+
-        conv_hyperparams, batch_norm_trainable)
+        self.conv_hyperparams_fn)
-               batch_norm_trainable=True,
+               conv_hyperparams_fn,
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-        pretrained batch norm params.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
-  def _extract_features(self, preprocessed_inputs):
+  def extract_features(self, preprocessed_inputs):
-    with slim.arg_scope(self._conv_hyperparams):
+    with slim.arg_scope(self._conv_hyperparams_fn()):
-                                is_training=True, batch_norm_trainable=True):
+                                is_training=True):
-        training or not
+
-        conv_hyperparams, batch_norm_trainable)
+        self.conv_hyperparams_fn)
-               batch_norm_trainable=True,
+               conv_hyperparams_fn,
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-        pretrained batch norm params.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
-  def _extract_features(self, preprocessed_inputs):
+  def extract_features(self, preprocessed_inputs):
-              is_training=(self._batch_norm_trainable and self._is_training))):
+          mobilenet_v1.mobilenet_v1_arg_scope(is_training=None)):
-      with slim.arg_scope(self._conv_hyperparams):
+      with slim.arg_scope(self._conv_hyperparams_fn()):
-                                use_explicit_padding=False):
+                                is_training=True, use_explicit_padding=False):
-        conv_hyperparams, batch_norm_trainable=batch_norm_trainable,
+        self.conv_hyperparams_fn,
-               batch_norm_trainable=True,
+               conv_hyperparams_fn,
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-        pretrained batch norm params.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise)
-  def _extract_features(self, preprocessed_inputs):
+  def extract_features(self, preprocessed_inputs):
-              bn_decay=0.9997)), \
+          mobilenet_v2.training_scope(is_training=None, bn_decay=0.9997)), \
-        with slim.arg_scope(self._conv_hyperparams):
+        with slim.arg_scope(self._conv_hyperparams_fn()):
-        conv_hyperparams,
+        self.conv_hyperparams_fn,
-               conv_hyperparams,
+               conv_hyperparams_fn,
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
-        use_explicit_padding, inplace_batchnorm_update)
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding)
-  def _extract_features(self, preprocessed_inputs):
+  def extract_features(self, preprocessed_inputs):
-            is_training=self._is_training and self._batch_norm_trainable,
+            is_training=None,
-      with slim.arg_scope(self._conv_hyperparams):
+      with slim.arg_scope(self._conv_hyperparams_fn()):
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-        inplace_batchnorm_update)
+        reuse_weights, use_explicit_padding)
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-        inplace_batchnorm_update)
+        reuse_weights, use_explicit_padding)
-               inplace_batchnorm_update=False):
+               use_depthwise=False):
-        inplace_batchnorm_update)
+        reuse_weights, use_explicit_padding)
-        use_explicit_padding=use_explicit_padding)
+        self.conv_hyperparams_fn, use_explicit_padding=use_explicit_padding)
-            conv_hyperparams, batch_norm_trainable,
+            is_training,
-            conv_hyperparams, batch_norm_trainable,
+            is_training,
-            image_resizer_config.fixed_shape_resizer.width]
+    return [
-  """Reads configuration from a pipeline_pb2.TrainEvalPipelineConfig.
+  """Reads config from a file containing pipeline_pb2.TrainEvalPipelineConfig.
-  a `TrainEvalPipelineConfig` object.
+  This function performs the inverse operation of
-  if meta_architecture == "ssd":
+  elif meta_architecture == "ssd":
-    if item.id == 0 and item.name != 'background':
+    if (item.id == 0 and item.name != 'background' and
-    labels: A boolean numpy array representing true/false positive labels
+    labels: A float numpy array representing weighted true/false positive labels
-    raise ValueError("labels must be single dimension bool numpy array")
+  if not isinstance(labels, np.ndarray) or len(labels.shape) != 1:
-      scores, np.ndarray) or len(scores.shape) != 1:
+  if not isinstance(scores, np.ndarray) or len(scores.shape) != 1:
-  false_positive_labels = 1 - true_positive_labels
+  false_positive_labels = (true_positive_labels <= 0).astype(float)
-                                                             np.ndarray):
+  if not isinstance(precision, np.ndarray) or not isinstance(
-      np.nan,
+      num_gt_imgs_per_class == 0, np.nan,
-                                                       dtype=int)
+    num_images_correctly_detected_per_class = np.array(
-                                                       dtype=int)
+    num_images_correctly_detected_per_class = np.array(
-                                   dtype=float)
+    processed_precision = np.array(
-               evaluate_masks=False):
+               evaluate_masks=False,
-        label_id_offset=self._label_id_offset)
+        label_id_offset=self._label_id_offset,
-               evaluate_corlocs=False):
+               evaluate_corlocs=False,
-        metric_prefix='OpenImagesV2')
+        metric_prefix=metric_prefix,
-               label_id_offset=0):
+               label_id_offset=0,
-        nms_max_output_boxes=nms_max_output_boxes)
+        nms_max_output_boxes=nms_max_output_boxes,
-    self.num_gt_instances_per_class = np.zeros(self.num_class, dtype=int)
+    self.num_gt_instances_per_class = np.zeros(self.num_class, dtype=float)
-      self.num_gt_instances_per_class[class_index] += num_gt_instances
+      num_groupof_gt_instances = self.group_of_weight * np.sum(
-        tp_fp_labels = np.array([], dtype=bool)
+        tp_fp_labels = np.array([], dtype=float)
-               nms_max_output_boxes=50):
+               nms_max_output_boxes=50,
-    """Evaluates detections as being tp, fp or ignored from a single image.
+    """Evaluates detections as being tp, fp or weighted from a single image.
-        group-of boxes and ignored if matched.
+        group-of boxes and weighted if matched.
-    ioa = np_box_mask_list_ops.ioa(gt_group_of_boxlist, detected_boxlist)
+    ioa = np.transpose(
-    ioa = np_box_list_ops.ioa(gt_group_of_boxlist, detected_boxlist)
+    ioa = np.transpose(
-    #    group-of boxes and ignored if matched.
+    #    group-of boxes and scored with weight w per ground truth box is
-      max_overlap_group_of_gt = np.max(ioa, axis=0)
+    if ioa.shape[1] > 0:
-            max_overlap_group_of_gt[i] >= self.matching_iou_threshold):
+            ioa[i, gt_id] >= self.matching_iou_threshold):
-                      & ~is_matched_to_group_of_box]
+          scores_group_of[gt_id] = max(scores_group_of[gt_id], scores[i])
-if __name__ == '__main__':
+if __name__ == "__main__":
-    data_format = data_format
+    data_format = flags.data_format
-  samples[common.LABEL] = tf.identity(samples[common.LABEL], 'input_label')
+  # add name to input and label nodes so we can add to summary
-    name = OUTPUT_MERGED_LOGITS_NODE
+    name = common.OUTPUT_TYPE
-        summaries.add(tf.summary.image('samples/%s' % OUTPUT_MERGED_LOGITS_NODE, predictions))
+        summary_image = graph.get_tensor_by_name(
-from official.utils.logging import hooks_helper
+from official.utils.logs import hooks_helper
-from official.utils.logging import logger
+from official.utils.logs import hooks_helper
-               "See official.utils.logging.hooks_helper for details.",
+               "See official.utils.logs.hooks_helper for details.",
-import tensorflow as tf # pylint: disable=g-bad-import-order
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-from official.utils.logging import logger
+from official.utils.logs import logger
-from official.utils.logging import metric_hook
+from official.utils.logs import hooks
-from official.utils.logging import hooks_helper
+from official.utils.logs import hooks_helper
-from official.utils.logging import hooks
+from official.utils.logs import hooks
-from official.utils.logging import logger
+from official.utils.logs import logger
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-from official.utils.logging import logger
+from official.utils.logs import logger
-from official.utils.logging import metric_hook
+from official.utils.logs import metric_hook  # pylint: disable=g-bad-import-order
-from official.utils.logging import hooks_helper
+from official.utils.logs import hooks_helper
-  flags.DEFINE_string('momentum', 0.9,
+  flags.DEFINE_float('momentum', 0.9,
-  exclude_list = ['global_step']
+  exclude_list = ['global_step', 'logits']
-      for endpoint, shape in endpoint_to_shape.iteritems():
+      for endpoint, shape in six.iteritems(endpoint_to_shape):
-      for endpoint, shape in endpoint_to_shape.iteritems():
+      for endpoint, shape in six.iteritems(endpoint_to_shape):
-      for endpoint, shape in endpoint_to_shape.iteritems():
+      for endpoint, shape in six.iteritems(endpoint_to_shape):
-        image_data = tf.gfile.FastGFile(image_files[i], 'r').read()
+        image_data = tf.gfile.FastGFile(image_files[i], 'rb').read()
-        seg_data = tf.gfile.FastGFile(label_files[i], 'r').read()
+        seg_data = tf.gfile.FastGFile(label_files[i], 'rb').read()
-  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))
+  def norm2bytes(value):
-        image_data = tf.gfile.FastGFile(image_filename, 'r').read()
+        image_data = tf.gfile.FastGFile(image_filename, 'rb').read()
-        seg_data = tf.gfile.FastGFile(seg_filename, 'r').read()
+        seg_data = tf.gfile.FastGFile(seg_filename, 'rb').read()
-    for metric_name, metric_value in metrics_to_values.iteritems():
+    for metric_name, metric_value in six.iteritems(metrics_to_values):
-        eval_op=metrics_to_updates.values(),
+        eval_op=list(metrics_to_updates.values()),
-  for output, num_classes in outputs_to_num_classes.iteritems():
+  for output, num_classes in six.iteritems(outputs_to_num_classes):
-  clone_batch_size = FLAGS.train_batch_size / config.num_clones
+  clone_batch_size = int(FLAGS.train_batch_size / config.num_clones)
-  for scale, logits in scales_to_logits.iteritems():
+  for scale, logits in six.iteritems(scales_to_logits):
-  with tf.Graph().as_default():
+  with tf.Graph().as_default() as graph:
-      FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)
+  if FLAGS.master is None and FLAGS.tpu is None:
-      cluster=tpu_cluster_resolver,
+      master=tpu_grpc_url,
-                            'object_detection/test_data/pets_examples.record'))
+  fname = os.path.join(tf.resource_loader.get_data_files_path(),
-          [1, height, width, 3]
+          [1, height, width, 3] or [1, height, width, 1]
-        features[fields.InputDataFields.image] (optional) is a
+        features[fields.InputDataFields.original_image] (optional) is a
-      labels = unstack_batch(labels, unpad_groundtruth_tensors=False)
+      # For evaling on train data, it is necessary to check whether groundtruth
-      ssd_config.inplace_batchnorm_update)
+      feature_extractor_config=ssd_config.feature_extractor,
-tf.flags.DEFINE_integer('num_eval_steps', 10000, 'Number of train steps.')
+tf.flags.DEFINE_integer('num_train_steps', None, 'Number of train steps.')
-                                 reuse_weights=None):
+                                 reuse_weights=None,
-                                 use_explicit_padding, use_depthwise)
+                                 use_explicit_padding, use_depthwise,
-                                                   is_training)
+  feature_extractor = _build_ssd_feature_extractor(
-    feature_extractor_config, is_training, reuse_weights=None):
+    feature_extractor_config, is_training, reuse_weights=None,
-      frcnn_config.feature_extractor, is_training)
+      frcnn_config.feature_extractor, is_training,
-               use_depthwise=False):
+               use_depthwise=False,
-    pass
+    batchnorm_updates_collections = (None if self._inplace_batchnorm_update
-               use_depthwise=False):
+               use_depthwise=False,
-        use_explicit_padding, use_depthwise)
+        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
-  def extract_features(self, preprocessed_inputs):
+  def _extract_features(self, preprocessed_inputs):
-               use_depthwise=False):
+               use_depthwise=False,
-        use_explicit_padding, use_depthwise)
+        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
-  def extract_features(self, preprocessed_inputs):
+  def _extract_features(self, preprocessed_inputs):
-               use_depthwise=False):
+               use_depthwise=False,
-        use_explicit_padding, use_depthwise)
+        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
-  def extract_features(self, preprocessed_inputs):
+  def _extract_features(self, preprocessed_inputs):
-               use_depthwise=False):
+               use_depthwise=False,
-        use_explicit_padding, use_depthwise)
+        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
-  def extract_features(self, preprocessed_inputs):
+  def _extract_features(self, preprocessed_inputs):
-               use_depthwise=False):
+               use_depthwise=False,
-        use_explicit_padding, use_depthwise)
+        use_explicit_padding, use_depthwise, inplace_batchnorm_update)
-  def extract_features(self, preprocessed_inputs):
+  def _extract_features(self, preprocessed_inputs):
-               use_depthwise=False):
+               use_depthwise=False,
-        use_explicit_padding)
+        use_explicit_padding, inplace_batchnorm_update)
-  def extract_features(self, preprocessed_inputs):
+  def _extract_features(self, preprocessed_inputs):
-               use_depthwise=False):
+               use_depthwise=False,
-        batch_norm_trainable, reuse_weights, use_explicit_padding)
+        batch_norm_trainable, reuse_weights, use_explicit_padding,
-               use_depthwise=False):
+               use_depthwise=False,
-        batch_norm_trainable, reuse_weights, use_explicit_padding)
+        batch_norm_trainable, reuse_weights, use_explicit_padding,
-               use_depthwise=False):
+               use_depthwise=False,
-        batch_norm_trainable, reuse_weights, use_explicit_padding)
+        batch_norm_trainable, reuse_weights, use_explicit_padding,
-                                   train_batch_norm=self._train_batch_norm):
+      with slim.arg_scope(
-              final_endpoint='Conv2d_13_pointwise',
+              final_endpoint='Conv2d_11_pointwise',
-    return activations['Conv2d_13_pointwise'], activations
+    return activations['Conv2d_11_pointwise'], activations
-                                 train_batch_norm=self._train_batch_norm):
+      with slim.arg_scope(
-      self.assertAllEqual(features_shape_out, [4, 7, 7, 1024])
+      self.assertAllEqual(features_shape_out, [4, 14, 14, 512])
-      self.assertAllEqual(features_shape_out, [4, 7, 7, 1024])
+      self.assertAllEqual(features_shape_out, [4, 14, 14, 512])
-      self.assertAllEqual(features_shape_out, [1, 4, 4, 1024])
+      self.assertAllEqual(features_shape_out, [1, 7, 7, 512])
-        new_masks = tf.reshape(masks, [0, new_size[0], new_size[1]])
+        # The shape function will be computed for both branches of the
-    return features, labels
+    input_dict = dataset_util.make_initializable_iterator(dataset).get_next()
-    return features, labels
+    return (_get_features_dict(input_dict), _get_labels_dict(input_dict))
-          groundtruth_keypoints_list=gt_keypoints_list)
+          groundtruth_keypoints_list=gt_keypoints_list,
-  return padded_tensor
+  if padded_tensor_height != tensor_height:
-  return os.path.join(FLAGS.test_srcdir, model_test_util.PATH_BASE, 'test_data',
+  return os.path.join(tf.resource_loader.get_data_files_path(), 'test_data',
-  return os.path.join(FLAGS.test_srcdir, model_test_util.PATH_BASE, 'data',
+  return os.path.join(tf.resource_loader.get_data_files_path(), 'data',
-                      model_name + '.config')
+  return os.path.join(tf.resource_loader.get_data_files_path(), 'samples',
-
+    # Create ops required to initialize the model from a given checkpoint.
-          file_read_func, cycle_length=config.num_readers, sloppy=True))
+          file_read_func, cycle_length=config.num_readers,
-  variables_to_ignore_patterns = filter(None, filter_regex_list)
+  variables_to_ignore_patterns = list(filter(None, filter_regex_list))
-        sess.run(box_list_ops.sort_by_field(boxes, 'weights').get())
+      if ops._USE_C_API:
-      return {}
+      return {}, {}
-          train_config.fine_tune_checkpoint_type = 'classification'
+    groundtruth_masks_list = None
-                    - label_id_offset, depth=model.num_classes)])
+                    - label_id_offset, depth=model.num_classes)],
-            img_summary, tf.no_op())
+        if img_summary is not None:
-  3. image_resizer_fn: applied only on instance mask tensor in tensor_dict.
+  3. image_resizer_fn: applied on original image and instance mask tensor in
-      the true shapes.
+    image_resizer_fn: image resizer function to apply on original image (if
-                original_image] = tensor_dict[fields.InputDataFields.image]
+    original_image_resized, _ = image_resizer_fn(
-  preprocessed_resized_image, true_image_shape = model_preprocess_fn(image)
+  image = tensor_dict[fields.InputDataFields.image]
-    def fake_image_resizer_fn(image, masks):
+    def fake_image_resizer_fn(image, masks=None):
-      return resized_image, resized_masks, tf.shape(resized_image)
+      results = [resized_image]
-        num_classes=num_classes)
+        num_classes=num_classes,
-        data_augmentation_fn=data_augmentation_fn)
+        data_augmentation_fn=data_augmentation_fn,
-        retain_original_image=True)
+        retain_original_image=eval_config.retain_original_images)
-      # Detection summaries during eval.
+    if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
-      eval_images = (
+      original_images = (
-          eval_images[0:1],
+          original_images[0:1],
-          include_metrics_per_category=False)
+        img_summary = tf.summary.image('Detections_Left_Groundtruth_Right',
-
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-from google3.third_party.tensorflow_models.gan.pix2pix import networks
+import networks
-from google3.third_party.tensorflow_models.gan.pix2pix import networks
+import networks
-from google3.third_party.tensorflow_models.gan.pix2pix import train
+import train
-    reshape = tf.reshape(pool2, [images.get_shape()[0], -1])
+    reshape = tf.reshape(pool2, [images.get_shape().as_list()[0], -1])
-def run_synthetic(main, tmp_root, extra_flags=None):
+def run_synthetic(main, tmp_root, extra_flags=None, synth=True, max_train=1):
-          "--max_train_steps", "1"] + extra_flags
+          "--epochs_between_evals", "1"] + extra_flags
-                      'loss': 'head/weighted_loss/Sum'})
+      tensors_to_log={'average_loss': loss_prefix + 'head/truediv',
-             'See the README for more details and relevant links.')
+        parsers.ImageModelParser(),
-  resnet_run_loop.resnet_main(flags, cifar10_model_fn, input_function)
+
-  resnet_run_loop.resnet_main(flags, imagenet_model_fn, input_function)
+
-    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)
+    # Return the predictions and the specification for serving a SavedModel
-  """Shared main loop for ResNet Models."""
+def resnet_main(flags, model_function, input_function, shape=None):
-  """
+  """Arguments for configuring and running a Resnet Model."""
-from six.moves import xrange
+from six.moves import xrange  # pylint: disable=redefined-builtin
-"""Process the ImageNet Challenge bounding boxes for TensorFlow model training.
+r"""Process the ImageNet Challenge bounding boxes for TensorFlow model training.
-from six.moves import xrange
+from six.moves import xrange  # pylint: disable=redefined-builtin
-from six.moves import xrange
+from six.moves import xrange  # pylint: disable=redefined-builtin
-    tf.summary.scalar(clone.scope + '/clone_loss', clone_loss, family='Losses')
+    tf.summary.scalar('/'.join(filter(None,
-                      family='Losses')
+    tf.summary.scalar('Losses/regularization_loss', regularization_loss)
-from six.moves import xrange
+from six.moves import xrange  # pylint: disable=redefined-builtin
-
+from six.moves import xrange  # pylint: disable=redefined-builtin
-from six.moves import xrange
+
-from six.moves import xrange
+from six.moves import xrange  # pylint: disable=redefined-builtin
-from six.moves import xrange
+
-    for name, op in end_points.iteritems():
+    for name, op in end_points.items():
-    items = defaults.items()
+    items = list(defaults.items())
-      prediciton_fn- what prediction function to use.
+      prediction_fn- what prediction function to use.
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for net in nets_factory.networks_map.keys()[:10]:
+    for net in list(nets_factory.networks_map.keys())[:10]:
-    for net in nets_factory.networks_map.keys()[10:]:
+    for net in list(nets_factory.networks_map.keys())[10:]:
-      benchmark_logger = logger.BenchmarkLogger(flags.benchmark_log_dir)
+    if benchmark_logger:
-  def __init__(self, add_help=False, benchmark_log_dir=True):
+  def __init__(self, add_help=False, benchmark_log_dir=True,
-        parsers.BenchmarkParser(benchmark_log_dir=True)
+        parsers.BenchmarkParser(benchmark_log_dir=True, bigquery_uploader=True)
-        benchmark_log_dir="/tmp/12345"
+        benchmark_log_dir="/tmp/12345",
-_BENCHMARK_RUN_LOG_FILE_NAME = "benchmark_run.log"
+METRIC_LOG_FILE_NAME = "metric.log"
-
+    if extras:
-        os.path.join(self._logging_dir, _METRIC_LOG_FILE_NAME), "a") as f:
+        os.path.join(self._logging_dir, METRIC_LOG_FILE_NAME), "a") as f:
-    run_info = {"model_name": model_name}
+    run_info = {
-        self._logging_dir, _BENCHMARK_RUN_LOG_FILE_NAME), "w") as f:
+        self._logging_dir, BENCHMARK_RUN_LOG_FILE_NAME), "w") as f:
-      k: v for k, v in os.environ.items() if k.startswith("TF_")}
+  run_info["tensorflow_environment_variables"] = [
-  run_info["cpu_info"] = cpu_info
+  run_info["machine_config"]["cpu_info"] = cpu_info
-  run_info["gpu_info"] = gpu_info
+  run_info["machine_config"]["gpu_info"] = gpu_info
-  run_info["memory_available"] = vmem.available
+  run_info["machine_config"]["memory_total"] = vmem.total
-from tensorflow.python.client import device_lib
+  def setUp(self):
-      self.assertEqual(metric["extras"], {"name": "value"})
+      self.assertEqual(metric["extras"], [{"name": "name", "value": "value"}])
-      self.assertEqual(accuracy["extras"], {"name": "value"})
+      self.assertEqual(accuracy["extras"], [{"name": "name", "value": "value"}])
-                     ["TF_ENABLE_WINOGRAD_NONFUSED"], "1")
+    expected_tf_envs = [
-    run_info = {}
+    run_info = {"machine_config": {}}
-    self.assertNotEqual(run_info["gpu_info"], {})
+    self.assertNotEqual(run_info["machine_config"]["gpu_info"], {})
-    run_info = {}
+    run_info = {"machine_config": {}}
-    self.assertIsNotNone(run_info["memory_available"])
+    self.assertIsNotNone(run_info["machine_config"]["memory_total"])
-
+  # Note: cpuinfo is not installed in the TensorFlow OSS tree.
-    return input_fn(train_file, flags.epochs_between_evals, True, flags.batch_size)
+    return input_fn(
-  # Train and evaluate the model every `FLAGS.epochs_per_eval` epochs.
+  # Train and evaluate the model every `flags.epochs_between_evals` epochs.
-    return input_fn(train_file, flags.epochs_per_eval, True, flags.batch_size)
+    return input_fn(train_file, flags.epochs_between_evals, True, flags.batch_size)
-  # Train and evaluate the model every `FLAGS.epochs_between_evals` epochs.
+  # Train and evaluate the model every `flags.epochs_between_evals` epochs.
-  > pip install --upgrade psutil
+packages need be installed. See README for details.
-      image_filename = image_names[i]
+      image_filename = os.path.basename(image_names[i])
-"""Logging utilities for benchmark."""
+"""Logging utilities for benchmark.
-                   'accuracy': 0.9285}
+    eval_result = {"loss": 0.46237424,
-# Settings for training strategry.
+# Settings for training strategy.
-    poses.append(obj['pose'].encode('utf8'))
+    
-    img1_str = image_file.read()
+    img1_str = image_file.read('rb')
-    img2_str = image_file.read()
+    img2_str = image_file.read('rb')
-    for key in eval_results:
+    for key in sorted(eval_results):
-
+      loss = json.loads(f.readline())
-        flags.hooks, batch_size=flags.batch_size)
+        flags.hooks,
-               "ProfilerHook, ExamplesPerSecondHook. "
+               "ProfilerHook, ExamplesPerSecondHook, LoggingMetricHook."
-        parsers.ImageModelParser(data_format=True)
+        parsers.ImageModelParser(data_format=True),
-    self.validate_train_hook_name(test_hook_name, 'loggingtensorhook')
+    self.validate_train_hook_name('LoggingTensorHook', 'loggingtensorhook')
-    self.validate_train_hook_name(test_hook_name, 'profilerhook')
+    self.validate_train_hook_name('ProfilerHook', 'profilerhook')
-    self.validate_train_hook_name(test_hook_name, 'examplespersecondhook')
+    self.validate_train_hook_name('ExamplesPerSecondHook',
-def main(_):
+def main(argv):
-    validate_batch_size_for_multi_gpu(FLAGS.batch_size)
+  if flags.multi_gpu:
-  data_format = FLAGS.data_format
+  data_format = flags.data_format
-      model_dir=FLAGS.model_dir,
+      model_dir=flags.model_dir,
-          'multi_gpu': FLAGS.multi_gpu
+          'multi_gpu': flags.multi_gpu
-    ds = ds.cache().shuffle(buffer_size=50000).batch(FLAGS.batch_size)
+    ds = dataset.train(flags.data_dir)
-    ds = ds.repeat(FLAGS.epochs_between_evals)
+    ds = ds.repeat(flags.epochs_between_evals)
-        FLAGS.batch_size).make_one_shot_iterator().get_next()
+    return dataset.test(flags.data_dir).batch(
-      FLAGS.hooks, batch_size=FLAGS.batch_size)
+      flags.hooks, batch_size=flags.batch_size)
-  for _ in range(FLAGS.train_epochs // FLAGS.epochs_between_evals):
+  for _ in range(flags.train_epochs // flags.epochs_between_evals):
-  if FLAGS.export_dir is not None:
+  if flags.export_dir is not None:
-    mnist_classifier.export_savedmodel(FLAGS.export_dir, input_fn)
+    mnist_classifier.export_savedmodel(flags.export_dir, input_fn)
-  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
+  main(argv=sys.argv)
-def main(_):
+def main(argv):
-  if FLAGS.no_gpu or tfe.num_gpus() <= 0:
+  if flags.no_gpu or tfe.num_gpus() <= 0:
-  if FLAGS.data_format is not None:
+  if flags.data_format is not None:
-  test_ds = mnist_dataset.test(FLAGS.data_dir).batch(FLAGS.batch_size)
+  train_ds = mnist_dataset.train(flags.data_dir).shuffle(60000).batch(
-  optimizer = tf.train.MomentumOptimizer(FLAGS.lr, FLAGS.momentum)
+  optimizer = tf.train.MomentumOptimizer(flags.lr, flags.momentum)
-  if FLAGS.output_dir:
+  if flags.output_dir:
-    tf.gfile.MakeDirs(FLAGS.output_dir)
+    train_dir = os.path.join(flags.output_dir, 'train')
-  checkpoint_prefix = os.path.join(FLAGS.model_dir, 'ckpt')
+  checkpoint_prefix = os.path.join(flags.model_dir, 'ckpt')
-  checkpoint.restore(tf.train.latest_checkpoint(FLAGS.model_dir))
+  checkpoint.restore(tf.train.latest_checkpoint(flags.model_dir))
-    for _ in range(FLAGS.train_epochs):
+    for _ in range(flags.train_epochs):
-        train(model, optimizer, train_ds, step_counter, FLAGS.log_interval)
+        train(model, optimizer, train_ds, step_counter, flags.log_interval)
-  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
+  main(argv=sys.argv)
-def main(_):
+def main(argv):
-  model = build_estimator(FLAGS.model_dir, FLAGS.model_type)
+  shutil.rmtree(flags.model_dir, ignore_errors=True)
-  test_file = os.path.join(FLAGS.data_dir, 'adult.test')
+  train_file = os.path.join(flags.data_dir, 'adult.data')
-    return input_fn(train_file, FLAGS.epochs_per_eval, True, FLAGS.batch_size)
+    return input_fn(train_file, flags.epochs_per_eval, True, flags.batch_size)
-    return input_fn(test_file, 1, False, FLAGS.batch_size)
+    return input_fn(test_file, 1, False, flags.batch_size)
-      FLAGS.hooks, batch_size=FLAGS.batch_size,
+      flags.hooks, batch_size=flags.batch_size,
-  for n in range(FLAGS.train_epochs // FLAGS.epochs_between_evals):
+  for n in range(flags.train_epochs // flags.epochs_between_evals):
-    print('Results at epoch', (n + 1) * FLAGS.epochs_between_evals)
+    print('Results at epoch', (n + 1) * flags.epochs_between_evals)
-  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
+  main(argv=sys.argv)
-  with tf.gfile.FastGFile(FLAGS.input_codes, 'r') as code_file:
+  with tf.gfile.FastGFile(FLAGS.input_codes, 'rb') as code_file:
-  with tf.gfile.FastGFile(FLAGS.input_image) as input_image:
+  with tf.gfile.FastGFile(FLAGS.input_image, 'rb') as input_image:
-                  'DeepLab model variants.')
+# When using 'mobilent_v2', we set atrous_rates = decoder_output_stride = None.
-# For `xception_65`, use decoder_output_stride = 4.
+flags.DEFINE_float('depth_multiplier', 1.0,
-    raise ValueError('MobileNetv2 support is coming soon.')
+    arg_scope = arg_scopes_map[model_variant](
-# atrous_rates/output_stride during training/evaluation.
+# rates = [6, 12, 18] if output_stride = 16. For `mobilenet_v2`, use None. Note
-      'mobilenet_v2' but atrous_rates or decoder_output_stride are not None.
+      crop_size information.
-    model_variants = ['xception_65']
+    model_variants = ['xception_65', 'mobilenet_v2']
-        model_variant='xception_65')
+        model_variant='mobilenet_v2')  # Employ MobileNetv2 for fast test.
-# atrous_rates/output_stride during training/evaluation.
+# rates = [6, 12, 18] if output_stride = 16. For `mobilenet_v2`, use None. Note
-# atrous_rates/output_stride during training/evaluation.
+# rates = [6, 12, 18] if output_stride = 16. For `mobilenet_v2`, use None. Note
-                        spatial_image_shape):
+def _get_padding_shapes(dataset, max_num_boxes=None, num_classes=None,
-      containing expected spatial shape of the imaage.
+      containing expected spatial shape of the image.
-  height, width = spatial_image_shape
+
-      ],
+  # Determine whether groundtruth_classes are integers or one-hot encodings, and
-          batch_size=1, max_num_boxes=None, num_classes=None,
+          batch_size=None, max_num_boxes=None, num_classes=None,
-  tf.data.Dataset.
+  records. Applies a padded batch to the resulting dataset.
-      padding. This is only used if batch_size is greater than 1.
+    batch_size: Batch size. If None, batching is not performed.
-    spatial_image_shape: a list of two integers of the form [height, width]
+      padding. If None, will use a dynamic shape.
-      only used if batch_size is greater than 1.
+      transform_input_data_fn. If None, will use dynamic shapes.
-                         '1 .')
+    if batch_size:
-        dataset_builder.build(input_reader_proto)).get_next()
+        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
-    self.assertEquals((4, 5, 3),
+    self.assertEquals((1, 4, 5, 3),
-                      output_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertAllEqual([[2]],
-        (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
+        (1, 1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
-        output_dict[fields.InputDataFields.groundtruth_boxes][0])
+        output_dict[fields.InputDataFields.groundtruth_boxes][0][0])
-        dataset_builder.build(input_reader_proto)).get_next()
+        dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
-        (1, 4, 5),
+        (1, 1, 4, 5),
-    learning_rate = tf.constant(config.learning_rate, dtype=tf.float32)
+    learning_rate = tf.constant(config.learning_rate, dtype=tf.float32,
-        staircase=config.staircase)
+        staircase=config.staircase, name='learning_rate')
-        learning_rate_sequence)
+        learning_rate_sequence, config.warmup)
-        config.warmup_steps)
+        config.warmup_steps,
-          step: 0
+          step: 100
-  def restore_map(self, from_detection_checkpoint=True):
+  def restore_map(self, fine_tune_checkpoint_type='detection'):
-      from_detection_checkpoint: whether to restore from a full detection
+      fine_tune_checkpoint_type: whether to restore from a full detection
-                 else [0] * num_classes)
+                 else [0] * num_classes.value)
-        flt_image * rgb_weights, rank_1, keepdims=True)
+        flt_image * rgb_weights, rank_1, keep_dims=True)
-    min_scale = tf.maximum(min_height / target_height, min_width / target_width)
+    min_scale = tf.minimum(
-    target_width = scale * target_width
+    target_height = tf.round(scale * target_height)
-        image, [new_height, new_width],
+        image, tf.stack([new_height, new_width]),
-      new_size = tf.constant([new_height, new_width], dtype=tf.int32)
+      new_size = tf.stack([new_height, new_width])
-              'image/object/class/text', table, default_value=''),
+          LookupTensor('image/object/class/text', table, default_value=''),
-                                                 table))
+        backup=tf_example_decoder.LookupTensor('image/object/class/text',
-                         'regions.')
+tf.app.flags.DEFINE_string('classes_to_use', 'car,pedestrian,dontcare',
-      classes_to_use=FLAGS.classes_to_use,
+      classes_to_use=FLAGS.classes_to_use.split(','),
-"""Common functions for repeatedly evaluating a checkpoint."""
+"""Common utility functions for evaluation."""
-  """Evaluates metrics defined in evaluators.
+                         save_graph_dir='',
-            result_dict = sess.run(tensor_dict)
+            if not losses_dict:
-          result_dict = batch_processor(tensor_dict, sess, batch, counters)
+          result_dict, result_losses_dict = batch_processor(
-                            save_graph_dir=''):
+                            save_graph_dir='',
-                                                  save_graph_dir)
+                                                  save_graph_dir,
-    image: A single 4D image tensor of shape [1, H, W, C].
+    image: A single 4D uint8 image tensor of shape [1, H, W, C].
-  input_data_fields = fields.InputDataFields()
+  input_data_fields = fields.InputDataFields
-  output_dict[detection_fields.detection_boxes] = detection_boxes
+
-  """Restores the model in a tensorflow session.
+def _extract_predictions_and_losses(model,
-    tensor_dict: A tensor dictionary with evaluations.
+    prediction_groundtruth_dict: A dictionary with postprocessed tensors (keyed
-  return eval_util.result_dict_for_single_example(
+    label_id_offset = 1
-  tensor_dict = _extract_prediction_tensors(
+  tensor_dict, losses_dict = _extract_predictions_and_losses(
-    """Evaluates tensors in tensor_dict, visualizing the first K examples.
+  def _process_batch(tensor_dict, sess, batch_index, counters,
-      result_dict = sess.run(tensor_dict)
+      if not losses_dict:
-    return result_dict
+    return result_dict, result_losses_dict
-      save_graph_dir=(eval_dir if eval_config.save_graph else ''))
+      save_graph_dir=(eval_dir if eval_config.save_graph else ''),
-  def restore_map(self, checkpoint_path, from_detection_checkpoint):
+  def restore_map(self, checkpoint_path, fine_tune_checkpoint_type):
-    dataset = dataset_builder.build(
+    dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
-                                    transform_input_data_fn=transform_data_fn)
+    dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
-
+    true_image_shape = tf.expand_dims(
-        features={fields.InputDataFields.image: images},
+        features={
-    self.assertAllEqual([None, None, 3],
+    self.assertAllEqual([1, None, None, 3],
-    self.assertAllEqual([],
+    self.assertAllEqual([1],
-        [None, 4],
+        [1, 50, 4],
-        [None, model_config.faster_rcnn.num_classes],
+        [1, 50, model_config.faster_rcnn.num_classes],
-        [None],
+        [1, 50],
-    rpn_features_to_crop = self._feature_extractor.extract_proposal_features(
+    rpn_features_to_crop, _ = self._feature_extractor.extract_proposal_features(
-            self._first_stage_obj_loss_weight * objectness_loss)
+
-            self._second_stage_loc_loss_weight * second_stage_loc_loss)
+      localization_loss = tf.multiply(self._second_stage_loc_loss_weight,
-            self._second_stage_cls_loss_weight * second_stage_cls_loss)
+      classification_loss = tf.multiply(self._second_stage_cls_loss_weight,
-              self._second_stage_mask_loss_weight * second_stage_mask_loss)
+        mask_loss = tf.multiply(self._second_stage_mask_loss_weight,
-                  from_detection_checkpoint=True,
+                  fine_tune_checkpoint_type='detection',
-      from_detection_checkpoint: whether to restore from a full detection
+      fine_tune_checkpoint_type: whether to restore from a full detection
-        True.
+        classification checkpoint for initialization prior to training.
-         the feature extractor scopes are included. Default False.
+         `fine_tune_checkpoint_type` is `detection`). If False, only variables
-    if not from_detection_checkpoint:
+    if fine_tune_checkpoint_type not in ['detection', 'classification']:
-                             num_outputs=3, kernel_size=1, scope='layer1')
+      proposal_features = 0 * slim.conv2d(
-      self.assertTrue('second_stage_classification_loss' not in loss_dict_out)
+      self.assertAllClose(loss_dict_out['Loss/RPNLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out['second_stage_mask_loss'], 0)
+      self.assertAllClose(loss_dict_out['Loss/RPNLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out['second_stage_mask_loss'], 0)
+      self.assertAllClose(loss_dict_out['Loss/RPNLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out['second_stage_mask_loss'], 0)
+      self.assertAllClose(loss_dict_out['Loss/RPNLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out['first_stage_localization_loss'],
+      self.assertAllClose(loss_dict_out['Loss/RPNLoss/localization_loss'],
-      self.assertAllClose(loss_dict_out['second_stage_classification_loss'], 0)
+      self.assertAllClose(loss_dict_out['Loss/RPNLoss/objectness_loss'], 0)
-      self.assertAllClose(loss_dict_out['second_stage_classification_loss'], 0)
+      self.assertAllClose(loss_dict_out[
-      var_map = model.restore_map(from_detection_checkpoint=False)
+      var_map = model.restore_map(fine_tune_checkpoint_type='classification')
-      var_map = model2.restore_map(from_detection_checkpoint=True)
+      var_map = model2.restore_map(fine_tune_checkpoint_type='detection')
-          from_detection_checkpoint=True,
+          fine_tune_checkpoint_type='detection',
-        self._summarize_input(
+        self._summarize_target_assignment(
-                               classification_loss)
+      localization_loss_normalizer = normalizer
-          'classification_loss': classification_loss
+          localization_loss.op.name: localization_loss,
-  def _summarize_input(self, groundtruth_boxes_list, match_list):
+  def _summarize_target_assignment(self, groundtruth_boxes_list, match_list):
-                      tf.reduce_mean(tf.to_float(ignored_anchors_per_image)))
+    tf.summary.scalar('AvgNumGroundtruthBoxesPerImage',
-                  from_detection_checkpoint=True,
+                  fine_tune_checkpoint_type='detection',
-      from_detection_checkpoint: whether to restore from a full detection
+      fine_tune_checkpoint_type: whether to restore from a full detection
-      if from_detection_checkpoint and load_all_detection_checkpoint_vars:
+      if (fine_tune_checkpoint_type == 'detection' and
-          if not from_detection_checkpoint:
+          if fine_tune_checkpoint_type == 'classification':
-      return (loss_dict['localization_loss'], loss_dict['classification_loss'])
+      return (
-      return (loss_dict['localization_loss'],)
+      return (_get_value_for_matching_key(loss_dict, 'Loss/localization_loss'),)
-      return (loss_dict['localization_loss'], loss_dict['classification_loss'])
+      return (
-          from_detection_checkpoint=True,
+          fine_tune_checkpoint_type='detection',
-      var_map = model.restore_map(from_detection_checkpoint=False)
+      var_map = model.restore_map(fine_tune_checkpoint_type='classification')
-          from_detection_checkpoint=True,
+          fine_tune_checkpoint_type='detection',
-                   for key, value in box_metrics.iteritems()}
+                   for key, value in iter(box_metrics.items())}
-  """Checks whether mask dtype is uint8 anf the values are either 0 or 1."""
+  """Checks whether mask dtype is uint8 and the values are either 0 or 1."""
-        detections
+      'DetectionMasks_Precision/mAP': mean average precision over classes
-  sliced along the `num_boxes` dimension using the value in tensor
+  When unpad_groundtruth_tensors is set to true, unstacked tensors of form 3
-            from_detection_checkpoint=train_config.from_detection_checkpoint,
+            fine_tune_checkpoint_type=train_config.fine_tune_checkpoint_type,
-          tf.expand_dims(features[fields.InputDataFields.original_image][0], 0),
+          eval_images[0:1],
-      if not use_tpu:
+      if not use_tpu and use_original_images:
-          detection_classes=eval_dict[detection_fields.detection_classes])
+      eval_metrics = eval_config.metrics_set
-def _build_experiment_fn(train_steps, eval_steps):
+def build_experiment_fn(train_steps, eval_steps):
-                                         FLAGS.num_eval_steps),
+      experiment_fn=build_experiment_fn(FLAGS.num_train_steps,
-  experiment_fn = model._build_experiment_fn(10, 10)
+  experiment_fn = model.build_experiment_fn(10, 10)
-    return rpn_feature_map
+          return inception_resnet_v2.inception_resnet_v2_base(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    return activations['Mixed_4e']
+    return activations['Mixed_4e'], activations
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    return rpn_feature_map
+    return rpn_feature_map, end_points
-    # pylint: enable=protected-access
+    # TODO(shlens,skornblith): Determine the appropriate drop path schedule.
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    return activations[handle]
+    return activations[handle], activations
-      rpn_feature_map = feature_extractor.extract_proposal_features(
+      rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-    rpn_feature_map = feature_extractor.extract_proposal_features(
+    rpn_feature_map, _ = feature_extractor.extract_proposal_features(
-                               reuse=self._reuse_weights) as scope:
+    with tf.variable_scope('MobilenetV1',
-        tf.summary.scalar(var.op.name, var)
+        tf.summary.scalar(var.op.name, var, family='LearningRate')
-          from_detection_checkpoint=train_config.from_detection_checkpoint,
+          fine_tune_checkpoint_type=train_config.fine_tune_checkpoint_type,
-      global_summaries.add(tf.summary.histogram(model_var.op.name, model_var))
+      global_summaries.add(tf.summary.histogram('ModelVars/' +
-      global_summaries.add(tf.summary.scalar(loss_tensor.op.name, loss_tensor))
+      global_summaries.add(tf.summary.scalar('Losses/' + loss_tensor.op.name,
-        tf.summary.scalar('TotalLoss', tf.losses.get_total_loss()))
+        tf.summary.scalar('Losses/TotalLoss', tf.losses.get_total_loss()))
-  def restore_map(self, from_detection_checkpoint=True):
+  def restore_map(self, fine_tune_checkpoint_type='detection'):
-      from_detection_checkpoint: whether to restore from a full detection
+      fine_tune_checkpoint_type: whether to restore from a full detection
-      raise ValueError('Label map ids should be >= 1.')
+    if item.id < 0:
-      post_burnin_learning_rate)
+      post_burnin_learning_rate, name='learning_rate')
-                             warmup_steps=0):
+                             warmup_steps=0,
-                         ) / float(total_steps - warmup_steps)))
+  learning_rate = 0.5 * learning_rate_base * (1 + tf.cos(
-  return learning_rate
+    warmup_rate = slope * tf.cast(global_step,
-def manual_stepping(global_step, boundaries, rates):
+def manual_stepping(global_step, boundaries, rates, warmup=False):
-  step_boundaries = tf.constant(boundaries, tf.int32)
+
-                                                   dtype=tf.float32))
+  rate_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),
-        position_sensitive_features, [1, 2], keepdims=True)
+        position_sensitive_features, [1, 2], keep_dims=True)
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
+    crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
+    crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)
-          ps_crop, reduction_indices=(1, 2), keepdims=True)
+          ps_crop, reduction_indices=(1, 2), keep_dims=True)
-  variables_to_ignore_patterns = list(filter(None, filter_regex_list))
+  variables_to_ignore_patterns = filter(None, filter_regex_list)
-    tf.summary.scalar(clone.scope + '/clone_loss', clone_loss)
+    tf.summary.scalar(clone.scope + '/clone_loss', clone_loss, family='Losses')
-    tf.summary.scalar('regularization_loss', regularization_loss)
+    tf.summary.scalar('regularization_loss', regularization_loss,
-
+from six.moves import xrange
-import contextlib2
+@contextlib.contextmanager
-
+  with _v1_compatible_scope_naming(scope) as scope:
-    defaults: dictionary mapping function to default_dict
+    defaults: dictionary/list of pairs, containing a mapping from
-    context manager
+    context manager where all defaults are set.
-    ]
+  if hasattr(defaults, 'items'):
-      self.assertAlmostEqual(3217920L, total_params)
+      self.assertAlmostEqual(3217920, total_params)
-  drop_path_keep_prob = 1.0 if not is_training else 0.6
+def cifar_config():
-      drop_path_keep_prob=drop_path_keep_prob,
+      drop_path_keep_prob=0.6,
-      use_aux_head=int(use_aux_head),
+      use_aux_head=1,
-  drop_path_keep_prob = 1.0 if not is_training else 0.7
+def large_imagenet_config():
-      use_aux_head=int(use_aux_head),
+      drop_path_keep_prob=0.7,
-def _mobile_imagenet_config(use_aux_head=True):
+def mobile_imagenet_config():
-      use_aux_head=int(use_aux_head),
+      use_aux_head=1,
-    images, num_classes, is_training=True, use_aux_head=True):
+def build_nasnet_cifar(images, num_classes,
-  hparams = _cifar_config(is_training=is_training, use_aux_head=use_aux_head)
+  hparams = cifar_config() if config is None else copy.deepcopy(config)
-                        use_aux_head=True):
+                        config=None):
-  hparams = _mobile_imagenet_config(use_aux_head=use_aux_head)
+  hparams = (mobile_imagenet_config() if config is None
-                       use_aux_head=True):
+                       config=None):
-                                   use_aux_head=use_aux_head)
+  hparams = (large_imagenet_config() if config is None
-                                                  use_aux_head=use_aux_head)
+                                                  config=config)
-                                                   use_aux_head=use_aux_head)
+                                                   config=config)
-                                                  use_aux_head=use_aux_head)
+                                                  config=config)
-import os.path
+from six.moves import xrange
-    self.act_dims_and_types = zip(self.act_dims, self.act_types)
+    self.obs_dims_and_types = tuple(zip(self.obs_dims, self.obs_types))
-  loss = cross_entropy + weight_decay * tf.add_n(
+  l2_loss = weight_decay * tf.add_n(
-              self.assertEquals(len(scales_to_logits), expected_num_logits[i])
+              self.assertEqual(len(scales_to_logits), expected_num_logits[i])
-      avg_pool = graph.get_tensor_by_name('final_avg_pool:0')
+      reduce_mean = graph.get_tensor_by_name('final_reduce_mean:0')
-        self.assertAllEqual(avg_pool.shape, reshape((1, 512, 1, 1)))
+        self.assertAllEqual(reduce_mean.shape, reshape((1, 512, 1, 1)))
-        self.assertAllEqual(avg_pool.shape, reshape((1, 2048, 1, 1)))
+        self.assertAllEqual(reduce_mean.shape, reshape((1, 2048, 1, 1)))
-    inputs = tf.identity(inputs, 'final_avg_pool')
+
-  tf.app.run(argv=sys.argv)
+  main(argv=sys.argv)
-  tf.app.run(argv=sys.argv)
+  main(argv=sys.argv)
-# ==============================================================================
+import gzip
-    num_images = read32(f)
+    read32(f)  # num_images, unused
-    num_items = read32(f)
+    read32(f)  # num_items, unused
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-  available GPUs.
+  """For multi-gpu, batch-size must be a multiple of the number of GPUs.
-  from tensorflow.python.client import device_lib
+  from tensorflow.python.client import device_lib  # pylint: disable=g-import-not-at-top
-      'were found. To use CPU, run without --multi_gpu.')
+                     'were found. To use CPU, run without --multi_gpu.')
-      ).format(num_gpus, batch_size, batch_size - remainder)
+           'must be a multiple of the number of available GPUs. '
-def main(unused_argv):
+def main(_):
-  for n in range(FLAGS.train_epochs // FLAGS.epochs_between_evals):
+  for _ in range(FLAGS.train_epochs // FLAGS.epochs_between_evals):
-      parsers.ImageModelParser()])
+        parsers.BaseParser(),
-import tensorflow.contrib.eager as tfe
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-  train_ds = dataset.train(FLAGS.data_dir).shuffle(60000).batch(
+  train_ds = mnist_dataset.train(FLAGS.data_dir).shuffle(60000).batch(
-  test_ds = dataset.test(FLAGS.data_dir).batch(FLAGS.batch_size)
+  test_ds = mnist_dataset.test(FLAGS.data_dir).batch(FLAGS.batch_size)
-  """Argument parser for running MNIST model with eager trainng loop."""
+  """Argument parser for running MNIST model with eager training loop."""
-      parsers.ImageModelParser()])
+        parsers.BaseParser(
-import tensorflow.contrib.eager as tfe
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-    for i in range(3):
+    for _ in range(3):
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-            FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)
+      FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)
-def main(unused_argv):
+def main(_):
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-                                                examples_per_epoch=num_images, multi_gpu=multi_gpu)
+  return resnet_run_loop.process_record_dataset(
-  return resnet_run_loop.get_synth_input_fn(_HEIGHT, _WIDTH, _NUM_CHANNELS, _NUM_CLASSES)
+  return resnet_run_loop.get_synth_input_fn(
-      version=resnet_model.DEFAULT_VERSION):
+               version=resnet_model.DEFAULT_VERSION):
-  def loss_filter_fn(name):
+  def loss_filter_fn(_):
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-        filename, cifar10_main._RECORD_BYTES)
+        filename, cifar10_main._RECORD_BYTES)  # pylint: disable=protected-access
-      fake_input = tf.random_uniform([batch_size, _HEIGHT, _WIDTH, _NUM_CHANNELS])
+      model = cifar10_main.Cifar10Model(
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-        _DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, _NUM_CHANNELS, _NUM_CLASSES)
+      _DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, _NUM_CHANNELS, _NUM_CLASSES)
-    version=resnet_model.DEFAULT_VERSION):
+               version=resnet_model.DEFAULT_VERSION):
-  """The number of block layers used for the Resnet model varies according
+  """Retrieve the size of each block_layer in the ResNet model.
-  `method` and other details each time.
+  """Simple wrapper around tf.resize_images.
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-      """
+      """Returns the expected dimensions depending on if a GPU is being used."""
-                                      num_classes=num_classes, version=version)
+      model = imagenet_main.ImagenetModel(
-  """
+  """A single block for ResNet v1, without a bottleneck.
-    The output tensor of the block.
+    The output tensor of the block; shape should match inputs.
-  """
+  """A single block for ResNet v2, without a bottleneck.
-    The output tensor of the block.
+    The output tensor of the block; shape should match inputs.
-  """
+  """A single block for ResNet v1, with a bottleneck.
-  """
+  """A single block for ResNet v2, without a bottleneck.
-  adapted to the ordering conventions of:
+  Adapted to the ordering conventions of:
-  """
+  """Base class for building the Resnet Model."""
-          "Resnet version should be 1 or 2. See README for citations.")
+          'Resnet version should be 1 or 2. See README for citations.')
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-from official.utils.logging import hooks_helper
+from official.utils.arg_parsers import parsers
-  and return an iterator over the records.
+  """Given a Dataset with raw records, return an iterator over the records.
-  def input_fn(is_training, data_dir, batch_size, *args):
+  def input_fn(is_training, data_dir, batch_size, *args):  # pylint: disable=unused-argument
-      return 'batch_normalization' not in name
+  def exclude_batch_norm(name):
-  available GPUs.
+  """For multi-gpu, batch-size must be a multiple of the number of GPUs.
-  from tensorflow.python.client import device_lib
+  from tensorflow.python.client import device_lib  # pylint: disable=g-import-not-at-top
-      'were found. To use CPU, run without --multi_gpu.')
+                     'were found. To use CPU, run without --multi_gpu.')
-      ).format(num_gpus, batch_size, batch_size - remainder)
+           'must be a multiple of the number of available GPUs. '
-                                               batch_size=flags.batch_size)
+    train_hooks = hooks_helper.get_train_hooks(
-        help="Version of ResNet. (1 or 2) See README.md for details."
+        help='Version of ResNet. (1 or 2) See README.md for details.'
-    --application_specific_arg APPLICATION_SPECIFIC_ARG, -asa APPLICATION_SPECIFIC_ARG
+    --app_specific_arg APP_SPECIFIC_ARG, -asa APP_SPECIFIC_ARG
-    --application_specific_arg <ASA>, -asa <ASA>
+    --app_specific_arg <ASA>, -asa <ASA>
-          choices=['channels_first', 'channels_last'],
+          choices=["channels_first", "channels_last"],
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-    kwargs: a dictionary of arguments to the hooks.
+    **kwargs: a dictionary of arguments to the hooks.
-    kwargs: a dictionary of arguments to LoggingTensorHook.
+    **kwargs: a dictionary of arguments to LoggingTensorHook.
-    kwargs: a dictionary of arguments to ProfilerHook.
+    **kwargs: a dictionary of arguments to ProfilerHook.
-    kwargs: a dictionary of arguments to ExamplesPerSecondHook.
+    **kwargs: a dictionary of arguments to ExamplesPerSecondHook.
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-from tensorflow.python.training import monitored_session
+  """Tests for the ExamplesPerSecondHook."""
-    mon_sess = monitored_session._HookedSession(sess, [hook])
+    mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access
-    mon_sess = monitored_session._HookedSession(sess, [hook])
+    mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access
-        "Metric value to log should be a number. Got %s", type(value))
+          "Metric value to log should be a number. Got %s", type(value))
-
+        tf.logging.warning("Failed to dump metric to log file: "
-
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-    main: The primary function used to excercise a code path. Generally this
+    main: The primary function used to exercise a code path. Generally this
-    extra_flags: Additional flags passed by the the caller of this function.
+    extra_flags: Additional flags passed by the caller of this function.
-def main(unused_argv):
+def main(_):
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-from official.utils.arg_parsers import parsers  # pylint: disable=g-bad-import-order
+from official.utils.arg_parsers import parsers
-def main(unused_argv):
+def main(_):
-        test_file, 1, False, FLAGS.batch_size))
+    model.train(input_fn=train_input_fn, hooks=train_hooks)
-import tensorflow as tf
+import tensorflow as tf  # pylint: disable=g-bad-import-order
-    'Husband,zyx,wvu,34,56,78,tsr,<=50K')
+              'Husband,zyx,wvu,34,56,78,tsr,<=50K')
-            TEST_CSV, num_epochs=1, shuffle=False, batch_size=1))
+    def get_input_fn(num_epochs, shuffle, batch_size):
-            TEST_CSV, num_epochs=1, shuffle=False, batch_size=1))
+    model.train(input_fn=get_input_fn(100, True, 3))
-import os
+
-    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)
+    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)
-    # LoggingTensorHook.
+
-  # Train the model
+  # Set up training and evaluation input functions.
-    return ds
+    ds = ds.cache().shuffle(buffer_size=50000).batch(FLAGS.batch_size)
-  mnist_classifier.train(input_fn=train_input_fn, hooks=[logging_hook])
+    # Iterate through the dataset a set number (`epochs_between_evals`) of times
-  print('Evaluation results:\n\t%s' % eval_results)
+  # Set up hook that outputs training logs every 100 steps.
-
+  """Argument parser for running MNIST model."""
-    super(MNISTArgParser, self).__init__()
+    super(MNISTArgParser, self).__init__(parents=[
-        help='The directory where the exported SavedModel will be stored.')
+        help='[default: %(default)s] If set, a SavedModel serialization of the '
-  parser = MNISTArgParser()
+  parser = MNISTArgParser()
-  checkpoint_prefix = os.path.join(FLAGS.checkpoint_dir, 'ckpt')
+
-  # Train and evaluate for 10 epochs.
+  checkpoint.restore(tf.train.latest_checkpoint(FLAGS.model_dir))
-    for _ in range(10):
+    for _ in range(FLAGS.train_epochs):
-      help='disables GPU usage even if a GPU is available')
+class MNISTEagerArgParser(argparse.ArgumentParser):
-                      epochs_per_eval=10,
+                      epochs_between_evals=10,
-    train_hooks = hooks_helper.get_train_hooks(flags.hooks, batch_size=flags.batch_size)
+  for _ in range(flags.train_epochs // flags.epochs_between_evals):
-                            flags.multi_gpu)
+                            flags.epochs_between_evals,
-    epochs_per_eval: Create a flag to specify the frequency of testing.
+    epochs_between_evals: Create a flag to specify the frequency of testing.
-               train_epochs=True, epochs_per_eval=True, batch_size=True,
+               train_epochs=True, epochs_between_evals=True, batch_size=True,
-          help="[default: %(default)s] The location of the model files.",
+          help="[default: %(default)s] The location of the model checkpoint "
-    if epochs_per_eval:
+    if epochs_between_evals:
-          "--epochs_per_eval", "-epe", type=int, default=1,
+          "--epochs_between_evals", "-ebe", type=int, default=1,
-          metavar="<EPE>"
+          metavar="<EBE>"
-        epochs_per_eval=15,
+        epochs_between_evals=15,
-def get_logging_tensor_hook(every_n_iter=100, **kwargs):  # pylint: disable=unused-argument
+def get_logging_tensor_hook(every_n_iter=100, tensors_to_log=None, **kwargs):  # pylint: disable=unused-argument
-      tensors=_TENSORS_TO_LOG,
+      tensors=tensors_to_log,
-          "--epochs_per_eval", "1", "--use_synthetic_data",
+          "--epochs_between_evals", "1", "--use_synthetic_data",
-      'set both arguments --train_data and --test_data.' % data_file)
+      '%s not found. Please make sure you have run data_download.py and '
-        FLAGS.train_data, FLAGS.epochs_per_eval, True, FLAGS.batch_size))
+  train_file = os.path.join(FLAGS.data_dir, 'adult.data')
-        FLAGS.test_data, 1, False, FLAGS.batch_size))
+        test_file, 1, False, FLAGS.batch_size))
-    print('Results at epoch', (n + 1) * FLAGS.epochs_per_eval)
+    print('Results at epoch', (n + 1) * FLAGS.epochs_between_evals)
-from official.benchmark import logger
+from official.utils.logging import logger
-          "value": value,
+          "value": float(value),
-      f.write("\n")
+      try:
-    integration.run_synthetic(main=cifar10_main.main, extra_flags=['-v', '1'])
+    integration.run_synthetic(
-    integration.run_synthetic(main=cifar10_main.main, extra_flags=['-v', '2'])
+    integration.run_synthetic(
-    integration.run_synthetic(main=imagenet_main.main, extra_flags=['-v', '1'])
+    integration.run_synthetic(
-    integration.run_synthetic(main=imagenet_main.main, extra_flags=['-v', '2'])
+    integration.run_synthetic(
-                              extra_flags=['-v', '1', '-rs', '18'])
+    integration.run_synthetic(
-                              extra_flags=['-v', '2', '-rs', '18'])
+    integration.run_synthetic(
-                              extra_flags=['-v', '1', '-rs', '200'])
+    integration.run_synthetic(
-                              extra_flags=['-v', '2', '-rs', '200'])
+    integration.run_synthetic(
-import time
+import tempfile
-def run_synthetic(main, extra_flags=None):
+def run_synthetic(main, tmp_root, extra_flags=None):
-    shutil.rmtree(model_dir)
+  model_dir = tempfile.mkdtemp(dir=tmp_root)
-      boundary_epochs.
+      for scaling the learning rate. It should have one more element
-def ptb_raw_data(data_path):
+def ptb_raw_data(data_path=None):
-def ptb_raw_data(data_path=None):
+def ptb_raw_data(data_path):
-
+def main(argv):
-  tf.app.run(argv=[sys.argv[0]] + unparsed)
+  flags = parser.parse_args(args=argv[1:])
-  resnet_run_loop.resnet_main(FLAGS, imagenet_model_fn, input_function)
+def main(argv):
-  tf.app.run(argv=[sys.argv[0]] + unparsed)
+  tf.app.run(argv=sys.argv)
-    classifier.train(input_fn=input_fn_train, hooks=train_hooks)
+    classifier.train(input_fn=input_fn_train, hooks=train_hooks,
-    eval_results = classifier.evaluate(input_fn=input_fn_eval)
+    # flags.max_train_steps is generally associated with testing and profiling.
-        metavar='<RS>'
+        help='[default: %(default)s] The size of the ResNet model to use.',
-               intra_op=True, use_synthetic_data=True):
+               intra_op=True, use_synthetic_data=True, max_train_steps=True):
-  ex.features.feature[name].bytes_list.value.extend([str(v) for v in value])
+  ex.features.feature[name].bytes_list.value.extend([
-        _set_bytes_feature(ex, col_name, [str(value)])
+        _set_bytes_feature(ex, col_name, [value])
-    with fits.open(open(filename, "r")) as hdu_list:
+    with fits.open(open(filename, "rb")) as hdu_list:
-  for time, flux in itertools.izip(all_time, all_flux):
+  for time, flux in zip(all_time, all_flux):
-  for time, flux in itertools.izip(all_time, all_flux):
+  for time, flux in zip(all_time, all_flux):
-  for time, masked_time, masked_spline in itertools.izip(
+  for time, masked_time, masked_spline in zip(
-    for time, flux in itertools.izip(all_time, all_flux):
+    for time, flux in zip(all_time, all_flux):
-        values=input_config.label_map.values(),
+        keys=list(input_config.label_map.keys()),
-    for name, time_series in self.time_series_features.iteritems():
+    for name, time_series in self.time_series_features.items():
-    for name, time_series in self.time_series_features.iteritems():
+    for name, time_series in self.time_series_features.items():
-  for col_name, value in tce.iteritems():
+  for col_name, value in tce.items():
-        for name, t in tensor_or_collection.iteritems()
+        for name, t in tensor_or_collection.items()
-        for feature_name, feature in input_config.features.iteritems()
+        for feature_name, feature in input_config.features.items()
-    for feature_name, value in parsed_features.iteritems():
+    for feature_name, value in parsed_features.items():
-  for feature, tensor in model.time_series_features.iteritems():
+  for feature, tensor in model.time_series_features.items():
-  for feature, tensor in model.aux_features.iteritems():
+  for feature, tensor in model.aux_features.items():
-  for feature_name, feature_spec in config.iteritems():
+  for feature_name, feature_spec in config.items():
-          for feature, tensor in features[feature_type].iteritems()
+          for feature, tensor in features[feature_type].items()
-      for name, spec in feature_spec.iteritems() if spec["is_time_series"]
+      for name, spec in feature_spec.items() if spec["is_time_series"]
-      for name, spec in feature_spec.iteritems() if not spec["is_time_series"]
+      for name, spec in feature_spec.items() if not spec["is_time_series"]
-  for path, value in flat_config.iteritems():
+  for path, value in flat_config.items():
-      for field, value in initial_dictionary.iteritems():
+      for field, value in initial_dictionary.items():
-    return inputs, inputs.pop("labels", None)
+    return dataset
-from official.resnet import resnet
+from official.resnet import resnet_model
-      examples_per_epoch=num_images, multi_gpu=multi_gpu)
+  return resnet_run_loop.process_record_dataset(dataset, is_training, batch_size,
-  return resnet.get_synth_input_fn(_HEIGHT, _WIDTH, _NUM_CHANNELS, _NUM_CLASSES)
+  return resnet_run_loop.get_synth_input_fn(_HEIGHT, _WIDTH, _NUM_CHANNELS, _NUM_CLASSES)
-class Cifar10Model(resnet.Model):
+class Cifar10Model(resnet_model.Model):
-      version=resnet.DEFAULT_VERSION):
+      version=resnet_model.DEFAULT_VERSION):
-  learning_rate_fn = resnet.learning_rate_with_decay(
+  learning_rate_fn = resnet_run_loop.learning_rate_with_decay(
-                                multi_gpu=params['multi_gpu'])
+  return resnet_run_loop.resnet_model_fn(features, labels, mode, Cifar10Model,
-  resnet.resnet_main(FLAGS, cifar10_model_fn, input_function)
+  resnet_run_loop.resnet_main(FLAGS, cifar10_model_fn, input_function)
-  parser = resnet.ResnetArgParser()
+  parser = resnet_run_loop.ResnetArgParser()
-from official.resnet import resnet
+from official.resnet import resnet_model
-  return resnet.process_record_dataset(
+  return resnet_run_loop.process_record_dataset(
-  return resnet.get_synth_input_fn(
+  return resnet_run_loop.get_synth_input_fn(
-class ImagenetModel(resnet.Model):
+class ImagenetModel(resnet_model.Model):
-    version=resnet.DEFAULT_VERSION):
+    version=resnet_model.DEFAULT_VERSION):
-  learning_rate_fn = resnet.learning_rate_with_decay(
+  learning_rate_fn = resnet_run_loop.learning_rate_with_decay(
-                                multi_gpu=params['multi_gpu'])
+  return resnet_run_loop.resnet_model_fn(features, labels, mode, ImagenetModel,
-  resnet.resnet_main(FLAGS, imagenet_model_fn, input_function)
+  resnet_run_loop.resnet_main(FLAGS, imagenet_model_fn, input_function)
-  parser = resnet.ResnetArgParser(
+  parser = resnet_run_loop.ResnetArgParser(
-    data_format):
+                       data_format):
-    data_format):
+                       data_format):
-    strides, data_format):
+                         strides, data_format):
-    strides, data_format):
+                         strides, data_format):
-    )
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-from official.resnet import resnet  # pylint: disable=g-bad-import-order
+from official.resnet import resnet_model  # pylint: disable=g-bad-import-order
-      return resnet.conv2d_fixed_padding(
+      return resnet_model.conv2d_fixed_padding(
-      block_fn = resnet._building_block_v1
+      block_fn = resnet_model._building_block_v1
-        block_fn = resnet._bottleneck_block_v1
+        block_fn = resnet_model._bottleneck_block_v1
-      block_fn = resnet._building_block_v2
+      block_fn = resnet_model._building_block_v2
-        block_fn = resnet._bottleneck_block_v2
+        block_fn = resnet_model._bottleneck_block_v2
-
+from official.utils.logging import hooks_helper
-        tensors=tensors_to_log, every_n_iter=100)
+    train_hooks = hooks_helper.get_train_hooks(flags.hooks, batch_size=flags.batch_size)
-    classifier.train(input_fn=input_fn_train, hooks=[logging_hook])
+    classifier.train(input_fn=input_fn_train, hooks=train_hooks)
-      official.utils.arg_parsers.DummyParser(use_synthetic_data=True),
+      arg_parsers.LocationParser(data_dir=True, model_dir=True),
-               multi_gpu=True):
+               multi_gpu=True, hooks=True):
-          metavar="<CF>",
+          metavar="<CF>"
-                                 warm_steps=10,
+                                 warm_steps=5,
-class BaseTest(tf.test.TestCase):
+class BaseTest(unittest.TestCase):
-  def test_get_train_hooks_LoggingTensorHook(self):
+  def test_get_train_hooks_logging_tensor_hook(self):
-  def test_get_train_hooks_ProfilerHook(self):
+  def test_get_train_hooks_profiler_hook(self):
-  def test_get_train_hooks_ExamplesPerSecondHook(self):
+  def test_get_train_hooks_examples_per_second_hook(self):
-(https://arxiv.org/abs1802.02611)
+(https://arxiv.org/abs/1802.02611)
-        help='The directory where the model will be stored.')
+    super(ResnetArgParser, self).__init__(parents=[
-        '--resnet_size', type=int, default=50,
+        '--resnet_size', '-rs', type=int, default=50,
-        help="Version of ResNet. (1 or 2) See README.md for details."
+        help='[default: %(default)s]The size of the ResNet model to use.',
-             'See TensorFlow config.proto for details.')
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-from official.resnet import vgg_preprocessing
+from official.resnet import imagenet_preprocessing
-  image/encoded (a JPEG-encoded string) and image/class/label (int)
+  The output of the build_image_data.py image preprocessing script is a dataset
-    label: Tensor tf.int64 containing the label.
+    label: Tensor tf.int32 containing the label.
-                                              default_value=-1)
+                                              default_value=-1),
-  return features['image/encoded'], features['image/class/label']
+  xmin = tf.expand_dims(features['image/object/bbox/xmin'].values, 0)
-      image=image,
+  """
-  label = tf.one_hot(label, _NUM_CLASSES)
+  label = tf.one_hot(tf.reshape(label, shape=[]), _NUM_CLASSES)
-from six.moves import xrange
+
-from six.moves import xrange
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-                           regularize_depthwise=False):
+                           regularize_depthwise=False,
-      'epsilon': 0.001,
+      'decay': batch_norm_decay,
-                       use_summaries=True, drop_connect_version='v3'):
+                       use_summaries=False, drop_connect_version='v3'):
-  Rather than adding that level of complication, we select a distinct ordering
+  Rather then adding that level of complication, we select a distinct ordering
-    reshape = tf.reshape(pool2, [FLAGS.batch_size, -1])
+    reshape = tf.reshape(pool2, [images.get_shape()[0], -1])
-  Rather then adding that level of complication, we select a distinct ordering
+  Rather than adding that level of complication, we select a distinct ordering
-  Rather then adding that level of complication, we select a distinct ordering
+  Rather than adding that level of complication, we select a distinct ordering
-  Rather then adding that level of complication, we select a distinct ordering
+  Rather than adding that level of complication, we select a distinct ordering
-  Rather then adding that level of complication, we select a distinct ordering
+  Rather than adding that level of complication, we select a distinct ordering
-import resnet
+from official.resnet import resnet  # pylint: disable=g-bad-import-order
-  def __init__(self, resnet_size, data_format=None, num_classes=_NUM_CLASSES):
+  def __init__(self, resnet_size, data_format=None, num_classes=_NUM_CLASSES,
-        block_fn=resnet.building_block,
+        version=version,
-  def cifar10_model_fn_helper(self, mode, multi_gpu=False):
+  def cifar10_model_fn_helper(self, mode, version, multi_gpu=False):
-    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.TRAIN)
+  def test_cifar10_model_fn_train_mode_v1(self):
-    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.TRAIN, multi_gpu=True)
+  def test_cifar10_model_fn_trainmode__v2(self):
-    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.EVAL)
+  def test_cifar10_model_fn_train_mode_multi_gpu_v1(self):
-    self.cifar10_model_fn_helper(tf.estimator.ModeKeys.PREDICT)
+  def test_cifar10_model_fn_train_mode_multi_gpu_v2(self):
-    output = model(fake_input, training=True)
+    for version in (1, 2):
-    self.assertAllEqual(output.shape, (batch_size, num_classes))
+      self.assertAllEqual(output.shape, (batch_size, num_classes))
-  def __init__(self, resnet_size, data_format=None, num_classes=_NUM_CLASSES):
+  def __init__(self, resnet_size, data_format=None, num_classes=_NUM_CLASSES,
-      block_fn = resnet.building_block
+      bottleneck = False
-      block_fn = resnet.bottleneck_block
+      bottleneck = True
-        block_fn=block_fn,
+        version=version,
-  def tensor_shapes_helper(self, resnet_size, with_gpu=False):
+  def tensor_shapes_helper(self, resnet_size, version, with_gpu=False):
-          data_format='channels_first' if with_gpu else 'channels_last')
+          data_format='channels_first' if with_gpu else 'channels_last',
-    self.tensor_shapes_helper(18)
+  def test_tensor_shapes_resnet_18_v1(self):
-    self.tensor_shapes_helper(34)
+  def test_tensor_shapes_resnet_18_v2(self):
-    self.tensor_shapes_helper(50)
+  def test_tensor_shapes_resnet_34_v1(self):
-    self.tensor_shapes_helper(101)
+  def test_tensor_shapes_resnet_34_v2(self):
-    self.tensor_shapes_helper(152)
+  def test_tensor_shapes_resnet_50_v1(self):
-    self.tensor_shapes_helper(200)
+  def test_tensor_shapes_resnet_50_v2(self):
-    self.tensor_shapes_helper(18, True)
+  def test_tensor_shapes_resnet_50_with_gpu_v1(self):
-    self.tensor_shapes_helper(34, True)
+  def test_tensor_shapes_resnet_50_with_gpu_v2(self):
-    self.tensor_shapes_helper(50, True)
+  def test_tensor_shapes_resnet_101_with_gpu_v1(self):
-    self.tensor_shapes_helper(101, True)
+  def test_tensor_shapes_resnet_101_with_gpu_v2(self):
-    self.tensor_shapes_helper(152, True)
+  def test_tensor_shapes_resnet_152_with_gpu_v1(self):
-    self.tensor_shapes_helper(200, True)
+  def test_tensor_shapes_resnet_152_with_gpu_v2(self):
-  def resnet_model_fn_helper(self, mode, multi_gpu=False):
+  @unittest.skipUnless(tf.test.is_built_with_cuda(), 'requires GPU')
-    self.resnet_model_fn_helper(tf.estimator.ModeKeys.TRAIN)
+  def test_resnet_model_fn_train_mode_v1(self):
-    self.resnet_model_fn_helper(tf.estimator.ModeKeys.TRAIN, multi_gpu=True)
+  def test_resnet_model_fn_eval_mode_v2(self):
-    self.resnet_model_fn_helper(tf.estimator.ModeKeys.EVAL)
+  def test_resnet_model_fn_predict_mode_v1(self):
-    self.resnet_model_fn_helper(tf.estimator.ModeKeys.PREDICT)
+  def test_resnet_model_fn_predict_mode_v2(self):
-    output = model(fake_input, training=True)
+    for version in (1, 2):
-    self.assertAllEqual(output.shape, (batch_size, num_classes))
+      self.assertAllEqual(output.shape, (batch_size, num_classes))
-(also known as ResNet v2).
+"""Contains definitions for Residual Networks.
-Residual networks (ResNets) were originally proposed in:
+Residual networks ('v1' ResNets) were originally proposed in:
-introduced by:
+The full preactivation 'v2' ResNet variant was introduced by:
-# Functions building the ResNet model.
+# Convenience functions for building the ResNet model.
-  """Performs a batch normalization followed by a ReLU."""
+def batch_norm(inputs, training, data_format):
-  inputs = tf.layers.batch_normalization(
+  return tf.layers.batch_normalization(
-  """Standard building block for residual networks with BN before convolutions.
+################################################################################
-  # since it performs a 1x1 convolution.
+    shortcut = batch_norm(inputs=shortcut, training=training,
-  inputs = batch_norm_relu(inputs, training, data_format)
+  inputs = batch_norm(inputs, training, data_format)
-  return inputs + shortcut
+  return inputs
-  """Bottleneck block variant for residual networks with BN before convolutions.
+def _building_block_v2(inputs, filters, training, projection_shortcut, strides,
-      that the third and final convolution will use 4 times as many filters.
+    filters: The number of filters for the convolutions.
-  inputs = batch_norm_relu(inputs, training, data_format)
+  inputs = batch_norm(inputs, training, data_format)
-  inputs = batch_norm_relu(inputs, training, data_format)
+  inputs = batch_norm(inputs, training, data_format)
-  inputs = batch_norm_relu(inputs, training, data_format)
+  inputs = conv2d_fixed_padding(
-                data_format):
+def block_layer(inputs, filters, bottleneck, block_fn, blocks, strides,
-  filters_out = 4 * filters if block_fn is bottleneck_block else filters
+  filters_out = filters * 4 if bottleneck else filters
-  """Base class for building the Resnet v2 Model.
+  """Base class for building the Resnet Model.
-  def __init__(self, resnet_size, num_classes, num_filters, kernel_size,
+  def __init__(self, resnet_size, bottleneck, num_classes, num_filters,
-               block_strides, final_size, data_format=None):
+               second_pool_size, second_pool_stride, block_sizes, block_strides,
-        the two functions defined above: building_block or bottleneck_block
+      version: Integer representing which version of the ResNet network to use.
-          data_format=self.data_format)
+          inputs=inputs, filters=num_filters, bottleneck=self.bottleneck,
-    inputs = batch_norm_relu(inputs, training, self.data_format)
+    inputs = batch_norm(inputs, training, self.data_format)
-                    data_format, loss_filter_fn=None, multi_gpu=False):
+                    data_format, version, loss_filter_fn=None, multi_gpu=False):
-  model = model_class(resnet_size, data_format)
+  model = model_class(resnet_size, data_format, version=version)
-import dataset
+from official.mnist import dataset
-import dataset
+from official.mnist import mnist
-import mnist_eager
+from official.mnist import mnist
-import mnist
+from official.mnist import mnist
-import mnist
+from official.mnist import dataset
-import resnet
+from official.resnet import resnet
-import cifar10_main
+from official.resnet import cifar10_main
-import vgg_preprocessing
+from official.resnet import resnet
-import imagenet_main
+from official.resnet import imagenet_main
-import wide_deep
+from official.wide_deep import wide_deep
-
+    template = ('\nPrediction is "{}" ({:.1f}%), expected "{}"')
-  os.chmod(FLAGS.output_file, 0744)  # Make the download script executable.
+  os.chmod(FLAGS.output_file, 0o744)  # Make the download script executable.
-      training_optimizer = tf.SyncReplicasOptimizer(
+      training_optimizer = tf.train.SyncReplicasOptimizer(
-          total_num_replicas=train_config.worker_replicas)
+          total_num_replicas=worker_replicas)
-      label_handler = slim_example_decoder.BackupHandler(
+      # TODO(lzc): note that here we are using BackupHandler defined in this
-        absolute_detection_boxlist.get())
+    detection_boxes = absolute_detection_boxlist.get()
-  output_dict[detection_fields.detection_scores] = detection_scores
+
-        correct type.
+      TypeError: if the `train_config`, `train_input_config` or `model_config`
-        correct type.
+      TypeError: if the `eval_config`, `eval_input_config` or `model_config`
-  configs = config_util.merge_external_params_with_configs(
+  get_configs_from_pipeline_file = MODEL_BUILD_UTIL_MAP[
-  train_input_fn = inputs.create_train_input_fn(
+  train_input_fn = create_train_input_fn(
-  eval_input_fn = inputs.create_eval_input_fn(
+  eval_input_fn = create_eval_input_fn(
-          serving_input_fn=inputs.create_predict_input_fn(
+          serving_input_fn=create_predict_input_fn(
-    pipeline_config_final = config_util.create_pipeline_proto_from_configs(
+    pipeline_config_final = create_pipeline_proto_from_configs(
-          for op in config.operations]
+      min_padded_size_ratio = [tuple(op.min_padded_size_ratio)
-    return (preprocessor.ssd_random_crop_pad_fixed_aspect_ratio, {})
+      kwargs['min_object_covered'] = [op.min_object_covered
-        max_padded_size_ratio: [2.0, 2.0]
+      min_padded_size_ratio: [1.0, 1.0]
-                            'max_padded_size_ratio': [(2.0, 2.0), (2.0, 2.0)]})
+                            'min_padded_size_ratio': (1.0, 1.0),
-import cPickle as pickle
+from six.moves import cPickle as pickle
-  with tf.gfile.GFile(DATA_FILE_FORMAT % 'train') as f:
+  with tf.gfile.GFile(DATA_FILE_FORMAT % 'train', 'rb') as f:
-  with tf.gfile.GFile(DATA_FILE_FORMAT % 'test') as f:
+  with tf.gfile.GFile(DATA_FILE_FORMAT % 'test', 'rb') as f:
-  ok_num_examples = [len(ll) == 20 for _, ll in train_data.iteritems()]
+  ok_num_examples = [len(ll) == 20 for _, ll in train_data.items()]
-  ok_num_examples = [len(ll) == 20 for _, ll in test_data.iteritems()]
+  ok_num_examples = [len(ll) == 20 for _, ll in test_data.items()]
-                        r + (episode_length - remainder) / episode_width)
+                        r + (episode_length - remainder) // episode_width)
-               for v in train_data.itervalues())
+               for v in train_data.values())
-               for v in valid_data.itervalues())
+               for v in valid_data.values())
-def train(model, optimizer, dataset, log_interval=None):
+def train(model, optimizer, dataset, step_counter, log_interval=None):
-    with tf.contrib.summary.record_summaries_every_n_global_steps(10):
+    with tf.contrib.summary.record_summaries_every_n_global_steps(
-          zip(grads, model.variables), global_step=global_step)
+          zip(grads, model.variables), global_step=step_counter)
-  # Train and evaluate for 11 epochs.
+  step_counter = tf.train.get_or_create_global_step()
-              (epoch, global_step.numpy(), end - start))
+    for _ in range(10):
-      tfe.Saver(all_variables).save(checkpoint_prefix, global_step=global_step)
+      checkpoint.save(checkpoint_prefix)
-    mnist_eager.train(model, optimizer, dataset)
+    mnist_eager.train(model, optimizer, dataset,
-  run_config = tf.estimator.RunConfig().replace(save_checkpoints_secs=1e9)
+  # Create session config based on values of inter_op_parallelism_threads and
-
+    # This context creates variables
-                           self.mem_age])
+    # Storing current memory state to restore it after prediction
-                        self.mem_age_reset: cur_memory[2]})
+    # Restoring memory state
-                           self.mem_age])
+    # Storing current memory state to restore it after prediction
-                        self.mem_age_reset: cur_memory[2]})
+    # Restoring memory state
-          seen_counts = [[0] * episode_width for _ in xrange(batch_size)]
+          seen_counts = [0] * episode_width
-              seen_counts[k][yyy % episode_width] = count + 1
+            yyy, yyy_preds = int(yy[0]), int(yy_preds[0])
-
+from six.moves import xrange
-
+from six.moves import xrange
-
+from six.moves import xrange
-
+from six.moves import xrange
-
+from six.moves import xrange
-
+from six.moves import xrange
-  resnet.resnet_main(FLAGS, cifar10_model_fn, input_fn)
+  input_function = FLAGS.use_synthetic_data and get_synth_input_fn() or input_fn
-    features, labels = self.input_fn()
+    input_fn = cifar10_main.get_synth_input_fn()
-      examples_per_epoch=num_images, multi_gpu=multi_gpu)
+  return resnet.process_record_dataset(
-  resnet.resnet_main(FLAGS, imagenet_model_fn, input_fn)
+  input_function = FLAGS.use_synthetic_data and get_synth_input_fn() or input_fn
-    features, labels = self.input_fn()
+    input_fn = imagenet_main.get_synth_input_fn()
-        'superseded by the --num_gpus flag.')
+        help='If set, run across all available GPUs.')
-  #
+
-                                                session_config=session_config)
+  # Set up a RunConfig to only save checkpoints once per training cycle.
-"""Tests for image.understanding.object_detection.core.visualization_utils."""
+"""Tests for object_detection.utils.visualization_utils."""
-    self.assertAllClose(custom_op_output, tf_op_output)
+      return custom_op_output
-    indices = np.array([2, 2, 1])
+    indices = np.array([2, 2, 1], dtype=np.int32)
-    indices = np.array([0, 3, 1])
+    indices = np.array([0, 3, 1], dtype=np.int32)
-    indices = np.array([0, 0, 0, 0, 0, 0])
+    indices = np.array([0, 0, 0, 0, 0, 0], dtype=np.int32)
-      if len(materialized_results) == 1:
+      if (len(materialized_results) == 1
-      if len(materialized_results) == 1:
+      if (len(materialized_results) == 1
-          final_endpoint='Cell_11')
+      with arg_scope([slim.conv2d,
-        sess.run(nms.get())
+    with self.assertRaisesWithPredicateMatch(ValueError,
-        ExportSingleImageGroundtruthToCoco(
+        coco_tools.ExportSingleImageGroundtruthToCoco(
-                                                 groundtruth_classes]))
+            groundtruth_boxes=groundtruth_dict[
-                                       groundtruth_masks=None):
+                                       groundtruth_masks=None,
-  TODO(jonathanhuang): pass in "iscrowd" array for evaluating on COCO dataset.
+  groundtruth bounding box.
-          'iscrowd': 0
+          'id':
-    expected_counts = ['04', '31', '4']
+    # Tests exporting with is_crowd.
-          from_detection_checkpoint=train_config.from_detection_checkpoint)
+          from_detection_checkpoint=train_config.from_detection_checkpoint,
-  run_config = tf.estimator.RunConfig().replace(save_checkpoints_secs=1e9)
+  # Create session config based on values of inter_op_parallelism_threads and
-            FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project))
+            FLAGS.tpu, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)
-# Cloud TPU Cluster Resolvers
+# Cloud TPU Cluster Resolver flags
-    "will attempt to automatically detect the GCE project from metadata.")
+    "tpu", default=None,
-    "will attempt to automatically detect the GCE project from metadata.")
+    help="[Optional] GCE zone where the Cloud TPU is located in. If not "
-    "this flag or --master.")
+    "gcp_project", default=None,
-    tpu_grpc_url = tpu_cluster_resolver.get_master()
+  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
-      evaluation_master=tpu_grpc_url,
+      cluster=tpu_cluster_resolver,
-SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'
+# CVDF mirror of http://yann.lecun.com/exdb/mnist/
-             num_parallel_calls=1):
+             num_parallel_calls=1, multi_gpu=False):
-      _NUM_IMAGES['train'], parse_record, num_epochs, num_parallel_calls)
+      _NUM_IMAGES['train'], parse_record, num_epochs, num_parallel_calls,
-                                loss_filter_fn=loss_filter_fn)
+                                loss_filter_fn=loss_filter_fn,
-  def cifar10_model_fn_helper(self, mode):
+  def cifar10_model_fn_helper(self, mode, multi_gpu=False):
-  # with values ranging from [0, 1).
+  # Results in a 3-D int8 Tensor. This will be converted to a float later,
-             num_parallel_calls=1):
+             num_parallel_calls=1, multi_gpu=False):
-      _SHUFFLE_BUFFER, parse_record, num_epochs, num_parallel_calls)
+      _SHUFFLE_BUFFER, parse_record, num_epochs, num_parallel_calls,
-                                loss_filter_fn=None)
+                                loss_filter_fn=None,
-  def resnet_model_fn_helper(self, mode):
+  def resnet_model_fn_helper(self, mode, multi_gpu=False):
-                           parse_record_fn, num_epochs=1, num_parallel_calls=1):
+                           parse_record_fn, num_epochs=1, num_parallel_calls=1,
-                    data_format, loss_filter_fn=None):
+                    data_format, loss_filter_fn=None, multi_gpu=False):
-    # Batch norm requires update ops to be added as a dependency to train_op
+    # If we are running multi-GPU, we need to wrap the optimizer.
-      train_op = optimizer.minimize(loss, global_step)
+    train_op = tf.group(optimizer.minimize(loss, global_step), update_ops)
-                            flags.epochs_per_eval, flags.num_parallel_calls)
+                            flags.epochs_per_eval, flags.num_parallel_calls,
-                            1, flags.num_parallel_calls)
+                            1, flags.num_parallel_calls, flags.multi_gpu)
-_B_MEAN = 103.94 / 255
+_R_MEAN = 123.68
-    new_width: and int32 scalar tensor indicating the new width.
+    new_width: an int32 scalar tensor indicating the new width.
-# Model specific paramenters
+# Model specific parameters
-      embedding_matrix = np.load(f)
+    embedding_matrix = np.load(embedding_matrix_file)
-                    or "local".""")
+                           """Name prefix of the Tensorflow eval master.""")
-tf.app.flags.DEFINE_string('master', 'local',
+tf.app.flags.DEFINE_string('master', '',
-tf.app.flags.DEFINE_string('eval_master', 'local',
+tf.app.flags.DEFINE_string('eval_master', '',
-    with tf.device(tf.ReplicaDeviceSetter(FLAGS.ps_tasks)):
+    with tf.device(tf.train.replica_device_setter(FLAGS.ps_tasks)):
-        sv = tf.Supervisor(
+        sv = tf.train.Supervisor(
-        `_generate` function in multiple_grid_anchor_generator.py)
+      boxes_list: a list of BoxLists each holding anchor boxes corresponding to
-    return anchors
+    return [anchors]
-      anchor_corners = anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
-      anchor_corners = anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
-      anchors = anchor_generator.generate(
+      anchors_list = anchor_generator.generate(
-      anchor_corners = anchors.get()
+      anchor_corners = anchors_list[0].get()
-        to generate it.
+      boxes_list: a list of BoxLists each holding anchor boxes corresponding to
-                anchor_strides, anchor_offsets)):
+    for feature_map_index, (grid_size, scales, aspect_ratios, stride,
-      anchor_grid_list.append(tiled_anchors)
+      if self._clip_window is not None:
-    return concatenated_anchors
+      anchor_indices = feature_map_index * tf.ones([num_anchors_in_layer])
-    scales: As list of anchor scales to use. When not None and not emtpy,
+    scales: As list of anchor scales to use. When not None and not empty,
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(height,
-      anchors = anchor_generator.generate(
+      anchors_list = anchor_generator.generate(
-      return anchors.get()
+      return anchors_list[0].get()
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(4, 4), (
-    anchor_corners_out = self.execute(graph_fn, [])
+    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(4, 4), (
-    anchor_corners_out = self.execute(graph_fn, [])
+    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)
-      anchors = anchor_generator.generate(
+      anchors_list = anchor_generator.generate(
-    anchor_corners_out = self.execute(graph_fn1, [])
+      return [anchors.get() for anchors in anchors_list]
-      anchors = anchor_generator.generate(
+      anchors_list = anchor_generator.generate(
-    anchor_corners_out = self.execute(graph_fn2, [])
+      return [anchors.get() for anchors in anchors_list]
-class MultiscaleGridAnchorGenerator(object):
+class MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):
-               scales_per_octave):
+               scales_per_octave, normalize_coordinates=True):
-  def generate(self, feature_map_shape_list, im_height, im_width):
+  def _generate(self, feature_map_shape_list, im_height, im_width):
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes_list: a list of BoxLists each holding anchor boxes corresponding to
-      # TODO check the feature_map_shape_list is consistent with
+      # TODO(rathodv) check the feature_map_shape_list is consistent with
-          ag.generate(feature_map_shape_list=[(feat_h, feat_w)]))
+      (anchor_grid,) = ag.generate(feature_map_shape_list=[(feat_h, feat_w)])
-    concatenated_anchors = box_list_ops.concatenate(anchor_grid_list)
+      if self._normalize_coordinates:
-    return concatenated_anchors
+    return anchor_grid_list
-    anchor_corners = anchors.get()
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-      anchor_generator.generate(feature_map_shape_list, im_height, im_width)
+      anchor_generator.generate(
-      anchor_corners = anchors.get()
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-      return (anchor_corners,)
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-    anchor_corners_out = self.execute(graph_fn, [])
+    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)
-      feature_map_shape_list = [(1, 1), (1, 1)]
+      feature_map_shape_list = [(1, 1)]
-      return (anchor_corners,)
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-      feature_map_shape_list = [(1, 1), (1, 1), (1, 1), (1, 1)]
+      feature_map_shape_list = [(1, 1)]
-      anchor_corners = anchors.get()
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-      return (anchor_corners,)
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
-    ])
+    anchor_corners_out = np.concatenate(
-        cfg.scales_per_octave
+        cfg.scales_per_octave,
-  # TODO: Make this a public api in slim arg_scope.py.
+  # TODO(rathodv): Make this a public api in slim arg_scope.py.
-      'image_resizer_oneof') == 'keep_aspect_ratio_resizer':
+  image_resizer_oneof = image_resizer_config.WhichOneof('image_resizer_oneof')
-      'image_resizer_oneof') == 'fixed_shape_resizer':
+  elif image_resizer_oneof == 'fixed_shape_resizer':
-    raise ValueError('Invalid image resizer option.')
+    raise ValueError(
-    return losses.WeightedSmoothL1LocalizationLoss()
+    return losses.WeightedSmoothL1LocalizationLoss(
-  def test_build_weighted_smooth_l1_localization_loss(self):
+  def test_build_weighted_smooth_l1_localization_loss_default_delta(self):
-      add_summaries=add_summaries)
+      add_summaries=add_summaries,
-                truncated_normal_initializer {
+                random_normal_initializer {
-    # TODO: Find a way to not depend on the private members.
+    # TODO(rathodv): Find a way to not depend on the private members.
-    TODO: remove **params from argument list and make stride and
+    TODO(rathodv): remove **params from argument list and make stride and
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes_list: a list of BoxLists each holding anchor boxes corresponding to
-      anchors = self._generate(feature_map_shape_list, **params)
+      anchors_list = self._generate(feature_map_shape_list, **params)
-      return anchors
+                anchors_list, feature_map_shape_list)]):
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes_list: a list of BoxList, each holding a collection of N anchor
-  def _assert_correct_number_of_anchors(self, anchors, feature_map_shape_list):
+  def _assert_correct_number_of_anchors(self, anchors_list,
-      anchors: box_list.BoxList object holding anchors generated
+      anchors_list: A list of box_list.BoxList object holding anchors generated.
-        self.num_anchors_per_location(), feature_map_shape_list):
+    actual_num_anchors = 0
-    return tf.assert_equal(expected_num_anchors, anchors.num_boxes())
+      actual_num_anchors += anchors.num_boxes()
-      # TODO: Remove with tf.device when top_k operation runs
+      # TODO(derekjchow): Remove with tf.device when top_k operation runs
-  TODO: Change function name to filter_scores_greater_than
+  TODO(jonathanhuang): Change function name to filter_scores_greater_than
-  # TODO: Handle the case where some boxes in selected_boxes do not
+  # TODO(kbanoop): Handle the case where some boxes in selected_boxes do not
-    and do not assume anything about their shapes.
+    Takes a list of high level image feature maps as input and produces a list
-          predictions for the proposals.
+        box_encodings: A list of float tensors of shape
-  # TODO: num_predictions_per_location could be moved to constructor.
+  # TODO(rathodv): num_predictions_per_location could be moved to constructor.
-          predictions for the proposals.
+        box_encodings: A list of float tensors of shape
-  Applies a position sensitve ROI pooling on position sensitive feature maps to
+  Applies a position sensitive ROI pooling on position sensitive feature maps to
-        predictions for the proposals.
+      box_encodings: A list of float tensors of shape
-    return {BOX_ENCODINGS: box_encodings,
+    return {BOX_ENCODINGS: [box_encodings],
-            class_predictions_with_background}
+            [class_predictions_with_background]}
-
+      box_encodings: A list of float tensors of shape
-    # TODO: Come up with a better way to generate scope names
+    # TODO(rathodv): Come up with a better way to generate scope names
-            tf.concat(class_predictions_list, axis=1)}
+    return {
-# TODO: Merge the implementation with ConvolutionalBoxPredictor above
+# TODO(rathodv): Merge the implementation with ConvolutionalBoxPredictor above
-          predictions for the proposals.
+      box_encodings: A list of float tensors of shape
-            tf.concat(class_predictions_list, axis=1)}
+    return {
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(box_predictions[box_predictor.BOX_ENCODINGS],
-  """Smooth L1 localization loss function.
+  """Smooth L1 localization loss function aka Huber Loss..
-  otherwise, where x is the difference between predictions and target.
+  The smooth L1_loss is defined elementwise as .5 x^2 if |x| <= delta and
-    return anchorwise_smooth_l1norm
+    return tf.reduce_sum(tf.losses.huber_loss(
-    # TODO: Also test logit_scale with anchorwise=False.
+    # TODO(yonib): Also test logit_scale with anchorwise=False.
-# TODO: This method is needed because the current
+# TODO(mttang): This method is needed because the current
-# TODO: Make sure the static shapes are preserved.
+# TODO(alirezafathi): Make sure the static shapes are preserved.
-# TODO: Investigate if instead the function should return None if
+# TODO(alirezafathi): Investigate if instead the function should return None if
-# TODO: This method pulls in all the implementation dependencies into
+# TODO(rathodv): This method pulls in all the implementation dependencies into
-    TODO: Make this test more general.
+    TODO(rathodv): Make this test more general.
-                dct_method=dct_method),
+            image,
-    # TODO filter out targets that are truncated or heavily occluded.
+    # TODO(talremez) filter out targets that are truncated or heavily occluded.
-# TODO: Add test for pet/PASCAL main files.
+# TODO(derekjchow): Add test for pet/PASCAL main files.
-# TODO: Add tests.
+# TODO(rathodv): Add tests.
-          # TODO: result_dict contains batches of images, while
+          # TODO(akuznetsa): result_dict contains batches of images, while
-# TODO: Add tests.
+# TODO(rathodv): Add tests.
-    # TODO: This should be done in model's postprocess
+    # TODO(rathodv): This should be done in model's postprocess
-             checkpoint_dir, eval_dir, graph_hook_fn=None):
+             checkpoint_dir, eval_dir, graph_hook_fn=None, evaluator_list=None):
-      evaluators=get_evaluators(eval_config, categories),
+      evaluators=evaluator_list,
-# TODO: Replace with freeze_graph.freeze_graph_with_def_protos when
+# TODO(derekjchow): Replace with freeze_graph.freeze_graph_with_def_protos when
-def _write_frozen_graph(frozen_graph_path, frozen_graph_def):
+def write_frozen_graph(frozen_graph_path, frozen_graph_def):
-                       outputs):
+def write_saved_model(saved_model_path,
-                                trained_checkpoint_prefix):
+def write_graph_and_checkpoint(inference_graph_def,
-  if graph_hook_fn: graph_hook_fn()
+  outputs, placeholder_tensor = _build_detection_graph(
-  _write_graph_and_checkpoint(
+  write_graph_and_checkpoint(
-                     placeholder_tensor, outputs)
+  write_frozen_graph(frozen_graph_path, frozen_graph_def)
-  # TODO: Make sure these tests work fine outside google3.
+  # TODO(ronnyvotel): Make sure these tests work fine outside google3.
-    TODO: Add num_valid_columns options to match only that many columns
+    TODO(rathodv): Add num_valid_columns options to match only that many columns
-    # TODO: add_summaries is currently unused. Respect that directive
+    # TODO(rathodv): add_summaries is currently unused. Respect that directive
-        (and if number_of_stages=1):
+        (and if number_of_stages > 1):
-        [(feature_map_shape[1], feature_map_shape[2])])
+    anchors = box_list_ops.concatenate(
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(
-        # TODO: Move `unmatched_cls_target` from constructor to assign
+        # TODO(rathodv): Move `unmatched_cls_target` from constructor to assign
-  # TODO: Split test into two - with and without masks.
+  # TODO(rathodv): Split test into two - with and without masks.
-  # TODO: Split test into two - with and without masks.
+  # TODO(rathodv): Split test into two - with and without masks.
-    # TODO: add_summaries is currently unused. Respect that directive
+    # TODO(rathodv): add_summaries is currently unused. Respect that directive
-        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+        tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)
-        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+        tf.concat(
-               add_summaries=True):
+               add_summaries=True,
-    # TODO: handle agnostic mode and positive/negative class
+    # TODO(jonathanhuang): handle agnostic mode
-        negative_class_weight=1.0,
+        negative_class_weight=negative_class_weight,
-      # TODO: revisit whether to always use batch size as
+      # TODO(jonathanhuang): revisit whether to always use batch size as
-        im_width=image_shape[2])
+    self._anchors = box_list_ops.concatenate(
-        'class_predictions_with_background']
+    box_encodings = tf.squeeze(
-                             localization_loss)
+        localization_loss_normalizer = normalizer
-    return box_list.BoxList(
+    return [box_list.BoxList(
-                    ], tf.float32))
+                    ], tf.float32))]
-  def _create_model(self, apply_hard_mining=True):
+  def _create_model(self, apply_hard_mining=True,
-        normalize_loss_by_num_matches, hard_example_miner, add_summaries=False)
+        encode_background_as_zeros, negative_class_weight, image_resizer_fn,
-TODO: wrap as a slim metric in metrics.py
+TODO(jonathanhuang): wrap as a slim metric in metrics.py
-  TODO: pass in "iscrowd" array for evaluating on COCO dataset.
+  TODO(jonathanhuang): pass in "iscrowd" array for evaluating on COCO dataset.
-  TODO: pass in "iscrowd" array for evaluating on COCO dataset.
+  TODO(jonathanhuang): pass in "iscrowd" array for evaluating on COCO dataset.
-          eval_dict, category_index, max_boxes_to_draw=20, min_score_thresh=0.2)
+        detection_and_groundtruth = (
-    train_steps = train_config.num_steps if train_config.num_steps else None
+  if train_steps is None and train_config.num_steps:
-    eval_steps = eval_config.num_examples if eval_config.num_examples else None
+  if eval_steps is None and eval_config.num_examples:
-    eval_input_fn: Input function for the evaluation run
+    eval_validation_input_fn: Input function to run for evaluation on
-    train_steps = train_config.num_steps if train_config.num_steps else None
+  if train_steps is None and train_config.num_steps:
-    eval_steps = eval_config.num_examples if eval_config.num_examples else None
+  if eval_steps is None and eval_config.num_examples:
-  eval_input_fn = inputs.create_eval_input_fn(
+  eval_validation_input_fn = inputs.create_eval_input_fn(
-  return estimator, train_input_fn, eval_input_fn, train_steps, eval_steps
+  return (estimator, train_input_fn, eval_validation_input_fn,
-          params=params))
+  (estimator, train_input_fn, eval_validation_input_fn, eval_training_input_fn,
-    eval_results = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)
+    eval_results = estimator.evaluate(
-            input_fn=eval_input_fn,
+            input_fn=input_fn,
-            checkpoint_path=ckpt)
+            checkpoint_path=ckpt,
-# TODO: Only fixed_shape_resizer is currently supported for NASNet
+# TODO(shlens): Only fixed_shape_resizer is currently supported for NASNet
-      # TODO: Figure out if it is needed when image
+      # TODO(chensun): Figure out if it is needed when image
-# TODO: add tests with different anchor strides.
+# TODO(rathodv): add tests with different anchor strides.
-  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-      pad_to_multiple, expected_feature_map_shapes):
+      pad_to_multiple, expected_feature_map_shapes, use_explicit_padding=False):
-                                                         pad_to_multiple)
+                                                         pad_to_multiple,
-      pad_to_multiple, expected_feature_map_shapes):
+      pad_to_multiple, expected_feature_map_shapes, use_explicit_padding=False):
-                                                         pad_to_multiple)
+                                                         pad_to_multiple,
-        features. Default is False.
+      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
-      # TODO: Enable fused batch norm once quantization supports it.
+      # TODO(skligys): Enable fused batch norm once quantization supports it.
-                                is_training=True, batch_norm_trainable=True):
+                                is_training=True, batch_norm_trainable=True,
-        conv_hyperparams, batch_norm_trainable)
+        conv_hyperparams, batch_norm_trainable=batch_norm_trainable,
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
-    # TODO: Change resnet endpoint to strip scope prefixes instead
+    # TODO(rathodv): Change resnet endpoint to strip scope prefixes instead
-  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-        conv_hyperparams, batch_norm_trainable)
+        conv_hyperparams, batch_norm_trainable,
-  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-            conv_hyperparams, batch_norm_trainable))
+            conv_hyperparams, batch_norm_trainable,
-  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-            conv_hyperparams, batch_norm_trainable))
+            conv_hyperparams, batch_norm_trainable,
-    # TODO: See if summaries can be added/extracted from global tf
+    # TODO(rathodv): See if summaries can be added/extracted from global tf
-        tf.logging.info("Overwriting label map path: %s", value)
+      _update_label_map_path(configs, value)
-  TODO: add runtime checks for depth and indices.
+  TODO(rathodv): add runtime checks for depth and indices.
-  TODO: Add option to scale by L2 norm of the entire input.
+  TODO(jonathanhuang): Add option to scale by L2 norm of the entire input.
-  # TODO: Make this a public function.
+  # TODO(rathodv): Make this a public function.
-  TODO: make this function fully interchangeable with tf.map_fn.
+  TODO(jonathanhuang): make this function fully interchangeable with tf.map_fn.
-# TODO: Consider replacing with tf.contrib.filter_variables in
+# TODO(derekjchow): Consider replacing with tf.contrib.filter_variables in
-  TODO: force input and output to be a dictionary.
+  TODO(rathodv): force input and output to be a dictionary.
-"""
+"""Tests for image.understanding.object_detection.core.visualization_utils."""
-    'batch_size', 50, 'The number of samples in each batch.')
+    'batch_size', 100, 'The number of samples in each batch.')
-    height, width = 300, 400
+    height, width = 256, 256
-from six.moves import xrange
+
-        self.assertItemsEqual(endpoints[:index+1], end_points)
+        self.assertItemsEqual(endpoints[:index+1], end_points.keys())
-    height, width = 400, 600
+    batch_size = 1
-                           [batch_size, 11, 17, 1536])
+                           [batch_size, 8, 11, 1536])
-    height, width = 400, 600
+    batch_size = 1
-      self.assertTupleEqual(pre_pool_out.shape, (batch_size, 11, 17, 1536))
+      self.assertTupleEqual(pre_pool_out.shape, (batch_size, 8, 11, 1536))
-        self.assertItemsEqual(endpoints[:index+1], end_points)
+        self.assertItemsEqual(endpoints[:index+1], end_points.keys())
-    height, width = 300, 400
+    batch_size = 1
-      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 10, 13, 1024])
+      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])
-        self.assertItemsEqual(endpoints[:index+1], end_points)
+        self.assertItemsEqual(endpoints[:index+1], end_points.keys())
-    height, width = 300, 400
+    batch_size = 1
-      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 10, 13, 1024])
+      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])
-        self.assertItemsEqual(endpoints[:index+1], end_points)
+        self.assertItemsEqual(endpoints[:index+1], end_points.keys())
-    height, width = 400, 600
+    batch_size = 1
-      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 11, 17, 2048])
+      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 11, 2048])
-        self.assertItemsEqual(all_endpoints[:index+1], end_points)
+        self.assertItemsEqual(all_endpoints[:index+1], end_points.keys())
-    height, width = 400, 600
+    batch_size = 1
-                         [batch_size, 11, 17, 1536])
+                         [batch_size, 9, 11, 1536])
-    height, width = 400, 600
+    batch_size = 1
-      self.assertTupleEqual(pre_pool_out.shape, (batch_size, 11, 17, 1536))
+      self.assertTupleEqual(pre_pool_out.shape, (batch_size, 9, 11, 1536))
-    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding='SAME'):
+    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=padding):
-        self.assertItemsEqual(endpoints[:index+1], end_points)
+        self.assertItemsEqual(endpoints[:index+1], end_points.keys())
-    for endpoint_name, expected_shape in endpoints_shapes.items():
+    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
-    for endpoint_name, expected_shape in endpoints_shapes.items():
+    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
-    for endpoint_name, expected_shape in endpoints_shapes.items():
+    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
-    for endpoint_name, expected_shape in endpoints_shapes.items():
+    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
-      self.assertAlmostEqual(3217920, total_params)
+      self.assertAlmostEqual(3217920L, total_params)
-    height, width = 300, 400
+    batch_size = 1
-      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 10, 13, 1024])
+      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])
-def _cifar_config(is_training=True):
+def _cifar_config(is_training=True, use_aux_head=True):
-      use_aux_head=1,
+      use_aux_head=int(use_aux_head),
-def _large_imagenet_config(is_training=True):
+def _large_imagenet_config(is_training=True, use_aux_head=True):
-      use_aux_head=1,
+      use_aux_head=int(use_aux_head),
-def _mobile_imagenet_config():
+def _mobile_imagenet_config(use_aux_head=True):
-      use_aux_head=1,
+      use_aux_head=int(use_aux_head),
-    images, num_classes, is_training=True):
+    images, num_classes, is_training=True, use_aux_head=True):
-  hparams = _cifar_config(is_training=is_training)
+  hparams = _cifar_config(is_training=is_training, use_aux_head=use_aux_head)
-                        final_endpoint=None):
+                        final_endpoint=None,
-  hparams = _mobile_imagenet_config()
+  hparams = _mobile_imagenet_config(use_aux_head=use_aux_head)
-                       final_endpoint=None):
+                       final_endpoint=None,
-  hparams = _large_imagenet_config(is_training=is_training)
+  hparams = _large_imagenet_config(is_training=is_training,
-    if add_and_check_endpoint('global_pool', net) or num_classes is None:
+    if add_and_check_endpoint('global_pool', net) or not num_classes:
-        axis=split_axis, num_or_size_splits=1, value=net)
+    net = tf.split(axis=split_axis, num_or_size_splits=1, value=net)
-    """Apply drop_path regularization to net."""
+  @tf.contrib.framework.add_arg_scope  # No public API. For internal use only.
-        tf.summary.scalar('drop_path_keep_prob', drop_path_keep_prob)
+      assert drop_connect_version in ['v1', 'v2', 'v3']
-          raise ValueError('The target output_stride cannot be reached.')
+        if store_non_strided_activations and i == len(block.args) - 1:
-        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)
+        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride,
-    self.assertItemsEqual(expected, end_points)
+    self.assertItemsEqual(expected, end_points.keys())
-        if num_classes is not None:
+        if num_classes:
-    self.assertItemsEqual(expected, end_points)
+    self.assertItemsEqual(expected, end_points.keys())
-        if spatial_squeeze and num_classes is not None:
+        if spatial_squeeze:
-    # Randomly distort the colors. There are 4 ways to do it.
+    # Randomly distort the colors. There are 1 or 4 ways to do it.
-        num_cases=4)
+        num_cases=num_distort_cases)
-  new_width = tf.to_int32(width * scale)
+  new_height = tf.to_int32(tf.rint(height * scale))
-flags.DEFINE_string('logdir', '', 'Directory for writing training event logs')
+flags.DEFINE_string('checkpoint_dir', '',
-        FLAGS.logdir,
+        FLAGS.checkpoint_dir,
-    optimizer=tf.train.AdamOptimizer(learning_rate=0.001))
+autoencoder = Autoencoder(n_layers=[784, 200],
-        self.n_hidden = n_hidden
+    def __init__(self, n_layers, transfer_function=tf.nn.softplus, optimizer=tf.train.AdamOptimizer()):
-        self.reconstruction = tf.add(tf.matmul(self.hidden, self.weights['w2']), self.weights['b2'])
+        self.x = tf.placeholder(tf.float32, [None, self.n_layers[0]])
-        all_weights['b2'] = tf.Variable(tf.zeros([self.n_input], dtype=tf.float32))
+        # Encoding network weights
-        return self.sess.run(self.cost, feed_dict = {self.x: X})
+        return self.sess.run(self.cost, feed_dict={self.x: X})
-        return self.sess.run(self.hidden, feed_dict={self.x: X})
+        return self.sess.run(self.hidden_encode[-1], feed_dict={self.x: X})
-    def generate(self, hidden = None):
+    def generate(self, hidden=None):
-        return self.sess.run(self.reconstruction, feed_dict={self.hidden: hidden})
+            hidden = np.random.normal(size=self.weights['encode'][-1]['b'])
-        return self.sess.run(self.weights['w1'])
+        raise NotImplementedError
-        return self.sess.run(self.weights['b1'])
+        raise NotImplementedError
-        correct_by_shot = dict((k, []) for k in xrange(self.episode_width + 1))
+        num_shots = episode_length // episode_width
-              count = seen_counts[k][yyy % self.episode_width]
+              count = seen_counts[k][yyy % episode_width]
-              seen_counts[k][yyy % self.episode_width] = count + 1
+              seen_counts[k][yyy % episode_width] = count + 1
-        logging.info('%d-shot: %.3f, ' * (self.episode_width + 1),
+        logging.info('%d-shot: %.3f, ' * num_shots,
-                           for k in xrange(self.episode_width + 1)], []))
+                           for k in xrange(num_shots)], []))
-  args.solver.pretrained_path = resnet_v2_50_path
+  args.solver.pretrained_path = rgb_resnet_v2_50_path
-from adversarial_text.data import data_utils
+from data import data_utils
-import data_utils
+from data import data_utils
-import document_generators
+from data import data_utils
-import document_generators
+from data import data_utils
-from adversarial_text.data import data_utils
+import graphs
-# distributed under the License is Distributed_TensorFlow on an "AS IS" BASIS,
+# distributed under the License is distributed on an "AS IS" BASIS,
-# Distributed_TensorFlow under the License is Distributed_TensorFlow on an "AS IS" BASIS,
+# distributed under the License is Distributed_TensorFlow on an "AS IS" BASIS,
-from adversarial_text import graphs
+import graphs
-from adversarial_text import layers as layers_lib
+import adversarial_losses as adv_lib
-from adversarial_text.data import data_utils
+from data import data_utils
-from adversarial_text import train_utils
+import graphs
-from adversarial_text import train_utils
+import graphs
-from adversarial_text.data import data_utils
+import data_utils
-from adversarial_text.data import document_generators
+import data_utils
-from adversarial_text.data import document_generators
+import data_utils
-      pred = tf.cast(tf.greater(tf.squeeze(logits, -1), 0.5), tf.int64)
+      pred = tf.cast(tf.greater(tf.squeeze(logits, -1), 0.), tf.int64)
-    class_label: bool.
+    class_label: integer, starting from 0.
-          label=int(row[0]),
+          label=int(row[0]) - 1,  # Labels should start from 0
-K = tf.contrib.keras
+K = tf.keras
-      logits = tf.matmul(x, self.lin_w) + self.lin_b
+      logits = self.multiclass_dense_layer(x)
-      pred = tf.argmax(logits, 1)
+      pred = tf.argmax(logits, 2)
-        train_op = tf.no_op(name='train_op')
+      with tf.control_dependencies([apply_gradient_op]):
-      save_model_secs=5 * 60,
+      save_summaries_secs=30,
-    if is_chief:
+    if is_chief and global_step_val >= FLAGS.max_steps:
-    self.global_step = tf.contrib.framework.get_or_create_global_step()
+    self.global_step = tf.train.get_or_create_global_step()
-    logits: 2-D float Tensor, [num_timesteps*batch_size, m], where m=1 if
+    logits: 3-D float Tensor, [batch_size, num_timesteps, m], where m=1 if
-              [num_timesteps * batch_size] if num_classes == 2.
+              [batch_size, num_timesteps, num_classes] if num_classes > 2, and
-    weights: 1-D float tensor with shape [num_timesteps * batch_size].
+    weights: 1-D float tensor with shape [batch_size, num_timesteps].
-    kl = tf.squeeze(kl)
+    kl = tf.squeeze(kl, 2)
-        q * (tf.nn.log_softmax(q_logits) - tf.nn.log_softmax(p_logits)), 1)
+        q * (tf.nn.log_softmax(q_logits) - tf.nn.log_softmax(p_logits)), -1)
-  weights.get_shape().assert_has_rank(1)
+  kl.get_shape().assert_has_rank(2)
-      weights = tf.gather_nd(inputs.weights, indices)
+      labels = tf.expand_dims(tf.gather_nd(inputs.labels, indices), 1)
-      weights = tf.gather_nd(inputs.weights, indices)
+      labels = tf.expand_dims(tf.gather_nd(inputs.labels, indices), 1)
-      weights = tf.gather_nd(inputs.weights, indices)
+      lstm_out = tf.expand_dims(tf.gather_nd(lstm_out, indices), 1)
-          logits=tf.squeeze(logits), labels=tf.cast(labels, tf.float32))
+          logits=tf.squeeze(logits, -1), labels=tf.cast(labels, tf.float32))
-      pred = tf.cast(tf.greater(tf.squeeze(logits), 0.5), tf.int64)
+      pred = tf.cast(tf.greater(tf.squeeze(logits, -1), 0.5), tf.int64)
-    acc = layers_lib.accuracy(logits, inputs.labels, inputs.weights)
+    if FLAGS.single_label:
-                layers_lib.predictions(logits), inputs.labels, inputs.weights)
+                layers_lib.predictions(logits), labels, weights)
-    loss = layers_lib.classification_loss(logits, inputs.labels, inputs.weights)
+    loss = layers_lib.classification_loss(logits, labels, weights)
-
+      tf.summary.scalar('total_classification_loss', total_loss)
-    tf.summary.scalar('total_classification_loss', total_loss)
+
-
+      tf.summary.scalar('total_classification_loss', total_loss)
-  """LSTM layer using static_rnn.
+  """LSTM layer using dynamic_rnn.
-      lstm_out, next_state = tf.contrib.rnn.static_rnn(
+      lstm_out, next_state = tf.nn.dynamic_rnn(
-      lstm_out = tf.transpose(lstm_out, [1, 0, 2])
+      # shape(lstm_out) = (batch_size, timesteps, cell_size)
-      # shape(lstm_out) = (timesteps*batch_size, cell_size)
+      lstm_out = tf.stack(lstm_out)
-      labels = tf.expand_dims(labels, -1)
+      labels_reshaped = tf.reshape(labels, [-1])
-          true_classes=labels,
+          true_classes=labels_reshaped,
-          inputs=x,
+          labels=labels_reshaped,
-import graphs
+from adversarial_text import graphs
-  sv = tf.train.Supervisor(logdir=FLAGS.eval_dir, saver=None, summary_op=None)
+  sv = tf.train.Supervisor(
-import layers as layers_lib
+from adversarial_text import adversarial_losses as adv_lib
-    self.global_step = tf.train.get_or_create_global_step()
+    self.global_step = tf.contrib.framework.get_or_create_global_step()
-import graphs
+from adversarial_text import graphs
-import train_utils
+from adversarial_text import graphs
-import train_utils
+from adversarial_text import graphs
-  test_ds = dataset.test(FLAGS.data_dir).batch(10000)
+  test_ds = dataset.test(FLAGS.data_dir).batch(FLAGS.batch_size)
-  """Class that defines a graph to recognize digits in the MNIST dataset.
+  """Model to recognize digits in the MNIST dataset.
-  But written as a tf.keras and tf.layers APIs.
+  But written as a tf.keras.Model using the tf.layers API.
-  """Class that defines a graph to recognize digits in the MNIST dataset."""
+class Model(tf.keras.Model):
-          tf.VarLenFeature(dtype=tf.int64),
+def _parse_example_proto(example_serialized):
-  parsed = tf.parse_single_example(raw_record, keys_to_features)
+  features = tf.parse_single_example(example_serialized, feature_map)
-  image = tf.image.convert_image_dtype(image, dtype=tf.float32)
+def parse_record(raw_record, is_training):
-      dtype=tf.int32)
+  label = tf.cast(tf.reshape(label, shape=[]), dtype=tf.int32)
-  return image, tf.one_hot(label, _NUM_CLASSES)
+  return image, label
-      less than the crop size.
+def _get_h_w(image):
-  """Crops the given list of images.
+  shape = tf.shape(image)
-    image, depths, normals = _random_crop([image, depths, normals], 120, 150)
+def _random_crop_and_flip(image, crop_height, crop_width):
-      varying channel.
+    image: a 3-D image tensor
-    the image_list with cropped images.
+    3-D tensor with cropped image.
-    asserts.extend([height_assert, width_assert])
+  height, width = _get_h_w(image)
-      [], maxval=max_offset_width, dtype=tf.int32)
+  total_crop_height = (height - crop_height)
-                crop_height, crop_width) for image in image_list]
+  cropped = tf.slice(
-def _central_crop(image_list, crop_height, crop_width):
+def _central_crop(image, crop_height, crop_width):
-      varying channel.
+    image: a 3-D image tensor
-    the list of cropped images.
+    3-D tensor with cropped image.
-    image_width = tf.shape(image)[1]
+  height, width = _get_h_w(image)
-  return outputs
+  total_crop_height = (height - crop_height)
-  return tf.concat(axis=2, values=channels)
+  # We have a 1-D tensor of means; convert to 3-D.
-  smallest_side = tf.convert_to_tensor(smallest_side, dtype=tf.int32)
+  smallest_side = tf.cast(smallest_side, tf.float32)
-  smallest_side = tf.to_float(smallest_side)
+  smaller_dim = tf.minimum(height, width)
-  width = shape[1]
+  height, width = _get_h_w(image)
-  return _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])
+  resized_image = tf.image.resize_images(
-                                resize_side_min, resize_side_max)
+    # For training, we want to randomize some of the distortions.
-                               resize_side_min)
+    resize_side = resize_side_min
-               use_display_name=False):
+               use_display_name=False,
-            image_key='image/encoded', format_key='image/format', channels=3),
+        fields.InputDataFields.image:
-            'image/object/area'),
+            slim_example_decoder.BoundingBox(['ymin', 'xmin', 'ymax', 'xmax'],
-      lambda: post_burnin_learning_rate)
+  return tf.where(
-        lambda: learning_rate)
+    learning_rate = tf.where(
-  step_boundaries = tf.constant(boundaries, tf.int64)
+  step_boundaries = tf.constant(boundaries, tf.int32)
-  return tf.reshape(tf.slice(learning_rates, index, [1]), [])
+  index = tf.reduce_min(
-class LearningSchedulesTest(tf.test.TestCase):
+class LearningSchedulesTest(test_case.TestCase):
-    burnin_steps = 2
+    def graph_fn(global_step):
-      self.assertAllClose(output_rates, exp_rates)
+    self.assertAllClose(output_rates, exp_rates, rtol=1e-4)
-    input_global_steps = [0, 4, 8, 9, 100]
+    def graph_fn(global_step):
-      self.assertAllClose(output_rates, exp_rates)
+    input_global_steps = [0, 4, 8, 9, 100]
-    rates = [1.0, 2.0, 3.0, 4.0]
+    def graph_fn(global_step):
-      self.assertAllClose(output_rates, exp_rates)
+    self.assertAllClose(output_rates, exp_rates)
-  for i in xrange(num_boxes):
+  for i in range(num_boxes):
-  for i in xrange(num_masks):
+  for i in range(num_masks):
-
+import logging
-          print 'Writing output image %d to %s' % (i, output_file)
+          logging.info('Writing output image %d to %s', i, output_file)
-    unmatched_cls_target = tf.constant([1] + self.num_classes * [0], tf.float32)
+    unmatched_cls_target = tf.constant([1] + self.num_classes * [0],
-
+    encode_background_as_zeros = False
-        hard_example_miner, add_summaries=False)
+        encode_background_as_zeros, image_resizer_fn, non_max_suppression_fn,
-
+import functools
-          worker_index=0, batch_size=1, max_num_boxes=None, num_classes=None,
+def build(input_reader_config, transform_input_data_fn=None,
-        input_reader_config, num_workers, worker_index)
+        functools.partial(tf.data.TFRecordDataset, buffer_size=8 * 1000 * 1000),
-            worker_index=FLAGS.task)).get_next()
+        dataset_builder.build(config)).get_next()
-    worker_index=0):
+def read_dataset(file_read_func, decode_func, input_files, config):
-  dataset = dataset.repeat(config.num_epochs or None)
+  filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)
-      file_read_func, cycle_length=cycle_length, block_length=1)
+    filename_dataset = filename_dataset.shuffle(
-  return dataset.prefetch(config.prefetch_buffer_size)
+    records_dataset.shuffle(config.shuffle_buffer_size)
-    indicator_matrix = tf.one_hot(indices, index_range)
+    params_shape = shape_utils.combined_static_and_dynamic_shape(params)
-                      indices.shape.concatenate(params.shape[1:]))
+                      tf.stack(indices_shape + params_shape[1:]))
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)
+    crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)
+    crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
-          ps_crop, reduction_indices=(1, 2), keep_dims=True)
+          ps_crop, reduction_indices=(1, 2), keepdims=True)
-    ValueError: if frequency edges are incorrectly ordered.
+    ValueError: if frequency edges are incorrectly ordered or out of range.
-                     (upper_edge_hertz, nyquist_hertz))  
+                     (upper_edge_hertz, nyquist_hertz))
-                     (upper_edge_hertz, nyquist_hertz))
+  if lower_edge_hertz < 0.0:
-from delf import datum_pb2
+from delf import datum_pb2
-import numpy as np
+
-"""DELF model implementation based on the following paper:
+"""DELF model implementation based on the following paper.
-
+from nets import resnet_v1
-from tensorflow.python.platform import app
+import tensorflow as tf
-            descriptors_out, attention_out)
+        feature_io.WriteToFile(out_desc_fullpath, locations_out,
-from delf import feature_io
+import sys
-import sys
+
-  distances, indices = d1_tree.query(
+  _, indices = d1_tree.query(
-      locations_2[i,] for i in range(num_features_2)
+      locations_2[i,]
-      locations_1[indices[i],] for i in range(num_features_2)
+      locations_1[indices[i],]
-  model_robust, inliers = ransac(
+  _, inliers = ransac(
-  fig, ax = plt.subplots()
+  _, ax = plt.subplots()
-        tf.shape(feature_map)[1], tf.shape(feature_map)[2], rf, stride, padding)
+        tf.shape(feature_map)[1],
-              final_boxes.get_field('scores'), 1))
+          final_boxes.get_field('features'),
-from delf import feature_extractor
+from delf import feature_extractor
-        model_fn=_test_model_fn)
+    boxes, feature_scales, features, scores = (
-from delf import datum_io
+from delf import feature_pb2
-  for i in xrange(num_features):
+  for i in range(num_features):
-  for i in xrange(num_features):
+  for i in range(num_features):
-import numpy as np
+
-    """
+
-  if FLAGS.conditional_eval:
+  if FLAGS.conditional_eval and FLAGS.write_to_disk:
-    if FLAGS.num_images_generated >= 100:
+    if FLAGS.num_images_generated >= 100 and FLAGS.write_to_disk:
-    data = generator_fn(generator_inputs)
+    data = generator_fn(generator_inputs, is_training=False)
-def generator(noise):
+def generator(noise, is_training=True):
-  images, _ = dcgan.generator(noise)
+  images, _ = dcgan.generator(noise, is_training=is_training)
-def conditional_generator(inputs):
+def conditional_generator(inputs, is_training=True):
-  images, _ = dcgan.generator(noise)
+  images, _ = dcgan.generator(noise, is_training=is_training)
-def discriminator(img, unused_conditioning):
+def discriminator(img, unused_conditioning, is_training=True):
-  logits, _ = dcgan.discriminator(img)
+  logits, _ = dcgan.discriminator(img, is_training=is_training)
-def conditional_discriminator(img, conditioning):
+def conditional_discriminator(img, conditioning, is_training=True):
-  logits, end_points = dcgan.discriminator(img)
+  logits, end_points = dcgan.discriminator(img, is_training=is_training)
-from six.moves import xrange
+from six.moves import xrange  # pylint: disable=redefined-builtin
-from six.moves import xrange
+flags.DEFINE_boolean('write_to_disk', True, 'If `True`, run images to disk.')
-    images = networks.conditional_generator((noise, one_hot_labels))
+    images = networks.conditional_generator(
-      tf.image.encode_png(data_provider.float_image_to_uint8(reshaped_img[0])))
+  image_write_ops = None
-          tf.random_normal([FLAGS.num_images_generated, FLAGS.noise_dims]))
+          tf.random_normal([FLAGS.num_images_generated, FLAGS.noise_dims]),
-    if FLAGS.num_images_generated >= 100:
+    if FLAGS.num_images_generated >= 100 and FLAGS.write_to_disk:
-  generator_fn = lambda x: networks.infogan_generator(x, len(CAT_SAMPLE_POINTS))
+  def generator_fn(inputs):
-      FLAGS.eval_dir, 'continuous2_infogan.png', reshaped_continuous2_img[0]))
+  if FLAGS.write_to_disk:
-    noise, is_conditional, one_hot_labels, weight_decay):
+    noise, is_conditional, one_hot_labels, weight_decay, is_training):
-def unconditional_generator(noise, weight_decay=2.5e-5):
+    with tf.contrib.framework.arg_scope(
-  return _generator_helper(noise, False, None, weight_decay)
+  return _generator_helper(noise, False, None, weight_decay, is_training)
-def conditional_generator(inputs, weight_decay=2.5e-5):
+def conditional_generator(inputs, weight_decay=2.5e-5, is_training=True):
-  return _generator_helper(noise, True, one_hot_labels, weight_decay)
+  return _generator_helper(
-def infogan_generator(inputs, categorical_dim, weight_decay=2.5e-5):
+def infogan_generator(inputs, categorical_dim, weight_decay=2.5e-5,
-  return _generator_helper(all_noise, False, None, weight_decay)
+  return _generator_helper(all_noise, False, None, weight_decay, is_training)
-from six.moves import xrange
+from six.moves import xrange  # pylint: disable=redefined-builtin
-from six.moves import xrange
+from six.moves import xrange  # pylint: disable=redefined-builtin
-      generator_fn=networks.unconditional_generator,
+      generator_fn=_unconditional_generator,
-T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar (https://arxiv.org/abs/1708.02002)
+"Focal Loss for Dense Object Detection" (https://arxiv.org/abs/1708.02002)
-    return self._aspect_ratios * self._scales_per_octave
+    return len(self._anchor_grid_info) * [
-
+    if not isinstance(im_height, int) or not isinstance(im_width, int):
-        cfg.max_lvl,
+        cfg.min_level,
-        cfg.aspect_ratios,
+        [float(aspect_ratio) for aspect_ratio in cfg.aspect_ratios],
-        class_prediction_bias_init=conv_box_predictor.class_prediction_bias_init
+        class_prediction_bias_init=(conv_box_predictor.
-  """Builds a tf.data.Dataset based on the InputReader config.
+def _get_padding_shapes(dataset, max_num_boxes, num_classes,
-    worker_index: Id for the current worker.
+    transform_input_data_fn: Function to apply to all records, or None if
-        tf.data.TFRecordDataset, decoder.decode, config.input_path[:],
+    def process_fn(value):
-        (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
+  def test_build_tf_record_input_reader_with_batch_size_two(self):
-        output_dict[fields.InputDataFields.groundtruth_boxes][0])
+        [[[0.0, 0.0, 1.0, 1.0],
-        (1, 4, 5),
+        [2, 2, 4, 5],
-    return functools.partial(
+    image_resizer_fn = functools.partial(
-  if image_resizer_config.WhichOneof(
+    if not keep_aspect_ratio_config.convert_to_grayscale:
-    return functools.partial(
+    image_resizer_fn = functools.partial(
-  raise ValueError('Invalid image resizer option.')
+    if not fixed_shape_resizer_config.convert_to_grayscale:
-        force_match_for_each_row=matcher.force_match_for_each_row)
+        force_match_for_each_row=matcher.force_match_for_each_row,
-    return bipartite_matcher.GreedyBipartiteMatcher()
+    matcher = matcher_config.bipartite_matcher
-def build(model_config, is_training):
+def build(model_config, is_training, add_summaries=True):
-    return _build_ssd_model(model_config.ssd, is_training)
+    return _build_ssd_model(model_config.ssd, is_training, add_summaries)
-    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training)
+    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,
-                                 use_explicit_padding)
+                                 use_explicit_padding, use_depthwise)
-def _build_ssd_model(ssd_config, is_training):
+def _build_ssd_model(ssd_config, is_training, add_summaries):
-      hard_example_miner)
+      hard_example_miner,
-def _build_faster_rcnn_model(frcnn_config, is_training):
+def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
-    desired FasterRCNNMetaArch or RFCNMetaArch.
+      desired FasterRCNNMetaArch or RFCNMetaArch.
-      'hard_example_miner': hard_example_miner}
+      'hard_example_miner': hard_example_miner,
-FEATURE_EXTRACTOR_MAPS = {
+FRCNN_RESNET_FEAT_MAPS = {
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
+    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
+    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():
-def build(optimizer_config, global_summaries):
+def build(optimizer_config):
-    An optimizer.
+    An optimizer and a list of variables for summary.
-        _create_learning_rate(config.learning_rate, global_summaries),
+        learning_rate,
-        _create_learning_rate(config.learning_rate, global_summaries),
+        learning_rate,
-        _create_learning_rate(config.learning_rate, global_summaries))
+    learning_rate = _create_learning_rate(config.learning_rate)
-  return optimizer
+  return optimizer, summary_vars
-def _create_learning_rate(learning_rate_config, global_summaries):
+def _create_learning_rate(learning_rate_config):
-    learning_rate = config.learning_rate
+    learning_rate = tf.constant(config.learning_rate, dtype=tf.float32)
-    self.assertAlmostEqual(learning_rate, 0.004)
+        learning_rate_proto)
-        learning_rate_proto, global_summaries)
+        learning_rate_proto)
-        learning_rate_proto, global_summaries)
+        learning_rate_proto)
-        learning_rate_proto, global_summaries)
+        learning_rate_proto)
-          learning_rate_proto, global_summaries)
+      optimizer_builder._create_learning_rate(learning_rate_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
-      optimizer_builder.build(optimizer_proto, global_summaries)
+      optimizer_builder.build(optimizer_proto)
-                           **params)
+                         **params)
-               class_prediction_bias_init=0.0):
+               class_prediction_bias_init=0.0,
-                scope='BoxEncodingPredictor')
+            if self._use_depthwise:
-                    self._class_prediction_bias_init))
+            if self._use_depthwise:
-  predicting from different feature maps.
+  predicting from different feature maps.  Separate multi-layer towers are
-        net = image_feature
+        box_encodings_net = image_feature
-                              scope='conv2d_{}'.format(i))
+            box_encodings_net = slim.conv2d(
-              net, num_predictions_per_location * self._box_code_size,
+              box_encodings_net,
-              net, num_predictions_per_location * num_class_slots,
+              class_predictions_net,
-               tf.random_uniform([4, 32, 32, 3], dtype=tf.float32))
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
-        'BoxPredictor/WeightSharedConvolutionalBoxPredictor/conv2d_1/biases',
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
-
+from object_detection.utils import ops
-  def __init__(self, match_results):
+  def __init__(self, match_results, use_matmul_gather=False):
-        tf.gather(self._match_results, self.matched_column_indices()))
+        self._gather_op(self._match_results, self.matched_column_indices()))
-    gathered_tensor = tf.gather(input_tensor, gather_indices)
+    gathered_tensor = self._gather_op(input_tensor, gather_indices)
-      return Match(self._match(similarity_matrix, **params))
+      return Match(self._match(similarity_matrix, **params),
-                          groundtruth_keypoints_list=None):
+                          groundtruth_keypoints_list=None,
-def _apply_with_random_selector(x, func, num_cases):
+def _apply_with_random_selector(x,
-  rand_sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)
+  generator_func = functools.partial(
-def _apply_with_random_selector_tuples(x, func, num_cases):
+def _apply_with_random_selector_tuples(x,
-  # Pass the real x only to one of the func calls.
+  generator_func = functools.partial(
-                           seed=None):
+                           seed=None,
-    do_a_flip_random = tf.greater(tf.random_uniform([], seed=seed), 0.5)
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
-                         seed=None):
+                         seed=None,
-    do_a_flip_random = tf.greater(tf.random_uniform([], seed=seed), 0.5)
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
-                      seed=None):
+                      seed=None,
-    do_a_rot90_random = tf.greater(tf.random_uniform([], seed=seed), 0.5)
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
-def random_pixel_value_scale(image, minval=0.9, maxval=1.1, seed=None):
+def random_pixel_value_scale(image,
-        seed=seed)
+    generator_func = functools.partial(
-                       seed=None):
+                       seed=None,
-                                  dtype=tf.float32, seed=seed)
+    generator_func = functools.partial(
-def random_rgb_to_gray(image, probability=0.1, seed=None):
+def random_rgb_to_gray(image,
-    image_gray1 = tf.image.rgb_to_grayscale(image)
+    image_gray1 = _rgb_to_grayscale(image)
-    do_gray_random = tf.random_uniform([], seed=seed)
+    # random variable defining whether to change to grayscale or not
-def random_adjust_brightness(image, max_delta=0.2):
+def random_adjust_brightness(image,
-    image = tf.image.random_brightness(image, max_delta)
+    generator_func = functools.partial(tf.random_uniform, [],
-def random_adjust_contrast(image, min_delta=0.8, max_delta=1.25):
+def random_adjust_contrast(image,
-    image = tf.image.random_contrast(image, min_delta, max_delta)
+    generator_func = functools.partial(tf.random_uniform, [],
-def random_adjust_hue(image, max_delta=0.02):
+def random_adjust_hue(image,
-    image = tf.image.random_hue(image, max_delta)
+    generator_func = functools.partial(tf.random_uniform, [],
-def random_adjust_saturation(image, min_delta=0.8, max_delta=1.25):
+def random_adjust_saturation(image,
-    image = tf.image.random_saturation(image, min_delta, max_delta)
+    generator_func = functools.partial(tf.random_uniform, [],
-def random_distort_color(image, color_ordering=0):
+def random_distort_color(image, color_ordering=0, preprocess_vars_cache=None):
-      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
+      image = random_adjust_brightness(
-      image = tf.image.random_hue(image, max_delta=0.2)
+      image = random_adjust_brightness(
-                              overlap_thresh=0.3):
+                              overlap_thresh=0.3,
-    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(
+    generator_func = functools.partial(
-                      seed=None):
+                      seed=None,
-        overlap_thresh=overlap_thresh)
+        overlap_thresh=overlap_thresh,
-    do_a_crop_random = tf.random_uniform([], seed=seed)
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
-                     seed=None):
+                     seed=None,
-
+    preprocess_vars_cache: PreprocessorCache object that records previously
-                          seed=None):
+                          seed=None,
-      seed=seed)
+      seed=seed,
-      seed=seed)
+      seed=seed,
-                                seed=None):
+                                seed=None,
-                               seed=None):
+                               seed=None,
-    scale = tf.random_uniform([], min_scale, max_scale, seed=seed)
+
-                         random_seed=None):
+                         random_seed=None,
-  def add_black_patch_to_image(image):
+  def add_black_patch_to_image(image, idx):
-        [], minval=0.0, maxval=(1.0 - size_to_image_ratio), seed=random_seed)
+
-          [], minval=0.0, maxval=1.0, dtype=tf.float32, seed=random_seed)
+    for idx in range(max_black_patches):
-
+          functools.partial(add_black_patch_to_image, image=image, idx=idx))
-def random_resize_method(image, target_size):
+def random_resize_method(image, target_size, preprocess_vars_cache=None):
-      num_cases=4)
+      num_cases=4,
-  return tf.image.rgb_to_grayscale(image)
+  return _rgb_to_grayscale(image)
-                    seed=None):
+                    seed=None,
-        seed=seed)
+        seed=seed,
-      num_cases=len(min_object_covered))
+      num_cases=len(min_object_covered),
-                        seed=None):
+                        seed=None,
-        seed=seed)
+        seed=seed,
-      num_cases=len(min_object_covered))
+      num_cases=len(min_object_covered),
-    seed=None):
+    seed=None,
-      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed)
+      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed,
-      seed=seed)
+      seed=seed,
-    seed=None):
+    seed=None,
-      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed)
+      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed,
-      seed=seed)
+      seed=seed,
-def preprocess(tensor_dict, preprocess_options, func_arg_map=None):
+def preprocess(tensor_dict,
-                         gt_class_targets_batch):
+                         gt_class_targets_batch,
-      anchors_batch, gt_box_batch, gt_class_targets_batch):
+  if gt_weights_batch is None:
-         anchors, gt_boxes, gt_class_targets)
+         anchors, gt_boxes, gt_class_targets, gt_weights)
-                    ['image/object/mask'], self._decode_png_instance_masks))
+                    ['image/object/mask', 'image/height', 'image/width'],
-        shape [None] indicating the weights of groundtruth boxes.
+    tensor_dict[fields.InputDataFields.num_groundtruth_boxes] = tf.shape(
-    return tf.map_fn(decode_png_mask, png_masks, dtype=tf.float32)
+    return tf.cond(
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    image_tensor = np.random.randint(255, size=(image_height,
+    image_tensor = np.random.randint(256, size=(image_height,
-    image_tensor = np.random.randint(255, size=(image_height,
+    image_tensor = np.random.randint(256, size=(image_height,
-    image_dir: Directory containing the image files.
+    image_dir: directory containing the image files.
-
+  image_id = image['id']
-          object_annotations['segmentation'], image_height, image_width)
+      run_len_encoding = mask.frPyObjects(object_annotations['segmentation'],
-      'image/object/area': dataset_util.float_list_feature(area),
+      'image/height':
-  return example, num_annotations_skipped
+  return key, example, num_annotations_skipped
-      tf_example, num_annotations_skipped = create_tf_example(
+      _, tf_example, num_annotations_skipped = create_tf_example(
-    ]
+    annotations_list = [{
-        3: {'name': 'human', 'id': 3}
+        1: {
-        image, annotations_list, image_dir, category_index)
+    (_, example,
-        [image_file_name])
+        [str(image['id'])])
-    ]
+    annotations_list = [{
-        1: {'name': 'dog', 'id': 1},
+        1: {
-        image, annotations_list, image_dir, category_index, include_masks=True)
+    (_, example,
-        [image_file_name])
+        [str(image['id'])])
-                 for encoded_mask_png in encoded_mask_pngs]
+    encoded_mask_pngs = [
-                         [0, 0, 0, 0, 1, 1, 1, 1]])
+                        [[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0],
-        object_detection_evaluation.OpenImagesDetectionEvaluator
+        object_detection_evaluation.OpenImagesDetectionEvaluator,
-    temp_checkpoint_file = tempfile.NamedTemporaryFile()
+    # This check is to be compatible with both version of SaverDef.
-    checkpoint_to_use = temp_checkpoint_file.name
+        temp_checkpoint_prefix)
-  saver = tf.train.Saver()
+  saver = tf.train.Saver(**saver_kwargs)
-from object_detection import trainer
+from object_detection.builders import image_resizer_builder
-from object_detection.core import prefetcher
+from object_detection.core import preprocessor
-FEATURES_KEY = 'key'
+HASH_KEY = 'hash'
-def create_train_input_fn(num_classes, train_config, train_input_config):
+def transform_input_data(tensor_dict,
-      category.
+    model_config: A model_pb2.DetectionModel.
-  def _train_input_fn():
+  def _train_input_fn(params=None):
-          unique identifier for the image.
+        features[fields.InputDataFields.image] is a [batch_size, H, W, C]
-          boxes.
+        labels[fields.InputDataFields.num_groundtruth_boxes] is a [batch_size]
-    create_tensor_dict_fn = functools.partial(get_next, train_input_config)
+    if not isinstance(model_config, model_pb2.DetectionModel):
-
+    data_augmentation_fn = functools.partial(
-        FEATURES_KEY: list(image_keys)
+        fields.InputDataFields.image: tensor_dict[fields.InputDataFields.image],
-        'classes_list': list(classes_tuple)
+        fields.InputDataFields.num_groundtruth_boxes: tensor_dict[
-      labels[fields.InputDataFields.groundtruth_weights] = list(weights_tuple)
+    if fields.InputDataFields.groundtruth_keypoints in tensor_dict:
-def create_eval_input_fn(num_classes, eval_config, eval_input_config):
+def create_eval_input_fn(eval_config, eval_input_config, model_config):
-      category.
+    model_config: A model_pb2.DetectionModel.
-  def _eval_input_fn():
+  def _eval_input_fn(params=None):
-          the image.
+        features[fields.InputDataFields.image] is a [1, H, W, C] float32 tensor
-          indicating if the boxes represent `difficult` instances.
+        labels[fields.InputDataFields.groundtruth_boxes] is a [1, num_boxes, 4]
-                      'eval_pb2.EvalConfig.')
+                      'train_pb2.EvalConfig.')
-    ]
+    labels = {
-      ]
+      labels[fields.InputDataFields.groundtruth_instance_masks] = input_dict[
-def create_predict_input_fn():
+def create_predict_input_fn(model_config):
-  def _predict_input_fn():
+  def _predict_input_fn(params=None):
-    decoder = tf_example_decoder.TfExampleDecoder(load_instance_masks=False)
+    num_classes = config_util.get_number_of_classes(model_config)
-    input_dict = decoder.decode(example)
+    transform_fn = functools.partial(
-        features={FEATURES_IMAGE: images},
+        features={fields.InputDataFields.image: images},
-    batch_size = configs['train_config'].batch_size
+    configs['train_config'].unpad_groundtruth_tensors = True
-        classes, configs['train_config'], configs['train_input_config'])
+        configs['train_config'], configs['train_input_config'], model_config)
-    self._assert_training_inputs(features, labels, classes, batch_size)
+
-                                                configs['eval_input_config'])
+    model_config = configs['model']
-    self._assert_eval_inputs(features, labels, classes)
+
-    classes = 37
+    model_config = configs['model']
-        classes, configs['train_config'], configs['train_input_config'])
+        configs['train_config'], configs['train_input_config'], model_config)
-    self._assert_training_inputs(features, labels, classes, batch_size)
+
-                                                configs['eval_input_config'])
+    model_config = configs['model']
-    self._assert_eval_inputs(features, labels, classes)
+
-    predict_input_fn = inputs.create_predict_input_fn()
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
-    image = serving_input_receiver.features['images']
+    image = serving_input_receiver.features[fields.InputDataFields.image]
-    self.assertEqual([1, None, None, 3], image.shape.as_list())
+        inputs.SERVING_FED_EXAMPLE_KEY]
-    classes = 37
+    configs['model'].ssd.num_classes = 37
-        train_input_config=configs['train_input_config'])
+        train_input_config=configs['train_input_config'],
-    classes = 37
+    configs['model'].ssd.num_classes = 37
-        train_input_config=configs['model'])  # Expecting `InputReader`.
+        train_input_config=configs['train_input_config'],
-    classes = 37
+    configs['model'].ssd.num_classes = 37
-        eval_input_config=configs['eval_input_config'])
+        eval_input_config=configs['eval_input_config'],
-    classes = 37
+    configs['model'].ssd.num_classes = 37
-        eval_input_config=configs['model'])  # Expecting `InputReader`.
+        eval_input_config=configs['model'],  # Expecting `InputReader`.
-               force_match_for_each_row=False):
+               force_match_for_each_row=False,
-               parallel_iterations=16):
+               parallel_iterations=16,
-               parallel_iterations=16):
+               parallel_iterations=16,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
-    TODO: group NMS parameters + score converter into
+    TODO(rathodv,jonathanhuang): group NMS parameters + score converter into
-    image_shape = tf.shape(preprocessed_inputs)
+    image_shape = shape_utils.combined_static_and_dynamic_shape(
-           keypoints)
+           keypoints, weights)
-                      groundtruth_keypoints_list=None):
+                      groundtruth_keypoints_list=None,
-        groundtruth_classes_with_background_list)
+        groundtruth_classes_with_background_list, groundtruth_weights_list)
-  def __init__(self, categories, all_metrics_per_category=False):
+  def __init__(self,
-      2. per_category_ap: category specific results with keys of the form:
+      2. per_category_ap: if include_metrics_per_category is True, category
-                          category_dict['name'])
+    if self._include_metrics_per_category:
-  def __init__(self, categories):
+  def __init__(self, categories, include_metrics_per_category=False):
-      2. per_category_ap: category specific results with keys of the form:
+      2. per_category_ap: if include_metrics_per_category is True, category
-    mask_metrics, mask_per_category_ap = mask_evaluator.ComputeMetrics()
+    mask_metrics, mask_per_category_ap = mask_evaluator.ComputeMetrics(
-"""Tests for image.understanding.object_detection.metrics.coco_evaluation."""
+"""Tests for tensorflow_models.object_detection.metrics.coco_evaluation."""
-  def ComputeMetrics(self, all_metrics_per_category=False):
+  def ComputeMetrics(self,
-"""Tests for google3.image.understanding.object_detection.metrics.coco_tools."""
+"""Tests for tensorflow_model.object_detection.metrics.coco_tools."""
-               use_explicit_padding=False):
+               use_explicit_padding=False,
-        use_explicit_padding)
+        use_explicit_padding, use_depthwise)
-               use_explicit_padding=False):
+               use_explicit_padding=False,
-        use_explicit_padding)
+        use_explicit_padding, use_depthwise)
-               use_explicit_padding=False):
+               use_explicit_padding=False,
-        use_explicit_padding)
+        use_explicit_padding, use_depthwise)
-               use_explicit_padding=False):
+               use_explicit_padding=False,
-        use_explicit_padding)
+        use_explicit_padding, use_depthwise)
-               use_explicit_padding=False):
+               use_explicit_padding=False,
-      resnet_scope_name: scope name to construct resnet
+      resnet_scope_name: scope name under which to construct resnet
-               use_explicit_padding=False):
+               use_explicit_padding=False,
-        features. Default is False.
+        features. Default is False. UNUSED currently.
-        conv_hyperparams, resnet_v1.resnet_v1_50, 'resnet_v1_50_fpn',
+        conv_hyperparams, resnet_v1.resnet_v1_50, 'resnet_v1_50', 'fpn',
-               use_explicit_padding=False):
+               use_explicit_padding=False,
-        features. Default is False.
+        features. Default is False. UNUSED currently.
-        conv_hyperparams, resnet_v1.resnet_v1_101, 'resnet_v1_101_fpn',
+        conv_hyperparams, resnet_v1.resnet_v1_101, 'resnet_v1_101', 'fpn',
-               use_explicit_padding=False):
+               use_explicit_padding=False,
-        features. Default is False.
+        features. Default is False. UNUSED currently.
-        conv_hyperparams, resnet_v1.resnet_v1_152, 'resnet_v1_152_fpn',
+        conv_hyperparams, resnet_v1.resnet_v1_152, 'resnet_v1_152', 'fpn',
-    SSDResnetFeatureExtractorTestBase):
+    SSDResnetFPNFeatureExtractorTestBase):
-    return 'resnet_v1_50_fpn'
+  def _resnet_scope_name(self):
-    SSDResnetFeatureExtractorTestBase):
+    SSDResnetFPNFeatureExtractorTestBase):
-    return 'resnet_v1_101_fpn'
+  def _resnet_scope_name(self):
-    SSDResnetFeatureExtractorTestBase):
+    SSDResnetFPNFeatureExtractorTestBase):
-    return 'resnet_v1_152_fpn'
+  def _resnet_scope_name(self):
-class SSDResnetFeatureExtractorTestBase(
+class SSDResnetFPNFeatureExtractorTestBase(
-  def _scope_name(self):
+  def _resnet_scope_name(self):
-        depth_multiplier, pad_to_multiple, self._scope_name())
+    g = tf.Graph()
-                                                   global_summaries)
+      training_optimizer, optimizer_summary_vars = optimizer_builder.build(
-      ) / float(total_steps - warmup_steps)))
+      1 + tf.cos(np.pi * (tf.cast(global_step, tf.float32) - warmup_steps
-    self.corloc_per_class = np.ones(self.num_class, dtype=float)
+    self._initialize_detections()
-    raise ValueError('`indices` must have rank 1')
+
-  saver = tf.train.Saver(vggish_vars, name='vggish_load_pretrained')
+  saver = tf.train.Saver(vggish_vars, name='vggish_load_pretrained',
-import pickle
+from six.moves import cPickle as pickle
-    data_dict = pickle.load(f, encoding='latin1')
+    data_dict = pickle.load(f)
-  
+
-        enables users to extend the same model to different datasets.
+        enables users to extend the same model to their own datasets.
-        enables users to extend the same model to different datasets.
+        enables users to extend the same model to their own datasets.
-        0.0, shape=[batch_size, _HEIGHT, _WIDTH, _NUM_CHANNELS])
+    fake_input = tf.random_uniform([batch_size, _HEIGHT, _WIDTH, _NUM_CHANNELS])
-        0.0, shape=[batch_size, 224, 224, 3])
+    fake_input = tf.random_uniform([batch_size, 224, 224, 3])
-    """These are the parameters that work for CIFAR-10 data."""
+    """These are the parameters that work for CIFAR-10 data.
-      for _ in range(1024):
+    for i in range(_NUM_CHANNELS):
-    self.assertEqual(image.get_shape().as_list(), [32, 32, 3])
+    self.assertAllEqual(label.shape, (10,))
-    features = tf.random_uniform([_BATCH_SIZE, 32, 32, 3])
+    features = tf.random_uniform([_BATCH_SIZE, _HEIGHT, _WIDTH, _NUM_CHANNELS])
-    """These are the parameters that work for Imagenet data."""
+    """These are the parameters that work for Imagenet data.
-    """
+
-        num_classes=_NUM_CLASSES,
+        num_classes=num_classes,
-    """
+
-        num_classes=_NUM_CLASSES,
+        num_classes=num_classes,
-import cPickle
+import pickle
-  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
+  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
-    data_dict = cPickle.load(f)
+  with tf.gfile.Open(filename, 'rb') as f:
-        tf.contrib.cluster_resolver.python.training.TPUClusterResolver(
+        tf.contrib.cluster_resolver.TPUClusterResolver(
-tf.flags.DEFINE_string("master", "local", "GRPC URL of the Cloud TPU instance.")
+  if FLAGS.master is None and FLAGS.tpu_name is None:
-      evaluation_master=FLAGS.master,
+      master=tpu_grpc_url,
-    features, labels = wide_deep.input_fn(self.input_csv, 1, False, 1)
+    dataset = wide_deep.input_fn(self.input_csv, 1, False, 1)
-def parse_record(raw_record):
+def parse_record(raw_record, is_training):
-  depth_major = tf.reshape(record_vector[label_bytes:record_bytes],
+  depth_major = tf.reshape(record_vector[1:_RECORD_BYTES],
-def input_fn(is_training, data_dir, batch_size, num_epochs=1):
+def input_fn(is_training, data_dir, batch_size, num_epochs=1,
-    A tuple of images and labels.
+    A dataset that can be used for iteration.
-  images, labels = iterator.get_next()
+  filenames = get_filenames(is_training, data_dir)
-  return images, labels
+  return resnet.process_record_dataset(dataset, is_training, batch_size,
-    fake_dataset = fake_dataset.map(cifar10_main.parse_record)
+    fake_dataset = tf.data.FixedLengthRecordDataset(
-          self.assertAllEqual(pixel, np.array([0, 1, 2]))
+          self.assertAllClose(pixel, np.array([-1.225, 0., 1.225]), rtol=1e-3)
-_FILE_SHUFFLE_BUFFER = 1024
+_NUM_TRAIN_FILES = 1024
-def filenames(is_training, data_dir):
+def get_filenames(is_training, data_dir):
-        for i in range(1024)]
+        for i in range(_NUM_TRAIN_FILES)]
-      filenames(is_training, data_dir))
+def input_fn(is_training, data_dir, batch_size, num_epochs=1,
-    dataset = dataset.shuffle(buffer_size=_FILE_SHUFFLE_BUFFER)
+    # Shuffle the input files
-  return images, labels
+  return resnet.process_record_dataset(dataset, is_training, batch_size,
-        hooks=[logging_hook])
+
-        False, flags.data_dir, flags.batch_size))
+    def input_fn_eval():
-    
+
-    return (images, labels)
+    return ds
-  return features, labels
+  return dataset
-    #                  initial_state=self._initial_state)
+    #                                   initial_state=self._initial_state)
-    # Simplified version of tensorflow_models/tutorials/rnn/rnn.py's rnn().
+    # Simplified version of tf.nn.static_rnn().
-    # In general, use the rnn() or state_saving_rnn() from rnn.py.
+    # In general, use tf.nn.static_rnn() or tf.nn.static_state_saving_rnn().
-    #                            initial_state=self._initial_state)
+    # inputs = tf.unstack(inputs, num=self.num_steps, axis=1)
-  with tf.gfile.FastGFile(file_path, 'r') as f:
+  with tf.gfile.FastGFile(file_path, 'rb') as f:
-  with tf.gfile.FastGFile(file_path, 'r') as f:
+  with tf.gfile.FastGFile(file_path, 'rb') as f:
-  variables_to_ignore_patterns = filter(None, filter_regex_list)
+  variables_to_ignore_patterns = list(filter(None, filter_regex_list))
-import resnet_shared
+import resnet
-
+class Cifar10Model(resnet.Model):
-        block_fn=resnet_model.building_block,
+        block_fn=resnet.building_block,
-  learning_rate_fn = resnet_shared.learning_rate_with_decay(
+  learning_rate_fn = resnet.learning_rate_with_decay(
-                                       loss_filter_fn=loss_filter_fn)
+  return resnet.resnet_model_fn(features, labels, mode, Cifar10Model,
-  resnet_shared.resnet_main(FLAGS, cifar10_model_fn, input_fn)
+  resnet.resnet_main(FLAGS, cifar10_model_fn, input_fn)
-  parser = resnet_shared.ResnetArgParser()
+  parser = resnet.ResnetArgParser()
-import resnet_shared
+import resnet
-class ImagenetModel(resnet_model.Model):
+class ImagenetModel(resnet.Model):
-      block_fn = resnet_model.building_block
+      block_fn = resnet.building_block
-      block_fn = resnet_model.bottleneck_block
+      block_fn = resnet.bottleneck_block
-  learning_rate_fn = resnet_shared.learning_rate_with_decay(
+  learning_rate_fn = resnet.learning_rate_with_decay(
-                                       loss_filter_fn=None)
+  return resnet.resnet_model_fn(features, labels, mode, ImagenetModel,
-  resnet_shared.resnet_main(FLAGS, imagenet_model_fn, input_fn)
+  resnet.resnet_main(FLAGS, imagenet_model_fn, input_fn)
-  parser = resnet_shared.ResnetArgParser(
+  parser = resnet.ResnetArgParser(
-"""Contains definitions for the preactivation form of Residual Networks.
+"""Contains definitions for the preactivation form of Residual Networks
-      if false, b will be an appropriately sized, non-trainable vector
+      if false, b will be None
-  bname = (name + "/b") if name else "/b"
+  b = None
-                          collections=b_collections, trainable=trainable)
+                          collections=b_collections,
-                          collections=b_collections, trainable=False)
+                      collections=b_collections,
-                  provided for dataset:", name)
+            print("Initializing trainable readin matrix with alignment matrix" \
-                  provided for dataset:", name)
+            print("Setting non-trainable readin matrix to alignment matrix" \
-                provided for dataset:", name)
+          print("Initializing trainable readin bias with alignment bias " \
-                provided for dataset:", name)
+          print("Setting non-trainable readin bias to alignment bias " \
-          # only add to IO transformations collection only if we want it to be learnable, because IO_transformations collection will be trained when do_train_io_only
+          # only add to IO transformations collection only if we want it to be
-                          initializer=tf.zeros_initializer(), trainable=False)
+                          initializer=tf.zeros_initializer(),
-            print("Initializing trainable readin matrix with alignment matrix provided for dataset:", name)
+            print("Initializing trainable readin matrix with alignment matrix \
-            print("Setting non-trainable readin matrix to alignment matrix provided for dataset:", name)
+            print("Setting non-trainable readin matrix to alignment matrix \
-          print("Initializing trainable readin bias with alignment bias provided for dataset:", name)
+          print("Initializing trainable readin bias with alignment bias \
-          print("Setting non-trainable readin bias to alignment bias provided for dataset:", name)
+          print("Setting non-trainable readin bias to alignment bias \
-flags.DEFINE_float("prior_ar_atau",  PRIOR_AR_AUTOCORRELATION, 
+flags.DEFINE_float("prior_ar_atau",  PRIOR_AR_AUTOCORRELATION,
-flags.DEFINE_float("prior_ar_nvar", PRIOR_AR_PROCESS_VAR, 
+flags.DEFINE_float("prior_ar_nvar", PRIOR_AR_PROCESS_VAR,
-                     DO_FEED_FACTORS_TO_CONTROLLER, 
+flags.DEFINE_boolean("do_feed_factors_to_controller",
-flags.DEFINE_integer("controller_input_lag", CONTROLLER_INPUT_LAG, 
+flags.DEFINE_integer("controller_input_lag", CONTROLLER_INPUT_LAG,
-flags.DEFINE_boolean("do_train_readin", DO_TRAIN_READIN, "Whether to train the readin matrices and bias vectors. False leaves them fixed at their initial values specified by the alignment matrices / vectors.")
+# neurons to input factors which are fed into the shared encoder. These are
-      print("Are you sure you sure a checkpoint in ", hps.lfads_save_dir, " exists?")
+      # cant print ckpt.model_check_point path if no ckpt
-  Args: 
+  Args:
-    do_bias (optional): Add a (learnable) bias vector to the operation, 
+    do_bias (optional): Add a (learnable) bias vector to the operation,
-      w = tf.Variable(mat_init_value, name=wname, collections=w_collections, trainable=trainable)
+      w = tf.Variable(mat_init_value, name=wname, collections=w_collections,
-      w = tf.Variable(mat_init_value, name=wname, collections=w_collections, trainable=trainable)
+      w = tf.Variable(mat_init_value, name=wname, collections=w_collections,
-        print("Using alignment matrix provided for dataset:", name)
+        if hps.do_train_readin:
-        print("Using alignment bias provided for dataset:", name)
+        if hps.do_train_readin:
-      in_fac_lin = init_linear(data_dim, used_in_factors_dim, do_bias=True,
+      if hps.do_train_readin:
-                               collections=['IO_transformations'])
+                               collections=collections_readin,
-          out_mat_fxc = np.linalg.pinv(in_mat_cxf)
+            out_mat_fxc = in_mat_cxf.T
-PS_NEXAMPLES_TO_PROCESS = 1e8 # if larger than number of examples, process all
+PS_NEXAMPLES_TO_PROCESS = MAX_INT # if larger than number of examples, process all
-NUM_STEPS_FOR_GEN_IC = np.inf # set to num_steps if greater than num_steps
+NUM_STEPS_FOR_GEN_IC = MAX_INT # set to num_steps if greater than num_steps
-      print("Are you sure you sure ", ckpt.model_checkpoint_path, " exists?")
+      # cant print ckpt.model_check_point path if no ckpt 
-  model = build_model(hps, kind="write_model_params", datasets=datasets) 
+  model = build_model(hps, kind="write_model_params", datasets=datasets)
-                normalized=False, name=None, collections=None):
+                normalized=False, name=None, collections=None, trainable=True):
-    do_bias (optional): Add a learnable bias vector to the operation.
+    do_bias (optional): Add a (learnable) bias vector to the operation, 
-      w = tf.Variable(mat_init_value, name=wname, collections=w_collections)
+      w = tf.Variable(mat_init_value, name=wname, collections=w_collections, trainable=trainable)
-                          collections=w_collections)
+                          collections=w_collections, trainable=trainable)
-      w = tf.Variable(mat_init_value, name=wname, collections=w_collections)
+      w = tf.Variable(mat_init_value, name=wname, collections=w_collections, trainable=trainable)
-                          collections=w_collections)
+                          collections=w_collections, trainable=trainable)
-  b = None
+  b_collections = [tf.GraphKeys.GLOBAL_VARIABLES]
-                          collections=b_collections)
+                          collections=b_collections, trainable=trainable)
-
+                      collections=b_collections, trainable=trainable)
-T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar
+T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar (https://arxiv.org/abs/1708.02002)
-  """Generate a grid of anchors for multiple CNN layers."""
+  """Generate a grid of anchors for multiple CNN layers of different scale."""
-  TODO: Change function name to FilterScoresGreaterThan
+  TODO: Change function name to filter_scores_greater_than
-      return self._predict(image_features, num_predictions_per_location,
+    return self._predict(image_features, num_predictions_per_location,
-and `postprocess` should be stateless.
+and `postprocess` should be reentrant.
-      for idx in xrange(num_shards)
+      for idx in range(num_shards)
-          EVAL_METRICS_CLASS_DICT[eval_metric_fn_key](categories=categories))
+    evaluators_list.append(
-    TODO(rathodv,jonathanhuang): group NMS parameters + score converter into
+    TODO: group NMS parameters + score converter into
-"""Wrappers for third party pycocotools to be used within i/u/object_detection.
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-         'automatically based on whether TensorFlow was built for CPU or GPU.')
+import resnet_shared
-_DEPTH = 3
+_NUM_CHANNELS = 3
-
+###############################################################################
-  record_bytes = _HEIGHT * _WIDTH * _DEPTH + 1
+  record_bytes = _DEFAULT_IMAGE_BYTES + 1
-  record_bytes = label_bytes + image_bytes
+  record_bytes = label_bytes + _DEFAULT_IMAGE_BYTES
-      record_vector[label_bytes:record_bytes], [_DEPTH, _HEIGHT, _WIDTH])
+  depth_major = tf.reshape(record_vector[label_bytes:record_bytes],
-    image = tf.random_crop(image, [_HEIGHT, _WIDTH, _DEPTH])
+    image = tf.random_crop(image, [_HEIGHT, _WIDTH, _NUM_CHANNELS])
-  metrics = {'accuracy': accuracy}
+###############################################################################
-      eval_metric_ops=metrics)
+def cifar10_model_fn(features, labels, mode, params):
-    print(eval_results)
+  resnet_shared.resnet_main(FLAGS, cifar10_model_fn, input_fn)
-import argparse
+import resnet_shared
-_WEIGHT_DECAY = 1e-4
+_NUM_CLASSES = 1001
-def record_parser(value, is_training):
+def parse_record(raw_record, is_training):
-  parsed = tf.parse_single_example(value, keys_to_features)
+  parsed = tf.parse_single_example(raw_record, keys_to_features)
-  return image, tf.one_hot(label, _LABEL_CLASSES)
+  return image, tf.one_hot(label, _NUM_CLASSES)
-  dataset = tf.data.Dataset.from_tensor_slices(filenames(is_training, data_dir))
+  dataset = tf.data.Dataset.from_tensor_slices(
-  dataset = dataset.map(lambda value: record_parser(value, is_training),
+  dataset = dataset.map(lambda value: parse_record(value, is_training),
-      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')
+###############################################################################
-    train_op = None
+  try:
-  tf.summary.scalar('train_accuracy', accuracy[1])
+def imagenet_model_fn(features, labels, mode, params):
-      eval_metric_ops=metrics)
+  return resnet_shared.resnet_model_fn(features, labels, mode, ImagenetModel,
-    print(eval_results)
+  resnet_shared.resnet_main(FLAGS, imagenet_model_fn, input_fn)
-      """Returns the expected dimensions depending on if a GPU is being used."""
+      """Returns the expected dimensions depending on if a
-          resnet_size, 456,
+      model = imagenet_main.ImagenetModel(
-      output = model(inputs, is_training=True)
+      output = model(inputs, training=True)
-      self.assertAllEqual(output.shape, (1, 456))
+      self.assertAllEqual(dense.shape, (1, _LABEL_CLASSES))
-    spec = imagenet_main.resnet_model_fn(
+    spec = imagenet_main.imagenet_model_fn(
-def batch_norm_relu(inputs, is_training, data_format):
+def batch_norm_relu(inputs, training, data_format):
-      scale=True, training=is_training, fused=True)
+      scale=True, training=training, fused=True)
-def building_block(inputs, filters, is_training, projection_shortcut, strides,
+def building_block(inputs, filters, training, projection_shortcut, strides,
-    is_training: A Boolean for whether the model is in training or inference
+    training: A Boolean for whether the model is in training or inference
-      a 1x1 convolution when downsampling the input).
+    projection_shortcut: The function to use for projection shortcuts
-  inputs = batch_norm_relu(inputs, is_training, data_format)
+  inputs = batch_norm_relu(inputs, training, data_format)
-  inputs = batch_norm_relu(inputs, is_training, data_format)
+  inputs = batch_norm_relu(inputs, training, data_format)
-def bottleneck_block(inputs, filters, is_training, projection_shortcut,
+def bottleneck_block(inputs, filters, training, projection_shortcut,
-    is_training: A Boolean for whether the model is in training or inference
+    filters: The number of filters for the first two convolutions. Note
-      a 1x1 convolution when downsampling the input).
+    projection_shortcut: The function to use for projection shortcuts
-  inputs = batch_norm_relu(inputs, is_training, data_format)
+  inputs = batch_norm_relu(inputs, training, data_format)
-  inputs = batch_norm_relu(inputs, is_training, data_format)
+  inputs = batch_norm_relu(inputs, training, data_format)
-  inputs = batch_norm_relu(inputs, is_training, data_format)
+  inputs = batch_norm_relu(inputs, training, data_format)
-def block_layer(inputs, filters, block_fn, blocks, strides, is_training, name,
+def block_layer(inputs, filters, block_fn, blocks, strides, training, name,
-    is_training: Either True or False, whether we are currently training the
+    training: Either True or False, whether we are currently training the
-  inputs = block_fn(inputs, filters, is_training, projection_shortcut, strides,
+  inputs = block_fn(inputs, filters, training, projection_shortcut, strides,
-    inputs = block_fn(inputs, filters, is_training, None, 1, data_format)
+    inputs = block_fn(inputs, filters, training, None, 1, data_format)
-    ValueError: If `resnet_size` is invalid.
+class Model(object):
-    if data_format == 'channels_first':
+  def __init__(self, resnet_size, num_classes, num_filters, kernel_size,
-        data_format=data_format)
+        inputs=inputs, filters=self.num_filters, kernel_size=self.kernel_size,
-    inputs = batch_norm_relu(inputs, is_training, data_format)
+    if self.first_pool_size:
-        data_format=data_format)
+        inputs=inputs, pool_size=self.second_pool_size,
-    inputs = tf.layers.dense(inputs=inputs, units=num_classes)
+    inputs = tf.reshape(inputs, [-1, self.final_size])
-      params['block'], params['layers'], num_classes, data_format)
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-    return dataset.make_one_shot_iterator().get_next()
+    # Return the dataset.
-    return dataset.make_one_shot_iterator().get_next()
+    # Return the dataset.
-    return dataset.make_one_shot_iterator().get_next()
+    # Return the dataset.
-SPECIES = ['Sentosa', 'Versicolor', 'Virginica']
+SPECIES = ['Setosa', 'Versicolor', 'Virginica']
-                        (length-2 float32 list, default=[256, 256])
+                        (length-2 float32 list or tensor, default=[256, 256])
-                     grid positions (length-2 float32 list, default=[16, 16])
+                     grid positions (length-2 float32 list or tensor,
-                     padding is used (length-2 float32 tensor, default=[0, 0])
+                     padding is used (length-2 float32 list or tensor,
-    base_anchor_size = tf.constant(base_anchor_size, tf.float32)
+    base_anchor_size = tf.to_float(tf.convert_to_tensor(base_anchor_size))
-    anchor_stride = tf.constant(anchor_stride, dtype=tf.float32)
+    anchor_stride = tf.to_float(tf.convert_to_tensor(anchor_stride))
-    anchor_offset = tf.constant(anchor_offset, dtype=tf.float32)
+    anchor_offset = tf.to_float(tf.convert_to_tensor(anchor_offset))
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes: a BoxList holding a collection of N anchor boxes.  Additionally
-                        self._anchor_offset)
+    anchors = tile_anchors(grid_height,
-
+import numpy as np
-class GridAnchorGeneratorTest(tf.test.TestCase):
+class GridAnchorGeneratorTest(test_case.TestCase):
-    anchor_offset = [7, -3]
+    def graph_fn():
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
-    aspect_ratios = [1.0]
+    def graph_fn():
-        anchor_offset=anchor_offset)
+  def test_construct_anchor_grid_with_dynamic_feature_map_shapes(self):
-    anchor_corners = anchors.get()
+      anchors = anchor_generator.generate(
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes: a BoxList holding a collection of N anchor boxes.  Additionally
-              anchor_offset=offset))
+    for feature_map_index, (
-      # TODO(jonathanhuang): make reshape an option for the clip_to_window op
+      # TODO: make reshape an option for the clip_to_window op
-class MultipleGridAnchorGeneratorTest(tf.test.TestCase):
+class MultipleGridAnchorGeneratorTest(test_case.TestCase):
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
-    box_specs_list = [[(0.5, 1.0), (1.0, 1.0), (2.0, 1.0)]]
+    def graph_fn():
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
-    box_specs_list = [[(1.0, 1.0)]]
+
-    anchor_corners = anchors.get()
+  def test_construct_dynamic_size_anchor_grid(self):
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    def graph_fn(height, width):
-    box_specs_list = [[(1.0, 1.0)]]
+    exp_anchor_corners = [[0., -0.25, 1., 0.75], [0., 0.25, 1., 1.25]]
-    exp_anchor_corners = [[0., 0., 1., 0.5], [0., 0.5, 1., 1.]]
+    anchor_corners_out = self.execute_cpu(graph_fn,
-    anchor_corners = anchors.get()
+  def test_construct_anchor_grid_normalized(self):
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    exp_anchor_corners = [[0., 0., 1., 0.5], [0., 0.5, 1., 1.]]
-                      [(1.0, 1.0), (1.0, 0.5)]]
+    def graph_fn():
-      self.assertAllClose(big_grid_corners, exp_big_grid_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
-                      [(1.0, 1.0), (1.0, 0.5)]]
+    def graph_fn():
-      self.assertAllClose(small_grid_corners, exp_small_grid_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
-class CreateSSDAnchorsTest(tf.test.TestCase):
+class CreateSSDAnchorsTest(test_case.TestCase):
-      self.assertEquals(anchor_corners_out.shape, (11640, 4))
+
-                                    mask_prediction_conv_depth),
+        mask_height=mask_rcnn_box_predictor.mask_height,
-        method=method)
+        method=method,
-    resized_images = image_resizer_fn(images)
+    resized_images, _ = image_resizer_fn(images)
-  def test_built_keep_aspect_ratio_resizer_returns_expected_shape(self):
+  def test_build_keep_aspect_ratio_resizer_returns_expected_shape(self):
-    resized_image = image_resizer_fn(image_placeholder)
+    resized_image, _ = image_resizer_fn(image_placeholder)
-        anchorwise_output=config.anchorwise_output)
+    return losses.WeightedSigmoidClassificationLoss()
-        anchorwise_output=config.anchorwise_output)
+        logit_scale=config.logit_scale)
-      anchorwise_output=True)
+      logit_scale=config.logit_scale)
-        anchorwise_output=config.anchorwise_output)
+    return losses.WeightedL2LocalizationLoss()
-        anchorwise_output=config.anchorwise_output)
+    return losses.WeightedSmoothL1LocalizationLoss()
-        anchorwise_output=config.anchorwise_output)
+    return losses.WeightedSigmoidClassificationLoss()
-        anchorwise_output=config.anchorwise_output)
+        bootstrap_type=('hard' if config.hard_bootstrap else 'soft'))
-    self.assertEqual(loss.shape, [1, 2])
+    self.assertEqual(loss.shape, [1, 2, 3])
-                                 batch_norm_trainable, reuse_weights)
+                                 batch_norm_trainable, reuse_weights,
-  first_stage_only = frcnn_config.first_stage_only
+  number_of_stages = frcnn_config.number_of_stages
-      'first_stage_only': first_stage_only,
+      'number_of_stages': number_of_stages,
-    # TODO(rathodv): Find a way to not depend on the private members.
+    # TODO: Find a way to not depend on the private members.
-        multiple_grid_anchor_generator) constructor arguments.
+    TODO: remove **params from argument list and make stride and
-    pruned_corners: a tensor with shape [M_out, 4] where M_out <= M_in
+    pruned_boxlist: a new BoxList with all bounding boxes partially or fully in
-  TODO: Change function name to filter_scores_greater_than
+  TODO: Change function name to FilterScoresGreaterThan
-              **params):
+  def predict(self, image_features, num_predictions_per_location,
-        predictions to be made per spatial location in the feature map.
+      image_features: A list of float tensors of shape [batch_size, height_i,
-    with tf.variable_scope(scope):
+    if len(image_features) != len(num_predictions_per_location):
-        predictions to be made per spatial location in the feature map.
+      image_features: A list of float tensors of shape [batch_size, height_i,
-        Currently, this must be set to 1, or an error will be raised.
+      image_features: A list of float tensors of shape [batch_size, height_i,
-        [batch_size, 1, num_classes, code_size] representing the
+        [batch_size, num_anchors, num_classes, code_size] representing the
-        [batch_size, 1, num_classes + 1] representing the class
+        [batch_size, num_anchors, num_classes + 1] representing the class
-      ValueError: if num_predictions_per_location is not 1.
+      ValueError: if num_predictions_per_location is not 1 or if
-    if num_predictions_per_location != 1:
+    if (len(num_predictions_per_location) != 1 or
-
+    if len(image_features) != 1:
-    net = image_features
+    net = image_feature
-        applied to the image_features in the mask prediciton branch.
+        applied to the image_features in the mask prediction branch. If set
-      ValueError: If predict_instance_masks or predict_keypoints is true.
+      ValueError: If predict_instance_masks is true but conv_hyperparams is not
-    anchors dimension.
+  @property
-    and a convolution.
+  def _predict_boxes_and_classes(self, image_features):
-      ValueError: if num_predictions_per_location is not 1.
+      box_encodings: A float tensor of shape
-                       'predicting a single box per class per location.')
+    return box_encodings, class_predictions_with_background
-            align_corners=True)
+    Returns:
-      predictions_dict[MASK_PREDICTIONS] = instance_masks
+            num_outputs=num_conv_channels,
-  def _predict(self, image_features, num_predictions_per_location):
+  def _predict(self, image_features, num_predictions_per_location_list):
-        predictions to be made per spatial location in the feature map.
+      image_features: A list of float tensors of shape [batch_size, height_i,
-    return {BOX_ENCODINGS: box_encodings,
+    box_encodings_list = []
-            class_predictions_with_background}
+            tf.concat(class_predictions_list, axis=1)}
-
+from object_detection.utils import test_case
-        image_features, num_predictions_per_location=1, scope='BoxPredictor')
+        [image_features], num_predictions_per_location=[1],
-        image_features, num_predictions_per_location=1, scope='BoxPredictor')
+        [image_features],
-  def test_do_not_return_instance_masks_and_keypoints_without_request(self):
+  def test_do_not_return_instance_masks_without_request(self):
-        image_features, num_predictions_per_location=1, scope='BoxPredictor')
+        [image_features], num_predictions_per_location=[1],
-        image_features, num_predictions_per_location=1, scope='BoxPredictor',
+        [image_features], num_predictions_per_location=[1],
-class ConvolutionalBoxPredictorTest(tf.test.TestCase):
+class ConvolutionalBoxPredictorTest(test_case.TestCase):
-      self.assertAllEqual(objectness_predictions_shape, [4, 320, 1])
+    def graph_fn(image_features):
-    image_features = tf.random_uniform([4, 8, 8, 64], dtype=tf.float32)
+    def graph_fn(image_features):
-        image_features, num_predictions_per_location=1, scope='BoxPredictor')
+        [image_features], num_predictions_per_location=[5],
-
+
-      self.assertAllEqual(objectness_predictions_shape, [4, 64, 1])
+           [tf.shape(box_encodings), tf.shape(objectness_predictions)],
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    def graph_fn(image_features):
-  def test_get_boxes_for_five_aspect_ratios_per_location_fully_convolutional(
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
-    conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(
+    conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
-        max_depth=32,
+        depth=32,
-    )
+        box_code_size=4)
-        image_features, num_predictions_per_location=5, scope='BoxPredictor')
+        [image_features], num_predictions_per_location=[5],
-      target_tensor: a tensor representing regression or classification targets.
+      prediction_tensor: an N-d tensor of shape [batch, anchors, ...]
-      loss: a tensor representing the value of the loss function
+      loss: an N-d tensor of shape [batch, anchors, ...] containing the loss per
-            or a float tensor of shape [batch_size, num_anchors]
+      loss: a float tensor of shape [batch_size, num_anchors] tensor
-    return tf.reduce_sum(square_diff)
+    return tf.reduce_sum(square_diff, 2)
-      loss: a (scalar) tensor representing the value of the loss function
+      loss: a float tensor of shape [batch_size, num_anchors] tensor
-    return tf.reduce_sum(anchorwise_smooth_l1norm)
+    return anchorwise_smooth_l1norm
-      loss: a (scalar) tensor representing the value of the loss function
+      loss: a float tensor of shape [batch_size, num_anchors] tensor
-    return tf.reduce_sum(tf.reshape(weights, [-1]) * per_anchor_iou_loss)
+    return tf.reshape(weights, [-1]) * per_anchor_iou_loss
-            or a float tensor of shape [batch_size, num_anchors]
+      loss: a float tensor of shape [batch_size, num_anchors, num_classes]
-    return tf.reduce_sum(per_entry_cross_ent * weights)
+    return per_entry_cross_ent * weights
-  def __init__(self, anchorwise_output=False, gamma=2.0, alpha=0.25):
+  def __init__(self, gamma=2.0, alpha=0.25):
-            or a float tensor of shape [batch_size, num_anchors]
+      loss: a float tensor of shape [batch_size, num_anchors, num_classes]
-    return tf.reduce_sum(focal_cross_entropy_loss * weights)
+    return focal_cross_entropy_loss * weights
-  def __init__(self, anchorwise_output=False, logit_scale=1.0):
+  def __init__(self, logit_scale=1.0):
-      loss: a (scalar) tensor representing the value of the loss function
+      loss: a float tensor of shape [batch_size, num_anchors]
-    return tf.reduce_sum(per_row_cross_ent * tf.reshape(weights, [-1]))
+    return tf.reshape(per_row_cross_ent, tf.shape(weights)) * weights
-  def __init__(self, alpha, bootstrap_type='soft', anchorwise_output=False):
+  def __init__(self, alpha, bootstrap_type='soft'):
-            or a float tensor of shape [batch_size, num_anchors]
+      loss: a float tensor of shape [batch_size, num_anchors, num_classes]
-    return tf.reduce_sum(per_entry_cross_ent * tf.expand_dims(weights, 2))
+    return per_entry_cross_ent * tf.expand_dims(weights, 2)
-  def testReturnsCorrectLoss(self):
+  def testReturnsCorrectWeightedLoss(self):
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    loss = tf.reduce_sum(loss_op(prediction_tensor, target_tensor,
-    loss_op = losses.WeightedL2LocalizationLoss(anchorwise_output=True)
+    loss_op = losses.WeightedL2LocalizationLoss()
-
+    loss = tf.reduce_sum(loss)
-    loss_op = losses.WeightedSigmoidClassificationLoss(True)
+    loss_op = losses.WeightedSigmoidClassificationLoss()
-    loss_op = losses.WeightedSigmoidClassificationLoss(True)
+    loss_op = losses.WeightedSigmoidClassificationLoss()
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=1.0)
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=0.0)
-        anchorwise_output=True)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.5, gamma=0.0)
-        anchorwise_output=True)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=None, gamma=0.0)
-        anchorwise_output=False, alpha=1.0, gamma=0.0)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=1.0, gamma=0.0)
-                               weights=weights)
+    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-        anchorwise_output=False, alpha=0.75, gamma=0.0)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.75, gamma=0.0)
-                               weights=weights)
+    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
-    loss_op = losses.WeightedSoftmaxClassificationLoss(True)
+    loss_op = losses.WeightedSoftmaxClassificationLoss()
-    # TODO(yonib): Also test logit_scale with anchorwise=False.
+    # TODO: Also test logit_scale with anchorwise=False.
-        anchorwise_output=True, logit_scale=logit_scale)
+    loss_op = losses.WeightedSoftmaxClassificationLoss(logit_scale=logit_scale)
-        alpha, bootstrap_type='hard', anchorwise_output=True)
+        alpha, bootstrap_type='hard')
-
+    loss = tf.reduce_sum(loss, axis=2)
-    """Method to be overriden by implementations.
+    """Method to be overridden by implementations.
-class AnchorMatcherTest(tf.test.TestCase):
+class MatchTest(tf.test.TestCase):
-    prediction.
+    It is also responsible for any resizing, padding that might be necessary
-  def predict(self, preprocessed_inputs):
+  def predict(self, preprocessed_inputs, true_image_shapes):
-  def postprocess(self, prediction_dict, **params):
+  def postprocess(self, prediction_dict, true_image_shapes, **params):
-  def loss(self, prediction_dict):
+  def loss(self, prediction_dict, true_image_shapes):
-      suppression.
+    clip_window: A float32 tensor of shape [batch_size, 4]  where each entry is
-
+  if change_coordinate_frame and clip_window is None:
-          for key, value in zip(additional_fields, args[3:-1])
+          for key, value in zip(additional_fields, args[4:-1])
-          clip_window=clip_window,
+          clip_window=per_image_clip_window,
-    batch_outputs = tf.map_fn(
+    batch_outputs = shape_utils.static_or_dynamic_map_fn(
-               [num_valid_boxes]),
+        elems=([boxes, scores, masks, clip_window] +
-  return tf.constant(new_size)
+  return tf.constant(new_size + [num_channels])
-  return new_size
+  return tf.stack(tf.unstack(new_size) + [num_channels])
-                    align_corners=False):
+                    align_corners=False,
-    A 3D tensor of shape [num_instances, new_height, new_width]
+    Note that the position of the resized_image_shape changes based on whether
-        image, new_size, method=method, align_corners=align_corners)
+        image, new_size[:-1], method=method, align_corners=align_corners)
-    result = new_image
+    result = [new_image]
-          new_masks, new_size, align_corners=align_corners)
+      new_masks = tf.image.resize_images(
-      result = [new_image, new_masks]
+      if pad_to_max_dimension:
-        size [num_instances, new_height, new_width].
+    Note that the position of the resized_image_shape changes based on whether
-    result = tf.squeeze(image, axis=0)
+    result = [tf.squeeze(image, axis=0)]
-      result = (result, tf.squeeze(masks, axis=3))
+      result.append(tf.squeeze(masks, axis=3))
-  """See `tf.image.resize_images` for detailed doc."""
+  """Resizes images to the given height and width.
-    result = new_image
+    image_shape = shape_utils.combined_static_and_dynamic_shape(image)
-      result = [new_image, masks]
+      result.append(masks)
-      out_image, out_masks = preprocessor.resize_image(
+      out_image, out_masks, _ = preprocessor.resize_image(
-      out_image, out_masks = preprocessor.resize_image(
+      out_image, out_masks, _ = preprocessor.resize_image(
-      out_image = preprocessor.resize_to_range(
+      out_image, _ = preprocessor.resize_to_range(
-      out_image = preprocessor.resize_to_range(
+      out_image, _ = preprocessor.resize_to_range(
-      out_image, out_masks = preprocessor.resize_to_range(
+      out_image, out_masks, _ = preprocessor.resize_to_range(
-      out_image, out_masks = preprocessor.resize_to_range(
+      out_image, out_masks, _ = preprocessor.resize_to_range(
-      out_image, out_masks = preprocessor.resize_to_range(
+      out_image, out_masks, _ = preprocessor.resize_to_range(
-      out_image = preprocessor.resize_to_range(
+      out_image, _ = preprocessor.resize_to_range(
-      out_image, out_masks = preprocessor.resize_to_min_dimension(
+      out_image, out_masks, _ = preprocessor.resize_to_min_dimension(
-      out_image, out_masks = preprocessor.resize_to_min_dimension(
+      out_image, out_masks, _ = preprocessor.resize_to_min_dimension(
-  """Naming converntions for storing the output of the detector.
+  """Naming conventions for storing the output of the detector.
-from object_detection.core import box_list_ops
+from object_detection.core import standard_fields as fields
-               unmatched_cls_target=None):
+               negative_class_weight=1.0, unmatched_cls_target=None):
-        anchors (default: 1.0)
+        anchors (default: 1.0). The weight must be in [0., 1.].
-             **params):
+             groundtruth_weights=None, **params):
-
+    unmatched_shape_assert = shape_utils.assert_shape_equal(
-          match, self._positive_class_weight, self._negative_class_weight)
+      reg_weights = self._create_regression_weights(match, groundtruth_weights)
-        cls_weights = self._reset_target_shape(cls_weights, num_anchors)
+    num_anchors = anchors.num_boxes_static()
-                                                 matched_anchors)
+    matched_gt_boxes = match.gather_based_on_match(
-    # TODO: summarize the number of matches on average.
+        self._default_regression_target(), [match_results_shape[0], 1])
-        which has shape [num_gt_boxes, d_1, d_2, ... d_k].
+      a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k], where the
-  def _create_regression_weights(self, match):
+    return match.gather_based_on_match(
-        regression weights
+      a float32 tensor with shape [num_anchors] representing regression weights.
-    return reg_weights
+    return match.gather_based_on_match(
-                                     negative_class_weight=1.0):
+                                     groundtruth_weights):
-      negative_class_weight: weight to be associated to negative anchors
+      groundtruth_weights: a float tensor of shape [M] indicating the weight to
-        classification weights.
+      a float32 tensor with shape [num_anchors] representing classification
-    return cls_weights
+    return match.gather_based_on_match(
-      BoxCoder: BoxCoder object.
+      BoxCoder object.
-                        positive_class_weight=positive_class_weight,
+from object_detection.box_coders import keypoint_box_coder
-class TargetAssignerTest(tf.test.TestCase):
+class TargetAssignerTest(test_case.TestCase):
-    boxes = box_list.BoxList(tf.constant(box_corners))
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners):
-  def test_assign_with_ignored_matches(self):
+
-    boxes = box_list.BoxList(tf.constant(box_corners))
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners):
-      self.assertEquals(matching_anchors_out.dtype, np.int32)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
-                                      [0, 0, 0, 1, 0, 0, 0]], tf.float32)
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
-                                      [[0, 1], [1, .5]]], tf.float32)
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,
-      self.assertEquals(matching_anchors_out.dtype, np.int32)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
-
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,
-      self.assertEquals(matching_anchors_out.dtype, np.int32)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
-        sess.run([cls_targets, cls_weights, reg_targets, reg_weights])
+    with self.assertRaisesRegexp(ValueError, 'Unequal shapes'):
-class BatchTargetAssignerTest(tf.test.TestCase):
+class BatchTargetAssignerTest(test_case.TestCase):
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
+    similarity_calc = region_similarity_calculator.IouSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
+    similarity_calc = region_similarity_calculator.IouSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
+    similarity_calc = region_similarity_calculator.IouSimilarity()
-    priors.add_field('stddev', prior_stddevs)
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,
-                                  [0, 0, 1, 0]], tf.float32)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
-    priors.add_field('stddev', prior_stddevs)
+  def test_batch_assign_multiclass_targets(self):
-                       [[0, 0, 0, 0],
+                        [0, 0, 0, 0,],
-      self.assertAllClose(match_out_1, exp_match_1)
+
-    priors.add_field('stddev', prior_stddevs)
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,
-                       [[0, 0, 0, 0],
+                        [0, 0, 0, 0,],
-      self.assertAllClose(match_out_1, exp_match_1)
+
-    priors.add_field('stddev', prior_stddevs)
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,
-      self.assertAllClose(match_out_0, exp_match_0)
+    gt_class_targets = np.zeros((0, num_classes + pad), dtype=np.float32)
-            slim_example_decoder.Tensor('image/object/group_of'))
+            slim_example_decoder.Tensor('image/object/group_of')),
-    label_handler = slim_example_decoder.Tensor('image/object/class/label')
+      if instance_mask_type in (input_reader_pb2.DEFAULT,
-      fields.InputDataFields.groundtruth_instance_masks - 3D int64 tensor of
+      fields.InputDataFields.groundtruth_instance_masks - 3D float32 tensor of
-    width] and cast to boolean type to save memory.
+    width].
-
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-    # TODO(talremez) filter out targets that are truncated or heavily occluded.
+    # TODO filter out targets that are truncated or heavily occluded.
-class DictToTFExampleTest(tf.test.TestCase):
+class CreateKittiTFRecordTest(tf.test.TestCase):
-class DictToTFExampleTest(tf.test.TestCase):
+class CreatePascalTFRecordTest(tf.test.TestCase):
-                       faces_only=True):
+                       faces_only=True,
-      mask_remapped = mask_np != 2
+      mask_remapped = (mask_np != 2).astype(np.uint8)
-        dataset_util.float_list_feature(masks_flattened.tolist()))
+    if mask_type == 'numerical':
-                     faces_only=True):
+                     faces_only=True,
-          data, mask_path, label_map_dict, image_dir, faces_only=faces_only)
+          data,
-# TODO(derekjchow): Add test for pet/PASCAL main files.
+# TODO: Add test for pet/PASCAL main files.
-                   image_dir, val_examples, faces_only=FLAGS.faces_only)
+  create_tf_record(
-from object_detection.builders import input_reader_builder
+from object_detection.builders import dataset_builder
-      input_config)
+  def get_next(config):
-  summary_writer = tf.summary.FileWriter(summary_dir)
+  summary_writer = tf.summary.FileWriterCache.get(summary_dir)
-  summary_writer.close()
+                                groundtruth_box_visualization_color='black',
-                                max_num_predictions=20):
+                                max_num_predictions=20,
-      'detection_classes'
+      input_fields.original_image,
-  if show_groundtruth and 'groundtruth_boxes' not in result_dict:
+  if show_groundtruth and input_fields.groundtruth_boxes not in result_dict:
-  detection_masks = result_dict.get('detection_masks', None)
+  image = np.squeeze(result_dict[input_fields.original_image], axis=0)
-    groundtruth_keypoints = result_dict.get('groundtruth_keypoints', None)
+    groundtruth_boxes = result_dict[input_fields.groundtruth_boxes]
-        category_index,
+        image=image,
-        max_boxes_to_draw=None)
+        max_boxes_to_draw=None,
-      agnostic_mode=agnostic_mode)
+      agnostic_mode=agnostic_mode,
-    export_path = os.path.join(export_dir, 'export-{}.png'.format(tag))
+    if keep_image_id_for_visualization_export and result_dict[fields.
-  summary_writer = tf.summary.FileWriter(summary_dir)
+  summary_writer = tf.summary.FileWriterCache.get(summary_dir)
-  summary_writer.close()
+        if not result_dict:
-          # TODO: Use image_id tensor once we fix the input data
+          # TODO(b/65130867): Use image_id tensor once we fix the input data
-      it does not matter whether boxes are expressed in absolute or relative
+    scale_to_absolute: Boolean indicating whether boxes and keypoints should be
-      masks. (Only present if available in `detections`)
+    'detection_masks': [max_detections, H, W] float32 tensor of binarized
-      output_dict[detection_fields.detection_masks] = detection_masks_reframed
+    # TODO: This should be done in model's postprocess
-    'pascal_voc_metrics':
+    'pascal_voc_detection_metrics':
-    'weighted_pascal_voc_metrics':
+    'weighted_pascal_voc_detection_metrics':
-    'open_images_metrics':
+    'pascal_voc_instance_segmentation_metrics':
-  detections = model.postprocess(prediction_dict)
+  preprocessed_image, true_image_shapes = model.preprocess(
-  ]
+  eval_metric_fn_keys = eval_config.metrics_set
-             checkpoint_dir, eval_dir):
+             checkpoint_dir, eval_dir, graph_hook_fn=None):
-          show_groundtruth=eval_config.visualization_export_dir)
+          show_groundtruth=eval_config.visualize_groundtruth_boxes,
-
+flags.DEFINE_string('config_override', '',
-from tensorflow.core.protobuf import rewriter_config_pb2
+from google.protobuf import text_format
-    config = tf.ConfigProto(graph_options=graph_options)
+    config = tf.ConfigProto(graph_options=tf.GraphOptions())
-  num_detections = postprocessed_tensors.get('num_detections')
+  boxes = postprocessed_tensors.get(detection_fields.detection_boxes)
-  outputs['num_detections'] = tf.identity(num_detections, name='num_detections')
+  outputs[detection_fields.detection_boxes] = tf.identity(
-    outputs['detection_masks'] = tf.identity(masks, name='detection_masks')
+    outputs[detection_fields.detection_masks] = tf.identity(
-    tf.add_to_collection(output_collection_name, outputs['detection_masks'])
+    tf.add_to_collection(output_collection_name,
-                            output_collection_name='inference_op'):
+                            output_collection_name='inference_op',
-  postprocessed_tensors = detection_model.postprocess(output_tensors)
+  preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
-    tensors to include in the frozen graph.
+      tensors to include in the frozen graph.
-                          input_shape, optimize_graph, output_collection_name)
+                          input_shape, output_collection_name,
-    return tf.identity(inputs)
+    true_image_shapes = []  # Doesn't matter for the fake model.
-  def predict(self, preprocessed_inputs):
+  def predict(self, preprocessed_inputs, true_image_shapes):
-  def postprocess(self, prediction_dict):
+  def postprocess(self, prediction_dict, true_image_shapes):
-  def loss(self, prediction_dict):
+  def loss(self, prediction_dict, true_image_shapes):
-      preprocessed_inputs = mock_model.preprocess(
+      preprocessed_inputs, true_image_shapes = mock_model.preprocess(
-      mock_model.postprocess(predictions)
+      predictions = mock_model.predict(preprocessed_inputs, true_image_shapes)
-      preprocessed_inputs = fake_model.preprocess(
+      preprocessed_inputs, true_image_shapes = fake_model.preprocess(
-      fake_model.postprocess(predictions)
+      predictions = fake_model.predict(preprocessed_inputs, true_image_shapes)
-                                   '^TensorArray has inconsistent shapes.'):
+                                   'TensorArray.*shape'):
-
+from object_detection.utils import shape_utils
-      return -1 * tf.ones([tf.shape(similarity_matrix)[1]], dtype=tf.int32)
+      similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(
-      matches = tf.argmax(similarity_matrix, 0)
+      matches = tf.argmax(similarity_matrix, 0, output_type=tf.int32)
-        _match_when_rows_are_non_empty, _match_when_rows_are_empty)
+        similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(
-class ArgMaxMatcherTest(tf.test.TestCase):
+class ArgMaxMatcherTest(test_case.TestCase):
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=None)
+                           [3, 0, -1, 0, 0]], dtype=np.float32)
-    self.assertEmpty(res_unmatched_cols)
+    self.assertAllEqual(res_match_results[res_matched_cols],
-      self.assertAllEqual(res_unmatched_cols, np.arange(5))
+    def graph_fn(similarity_matrix):
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3)
+                           [3, 0, -1, 0, 0]], dtype=np.float32)
-    init_op = tf.global_variables_initializer()
+    (res_matched_cols, res_unmatched_cols,
-      res_unmatched_cols = sess.run(unmatched_cols)
+  def test_return_correct_matches_with_matched_and_unmatched_threshold(self):
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    def graph_fn(similarity):
-                                           unmatched_threshold=2)
+                           [3, 0, -1, 0, 0]], dtype=np.float32)
-    unmatched_cols = match.unmatched_column_indices()
+    (res_matched_cols, res_unmatched_cols,
-      res_unmatched_cols = sess.run(unmatched_cols)
+  def test_return_correct_matches_negatives_lower_than_unmatched_false(self):
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    def graph_fn(similarity):
-                                           negatives_lower_than_unmatched=False)
+                           [3, 0, -1, 0, 0]], dtype=np.float32)
-    unmatched_cols = match.unmatched_column_indices()
+    (res_matched_cols, res_unmatched_cols,
-      res_unmatched_cols = sess.run(unmatched_cols)
+  def test_return_correct_matches_unmatched_row_not_using_force_match(self):
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    def graph_fn(similarity):
-                                           unmatched_threshold=2)
+                           [3, 0, -1, 2, 0]], dtype=np.float32)
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    (res_matched_cols, res_unmatched_cols,
-                                           force_match_for_each_row=True)
+                           [3, 0, -1, 2, 0]], dtype=np.float32)
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    (res_matched_cols, res_unmatched_cols,
-        all the rows.
+    TODO: Add num_valid_columns options to match only that many columns
-detections.
+We allow for three modes: number_of_stages={1, 2, 3}. In case of 1 stage,
-    with tf.variable_scope(scope, values=[proposal_feature_maps]):
+    with tf.variable_scope(
-               first_stage_only,
+               number_of_stages,
-        builders/image_resizer_builder.py.
+        (corresponding to a single image), an optional rank-3 instance mask
-        (RPN) part of the model.
+      number_of_stages:  An integer values taking values in {1, 2, 3}. If
-    self._first_stage_only = first_stage_only
+    self._number_of_stages = number_of_stages
-        losses.WeightedSmoothL1LocalizationLoss(anchorwise_output=True))
+        losses.WeightedSmoothL1LocalizationLoss())
-        losses.WeightedSoftmaxClassificationLoss(anchorwise_output=True))
+        losses.WeightedSoftmaxClassificationLoss())
-        losses.WeightedSmoothL1LocalizationLoss(anchorwise_output=True))
+        losses.WeightedSmoothL1LocalizationLoss())
-        losses.WeightedSigmoidClassificationLoss(anchorwise_output=True))
+        losses.WeightedSigmoidClassificationLoss())
-      return self._feature_extractor.preprocess(resized_inputs)
+      outputs = shape_utils.static_or_dynamic_map_fn(
-  def predict(self, preprocessed_inputs):
+    This function assumes that the clip window's left top corner is at (0, 0).
-    If `first_stage_only` is True, this function only returns first stage
+    If `number_of_stages` is 1, this function only returns first stage
-        (and if first_stage_only=False):
+        (and if number_of_stages=1):
-    anchors = anchors_boxlist.get()
+    self._anchors = anchors_boxlist
-        'anchors': anchors
+        'anchors': self._anchors.get()
-    if not self._first_stage_only:
+    if self._number_of_stages >= 2:
-          anchors, image_shape))
+          self._anchors.get(), image_shape, true_image_shapes))
-                            image_shape):
+                            image_shape,
-        anchors, image_shape)
+        anchors, image_shape_2d, true_image_shapes)
-        scope=self.second_stage_box_predictor_scope)
+        [box_classifier_features],
-        num_anchors_per_location[0],
+        [rpn_box_predictor_features],
-      return tf.map_fn(
+      return shape_utils.static_or_dynamic_map_fn(
-  def postprocess(self, prediction_dict):
+  def postprocess(self, prediction_dict, true_image_shapes):
-    If first_stage_only=True, the returned results represent proposals from the
+    If number_of_stages=1, the returned results represent proposals from the
-        documentation for the predict method.  If first_stage_only=True, we
+        documentation for the predict method.  If number_of_stages=1, we
-        prediction_dict to additionally contain `refined_box_encodings`,
+        and `anchors` fields.  Otherwise we expect prediction_dict to
-      if self._first_stage_only:
+      if self._number_of_stages == 1:
-            image_shape)
+            true_image_shapes,
-            'num_detections': tf.to_float(num_proposals)
+            fields.DetectionResultFields.detection_boxes: proposal_boxes,
-          mask_predictions=mask_predictions)
+      if self._number_of_stages == 2:
-                       image_shape):
+                       image_shapes,
-      image_shape: A 1-D tensor representing the input image shape.
+      image_shapes: A 2-D tensor of shape [batch, 3] containing the shapes of
-    clip_window = tf.to_float(tf.stack([0, 0, image_shape[1], image_shape[2]]))
+    clip_window = self._compute_clip_window(image_shapes)
-         _) = self._format_groundtruth_data(image_shape)
+         _) = self._format_groundtruth_data(true_image_shapes)
-    return proposal_boxes, proposal_scores, num_proposals
+    def normalize_boxes(args):
-        represented as normalized coordinates.
+        represented in absolute coordinates.
-        are represented as normalized coordinates.
+        are represented in absolute coordinates.
-  def _format_groundtruth_data(self, image_shape):
+  def _format_groundtruth_data(self, true_image_shapes):
-        input image batch.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
-        for boxes in self.groundtruth_lists(fields.BoxListFields.boxes)]
+            box_list.BoxList(boxes), true_image_shapes[i, 0],
-        resized_masks_list.append(tf.squeeze(resized_4d_mask, axis=3))
+        _, resized_mask, _ = self._image_resizer_fn(
-                                  image_shape,
+                                  image_shapes,
-      image_shape: a 1-D int32 tensor representing the input image shape.
+      image_shapes: a 2-D int32 tensor containing shapes of input image in the
-
+    clip_window = self._compute_clip_window(image_shapes)
-                  'num_detections': tf.to_float(num_detections)}
+    detections = {
-      detections['detection_masks'] = nmsed_masks
+      detections[fields.DetectionResultFields.detection_masks] = nmsed_masks
-  def loss(self, prediction_dict, scope=None):
+  def loss(self, prediction_dict, true_image_shapes, scope=None):
-    If first_stage_only=True, only RPN related losses are computed (i.e.,
+    If number_of_stages=1, only RPN related losses are computed (i.e.,
-        documentation for the predict method.  If first_stage_only=True, we
+        documentation for the predict method.  If number_of_stages=1, we
-      ) = self._format_groundtruth_data(prediction_dict['image_shape'])
+       groundtruth_masks_list) = self._format_groundtruth_data(
-      if not self._first_stage_only:
+      if self._number_of_stages > 1:
-      batch_sampled_indices = tf.to_float(tf.map_fn(
+      batch_sampled_indices = tf.to_float(shape_utils.static_or_dynamic_map_fn(
-          weights=batch_cls_weights) / normalizer
+      second_stage_cls_losses = ops.reduce_sum_trailing_dimensions(
-        # targets and mask targets.
+        # TODO: Move `unmatched_cls_target` from constructor to assign
-                           tf.ones((batch_size, 1))))
+        second_stage_mask_losses = ops.reduce_sum_trailing_dimensions(
-  def restore_map(self, from_detection_checkpoint=True):
+  def restore_map(self,
-        classification checkpoint for initialization prior to training.
+        classification checkpoint for initialization prior to training. Default
-                          self.second_stage_feature_extractor_scope])
+        variables_to_restore, include_patterns=include_patterns)
-        is_training=False, first_stage_only=False, second_stage_batch_size=6)
+        is_training=False, number_of_stages=2, second_stage_batch_size=6)
-    })
+    }, true_image_shapes)
-  def _get_second_stage_box_predictor(self, num_classes, is_training):
+  def _add_mask_to_second_stage_box_predictor_text_proto(self):
-                   first_stage_only,
+                   number_of_stages,
-      return tf.identity(image)
+                   softmax_second_stage_classification_loss=True,
-          losses.WeightedSoftmaxClassificationLoss(anchorwise_output=True))
+          losses.WeightedSoftmaxClassificationLoss())
-          losses.WeightedSigmoidClassificationLoss(anchorwise_output=True))
+          losses.WeightedSigmoidClassificationLoss())
-        'first_stage_only': first_stage_only,
+        'number_of_stages': number_of_stages,
-        num_classes=num_classes, is_training=is_training), **common_kwargs)
+    return self._get_model(
-          is_training=False, first_stage_only=True, second_stage_batch_size=2)
+          is_training=False, number_of_stages=1, second_stage_batch_size=2)
-      prediction_dict = model.predict(preprocessed_inputs)
+      _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph) as sess:
-          is_training=True, first_stage_only=True, second_stage_batch_size=2)
+          is_training=True, number_of_stages=1, second_stage_batch_size=2)
-      prediction_dict = model.predict(preprocessed_inputs)
+      _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph) as sess:
-      self):
+  def test_predict_correct_shapes_in_inference_mode_two_stages(self):
-            second_stage_batch_size=2)
+            is_training=False,
-        result_tensor_dict = model.predict(preprocessed_inputs)
+        _, true_image_shapes = model.preprocess(preprocessed_inputs)
-          is_training=True, first_stage_only=False, second_stage_batch_size=7)
+          is_training=True,
-      result_tensor_dict = model.predict(preprocessed_inputs)
+      result_tensor_dict = model.predict(preprocessed_inputs, true_image_shapes)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph) as sess:
-  def test_postprocess_first_stage_only_inference_mode(self):
+  def _test_postprocess_first_stage_only_inference_mode(
-        is_training=False, first_stage_only=True, second_stage_batch_size=6)
+        is_training=False, number_of_stages=1, second_stage_batch_size=6,
-        'image_shape': image_shape})
+        'anchors': anchors}, true_image_shapes)
-  def test_postprocess_first_stage_only_train_mode(self):
+  def test_postprocess_first_stage_only_inference_mode(self):
-        is_training=True, first_stage_only=True, second_stage_batch_size=2)
+        is_training=True, number_of_stages=1, second_stage_batch_size=2,
-        'image_shape': image_shape})
+        'anchors': anchors}, true_image_shapes)
-    num_proposals_shapes = [(2), (None)]
+  def test_postprocess_first_stage_only_train_mode(self):
-            second_stage_batch_size=6)
+            is_training=False, number_of_stages=2,
-        })
+        }, true_image_shapes)
-          is_training=False, first_stage_only=False, second_stage_batch_size=6)
+          is_training=False, number_of_stages=2, second_stage_batch_size=6)
-      preprocessed_inputs = model.preprocess(image_placeholder)
+      preprocessed_inputs, _ = model.preprocess(image_placeholder)
-        is_training=True, first_stage_only=True, second_stage_batch_size=6)
+        is_training=True, number_of_stages=1, second_stage_batch_size=6)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
-        is_training=True, first_stage_only=False, second_stage_batch_size=6)
+        is_training=True, number_of_stages=2, second_stage_batch_size=6)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
-        is_training=True, first_stage_only=False, second_stage_batch_size=6)
+        is_training=True, number_of_stages=2, second_stage_batch_size=6)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
-        is_training=True, first_stage_only=False, second_stage_batch_size=6,
+        is_training=True, number_of_stages=2, second_stage_batch_size=6,
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
-        is_training=True, first_stage_only=False, second_stage_batch_size=6)
+        is_training=True, number_of_stages=2, second_stage_batch_size=6)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
-                              first_stage_only=False,
+                              number_of_stages=2,
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_classification) as sess:
-          is_training=False, first_stage_only=False, second_stage_batch_size=6)
+          is_training=False, number_of_stages=2, second_stage_batch_size=6)
-      model.postprocess(prediction_dict)
+      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_classification) as sess:
-                           var.name)
+          self.assertNotIn(model.first_stage_feature_extractor_scope, var)
-          is_training=False, first_stage_only=False, second_stage_batch_size=6)
+          is_training=False, number_of_stages=2, second_stage_batch_size=6)
-      model.postprocess(prediction_dict)
+      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_detection1) as sess:
-      model2 = self._build_model(is_training=False, first_stage_only=False,
+      model2 = self._build_model(is_training=False, number_of_stages=2,
-      model2.postprocess(prediction_dict2)
+      preprocessed_inputs2, true_image_shapes = model2.preprocess(inputs2)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_detection2) as sess:
-                           var.name)
+        uninitialized_vars_list = sess.run(tf.report_uninitialized_variables())
-first_stage_only=False.  In the former setting, all of the user facing methods
+Similar to Faster R-CNN we allow for two modes: number_of_stages=1 and
-               first_stage_only,
+               number_of_stages,
-        (RPN) part of the model.
+      number_of_stages:  Valid values are {1, 2}. If 1 will only construct the
-        first_stage_only,
+        number_of_stages,
-    """Predicts the output tensors from 2nd stage of FasterRCNN.
+                            image_shape,
-      rpn_box_encodings: 4-D float tensor of shape
+      rpn_box_encodings: 3-D float tensor of shape
-      rpn_objectness_predictions_with_background: 2-D float tensor of shape
+      rpn_objectness_predictions_with_background: 3-D float tensor of shape
-        2) class_predictions_with_background: a 3-D tensor with shape
+        2) class_predictions_with_background: a 2-D tensor with shape
-        anchors, image_shape)
+        anchors, image_shape_2d, true_image_shapes)
-        num_predictions_per_location=1,
+        [box_classifier_features],
-from object_detection.core import box_predictor as bpredictor
+from object_detection.utils import ops
-               reuse_weights=None):
+               reuse_weights=None,
-    and losses.
+    TODO(rathodv,jonathanhuang): group NMS parameters + score converter into
-        returns a rank-3 image tensor, possibly with new spatial dimensions.
+        returns a rank-3 image tensor, possibly with new spatial dimensions and
-    # TODO: handle agnostic mode and positive/negative class weights
+    # TODO: handle agnostic mode and positive/negative class
-    See base class.
+    SSD meta architecture uses a default clip_window of [0, 0, 1, 1] during
-  def predict(self, preprocessed_inputs):
+      # TODO: revisit whether to always use batch size as
-        1) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,
+        1) preprocessed_inputs: the [batch, height, width, channels] image
-        2) class_predictions_with_background: 3-D float tensor of shape
+        3) class_predictions_with_background: 3-D float tensor of shape
-        3) feature_maps: a list of tensors where the ith tensor has shape
+        4) feature_maps: a list of tensors where the ith tensor has shape
-        4) anchors: 2-D float tensor of shape [num_anchors, 4] containing
+        5) anchors: 2-D float tensor of shape [num_anchors, 4] containing
-    ) = self._add_box_predictions_to_feature_maps(feature_maps)
+    prediction_dict = self._box_predictor.predict(
-  def postprocess(self, prediction_dict):
+  def postprocess(self, prediction_dict, true_image_shapes):
-        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
+        1) preprocessed_inputs: a [batch, height, width, channels] image
-        2) class_predictions_with_background: 3-D float tensor of shape
+        3) class_predictions_with_background: 3-D float tensor of shape
-      clip_window = tf.constant([0, 0, 1, 1], tf.float32)
+      detection_scores_with_background = self._score_conversion_fn(
-           clip_window=clip_window,
+           clip_window=self._compute_clip_window(
-                        'num_detections': tf.to_float(num_detections)}
+      detection_dict = {
-            fields.BoxListFields.keypoints]
+        detection_dict[fields.DetectionResultFields.detection_keypoints] = (
-  def loss(self, prediction_dict, scope=None):
+  def loss(self, prediction_dict, true_image_shapes, scope=None):
-          weights=batch_cls_weights)
+      cls_losses = ops.reduce_sum_trailing_dimensions(
-        normalizer = tf.maximum(tf.to_float(tf.reduce_sum(num_matches)), 1.0)
+        normalizer = tf.maximum(tf.to_float(tf.reduce_sum(batch_reg_weights)),
-  def restore_map(self, from_detection_checkpoint=True):
+  def restore_map(self,
-                               var_name)[-1])
+      var_name = variable.op.name
-                             kernel_size=[1, 1], scope='layer1')
+                             kernel_size=1, scope='layer1')
-                     [.5, .5, 1, 1]], tf.float32))
+                     [1., 1., 1.5, 1.5]  # Anchor that is outside clip_window.
-    """Set up mock SSD model.
+class SsdMetaArchTest(test_case.TestCase):
-    """
+  def _create_model(self, apply_hard_mining=True):
-    self._num_classes = 1
+    num_classes = 1
-        is_training, self._num_classes)
+        is_training, num_classes)
-      return tf.identity(image)
+      return [tf.identity(image), tf.shape(image)]
-        anchorwise_output=True)
+    classification_loss = losses.WeightedSigmoidClassificationLoss()
-        iou_threshold=1.0)
+    hard_example_miner = None
-    self._model = ssd_meta_arch.SSDMetaArch(
+    code_size = 4
-        hard_example_miner)
+        hard_example_miner, add_summaries=False)
-  def test_preprocess_preserves_input_shapes(self):
+  def test_preprocess_preserves_shapes_with_dynamic_input_image(self):
-      preprocessed_inputs = self._model.preprocess(image_placeholder)
+      preprocessed_inputs, _ = model.preprocess(image_placeholder)
-  def test_predict_results_have_correct_keys_and_shapes(self):
+  def test_preprocess_preserves_shape_with_static_input_image(self):
-                    (None, image_size, image_size, 3),
+    input_shapes = [(None, image_size, image_size, 3),
-        batch_size, self._num_anchors, self._num_classes+1)
+        model, num_classes, num_anchors, code_size = self._create_model()
-        prediction_dict = self._model.predict(preprocessed_input_placeholder)
+        prediction_dict = model.predict(
-                                [0, 0, 0, 0]],
+                                [0, 0, 0, 0],   # pruned prediction
-                                [0, 0, 0, 0]]])
+                                [0, 0, 0, 0],  # pruned prediction
-    expected_num_detections = np.array([4, 4])
+    expected_num_detections = np.array([3, 3])
-        detections = self._model.postprocess(prediction_dict)
+        model, _, _, _ = self._create_model()
-                                      preprocessed_input_placeholder:
+                                      input_placeholder:
-    self.assertTrue('classification_loss' in loss_dict)
+    with tf.Graph().as_default():
-      losses_out = sess.run(loss_dict)
+    expected_classification_loss = (batch_size * num_anchors
-                          expected_classification_loss)
+    batch_size = 2
-      var_map = self._model.restore_map(from_detection_checkpoint=True)
+      var_map = model.restore_map(
-        self.assertNotIn('FeatureExtractor', var.name)
+        self.assertNotIn('FeatureExtractor', var)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_classification) as sess:
-      var_map = self._model.restore_map(from_detection_checkpoint=False)
+      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_detection) as sess:
-          self.assertNotIn('FeatureExtractor', var.name)
+          self.assertNotIn('FeatureExtractor', var)
-- open_images_metrics: Open Image V2 metric
+- open_images_detection_metrics: Open Image V2 metric
-               reuse_weights=None):
+               reuse_weights=None,
-        conv_hyperparams, batch_norm_trainable, reuse_weights)
+        conv_hyperparams, batch_norm_trainable, reuse_weights,
-        ['image size must be 256 in both height and width.'])
+    image_shape = preprocessed_inputs.get_shape()
-      with slim.arg_scope(self._conv_hyperparams):
+    with slim.arg_scope(self._conv_hyperparams):
-    ssd_feature_extractor_test.SsdFeatureExtractorTestBase, tf.test.TestCase):
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
-                                  (4, 1, 1, 256)]
+    expected_feature_map_shape = [(2, 16, 16, 512), (2, 8, 8, 1024),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 32), (4, 1, 1, 32)]
+    expected_feature_map_shape = [(2, 16, 16, 32), (2, 8, 8, 32), (2, 4, 4, 32),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 1, 1, 256)]
+    expected_feature_map_shape = [(2, 16, 16, 512), (2, 8, 8, 1024),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-    TODO: revisit whether it's possible to force the
+    TODO(jonathanhuang,rathodv): revisit whether it's possible to force the
-      # TODO: Figure out if it is needed when image batch size is bigger.
+      # TODO: Figure out if it is needed when image
-      stride = 2
+      stride = 2
-            padding='SAME',
+            padding=padding,
-            padding='SAME',
+            padding=padding,
-# TODO(rathodv): add tests with different anchor strides.
+# TODO: add tests with different anchor strides.
-        self.assertAllEqual(shape_out, exp_shape_out)
+class SsdFeatureExtractorTestBase(test_case.TestCase):
-        feature_extractor, preprocessed_inputs, expected_feature_map_shapes_out)
+      self, batch_size, image_height, image_width, depth_multiplier,
-               reuse_weights=None):
+               reuse_weights=None,
-        conv_hyperparams, batch_norm_trainable, reuse_weights)
+        conv_hyperparams, batch_norm_trainable, reuse_weights,
-        ['image size must at least be 33 in both height and width.'])
+    preprocessed_inputs = shape_utils.check_min_image_dim(
-              image_features=image_features)
+    with slim.arg_scope(self._conv_hyperparams):
-    ssd_feature_extractor_test.SsdFeatureExtractorTestBase, tf.test.TestCase):
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
-                                  (4, 1, 1, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 8, 8, 576), (2, 4, 4, 1024),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 19, 19, 576), (2, 10, 10, 1024),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 32), (4, 1, 1, 32)]
+    expected_feature_map_shape = [(2, 19, 19, 128), (2, 10, 10, 128),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 20, 20, 576), (2, 10, 10, 1024),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-               reuse_weights=None):
+               reuse_weights=None,
-        conv_hyperparams, batch_norm_trainable, reuse_weights)
+        conv_hyperparams, batch_norm_trainable, reuse_weights,
-        ['image size must at least be 33 in both height and width.'])
+    preprocessed_inputs = shape_utils.check_min_image_dim(
-              image_features=image_features)
+    with slim.arg_scope(self._conv_hyperparams):
-    ssd_feature_extractor_test.SsdFeatureExtractorTestBase, tf.test.TestCase):
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
-                                  (4, 1, 1, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 13, 13, 288), (2, 6, 6, 768),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 35, 35, 288), (2, 17, 17, 768),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 32), (4, 1, 1, 32)]
+    expected_feature_map_shape = [(2, 35, 35, 128), (2, 17, 17, 128),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 37, 37, 288), (2, 18, 18, 768),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-               reuse_weights=None):
+               reuse_weights=None,
-        conv_hyperparams, batch_norm_trainable, reuse_weights)
+        conv_hyperparams, batch_norm_trainable, reuse_weights,
-        ['image size must at least be 33 in both height and width.'])
+    preprocessed_inputs = shape_utils.check_min_image_dim(
-                image_features=image_features)
+    with slim.arg_scope(self._conv_hyperparams):
-    ssd_feature_extractor_test.SsdFeatureExtractorTestBase, tf.test.TestCase):
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
-                                  (4, 1, 1, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 8, 8, 512), (2, 4, 4, 1024),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 19, 19, 512), (2, 10, 10, 1024),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 32), (4, 1, 1, 32)]
+    expected_feature_map_shape = [(2, 19, 19, 32), (2, 10, 10, 32),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 20, 20, 512), (2, 10, 10, 1024),
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
-    test_image = np.random.rand(4, image_height, image_width, 3)
+    test_image = np.random.rand(2, image_height, image_width, 3)
-from object_detection.builders import input_reader_builder
+from object_detection.builders import dataset_builder
-      input_reader_builder.build, input_config)
+  def get_next(config):
-    return image, key, location_gt, classes_gt, masks_gt, keypoints_gt
+    weights_gt = read_data.get(
-   groundtruth_masks_list, groundtruth_keypoints_list) = get_inputs(
+   groundtruth_masks_list, groundtruth_keypoints_list, _) = get_inputs(
-  images = tf.concat(images, 0)
+
-  prediction_dict = detection_model.predict(images)
+  prediction_dict = detection_model.predict(images, true_image_shapes)
-  losses_dict = detection_model.loss(prediction_dict)
+  losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
-          is_chief, train_dir):
+          is_chief, train_dir, graph_hook_fn=None):
-    # TODO(rathodv): See if summaries can be added/extracted from global tf
+    # TODO: See if summaries can be added/extracted from global tf
-          clones, training_optimizer, regularization_losses=None)
+          clones, training_optimizer,
-      update_op = tf.group(*update_ops)
+      update_op = tf.group(*update_ops, name='update_barrier')
-        anchorwise_output=True)
+    self._classification_loss = losses.WeightedSigmoidClassificationLoss()
-    return tf.image.resize_images(inputs, [28, 28])
+    true_image_shapes = [inputs.shape[:-1].as_list()
-  def predict(self, preprocessed_inputs):
+  def predict(self, preprocessed_inputs, true_image_shapes):
-  def postprocess(self, prediction_dict, **params):
+  def postprocess(self, prediction_dict, true_image_shapes, **params):
-  def loss(self, prediction_dict):
+  def loss(self, prediction_dict, true_image_shapes):
-import tensorflow.google as tf
+import tensorflow as tf
-    hparams = tf.HParams(learning_rate=0.15)
+    hparams = tf.contrib.training.HParams(learning_rate=0.15)
-    hparams = tf.HParams(batch_size=16)
+    hparams = tf.contrib.training.HParams(batch_size=16)
-    hparams = tf.HParams(batch_size=0.5)
+    hparams = tf.contrib.training.HParams(batch_size=0.5)
-    hparams = tf.HParams(momentum_optimizer_value=1.1)
+    hparams = tf.contrib.training.HParams(momentum_optimizer_value=1.1)
-    hparams = tf.HParams(
+    hparams = tf.contrib.training.HParams(
-    hparams = tf.HParams(focal_loss_alpha=new_alpha, focal_loss_gamma=new_gamma)
+    hparams = tf.contrib.training.HParams(
-  By default, Gather returns boxes corresponding to the input index list, as
+  By default, gather returns boxes corresponding to the input index list, as
-    self.assertSameElements(boxlist.get_extra_fields(), [])
+    self.assertItemsEqual(boxlist.get_extra_fields(), [])
-    self.assertSameElements(boxlist.get_extra_fields(), ['scores'])
+    self.assertItemsEqual(boxlist.get_extra_fields(), ['scores'])
-    self.assertSameElements(boxlist.get_extra_fields(), ['scores', 'labels'])
+    self.assertItemsEqual(boxlist.get_extra_fields(), ['scores', 'labels'])
-               use_weighted_mean_ap=False):
+               use_weighted_mean_ap=False,
-        self._num_classes,
+        num_groundtruth_classes=self._num_classes,
-    self._metric_prefix = (metric_prefix + '/') if metric_prefix else ''
+    self._metric_prefix = (metric_prefix + '_') if metric_prefix else ''
-      ValueError: On adding groundtruth for an image more than once.
+      ValueError: On adding groundtruth for an image more than once. Will also
-    groundtruth_classes -= self._label_id_offset
+    groundtruth_classes = (
-        groundtruth_is_difficult_list=groundtruth_difficult)
+        image_key=image_id,
-    detection_classes -= self._label_id_offset
+    detection_classes = (
-        detection_classes)
+        image_key=image_id,
-        self._num_classes,
+        num_groundtruth_classes=self._num_classes,
-        metric_prefix='PASCAL',
+        metric_prefix='PascalBoxes',
-        metric_prefix='WeightedPASCAL',
+        metric_prefix='WeightedPascalBoxes',
-    groundtruth_classes -= self._label_id_offset
+    groundtruth_classes = (
-        nms_max_output_boxes)
+        num_groundtruth_classes=num_groundtruth_classes,
-                                         groundtruth_is_group_of_list=None):
+                                         groundtruth_is_group_of_list=None,
-                                     detected_scores, detected_class_labels):
+                                     detected_scores, detected_class_labels,
-            groundtruth_is_difficult_list, groundtruth_is_group_of_list))
+            detected_boxes=detected_boxes,
-        metrics['OpenImagesV2/PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
+        metrics['OpenImagesV2_PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
-        metrics['OpenImagesV2/PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
+        metrics['OpenImagesV2_PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
-    self.assertAlmostEqual(metrics['OpenImagesV2/Precision/mAP@0.5IOU'],
+        metrics['OpenImagesV2_PerformanceByCategory/AP@0.5IOU/cat'], 0.16666666)
-  def test_returns_correct_metric_values(self):
+  def test_returns_correct_metric_values_on_boxes(self):
-        metrics['PASCAL/PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
+        metrics['PascalBoxes_PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
-        metrics['PASCAL/PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
+        metrics['PascalMasks_PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
-    self.assertAlmostEqual(metrics['PASCAL/Precision/mAP@0.5IOU'], 0.05555555)
+        metrics['PascalMasks_PerformanceByCategory/AP@0.5IOU/cat'], 0.16666666)
-  absolute_boxes = tf.map_fn(
+  absolute_boxes = shape_utils.static_or_dynamic_map_fn(
-        position_sensitive_features, [1, 2], keep_dims=True)
+        position_sensitive_features, [1, 2], keepdims=True)
-      groundtruth_is_difficult_lists, groundtruth_is_group_of_list):
+      groundtruth_is_difficult_list, groundtruth_is_group_of_list,
-      groundtruth_is_difficult_lists: A boolean numpy array of length M denoting
+      groundtruth_is_difficult_list: A boolean numpy array of length M denoting
-    detected_boxes, detected_scores, detected_class_labels = (
+    detected_boxes, detected_scores, detected_class_labels, detected_masks = (
-                                   detected_class_labels))
+                                   detected_class_labels, detected_masks))
-        groundtruth_is_difficult_lists, groundtruth_is_group_of_list)
+        detected_boxes=detected_boxes,
-        groundtruth_boxes, groundtruth_class_labels)
+        detected_boxes=detected_boxes,
-                       groundtruth_class_labels):
+                       groundtruth_class_labels, detected_masks=None,
-      detected_scores_at_ith_class = detected_scores[detected_class_labels == i]
+      (gt_boxes_at_ith_class, gt_masks_at_ith_class,
-              gt_boxes_at_ith_class))
+          self._compute_is_class_correctly_detected_in_image(
-      self, detected_boxes, detected_scores, groundtruth_boxes):
+  def _compute_is_class_correctly_detected_in_image(
-        iou = np_box_list_ops.iou(detected_boxlist, gt_boxlist)
+        mask_mode = False
-                     groundtruth_is_group_of_list):
+                     groundtruth_class_labels, groundtruth_is_difficult_list,
-      groundtruth_is_difficult_lists: A boolean numpy array of length M denoting
+      groundtruth_is_difficult_list: A boolean numpy array of length M denoting
-          groundtruth_is_difficult_lists[groundtruth_class_labels == i])
+          groundtruth_is_difficult_list[groundtruth_class_labels == i])
-      detected_scores_at_ith_class = detected_scores[detected_class_labels == i]
+      (gt_boxes_at_ith_class, gt_masks_at_ith_class,
-          groundtruth_is_group_of_list_at_ith_class)
+          detected_boxes=detected_boxes_at_ith_class,
-            detected_class_labels[valid_indices])
+  def _get_overlaps_and_scores_mask_mode(
-      groundtruth_is_difficult_list, groundtruth_is_group_of_list):
+      groundtruth_is_difficult_list, groundtruth_is_group_of_list,
-    scores = detected_boxlist.get_field('scores')
+    mask_mode = False
-      return scores, np.zeros(detected_boxlist.num_boxes(), dtype=bool)
+      return scores, np.zeros(num_detected_boxes, dtype=bool)
-        detected_boxlist.num_boxes(), dtype=bool)
+    tp_fp_labels = np.zeros(num_detected_boxes, dtype=bool)
-    if gt_non_group_of_boxlist.num_boxes() > 0:
+    if iou.shape[1] > 0:
-      for i in range(detected_boxlist.num_boxes()):
+      is_gt_box_detected = np.zeros(iou.shape[1], dtype=bool)
-      ioa = np_box_list_ops.ioa(gt_group_of_boxlist, detected_boxlist)
+    if ioa.shape[0] > 0:
-      for i in range(detected_boxlist.num_boxes()):
+      for i in range(num_detected_boxes):
-  def test_match_to_not_difficult_box(self):
+    groundtruth_masks_0 = np.array([[1, 1, 0, 0],
-  def test_match_to_difficult_box(self):
+  def test_mask_match_to_gt_mask_0(self):
-    matching_iou_threshold2 = 0.1
+    matching_iou_threshold_high_iou = 0.5
-        nms_max_output_boxes)
+    self.eval_high_iou = per_image_evaluation.PerImageEvaluation(
-        nms_max_output_boxes)
+    self.eval_low_iou = per_image_evaluation.PerImageEvaluation(
-    scores, tp_fp_labels = self.eval1._compute_tp_fp_for_single_class(
+    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(
-    scores, tp_fp_labels = self.eval1._compute_tp_fp_for_single_class(
+    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(
-    scores, tp_fp_labels = self.eval2._compute_tp_fp_for_single_class(
+    scores, tp_fp_labels = self.eval_low_iou._compute_tp_fp_for_single_class(
-    scores, tp_fp_labels = self.eval1._compute_tp_fp_for_single_class(
+    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(
-  dynamic_shape = tf.shape(tensor)
+  static_tensor_shape = tensor.shape.as_list()
-  for index, dim in enumerate(static_shape):
+  for index, dim in enumerate(static_tensor_shape):
-      combined_shape.append(dynamic_shape[index])
+      combined_shape.append(dynamic_tensor_shape[index])
-        image_features)
+        image_feature)
-    zero = tf.reduce_sum(0 * image_features)
+    zero = tf.reduce_sum(0 * image_feature)
-def get_variables_available_in_checkpoint(variables, checkpoint_path):
+def get_variables_available_in_checkpoint(variables,
-  ckpt_vars = ckpt_reader.get_variable_to_shape_map().keys()
+  ckpt_vars_to_shape_map = ckpt_reader.get_variable_to_shape_map()
-      vars_in_ckpt[variable_name] = variable
+    if variable_name in ckpt_vars_to_shape_map:
-      logging.warning('Variable [%s] not available in checkpoint',
+      logging.warning('Variable [%s] is not available in checkpoint',
-        tf.Variable(1.0, name='weights'),
+        weight_variable,
-    self.assertItemsEqual(out_variables, graph1_variables)
+        graph2_variables, checkpoint_path, include_global_step=False)
-import matplotlib.pyplot as plt
+# Set headless-friendly backend.
-    ymin: ymin of bounding box in normalized coordinates (same below).
+    ymin: ymin of bounding box.
-  """Draws bounding boxes on batch of image tensors.
+  """Draws bounding boxes, masks, and keypoints on batch of image tensors.
-      line_thickness=4)
+  visualization_keyword_args = {
-  def draw_boxes(image_boxes_classes_scores):
+  def draw_boxes(image_and_detections):
-                                  [image, boxes, classes, scores], tf.uint8)
+    image_with_boxes = tf.py_func(visualize_boxes_fn, image_and_detections,
-      back_prop=False)
+  images = tf.map_fn(draw_boxes, elems, dtype=tf.uint8, back_prop=False)
-def draw_mask_on_image_array(image, mask, color='red', alpha=0.7):
+def draw_mask_on_image_array(image, mask, color='red', alpha=0.4):
-    alpha: transparency value between 0 and 1. (default: 0.7)
+    alpha: transparency value between 0 and 1. (default: 0.4)
-                                              line_thickness=4):
+def visualize_boxes_and_labels_on_image_array(
-      be None
+    instance_masks: a numpy array of shape [N, image_height, image_width] with
-        box_to_color_map[box] = 'black'
+        box_to_color_map[box] = groundtruth_box_visualization_color
-            class_name = category_index[classes[i]]['name']
+        display_str = ''
-          display_str = 'score: {}%'.format(int(100 * scores[i]))
+            display_str = '{}: {}%'.format(display_str, int(100*scores[i]))
-        1, height, width, 3)
+        1, int(height), int(width), 3)
-          print('Writing output image %d to %s' % (i, output_file))
+          print 'Writing output image %d to %s' % (i, output_file)
-                    labels=tf.argmax(labels, axis=1),
+                    labels=labels,
-    return tf.one_hot(label, 10)
+  def decode_label(label):
-      labels_file, 1, header_bytes=8).map(one_hot_label)
+      labels_file, 1, header_bytes=8).map(decode_label)
-    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
+    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
-        labels=tf.argmax(labels, axis=1), predictions=tf.argmax(logits, axis=1))
+        labels=labels, predictions=tf.argmax(logits, axis=1))
-    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
+    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
-  return image, tf.one_hot(labels, 10)
+  labels = tf.random_uniform([BATCH_SIZE, 1], maxval=9, dtype=tf.int32)
-      labels=tf.argmax(labels, axis=1), predictions=tf.argmax(logits, axis=1))
+      labels=labels, predictions=tf.argmax(logits, axis=1))
-  loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
+  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
-    return tf.to_int32(label)
+  def one_hot_label(label):
-      labels_file, 1, header_bytes=8).map(decode_label)
+      labels_file, 1, header_bytes=8).map(one_hot_label)
-    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
+    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
-        labels=labels, predictions=tf.argmax(logits, axis=1))
+        labels=tf.argmax(labels, axis=1), predictions=tf.argmax(logits, axis=1))
-    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
+    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
-  return image, labels
+  labels = tf.random_uniform([BATCH_SIZE], maxval=9, dtype=tf.int32)
-      labels=labels, predictions=tf.argmax(logits, axis=1))
+      labels=tf.argmax(labels, axis=1), predictions=tf.argmax(logits, axis=1))
-  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
+  loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
-                                         wd=0.0)
+                                         wd=None)
-                                         wd=0.0)
+                                         wd=None)
-                                          stddev=1/192.0, wd=0.0)
+                                          stddev=1/192.0, wd=None)
-  with tf.gfile.FastGFile(filename, 'r') as f:
+  with tf.gfile.FastGFile(filename, 'rb') as f:
-  seq_len = tf.shape(sequence_parse[v])[0]
+  seq_len = tf.shape(sequence_parse[view_names[-1]])[0]
-      model_fn=model_fn,
+      model_fn=model_function,
-          'data_format': data_format
+          'data_format': data_format,
-      help='The directory where the exported SavedModel will be stored.')
+class MNISTArgParser(argparse.ArgumentParser):
-  def mnist_model_fn_helper(self, mode):
+  def mnist_model_fn_helper(self, mode, multi_gpu=False):
-        'data_format': 'channels_last'
+        'data_format': 'channels_last',
-import src.rotation_utils as ru 
+import src.rotation_utils as ru
-  with fopen(file_name, 'r') as f:
+  with fopen(image_path, 'r') as f:
-import graph_tool.generation 
+import graph_tool.generation
-  
+
-  
+
-  # edge_wts.get_array()[:] = d*wts 
+  # edge_wts.get_array()[:] = d*wts
-  Returns: 
+  Returns:
-    class_maps_one_hot[:,:,i] = class_maps__ == i 
+    class_maps_one_hot[:,:,i] = class_maps__ == i
-  
+
-def downloadDataset(url, file):
+def download_dataset(url, file):
-downloadDataset(URL_TEST, FILE_TEST)
+download_dataset(URL_TRAIN, FILE_TRAIN)
-# Train our model, use the previously function my_input_fn
+# Train our model, use the previously defined function my_input_fn
-    return tf.one_hot(label, 10)
+  def decode_label(label):
-      labels_file, 1, header_bytes=8).map(one_hot_label)
+      labels_file, 1, header_bytes=8).map(decode_label)
-    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
+    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
-        labels=tf.argmax(labels, axis=1), predictions=tf.argmax(logits, axis=1))
+        labels=labels, predictions=tf.argmax(logits, axis=1))
-    loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
+    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
-  return image, tf.one_hot(labels, 10)
+  labels = tf.random_uniform([BATCH_SIZE, 1], maxval=9, dtype=tf.int32)
-      labels=tf.argmax(labels, axis=1), predictions=tf.argmax(logits, axis=1))
+      labels=labels, predictions=tf.argmax(logits, axis=1))
-  loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
+  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
-      model_fn=model_function,
+      model_fn=model_fn,
-          'multi_gpu': FLAGS.multi_gpu
+          'data_format': data_format
-  parser = MNISTArgParser()
+  parser = argparse.ArgumentParser()
-  def mnist_model_fn_helper(self, mode, multi_gpu=False):
+  def mnist_model_fn_helper(self, mode):
-        'multi_gpu': multi_gpu
+        'data_format': 'channels_last'
-      action_latex = ['$\odot$ ', '$\curvearrowright$ ', '$\curvearrowleft$ ', '$\Uparrow$ ']
+      action_latex = ['$\odot$ ', '$\curvearrowright$ ', '$\curvearrowleft$ ', r'$\Uparrow$ ']
-    with file(os.path.join(root_dir, 'testdata', spec_path), 'r') as fin:
+    with open(os.path.join(root_dir, 'testdata', spec_path), 'r') as fin:
-    with file(os.path.join(testdata, spec_path), 'r') as fin:
+    with open(os.path.join(testdata, spec_path), 'r') as fin:
-    with file(os.path.join(FLAGS.test_tmpdir, 'tag-to-category'), 'r') as f:
+    with open(os.path.join(FLAGS.test_tmpdir, 'tag-to-category'), 'r') as f:
-    with file(os.path.join(FLAGS.test_tmpdir, map_name), 'r') as f:
+    with open(os.path.join(FLAGS.test_tmpdir, map_name), 'r') as f:
-  if args.num_gpus>0:
+  if args.num_gpus > 0:
-import render.swiftshader_renderer as renderer 
+from datasets import factory
-  
+
-  
+
-  print image.shape
+  print(image.shape)
- 
+
-  
+
-  
+    root.destroy()
-  
+
-  
+
-  
+
-  
+
-  root.mainloop() 
+  root.mainloop()
-  CUDA_VISIBLE_DEVICES=0 LD_LIBRARY_PATH=/opt/cuda-8.0/lib64:/opt/cudnnv51/lib64 
+  CUDA_VISIBLE_DEVICES=0 LD_LIBRARY_PATH=/opt/cuda-8.0/lib64:/opt/cudnnv51/lib64
-from tensorflow.python.platform import flags 
+from tensorflow.python.platform import app
-import scripts.script_nav_agent_release as sna 
+import scripts.script_nav_agent_release as sna
-from src import graph_utils 
+from src import graph_utils
-    gt_dist_to_goal = [e.episode.dist_to_goal[0][j][s] 
+    gt_dist_to_goal = [e.episode.dist_to_goal[0][j][s]
-  
+
-  
+
-  writer = Writer(fps=3*(samples_per_action+wait_at_action), 
+  writer = Writer(fps=3*(samples_per_action+wait_at_action),
-  
+
-  
+
-  
+
-    
+
-  
+
-    
+
-          "Top corners show diagnostics (distance, agents' action) not input to agent.", 
+          "First Person View\n" +
-      
+
-      plt.setp(tt, color='white') 
+      plt.setp(tt, color='white')
-    print out_file_name
+    print(out_file_name)
-  
+
-    locs = np.concatenate((dt['all_locs'][i,:,:], 
+    locs = np.concatenate((dt['all_locs'][i,:,:],
-    ax.plot(dt['all_locs'][i, 0, 0], 
+    ax.plot(dt['all_locs'][i, 0, 0],
-    ax.plot(dt['all_goal_locs'][i, 0, 0], 
+    ax.plot(dt['all_goal_locs'][i, 0, 0],
-               c=5+np.arange(all_locs.shape[0])*1./all_locs.shape[0], 
+               c=5+np.arange(all_locs.shape[0])*1./all_locs.shape[0],
-    
+
-    with fu.fopen(file_name, 'w') as f: 
+    print(file_name)
-  
+
-  
+
-  
+
-    print string
+    print(string)
-      print string
+      print(string)
-  z = np.zeros((1,1)) 
+  z = np.zeros((1,1))
-    print '\nInput codes not found.\n'
+    print('\nInput codes not found.\n')
-          print np_tensors['code_length']
+          print(np_tensors['code_length'])
-      print "Got ValueError in math.log for values :" + str((q, priv_eps, l, t))
+      print("Got ValueError in math.log for values :" + str((q, priv_eps, l, t)))
-    print "l too large to compute sensitivity"
+    print("l too large to compute sensitivity")
-  print "Smoothed sensitivities (Noisy Max): " + str(total_ss_nm / l_list)
+  print("Epsilons (Noisy Max): " + str(eps_list_nm))
-  print "Epsilon = " + str(min(eps_list_nm)) + "."
+  print("To get an " + str(ss_eps) + "-DP estimate of epsilon, ")
-    print "Warning: May not have used enough values of l"
+    print("Warning: May not have used enough values of l")
-  print "Data independent bound = " + str(min(data_ind_eps_list)) + "."
+  print("Data independent bound = " + str(min(data_ind_eps_list)) + ".")
-    print "A: by binomial expansion    {} = {} + {}".format(
+    print("A: by binomial expansion    {} = {} + {}".format(
-        q * a_lambda_second_term_exact)
+        q * a_lambda_second_term_exact))
-    print "f(-M) = {} f(M) = {}".format(b_fn(-m), b_fn(m))
+    print("M =", m)
-  print b_lambda, b_bound
+    print("B: by numerical integration", b_lambda)
-    print "A: by numerical integration {} = {} + {}".format(
+    print("A: by numerical integration {} = {} + {}".format(
-        q * a_lambda_second_term)
+        q * a_lambda_second_term))
-    print "f(-M) = {} f(M) = {}".format(b_fn(-m), b_fn(m))
+    print("M =", m)
-    print "B must be no more than    ", b_bound
+    print("B by numerical integration", b_lambda)
-            def decoder_loop_fn((state, prev_cell_out, _), (cell_inp, cur_tgt)):
+            def decoder_loop_fn(state__prev_cell_out__unused, cell_inp__cur_tgt):
-            print "  reading data line %d" % counter
+            print("  reading data line %d" % counter)
-    print "  Finished global data reading (%d)." % train_total_size
+    print("  Finished global data reading (%d)." % train_total_size)
-    print "target: ", tgt_prog
+    print("target: ", tgt_prog)
-        print "AAAAA"
+        print([program_utils.prog_vocab[x] for x in ilist if x > 0])
-    print "best score: ", best_score, " best prog: ", best_prog
+    print("best score: ", best_score, " best prog: ", best_prog)
-            print scores
+            print(scores)
-          print "seq_err: ", seq_err
+          print("seq_err: ", seq_err)
-    print total
+      print(v.name, shape, mul(shape))
-      print [rev_en_vocab[t] for t in token_ids]
+      print([rev_en_vocab[t] for t in token_ids])
-            print linearize(outputs, rev_fr_vocab)
+            print([rev_fr_vocab[t] for t in outputs])
-            print linearize(cures, rev_fr_vocab)
+            print(cures)
-        print linearize(result, rev_fr_vocab)
+        print("FINAL", result_cost)
-        print "TOOO_LONG"
+        print("TOOO_LONG")
-  
+
-    self.name = name 
+    self.name = name
-def sq(x): return x**2 
+def sq(x): return x**2
-def pos(x): return x > 0 
+# Int -> Bool
-                        ListType("Int"), 
+f_map = Function("map", [ListType("Int")],
-                              ListType("Int"), 
+f_filter = Function("filter", [ListType("Int")],
-                              "Int", 
+f_count = Function("c_count", [ListType("Int")],
-                                  ListType("Int"), 
+f_zipwith = Function("c_zipwith", [ListType("Int"), ListType("Int")],
-                            ListType("Int"), 
+                            ListType("Int"),
-      exec exec_str + " print(out)"
+      exec(exec_str + " print(out)")
-  
+
-      exec inp_str + self.body + "; print(out)"
+      exec(inp_str + self.body + "; print(out)")
-      print "== proggen: kept:  " + str(len(outcomes_to_programs))
+      print("== proggen: tried: " + str(counter))
-      print "saving..."
+      print("saving...")
-            print "saving %d of %d" % (save_counter, len(outcomes_to_programs))
+            print("saving %d of %d" % (save_counter, len(outcomes_to_programs)))
-    print "Creating directory %s" % directory
+    print("Creating directory %s" % directory)
-    print "Downloading %s to %s" % (url, filepath)
+    print("Downloading %s to %s" % (url, filepath))
-    print "Successfully downloaded", filename, statinfo.st_size, "bytes"
+    print("Successfully downloaded", filename, statinfo.st_size, "bytes")
-  print "Unpacking %s to %s" % (gz_path, new_path)
+  print("Unpacking %s to %s" % (gz_path, new_path))
-    print "Extracting tar file %s" % corpus_file
+    print("Extracting tar file %s" % corpus_file)
-    print "Extracting tgz file %s" % dev_file
+    print("Extracting tgz file %s" % dev_file)
-    print "Creating vocabulary %s from data %s" % (vocabulary_path, data_path)
+    print("Creating vocabulary %s from data %s" % (vocabulary_path, data_path))
-          print "  processing fr line %d" % counter
+          print("  processing fr line %d" % counter)
-          print "  processing en line %d" % counter
+          print("  processing en line %d" % counter)
-    print "Tokenizing data in %s" % data_path
+    print("Tokenizing data in %s" % data_path)
-            print "  tokenizing line %d" % counter
+            print("  tokenizing line %d" % counter)
-      utility.entry_match_token], utility.entry_match_token_id
+  print("entry match token: ", utility.word_ids[
-      utility.column_match_token], utility.column_match_token_id
+  print("entry match token: ", utility.word_ids[
-      print "step: ", curr_pass
+      print("step: ", curr_pass)
-    self.total_cost = self.compute_error() 
+    self.total_cost = self.compute_error()
-    print "optimize params ", optimize_names
+    print("optimize params ", optimize_names)
-      print "grads: ", p, name
+      print("grads: ", p, name)
-    self.step = adam.apply_gradients(zip(grads, optimize_params), 
+    self.step = adam.apply_gradients(zip(grads, optimize_params),
-  print "--------"
+  print("dev set accuracy   after ", i, " : ", gc / num_examples)
-      print "step ", i, " ", time_taken, " seconds "
+      print("step ", i, " ", time_taken, " seconds ")
-      print " printing train set loss: ", train_set_loss / utility.FLAGS.eval_cycle
+      print(" printing train set loss: ", train_set_loss / utility.FLAGS.eval_cycle)
-  batch_size = utility.FLAGS.batch_size 
+  batch_size = utility.FLAGS.batch_size
-	print "list of models: ", file_list
+        print("list of models: ", file_list)
-          print "restoring: ", model_file
+          print("restoring: ", model_file)
-          print "evaluating on dev ", model_file, model_step
+          print("evaluating on dev ", model_file, model_step)
-      print "model dir: ", model_dir
+      print("model dir: ", model_dir)
-        print "create dir: ", utility.FLAGS.output_dir
+        print("create dir: ", utility.FLAGS.output_dir)
-        print "create dir: ", model_dir
+        print("create dir: ", model_dir)
-  print "running open source"
+  print("# train examples ", len(train_data))
-          print "forget gate bias"
+          print("forget gate bias")
-  string = re.sub(ur'[\u2E00-\uFFFF]', "", string)
+  string = re.sub(r'[\u2E00-\uFFFF]', "", string)
-  string = re.sub(ur'[\u007F-\uFFFF]', "", string.strip())
+  string = re.sub(r'[\u007F-\uFFFF]', "", string.strip())
-    self.root_folder = root_folder   
+    self.root_folder = root_folder
-    print "Annotated examples loaded ", len(self.annotated_examples)
+    print("Annotated examples loaded ", len(self.annotated_examples))
-          print 'Writing output image %d to %s' % (i, output_file)
+          print('Writing output image %d to %s' % (i, output_file))
-            print example_idx, "/", num_examples
+            print(example_idx, "/", num_examples)
-            print "Writing on:", file_out
+            print("Writing on:", file_out)
-            print example_idx, "/", num_examples
+            print(example_idx, "/", num_examples)
-            print "Writing on:", file_out
+            print("Writing on:", file_out)
-            print example_idx, "/", num_examples
+            print(example_idx, "/", num_examples)
-            print progress_bar,
+            print(progress_bar, end=' ')
-        print ""
+        print("")
-                print "Loading file %s" % ckpt_state.model_checkpoint_path
+                print("Loading file %s" % ckpt_state.model_checkpoint_path)
-                                        examples_per_sec, duration)
+                    print(format_str % (datetime.now(), global_step_val, loss,
-                        print "No model to eval yet at %s" % traindir
+                        print("No model to eval yet at %s" % traindir)
-                    print "Loading file %s" % ckpt_state.model_checkpoint_path
+                    print("Loading file %s" % ckpt_state.model_checkpoint_path)
-                        print "Waiting for the checkpoint to be updated."
+                        print("Waiting for the checkpoint to be updated.")
-                    print "Evaluating..."
+                    print("Evaluating...")
-                    print "Writing summary..."
+                    print("Epoch: %d, %s -> %.3f bits/dim"
-                            print "No model to eval yet at %s" % traindir
+                            print("No model to eval yet at %s" % traindir)
-                        print "Waiting for the checkpoint to be updated."
+                        print("Waiting for the checkpoint to be updated.")
-          print 'Error rates=', rates
+          print('Error rates=', rates)
-  print 'vocabulary contains %d tokens' % num_words
+  print('vocabulary contains %d tokens' % num_words)
-  print 'done!'
+  print('done!')
-except GetoptError, e:
+except GetoptError as e:
-except KeyError, e:
+except KeyError as e:
-        print 'Parse:'
+        print('Input: %s' % sentence.text)
-          print pat.sub('', tr_ln)
+          print(pat.sub('', tr_ln))
-        print 'alignment so far %f' % alignment
+        print('alignment so far %f' % alignment)
-  print 'Average alignment %f' % average_alignment
+  print('Average alignment %f' % average_alignment)
-      raw_input('Press Enter to continue...')
+      input('Press Enter to continue...')
-        shard_idx = long(image_id, 16) % FLAGS.num_shards
+        shard_idx = int(image_id, 16) % FLAGS.num_shards
-  if isinstance(pad_axes, (int, long)):
+  if isinstance(pad_axes, six.integer_types):
-    if isinstance(dims, (int, long)):
+    if isinstance(dims, six.integer_types):
-    if isinstance(dims, (int, long)):
+    if isinstance(dims, six.integer_types):
-  if depth < 0 or not isinstance(depth, (int, long) if six.PY2 else int):
+  if depth < 0 or not isinstance(depth, six.integer_types):
-  if left_pad < 0 or not isinstance(left_pad, (int, long) if six.PY2 else int):
+  if left_pad < 0 or not isinstance(left_pad, six.integer_types):
-  with tf.gfile.Open(filename,'rb') as f:
+  with tf.gfile.Open(filename, 'rb') as f:
-  with tf.gfile.Open(filename,'rb') as f:
+  with tf.gfile.Open(filename, 'rb') as f:
-  """Download (and unzip) a file from the MNIST dataset, if it doesn't already exist."""
+  """Download (and unzip) a file from the MNIST dataset if not already done."""
-      model_fn=model_fn,
+      model_fn=model_function,
-          'data_format': data_format
+          'data_format': data_format,
-      help='The directory where the exported SavedModel will be stored.')
+class MNISTArgParser(argparse.ArgumentParser):
-  def mnist_model_fn_helper(self, mode):
+  def mnist_model_fn_helper(self, mode, multi_gpu=False):
-        'data_format': 'channels_last'
+        'data_format': 'channels_last',
-
+from six.moves import xrange
-
+from six.moves import xrange
-
+from six.moves import xrange
-
+from six.moves import xrange
-
+from six.moves import xrange
-
+from six.moves import xrange
-
+from six.moves import xrange
-                    """Path to directory with checkpoints of model 
+                    """Path to directory with checkpoints of model
-                    you would like to restore, you would point it to 
+                    checkpoint that is provided with the code
-
+from six.moves import xrange
-  
+
-  
+
-  
+
- 
+
-
+from six.moves import xrange
-
+from six.moves import xrange
-    tf.gfile.MakeDirs(directory)
+  if not tf.gfile.Exists(directory):
-  estimator.evaluate(input_fn=eval_input_fn, steps=FLAGS.eval_steps)
+  if FLAGS.eval_steps:
-    self.model.setup()
+  def setup(self, train=True):
-  def sample_episodes(self, sess):
+  def sample_episodes(self, sess, greedy=False):
-       pads) = self._sample_episodes(sess)
+       pads) = self._sample_episodes(sess, greedy=greedy)
-          avg_episode_reward=np.mean(self.episode_rewards))
+          avg_episode_reward=avg_episode_reward,
-          avg_episode_reward=np.mean(self.episode_rewards))
+          avg_episode_reward=avg_episode_reward,
-      eps_lambda = find_best_eps_lambda(episode_rewards, episode_lengths)
+      eps_lambda = find_best_eps_lambda(
-     pads) = self._sample_episodes(sess, greedy=True)
+     pads, terminated) = self.sample_episodes(sess, greedy=True)
-    return np.mean(total_rewards)
+    return total_rewards, self.episode_rewards
-      ep_obs = [obs[:length, i, ...] for obs in observations]
+      ep_obs = [obs[:length + 1, i, ...] for obs in observations]
-          entropies, logits):
+          entropies, logits,
-  def setup(self):
+  def setup(self, train=True):
-            self.regression_input, self.regression_weight)
+      if train:
-                 avg_episode_reward=0):
+                 avg_episode_reward=0, greedy_episode_reward=0):
-                 self.avg_episode_reward: avg_episode_reward}
+                 self.avg_episode_reward: avg_episode_reward,
-                        avg_episode_reward=0):
+                        avg_episode_reward=0,
-                 self.avg_episode_reward: avg_episode_reward}
+                 self.avg_episode_reward: avg_episode_reward,
-          entropies, logits):
+          entropies, logits,
-               eps_lambda=0.0, clip_adv=None):
+               eps_lambda=0.0, clip_adv=None,
-        'eps_lambda', [], initializer=tf.constant_initializer(eps_lambda))
+        'eps_lambda', [], initializer=tf.constant_initializer(eps_lambda),
-        0.95 * self.eps_lambda + 0.05 * self.new_eps_lambda)
+        0.99 * self.eps_lambda + 0.01 * self.new_eps_lambda)
-          entropies, logits):
+          entropies, logits,
-                               final_values)
+    if self.use_target_values:
-          entropies, logits):
+          entropies, logits,
-                               final_values)
+
-          entropies, logits):
+          entropies, logits,
-                               final_values)
+
-    idxs = np.random.choice(self.cur_size, size=n, replace=False, p=p)
+    idxs = np.random.choice(self.cur_size, size=int(n), replace=False, p=p)
-    assert self.value_opt is None or self.critic_weight == 0.0
+    assert self.value_opt is None or self.value_opt == 'None' or \
-        FLAGS.save_trajectories_dir or FLAGS.save_dir)
+    self.save_trajectories_dir = FLAGS.save_trajectories_dir
-                 eps_lambda=self.eps_lambda, clip_adv=self.clip_adv)
+                 eps_lambda=self.eps_lambda, clip_adv=self.clip_adv,
-  def get_controller(self):
+  def get_controller(self, env):
-    return cls(self.env, self.env_spec, self.internal_dim,
+    return cls(env, self.env_spec, self.internal_dim,
-               use_value_opt=self.value_opt is not None,
+               use_value_opt=self.value_opt not in [None, 'None'],
-          saver.restore(sess, FLAGS.load_path)
+        saver.restore(sess, FLAGS.load_path)
-        self.global_step = tf.train.get_or_create_global_step()
+        self.global_step = tf.contrib.framework.get_or_create_global_step()
-        self.controller = self.get_controller()
+        self.controller = self.get_controller(self.env)
-      self.controller = self.get_controller()
+      self.global_step = tf.contrib.framework.get_or_create_global_step()
-      if random.random() < 1 and is_chief and sv and sv._summary_writer:
+      if (random.random() < 0.1 and summary and episode_rewards and
-                     'episode rewards: %f',
+                     'episode rewards: %f, greedy rewards: %f',
-                     np.mean(all_ep_rewards))
+                     np.mean(all_ep_rewards),
-         'This will take a few minutes.' % min_queue_examples)
+  with tf.name_scope('data_augmentation'):
-  filename_queue = tf.train.string_input_producer(filenames)
+  with tf.name_scope('input'):
-  reshaped_image = tf.cast(read_input.uint8image, tf.float32)
+    # Read examples from files in the filename queue.
-  width = IMAGE_SIZE
+    height = IMAGE_SIZE
-                                                         height, width)
+    # Image processing for evaluation.
-  float_image = tf.image.per_image_standardization(resized_image)
+    # Subtract off the mean and divide by the variance of the pixels.
-  read_input.label.set_shape([1])
+    # Set the shapes of tensors.
-                           min_fraction_of_examples_in_queue)
+    # Ensure that the random shuffling has good mixing properties.
-  with tf.gfile.Open(filename) as f:
+  with tf.gfile.Open(filename,'rb') as f:
-  with tf.gfile.Open(filename) as f:
+  with tf.gfile.Open(filename,'rb') as f:
-tf.flags.DEFINE_integer("batch_size", 128,
+tf.flags.DEFINE_integer("batch_size", 1024,
-    def env_step(action):
+    def env_step(env, action):
-    outputs = [env_step(action)
+    outputs = [env_step(env, action)
-          layout_optimizer=1)
+          layout_optimizer=rewriter_config_pb2.RewriterConfig.ON)
-        label = parsed_line[-1:]  # Last element is the label
+        label = parsed_line[-1]  # Last element is the label
-  with open(filename) as f:
+  with tf.gfile.Open(filename) as f:
-  with open(filename) as f:
+  with tf.gfile.Open(filename) as f:
-                  FLAGS.image_path_pattern):
+  predictions = run(FLAGS.checkpoint, FLAGS.batch_size, FLAGS.dataset_name,
-to retrain (or at least fine-tune) it using images from that distribution. 
+NOTE #1: The Attention OCR model was trained only using FSNS train dataset and
-import model as attention_ocr
+import data_provider
-                                  dtype='float32')
+                                  dtype='uint8')
-def load_model(checkpoint, batch_size, dataset_name):
+def create_model(batch_size, dataset_name):
-  return images_placeholder, endpoints, init_fn
+    num_char_classes=dataset.num_char_classes,
-  for line in predictions:
+  for line in run(FLAGS.checkpoint, FLAGS.batch_size, FLAGS.dataset_name,
-        mapping=mapping_strings, default_value=default_character)
+      mapping=mapping_strings, default_value=default_character)
-        self.table.lookup(tf.to_int64(ids)), reduction_indices=1)
+      self.table.lookup(tf.to_int64(ids)), reduction_indices=1)
-          logits=logits, labels=labels))
+        logits=logits, labels=labels))
-          logits=logits, labels=labels)
+        logits=logits, labels=labels)
-      charset=None):
+               num_char_classes,
-        null_code=null_code)
+      num_char_classes=num_char_classes,
-            lstm_state_clip_value=10.0),
+          use_attention=True,
-            average_across_timesteps=False),
+          label_smoothing=0.1,
-                images, final_endpoint=mparams.final_endpoint)
+      with slim.arg_scope(inception.inception_v3_arg_scope()):
-          merged_net, kernel_size=[len(nets_list), 1], stride=1)
+        merged_net, kernel_size=[len(nets_list), 1], stride=1)
-        slim.one_hot_encoding(ids, self._params.num_char_classes), tf.bool)
+      slim.one_hot_encoding(ids, self._params.num_char_classes), tf.bool)
-      reuse=None):
+                  images,
-          value=images, num_or_size_splits=self._params.num_views, axis=2)
+        value=images, num_or_size_splits=self._params.num_views, axis=2)
-        predicted_text=predicted_text)
+      chars_logit=chars_logit,
-        chars_labels, depth=self._params.num_char_classes, axis=-1)
+      chars_labels, depth=self._params.num_char_classes, axis=-1)
-            chars_labels, mparams.label_smoothing)
+          chars_labels, mparams.label_smoothing)
-            dtype=tf.int64)
+          self._params.num_char_classes - 1,
-          average_across_timesteps=mparams.average_across_timesteps)
+        logits_list,
-          sname('image/orig'), data.images_orig, max_outputs=max_outputs)
+        sname('image/orig'), data.images_orig, max_outputs=max_outputs)
-                     rej_char=self._params.null_code))
+                   endpoints.predicted_chars,
-                     rej_char=self._params.null_code))
+                   endpoints.predicted_chars,
-      inception_checkpoint=None):
+                                inception_checkpoint=None):
-          'AttentionOcr_v1/conv_tower_fn/INCE', strip_scope=True)
+        'AttentionOcr_v1/conv_tower_fn/INCE', strip_scope=True)
-parser = argparse.ArgumentParser()
+FLAGS = tf.app.flags.FLAGS
-FLAGS = parser.parse_args()
+tf.app.flags.DEFINE_integer('batch_size', 128,
-                    help='Whether to run eval only once.')
+FLAGS = tf.app.flags.FLAGS
-parser = cifar10.parser
+FLAGS = tf.app.flags.FLAGS
-                    help='Whether to log device placement.')
+tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',
-parser = cifar10.parser
+FLAGS = tf.app.flags.FLAGS
-                    help='How often to log results to the console.')
+tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',
-          optimize_tensor_layout=True)
+          layout_optimizer=1)
-def maybe_download(directory, filename):
+def download(directory, filename):
-      filepath, 'wb') as f_out:
+  zipped_filepath = filepath + '.gz'
-  labels_file = maybe_download(directory, labels_file)
+  images_file = download(directory, images_file)
-  """Download a file from the MNIST dataset, if it doesn't already exist."""
+  """Download (and unzip) a file from the MNIST dataset, if it doesn't already exist."""
-  return tf.data.Dataset.from_tensors((data.images, data.labels))
+import dataset
-    dataset = dataset.shuffle(buffer_size=50000).batch(FLAGS.batch_size).repeat(
+    ds = dataset.train(FLAGS.data_dir)
-    (images, labels) = dataset.make_one_shot_iterator().get_next()
+    (images, labels) = ds.make_one_shot_iterator().get_next()
-    return eval_dataset(FLAGS.data_dir).make_one_shot_iterator().get_next()
+    return dataset.test(FLAGS.data_dir).batch(
-      with tf.gfile.GFile(filename, "r") as f:
+      with tf.gfile.GFile(filename, "rb") as f:
-  for i in xrange(num_eval_batches):
+  for i in range(num_eval_batches):
-    except Exception, e:  # pylint: disable=broad-except
+    except Exception as e:  # pylint: disable=broad-except
-
+  parser = argparse.ArgumentParser()
-#   (https://goo.gl/Ujm2Ep)
+# https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html
-    # between Iris Sentosa, Versicolor, and Viginica
+    # between Iris Setosa, Versicolor, and Viginica
-    # is Iris Sentosa, Vericolor, Virginica, respectively.
+    # is Iris Setosa, Vericolor, Virginica, respectively.
-                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa
+                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Setosa
-        tf.logging.info("...I think: {}, is Iris Sentosa".format(prediction_input[idx]))
+        tf.logging.info("...I think: {}, is Iris Setosa".format(prediction_input[idx]))
-    self.dropout = tf.layers.Dropout(0.5)
+    self.dropout = tf.layers.Dropout(0.4)
-
+  image = features
-    return predict_spec(model, features)
+    logits = model(image, training=False)
-    return train_spec(model, features, labels)
+    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)
-    return eval_spec(model, features, labels)
+    logits = model(image, training=False)
-        'image': tf.placeholder(tf.float32, [None, 28, 28])
+        'image': image,
-    logits = tf.layers.Dense(3, activation=tf.nn.relu)(h2)
+    logits = tf.layers.Dense(3)(h2)
-    from urllib.request import urlopen
+import six.moves.urllib.request as request
-        data = urlopen(url).read()
+        data = request.urlopen(url).read()
-        label = parsed_line[-1:]  # Last element is the label
+        label = parsed_line[-1]  # Last element is the label
-    labels = tf.squeeze(labels, 1)          # Convert to shape [batch_size]
+    # Calculate the loss
-
+class Model(object):
-      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')
+      'classes': tf.argmax(logits, axis=1),
-                                      export_outputs=export_outputs)
+def train_spec(model, image, labels):
-  # Create a tensor named train_accuracy for logging purposes
+      labels=tf.argmax(labels, axis=1), predictions=tf.argmax(logits, axis=1))
-      predictions=predictions,
+      mode=tf.estimator.ModeKeys.EVAL,
-      eval_metric_ops=metrics)
+      eval_metric_ops={
-  # Create the Estimator
+  data_format = FLAGS.data_format
-      model_fn=mnist_model_fn,
+      model_fn=model_fn,
-          'data_format': FLAGS.data_format
+          'data_format': data_format
-
+  # Set up training hook that logs the training accuracy every 100 steps.
-    mnist_classifier.export_savedmodel(FLAGS.export_dir, serving_input_fn)
+    image = tf.placeholder(tf.float32, [None, 28, 28])
-tf.logging.set_verbosity(tf.logging.ERROR)
+BATCH_SIZE = 100
-class BaseTest(tf.test.TestCase):
+def dummy_input_fn():
-        features, labels, mode, {'data_format': 'channels_last'})
+def make_estimator():
-      self.assertEqual(loss.dtype, tf.float32)
+class Tests(tf.test.TestCase):
-      self.assertEqual(eval_metric_ops['accuracy'][1].dtype, tf.float32)
+  def test_mnist(self):
-    self.mnist_model_fn_helper(tf.estimator.ModeKeys.TRAIN)
+    loss = eval_results['loss']
-    self.mnist_model_fn_helper(tf.estimator.ModeKeys.EVAL)
+    input_fn = lambda: tf.random_uniform([3, 784])
-    self.mnist_model_fn_helper(tf.estimator.ModeKeys.PREDICT)
+
-    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)
+    export_outputs={'classify': tf.estimator.export.PredictOutput(predictions)}
-    masks_flattened = np.reshape(mask_stack, [-1])
+    mask_stack = np.stack(masks).astype(np.float32)
-  input_config = configs['eval_input_config']
+  if FLAGS.eval_training_data:
-import pandas as pd
+import iris_data
-        onehot_labels=onehot_labels, logits=logits)
+    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
-    test_x = dict(test_x)
+    (train_x, train_y), (test_x, test_y) = iris_data.load_data()
-        input_fn=lambda:train_input_fn(train_x, train_y, args.batch_size),
+        input_fn=lambda:iris_data.train_input_fn(train_x, train_y, args.batch_size),
-        input_fn=lambda:eval_input_fn(test_x, test_y, args.batch_size))
+        input_fn=lambda:iris_data.eval_input_fn(test_x, test_y, args.batch_size))
-        input_fn=lambda:eval_input_fn(predict_x, batch_size=args.batch_size))
+        input_fn=lambda:iris_data.eval_input_fn(predict_x,
-        print(template.format(SPECIES[class_id], 100 * probability, expec))
+
-  df = pd.read_csv(text, names=premade_estimator.CSV_COLUMN_NAMES)
+  df = pd.read_csv(text, names=iris_data.CSV_COLUMN_NAMES)
-import pandas as pd
+import iris_data
-    test_x = dict(test_x)
+    (train_x, train_y), (test_x, test_y) = iris_data.load_data()
-        input_fn=lambda:train_input_fn(train_x, train_y, args.batch_size),
+        input_fn=lambda:iris_data.train_input_fn(train_x, train_y,
-        input_fn=lambda:eval_input_fn(test_x, test_y, args.batch_size))
+        input_fn=lambda:iris_data.eval_input_fn(test_x, test_y,
-        input_fn=lambda:eval_input_fn(predict_x, batch_size=args.batch_size))
+        input_fn=lambda:iris_data.eval_input_fn(predict_x,
-        print(template.format(SPECIES[class_id], 100 * probability, expec))
+
-  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
+from tensorflow.examples.tutorials.mnist import input_data
-                    help='Number of images to process in a batch')
+parser.add_argument(
-                    help='Path to the MNIST data directory.')
+parser.add_argument(
-                    help='The directory where the model will be stored.')
+parser.add_argument(
-                    help='Number of epochs to train.')
+parser.add_argument(
-    '--data_format', type=str, default=None,
+    '--data_format',
-    dataset = dataset.shuffle(buffer_size=_NUM_IMAGES['train'])
+    'provides a performance boost on GPU but is not always compatible '
-  dataset = dataset.repeat(num_epochs)
+def train_dataset(data_dir):
-  return images, labels
+def eval_dataset(data_dir):
-                   'channels_last')
+    data_format = ('channels_first'
-                                  data_format=data_format)
+  pool1 = tf.layers.max_pooling2d(
-                                  data_format=data_format)
+  pool2 = tf.layers.max_pooling2d(
-                          activation=tf.nn.relu)
+  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
-      params={'data_format': FLAGS.data_format})
+      model_fn=mnist_model_fn,
-  }
+  tensors_to_log = {'train_accuracy': 'train_accuracy'}
-      hooks=[logging_hook])
+  def train_input_fn():
-      input_fn=lambda: input_fn(False, test_file, FLAGS.batch_size))
+  def eval_input_fn():
-"""Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API."""
+"""Example code for TensorFlow Wide & Deep Tutorial using tf.estimator API."""
-flags.DEFINE_integer('batch_size', 1, 'Batch size while training.')
+flags.DEFINE_integer('batch_size', 6, 'Batch size while training.')
-flags.DEFINE_string('master', 'local', '')
+flags.DEFINE_string('master', '', '')
-  rnn_mode = CUDNN
+  rnn_mode = BLOCK
-          num_inception_images=10)
+  # Mock `inception_score` which is expensive.
-from google3.third_party.tensorflow_models.gan.mnist  import eval  # pylint:disable=redefined-builtin
+import eval  # pylint:disable=redefined-builtin
-"""Tests for tfgan.examples.mnist.train."""
+"""Tests for mnist.train."""
-  def test_run_one_train_step(self):
+  @mock.patch.object(train, 'data_provider', autospec=True)
-      train.main(None)
+    mock_data_provider.provide_data.return_value = (mock_imgs, mock_lbls, None)
-  def test_full_flow(self):
+  @mock.patch.object(train, 'data_provider', autospec=True)
-      train.main(None)
+    mock_data_provider.provide_data.return_value = (mock_imgs, mock_lbls, None)
-    if not self.hps.do_train_io_only:
+    # train the io matrices only
-        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
+        tf.get_collection('IO_transformations',
-        tf.get_collection('IO_transformations',
+        tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
-          cell, output_keep_prob=config.keep_prob)
+    def make_cell():
-        [cell for _ in range(config.num_layers)], state_is_tuple=True)
+        [make_cell() for _ in range(config.num_layers)], state_is_tuple=True)
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    for endpoint_name, expected_shape in endpoints_shapes.items():
-from six import xrange
+from six.moves import xrange
-
+
-      self.assertAlmostEqual(3217920L, total_params)
+      self.assertAlmostEqual(3217920, total_params)
-    python create_kitti_tf_record.py \
+    python object_detection/dataset_tools/create_kitti_tf_record.py \
-  ./create_oid_tf_record \
+  python object_detection/dataset_tools/create_oid_tf_record.py \
-    ./create_pascal_tf_record --data_dir=/home/user/VOCdevkit \
+    python object_detection/dataset_tools/create_pascal_tf_record.py \
-    ./create_pet_tf_record --data_dir=/home/user/pet \
+    python object_detection/dataset_tools/create_pet_tf_record.py \
-    result = offline_eval._generate_sharded_filenames('/path/to/@3.sst')
+    result = offline_eval._generate_sharded_filenames('/path/to/@3.record')
-        '/path/to/-00002-of-00003.sst'
+        '/path/to/-00000-of-00003.record', '/path/to/-00001-of-00003.record',
-    test_filenames = ['/path/to/file', '/path/to/@3.sst']
+    test_filenames = ['/path/to/file', '/path/to/@3.record']
-        '/path/to/-00001-of-00003.sst', '/path/to/-00002-of-00003.sst'
+        '/path/to/file', '/path/to/-00000-of-00003.record',
-from object_detection import create_pascal_tf_record
+from object_detection.dataset_tools import create_pascal_tf_record
-                       ignore_difficult_instances=False):
+                       ignore_difficult_instances=False,
-  ymax = []
+  xmins = []
-    ymax.append(float(obj['bndbox']['ymax']) / height)
+    if faces_only:
-  example = tf.train.Example(features=tf.train.Features(feature={
+  feature_dict = {
-      'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),
+      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),
-  }))
+  }
-                     examples):
+                     examples,
-    path = os.path.join(annotations_dir, 'xmls', example + '.xml')
+    xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')
-      logging.warning('Could not find %s, ignoring example.', path)
+    if not os.path.exists(xml_path):
-    with tf.gfile.GFile(path, 'r') as fid:
+    with tf.gfile.GFile(xml_path, 'r') as fid:
-    writer.write(tf_example.SerializeToString())
+    try:
-# TODO: Add test for pet/PASCAL main files.
+# TODO(derekjchow): Add test for pet/PASCAL main files.
-                   image_dir, train_examples)
+                   image_dir, train_examples, faces_only=FLAGS.faces_only)
-                   image_dir, val_examples)
+                   image_dir, val_examples, faces_only=FLAGS.faces_only)
-"""Example of DNNClassifier for Iris plant dataset."""
+"""An Example of a custom Estimator for the Iris dataset."""
-parser.add_argument('--train_steps', default=200, type=int,
+parser.add_argument('--train_steps', default=1000, type=int,
-COLUMNS = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']
+CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',
-def load_data(train_fraction=0.8, seed=0, y_name='Species'):
+def load_data(y_name='Species'):
-    train = pd.read_csv(train_path, names=COLUMNS, header=0)
+    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
-    test = pd.read_csv(test_path, names=COLUMNS, header=0)
+    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)
-    return tf.data.Dataset.from_tensor_slices(inputs)
+def train_input_fn(features, labels, batch_size):
-    return lambda: ds.make_one_shot_iterator().get_next()
+    # Shuffle, repeat, and batch the examples.
-    for units in params.get('hidden_units', [10, 20, 10]):
+    for units in params['hidden_units']:
-                       for col_name in COLUMNS[:-1]]
+    # Feature columns describe how to use the input.
-    # Build 3 layer DNN with 10, 20, 10 units respectively.
+    # Build 2 hidden layer DNN with 10, 10 units respectively.
-            'hidden_units': [10, 20, 10],
+            'feature_columns': my_feature_columns,
-    classifier.train(input_fn=from_dataset(train), steps=args.train_steps)
+    classifier.train(
-    eval_result = classifier.evaluate(input_fn=from_dataset(test))
+    eval_result = classifier.evaluate(
-        print(template.format(SPECIES[class_id], 100 * probability))
+    expected = ['Setosa', 'Versicolor', 'Virginica']
-  df = pd.read_csv(text, names=premade_estimator.COLUMNS)
+  df = pd.read_csv(text, names=premade_estimator.CSV_COLUMN_NAMES)
-"""Example of DNNClassifier for Iris plant dataset."""
+"""An Example of a DNNClassifier for the Iris dataset."""
-parser.add_argument('--train_steps', default=200, type=int,
+parser.add_argument('--train_steps', default=1000, type=int,
-COLUMNS = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']
+CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',
-def load_data(train_fraction=0.8, seed=0, y_name='Species'):
+def load_data(y_name='Species'):
-    train = pd.read_csv(train_path, names=COLUMNS, header=0)
+    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
-    test = pd.read_csv(test_path, names=COLUMNS, header=0)
+    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)
-    return tf.data.Dataset.from_tensor_slices(inputs)
+def train_input_fn(features, labels, batch_size):
-    return lambda: ds.make_one_shot_iterator().get_next()
+    # Return the read end of the pipeline.
-                       for col_name in COLUMNS[:-1]]
+    # Feature columns describe how to use the input.
-    # Build 3 layer DNN with 10, 20, 10 units respectively.
+    # Build 2 hidden layer DNN with 10, 10 units respectively.
-        hidden_units=[10, 20, 10],
+        feature_columns=my_feature_columns,
-    classifier.train(input_fn=from_dataset(train), steps=args.train_steps)
+    classifier.train(
-    eval_result = classifier.evaluate(input_fn=from_dataset(test))
+    eval_result = classifier.evaluate(
-        print(template.format(SPECIES[class_id], 100 * probability))
+    expected = ['Setosa', 'Versicolor', 'Virginica']
-    'batch_size', 100, 'The number of samples in each batch.')
+    'batch_size', 50, 'The number of samples in each batch.')
-assert "1.3" <= tf_version, "TensorFlow r1.3 or later is needed"
+assert "1.4" <= tf_version, "TensorFlow r1.4 or later is needed"
-    self.global_step = tf.contrib.framework.get_or_create_global_step()
+    self.global_step = tf.train.get_or_create_global_step()
-    self.global_step = tf.contrib.framework.get_or_create_global_step()
+    self.global_step = tf.train.get_or_create_global_step()
-        self.global_step = tf.contrib.framework.get_or_create_global_step()
+        self.global_step = tf.train.get_or_create_global_step()
-      self.global_step = tf.contrib.framework.get_or_create_global_step()
+      self.global_step = tf.train.get_or_create_global_step()
-    self.global_step = tf.contrib.framework.get_or_create_global_step()
+    self.global_step = tf.train.get_or_create_global_step()
-      current_step = tf.cast(tf.contrib.framework.get_or_create_global_step(),
+      current_step = tf.cast(tf.train.get_or_create_global_step(),
-    global_step = tf.contrib.framework.get_or_create_global_step()
+    global_step = tf.train.get_or_create_global_step()
-        global_step=tf.contrib.framework.get_or_create_global_step())
+        global_step=tf.train.get_or_create_global_step())
-    self._layers.append(network_units.Layer(self, 'adjacency', -1))
+    self._layers.append(network_units.Layer(component, 'adjacency', -1))
-    self._layers.append(network_units.Layer(self, 'labels', self._num_labels))
+    self._layers.append(
-          logits, tf.cast(gold, tf.int64))) / tf.cast(total, tf.float32)
+  with tf.control_dependencies([tf.assert_positive(total)]):
-
+  def testBuildLossFailsOnNoExamples(self):
-        recent MasterState::Reset().
+    handle: string tensor handle to the underlying ComputeSession.
-    with tf.control_dependencies([tf.assert_equal(self.training_beam_size, 1)]):
+    # Add 0 to training_beam_size to disable eager static evaluation.
-    cost = output[1]
+    # Note: Total could be zero by a number of reasons, including:
-              handle, logits, component=self.name)
+          if self.num_actions == 1:  # deterministic; take oracle transition
-
+import functools
-               name='Composite'):
+               name="Composite"):
-
+    return tf.cond(self._switch,
-    return slot1 or slot2
+    if name.startswith("c1-"):
-                  self._optimizer2.get_slot_names())
+    opt1_names = self._optimizer1.get_slot_names()
-                                                         switch)
+      optimizer = composite_optimizer.CompositeOptimizer(
-        momentum_counter = sess.run(optimizer.get_slot(w, "momentum_counter"))
+        adam_slots = ["c1-m", "c1-v", "c1-adam_counter"]
-                       adam_counter, momentum_counter)
+          logging.info("%d %s %d %d", iteration,
-    raise e
+from dragnn.core.ops.gen_dragnn_bulk_ops import *
-
+import collections
-      hyperparams.decay_base,
+      learning_rate=base_rate,
-        [comp.build_post_restore_hook() for comp in self.components]):
+    control_ops = []
-        [x.name for x in tf.global_variables() if 'quantized' not in x.name]))
+    logging.info('Saving variables:\n\t%s',
-        ],
+        var_list=[x for x in tf.global_variables()],
-  FLAGS.test_tmpdir = tf.test.get_temp_dir()
+
-        gold_doc.SerializeToString(), gold_doc_2.SerializeToString()
+        gold_doc.SerializeToString(),
-    grid_point.composite_optimizer_spec.method2.momentum = 0.9
+    spec = grid_point.composite_optimizer_spec
-        gold_doc.SerializeToString(), gold_doc_2.SerializeToString()
+        gold_doc.SerializeToString(),
-        test_doc_2.SerializeToString(), test_doc.SerializeToString()
+        test_doc.SerializeToString(),
-      hyperparam_config = spec_pb2.GridPoint()
+      if not hyperparam_config:
-      'char-map', 'char-ngram-map', 'label-map', 'prefix-table', 'suffix-table'
+      'char-map', 'char-ngram-map', 'label-map', 'prefix-table', 'suffix-table',
-  FLAGS.test_tmpdir = tf.test.get_temp_dir()
+input { name: "known-word-map" Part { file_pattern: "/tmp/known-word-map" } }
-
+
-    return tf.get_variable(name, initializer=tf.reshape(embeddings, shape))
+    return tf.get_variable(
-            stddev=1.0 / feature_spec.embedding_dim**.5, seed=seed))
+            stddev=1.0 / feature_spec.embedding_dim**.5, seed=seed),
-    state: MasterState object for the live nlp_saft::dragnn::MasterState.
+    state: MasterState object for the live ComputeSession.
-                 (name, named_tensors))
+  raise KeyError('Name "%s" not found in named tensors: %s' % (name,
-    state: MasterState object for the live nlp_saft::dragnn::MasterState.
+    state: MasterState object for the live ComputeSession.
-    state: MasterState object for the live nlp_saft::dragnn::MasterState.
+    state: MasterState object for the live ComputeSession.
-          inputs, mean, variance, beta, gamma, variance_epsilon)
+      outputs = nn.batch_normalization(inputs, mean, variance, beta, gamma,
-                                  name='%s_array' % self.name)
+    tensor_array = ta.TensorArray(
-  flat = (inputs.get_shape().ndims == 2)
+  # We only check the dims if we are applying per-sequence dropout
-  __metaclass__ = abc.ABCMeta  # required for @abstractmethod
+  __metaclass__ = abc.ABCMeta  # required for @abc.abstractmethod
-                  self._linked_feature_dims.values())
+    input_dims = (
-        0)
+            0), 0)
-        component.spec.network_unit.parameters, defaults={
+        component.spec.network_unit.parameters,
-            'dropout_all_layers': False})
+            'dropout_all_layers': False
-    # the base initializer may need to know the size of of the hidden layer for
+    # the base initializer may need to know the size of the hidden layer for
-        if self._attrs['hidden_layer_sizes'] else [])
+    self._hidden_layer_sizes = (map(
-                                          tf.float32)
+      self._layer_norm_hidden = LayerNorm(
-                    0.2, dtype=tf.float32)))
+                initializer=tf.constant_initializer(0.2, dtype=tf.float32)))
-              component, name='layer_%d' % index, dim=hidden_layer_size))
+          Layer(component, name='layer_%d' % index, dim=hidden_layer_size))
-    if component.num_actions:
+    if component.num_actions and not self._attrs['omit_logits']:
-              component, name='logits', dim=component.num_actions))
+          Layer(component, name='logits', dim=component.num_actions))
-          layer_name)
+      logging.fatal('Invalid layer name: "%s" Can only retrieve from "logits", '
-        self._c2o, self._bo, self._x2c, self._h2c, self._bc])
+        self._c2o, self._bo, self._x2c, self._h2c, self._bc
-            component, name='layer_0', dim=self._hidden_layer_sizes))
+        Layer(component, name='layer_0', dim=self._hidden_layer_sizes))
-        initializer=tf.random_normal_initializer(stddev=1e-4)))
+    self.params.append(
-            component, name='logits', dim=component.num_actions))
+        Layer(component, name='logits', dim=component.num_actions))
-    logits = tf.nn.xw_plus_b(ht, tf.get_variable('weights_softmax'),
+    logits = tf.nn.xw_plus_b(ht,
-              convolutional kernel at every layer.
+              convolutional kernel at every layer except the first.
-        component.spec.network_unit.parameters, defaults={
+        component.spec.network_unit.parameters,
-            'dropout_per_sequence': False})
+            'dropout_per_sequence': False
-    self._depths = map(int, self._attrs['depths'].split(','))
+    self._depths = [self._concatenated_input_dim]
-                                     self._weights)
+        Layer(component, name='conv_output', dim=self._depths[-1]))
-                         "bulk feature extractor component.")
+                         'bulk feature extractor component.')
-            conv, [-1, self._depths[-1]], name='reshape_activations')
+            side_conv, [-1, self._side_depths[-1]],
-  and modifier representations from different inputs.
+  For two sequences of representations of N tokens, all N^2 pairs of
-    concatenated size of the input features.
+      dropout: comma separated list of floats, dropout keep probability for each
-      ValueError: if the final depth is not equal to 1.
+      RuntimeError: if the lists of dropout, bias_init, initialization, and
-    self._depths.extend(map(int, parameters['depths'].split(',')))
+    self._depths = [self._source_dim + self._target_dim]
-                       parameters['depths'])
+    self._dropout = map(float, parameters['dropout'].split(',')) if parameters[
-      self._relu_layers = set(range(self._num_layers - 1))
+      if self._activation == 'glu' and i in self._activation_layers:
-        bias_init = 0.0 if i in self._relu_layers else 0.2
+            add_var_initialized('weights', kernel_shape, self._initialization[
-                initializer=tf.constant_initializer(bias_init),
+                initializer=tf.constant_initializer(self._bias_init[i]),
-    arg2 = tf.tile(arg2, tf.stack([1, 1, num_steps, 1]))
+    sources = lookup_named_tensor('sources', linked_embeddings).tensor
-            padding='SAME')
+        if during_training:
-    return [tf.reshape(conv, [-1, num_steps], name='reshape_activations')]
+        if i in self._activation_layers:
-          Layer(self, 'slice_%s' % slice_index, self._slice_dim))
+          Layer(component, 'slice_%s' % slice_index, self._slice_dim))
-
+  def get_variable(self, name):
-  Modified version of nlp/saft/opensource/dragnn/tools/parser_trainer.py
+  Modified version of dragnn/tools/parser_trainer.py
-  """A reader for conll files, with optional projectivizing."""
+class FormatSentenceReader(object):
-               projectivize=False, morph_to_pos=False):
+  def __init__(self,
-            record_format: 'conll-sentence'
+            record_format: '%s'
-          }""" % filepath
+          }""" % (record_format, filepath)
-      self._source = gen_parser_ops.well_formed_filter(self._source)
+      if check_well_formed:
-  FLAGS.test_tmpdir = tf.test.get_temp_dir()
+
-               name=None, **kwargs):
+  def add_link(self,
-        name=name, source_translator=source_translator,
+        source_component=source.spec.name,
-            [str(x.embedding_dim) for x in self.spec.fixed_feature]))
+        value=';'.join([str(x.embedding_dim) for x in self.spec.fixed_feature]))
-  FLAGS.test_tmpdir = tf.test.get_temp_dir()
+
-        tf.logging.info('Updating best eval to %.2f%%, saving checkpoint.',
+        tf.logging.info('Updating best eval to %.2f, saving checkpoint.',
-
+
-    self._params.extend(created_vars.values())
+    """Captures variables created by a function in |self._params|."""
-      return function(scope)
+    """Applies a function using previously-captured variables."""
-  bazel run -c opt <...>:dragnn_eval -- \
+  bazel run -c opt <...>:evaluator -- \
-
+# Copyright 2017 Google Inc. All Rights Reserved.
-    url='https://github.com/tensorflow/models/tree/master/research/syntaxnet',
+    url='https://github.com/tensorflow/models/tree/master/syntaxnet',
-
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+          cache_vectors_locally=False,
-# pylint: disable=no-name-in-module,unused-import,g-bad-import-order,maybe-no-member,no-member,g-importing-member
+
-from syntaxnet.ops.gen_parser_ops import *  # pylint: disable=wildcard-import
+from syntaxnet.ops.gen_parser_ops import *
-5	.	.	PUNCT	.	_	3	punct	_	_
+1-2\tWe've\t_
-    doc1_lines = ['æµè¯	NO_SPACE\n', 'ç	NO_SPACE\n', 'å¥å­	NO_SPACE']
+    doc1_lines = ['æµè¯\tNO_SPACE\n', 'ç\tNO_SPACE\n', 'å¥å­\tNO_SPACE']
-        'point	NO_SPACE\n', '.	NO_SPACE'
+        'That\tNO_SPACE\n', '\'s\tSPACE\n', 'a\tSPACE\n', 'good\tSPACE\n',
-module containing each subclass.  It is sufficient to add subclasses as build
+# Copyright 2017 Google Inc. All Rights Reserved.
-the desired subclass.
+Unlike RegisterableClass<>, which allows subclasses to be registered under
-    logging.debug('Malformed type: "%s"', name)
+    logging.info('Malformed type: "%s"', name)
-    logging.debug('Unable to find module "%s": "%s"', module_path, e)
+    logging.info('Unable to find module "%s": "%s"', module_path, e)
-                  module_path)
+    logging.info('Name "%s" not found in module: "%s"', class_name, module_path)
-    logging.debug('Name does not refer to a class: "%s"', name)
+    logging.info('Name does not refer to a class: "%s"', name)
-                  baseclass.__name__)
+    logging.info('Class "%s" is not a subclass of "%s"', subclass_name,
-  manner.  For example, if |path| is 'google3.foo.bar' and |subclass_name| is
+  manner.  For example, if |path| is 'syntaxnet.foo.bar' and |subclass_name| is
-    'google3.baz.ClassName'
+    'syntaxnet.foo.bar.baz.ClassName'
-    if not elements: break  # no more paths to try
+    if subclass:
-        'registry_test_impl.Bad', 'Impl'
+        'bad.syntaxnet.util.registry_test_base.Impl',
-            images, final_endpoint=mparams.final_endpoint)
+      with slim.arg_scope(
-      raise ValueError('min_padded_size_ratio should have 3 elements if set!')
+      raise ValueError('min_padded_size_ratio should have 2 elements if set!')
-      raise ValueError('max_padded_size_ratio should have 3 elements if set!')
+      raise ValueError('max_padded_size_ratio should have 2 elements if set!')
-            })
+    kwargs = {
-        'pad_color': None,
+    })
-  # variables, which includes batch norm beta and gamma variables.
+  # Add weight decay to the loss. We exclude the batch norm variables because
-      [tf.nn.l2_loss(v) for v in tf.trainable_variables()])
+      [tf.nn.l2_loss(v) for v in tf.trainable_variables()
-      # provides a large performance boost on GPU. See
+      # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).
-      # provides a large performance boost on GPU.
+      # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).
-    '--train_epochs', type=int, default=20, help='Number of training epochs.')
+    '--train_epochs', type=int, default=40, help='Number of training epochs.')
-    print('-' * 30)
+    print('-' * 60)
-  """Parse an Imagenet record from value."""
+def record_parser(value, is_training):
-      filenames(is_training, data_dir))
+  dataset = tf.data.Dataset.from_tensor_slices(filenames(is_training, data_dir))
-_SHUFFLE_BUFFER = 100000
+_NUM_EXAMPLES = {
-    dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)
+    dataset = dataset.shuffle(buffer_size=_NUM_EXAMPLES['train'])
-                                      .reduce_boxes_in_lowest_layer))
+        interpolated_scale_aspect_ratio=(
-
+import math
-  def test_build_ssd_anchor_generator_withoud_reduced_boxes(self):
+  def test_build_ssd_anchor_generator_with_custom_scales(self):
-        [(0.1, 0.3, 0.3), (0.8,)]):
+        [(0.1, 0.3, 0.3), (0.8, 0.894)]):
-        [(1.0, 2.0, 0.5), (2.0,)]):
+        [(1.0, 2.0, 0.5), (2.0, 1.0)]):
-                        (length-2 float tensor, default=[256, 256]).
+                        (length-2 float tensor, default=[1.0, 1.0]).
-                anchor_offsets=None):
+  def _generate(self, feature_map_shape_list, im_height=1, im_width=1):
-                         tf.to_float(im_width) / tf.to_float(pair[1]))
+
-    if not anchor_offsets:
+    else:
-    base_anchor_size = min_im_shape * self._base_anchor_size
+    min_im_shape = tf.minimum(im_height, im_width)
-      # TODO: make reshape an option for the clip_to_window op
+          concatenated_anchors, self._clip_window, filter_nonoverlapping=False)
-                       aspect_ratios=(1.0, 2.0, 3.0, 1.0/2, 1.0/3),
+                       scales=None,
-            for i in range(num_layers)] + [1.0]
+  if scales is None or not scales:
-          layer_box_specs.append((np.sqrt(scale*scale_next), 1.0))
+      # Add one more anchor, with a scale between the current scale, and the
-  return MultipleGridAnchorGenerator(box_specs_list, base_anchor_size)
+
-                                        anchor_offsets=[(7, -3)])
+        box_specs_list,
-                                        anchor_offsets=[(0, 0)])
+        box_specs_list,
-                                                      base_anchor_size)
+    anchor_generator = ag.MultipleGridAnchorGenerator(
-    base_anchor_size = tf.constant([1, 1], dtype=tf.float32)
+  def test_construct_anchor_grid_normalized(self):
-    exp_anchor_corners = [[0., 0., 320., 320.], [0., 320., 320., 640.]]
+    exp_anchor_corners = [[0., 0., 1., 0.5], [0., 0.5, 1., 1.]]
-                                                      base_anchor_size)
+    anchor_generator = ag.MultipleGridAnchorGenerator(
-                                                        (.25, .25)])
+        box_specs_list,
-        box_specs_list, base_anchor_size, clip_window=clip_window)
+        box_specs_list,
-                                anchor_offsets=[(.125, .125), (.25, .25)])
+      anchor_generator = ag.MultipleGridAnchorGenerator(
-                                anchor_offsets=[(.125, .125), (.25, .25)])
+      anchor_generator = ag.MultipleGridAnchorGenerator(
-                                anchor_offsets=[(.25, .25)])
+      anchor_generator = ag.MultipleGridAnchorGenerator(
-                                anchor_offsets=[(.125, .125), (.25, .25)])
+      anchor_generator = ag.MultipleGridAnchorGenerator(
-                                                (.25, .25)])
+      anchor_generator = ag.MultipleGridAnchorGenerator(
-                                anchor_offsets=[(.125), (.25)])
+      anchor_generator = ag.MultipleGridAnchorGenerator(
-        aspect_ratios=(1.0, 2.0, 3.0, 1.0/2, 1.0/3),
+        num_layers=6,
-            standard_fields.InputDataFields.groundtruth_difficult, None))
+        groundtruth_is_difficult_list=groundtruth_difficult)
-
+    # If the key is not present in the groundtruth_dict or the array is empty
-            standard_fields.InputDataFields.groundtruth_group_of, None))
+        groundtruth_is_group_of_list=groundtruth_group_of)
-            groundtruth_class_labels1
+            groundtruth_class_labels1,
-         groundtruth_class_labels1})
+         groundtruth_class_labels1,
-    # Train for 40 steps at batch size 2 and evaluate final loss
+    # Train for 100 epochs at batch size 3 and evaluate final loss
-        steps=40)
+            TEST_CSV, num_epochs=100, shuffle=True, batch_size=3))
-    image = tf.image.resize_image_with_crop_or_pad(image, _HEIGHT + 8, _WIDTH + 8)
+    image = tf.image.resize_image_with_crop_or_pad(
-    dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)
+    # randomness, while smaller sizes have better performance. Because CIFAR-10
-
+  dataset = dataset.map(lambda value: dataset_parser(value, is_training),
-  dataset = dataset.map(parse_csv, num_parallel_calls=5)
+  dataset = dataset.map(parse_csv, num_parallel_calls=5)
-  # The first byte represents the label, which we convert from uint8 to int32.
+  # The first byte represents the label, which we convert from uint8 to int32
-                           [_DEPTH, _HEIGHT, _WIDTH])
+  depth_major = tf.reshape(
-  image = tf.image.random_flip_left_right(image)
+  return image, label
-  return image
+def preprocess_image(image, is_training):
-  image, label = parse_record(record)
+    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.
-    image = train_preprocess_fn(image)
+    # Randomly flip the image horizontally.
-  return image, label
+  return image
-      lambda record: parse_and_preprocess(record, is_training))
+      lambda image, label: (preprocess_image(image, is_training), label))
-  """Parse and preprocess a CIFAR-10 image and label from a raw record."""
+def parse_record(raw_record):
-
+def parse_and_preprocess(record, is_training):
-      lambda record: parse_and_preprocess_record(record, is_training))
+      lambda record: parse_and_preprocess(record, is_training))
-    fake_dataset = fake_dataset.map(cifar10_main.dataset_parser)
+    fake_dataset = fake_dataset.map(cifar10_main.parse_record)
-      example_parser, num_threads=1, output_buffer_size=batch_size)
+  dataset = dataset.map(example_parser).prefetch(batch_size)
-  """Parse a CIFAR-10 record from value."""
+def parse_and_preprocess_record(raw_record, is_training):
-  raw_record = tf.decode_raw(value, tf.uint8)
+  # Convert bytes to a vector of uint8 that is record_bytes long.
-  label = tf.cast(raw_record[0], tf.int32)
+  label = tf.cast(record_vector[0], tf.int32)
-  depth_major = tf.reshape(raw_record[label_bytes:record_bytes],
+  depth_major = tf.reshape(record_vector[label_bytes:record_bytes],
-def train_preprocess_fn(image, label):
+def train_preprocess_fn(image):
-  return image, label
+  return image
-    batch_size: The number samples per batch.
+    batch_size: The number of samples per batch.
-      lambda image, label: (tf.image.per_image_standardization(image), label))
+      lambda record: parse_and_preprocess_record(record, is_training))
-                        num_parallel_calls=5)
+                        num_parallel_calls=5).prefetch(batch_size)
-  """A simple input_fn using the contrib.data input pipeline."""
+  """A simple input_fn using the tf.data input pipeline."""
-  dataset = tf.contrib.data.TFRecordDataset([filename])
+  dataset = tf.data.TFRecordDataset([filename])
-  return tf.contrib.data.FixedLengthRecordDataset(filenames, record_bytes)
+  return tf.data.FixedLengthRecordDataset(filenames, record_bytes)
-  """Input_fn using the contrib.data input pipeline for CIFAR-10 dataset.
+  """Input_fn using the tf.data input pipeline for CIFAR-10 dataset.
-                        output_buffer_size=2 * batch_size)
+  dataset = dataset.map(dataset_parser)
-                          output_buffer_size=2 * batch_size)
+    dataset = dataset.map(train_preprocess_fn)
-      output_buffer_size=2 * batch_size)
+      lambda image, label: (tf.image.per_image_standardization(image), label))
-  dataset = tf.contrib.data.Dataset.from_tensor_slices(
+  dataset = tf.data.Dataset.from_tensor_slices(
-  dataset = dataset.flat_map(tf.contrib.data.TFRecordDataset)
+  dataset = dataset.flat_map(tf.data.TFRecordDataset)
-                        output_buffer_size=batch_size)
+                        num_parallel_calls=5)
-  dataset = dataset.map(parse_csv, num_threads=5)
+  dataset = tf.data.TextLineDataset(data_file)
-
+  # We call repeat after shuffling, rather than before, to prevent separate
-  images, labels = dataset.make_one_shot_iterator().get_next()
+  iterator = dataset.make_one_shot_iterator()
-  iterator = dataset.batch(batch_size).make_one_shot_iterator()
+  dataset = dataset.batch(batch_size)
-        for i in range(0, 1024)]
+        for i in range(1024)]
-        for i in range(0, 128)]
+        for i in range(128)]
-  dataset = dataset.flat_map(tf.contrib.data.TFRecordDataset)
+    dataset = dataset.shuffle(buffer_size=_FILE_SHUFFLE_BUFFER)
-    dataset = dataset.repeat(num_epochs)
+  dataset = dataset.flat_map(tf.contrib.data.TFRecordDataset)
-  iterator = dataset.batch(batch_size).make_one_shot_iterator()
+  # We call repeat after shuffling, rather than before, to prevent separate
-    # 256, the learning rate should be 0.1.
+    # Scale the learning rate linearly with the batch size. When the batch size
-  dataset = dataset.batch(batch_size)
+  if shuffle:
-  return _input_fn
+  iterator = dataset.make_one_shot_iterator()
-    results = model.evaluate(input_fn=eval_input_fn)
+    model.train(input_fn=lambda: input_fn(
-    features, labels = wide_deep.input_fn(self.input_csv, 1, False, 1)()
+    features, labels = wide_deep.input_fn(self.input_csv, 1, False, 1)
-        input_fn=wide_deep.input_fn(
+        input_fn=lambda: wide_deep.input_fn(
-        input_fn=wide_deep.input_fn(
+        input_fn=lambda: wide_deep.input_fn(
-        input_fn=wide_deep.input_fn(
+        input_fn=lambda: wide_deep.input_fn(
-        input_fn=wide_deep.input_fn(
+        input_fn=lambda: wide_deep.input_fn(
-"""A dataset loader for imports85.data."""
+"""Utility functions for loading the automobile data set."""
-  """Load the imports85 data as a pd.DataFrame."""
+  """Load the automobile data set as a pd.DataFrame."""
-  # Load it into a pandas dataframe
+  # Load it into a pandas DataFrame
-  """Get the imports85 data set.
+  """Load the automobile data set and split it train/test and features/label.
-    train_fraction: the fraction of the dataset to use for training.
+    train_fraction: the fraction of the data set to use for training.
-    `(x_train, y_train), (x_test, y_test) = get_imports85_dataset(...)`
+    `(x_train, y_train), (x_test, y_test) = load_data(...)`
-  # Extract the label from the features dataframe.
+  # Extract the label from the features DataFrame.
-    """Create a slice dataset from a pandas DataFrame and labels"""
+    """Create a slice Dataset from a pandas DataFrame and labels"""
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.iteritems():
+    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.iteritems():
+    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
-  for key, value in kwargs.iteritems():
+  for key, value in kwargs.items():
-      label_handler = slim_example_decoder.Tensor('image/object/class/label')
+    # TODO: Add label_handler that decodes from 'image/object/class/text'
-
+def nasnet_large_arg_scope_for_detection(is_batch_norm_training=False):
-    with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
+    with slim.arg_scope(nasnet_large_arg_scope_for_detection(
-                        is_training=True, is_batchnorm_training=True,
+                        is_training=True,
-  with arg_scope([slim.dropout, nasnet_utils.drop_path],
+  with arg_scope([slim.dropout, nasnet_utils.drop_path, slim.batch_norm],
-                                  final_endpoint=final_endpoint)
+    with arg_scope([slim.avg_pool2d,
-                       is_training=True, is_batchnorm_training=True,
+                       is_training=True,
-  with arg_scope([slim.dropout, nasnet_utils.drop_path],
+  with arg_scope([slim.dropout, nasnet_utils.drop_path, slim.batch_norm],
-                                  final_endpoint=final_endpoint)
+    with arg_scope([slim.avg_pool2d,
-                  'will be partially specified as `[None, None, None, 3]`.')
+flags.DEFINE_string('input_shape', None,
-        int(dim) if dim != '-1' else None for dim in FLAGS.input_shape
+        int(dim) if dim != '-1' else None
-
+
-
+import google3
-
+from object_detection.utils import label_map_util
-    """Constructor sets keys_to_features and items_to_handlers."""
+  def __init__(self,
-        'image/width': tf.FixedLenFeature((), tf.int64, 1),
+        'image/encoded':
-        'image/segmentation/object/class': tf.VarLenFeature(tf.int64)
+        'image/object/bbox/xmin':
-            slim_example_decoder.Tensor('image/segmentation/object/class')),
+        fields.InputDataFields.groundtruth_group_of: (
-      A 3-D boolean tensor of shape [num_instances, height, width].
+      A 3-D float tensor of shape [num_instances, height, width] with values
-    return tf.cast(tf.reshape(masks, to_shape), tf.bool)
+    masks = keys_to_tensors['image/object/mask']
-    instance_segmentation = (
+    instance_masks = (
-                                   image_width)).astype(np.int64))
+                                   image_width)).astype(np.float32))
-    instance_segmentation_classes = np.random.randint(
+    object_classes = np.random.randint(
-    example_decoder = tf_example_decoder.TfExampleDecoder()
+        'image/object/mask': self._FloatFeature(instance_masks_flattened),
-        tensor_dict[fields.InputDataFields.groundtruth_instance_classes].
+        tensor_dict[fields.InputDataFields.groundtruth_classes].
-        instance_segmentation.astype(np.bool),
+        instance_masks.astype(np.float32),
-        tensor_dict[fields.InputDataFields.groundtruth_instance_classes])
+        object_classes,
-    frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor
+  pad_to_multiple = feature_extractor_config.pad_to_multiple
-                                 reuse_weights)
+  return feature_extractor_class(is_training, depth_multiplier, min_depth,
-      is_training, first_stage_features_stride, reuse_weights)
+      is_training, first_stage_features_stride,
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
+    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.iteritems():
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
+    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.iteritems():
-        is_training, first_stage_features_stride, reuse_weights, weight_decay)
+        is_training, first_stage_features_stride, batch_norm_trainable,
-      with slim.arg_scope([slim.batch_norm], is_training=False):
+      with slim.arg_scope([slim.batch_norm],
-        with slim.arg_scope([slim.batch_norm], is_training=False):
+        with slim.arg_scope([slim.batch_norm],
-        is_training, first_stage_features_stride, reuse_weights, weight_decay)
+        is_training, first_stage_features_stride, batch_norm_trainable,
-              is_training=False,
+              is_training=self._train_batch_norm,
-        with slim.arg_scope([slim.batch_norm], is_training=False):
+        with slim.arg_scope([slim.batch_norm],
-        first_stage_features_stride, reuse_weights, weight_decay)
+        first_stage_features_stride, batch_norm_trainable,
-        first_stage_features_stride, reuse_weights, weight_decay)
+        first_stage_features_stride, batch_norm_trainable,
-        first_stage_features_stride, reuse_weights, weight_decay)
+        first_stage_features_stride, batch_norm_trainable,
-from object_detection.utils import ops
+     By default convolution kernel size is set to 3, and it can be customized
-    'anchor_strides': [16, 32, 64, -1, -1, -1]
+    'layer_depth': [-1, -1, -1, 512, 256, 128]
-        'anchor_strides': [16, 32, 64, -1, -1, -1]
+        'layer_depth': [-1, -1, -1, 512, 256, 128]
-        'anchor_strides': [16, 32, 64, -1, -1, -1]
+        'layer_depth': [-1, -1, -1, 512, 256, 128]
-      divided by the number of anchors.
+      design. Convolution kernel size is set to 3 by default, and can be
-      zip(feature_map_layout['from_layer'], feature_map_layout['layer_depth'])):
+  for index, from_layer in enumerate(feature_map_layout['from_layer']):
-          base_from_layer, index, depth_fn(layer_depth))
+      layer_name = '{}_2_Conv2d_{}_{}x{}_s2_{}'.format(
-            None, [3, 3],
+            intermediate_layer,
-            depth_fn(layer_depth), [3, 3],
+            intermediate_layer,
-            stride * feature_map_strides[index - 1])
+EMBEDDED_SSD_MOBILENET_V1_LAYOUT = {
-# TODO: add tests with different anchor strides.
+# TODO(rathodv): add tests with different anchor strides.
-  def _create_feature_extractor(self, depth_multiplier):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
-      depth_multiplier,
+      self, image_height, image_width, depth_multiplier, pad_to_multiple,
-    feature_extractor = self._create_feature_extractor(depth_multiplier)
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
-    feature_extractor = self._create_feature_extractor(depth_multiplier)
+      self, image_height, image_width, depth_multiplier, pad_to_multiple):
-                                                    scope_name):
+  def check_feature_extractor_variables_under_scope(
-      feature_extractor = self._create_feature_extractor(depth_multiplier)
+      feature_extractor = self._create_feature_extractor(
-        depth_multiplier, min_depth, conv_hyperparams, reuse_weights)
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
-              preprocessed_inputs,
+              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
-    tf.test.TestCase):
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase, tf.test.TestCase):
-  def _create_feature_extractor(self, depth_multiplier):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-        depth_multiplier, min_depth, conv_hyperparams)
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+        image_height, image_width, depth_multiplier, pad_to_multiple,
-        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+        image_height, image_width, depth_multiplier, pad_to_multiple,
-        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+        image_height, image_width, depth_multiplier, pad_to_multiple,
-        image_height, image_width, depth_multiplier)
+        image_height, image_width, depth_multiplier, pad_to_multiple)
-    feature_extractor = self._create_feature_extractor(depth_multiplier)
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                       scope_name)
+    self.check_feature_extractor_variables_under_scope(
-        depth_multiplier, min_depth, conv_hyperparams, reuse_weights)
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
-              image_features=image_features)
+        with slim.arg_scope([slim.batch_norm], fused=False):
-  def _create_feature_extractor(self, depth_multiplier):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-    conv_hyperparams = {}
+    with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm) as sc:
-        depth_multiplier, min_depth, conv_hyperparams)
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+        image_height, image_width, depth_multiplier, pad_to_multiple,
-        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+        image_height, image_width, depth_multiplier, pad_to_multiple,
-        image_height, image_width, depth_multiplier, expected_feature_map_shape)
+        image_height, image_width, depth_multiplier, pad_to_multiple,
-        image_height, image_width, depth_multiplier)
+        image_height, image_width, depth_multiplier, pad_to_multiple)
-    feature_extractor = self._create_feature_extractor(depth_multiplier)
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                       scope_name)
+    self.check_feature_extractor_variables_under_scope(
-        break
+        return
-  def draw_boxes((image, boxes, classes, scores)):
+  def draw_boxes(image_boxes_classes_scores):
-  `nms_boxes`, `nms_scores`, `nms_nms_classes` and `num_detections`. See
+  `clip_window`, `parallel_iterations` `masks, and `scope` as inputs. It returns
-      post_processing_config.score_converter)
+      post_processing_config.score_converter,
-def _build_score_converter(score_converter_config):
+def _score_converter_fn_with_logit_scale(tf_score_converter_fn, logit_scale):
-    return tf.identity
+    return _score_converter_fn_with_logit_scale(tf.identity, logit_scale)
-    return tf.sigmoid
+    return _score_converter_fn_with_logit_scale(tf.sigmoid, logit_scale)
-    return tf.nn.softmax
+    return _score_converter_fn_with_logit_scale(tf.nn.softmax, logit_scale)
-    self.assertEqual(score_converter, tf.identity)
+    self.assertEqual(score_converter.__name__, 'identity_with_logit_scale')
-    self.assertEqual(score_converter, tf.sigmoid)
+    self.assertEqual(score_converter.__name__, 'sigmoid_with_logit_scale')
-    self.assertEqual(score_converter, tf.nn.softmax)
+    self.assertEqual(score_converter.__name__, 'softmax_with_logit_scale')
-      contain masks, keypoints, keypoint_heatmaps corresponding to boxes.
+                                         additional_fields=None,
-        paddings.
+      for each image in the batch.  This parameter allows for ignoring zero
-      nms_boxes[i], nms_scores[i] and nms_class[i] are valid. the rest of the
+      nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the
-    # of single_image_nms_fn and discard the dummy masks after map_fn.
+    # of _single_image_nms_fn and discard the dummy masks after map_fn.
-       per_image_num_valid_boxes) = args
+    if additional_fields is None:
-
+      if per_image_additional_fields is not None:
-          change_coordinate_frame=change_coordinate_frame)
+          change_coordinate_frame=change_coordinate_frame,
-              num_detections]
+      nmsed_additional_fields = [
-         parallel_iterations=parallel_iterations)
+    batch_outputs = tf.map_fn(
-            batch_nmsed_masks, batch_num_detections)
+            batch_nmsed_masks, batch_nmsed_additional_fields,
-         max_size_per_class=max_output_size, max_total_size=max_output_size)
+     nmsed_additional_fields, num_detections
-         max_size_per_class=max_output_size, max_total_size=max_output_size)
+     nmsed_additional_fields, num_detections
-         masks=masks)
+     nmsed_additional_fields, num_detections
-         masks=masks_placeholder)
+     nmsed_additional_fields, num_detections
-         num_valid_boxes=num_valid_boxes, masks=masks)
+     nmsed_additional_fields, num_detections
-TODO: Support TPU implementations and sigmoid loss.
+TODO: Support TPU implementations.
-               hard_example_miner,
+               second_stage_classification_loss,
-        See builders/image_resizer_builder.py.
+      image_resizer_fn: A callable for image resizing.  This callable
-      second_stage_classification_loss_weight: A float
+      second_stage_localization_loss_weight: A float indicating the scale factor
-      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`
+      ValueError: If `second_stage_batch_size` > `first_stage_max_proposals` at
-    if second_stage_batch_size > first_stage_max_proposals:
+    if is_training and second_stage_batch_size > first_stage_max_proposals:
-        losses.WeightedSoftmaxClassificationLoss(anchorwise_output=True))
+    self._second_stage_classification_loss = second_stage_classification_loss
-        8) class_predictions_with_background: a 2-D tensor with shape
+        8) class_predictions_with_background: a 3-D tensor with shape
-          decoded proposal bounding boxes (in absolute coordinates).
+          decoded proposal bounding boxes in absolute coordinates.
-      rpn_box_encodings: 3-D float tensor of shape
+      rpn_box_encodings: 4-D float tensor of shape
-      rpn_objectness_predictions_with_background: 3-D float tensor of shape
+      rpn_objectness_predictions_with_background: 2-D float tensor of shape
-        2) class_predictions_with_background: a 2-D tensor with shape
+        2) class_predictions_with_background: a 3-D tensor with shape
-        5) mask_predictions: (optional) a 4-D tensor with shape
+          decoded proposal bounding boxes in absolute coordinates.
-      box_encodings: 3-D float tensor of shape
+      box_encodings: 4-D float tensor of shape
-      objectness_predictions_with_background: 3-D float tensor of shape
+      objectness_predictions_with_background: 2-D float tensor of shape
-            'num_detections': num_proposals
+            'num_detections': tf.to_float(num_proposals)
-    (proposal_boxes, proposal_scores, _, _,
+    (proposal_boxes, proposal_scores, _, _, _,
-         ) = self._format_groundtruth_data(image_shape)
+         _) = self._format_groundtruth_data(image_shape)
-    return groundtruth_boxlists, groundtruth_classes_with_background_list
+
-                                  mask_threshold=0.5):
+                                  mask_predictions=None):
-      refined_box_encodings: a 3-D tensor with shape
+      refined_box_encodings: a 3-D float tensor with shape
-      class_predictions_with_background: a 3-D tensor with shape
+      class_predictions_with_background: a 3-D tensor float with shape
-      mask_predictions: (optional) a 4-D tensor with shape
+      proposal_boxes: a 3-D float tensor with shape
-        rounded to 0 or 1.
+        containing instance mask prediction logits.
-          (optional) [batch, max_detections, mask_height, mask_width]
+          (optional) [batch, max_detections, mask_height, mask_width]. Note
-    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks, _,
-      (groundtruth_boxlists, groundtruth_classes_with_background_list
+      (groundtruth_boxlists, groundtruth_classes_with_background_list,
-                groundtruth_classes_with_background_list))
+                groundtruth_classes_with_background_list,
-      rpn_box_encodings: A 3-D float tensor of shape
+      rpn_box_encodings: A 4-D float tensor of shape
-      rpn_objectness_predictions_with_background: A 3-D float tensor of shape
+      rpn_objectness_predictions_with_background: A 2-D float tensor of shape
-      }
+      loss_dict = {}
-                           groundtruth_classes_with_background_list):
+                           groundtruth_classes_with_background_list,
-          tf.greater(flat_cls_targets_with_background, 0))
+          tf.greater(one_hot_flat_cls_targets_with_background, 0))
-      }
+      loss_dict = {}
-    mask_predictions = .6 * tf.ones(
+    mask_predictions = 30. * tf.ones(
-                            [[0, 0], [0, 0]]]]
+    exp_detection_masks = np.array([[[[1, 1], [1, 1]],
-                   hard_mining=False):
+                   hard_mining=False,
-  def test_predict_correct_shapes_in_inference_mode_both_stages(
+  def test_predict_gives_correct_shapes_in_inference_mode_first_stage_only(
-      with self.test_session(graph=test_graph) as sess:
+    test_graph = tf.Graph()
-        self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
+        prediction_out = sess.run(prediction_dict,
-      expected_num_anchors = image_size * image_size * 3 * 3
+  def test_predict_correct_shapes_in_inference_mode_both_stages(
-      with self.test_session() as sess:
+    input_shapes = [(batch_size, image_size, image_size, 3),
-          self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
+        tensor_dict_out = sess.run(result_tensor_dict, feed_dict={
-          'class_predictions_with_background': (2 * 7, 2 + 1),
+          'refined_box_encodings': (2 * max_num_proposals, 2, 4),
-          'proposal_boxes': (2, 7, 4),
+          'proposal_boxes': (2, max_num_proposals, 4),
-        'num_proposals': num_proposals
+        'num_proposals': num_proposals,
-                              groundtruth_classes_list)
+                              groundtruth_classes_list,
-        'num_proposals': num_proposals
+        'num_proposals': num_proposals,
-                              groundtruth_classes_list)
+                              groundtruth_classes_list,
-    # actual proposals (not counting zero paddings) is fewer (3).
+    # actual proposals (not counting zero paddings) is fewer.
-          [0, 0, 0, 0],
+          [0, 0, 0, 0],
-      rpn_box_encodings: 3-D float tensor of shape
+      rpn_box_encodings: 4-D float tensor of shape
-      rpn_objectness_predictions_with_background: 3-D float tensor of shape
+      rpn_objectness_predictions_with_background: 2-D float tensor of shape
-          number of proposals generated by the RPN.  `num_proposals` allows us
+          number of proposals generated by the RPN. `num_proposals` allows us
-
+from object_detection.utils import visualization_utils
-    postprocessing and losses.
+    TODO: group NMS parameters + score converter into a class and loss
-      # parallel iterations vs allow for dynamic batching.
+      # TODO: revisit whether to always use batch size as the number of parallel
-        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
+        1) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,
-    self._anchors = self._anchor_generator.generate(feature_map_spatial_dims)
+    image_shape = tf.shape(preprocessed_inputs)
-        'feature_maps': feature_maps
+        'feature_maps': feature_maps,
-        detection_boxes: [batch, max_detection, 4]
+        detection_boxes: [batch, max_detections, 4]
-      detection_boxes = self._batch_decode(box_encodings)
+      detection_boxes, detection_keypoints = self._batch_decode(box_encodings)
-              'num_detections': tf.to_float(num_detections)}
+      additional_fields = None
-          (logits) for each of the anchors.  Note that this tensor *includes*
+          (logits) for each of the anchors. Note that this tensor *includes*
-           self.groundtruth_lists(fields.BoxListFields.classes))
+           self.groundtruth_lists(fields.BoxListFields.classes),
-      classification_loss = tf.reduce_sum(cls_losses)
+      else:
-                                  normalizer) * classification_loss
+          'localization_loss': localization_loss,
-  def _assign_targets(self, groundtruth_boxes_list, groundtruth_classes_list):
+  def _summarize_anchor_classification_loss(self, class_ids, cls_losses):
-                 [0, 0, 1], class_pred_shape), class_pred_shape)
+    class_predictions = tf.slice(
-    decoded_boxes = self._batch_decode(prediction_dict['box_encodings'])
+    decoded_boxes, _ = self._batch_decode(prediction_dict['box_encodings'])
-        tf.reshape(tiled_anchor_boxes, [-1, self._box_coder.code_size]))
+        tf.reshape(tiled_anchor_boxes, [-1, 4]))
-                                4]))
+    decoded_keypoints = None
-        depth_multiplier=0, min_depth=0, conv_hyperparams=None)
+        is_training=True,
-  def _generate(self, feature_map_shape_list):
+  def _generate(self, feature_map_shape_list, im_height, im_width):
-    saver = tf_saver.Saver()
+    saver = tf.train.Saver()
-    inputs = slim.batch_norm(inputs, decay=0.1)
+    inputs = slim.batch_norm(inputs, decay=0.1, fused=True)
-                                             0.234375, 0.1875])
+        expected_mean = np.array([0.125, 0.25, 0.375, 0.25])
-               scope='alexnet_v2'):
+               scope='alexnet_v2',
-        convolutional mode, set spatial_squeeze to false.
+        To use in classification mode, resize input to 224x224 or set
-    num_classes: number of predicted classes.
+    num_classes: the number of predicted classes. If 0 or None, the logits layer
-      outputs. Useful to remove unnecessary dimensions for classification.
+      logits. Useful to remove unnecessary dimensions for classification.
-    the last op containing the log predictions and end_points dict.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
-        end_points[sc.name + '/fc8'] = net
+        # Convert end_points_collection into a end_point dict.
-    num_classes: the number of classes in the dataset.
+    num_classes: the number of classes in the dataset. If 0 or None, the logits
-      [batch_size, `num_classes`]
+    net: a 2D Tensor with the logits (pre-softmax activations) if num_classes
-  with tf.variable_scope(scope, 'CifarNet', [images, num_classes]):
+  with tf.variable_scope(scope, 'CifarNet', [images]):
-    net += scale * up
+    scaled_up = up * scale
-    net += scale * up
+
-    net += scale * up
+
-                             scope=None):
+                             scope=None,
-      net = slim.repeat(net, 10, block35, scale=0.17)
+      net = slim.repeat(net, 10, block35, scale=0.17,
-        net = slim.repeat(net, 20, block17, scale=0.10)
+        net = slim.repeat(net, 20, block17, scale=0.10,
-      net = slim.repeat(net, 9, block8, scale=0.20)
+      net = slim.repeat(net, 9, block8, scale=0.20, activation_fn=activation_fn)
-                        create_aux_logits=True):
+                        create_aux_logits=True,
-    num_classes: number of predicted classes.
+      Dimension batch_size may be undefined. If create_aux_logits is false,
-    logits: the logits outputs of the model.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
-  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs, num_classes],
+  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs],
-      net, end_points = inception_resnet_v2_base(inputs, scope=scope)
+      net, end_points = inception_resnet_v2_base(inputs, scope=scope,
-      if create_aux_logits:
+      if create_aux_logits and num_classes:
-                              scope='AvgPool_1a_8x8')
+        # TODO(sguada,arnoegw): Consider adding a parameter global_pool which
-                                  batch_norm_epsilon=0.001):
+                                  batch_norm_epsilon=0.001,
-    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,
+    with slim.arg_scope([slim.conv2d], activation_fn=activation_fn,
-                        batch_norm_epsilon=0.001):
+                        batch_norm_epsilon=0.001,
-        activation_fn=tf.nn.relu,
+        activation_fn=activation_fn,
-                 scope='InceptionV1'):
+                 scope='InceptionV1',
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
-      [batch_size, num_classes]
+    net: a Tensor with the logits (pre-softmax activations) if num_classes
-                         reuse=reuse) as scope:
+  with tf.variable_scope(scope, 'InceptionV1', [inputs], reuse=reuse) as scope:
-                           dropout_keep_prob, scope='Dropout_0b')
+        if global_pool:
-    self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
+    self.assertTrue(logits.op.name.startswith(
-                 scope='InceptionV2'):
+                 scope='InceptionV2',
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
-      [batch_size, num_classes]
+    net: a Tensor with the logits (pre-softmax activations) if num_classes
-                         reuse=reuse) as scope:
+  with tf.variable_scope(scope, 'InceptionV2', [inputs], reuse=reuse) as scope:
-                              scope='AvgPool_1a_{}x{}'.format(*kernel_size))
+        if global_pool:
-    self.assertTrue(logits.op.name.startswith('InceptionV2/Logits'))
+    self.assertTrue(logits.op.name.startswith(
-                 scope='InceptionV3'):
+                 scope='InceptionV3',
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
-      [batch_size, num_classes]
+    net: a Tensor with the logits (pre-softmax activations) if num_classes
-                         reuse=reuse) as scope:
+  with tf.variable_scope(scope, 'InceptionV3', [inputs], reuse=reuse) as scope:
-      if create_aux_logits:
+      if create_aux_logits and num_classes:
-                              scope='AvgPool_1a_{}x{}'.format(*kernel_size))
+        if global_pool:
-    self.assertTrue(logits.op.name.startswith('InceptionV3/Logits'))
+    self.assertTrue(logits.op.name.startswith(
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
-    logits: the logits outputs of the model.
+    net: a Tensor with the logits (pre-softmax activations) if num_classes
-        if create_aux_logits:
+        if create_aux_logits and num_classes:
-                                scope='AvgPool_1a')
+          kernel_size = net.get_shape()[1:3]
-    num_classes: the number of classes in the dataset.
+    num_classes: the number of classes in the dataset. If 0 or None, the logits
-      [batch_size, `num_classes`]
+     net: a 2D Tensor with the logits (pre-softmax activations) if num_classes
-    net = slim.max_pool2d(net, [2, 2], 2, scope='pool2')
+  with tf.variable_scope(scope, 'LeNet', [images]):
-                                  scope='fc4')
+    net = end_points['fc3'] = slim.fully_connected(net, 1024, scope='fc3')
-      'Conv2d_3_pointwise', 'Conv2d_4_pointwise', 'Conv2d_5_pointwise',
+      'Conv2d_3_pointwise', 'Conv2d_4_pointwise', 'Conv2d_5'_pointwise,
-                 scope='MobilenetV1'):
+                 scope='MobilenetV1',
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
-      [batch_size, num_classes]
+    net: a 2D Tensor with the logits (pre-softmax activations) if num_classes
-                         reuse=reuse) as scope:
+  with tf.variable_scope(scope, 'MobilenetV1', [inputs], reuse=reuse) as scope:
-        end_points['AvgPool_1a'] = net
+        if global_pool:
-    self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
+    self.assertTrue(logits.op.name.startswith(
-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
-    num_classes: The number of classes to use for classification.
+    num_classes: The number of classes to use for classification. If 0 or None,
-        logits, end_points = network_fn(images)
+          net, end_points = network_fn(images)
-  def network_fn(images):
+  def network_fn(images, **kwargs):
-      return func(images, num_classes, is_training=is_training)
+      return func(images, num_classes, is_training=is_training, **kwargs)
-             scope='overfeat'):
+             scope='overfeat',
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer is
-
+    net: the output of the logits layer (if num_classes is a non-zero integer),
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
-        end_points[sc.name + '/fc8'] = net
+        # Convert end_points_collection into a end_point dict.
-                                            sc.original_name_scope,
+                                            sc.name,
-    is_training: whether is training or not.
+    num_classes: Number of predicted classes for classification tasks.
-      activations.
+      else both height_out and width_out equal one. If num_classes is 0 or None,
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
-        if num_classes is not None:
+          end_points['global_pool'] = net
-        if num_classes is not None:
+            end_points[sc.name + '/spatial_squeeze'] = net
-   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):
+   with slim.arg_scope(resnet_v2.resnet_arg_scope()):
-                                            sc.original_name_scope,
+                                            sc.name,
-    is_training: whether is training or not.
+    num_classes: Number of predicted classes for classification tasks.
-      activations.
+      else both height_out and width_out equal one. If num_classes is 0 or None,
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
-        if num_classes is not None:
+            end_points[sc.name + '/spatial_squeeze'] = net
-          fc_conv_padding='VALID'):
+          fc_conv_padding='VALID',
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer is
-    the last op containing the log predictions and end_points dict.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
-        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
+      if global_pool:
-           fc_conv_padding='VALID'):
+           fc_conv_padding='VALID',
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer is
-    the last op containing the log predictions and end_points dict.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
-        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
+      if global_pool:
-           fc_conv_padding='VALID'):
+           fc_conv_padding='VALID',
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer is
-
+    global_pool: Optional boolean flag. If True, the input to the classification
-    the last op containing the log predictions and end_points dict.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
-        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
+      if global_pool:
-                         padding=_PADDING):
+                         padding=_PADDING,
-  tf.summary.image('image', tf.expand_dims(image, 0))
+  if add_image_summaries:
-  tf.summary.image('distorted_image', tf.expand_dims(distorted_image, 0))
+  if add_image_summaries:
-def preprocess_for_eval(image, output_height, output_width):
+def preprocess_for_eval(image, output_height, output_width,
-  tf.summary.image('image', tf.expand_dims(image, 0))
+  if add_image_summaries:
-  tf.summary.image('resized_image', tf.expand_dims(resized_image, 0))
+  if add_image_summaries:
-def preprocess_image(image, output_height, output_width, is_training=False):
+def preprocess_image(image, output_height, output_width, is_training=False,
-    return preprocess_for_train(image, output_height, output_width)
+    return preprocess_for_train(
-    return preprocess_for_eval(image, output_height, output_width)
+    return preprocess_for_eval(
-                         scope=None):
+                         scope=None,
-    tf.summary.image('image_with_bounding_boxes', image_with_box)
+    if add_image_summaries:
-                     image_with_distorted_box)
+    if add_image_summaries:
-                     tf.expand_dims(distorted_image, 0))
+    if add_image_summaries:
-                     tf.expand_dims(distorted_image, 0))
+    if add_image_summaries:
-                     fast_mode=True):
+                     fast_mode=True,
-    return preprocess_for_train(image, height, width, bbox, fast_mode)
+    return preprocess_for_train(image, height, width, bbox, fast_mode,
-      FLAGS.output_directory)
+  if FLAGS.input_shape:
-    optimize_graph=False,
+    optimize_graph=True,
-                                name='image_tensor')
+  If the current checkpoint has shadow variables maintaining moving averages of
-    a tuple of placeholder and input nodes that output decoded images.
+    a tuple of input placeholder and the output decoded images.
-    a tuple of placeholder and input nodes that output decoded images.
+    a tuple of input placeholder and the output decoded images.
-                            optimize_graph=False,
+                            additional_output_tensor_names=None,
-  placeholder_tensor, input_tensors = input_placeholder_fn_map[input_type]()
+  placeholder_args = {}
-    saver = tf.train.Saver(variables_to_restore)
+    temp_checkpoint_file = tempfile.NamedTemporaryFile()
-    saver = tf.train.Saver()
+    checkpoint_to_use = trained_checkpoint_prefix
-      trained_checkpoint_prefix=trained_checkpoint_prefix)
+      trained_checkpoint_prefix=checkpoint_to_use)
-      output_node_names=','.join(outputs.keys()),
+      input_checkpoint=checkpoint_to_use,
-                     outputs)
+  _write_saved_model(saved_model_path, frozen_graph_def,
-                           output_collection_name='inference_op'):
+                           input_shape=None,
-                          optimize_graph, output_collection_name)
+                          trained_checkpoint_prefix,
-        tf.saved_model.loader.load(
+        meta_graph = tf.saved_model.loader.load(
-        num_detections = od_graph.get_tensor_by_name('num_detections:0')
+
-from object_detection.protos import pipeline_pb2
+from object_detection.utils import config_util
-
+flags.DEFINE_boolean('run_once', False, 'Option to only run a single pass of '
-
+  tf.gfile.MakeDirs(FLAGS.eval_dir)
-    model_config, eval_config, input_config = get_configs_from_pipeline_file()
+    configs = config_util.get_configs_from_pipeline_file(
-    model_config, eval_config, input_config = get_configs_from_multiple_files()
+    configs = config_util.get_configs_from_multiple_files(
-import copy
+"""Common functions for repeatedly evaluating a checkpoint."""
-from object_detection.utils import object_detection_evaluation
+from object_detection.utils import ops
-              image)))
+      tf.Summary.Value(
-  tensor.
+def _run_checkpoint_once(tensor_dict,
-        in tensor_dict.  The length of each such list is num_batches.
+    evaluators: a list of object of type DetectionEvaluator to be used for
-      has only one directory, EnsembleModel will not be used -- a DetectionModel
+      has only one directory, EnsembleModel will not be used --
-      still be evaluated for each batch, but won't be added to results_list.
+
-            (result_dict, _) = sess.run([tensor_dict, update_op])
+            result_dict = sess.run(tensor_dict)
-        other_metrics = sess.run(metric_names_to_values)
+          result_dict = batch_processor(tensor_dict, sess, batch, counters)
-      write_metrics(metrics, global_step, summary_dir)
+      all_evaluator_metrics = {}
-                            aggregated_result_processor=None,
+                            evaluators,
-                            keys_to_exclude_from_results=()):
+                            save_graph_dir=''):
-        in tensor_dict.  The length of each such list is num_batches.
+    evaluators: a list of object of type DetectionEvaluator to be used for
-      still be evaluated for each batch, but won't be added to results_list.
+
-                                                           time.gmtime()))
+    logging.info('Starting evaluation at ' + time.strftime(
-                          keys_to_exclude_from_results)
+      global_step, metrics = _run_checkpoint_once(tensor_dict, evaluators,
-
+
-    'pascal_voc_metrics': eval_util.evaluate_detection_results_pascal_voc
+from object_detection.utils import object_detection_evaluation
-  # load groundtruth fields into tensor_dict
+  groundtruth = None
-  return tensor_dict
+    groundtruth = {
-  def _process_batch(tensor_dict, sess, batch_index, counters, update_op):
+  def _process_batch(tensor_dict, sess, batch_index, counters):
-      (result_dict, _) = sess.run([tensor_dict, update_op])
+      result_dict = sess.run(tensor_dict)
-    global_step = tf.train.global_step(sess, slim.get_global_step())
+    global_step = tf.train.global_step(sess, tf.train.get_global_step())
-          result_dict, tag, global_step, categories=categories,
+          result_dict,
-  global_step = slim.get_or_create_global_step()
+  global_step = tf.train.get_or_create_global_step()
-  eval_util.repeated_checkpoint_run(
+  metrics = eval_util.repeated_checkpoint_run(
-      aggregated_result_processor=_process_aggregated_results,
+      evaluators=get_evaluators(eval_config, categories),
-          None),
+      max_number_of_evaluations=(1 if eval_config.ignore_groundtruth else
-from object_detection.protos import train_pb2
+from object_detection.utils import config_util
-
+  if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)
-    model_config, train_config, input_config = get_configs_from_pipeline_file()
+    configs = config_util.get_configs_from_pipeline_file(
-    model_config, train_config, input_config = get_configs_from_multiple_files()
+    configs = config_util.get_configs_from_multiple_files(
-                        prefetch_queue_capacity, data_augmentation_options):
+def create_input_queue(batch_size_per_clone, create_tensor_dict_fn,
-                                          data_augmentation_options)
+    tensor_dict = preprocessor.preprocess(
-  """Dequeue batch and construct inputs to object detection model.
+def get_inputs(input_queue, num_classes, merge_multiple_label_boxes=False):
-                                                  depth=num_classes, left_pad=0)
+    if merge_multiple_label_boxes:
-    return image, location_gt, classes_gt, masks_gt
+    keypoints_gt = read_data.get(fields.InputDataFields.groundtruth_keypoints)
-def _create_losses(input_queue, create_model_fn):
+def _create_losses(input_queue, create_model_fn, train_config):
-  ) = _get_inputs(input_queue, detection_model.num_classes)
+  (images, _, groundtruth_boxes_list, groundtruth_classes_list,
-                                      groundtruth_masks_list)
+                                      groundtruth_masks_list,
-                                        data_augmentation_options)
+      input_queue = create_input_queue(
-                                 create_model_fn=create_model_fn)
+                                 create_model_fn=create_model_fn,
-            ssd_anchor_generator_config.reduce_boxes_in_lowest_layer))
+        reduce_boxes_in_lowest_layer=(ssd_anchor_generator_config
-  def test_build_ssd_anchor_generator_without_reduced_boxes(self):
+  def test_build_ssd_anchor_generator_withoud_reduced_boxes(self):
-        [(0.1, 0.3, 0.3), (0.8, 0.894)]):
+        [(0.1, 0.3, 0.3), (0.8,)]):
-        [(1.0, 2.0, 0.5), (2.0, 1.0)]):
+        [(1.0, 2.0, 0.5), (2.0,)]):
-
+    anchor_strides = None
-                                      .reduce_boxes_in_lowest_layer))
+        interpolated_scale_aspect_ratio=(
-
+import math
-  def test_build_ssd_anchor_generator_withoud_reduced_boxes(self):
+  def test_build_ssd_anchor_generator_with_custom_scales(self):
-        [(0.1, 0.3, 0.3), (0.8,)]):
+        [(0.1, 0.3, 0.3), (0.8, 0.894)]):
-        [(1.0, 2.0, 0.5), (2.0,)]):
+        [(1.0, 2.0, 0.5), (2.0, 1.0)]):
-                               faster_rcnn_box_coder.FasterRcnnBoxCoder))
+    self.assertIsInstance(box_coder_object,
-                               faster_rcnn_box_coder.FasterRcnnBoxCoder))
+    self.assertIsInstance(box_coder_object,
-        apply_sigmoid_to_scores=conv_box_predictor.apply_sigmoid_to_scores)
+        apply_sigmoid_to_scores=conv_box_predictor.apply_sigmoid_to_scores,
-
+import tensorflow as tf
-            <= keep_aspect_ratio_config.max_dimension):
+    if not (keep_aspect_ratio_config.min_dimension <=
-        max_dimension=keep_aspect_ratio_config.max_dimension)
+        max_dimension=keep_aspect_ratio_config.max_dimension,
-                             new_width=fixed_shape_resizer_config.width)
+    method = _tf_resize_method(fixed_shape_resizer_config.resize_method)
-
+import numpy as np
-      self, input_shape, text_proto):
+  def _shape_of_resized_random_image_given_text_proto(self, input_shape,
-        input_shape, minval=0, maxval=255, dtype=tf.int32))
+    images = tf.to_float(
-
+    ValueError: If no input paths are specified.
-        config.input_path,
+        config.input_path[:],  # Convert `RepeatedScalarContainer` to list.
-    return tf_example_decoder.TfExampleDecoder().decode(string_tensor)
+    label_map_proto_file = None
-        anchorwise_output=config.anchorwise_output)
+        anchorwise_output=config.anchorwise_output,
-        slim.get_or_create_global_step(),
+        tf.train.get_or_create_global_step(),
-        slim.get_or_create_global_step(), learning_rate_step_boundaries,
+        tf.train.get_or_create_global_step(), learning_rate_step_boundaries,
-  global_summaries.add(tf.summary.scalar('Learning Rate', learning_rate))
+  global_summaries.add(tf.summary.scalar('Learning_Rate', learning_rate))
-    # TODO: Find a way to not depend on the private members.
+    # TODO(rathodv): Find a way to not depend on the private members.
-    'random_horizontal_flip': preprocessor.random_horizontal_flip,
+  if step_type == 'random_horizontal_flip':
-        min_padded_size_ratio: [0.0, 0.0]
+        min_padded_size_ratio: [1.0, 1.0]
-        min_padded_size_ratio: [0.0, 0.0]
+        min_padded_size_ratio: [1.0, 1.0]
-                            'min_padded_size_ratio': [(0.0, 0.0), (0.0, 0.0)],
+                            'min_padded_size_ratio': [(1.0, 1.0), (1.0, 1.0)],
-def convert_to(data_set, name):
+def convert_to(dataset, name, directory):
-  num_examples = data_set.num_examples
+  images = dataset.images
-  filename = os.path.join(FLAGS.directory, name + '.tfrecords')
+  filename = os.path.join(directory, name + '.tfrecords')
-  data_sets = mnist.read_data_sets(FLAGS.directory,
+  datasets = mnist.read_data_sets(FLAGS.directory,
-  convert_to(data_sets.test, 'test')
+  convert_to(datasets.train, 'train', FLAGS.directory)
-def input_fn(mode, batch_size=1):
+def input_fn(is_training, filename, batch_size=1, num_epochs=1):
-    tfrecords_file = os.path.join(FLAGS.data_dir, 'test.tfrecords')
+  dataset = tf.contrib.data.TFRecordDataset([filename])
-      'file format.')
+  if is_training:
-    dataset = dataset.repeat()
+  dataset = dataset.repeat(num_epochs)
-def mnist_model(inputs, mode):
+def mnist_model(inputs, mode, data_format):
-def mnist_model_fn(features, labels, mode):
+def mnist_model_fn(features, labels, mode, params):
-  logits = mnist_model(features, mode)
+  logits = mnist_model(features, mode, params['data_format'])
-      model_fn=mnist_model_fn, model_dir=FLAGS.model_dir)
+      model_fn=mnist_model_fn, model_dir=FLAGS.model_dir,
-  # Train the model
+  # Set up training hook that logs the training accuracy every 100 steps.
-
+  # Train the model
-      steps=FLAGS.train_epochs * batches_per_epoch,
+      input_fn=lambda: input_fn(
-      input_fn=lambda: input_fn(tf.estimator.ModeKeys.EVAL))
+      input_fn=lambda: input_fn(False, test_file, FLAGS.batch_size))
-  print('Evaluation results:\n    %s' % eval_results)
+  print('Evaluation results:\n\t%s' % eval_results)
-    spec = mnist.mnist_model_fn(features, labels, mode)
+    spec = mnist.mnist_model_fn(
-  mnist.FLAGS = mnist.parser.parse_args()
+_SHUFFLE_BUFFER = 20000
-def get_filenames(is_training):
+def get_filenames(is_training, data_dir):
-  data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')
+  data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')
-def input_fn(is_training, num_epochs=1):
+def input_fn(is_training, data_dir, batch_size, num_epochs=1):
-  dataset = record_dataset(get_filenames(is_training))
+  dataset = record_dataset(get_filenames(is_training, data_dir))
-                        output_buffer_size=2 * FLAGS.batch_size)
+                        output_buffer_size=2 * batch_size)
-                          output_buffer_size=2 * FLAGS.batch_size)
+                          output_buffer_size=2 * batch_size)
-    dataset = dataset.shuffle(buffer_size=buffer_size)
+    # When choosing shuffle buffer sizes, larger sizes result in better
-      output_buffer_size=2 * FLAGS.batch_size)
+      output_buffer_size=2 * batch_size)
-  iterator = dataset.batch(FLAGS.batch_size).make_one_shot_iterator()
+  iterator = dataset.batch(batch_size).make_one_shot_iterator()
-def cifar10_model_fn(features, labels, mode):
+def cifar10_model_fn(features, labels, mode, params):
-      FLAGS.resnet_size, _NUM_CLASSES, FLAGS.data_format)
+      params['resnet_size'], _NUM_CLASSES, params['data_format'])
-    batches_per_epoch = _NUM_IMAGES['train'] / FLAGS.batch_size
+    initial_learning_rate = 0.1 * params['batch_size'] / 128
-      model_fn=cifar10_model_fn, model_dir=FLAGS.model_dir, config=run_config)
+      model_fn=cifar10_model_fn, model_dir=FLAGS.model_dir, config=run_config,
-            is_training=True, num_epochs=FLAGS.epochs_per_eval),
+            True, FLAGS.data_dir, FLAGS.batch_size, FLAGS.epochs_per_eval),
-        input_fn=lambda: input_fn(is_training=False))
+        input_fn=lambda: input_fn(False, FLAGS.data_dir, FLAGS.batch_size))
-    features = tf.random_uniform([FLAGS.batch_size, 32, 32, 3])
+    features = tf.random_uniform([_BATCH_SIZE, 32, 32, 3])
-        [FLAGS.batch_size], maxval=9, dtype=tf.int32)
+        [_BATCH_SIZE], maxval=9, dtype=tf.int32)
-    spec = cifar10_main.cifar10_model_fn(features, labels, mode)
+    spec = cifar10_main.cifar10_model_fn(
-                        (FLAGS.batch_size, 10))
+                        (_BATCH_SIZE, 10))
-    self.assertAllEqual(predictions['classes'].shape, (FLAGS.batch_size,))
+    self.assertAllEqual(predictions['classes'].shape, (_BATCH_SIZE,))
-  FLAGS = cifar10_main.FLAGS
+_SHUFFLE_BUFFER = 1500
-def filenames(is_training):
+
-        os.path.join(FLAGS.data_dir, 'train-%05d-of-01024' % i)
+        os.path.join(data_dir, 'train-%05d-of-01024' % i)
-        os.path.join(FLAGS.data_dir, 'validation-%05d-of-00128' % i)
+        os.path.join(data_dir, 'validation-%05d-of-00128' % i)
-def input_fn(is_training, num_epochs=1):
+def input_fn(is_training, data_dir, batch_size, num_epochs=1):
-  dataset = tf.contrib.data.Dataset.from_tensor_slices(filenames(is_training))
+  dataset = tf.contrib.data.Dataset.from_tensor_slices(
-                        output_buffer_size=FLAGS.batch_size)
+                        output_buffer_size=batch_size)
-    dataset = dataset.shuffle(buffer_size=buffer_size)
+    # When choosing shuffle buffer sizes, larger sizes result in better
-  iterator = dataset.batch(FLAGS.batch_size).make_one_shot_iterator()
+  iterator = dataset.batch(batch_size).make_one_shot_iterator()
-def resnet_model_fn(features, labels, mode):
+def resnet_model_fn(features, labels, mode, params):
-      FLAGS.resnet_size, _LABEL_CLASSES, FLAGS.data_format)
+      params['resnet_size'], _LABEL_CLASSES, params['data_format'])
-    batches_per_epoch = _NUM_IMAGES['train'] / FLAGS.batch_size
+    initial_learning_rate = 0.1 * params['batch_size'] / 256
-      model_fn=resnet_model_fn, model_dir=FLAGS.model_dir, config=run_config)
+      model_fn=resnet_model_fn, model_dir=FLAGS.model_dir, config=run_config,
-            is_training=True, num_epochs=FLAGS.epochs_per_eval),
+            True, FLAGS.data_dir, FLAGS.batch_size, FLAGS.epochs_per_eval),
-        input_fn=lambda: input_fn(is_training=False))
+        input_fn=lambda: input_fn(False, FLAGS.data_dir, FLAGS.batch_size))
-    features = tf.random_uniform([FLAGS.batch_size, 224, 224, 3])
+    features = tf.random_uniform([_BATCH_SIZE, 224, 224, 3])
-            [FLAGS.batch_size], maxval=_LABEL_CLASSES - 1,
+            [_BATCH_SIZE], maxval=_LABEL_CLASSES - 1,
-    spec = imagenet_main.resnet_model_fn(features, labels, mode)
+    spec = imagenet_main.resnet_model_fn(
-                        (FLAGS.batch_size, _LABEL_CLASSES))
+                        (_BATCH_SIZE, _LABEL_CLASSES))
-    self.assertAllEqual(predictions['classes'].shape, (FLAGS.batch_size,))
+    self.assertAllEqual(predictions['classes'].shape, (_BATCH_SIZE,))
-      # TODO: Remove with tf.device when top_k operation runs correctly on GPU.
+      # TODO: Remove with tf.device when top_k operation runs
-  TODO: Change function name to FilterScoresGreaterThan
+  TODO: Change function name to filter_scores_greater_than
-                            check_range=True, scope=None):
+def to_absolute_coordinates(boxlist,
-  value is larger than 1.01 (in which case coordinates are already absolute).
+  value is larger than maximum_normalized_coordinate (in which case coordinates
-                              'than 1.01: ', box_maximum])
+      max_assert = tf.Assert(
-  # the original boxes without voting.
+  # TODO: Handle the case where some boxes in selected_boxes do not
-      raise ValueError('Mask prediction is unimplemented.')
+    Also optionally predicts instance masks.
-        upsampled_features = slim.conv2d_transpose(
+        upsampled_features = tf.image.resize_bilinear(
-            stride=2)
+            kernel_size=[2, 2])
-                                       kernel_size=[1, 1])
+                                       kernel_size=[3, 3])
-               apply_sigmoid_to_scores=False):
+               apply_sigmoid_to_scores=False,
-      # Add additional conv layers before the predictor.
+      # Add additional conv layers before the class predictor.
-            [self._kernel_size, self._kernel_size], scope='ClassPredictor')
+            [self._kernel_size, self._kernel_size], scope='ClassPredictor',
-    """Method to be overriden by implementations.
+    """Method to be overridden by implementations.
-  def __init__(self, anchorwise_output=False):
+  def __init__(self, anchorwise_output=False, logit_scale=1.0):
-
+  def groundtruth_has_field(self, field):
-      and the first non-background class is mapped to 0.
+      and the first non-background class is mapped to 0. If the model produces
-        shape [max_detections, height_in, width_in] containing instance
+      groundtruth_masks_list: a list of 3-D tf.float32 tensors of
-        shape [batch, max_detections, num_keypoints, 2] containing keypoints.
+      groundtruth_keypoints_list: a list of 3-D tf.float32 tensors of
-    boxes, labels, label_scores, masks=None, keypoints=None, threshold=0.0):
+def retain_boxes_above_threshold(boxes,
-  """Left-right flips masks.
+def _flip_boxes_left_right(boxes):
-  """Randomly decides whether to mirror the image and detections or not.
+def _flip_masks_up_down(masks):
-    keypoint_flip_permutation: rank 1 int32 tensor containing keypoint flip
+    keypoint_flip_permutation: rank 1 int32 tensor containing the keypoint flip
-    If boxes, masks, keypoints, and keypoint_flip_permutation is not None,
+    If boxes, masks, keypoints, and keypoint_flip_permutation are not None,
-        tf.greater(tf.size(boxes), 0), tf.greater(do_a_flip_random, 0.5))
+    do_a_flip_random = tf.greater(tf.random_uniform([], seed=seed), 0.5)
-          do_a_flip_random, lambda: flip_boxes(boxes), lambda: boxes)
+      boxes = tf.cond(do_a_flip_random, lambda: _flip_boxes_left_right(boxes),
-          do_a_flip_random, lambda: _flip_masks(masks), lambda: masks)
+      masks = tf.cond(do_a_flip_random, lambda: _flip_masks_left_right(masks),
-
+    If label_scores, masks, or keypoints is not None, the function also returns:
-
+    If label_scores, masks, or keypoints are not None, the function also
-                     strict_random_crop_image_fn,
+    result = tf.cond(do_a_crop_random, strict_random_crop_image_fn,
-    pad_color = tf.reduce_mean(image, reduction_indices=[0, 1])
+    pad_color = tf.reduce_mean(image, axis=[0, 1])
-      target_height=target_height, target_width=target_width)
+      image,
-  new_image += image_color_paded
+      image_ones,
-                          max_padded_size_ratio=None,
+                          min_padded_size_ratio=(1.0, 1.0),
-                           be set to [0.0, 0.0].
+                           input image's height and width.
-                           be set to [2.0, 2.0].
+                           input image's height and width.
-  cropped_image, cropped_boxes, cropped_labels = random_crop_image(
+  result = random_crop_image(
-  return padded_image, padded_boxes, cropped_labels
+  cropped_padded_output = (padded_image, padded_boxes, cropped_labels)
-
+    If label_scores, masks, or keypoints is not None, the function also returns:
-        target_height_fn)
+      return tf.to_int32(tf.round(tf.to_float(orig_width) / new_aspect_ratio))
-        target_width_fn)
+      return tf.to_int32(tf.round(tf.to_float(orig_height) * new_aspect_ratio))
-                                                  tf.float32))
+                                              tf.constant([0.0, 0.0, 1.0, 1.0],
-                                      dtype=tf.float32, seed=random_seed)
+      random_prob = tf.random_uniform(
-                             max_dimension):
+def _compute_new_static_size(image, min_dimension, max_dimension):
-                              max_dimension):
+def _compute_new_dynamic_size(image, min_dimension, max_dimension):
-                                          max_dimension)
+      new_size = _compute_new_static_size(image, min_dimension, max_dimension)
-                                       align_corners=align_corners)
+      new_size = _compute_new_dynamic_size(image, min_dimension, max_dimension)
-                                                   align_corners=align_corners)
+      new_masks = tf.image.resize_nearest_neighbor(
-                                       align_corners=align_corners)
+    new_image = tf.image.resize_images(
-                      resize_masks_branch,
+      masks = tf.cond(num_instances > 0, resize_masks_branch,
-
+    If label_scores, masks, or keypoints is not None, the function also returns:
-          t for t in (image, boxes, labels, masks, keypoints) if t is not None),
+          t for t in (image, boxes, labels, label_scores, masks, keypoints)
-                        max_padded_size_ratio=(None,) * 6,
+                        min_padded_size_ratio=((1.0, 1.0),) * 6,
-                           be set to [0.0, 0.0].
+                           input image's height and width.
-                           be set to [2.0, 2.0].
+                           input image's height and width.
-    image, boxes, labels = image_boxes_labels
+    i = 3
-      (image, boxes, labels),
+  return _apply_with_random_selector_tuples(
-  return new_image, new_boxes, new_labels
+    label_scores=None,
-    If masks, or keypoints is not None, the function also returns:
+    If masks or keypoints is not None, the function also returns:
-                                area_range, overlap_thresh, random_coef, seed)
+  crop_result = ssd_random_crop(
-def get_default_func_arg_map(include_instance_masks=False,
+def ssd_random_crop_pad_fixed_aspect_ratio(
-                               groundtruth_keypoints,),
+      random_horizontal_flip: (
-                           groundtruth_instance_masks,),
+      random_image_scale: (
-                          groundtruth_keypoints,),
+      random_crop_image: (
-                                    groundtruth_keypoints,),
+                              fields.InputDataFields.groundtruth_classes,
-          fields.InputDataFields.groundtruth_label_scores,
+          groundtruth_label_scores,
-                        groundtruth_instance_masks,),
+      resize_to_range: (
-                     groundtruth_instance_masks,),
+      resize_image: (
-                        groundtruth_keypoints,),
+      ssd_random_crop: (
-                            fields.InputDataFields.groundtruth_classes),
+                            fields.InputDataFields.groundtruth_classes,
-  import mock # pylint: disable=g-import-not-at-top
+  import mock  # pylint: disable=g-import-not-at-top
-  from unittest import mock # pylint: disable=g-import-not-at-top
+  from unittest import mock  # pylint: disable=g-import-not-at-top
-  def expectedImagesAfterMirroring(self):
+  def expectedImagesAfterLeftRightFlip(self):
-  def expectedBoxesAfterMirroring(self):
+  def expectedImagesAfterUpDownFlip(self):
-    boxes = tf.constant([[0.25, 0.0, 1.0, 0.75], [0.5, 0.25, 1, 0.75]],
+  def expectedBoxesAfterUpDownFlip(self):
-  def expectedMasksAfterMirroring(self):
+  def expectedBoxesAfterRot90(self):
-  def testRandomFlipBoxes(self):
+  def testFlipBoxesLeftRight(self):
-    boxes_expected2 = boxes
+  def testFlipBoxesUpDown(self):
-    expected_result = tf.zeros_like(boxes_diff)
+  def testRot90Boxes(self):
-      self.assertAllEqual(boxes_diff, expected_result)
+      flipped_mask, expected_mask = sess.run([flipped_mask, expected_mask])
-  def testFlipMasks(self):
+  def testFlipMasksUpDown(self):
-    expected_mask = self.expectedMasksAfterMirroring()
+    flipped_mask = preprocessor._flip_masks_up_down(test_mask)
-    boxes_expected1 = self.expectedBoxesAfterMirroring()
+    images_expected1 = self.expectedImagesAfterLeftRightFlip()
-                   fields.InputDataFields.groundtruth_classes: labels}
+    tensor_dict = {
-        fields.InputDataFields.groundtruth_classes: labels
+        fields.InputDataFields.groundtruth_classes: labels,
-                   fields.InputDataFields.groundtruth_classes: labels}
+    tensor_dict = {
-                   fields.InputDataFields.groundtruth_classes: labels}
+    label_scores = self.createTestLabelScores()
-       boxes_, distorted_boxes_, labels_, distorted_labels_) = sess.run(
+       boxes_, distorted_boxes_, labels_, distorted_labels_,
-            boxes, distorted_boxes, labels, distorted_labels])
+            boxes, distorted_boxes, labels, distorted_labels,
-                   fields.InputDataFields.groundtruth_classes: labels}
+
-           image, boxes, labels, masks=masks)
+      new_image, new_boxes, new_labels, new_masks = (
-        ], dtype=np.float32)
+        new_image, new_boxes, new_labels, new_masks = sess.run(
-           image, boxes, labels, keypoints=keypoints)
+      new_image, new_boxes, new_labels, new_keypoints = (
-            new_image, new_boxes, new_labels, new_keypoints])
+        new_image, new_boxes, new_labels, new_keypoints = sess.run(
-        ], dtype=np.float32)
+            [0.23157893, 0.24050637, 0.75789469, 1.0],], dtype=np.float32)
-
+    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
-        tensor_dict, preprocessing_options)
+        tensor_dict, preprocessing_options, func_arg_map=preprocessor_arg_map)
-                   fields.InputDataFields.groundtruth_classes: labels}
+    tensor_dict = {
-                   fields.InputDataFields.groundtruth_classes: labels}
+    tensor_dict = {
-        fields.InputDataFields.groundtruth_classes: labels
+        fields.InputDataFields.groundtruth_classes: labels,
-    tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
+    tensor_dict = preprocessor.preprocess(tensor_dict, [])
-    expected_masks_shape_list = [[0, 50, 100], [0, 50, 100]]
+    min_dim = 50
-          in_image, in_masks, new_height=height, new_width=width)
+      out_image, out_masks = preprocessor.resize_to_range(
-                   fields.InputDataFields.groundtruth_classes: labels}
+    tensor_dict = {
-                   fields.InputDataFields.groundtruth_classes: labels}
+    tensor_dict = {
-  def testSSDRandomCropFixedAspectRatio(self):
+  def _testSSDRandomCropFixedAspectRatio(self,
-    distorted_boxes_rank = tf.rank(distorted_boxes)
+    if include_label_scores:
-        include_instance_masks=True, include_keypoints=True)
+        include_label_scores=include_label_scores,
-
+  def testSSDRandomCropFixedAspectRatio(self):
-    groundtruth_is_crowd: is the groundtruth a single object or a crowd.
+    groundtruth_is_crowd: [DEPRECATED, use groundtruth_group_of instead]
-    object_class_text: labels in numbers, e.g. [16, 8]
+    object_class_label: labels in numbers, e.g. [16, 8]
-    object_is_crowd: is the object a single object or a crowd
+    object_group_of: is object a single object or a group of objects
-    """Construct Multibox Target Assigner.
+    """Construct Object Detection Target Assigner.
-      groundtruth_labels:  a tensor of shape [num_gt_boxes, d_1, ... d_k]
+      groundtruth_labels:  a tensor of shape [M, d_1, ... d_k]
-    with tf.control_dependencies([shape_assert]):
+    unmatched_shape_assert = tf.assert_equal(
-# Therefore its best to have this factory method outside of core.
+# TODO: This method pulls in all the implementation dependencies into
-def get_label_map_dict(label_map_path):
+def get_label_map_dict(label_map_path, use_display_name=False):
-    label_map_dict[item.name] = item.id
+    if use_display_name:
-
+import numpy as np
-      tf.greater(step_boundaries, global_step)), [-1])
+  unreached_boundaries = tf.reshape(
-  if not all(recall[i] <= recall[i + 1] for i in moves.range(len(recall) - 1)):
+  if not all(recall[i] <= recall[i + 1] for i in range(len(recall) - 1)):
-      for i in moves.range(data.shape[0]):
+      for i in range(data.shape[0]):
-
+from abc import ABCMeta
-  """Evaluate Object Detection Result."""
+  """Internal implementation of Pascal object detection metrics."""
-               nms_max_output_boxes=10000):
+               nms_max_output_boxes=10000,
-    """Add ground truth info of a single image into the evaluation database.
+                                         groundtruth_is_difficult_list=None,
-          labels
+      image_key: A unique string/integer identifier for the image.
-          the case that no boxes are difficult, it is by default set as None.
+        whether a ground truth box is a difficult instance or not. To support
-                                         groundtruth_is_difficult_list)
+    if groundtruth_is_group_of_list is None:
-    """Add detected result of a single image into the evaluation database.
+    """Adds detections for a single image to be used for evaluation.
-          labels
+      image_key: A unique string/integer identifier for the image.
-                  do not have the same length.
+      ValueError: if the number of boxes, scores and class labels differ in
-            groundtruth_is_difficult_list))
+            groundtruth_is_difficult_list, groundtruth_is_group_of_list))
-      self.tp_fp_labels_per_class[i].append(tp_fp_labels[i])
+      if scores[i].shape[0] > 0:
-                                      groundtruth_is_difficult_list):
+                                      groundtruth_is_difficult_list,
-          ~groundtruth_is_difficult_list] == class_index)
+          ~groundtruth_is_difficult_list
-      mean_corloc: Mean CorLoc score for each class, float scalar
+      A named tuple with the following fields -
-          np.squeeze(np.argwhere(self.num_gt_instances_per_class == 0)))
+          np.squeeze(np.argwhere(self.num_gt_instances_per_class == 0)) +
-      tp_fp_labels = np.concatenate(self.tp_fp_labels_per_class[class_index])
+      if not self.scores_per_class[class_index]:
-    mean_ap = np.nanmean(self.average_precision_per_class)
+    if self.use_weighted_mean_ap:
-    self.average_precisions = average_precisions
+    return ObjectDetectionEvalMetrics(
-    image_key1 = "img1"
+    image_key1 = 'img1'
-    image_key2 = "img2"
+    image_key2 = 'img2'
-    image_key3 = "img3"
+        groundtruth_is_difficult_list2, groundtruth_is_group_of_list2)
-    image_key = "img2"
+    image_key = 'img2'
-    expected_num_gt_instances_per_class = np.array([3, 1, 2], dtype=int)
+    expected_num_gt_instances_per_class = np.array([3, 1, 1], dtype=int)
-    self.assertTrue(np.allclose(self.od_eval.groundtruth_boxes["img2"],
+    self.assertTrue(np.allclose(self.od_eval.groundtruth_boxes['img2'],
-        self.od_eval.groundtruth_is_difficult_list["img2"],
+        self.od_eval.groundtruth_is_difficult_list['img2'],
-        "img1"], groundtruth_class_labels1))
+        'img1'], groundtruth_class_labels1))
-if __name__ == "__main__":
+if __name__ == '__main__':
-                 fields.InputDataFields.groundtruth_classes]:
+                 fields.InputDataFields.groundtruth_classes,
-    """Compute Object Detection related metrics from a single image.
+  def compute_object_detection_metrics(
-        groundtruth_is_difficult_lists)
+        groundtruth_is_difficult_lists, groundtruth_is_group_of_list)
-          detected_class_labels == i, :]
+      gt_boxes_at_ith_class = groundtruth_boxes[groundtruth_class_labels ==
-                     groundtruth_class_labels, groundtruth_is_difficult_lists):
+                     groundtruth_class_labels, groundtruth_is_difficult_lists,
-          gt_boxes_at_ith_class, groundtruth_is_difficult_list_at_ith_class)
+          gt_boxes_at_ith_class, groundtruth_is_difficult_list_at_ith_class,
-                                      groundtruth_is_difficult_list):
+  def _compute_tp_fp_for_single_class(
-          whether a ground truth box is a difficult instance or not
+          whether a ground truth box is a difficult instance or not. If a
-      scores: A numpy array representing the detection scores
+      Two arrays of the same size, containing all boxes that were evaluated as
-      true positive.
+          true positive.
-        ~is_matched_to_difficult_box]
+    is_matched_to_group_of_box = np.zeros(
-        groundtruth_groundtruth_is_difficult_list)
+        groundtruth_groundtruth_is_difficult_list,
-        groundtruth_groundtruth_is_difficult_list)
+        groundtruth_groundtruth_is_difficult_list,
-        groundtruth_groundtruth_is_difficult_list)
+        groundtruth_groundtruth_is_difficult_list,
-        groundtruth_groundtruth_is_difficult_list)
+        groundtruth_groundtruth_is_difficult_list,
-        groundtruth_groundtruth_is_difficult_list)
+        groundtruth_groundtruth_is_difficult_list,
-        groundtruth_groundtruth_is_difficult_list)
+        groundtruth_groundtruth_is_difficult_list,
-        groundtruth_groundtruth_is_difficult_list)
+        groundtruth_groundtruth_is_difficult_list,
-  text_bottom = top
+  # If the total height of the display strings added to the top of the bounding
-      values between 0 and 1
+    mask: a uint8 numpy array of shape (img_height, img_height) with
-  if np.any(np.logical_or(mask > 1.0, mask < 0.0)):
+  if mask.dtype != np.uint8:
-  and does not return anything.
+  on the image. Note that this function modifies the image in place, and returns
-    classes: a numpy array of shape [N]
+    classes: a numpy array of shape [N]. Note that class indices are 1-based,
-                       [1.0, 1.0]], dtype=np.float32)
+    mask = np.asarray([[0, 1],
-  tkx0 = (kx0 - xa) / ha
+  tkx0 = (kx0 - xa) / wa
-  tkx1 = (kx1 - xa) / ha
+  tkx1 = (kx1 - xa) / wa
-    """The model for Alice, Bob, and Eve.  If key=None, the first FC layer
+    """The model for Alice, Bob, and Eve.  If key=None, the first fully connected layer
-    Bob and eve's loss, as a percent of bits incorrect.
+    Bob and Eve's loss, as a percent of bits incorrect.
-  print('%d %.2f %.2f' % (itercount, bob_loss_percent, eve_loss_percent))
+  print('%10d\t%20.2f\t%20.2f'%(itercount, bob_loss_percent, eve_loss_percent))
-    print('# Iter Bob_Recon_Error Eve_Recon_Error')
+    print('# %10s\t%20s\t%20s'%("Iter","Bob_Recon_Error","Eve_Recon_Error"))
-  tf.app.run()
+  FLAGS, unparsed = parser.parse_known_args()
-  tf.app.run()
+  FLAGS, unparsed = parser.parse_known_args()
-                    help='Number of steps to train.')
+parser.add_argument('--train_epochs', type=int, default=40,
-  data_format = 'channels_last'
+  data_format = FLAGS.data_format
-  if tf.test.is_built_with_cuda():
+  if data_format is None:
-    data_format = 'channels_first'
+    data_format = ('channels_first' if tf.test.is_built_with_cuda() else
-      steps=FLAGS.steps,
+      steps=FLAGS.train_epochs * batches_per_epoch,
-                    help='The number of batches to run in between evaluations.')
+                    help='The number of epochs to run in between evaluations.')
-      FLAGS.resnet_size, _NUM_CLASSES)
+      FLAGS.resnet_size, _NUM_CLASSES, FLAGS.data_format)
-      resnet_size=FLAGS.resnet_size, num_classes=_LABEL_CLASSES)
+  network = resnet_model.imagenet_resnet_v2(
-      model = resnet_model.resnet_v2(
+      model = resnet_model.imagenet_resnet_v2(
-def resnet_v2(resnet_size, num_classes, data_format=None):
+def imagenet_resnet_v2(resnet_size, num_classes, data_format=None):
-
+_MOMENTUM = 0.9
-_BATCHES_PER_EPOCH = _NUM_IMAGES['train'] / FLAGS.batch_size
+_NUM_IMAGES = {
-    values = [_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 0.001]]
+    boundaries = [int(batches_per_epoch * epoch) for epoch in [100, 150, 200]]
-  tf.app.run()
+  FLAGS, unparsed = parser.parse_known_args()
-  cifar10_main.FLAGS = FLAGS
+  cifar10_main.FLAGS = cifar10_main.parser.parse_args()
-
+_DEFAULT_IMAGE_SIZE = 224
-  image = image_preprocessing_fn(
+  image = vgg_preprocessing.preprocess_image(
-      output_width=network.default_image_size,
+      output_height=_DEFAULT_IMAGE_SIZE,
-        _INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]
+        initial_learning_rate * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]
-  tf.app.run()
+  FLAGS, unparsed = parser.parse_known_args()
-                                 'wide_deep_test.csv')
+TEST_CSV = os.path.join(os.path.dirname(__file__), 'wide_deep_test.csv')
-            TEST_TRAINING_CSV, num_epochs=1, shuffle=True, batch_size=1),
+            TEST_CSV, num_epochs=1, shuffle=True, batch_size=1),
-            TEST_TRAINING_CSV, num_epochs=1, shuffle=False, batch_size=1))
+            TEST_CSV, num_epochs=1, shuffle=False, batch_size=1))
-            TEST_TRAINING_CSV, num_epochs=None, shuffle=True, batch_size=2),
+            TEST_CSV, num_epochs=None, shuffle=True, batch_size=2),
-            TEST_TRAINING_CSV, num_epochs=1, shuffle=False, batch_size=1))
+            TEST_CSV, num_epochs=1, shuffle=False, batch_size=1))
-
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
-    ids = tf.to_int32(tf.argmax(log_prob, dimension=2), name='predicted_chars')
+    ids = tf.to_int32(tf.argmax(log_prob, axis=2), name='predicted_chars')
-      tfprof_root = model_analyzer.print_model_analysis(
+      tfprof_root = tf.profiler.profile(
-          tfprof_options=model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)
+          options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())
-    prediction = tf.argmax(logit, dimension=1)
+    prediction = tf.argmax(logit, axis=1)
-                    help='The number of batches to train.')
+parser.add_argument('--train_epochs', type=int, default=250,
-parser.add_argument('--steps_per_eval', type=int, default=4000,
+parser.add_argument('--epochs_per_eval', type=int, default=10,
-_BATCHES_PER_EPOCH = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size
+_BATCHES_PER_EPOCH = _NUM_IMAGES['train'] / FLAGS.batch_size
-  record_bytes = HEIGHT * WIDTH * DEPTH + 1
+  record_bytes = _HEIGHT * _WIDTH * _DEPTH + 1
-  """Returns a list of filenames based on 'mode'."""
+def get_filenames(is_training):
-  if mode == tf.estimator.ModeKeys.TRAIN:
+  if is_training:
-        for i in range(1, NUM_DATA_BATCHES + 1)
+        for i in range(1, _NUM_DATA_FILES + 1)
-    raise ValueError('Invalid mode: %s' % mode)
+    return [os.path.join(data_dir, 'test_batch.bin')]
-  image_bytes = HEIGHT * WIDTH * DEPTH
+  image_bytes = _HEIGHT * _WIDTH * _DEPTH
-                           [DEPTH, HEIGHT, WIDTH])
+                           [_DEPTH, _HEIGHT, _WIDTH])
-  return image, tf.one_hot(label, NUM_CLASSES)
+  return image, tf.one_hot(label, _NUM_CLASSES)
-  image = tf.image.resize_image_with_crop_or_pad(image, HEIGHT + 8, WIDTH + 8)
+  image = tf.image.resize_image_with_crop_or_pad(image, _HEIGHT + 8, _WIDTH + 8)
-  image = tf.random_crop(image, [HEIGHT, WIDTH, DEPTH])
+  # Randomly crop a [_HEIGHT, _WIDTH] section of the image.
-def input_fn(mode, batch_size):
+def input_fn(is_training, num_epochs=1):
-    batch_size: The number of samples per batch of input requested.
+    is_training: A boolean denoting whether the input is for training.
-
+  dataset = record_dataset(get_filenames(is_training))
-                        output_buffer_size=2 * batch_size)
+                        output_buffer_size=2 * FLAGS.batch_size)
-  if mode == tf.estimator.ModeKeys.TRAIN:
+  if is_training:
-                          output_buffer_size=2 * batch_size)
+                          output_buffer_size=2 * FLAGS.batch_size)
-    buffer_size = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN * 0.4) + 3 * batch_size
+    buffer_size = int(0.4 * _NUM_IMAGES['train'])
-      output_buffer_size=2 * batch_size)
+      output_buffer_size=2 * FLAGS.batch_size)
-  iterator = dataset.batch(batch_size).make_one_shot_iterator()
+  iterator = dataset.batch(FLAGS.batch_size).make_one_shot_iterator()
-      FLAGS.resnet_size, NUM_CLASSES)
+      FLAGS.resnet_size, _NUM_CLASSES)
-  inputs = tf.reshape(features, [-1, HEIGHT, WIDTH, DEPTH])
+  inputs = tf.reshape(features, [-1, _HEIGHT, _WIDTH, _DEPTH])
-      model_fn=cifar10_model_fn, model_dir=FLAGS.model_dir)
+      model_fn=cifar10_model_fn, model_dir=FLAGS.model_dir, config=run_config)
-  for _ in range(FLAGS.train_steps // FLAGS.steps_per_eval):
+  for _ in range(FLAGS.train_epochs // FLAGS.epochs_per_eval):
-        steps=FLAGS.steps_per_eval,
+        input_fn=lambda: input_fn(
-                                  batch_size=FLAGS.batch_size))
+        input_fn=lambda: input_fn(is_training=False))
-    help='The number of steps to use for training.')
+    '--train_epochs', type=int, default=100,
-    help='The number of training steps to run between evaluations.')
+    '--epochs_per_eval', type=int, default=1,
-  """Input function which provides a single batch for train or eval."""
+def input_fn(is_training, num_epochs=1):
-    dataset = dataset.repeat()
+    dataset = dataset.repeat(num_epochs)
-                        num_threads=FLAGS.map_threads,
+                        num_threads=5,
-    # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.
+    # Multiply the learning rate by 0.1 at 30, 60, 80, and 90 epochs.
-        int(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]
+        int(batches_per_epoch * epoch) for epoch in [30, 60, 80, 90]]
-      model_fn=resnet_model_fn, model_dir=FLAGS.model_dir)
+      model_fn=resnet_model_fn, model_dir=FLAGS.model_dir, config=run_config)
-  for _ in range(FLAGS.train_steps // FLAGS.steps_per_eval):
+  for _ in range(FLAGS.train_epochs // FLAGS.epochs_per_eval):
-        steps=FLAGS.first_cycle_steps or FLAGS.steps_per_eval,
+        input_fn=lambda: input_fn(
-    eval_results = resnet_classifier.evaluate(input_fn=lambda: input_fn(False))
+    eval_results = resnet_classifier.evaluate(
-flags.DEFINE_string('master', '', 'DNS name of the TensorFlow master to use.')
+flags.DEFINE_string('master', '', 'Name of the TensorFlow master to use.')
-DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'
+DATA_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'
-  all_labels = sine_labels + const_labels + noise_labels
+  all_examples = np.concatenate((sine_examples, const_examples, noise_examples))
-flags.DEFINE_string('master', '', 'BNS name of the TensorFlow master to use.')
+flags.DEFINE_string('master', '', 'DNS name of the TensorFlow master to use.')
-    for name, op in ops.iteritems():
+    for name, op in ops.items():
-    for name, model in models.iteritems():
+    for name, model in models.items():
-      for _ in xrange(1024):
+    for i in range(3):
-        for i in xrange(0, 1024)]
+        for i in range(0, 1024)]
-        for i in xrange(0, 128)]
+        for i in range(0, 128)]
-  tf.app.run()
+  FLAGS, unparsed = parser.parse_known_args()
-  def parser(serialized_example):
+  def example_parser(serialized_example):
-      'convert the MNIST data to TFRecord file format.')
+  assert tf.gfile.Exists(tfrecords_file), (
-  dataset = dataset.map(parser, num_threads=1, output_buffer_size=batch_size)
+  # Map example_parser over dataset, and batch results by up to batch_size
-  tf.app.run()
+  FLAGS, unparsed = parser.parse_known_args()
-          100.0 * count * block_size / total_size))
+      sys.stdout.write('\r>> Downloading %s %.1f%%' % (
-def filenames(mode):
+def get_filenames(mode):
-      'to download and extract the CIFAR-10 data.')
+  assert os.path.exists(data_dir), (
-    mode: Standard names for model modes (tf.estimators.ModeKeys).
+    mode: Standard names for model modes from tf.estimator.ModeKeys.
-  dataset = record_dataset(filenames(mode))
+  dataset = record_dataset(get_filenames(mode))
-  accuracy= tf.metrics.accuracy(
+  accuracy = tf.metrics.accuracy(
-  for cycle in range(FLAGS.train_steps // FLAGS.steps_per_eval):
+  for _ in range(FLAGS.train_steps // FLAGS.steps_per_eval):
-      for j in xrange(1024):
+      for _ in xrange(1024):
-    file.close()
+    data_file = open(filename, 'wb')
-  """ Our model_fn for ResNet to be used with our Estimator."""
+  """Our model_fn for ResNet to be used with our Estimator."""
-  for cycle in range(FLAGS.train_steps // FLAGS.steps_per_eval):
+  for _ in range(FLAGS.train_steps // FLAGS.steps_per_eval):
-      """
+      """Returns the expected dimensions depending on if a GPU is being used."""
-  # See https://www.tensorflow.org/performance/performance_guide#common_fused_ops
+  # We set fused=True for a significant performance boost. See
-  """
+  """Strided 2-D convolution with explicit padding."""
-  for i in range(1, blocks):
+  for _ in range(1, blocks):
-    data_format = 'channels_first' if tf.test.is_built_with_cuda() else 'channels_last'
+    data_format = (
-      # See https://www.tensorflow.org/performance/performance_guide#data_formats
+      # provides a large performance boost on GPU. See
-    data_format = 'channels_first' if tf.test.is_built_with_cuda() else 'channels_last'
+    data_format = (
-  return labels_to_class_names
+
-    '--train_batch_size', type=int, default=32, help='Batch size for training.')
+    '--batch_size', type=int, default=32,
-    help='Batch size for evaluation.')
+    '--map_threads', type=int, default=5,
-_INITIAL_LEARNING_RATE = 0.1 * FLAGS.train_batch_size / 256
+_INITIAL_LEARNING_RATE = 0.1 * FLAGS.batch_size / 256
-eval_dataset = imagenet.get_split('validation', FLAGS.data_dir)
+_NUM_IMAGES = {
-    resnet_size=FLAGS.resnet_size, num_classes=train_dataset.num_classes)
+    resnet_size=FLAGS.resnet_size, num_classes=_LABEL_CLASSES)
-batches_per_epoch = train_dataset.num_samples / FLAGS.train_batch_size
+  image = tf.image.decode_image(
-  labels = tf.one_hot(labels, imagenet._NUM_CLASSES)
+  dataset = tf.contrib.data.Dataset.from_tensor_slices(filenames(is_training))
-      input_fn=lambda: input_fn(False), steps=_EVAL_STEPS)
+    eval_results = resnet_classifier.evaluate(input_fn=lambda: input_fn(False))
-    features = tf.random_uniform([FLAGS.train_batch_size, 224, 224, 3])
+    features = tf.random_uniform([FLAGS.batch_size, 224, 224, 3])
-            [FLAGS.train_batch_size], maxval=_LABEL_CLASSES - 1,
+            [FLAGS.batch_size], maxval=_LABEL_CLASSES - 1,
-                        (FLAGS.train_batch_size, _LABEL_CLASSES))
+                        (FLAGS.batch_size, _LABEL_CLASSES))
-    self.assertAllEqual(predictions['classes'].shape, (FLAGS.train_batch_size,))
+    self.assertAllEqual(predictions['classes'].shape, (FLAGS.batch_size,))
-    inputs = tf.reshape(inputs, [inputs.get_shape()[0].value, -1])
+    inputs = tf.reshape(inputs,
-_B_MEAN = 103.94
+_R_MEAN = 123.68 / 255
-  return '.png' in filename
+  return filename.endswith('.png')
-        print('SKIPPED: Unexpected eror while decoding %s.' % filename)
+        print('SKIPPED: Unexpected error while decoding %s.' % filename)
-  'convert the MNIST data to TFRecord file format.')
+  assert tf.gfile.Exists(tfrecords_file), ('Run convert_to_records.py first to '
-  https://github.com/tensorflow/models/blob/master/research/inception/inception/data/build_imagenet_data.py
+  https://github.com/tensorflow/models/blob/master/research/slim/datasets/build_imagenet_data.py
-  base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/inception/inception/data/'
+  base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/slim/datasets/'
-  https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L463
+  https://github.com/tensorflow/models/blob/master/research/inception/inception/data/build_imagenet_data.py
-  base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/inception/inception/data/'
+  base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/inception/inception/data/'
-https://github.com/tensorflow/models/blob/master/slim/nets/vgg.py
+https://github.com/tensorflow/models/blob/master/research/slim/nets/vgg.py
-github.com/tensorflow/models/tree/master/transformer
+github.com/tensorflow/models/tree/master/research/transformer
-    url='https://github.com/tensorflow/models/tree/master/syntaxnet',
+    url='https://github.com/tensorflow/models/tree/master/research/syntaxnet',
-import argparse
+
-tensorflow/models/slim/datasets/download_and_convert_cifar10.py
+tensorflow/models/research/slim/datasets/download_and_convert_cifar10.py
-tensorflow/models/slim/datasets/download_and_convert_flowers.py
+tensorflow/models/research/slim/datasets/download_and_convert_flowers.py
-  https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py#L463
+  https://github.com/tensorflow/models/blob/master/research/inception/inception/data/build_imagenet_data.py#L463
-  base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/inception/inception/data/'
+  base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/inception/inception/data/'
-tensorflow/models/slim/datasets/download_and_convert_mnist.py
+tensorflow/models/research/slim/datasets/download_and_convert_mnist.py
-bazel-bin/tensorflow_models/slim/export_inference_graph \
+bazel build tensorflow_models/research/slim:export_inference_graph
-parser.add_argument('--use_fp16', type=bool, default=False, help='Train the model using fp16.')
+parser.add_argument('--batch_size', type=int, default=128,
-parser.add_argument('--eval_dir', type=str, default='/tmp/cifar10_eval', help='Directory where to write event logs.')
+parser.add_argument('--eval_dir', type=str, default='/tmp/cifar10_eval',
-parser.add_argument('--eval_data', type=str, default='test', help='Either `test` or `train_eval`.')
+parser.add_argument('--eval_data', type=str, default='test',
-parser.add_argument('--checkpoint_dir', type=str, default='/tmp/cifar10_train', help='Directory where to read model checkpoints.')
+parser.add_argument('--checkpoint_dir', type=str, default='/tmp/cifar10_train',
-parser.add_argument('--eval_interval_secs', type=int, default=60*5, help='How often to run the eval.')
+parser.add_argument('--eval_interval_secs', type=int, default=60*5,
-parser.add_argument('--num_examples', type=int, default=10000, help='Number of examples to run.')
+parser.add_argument('--num_examples', type=int, default=10000,
-parser.add_argument('--run_once', type=bool, default=False, help='Whether to run eval only once.')
+parser.add_argument('--run_once', type=bool, default=False,
-parser.add_argument('--train_dir', type=str, default='/tmp/cifar10_train', help='Directory where to write event logs and checkpoint.')
+parser.add_argument('--train_dir', type=str, default='/tmp/cifar10_train',
-parser.add_argument('--max_steps', type=int, default=1000000, help='Number of batches to run.')
+parser.add_argument('--max_steps', type=int, default=1000000,
-parser.add_argument('--num_gpus', type=int, default=1, help='How many GPUs to use.')
+parser.add_argument('--num_gpus', type=int, default=1,
-parser.add_argument('--log_device_placement', type=bool, default=False, help='Whether to log device placement.')
+parser.add_argument('--log_device_placement', type=bool, default=False,
-parser.add_argument('--train_dir', type=str, default='/tmp/cifar10_train', help='Directory where to write event logs and checkpoint.')
+parser.add_argument('--train_dir', type=str, default='/tmp/cifar10_train',
-parser.add_argument('--max_steps', type=int, default=1000000, help='Number of batches to run.')
+parser.add_argument('--max_steps', type=int, default=1000000,
-parser.add_argument('--log_device_placement', type=bool, default=False, help='Whether to log device placement.')
+parser.add_argument('--log_device_placement', type=bool, default=False,
-parser.add_argument('--log_frequency', type=int, default=10, help='How often to log results to the console.')
+parser.add_argument('--log_frequency', type=int, default=10,
-parser = argparse.ArgumentParser()
+parser = cifar10.parser
-parser = argparse.ArgumentParser()
+parser = cifar10.parser
-  #    print(arg, getattr(FLAGS, arg))
+from cifar10 import FLAGS
-parser = argparse.ArgumentParser()
+parser = cifar10.parser
-FLAGS = tf.app.flags.FLAGS
+parser = argparse.ArgumentParser()
-                            """Train the model using fp16.""")
+parser.add_argument('--batch_size', type=int, default=128, help='Number of images to process in a batch.')
-FLAGS = tf.app.flags.FLAGS
+parser = argparse.ArgumentParser()
-                            """Whether to log device placement.""")
+parser.add_argument('--train_dir', type=str, default='/tmp/cifar10_train', help='Directory where to write event logs and checkpoint.')
-FLAGS = tf.app.flags.FLAGS
+parser = argparse.ArgumentParser()
-                            """How often to log results to the console.""")
+parser.add_argument('--train_dir', type=str, default='/tmp/cifar10_train', help='Directory where to write event logs and checkpoint.')
-FLAGS = tf.app.flags.FLAGS
+parser = argparse.ArgumentParser()
-                         """Whether to run eval only once.""")
+parser.add_argument('--eval_dir', type=str, default='/tmp/cifar10_eval', help='Directory where to write event logs.')
-"""Converts MNIST data to TFRecords file format with Example protos."""
+"""Converts MNIST data to TFRecords file format with Example protos.
-"""Converts MNIST data to TFRecords file format with Example protos."""
+"""Downloads and extracts the binary version of the CIFAR-10 dataset."""
-        inputs=inputs, units=num_classes)
+    inputs = tf.layers.dense(inputs=inputs, units=num_classes)
-configuration file to define what type of DetectionModel is being evaulated, an
+configuration file to define what type of DetectionModel is being evaluated, an
-      'Conv2d_3_pointwise', 'Conv2d_4_pointwise', 'Conv2d_5'_pointwise,
+      'Conv2d_3_pointwise', 'Conv2d_4_pointwise', 'Conv2d_5_pointwise',
-    for variable in tf.all_variables():
+    for variable in tf.global_variables():
-    
+ 
-        model_fn=get_model_fn(num_gpus, variable_strategy, num_workers),
+        model_fn=get_model_fn(num_gpus, variable_strategy,
-
+    num_workers = run_config.num_worker_replicas
-        [ 1 2 5 4 5 ],
+        [ 1 2 3 4 5 ],
-        8) class_predictions_with_background: a 3-D tensor with shape
+        8) class_predictions_with_background: a 2-D tensor with shape
-      rpn_box_encodings: 4-D float tensor of shape
+      rpn_box_encodings: 3-D float tensor of shape
-      rpn_objectness_predictions_with_background: 2-D float tensor of shape
+      rpn_objectness_predictions_with_background: 3-D float tensor of shape
-        2) class_predictions_with_background: a 3-D tensor with shape
+        2) class_predictions_with_background: a 2-D tensor with shape
-      box_encodings: 4-D float tensor of shape
+      box_encodings: 3-D float tensor of shape
-      objectness_predictions_with_background: 2-D float tensor of shape
+      objectness_predictions_with_background: 3-D float tensor of shape
-      refined_box_encodings: a 3-D tensor with shape
+      refined_box_encodings: a 4-D tensor with shape
-      rpn_box_encodings: A 4-D float tensor of shape
+      rpn_box_encodings: A 3-D float tensor of shape
-      rpn_objectness_predictions_with_background: A 2-D float tensor of shape
+      rpn_objectness_predictions_with_background: A 3-D float tensor of shape
-      class_predictions_with_background: a 3-D tensor with shape
+      class_predictions_with_background: a 2-D tensor with shape
-      rpn_box_encodings: 4-D float tensor of shape
+      rpn_box_encodings: 3-D float tensor of shape
-      rpn_objectness_predictions_with_background: 2-D float tensor of shape
+      rpn_objectness_predictions_with_background: 3-D float tensor of shape
-        1) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,
+        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
-      box_encodings: 4-D float tensor of shape [batch_size, num_anchors,
+      box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
-      class_predictions_with_background: 2-D float tensor of shape
+      class_predictions_with_background: 3-D float tensor of shape
-        1) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,
+        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
-        2) class_predictions_with_background: 2-D float tensor of shape
+        2) class_predictions_with_background: 3-D float tensor of shape
-        1) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,
+        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
-        2) class_predictions_with_background: 2-D float tensor of shape
+        2) class_predictions_with_background: 3-D float tensor of shape
-        1) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,
+        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
-        2) class_predictions_with_background: 2-D float tensor of shape
+        2) class_predictions_with_background: 3-D float tensor of shape
-
+
-      learning_rate = tf.identity(learning_rate, name='learning_rate')
+      loss = tf.reduce_mean(tower_losses, name='loss')
-      chief_hooks = []
+
-        chief_hooks.append(sync_replicas_hook)
+        sync_replicas_hook = optimizer.make_session_run_hook(params.is_chief)
-        training_chief_hooks=chief_hooks,
+        training_hooks=train_hooks,
-    hooks = [logging_hook, examples_sec_hook]
+    if run_config.num_worker_replicas:
-                              run_config.num_worker_replicas or 1),
+        model_fn=get_model_fn(num_gpus, variable_strategy, num_workers),
-    experiment = tf.contrib.learn.Experiment(
+    return tf.contrib.learn.Experiment(
-      hparams=tf.contrib.training.HParams(**hparams))
+      hparams=tf.contrib.training.HParams(
-    example: The converted tf.Example.
+    A string of the class name.
-  from google3.third_party.tensorflow.python.grappler import tf_optimizer
+  from tensorflow.python.grappler import tf_optimizer
-  def testGetNetworkFn(self):
+  def testGetNetworkFnFirstHalf(self):
-      with self.test_session():
+    for net in nets_factory.networks_map.keys()[:10]:
-  def testGetNetworkFnArgScope(self):
+  def testGetNetworkFnSecondHalf(self):
-                          device='/CPU:0'):
+    num_classes = 1000
-      self.assertDeviceEqual('/CPU:0', weights.device)
+        logits, end_points = net_fn(inputs)
-        lambda x, method: tf.image.resize_images(x, [height, width], method=method),
+        lambda x, method: tf.image.resize_images(x, [height, width], method),
-      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)
+      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).
-    image: 3-D Tensor [height, width, channels] with the image.
+    image: 3-D Tensor [height, width, channels] with the image. If dtype is
-        momentum=FLAGS.momentum,
+        momentum=FLAGS.rmsprop_momentum,
-        images, labels = batch_queue.dequeue()
+      images, labels = batch_queue.dequeue()
-          label_smoothing=FLAGS.label_smoothing, weights=1.0)
+        slim.losses.softmax_cross_entropy(
-          total_num_replicas=FLAGS.worker_replicas)
+          variables_to_average=moving_average_variables)
-import inspect
+import util
-
+flags.DEFINE_integer("num_gpus", 1,
-    num_steps = input_.num_steps
+    self._rnn_params = None
-        outputs.append(cell_output)
+    output, state = self._build_rnn_graph(inputs, config, is_training)
-    logits = tf.matmul(output, softmax_w) + softmax_b
+    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)
-    # use the contrib sequence loss and average over the batches
+    # Use the contrib sequence loss and average over the batches
-        tf.ones([batch_size, num_steps], dtype=data_type()),
+        tf.ones([self.batch_size, self.num_steps], dtype=data_type()),
-    )
+        average_across_batch=True)
-    self._cost = cost = tf.reduce_sum(loss)
+    # Update the cost
-    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
+    grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars),
-             iters * model.input.batch_size / (time.time() - start_time)))
+             iters * model.input.batch_size * max(1, FLAGS.num_gpus) /
-    return SmallConfig()
+    config = SmallConfig()
-    return MediumConfig()
+    config = MediumConfig()
-    return LargeConfig()
+    config = LargeConfig()
-    return TestConfig()
+    config = TestConfig()
-      test_input = PTBInput(config=eval_config, data=test_data, name="TestInput")
+      test_input = PTBInput(
-    with sv.managed_session() as session:
+    config_proto = tf.ConfigProto(allow_soft_placement=soft_placement)
-def get_model_fn(num_gpus, variable_strategy, data_format, num_workers):
+def get_model_fn(num_gpus, variable_strategy, num_workers):
-        model_fn=get_model_fn(num_gpus, variable_strategy, data_format,
+        model_fn=get_model_fn(num_gpus, variable_strategy,
-    )
+        params=hparams)
-def main(job_dir, data_dir, num_gpus, variable_strategy, data_format,
+def main(job_dir, data_dir, num_gpus, variable_strategy,
-      get_experiment_fn(data_dir, num_gpus, variable_strategy, data_format,
+      get_experiment_fn(data_dir, num_gpus, variable_strategy,
-    if args.num_gpus < 0:
+  if args.num_gpus < 0:
-        '--variable-strategy=CPU.')
+    raise ValueError('num-gpus=0, CPU must be used as parameter server. Set'
-  predictions = 3 - tf.to_int32(predictions) * 2
+  labels = (2 - tf.to_int32(labels)) - 1
-def get_model_fn(num_gpus, variable_strategy, num_workers):
+def get_model_fn(num_gpus, variable_strategy, num_workers, sync):
-      params: Dictionary of Hyperparameters suitable for tuning
+      params: Hyperparameters suitable for tuning
-    momentum = params['momentum']
+    weight_decay = params.weight_decay
-                params['batch_norm_epsilon'])
+                params.num_layers,
-          'train') // (params['train_batch_size'] * num_workers)
+          'train') // (params.train_batch_size * num_workers)
-      staged_lr = [params['learning_rate'] * x for x in [1, 0.1, 0.01, 0.002]]
+      staged_lr = [params.learning_rate * x for x in [1, 0.1, 0.01, 0.002]]
-      if params['sync']:
+      if sync:
-                      use_distortion_for_training=True):
+                      use_distortion_for_training=True,
-            num_gpus, is_gpu_ps, run_config.num_worker_replicas or 1),
+            num_gpus, is_gpu_ps, run_config.num_worker_replicas or 1, sync),
-        params=vars(hparams)
+        params=hparams
-          use_distortion_for_training
+          use_distortion_for_training,
-  ) 
+  )
-      default=1,
+      default=0,
-      parallelism are scheduled to run on GPUs.\
+      Number of threads to use for intra-op parallelism. When training on CPU
-        'Invalid GPU count: \"num_gpus\" must be 0 or a positive integer.')
+        'Invalid GPU count: \"--num-gpus\" must be 0 or a positive integer.')
-        'No GPU available for use, must use CPU to average gradients.')
+        'num-gpus=0, CPU must be used as parameter server. Set'
-    raise ValueError('Invalid num_layers parameter.')
+    raise ValueError('Invalid --num-layers parameter.')
-    raise ValueError('train_batch_size must be multiple of num_gpus.')
+    raise ValueError('--train-batch-size must be multiple of --num-gpus.')
-    raise ValueError('eval_batch_size must be multiple of num_gpus.')
+    raise ValueError('--eval-batch-size must be multiple of --num-gpus.')
-mnist = input_data.read_data_sets('MNIST_data', one_hot = True)
+mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
-    scale = 0.01)
+    n_input=784,
-mnist = input_data.read_data_sets('MNIST_data', one_hot = True)
+mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
-    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
+    n_input=784,
-mnist = input_data.read_data_sets('MNIST_data', one_hot = True)
+mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
-    dropout_probability = 0.95)
+    n_input=784,
-mnist = input_data.read_data_sets('MNIST_data', one_hot = True)
+mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
-    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
+    n_input=784,
-                          output_buffer_size=2 * batch_size)
+    dataset = dataset.map(
-
+import numpy as np
-def get_model_fn(num_gpus, variable_strategy, num_workers):
+def get_model_fn(num_gpus, variable_strategy, data_format, num_workers):
-              worker_device=worker_device)
+        device_setter = cifar10_utils.local_device_setter(
-          )
+        device_setter = cifar10_utils.local_device_setter(
-                params['batch_norm_decay'],
+                is_training, weight_decay, tower_features[i], tower_labels[i],
-            replicas_to_aggregate=num_workers)
+            optimizer, replicas_to_aggregate=num_workers)
-          'accuracy': tf.metrics.accuracy(stacked_labels, predictions['classes'])
+          'accuracy':
-  """Build computation tower for each device (CPU or GPU).
+def _tower_fn(is_training, weight_decay, feature, label, data_format,
-    is_cpu: true if build tower on CPU.
+    data_format: channels_last (NHWC) or channels_first (NCHW).
-      is_training=is_training, data_format=data_format)
+      is_training=is_training,
-def input_fn(data_dir, subset, num_shards, batch_size,
+def input_fn(data_dir,
-def get_experiment_fn(data_dir, num_gpus, is_gpu_ps,
+def get_experiment_fn(data_dir,
-      is_gpu_ps: bool. If true, average gradients on GPUs.
+      variable_strategy: String. CPU to use CPU as the parameter server
-    )
+        use_distortion_for_training=use_distortion_for_training)
-    )
+        num_shards=num_gpus)
-      raise ValueError('validation set size must be multiple of eval_batch_size')
+      raise ValueError(
-      hparams.train_batch_size, every_n_steps=10)
+        hparams.train_batch_size, every_n_steps=10)
-                      'loss': 'loss'}
+    tensors_to_log = {'learning_rate': 'learning_rate', 'loss': 'loss'}
-      tensors=tensors_to_log, every_n_iter=100)
+        tensors=tensors_to_log, every_n_iter=100)
-            num_gpus, is_gpu_ps, run_config.num_worker_replicas or 1),
+        model_fn=get_model_fn(num_gpus, variable_strategy, data_format,
-    )
+        params=vars(hparams))
-         num_intra_threads,
+def main(job_dir, data_dir, num_gpus, variable_strategy, data_format,
-  )
+      gpu_options=tf.GPUOptions(force_gpu_compatible=True))
-      model_dir=job_dir)
+      session_config=sess_config, model_dir=job_dir)
-      ),
+      get_experiment_fn(data_dir, num_gpus, variable_strategy, data_format,
-  )
+      hparams=tf.contrib.training.HParams(**hparams))
-  )
+      help='The directory where the CIFAR-10 input data is stored.')
-  )
+      help='The directory where the model will be stored.')
-  )
+      help='Where to locate variable operations')
-  )
+      help='The number of gpus used. Uses only CPU if set to 0.')
-  )
+      help='The number of layers of the model.')
-  )
+      help='The number of steps to use for training.')
-  )
+      help='Batch size for training.')
-  )
+      help='Batch size for validation.')
-  )
+      help='Momentum for MomentumOptimizer.')
-  ) 
+      help='Weight decay for convolutions.')
-  )
+      """)
-  )
+      help='If doing image distortion for training.')
-  )
+      """)
-  )
+      """)
-  )
+      """)
-  )
+      help='Whether to log device placement.')
-  )
+      help='Decay for batch norm.')
-  )
+      help='Epsilon for batch norm.')
-"""Read CIFAR-10 data from pickled numpy arrays and write TFExamples.
+"""Read CIFAR-10 data from pickled numpy arrays and writes TFRecords.
-downloaded from https://www.cs.toronto.edu/~kriz/cifar.html.
+Generates tf.train.Example protos and writes them to TFRecord files from the
-CIFAR_10_FILE_NAME = 'cifar-10-python.tar.gz'
+CIFAR_FILENAME = 'cifar-10-python.tar.gz'
-  tarfile.open(os.path.join(data_dir,CIFAR_10_FILE_NAME), 'r:gz').extractall(data_dir)
+  tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir,
-  """Converts a file to tfrecords."""
+  """Converts a file to TFRecords."""
-            features=tf.train.Features(feature={
+        example = tf.train.Example(features=tf.train.Features(
-        os.path.join(input_dir, f) for f in files]
+    input_files = [os.path.join(input_dir, f) for f in files]
-    # Convert to Examples and write the result to TFRecords.
+    # Convert to tf.train.Example and write the to TFRecords.
-  )
+      help='Directory to download and extract CIFAR-10 to.')
-  main(args.data_dir)
+  main(args.data_dir)
-
+import tarfile
-def main(input_dir, output_dir):
+def main(data_dir):
-    output_file = os.path.join(output_dir, mode + '.tfrecords')
+    output_file = os.path.join(data_dir, mode + '.tfrecords')
-      '--output-dir',
+      '--data-dir',
-      """
+      help='Directory to download and extract CIFAR-10 to.'
-  main(args.input_dir, args.output_dir)
+  main(args.data_dir)
-  print >> sys.stderr, e
+except GetoptError as e:
-          ', '.join(tok for tok, v in zip(parts, vs) if v is None))
+      print('not in vocabulary: %s' % (
-    print 'use a single word to query neighbors, or three words for analogy'
+    print('use a single word to query neighbors, or three words for analogy')
-    print '%0.4f: %s' % (sim, word)
+    print('%0.4f: %s' % (sim, word))
-  print
+  print()
-import struct
+
-      # vectors.
+      # If column vectors were specified, then open them and add them to the
-    if isinstance(query, basestring):
+    if isinstance(query, string_types):
-  print >> sys.stderr, e
+except GetoptError as e:
-  print >> sys.stderr, 'please specify a vocabulary file with "--vocab"'
+  print('please specify a vocabulary file with "--vocab"', file=sys.stderr)
-  print >> sys.stderr, 'please specify the embeddings with "--embeddings"'
+  print('please specify the embeddings with "--embeddings"', file=sys.stderr)
-  print >> sys.stderr, e
+except IOError as e:
-    print '%0.3f %s' % (evaluate(lines), filename)
+    print('%0.3f %s' % (evaluate(lines), filename))
-    return f.read().decode("utf-8").replace("\n", "<eos>").split()
+    if Py3:
-    # Adding hooks to be used by the estimator on training mode.
+    # Adding hooks to be used by the estimator on training modes
-  config = tf.contrib.learn.RunConfig(
+  config = cifar10_utils.RunConfig(
-              "Cost: ", "{:.9f}".format(avg_cost))
+        print("Epoch:", '%d,' % (epoch + 1),
-              "Cost: ", "{:.9f}".format(avg_cost))
+        print("Epoch:", '%d,' % (epoch + 1),
-              "Cost: ", "{:.9f}".format(avg_cost))
+        print("Epoch:", '%d,' % (epoch + 1),
-        
+        print("Epoch:", '%d,' % (epoch + 1),
-import numpy as np
+from __future__ import absolute_import
-                                               scale = 0.01)
+autoencoder = AdditiveGaussianNoiseAutoencoder(
-        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
+        print("Epoch: ", '%d,' % (epoch + 1),
-import numpy as np
+from __future__ import absolute_import
-                          optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
+autoencoder = Autoencoder(
-
+        print("Epoch: ", '%d,' % (epoch + 1),
-import numpy as np
+from __future__ import absolute_import
-X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)
+X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)
-                                      dropout_probability = 0.95)
+autoencoder = MaskingNoiseAutoencoder(
-        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
+        print("Epoch: ", '%d,' % (epoch + 1),
-
+        print("Epoch: ", '%d,' % (epoch + 1),
-import numpy as np
+from __future__ import absolute_import
-                                     optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
+autoencoder = VariationalAutoencoder(
-        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
+        print("Epoch: ", '%d,' % (epoch + 1), "Cost: ", "{:.9f}".format(avg_cost))
-                                               name_scope)
+    if num_gpus == 0:
-          update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scope)
+      num_devices = num_gpus
-      '--variable_strategy',
+      '--variable-strategy',
-      '--batch_norm_decay',
+      '--batch-norm-decay',
-      '--batch_norm_epsilon',
+      '--batch-norm-epsilon',
-      inputs, outputs, params)
+  """Aggregate the metrics for rotator model.
-from Queue import Queue
+try:
-    for line in file(self.config.KB_file):
+    for line in open(self.config.KB_file):
-      for line in file(self.config.data_files[name]):
+      for line in open(self.config.data_files[name]):
-        FLAGS.data_sst_path,
+        FLAGS.inp_dir,
-        FLAGS.data_sst_path,
+        FLAGS.inp_dir,
-import operator
+import itertools
-from tensorflow.python.training import training_util
+import cifar10_utils
-  """
+tf.logging.set_verbosity(tf.logging.INFO)
-def get_model_fn(num_gpus, avg_on_gpu, num_workers):
+def get_model_fn(num_gpus, variable_strategy, num_workers):
-    either one of the following scheme.
+    Support single host, one or more GPU training. Parameter distribution can
-        device_setter = _create_device_setter(avg_on_gpu, worker, num_gpus)
+        worker_device = '/gpu:{}'.format(i)
-
+    with tf.name_scope('gradient_averaging'):
-  return tower_loss, tower_grad, tower_pred
+  return tower_loss, zip(tower_grad, model_params), tower_pred
-    examples_sec_hook = ExamplesPerSecondHook(
+    examples_sec_hook = cifar10_utils.ExamplesPerSecondHook(
-                      'loss': 'gradient_averaging/loss'}
+                      'loss': 'loss'}
-            num_gpus, is_gpu_ps, run_config.num_worker_replicas),
+            num_gpus, is_gpu_ps, run_config.num_worker_replicas or 1),
-         avg_on_gpu,
+         variable_strategy,
-          force_gpu_compatible=force_gpu_compatible
+          force_gpu_compatible=True
-          avg_on_gpu,
+          variable_strategy,
-      help='If present, use GPU to average gradients.'
+      '--variable_strategy',
-  
+  ) 
-  if args.num_gpus == 0 and args.avg_on_gpu:
+  if args.num_gpus == 0 and args.variable_strategy == 'GPU':
-def main(unused_argv):
+def main(input_dir, output_dir):
-    output_file = os.path.join(FLAGS.output_dir, mode + '.tfrecords')
+        os.path.join(input_dir, f) for f in files]
-      '--input_dir',
+      '--input-dir',
-      '--output_dir',
+      '--output-dir',
-  tf.app.run(main)
+  args = parser.parse_args()
-from . import cifar10_model
+import cifar10
-                        tower_preds, False, params['num_layers'])
+              loss, gradvars, preds = _tower_fn(
-                    tower_losses, tower_gradvars, tower_preds, True)
+          loss, gradvars, preds = _tower_fn(
-              tower_gradvars, tower_preds, is_cpu, num_layers):
+def _tower_fn(is_training,
-      num_layers, is_training=is_training, data_format=data_format)
+      num_layers,
-  tower_gradvars.append(zip(tower_grad, model_params))
+
-  if args.num_gpus == 0 and not args.avg_on_gpu:
+  if args.num_gpus == 0 and args.avg_on_gpu:
-from . import model_base
+import model_base
-    super(ResNetCifar10, self).__init__(is_training, data_format)
+  def __init__(self,
-  def __init__(self, is_training, data_format):
+  def __init__(self, is_training, data_format, batch_norm_decay, batch_norm_epsilon):
-        decay=FLAGS.batch_norm_decay,
+        decay=self._batch_norm_decay,
-        epsilon=FLAGS.batch_norm_epsilon,
+        epsilon=self._batch_norm_epsilon,
-  parser.add_argument(
+import argparse
-
+from . import cifar10
-def _create_device_setter(is_cpu_ps, worker, num_gpus):
+def _create_device_setter(avg_on_gpu, worker, num_gpus):
-  if is_cpu_ps:
+  if avg_on_gpu:
-  """Resnet model body.
+def get_model_fn(num_gpus, avg_on_gpu, num_workers):
-     manages gradient updates.
+    Support single host, one or more GPU training. Parameter distribution can be
-      eval_metric_ops=metrics)
+    Args:
-              tower_gradvars, tower_preds, is_cpu):
+              tower_gradvars, tower_preds, is_cpu, num_layers):
-      FLAGS.num_layers, is_training=is_training, data_format=data_format)
+      num_layers, is_training=is_training, data_format=data_format)
-def input_fn(subset, num_shards):
+def input_fn(data_dir, subset, num_shards, batch_size,
-    dataset = cifar10.Cifar10DataSet(FLAGS.data_dir, subset, use_distortion)
+    use_distortion = subset == 'train' and use_distortion_for_training
-                      train_hooks):
+def get_experiment_fn(data_dir, num_gpus, is_gpu_ps,
-  fashion for distributed training.
+  fashion for distributed training. Arguments passed directly to this
-                                        config=run_config)
+    train_input_fn = functools.partial(
-    experiment.extend_train_hooks(train_hooks)
+    experiment.extend_train_hooks(hooks)
-def main(unused_argv):
+def main(job_dir,
-      inter_op_parallelism_threads=FLAGS.num_inter_threads,
+      log_device_placement=log_device_placement,
-          force_gpu_compatible=FLAGS.force_gpu_compatible
+          force_gpu_compatible=force_gpu_compatible
-  hooks = [logging_hook, examples_sec_hook]
+  config = tf.contrib.learn.RunConfig(
-                     hooks=hooks)
+if __name__ == '__main__':
-    print(eval_results)
+  if args.num_gpus < 0:
-  tf.app.run()
+  main(**vars(args))
-FLAGS = tf.flags.FLAGS
+FLAGS = None
-                       ' the CIFAR10 inputs + .tfrecords.')
+  parser = argparse.ArgumentParser()
-FLAGS = tf.flags.FLAGS
+FLAGS = None
-tf.flags.DEFINE_float('batch_norm_epsilon', 1e-5, 'Epsilon for batch norm.')
+from six.moves import xrange  # pylint: disable=redefined-builtin
-  verts, faces = measure.marching_cubes(points, 0, spacing=(0.1, 0.1, 0.1))
+  verts, faces = measure.marching_cubes_classic(points, 0, spacing=(0.1, 0.1, 0.1))
-
+# Copyright 2016 Google Inc. All Rights Reserved.
-    a TF-Feature.
+    A TF-Feature.
-    a TF-Feature.
+    A TF-Feature.
-from six.moves import cPickle
+from six.moves import cPickle
-
+from six.moves import range
-  for i in xrange(num_batches):
+  for i in range(num_batches):
-    'chars_logit', 'chars_log_prob', 'predicted_chars', 'predicted_scores'
+  'chars_logit', 'chars_log_prob', 'predicted_chars', 'predicted_scores',
-    'num_char_classes', 'seq_length', 'num_views', 'null_code'
+  'num_char_classes', 'seq_length', 'num_views', 'null_code'
-    'lstm_state_clip_value'
+  'use_attention', 'use_autoregression', 'num_lstm_units', 'weight_decay',
-    'label_smoothing', 'ignore_nulls', 'average_across_timesteps'
+  'label_smoothing', 'ignore_nulls', 'average_across_timesteps'
-               mparams=None):
+      num_char_classes,
-        'conv_tower_fn':
+      'conv_tower_fn':
-        'sequence_logit_fn':
+      'sequence_logit_fn':
-        'sequence_loss_fn':
+      'sequence_loss_fn':
-        'encode_coordinates_fn': EncodeCoordinatesParams(enabled=False)
+      'encode_coordinates_fn': EncodeCoordinatesParams(enabled=False)
-        d.value for d in nets_list[0].get_shape().dims
+      d.value for d in nets_list[0].get_shape().dims
-                  reuse=None):
+      images,
-          for i, v in enumerate(views)
+        self.conv_tower_fn(v, is_training, reuse=(i != 0))
-
+        self.char_predictions(chars_logit))
-        predicted_scores=predicted_scores)
+        predicted_scores=predicted_scores,
-  def create_init_fn_to_restore(self, master_checkpoint, inception_checkpoint):
+  def create_init_fn_to_restore(self, master_checkpoint,
-  def create_model(self):
+  def create_model(self, charset=None):
-        self.num_char_classes, self.seq_length, num_views=4, null_code=62)
+        self.num_char_classes, self.seq_length, num_views=4, null_code=62,
-  sess_config.gpu_options.force_gpu_compatible = FLAGS.force_gpu_compatible
+  sess_config = tf.ConfigProto(
-import cifar10_model
+from . import cifar10
-import model_base
+from . import model_base
-  with open(filename, 'r') as f:
+  with tf.gfile.Open(filename, 'r') as f:
-  record_writer.close()
+  with tf.python_io.TFRecordWriter(output_file) as record_writer:
-  app.run()
+try:
-    learning_signal = tf.stop_gradient(center(reinforce_learning_signal))
+    learning_signal = tf.stop_gradient(U.center(reinforce_learning_signal))
-                        ' parallelism. If set to 0, the system will pick
+                        'Number of threads to use for inter-op'
-                      """)
+                      'This is the inital learning rate value.'
-                        """)
+                        'If True will run an experiment,'
-                        """)
+                        'If true when running in a distributed environment'
-                        """)
+                        'Number of threads to use for intra-op parallelism.'
-                       an appropriate number.""")
+                        'Number of threads to use for inter-op
-                        for details.""")
+                        'Whether to enable force_gpu_compatible in'
-    is_training: true if is for training graph.
+    is_training: true if is training graph.
-    # create estimator
+    del hparams  # Unused arg.
-    # adding hooks to estimator on training mode
+    # Adding hooks to be used by the estimator on training mode.
-                                 shape=[1, image_size, image_size, 3])
+                                 shape=[FLAGS.batch_size, image_size,
-        s.run(eve_optimizer)
+        s.run(ac.eve_optimizer)
-        s.run(reset_eve_vars)
+        s.run(ac.reset_eve_vars)
-            s.run(eve_optimizer)
+            s.run(ac.eve_optimizer)
-    def test_keep_categories_with_unique_id(self):
+  def test_keep_categories_with_unique_id(self):
-    ]
+    staged_lr = [FLAGS.learning_rate * x for x in [1, 0.1, 0.01, 0.002]]
-
+def main(unused_argv):
-import model_base
+import model_base
-        for x in [1, 0.1, 0.01, 0.002]
+        FLAGS.learning_rate * x for x in [1, 0.1, 0.01, 0.002]
-
+import tensorflow as tf
-  and evaluation in the input_dir."""
+  """Returns the file names expected to exist in the input_dir."""
-                      """.)
+                      """)
-    
+    staged_lr = [
-                      [0.1, 0.001, 0.0001, 0.00002]
+tf.flags.DEFINE_float('learning_rate', 0.1,
-        for x in [0.1, 0.01, 0.001, 0.0002]]
+    staged_lr = [FLAGS.learning_rate * x 
-    image = tf.cast(image, tf.float32)
+    # Custom preprocessing.
-  file_names.append('test_batch')
+  """Returns the file names expected to exist for training, validation
-def convert_to_tfrecord(input_file, name):
+def convert_to_tfrecord(input_files, output_file):
-    record_writer.write(example.SerializeToString())
+  for input_file in input_files:
-    output_file = os.path.join(FLAGS.output_dir, file_name + '.tfrecords')
+  for mode, files in file_names.items():
-
+    convert_to_tfrecord(input_files, output_file)
-      return [os.path.join(self.data_dir, 'test_batch.tfrecords')]
+    if self.subset in ['train', 'validation', 'eval']:
-    image = tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0])
+    image = tf.cast(
-  """Yields the scope with the default parameters for inception_resnet_v2.
+  """Returns the scope with the default parameters for inception_resnet_v2.
-          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(
-          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(
-          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(
-          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(
-          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(
-          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(
-          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(
-          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(
-          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(
-        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.
+    spatial_squeeze: if True, logits is of shape [B, C], if false logits is of
-        stride=1, padding='SAME'):
+        [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
-          scope=end_point)
+
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
+        net = tf.concat(axis=concat_dim, values=[branch_0, branch_1, branch_2])
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
+        net = tf.concat(
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(
-        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(
-        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.
+    spatial_squeeze: if True, logits is of shape [B, C], if false logits is of
-                        tf.minimum(shape[2], kernel_size[1])])
+      return = tf.stack([tf.minimum(shape[1], kernel_size[0]),
-        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.
+    spatial_squeeze: if True, logits is of shape [B, C], if false logits is of
-                        tf.minimum(shape[2], kernel_size[1])])
+      return = tf.stack([tf.minimum(shape[1], kernel_size[0]),
-  2. Paramters are distributed evenly across all GPUs, and the first GPU
+  2. Parameters are distributed evenly across all GPUs, and the first GPU
-                        ' distributed training.')
+                        """If True will run an experiment,
-                        'will run on sync mode')
+                        """If true when running in a distributed environment
-tf.flags.DEFINE_integer('num_workers', 1, 'Number of workers')
+tf.flags.DEFINE_integer('num_workers', 1, 'Number of workers.')
-    staged_lr = [0.1, 0.01, 0.001, 0.0002]
+    staged_lr = [
-    record_writer.close()
+    # Convert to Examples and write the result to TFRecords.
-                        ' eval in a sensible fashion for distributed training.')
+                        'Experiments perform training on several workers in'
-    
+  """Hook to print out examples per second.
-    Args:
+
-      """
+    """
-          'exactly one of every_n_steps and every_n_secs should be provided.')
+      raise ValueError('exactly one of every_n_steps'
-      
+
-    raise ValueError('Subset must be one of \'train\', \'validate\' and \'eval\'')
+    raise ValueError('Subset must be one of \'train\''
-  
+
-import numpy as np
+
- 
+
-            "label": tf.FixedLenFeature([], tf.int64),
+            'image': tf.FixedLenFeature([], tf.string),
-    
+    image = tf.decode_raw(features['image'], tf.uint8)
-    label = tf.cast(features["label"], tf.int32)
+    image = tf.transpose(tf.reshape(image, [DEPTH, HEIGHT, WIDTH]), [1, 2, 0])
-    
+
-      output_buffer_size=2 * batch_size)
+                          output_buffer_size=2 * batch_size)
-    print(image_batch, label_batch)
+
-  
+
-  def __init__(self, data_dir):
+  def __init__(self, data_dir, subset='train', use_distortion=True):
-          os.path.join(self.data_dir, 'data_batch_%d' % i)
+    self.subset = subset
-      filenames = [os.path.join(self.data_dir, 'test_batch')]
+    elif self.subset == 'validation':
-      raise ValueError('Invalid data subset "%s"' % subset)
+      raise ValueError('Invalid data subset "%s"' % self.subset)
-    return all_images, all_labels
+  def parser(self, serialized_example):
-      return image
+    # Custom preprocessing .
-tf.flags.DEFINE_boolean('is_cpu_ps', False,
+tf.flags.DEFINE_boolean('is_cpu_ps', True,
-tf.flags.DEFINE_integer('train_steps', 10000,
+tf.flags.DEFINE_integer('train_steps', 80000,
-                        'using the estimator interface')
+                        'using the estimator interface.'
-                        'Number of workers')
+tf.flags.DEFINE_integer('num_workers', 1, 'Number of workers')
-  """Helper class to assign variables on the least loaded ps-device."""
+class ExamplesPerSecondHook(session_run_hook.SessionRunHook):
-    """Initializer for ParamServerDeviceSetter.
+  def __init__(
-      ps_devices: a list of devices to use for Variable ops. Each variable is
+      worker_device: the device to use for computation Ops.
-
+      
-def _create_device_setter(is_cpu_ps, worker):
+def _create_device_setter(is_cpu_ps, worker, num_gpus):
-    return ParamServerDeviceSetter(worker, gpus)
+    gpus = ['/gpu:%d' % i for i in range(num_gpus)]
-      device_setter = _create_device_setter(is_cpu_ps, worker)
+      device_setter = _create_device_setter(is_cpu_ps, worker, FLAGS.num_gpus)
-      loss = tf.reduce_mean(tower_losses)
+      loss = tf.reduce_mean(tower_losses, name='loss')
-  # session configuration
+  # Session configuration.
-  tensors_to_log = {'learning_rate': 'learning_rate'}
+  # Hooks that add extra logging that is useful to see the loss more often in
-                          [logging_hook]), run_config=config)
+                          hooks), run_config=config)
-                     hooks=[logging_hook])
+                     hooks=hooks)
-    TODO(jonathanhuang,rathodv): revisit whether it's possible to force the
+    TODO: revisit whether it's possible to force the
-       posterior_sample_and_average - sample from the posterior, this is used 
+       posterior_sample_and_average - sample from the posterior, this is used
-    
+
-          print(var.name, """ is not numpy array, saving as numpy array 
+          print(var.name, """ is not numpy array, saving as numpy array
-        'Provided bias_init_value must have shape [1,%d].'%(1,out_size))
+        'Provided bias_init_value must have shape [1,%d].'%(out_size,))
-        'train') // FLAGS.train_batch_size
+        'train') // (FLAGS.train_batch_size * FLAGS.num_workers)
-  """Helper class to assign variables on the least loaded ps-device."""
+class GpuParamServerDeviceSetter(object):
-    """Initializer for ParamServerDeviceSetter.
+    A common use for this class is to pass a list of GPU devices, e.g. ['gpu:0',
-      ps_devices: a list of devices to use for Variable ops. Each variable is
+      worker_device: the device to use for computation Ops.
-
+      
-def _create_device_setter(is_cpu_ps, worker):
+def _create_device_setter(is_cpu_ps, worker, num_gpus):
-    return ParamServerDeviceSetter(worker, gpus)
+    gpus = ['/gpu:%d' % i for i in range(num_gpus)]
-      device_setter = _create_device_setter(is_cpu_ps, worker)
+      device_setter = _create_device_setter(is_cpu_ps, worker, FLAGS.num_gpus)
-      sync_replicas_hook = opt.make_session_run_hook(True)
+      sync_replicas_hook = optimizer.make_session_run_hook(True)
-tf.flags.DEFINE_float('weight_decay', 1e-4, 'Weight decay for convolutions.')
+tf.flags.DEFINE_float('weight_decay', 2e-4, 'Weight decay for convolutions.')
-  depth_major = tf.reshape(tf.slice(record, [label_bytes], [image_bytes]),
+  depth_major = tf.reshape(tf.slice(record, [label_offset + label_bytes], [image_bytes]),
-  # log learning_rate 
+  # log learning_rate
-tf.flags.DEFINE_string('data_dir', 'cifar10',
+tf.flags.DEFINE_string('data_dir', '',
-tf.flags.DEFINE_string('model_dir', 'output2_2',
+tf.flags.DEFINE_string('model_dir', '',
-tf.flags.DEFINE_boolean('is_cpu_ps', True,
+tf.flags.DEFINE_boolean('is_cpu_ps', False,
-tf.flags.DEFINE_integer('train_batch_size', 1024, 'Batch size for training.')
+tf.flags.DEFINE_integer('train_steps', 10000,
-                        'The number of steps to use for training.') # 40 epochs
+tf.flags.DEFINE_integer('train_batch_size', 128, 'Batch size for training.')
-tf.flags.DEFINE_integer('eval_batch_size', 200, 'Batch size for validation.')
+tf.flags.DEFINE_integer('eval_batch_size', 100, 'Batch size for validation.')
-					     name_scope)
+        with tf.name_scope('tower_%d' % i) as name_scope:
-	update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scope)
+        _tower_fn(is_training, weight_decay, tower_features[0], tower_labels[0],
-	gradvars.append((avg_grad, var))
+        # Averaging one var's gradients computed from multiple towers
-	'train') // FLAGS.train_batch_size
+        'train') // FLAGS.train_batch_size
-	for x in np.array([82, 123, 300], dtype=np.int64)
+        num_batches_per_epoch * x
-						boundaries, staged_lr)
+    learning_rate = tf.train.piecewise_constant(tf.train.get_global_step(),
-					   momentum=momentum)
+    optimizer = tf.train.MomentumOptimizer(
-	    gradvars, global_step=global_step)
+        optimizer.apply_gradients(
-	    tf.concat([p['probabilities'] for p in tower_preds], axis=0)
+        'classes':
-	'accuracy': tf.metrics.accuracy(stacked_labels, predictions['classes'])
+        'accuracy': tf.metrics.accuracy(stacked_labels, predictions['classes'])
-  if is_training:
+  if subset == 'train':
-  else:
+  elif subset == 'validate' or subset == 'eval':
-    image_batch, label_batch = iterator.get_next()
+  else:
-        return experiment
+# create experiment
-  config = tf.contrib.learn.RunConfig(model_dir=FLAGS.model_dir)
+  train_input_fn = functools.partial(input_fn, subset='train',
-  tensors_to_log = {'learning_rate': 'learning_rate'} 
+  # log learning_rate 
-  learn_runner.run(create_experiment_fn(train_input, test_input, hooks), run_config=config)
+  if FLAGS.run_experiment:
-tf.flags.DEFINE_string('data_dir', '',
+tf.flags.DEFINE_string('data_dir', 'cifar10',
-tf.flags.DEFINE_string('model_dir', '',
+tf.flags.DEFINE_string('model_dir', 'output2_2',
-tf.flags.DEFINE_boolean('is_cpu_ps', False,
+tf.flags.DEFINE_boolean('is_cpu_ps', True,
-                        'The number of steps to use for training.')
+tf.flags.DEFINE_integer('train_batch_size', 1024, 'Batch size for training.')
-tf.flags.DEFINE_integer('train_batch_size', 128, 'Batch size for training.')
+tf.flags.DEFINE_integer('train_steps', (50000.0/FLAGS.train_batch_size) * 40,
-tf.flags.DEFINE_integer('eval_batch_size', 100, 'Batch size for validation.')
+tf.flags.DEFINE_integer('eval_batch_size', 200, 'Batch size for validation.')
-                                             name_scope)
+	with tf.name_scope('tower_%d' % i) as name_scope:
-        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, name_scope)
+	_tower_fn(is_training, weight_decay, tower_features[0], tower_labels[0],
-        gradvars.append((avg_grad, var))
+	# Averaging one var's gradients computed from multiple towers
-        'train') // FLAGS.train_batch_size
+	'train') // FLAGS.train_batch_size
-        for x in np.array([82, 123, 300], dtype=np.int64)
+	num_batches_per_epoch * x
-                                                boundaries, staged_lr)
+    
-
+    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,
-            gradvars, global_step=tf.train.get_global_step())
+	optimizer.apply_gradients(
-            tf.concat([p['probabilities'] for p in tower_preds], axis=0)
+	'classes':
-        'accuracy': tf.metrics.accuracy(stacked_labels, predictions['classes'])
+	'accuracy': tf.metrics.accuracy(stacked_labels, predictions['classes'])
-  return _experiment_fn
+        experiment.extend_train_hooks(hooks)
-
+  train_input = functools.partial(input_fn, subset='train', num_shards=FLAGS.num_gpus)
-    spikified_data_e: a list of length b of the data represented as spikes,
+    spikified_e: a list of length b of the data represented as spikes,
-    sampled from the underlying poisson process.
+    gauss_e: a list of length b of the data with noise.
-          raise ValueError("""Alignment matrix must have dimensions %d x %d 
+          raise ValueError("""Alignment matrix must have dimensions %d x %d
-        out_mat_cxf = None
+        out_mat_fxc = None
-          out_mat_cxf = in_mat_cxf.T
+          out_mat_fxc = np.linalg.pinv(in_mat_cxf)
-                                    mat_init_value=out_mat_cxf,
+                                    mat_init_value=out_mat_fxc,
-                          mat_init_value=out_mat_cxf,
+                          mat_init_value=out_mat_fxc,
-                          mat_init_value=out_mat_cxf,
+                          mat_init_value=mat_init_value,
-from synthetic_data_utils import spikify_data, split_list_by_inds
+from synthetic_data_utils import spikify_data, gaussify_data, split_list_by_inds
-                     "Number of spikifications of the same underlying rates.")
+flags.DEFINE_integer("nreplications", 40,
-E = nspikifications * C         # total number of trials
+nreplications = FLAGS.nreplications
-  rem_check = nspikifications * train_percentage
+  rem_check = nreplications * train_percentage
-    'Train percentage  * nspikifications should be integral number.'
+    'Train percentage  * nreplications should be integral number.'
-  for ns in range(nspikifications):
+  x0s.append(np.tile(x0, nreplications)) # replicate x0 nreplications times
-  # independent of n_spikifications.
+  # replications this allows the random state for rate generation to be
-  # split into train and validation sets
+  if FLAGS.noise_type == "poisson":
-                                                  nspikifications)
+                                                  nreplications)
-      split_list_by_inds(spikes, train_inds, valid_inds)
+  noisy_data_train, noisy_data_valid = \
-  # Turn rates, spikes, and input into numpy arrays.
+  # Turn rates, noisy_data, and input into numpy arrays.
-  spikes_valid = nparray_and_transpose(spikes_valid)
+  noisy_data_train = nparray_and_transpose(noisy_data_train)
-  # trains.  The rest is either for printing or posterity.
+  # structure, the only data that is used in LFADS are the noisy
-          'valid_data' : spikes_valid,
+          'train_data' : noisy_data_train,
-          'nspikifications' : nspikifications,
+          'nreplications' : nreplications,
-  spikifies_data_e = []
+def gaussify_data(data_e, rng, dt=1.0, max_firing_rate=100):
-                name=None, collections=None):
+def init_linear(in_size, out_size, do_bias=True, mat_init_value=None,
-    b = None
+    if bias_init_value is None:
-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2017 Google Inc.
-# ==============================================================================
+
-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2017 Google Inc.
-# http://www.apache.org/licenses/LICENSE-2.0
+#     http://www.apache.org/licenses/LICENSE-2.0
-# ==============================================================================
+
-                        name='image_tensor')
+  """Returns placeholder and input node that accepts a batch of uint8 images."""
-  """Returns input node that accepts a batch of strings with tf examples."""
+  """Returns input that accepts a batch of strings with tf examples.
-                   back_prop=False)
+  return (batch_tf_example_placeholder,
-  """Returns input node that accepts a batch of PNG or JPEG strings."""
+  """Returns input that accepts a batch of PNG or JPEG strings.
-                   back_prop=False)
+  return (batch_image_str_placeholder,
-              'signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY':
+              signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
-  inputs = tf.to_float(input_placeholder_fn_map[input_type]())
+  placeholder_tensor, input_tensors = input_placeholder_fn_map[input_type]()
-  _write_saved_model(saved_model_path, frozen_graph_def, inputs, outputs)
+  _write_saved_model(saved_model_path, frozen_graph_def, placeholder_tensor,
-    this_out_fac_b = tf.case(pf_pairs_out_fac_bs, case_default, exclusive=True)
+    def _case_with_no_default(pairs):
-tf.app.flags.MarkFlagAsRequired('output_directory')
+  assert FLAGS.pipeline_config_path, '`pipeline_config_path` is missing'
-    optimize_graph=True,
+    optimize_graph=False,
-                            optimize_graph=True,
+                            optimize_graph=False,
-                           optimize_graph=True,
+                           optimize_graph=False,
-    case is always 1.
+  * `image_tensor`: Accepts a uint8 4-D tensor of shape [None, None, None, 3]
-
+  """Returns input node that accepts a batch of uint8 images."""
-                        shape=(1, None, None, 3),
+                        shape=(None, None, None, 3),
-  return tf.expand_dims(image_tensor, axis=0)
+  """Returns input node that accepts a batch of PNG or JPEG strings."""
-          'num_detections': tf.constant([2], tf.float32)
+          'detection_boxes': tf.constant([[[0.0, 0.0, 0.5, 0.5],
-            np.arange(32).reshape([2, 4, 4]), tf.float32)
+            np.arange(64).reshape([2, 2, 4, 4]), tf.float32)
-          tf.ones([1, 3, 4, 3], tf.float32))
+          tf.placeholder(tf.float32, shape=[None, None, None, 3]))
-      (boxes, scores, classes, masks, num_detections) = sess.run(
+      (boxes_np, scores_np, classes_np, masks_np, num_detections_np) = sess.run(
-      self.assertAllClose(num_detections, [2])
+          feed_dict={image_tensor: np.ones((2, 4, 4, 3)).astype(np.uint8)})
-        self.assertAllClose(num_detections_np, [2])
+             feed_dict={image_str_tensor: image_str_batch_np})
-      (boxes, scores, classes, masks, num_detections) = sess.run(
+      (boxes_np, scores_np, classes_np, masks_np, num_detections_np) = sess.run(
-      self.assertAllClose(num_detections, [2])
+          feed_dict={tf_example: tf_example_np})
-        self.assertAllClose(num_detections, [2])
+        (boxes_np, scores_np, classes_np, masks_np,
-        self.assertAllClose(num_detections, [2])
+        (boxes_np, scores_np, classes_np, masks_np,
-graph or a SavedModel (https://tensorflow.github.io/serving/serving_basic.html).
+configuration and an optional trained checkpoint. Outputs inference
-which weights to freeze.
+Notes:
-    --inference_graph_path path/to/inference_graph.pb
+    --trained_checkpoint_prefix path/to/model.ckpt \
-flags.DEFINE_string('pipeline_config_path', '',
+flags.DEFINE_string('pipeline_config_path', None,
-                  'should be saved as a SavedModel')
+flags.DEFINE_string('trained_checkpoint_prefix', None,
-                                  FLAGS.export_as_saved_model)
+  exporter.export_inference_graph(
-      saver.restore(sess, input_checkpoint)
+  with tf.Graph().as_default():
-        variable_names_blacklist=variable_names_blacklist)
+      logging.info('Graph Rewriter optimizations disabled')
-def _add_output_tensor_nodes(postprocessed_tensors):
+def _add_output_tensor_nodes(postprocessed_tensors,
-  is restored.
+def _write_frozen_graph(frozen_graph_path, frozen_graph_def):
-      detection_scores, detection_boxes, detection_classes.
+    frozen_graph_path: Path to write inference graph.
-                       checkpoint_path=None, use_moving_averages=False):
+  with gfile.GFile(frozen_graph_path, 'wb') as f:
-    inference_graph_path: Path to write inference graph.
+    saved_model_path: Path to write SavedModel.
-      tf.import_graph_def(checkpoint_graph_def)
+      tf.import_graph_def(frozen_graph_def, name='')
-      builder = tf.saved_model.builder.SavedModelBuilder(inference_graph_path)
+      builder = tf.saved_model.builder.SavedModelBuilder(saved_model_path)
-              signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
+              'signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY':
-                            export_as_saved_model=False):
+                            trained_checkpoint_prefix,
-                       use_moving_averages)
+  outputs = _add_output_tensor_nodes(postprocessed_tensors,
-                           output_node_names=','.join(out_node_names))
+    saver = tf.train.Saver()
-                           inference_graph_path, export_as_saved_model=False):
+def export_inference_graph(input_type,
-                           false, it is saved as an inference graph.
+    trained_checkpoint_prefix: Path to the trained checkpoint file.
-                          export_as_saved_model)
+                          trained_checkpoint_prefix, output_directory,
-
+      output_directory = os.path.join(tmp_dir, 'output')
-          inference_graph_path=inference_graph_path)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-                                          'exported_graph.pbtxt')
+      output_directory = os.path.join(tmp_dir, 'output')
-          inference_graph_path=inference_graph_path)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-    self._save_checkpoint_from_mock_model(checkpoint_path,
+    tmp_dir = self.get_temp_dir()
-                                        'exported_graph.pb')
+      output_directory = os.path.join(tmp_dir, 'output')
-          input_type='image_tensor',
+          input_type='encoded_image_string_tensor',
-          inference_graph_path=inference_graph_path)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-    self._save_checkpoint_from_mock_model(checkpoint_path,
+  def test_export_graph_with_moving_averages(self):
-                                        'exported_graph.pb')
+    output_directory = os.path.join(tmp_dir, 'output')
-          inference_graph_path=inference_graph_path)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-                                        'exported_graph.pb')
+    tmp_dir = self.get_temp_dir()
-          inference_graph_path=inference_graph_path)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-                                        'exported_graph.pb')
+    tmp_dir = self.get_temp_dir()
-          inference_graph_path=inference_graph_path)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-                                        'exported_graph.pb')
+    tmp_dir = self.get_temp_dir()
-          inference_graph_path=inference_graph_path)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-                                        'exported_graph.pb')
+    tmp_dir = self.get_temp_dir()
-          inference_graph_path=inference_graph_path)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-                                        'exported_graph.pb')
+    tmp_dir = self.get_temp_dir()
-          inference_graph_path=inference_graph_path)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-    self._save_checkpoint_from_mock_model(checkpoint_path,
+    tmp_dir = self.get_temp_dir()
-                                        'saved_model')
+    output_directory = os.path.join(tmp_dir, 'output')
-          export_as_saved_model=True)
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
-        num_detections = od_graph.get_tensor_by_name('import/num_detections:0')
+            sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)
-from object_detection.core import box_coder as bcoder
+from object_detection.utils import shape_utils
-        feature_map.get_shape().as_list() for feature_map in feature_maps
+        shape_utils.combined_static_and_dynamic_shape(
-                                            self.anchors)
+      detection_boxes = self._batch_decode(box_encodings)
-                                        self._box_coder, self.anchors)
+    decoded_boxes = self._batch_decode(prediction_dict['box_encodings'])
-
+    image_size = 2
-      prediction_out = sess.run(prediction_dict)
+
-    detections = self._model.postprocess(prediction_dict)
+    image_size = 2
-      detections_out = sess.run(detections)
+    for input_shape in input_shapes:
-                   * image_features.get_shape().as_list()[2])
+    combined_feature_shape = shape_utils.combined_static_and_dynamic_shape(
-          class_predictions_with_background, [batch_size, -1, num_class_slots])
+    combined_feature_map_shape = shape_utils.combined_static_and_dynamic_shape(
-        [inputs_shape[-1]]], 0)
+    combined_shape = shape_utils.combined_static_and_dynamic_shape(inputs)
-            tf.stack(num_proposals_list))
+      proposal_boxes = tf.stop_gradient(proposal_boxes)
-    refined_decoded_boxes_batch = self._batch_decode_refined_boxes(
+    refined_decoded_boxes_batch = self._batch_decode_boxes(
-  def _batch_decode_refined_boxes(self, refined_box_encodings, proposal_boxes):
+  def _batch_decode_boxes(self, box_encodings, anchor_boxes):
-        tf.reshape(tiled_proposal_boxes, [-1, 4]))
+    """Decodes box encodings with respect to the anchor boxes.
-        tiled_proposals_boxlist)
+        tf.reshape(box_encodings, [-1, self._box_coder.code_size]),
-                      [-1, self.max_num_proposals, self.num_classes, 4])
+                      tf.stack([combined_shape[0], combined_shape[1],
-  def test_predict_gives_correct_shapes_in_inference_mode_first_stage_only(
+  def test_predict_correct_shapes_in_inference_mode_both_stages(
-      with self.test_session() as sess:
+    batch_size = 2
-        self.assertTrue(np.all(np.less_equal(anchors[:, 3], width)))
+        tensor_dict_out = sess.run(result_tensor_dict, feed_dict={
-
+    num_proposals_shapes = [(2), (None)]
-      detections_out = sess.run(detections)
+    image_shape = np.array((2, 36, 48, 3), dtype=np.int32)
-    'detection_boxes': A [batch_size, max_detections, 4] float32 tensor
+    'nmsed_boxes': A [batch_size, max_detections, 4] float32 tensor
-    'detection_scores': A [bath_size, max_detections] float32 tensor containing
+    'nmsed_scores': A [batch_size, max_detections] float32 tensor containing
-    'detection_classes': A [batch_size, max_detections] float32 tensor
+    'nmsed_classes': A [batch_size, max_detections] float32 tensor
-    'num_detections': A [batchsize] float32 tensor indicating the number of
+    'nmsed_masks': (optional) a
-      a valid scores field or if num_anchors is not statically defined.
+    ValueError: if `q` in boxes.shape is not 1 or not equal to number of
-                       'defined.')
+      num_anchors = tf.shape(boxes)[1]
-      num_valid_boxes = tf.fill(num_valid_boxes_shape, num_anchors)
+      num_valid_boxes = tf.ones([batch_size], dtype=tf.int32) * num_anchors
-      per_image_boxes, per_image_scores, per_image_masks, num_valid_boxes = args
+      (per_image_boxes, per_image_scores, per_image_masks,
-                   tf.stack([num_valid_boxes, -1, -1])), [-1, q, 4])
+                   tf.stack([per_image_num_valid_boxes, -1, -1])), [-1, q, 4])
-                   tf.stack([num_valid_boxes, -1])), [-1, num_classes])
+                   tf.stack([per_image_num_valid_boxes, -1])),
-                   tf.stack([num_valid_boxes, -1, -1, -1])),
+                   tf.stack([per_image_num_valid_boxes, -1, -1, -1])),
-              detection_masks, num_detections]
+      num_detections = nmsed_boxlist.num_boxes()
-     batch_detection_classes, batch_detection_masks,
+    (batch_nmsed_boxes, batch_nmsed_scores,
-         dtype=[tf.float32, tf.float32, tf.float32, tf.float32, tf.float32],
+         dtype=[tf.float32, tf.float32, tf.float32, tf.float32, tf.int32],
-    return nms_dict
+    if original_masks is None:
-        max_size_per_class=max_output_size, max_total_size=max_output_size)
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-      self.assertEqual(nms_output['num_detections'], [4])
+      (nmsed_boxes, nmsed_scores, nmsed_classes,
-        max_size_per_class=max_output_size, max_total_size=max_output_size)
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-    self.assertAllEqual(nms_dict['detection_boxes'].get_shape().as_list(),
+    self.assertAllEqual(nmsed_boxes.shape.as_list(),
-    self.assertAllEqual(nms_dict['detection_scores'].get_shape().as_list(),
+    self.assertAllEqual(nmsed_scores.shape.as_list(),
-    self.assertAllEqual(nms_dict['detection_classes'].get_shape().as_list(),
+    self.assertAllEqual(nmsed_classes.shape.as_list(),
-    self.assertEqual(nms_dict['num_detections'].get_shape().as_list(), [2])
+    self.assertEqual(num_detections.shape.as_list(), [2])
-      self.assertAllClose(nms_output['num_detections'], [2, 3])
+      (nmsed_boxes, nmsed_scores, nmsed_classes,
-        masks=masks)
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-    self.assertEqual(nms_dict['num_detections'].get_shape().as_list(), [2])
+    self.assertAllEqual(nmsed_boxes.shape.as_list(), exp_nms_corners.shape)
-    masks_placeholder = tf.placeholder(tf.float32, shape=(None, 4, 2, 2, 2))
+      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-        masks=masks_placeholder)
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-    self.assertEqual(nms_dict['num_detections'].get_shape().as_list(), [None])
+    self.assertAllEqual(nmsed_boxes.shape.as_list(), [None, 4, 4])
-      self.assertAllClose(nms_output['detection_masks'], exp_nms_masks)
+      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-        num_valid_boxes=num_valid_boxes, masks=masks)
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-      self.assertAllClose(nms_output['detection_masks'], exp_nms_masks)
+      (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-        masks=mask_predictions_batch)
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
-    return detections
+      (nmsed_boxes, nmsed_scores, nmsed_classes, _,
-                                         scope=None):
+                                         scope=None,
-      a valid scores field.
+      a valid scores field or if num_anchors is not statically defined.
-      per_image_masks_list = tf.unstack(masks)
+    boxes_shape = boxes.shape
-              [-1, q, masks.shape[3].value, masks.shape[4].value])
+    def single_image_nms_fn(args):
-      num_detections_list.append(tf.to_float(nmsed_boxlist.num_boxes()))
+      num_detections = tf.to_float(nmsed_boxlist.num_boxes())
-            padded_boxlist.get_field(fields.BoxListFields.masks))
+      detection_boxes = padded_boxlist.get()
-        'num_detections': tf.stack(num_detections_list)
+        'detection_boxes': batch_detection_boxes,
-      nms_dict['detection_masks'] = tf.stack(detection_masks_list)
+    if original_masks is not None:
-        boxes, scores, score_thresh, iou_thresh,
+        boxes_placeholder, scores_placeholder, score_thresh, iou_thresh,
-      nms_output = sess.run(nms_dict)
+      nms_output = sess.run(nms_dict, feed_dict={boxes_placeholder: boxes,
-                      [[0, 0], [0, 0]]]]
+    exp_nms_corners = np.array([[[0, 10, 1, 11],
-          lambda: small_size, lambda: large_size)
+    if image.get_shape().is_fully_defined():
-
+      new_size = _compute_new_dynamic_size(image, min_dimension,
-      result = [new_image, masks]
+      new_masks = tf.expand_dims(masks, 3)
-  def testResizeToRange(self):
+  def testResizeToRangePreservesStaticSpatialShape(self):
-      out_image_shape = tf.shape(out_image)
+      self.assertAllEqual(out_image.get_shape().as_list(), expected_shape)
-        out_image_shape = sess.run(out_image_shape)
+        out_image_shape = sess.run(out_image_shape,
-  def testResizeToRangeWithMasks(self):
+  def testResizeToRangeWithMasksPreservesStaticSpatialShape(self):
-        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+      self.assertAllEqual(out_masks.get_shape().as_list(), expected_mask_shape)
-  def testResizeToRangeWithNoInstanceMask(self):
+  def testResizeToRangeWithMasksAndDynamicSpatialShape(self):
-    in_masks_shape_list = [[0, 60, 40], [0, 15, 30]]
+    in_masks_shape_list = [[15, 60, 40], [10, 15, 30]]
-    expected_masks_shape_list = [[0, 75, 50], [0, 50, 100]]
+    expected_masks_shape_list = [[15, 75, 50], [10, 50, 100]]
-      in_image = tf.random_uniform(in_image_shape)
+      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-            [out_image_shape, out_masks_shape])
+            [out_image_shape, out_masks_shape],
-  def testResizeImageWithNoInstanceMask(self):
+  def testResizeToRangeWithInstanceMasksTensorOfSizeZero(self):
-      saver = tf.train.Saver(var_map)
+      available_var_map = (variables_helper.
-        saver.restore(sess, train_config.fine_tune_checkpoint)
+        init_saver.restore(sess, train_config.fine_tune_checkpoint)
-    """Return callable for loading a foreign checkpoint into tensorflow graph.
+  def restore_map(self, from_detection_checkpoint=True):
-    from another task. For example, the feature extractor variables from a
+    Returns a map of variable names to load from a checkpoint to variables in
-        run.
+      A dict mapping variable names (to load from a checkpoint) to variables in
-  def restore_fn(self, checkpoint_path, from_detection_checkpoint):
+  def restore_map(self, checkpoint_path, from_detection_checkpoint):
-    """Returns callable for loading a checkpoint into the tensorflow graph.
+    """Returns a map of variables to load from a foreign checkpoint.
-        run.
+      A dict mapping variable names (to load from a checkpoint) to variables in
-    return restore
+    return variables_to_restore
-    """Returns callable for loading a checkpoint into the tensorflow graph.
+  def restore_map(self, from_detection_checkpoint=True):
-        of the num_classes parameter).
+      from_detection_checkpoint: whether to restore from a full detection
-        run.
+      A dict mapping variable names (to load from a checkpoint) to variables in
-    first_stage_variables = tf.contrib.framework.filter_variables(
+    feature_extractor_variables = tf.contrib.framework.filter_variables(
-  def test_restore_fn_classification(self):
+  def test_restore_map_for_classification_ckpt(self):
-                                    from_detection_checkpoint=False)
+      var_map = model.restore_map(from_detection_checkpoint=False)
-        restore_fn(sess)
+        saver.restore(sess, saved_model_path)
-  def test_restore_fn_detection(self):
+  def test_restore_map_for_detection_ckpt(self):
-                                     from_detection_checkpoint=True)
+      var_map = model2.restore_map(from_detection_checkpoint=True)
-        restore_fn(sess)
+        saver.restore(sess, saved_model_path)
-    """Return callable for loading a checkpoint into the tensorflow graph.
+  def restore_map(self, from_detection_checkpoint=True):
-        run.
+      A dict mapping variable names (to load from a checkpoint) to variables in
-              re.split('^' + self._extract_features_scope + '/', var_name)[-1])
+          var_name = (re.split('^' + self._extract_features_scope + '/',
-    return restore
+    return variables_to_restore
-  def test_restore_fn_detection(self):
+  def test_restore_map_for_detection_ckpt(self):
-      restore_fn(sess)
+      var_map = self._model.restore_map(from_detection_checkpoint=True)
-  def test_restore_fn_classification(self):
+  def test_restore_map_for_classification_ckpt(self):
-                                          from_detection_checkpoint=False)
+      var_map = self._model.restore_map(from_detection_checkpoint=False)
-        restore_fn(sess)
+        saver.restore(sess, saved_model_path)
-    """Returns callable for loading a checkpoint into the tensorflow graph.
+    """Returns a map of variables to load from a foreign checkpoint.
-    `Repeat_2`) so that the default restore_fn can be used.
+    TODO(jonathanhuang,rathodv): revisit whether it's possible to force the
-        run.
+      A dict mapping variable names (to load from a checkpoint) to variables in
-    return restore
+    return variables_to_restore
-          train_config.fine_tune_checkpoint,
+      var_map = detection_model.restore_map(
-    """Return callable for loading a checkpoint into the tensorflow graph.
+  def restore_map(self, from_detection_checkpoint=True):
-      a callable which takes a tf.Session and does nothing.
+      A dict mapping variable names to variables.
-    return restore
+    return {var.op.name: var for var in tf.global_variables()}
-      return f.read().decode("utf-8").replace("\n", "<eos>").split()
+    return f.read().decode("utf-8").replace("\n", "<eos>").split()
-from abc import abstractmethod
+from __future__ import absolute_import
-        scaling_coefficient=1.0,
+    embeddings = syntaxnet_ops.word_embedding_initializer(
-  __metaclass__ = ABCMeta  # required for @abstractmethod
+  __metaclass__ = abc.ABCMeta  # required for @abstractmethod
-  @abstractmethod
+  @abc.abstractmethod
-
+  def testWordEmbeddingInitializerFailIfNeitherTaskContextOrVocabulary(self):
-      as category name.  If False of if the display_name field does not exist,
+      as category name.  If False or if the display_name field does not exist,
-from google3.third_party.tensorflow_models.slim import export_inference_graph
+import export_inference_graph
-    self.writer = tf.summary.FileWriter(self.logfile, session.graph)
+    self.writer = tf.summary.FileWriter(self.logfile)
-import tensorflow as tf
+import tensorflow as tf
-                     batch_norm_scale=True):
+                     batch_norm_scale=True,
-      normalizer_fn=slim.batch_norm,
+      activation_fn=activation_fn,
-               outputs_collections=None, scope=None):
+def bottleneck(inputs,
-                             activation_fn=None, scope='shortcut')
+      shortcut = slim.conv2d(
-    output = tf.nn.relu(shortcut + residual)
+    if use_bounded_activations:
-              spatial_squeeze=False,
+              spatial_squeeze=True,
-              spatial_squeeze=False,
+              spatial_squeeze=True,
-                 spatial_squeeze=False,
+                 spatial_squeeze=True,
-                  spatial_squeeze=False,
+                  spatial_squeeze=True,
-                  spatial_squeeze=False,
+                  spatial_squeeze=True,
-                  spatial_squeeze=False,
+                  spatial_squeeze=True,
-      the output prediction map will be (input / 32) - 6 in case of 'VALID' padding.
+      get a prediction map downsampled by a factor of 32 as an output.
-      the output prediction map will be (input / 32) - 6 in case of 'VALID' padding.
+      get a prediction map downsampled by a factor of 32 as an output.
-      the output prediction map will be (input / 32) - 6 in case of 'VALID' padding.
+      get a prediction map downsampled by a factor of 32 as an output.
-  def __init__(self, data_dir):
+  def __init__(self, data_dir, subset='train', use_distortion=True):
-          os.path.join(self.data_dir, 'data_batch_%d' % i)
+    self.subset = subset
-      filenames = [os.path.join(self.data_dir, 'test_batch')]
+    elif self.subset == 'validation':
-      raise ValueError('Invalid data subset "%s"' % subset)
+      raise ValueError('Invalid data subset "%s"' % self.subset)
-    return all_images, all_labels
+  def make_batch(self, batch_size):
-      return image
+  def parser(self, value):
-  if is_training:
+  if subset == 'train':
-  else:
+  elif subset == 'validate' or subset == 'eval':
-    image_batch, label_batch = iterator.get_next()
+  else:
-          end_points['AuxLogits'] = aux_logits
+      if create_aux_logits:
-# TODO: double check documentaion.
+  _validate_label_map(label_map)
-  def test_keep_categories_with_unique_id(self):
+  def test_load_bad_label_map(self):
-                        "using the estimator interface")
+                        'If True will run an experiment,'
-                                   num_shards=FLAGS.num_gpus)
+                                    num_shards=FLAGS.num_gpus)
-                        "using the estimator's methods")
+                        "If True will run an experiment,"
-	)
+def get_experiment_fn(train_input_fn, eval_input_fn, train_steps, eval_steps):
-    tf.contrib.learn.learn_runner.run(experiment_fn, run_config=run_config)
+    tf.contrib.learn.learn_runner.run(
-        hooks=[logging_hook])
+    classifier.train(input_fn=train_input_fn,
-        steps=num_eval_examples // FLAGS.eval_batch_size)
+        input_fn=eval_input_fn,
-  config = tf.estimator.RunConfig()
+  config = tf.contrib.learn.RunConfig(model_dir=FLAGS.model_dir)
-  print(eval_results)
+  if FLAGS.run_experiment:
-    dataset = dataset_factory.get_dataset(FLAGS.dataset_name, 'validation',
+    dataset = dataset_factory.get_dataset(FLAGS.dataset_name, 'train',
-  for box, color in six.iteritems(box_to_color_map):
+  for box, color in box_to_color_map.items():
-        for output_key, expected_shape in expected_output_shapes.iteritems():
+        for output_key, expected_shape in expected_output_shapes.items():
-          (key, value.shape) for key, value in out_feature_maps.iteritems())
+          (key, value.shape) for key, value in out_feature_maps.items())
-          (key, value.shape) for key, value in out_feature_maps.iteritems())
+          (key, value.shape) for key, value in out_feature_maps.items())
-  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)
+    arg_scope = arg_scopes_map[name](weight_decay=weight_decay)
-            average_across_timesteps=False)
+            average_across_timesteps=False),
-        self.rng.randn(*self.conv_tower_shape).astype('float32'))
+    self.fake_conv_tower_np = self.rng.randn(
-         for key, tensor in tensor_dict.iteritems()})
+         for key, tensor in tensor_dict.items()})
-        'dataset_name [%s] was not recognized.' % FLAGS.dataset_dir)
+        'dataset_name [%s] was not recognized.' % FLAGS.dataset_name)
-  prev_images = tf.split(axis=0, num_or_size_splits=batch_size, value=prev_image)
+  # Treat the color channel dimension as the batch dimension since the same
-  transformed = tf.split(axis=3, num_or_size_splits=num_masks, value=transformed)
+  transformed = tf.nn.depthwise_conv2d(prev_image, cdna_kerns, [1, 1, 1, 1], 'SAME')
-              'signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY':
+              signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:
-        tensor_dict = {k: v for (k, v) in tensor_dict.iteritems()
+        tensor_dict = {k: v for (k, v) in tensor_dict.items()
-  if not all(recall[i] <= recall[i + 1] for i in xrange(len(recall) - 1)):
+  if not all(recall[i] <= recall[i + 1] for i in moves.range(len(recall) - 1)):
-      for i in xrange(data.shape[0]):
+      for i in moves.range(data.shape[0]):
-  output = six.StringIO()
+  output = six.BytesIO()
-    'The image size to use if the model does not define it.')
+    'image_size', None,
-      image_size = FLAGS.default_image_size
+    image_size = FLAGS.image_size or network_fn.default_image_size
-configuration and an optional trained checkpoint.
+configuration and an optional trained checkpoint. Outputs either an inference
-                                  FLAGS.inference_graph_path)
+                                  FLAGS.inference_graph_path,
-  logging.info('%d ops in the final graph.', len(output_graph_def.node))
+  return output_graph_def
-  tf.identity(num_detections, name='num_detections')
+  outputs = {}
-    tf.identity(masks, name='detection_masks')
+    outputs['detection_masks'] = tf.identity(masks, name='detection_masks')
-        input_saver_def=saver.as_saver_def(),
+    output_graph_def = get_frozen_graph_def(
-        initializer_nodes='')
+    )
-                            inference_graph_path):
+                            inference_graph_path,
-                         output_node_names=','.join(out_node_names))
+  outputs = _add_output_tensor_nodes(postprocessed_tensors)
-                           inference_graph_path):
+                           inference_graph_path, export_as_saved_model=False):
-                          checkpoint_path, inference_graph_path)
+                          checkpoint_path, inference_graph_path,
-import mock
+import six
-    return tf_example_decoder.TfExampleDecoder().Decode(string_tensor)
+    return tf_example_decoder.TfExampleDecoder().decode(string_tensor)
-  def Decode(self, data):
+  def decode(self, data):
-  def Decode(self, tf_example_string_tensor):
+  def decode(self, tf_example_string_tensor):
-    tensor_dict = example_decoder.Decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    tensor_dict = example_decoder.Decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    tensor_dict = example_decoder.Decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    tensor_dict = example_decoder.Decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    tensor_dict = example_decoder.Decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    tensor_dict = example_decoder.Decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    tensor_dict = example_decoder.Decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    tensor_dict = example_decoder.Decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    tensor_dict = example_decoder.Decode(tf.convert_to_tensor(example))
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-  tensor_dict = tf_example_decoder.TfExampleDecoder().Decode(
+  tensor_dict = tf_example_decoder.TfExampleDecoder().decode(
-  tensor_dict = tf_example_decoder.TfExampleDecoder().decode(
+  tensor_dict = tf_example_decoder.TfExampleDecoder().Decode(
-The inference graph contains one of two input nodes depending on the user
+The inference graph contains one of three input nodes depending on the user
-  * `num_detections` : Outputs float32 tensors of the form [batch]
+and the following output nodes returned by the model.postprocess(..):
-  * `detection_boxes`  : Outputs float32 tensors of the form
+  * `detection_boxes`: Outputs float32 tensors of the form
-  * `detection_scores` : Outputs float32 tensors of the form
+  * `detection_scores`: Outputs float32 tensors of the form
-                    'one of [`image_tensor` `tf_example_proto`]')
+                    'one of [`image_tensor`, `encoded_image_string_tensor`, '
-# version of Tensorflow becomes more common.
+# TODO: Replace with freeze_graph.freeze_graph_with_def_protos when
-    return -1
+    raise ValueError(
-    return -1
+    raise ValueError(
-    'image_tensor': _image_tensor_input_placeholder
+    * detection_masks: (Optional) float32 tensor of shape
-                         use_moving_averages)
+                         use_moving_averages,
-                            dtype=tf.float32))
+    return tf.identity(inputs)
-    return {'image': tf.identity(preprocessed_inputs)}
+    return {'image': tf.layers.conv2d(preprocessed_inputs, 3, 1)}
-      return {
+      postprocessed_tensors = {
-      mock_model.preprocess(tf.constant([1, 3, 4, 3], tf.float32))
+      mock_model = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel(add_detection_masks=True)
-          [boxes, scores, classes, num_detections],
+      (boxes, scores, classes, masks, num_detections) = sess.run(
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel(add_detection_masks=True)
-          [boxes, scores, classes, num_detections],
+      (boxes, scores, classes, masks, num_detections) = sess.run(
-The inference graph contains one of two input nodes depending on the user
+The inference graph contains one of three input nodes depending on the user
-  * `num_detections` : Outputs float32 tensors of the form [batch]
+and the following output nodes returned by the model.postprocess(..):
-  * `detection_boxes`  : Outputs float32 tensors of the form
+  * `detection_boxes`: Outputs float32 tensors of the form
-  * `detection_scores` : Outputs float32 tensors of the form
+  * `detection_scores`: Outputs float32 tensors of the form
-                    'one of [`image_tensor` `tf_example_proto`]')
+                    'one of [`image_tensor`, `encoded_image_string_tensor`, '
-# version of Tensorflow becomes more common.
+# TODO: Replace with freeze_graph.freeze_graph_with_def_protos when
-    return -1
+    raise ValueError(
-    return -1
+    raise ValueError(
-  tensor_dict = tf_example_decoder.TfExampleDecoder().Decode(
+  tensor_dict = tf_example_decoder.TfExampleDecoder().decode(
-    'image_tensor': _image_tensor_input_placeholder
+    * detection_masks: (Optional) float32 tensor of shape
-                         use_moving_averages)
+                         use_moving_averages,
-                            dtype=tf.float32))
+    return tf.identity(inputs)
-    return {'image': tf.identity(preprocessed_inputs)}
+    return {'image': tf.layers.conv2d(preprocessed_inputs, 3, 1)}
-      return {
+      postprocessed_tensors = {
-      mock_model.preprocess(tf.constant([1, 3, 4, 3], tf.float32))
+      mock_model = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel()
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel(add_detection_masks=True)
-          [boxes, scores, classes, num_detections],
+      (boxes, scores, classes, masks, num_detections) = sess.run(
-      mock_builder.return_value = FakeModel(num_classes=1)
+      mock_builder.return_value = FakeModel(add_detection_masks=True)
-          [boxes, scores, classes, num_detections],
+      (boxes, scores, classes, masks, num_detections) = sess.run(
-      masks_box_size = [im_box_size[2], im_box_size[0], im_box_size[1]]
+      masks_box_begin = [0, im_box_begin[0], im_box_begin[1]]
-        {(key + '_runtime_shapes'): tf.shape(tensor)
+        {(key + rt_shape_str): tf.shape(tensor)
-          shapes[(key[:-15], i)] = unbatched_tensor
+        if rt_shape_str in key:
-  for variable_name, variable in sorted(variable_names_map.iteritems()):
+  for variable_name, variable in sorted(variable_names_map.items()):
-    takes only the Key as inputs.  Otherwise, it uses both the key
+    takes only the message as inputs.  Otherwise, it uses both the key
-        {key: tensor.get_shape() for key, tensor in tensor_dict.iteritems()})
+        {key: tensor.get_shape() for key, tensor in tensor_dict.items()})
-         for key, tensor in tensor_dict.iteritems()})
+         for key, tensor in tensor_dict.items()})
-    for key, batched_tensor in batched_tensors.iteritems():
+    for key, batched_tensor in batched_tensors.items():
-        for key, tensor in additional_fields.iteritems():
+        for key, tensor in additional_fields.items():
-  names = tensor_dict.keys()
+  names = list(tensor_dict.keys())
-import mock
+import six
-  if depth < 0 or not isinstance(depth, (int, long)):
+  if depth < 0 or not isinstance(depth, (int, long) if six.PY2 else int):
-  if left_pad < 0 or not isinstance(left_pad, (int, long)):
+  if left_pad < 0 or not isinstance(left_pad, (int, long) if six.PY2 else int):
-    bin_crop_size.append(crop_dim / num_bins)
+    bin_crop_size.append(crop_dim // num_bins)
-    return slim.l1_regularizer(scale=regularizer.l1_regularizer.weight)
+    return slim.l1_regularizer(scale=float(regularizer.l1_regularizer.weight))
-    return slim.l2_regularizer(scale=regularizer.l2_regularizer.weight)
+    return slim.l2_regularizer(scale=float(regularizer.l2_regularizer.weight))
-    f = tf.concat_v2([visit_count, last_visit], 1)
+    f = tf.concat([visit_count, last_visit], 1)
-    pre_ego = tf.concat_v2([locs, tf.sin(thetas), tf.cos(thetas)], 2)
+    pre_ego = tf.concat([locs, tf.sin(thetas), tf.cos(thetas)], 2)
-        res = tf.concat_v2([res, mask], 3)
+        res = tf.concat([res, -res], 3)
-        res = tf.concat_v2([res, mask], 3)
+        res = tf.concat([res, -res], 3)
-        res = tf.concat_v2([res, -res], 3)
+        res = tf.concat([res, -res], 3)
-            log_diff = tf.concat_v2([tf.zeros_like(log_diff), log_diff], 3)
+            res = tf.concat([input_, res], 3)
-            log_diff = tf.concat_v2([log_diff, tf.zeros_like(log_diff)], 3)
+            res = tf.concat([res, input_], 3)
-        res = tf.concat_v2([res, -res], 3)
+        res = tf.concat([res, -res], 3)
-            log_diff = tf.concat_v2([tf.zeros_like(log_diff), log_diff], 3)
+            res = tf.concat([input_, res], 3)
-            log_diff = tf.concat_v2([log_diff, tf.zeros_like(log_diff)], 3)
+            res = tf.concat([res, input_], 3)
-            res = tf.concat_v2([res_1, res_2], 3)
+            res = tf.concat([res_1, res_2], 3)
-            log_diff = tf.concat_v2([log_diff_1, log_diff_2], 3)
+            log_diff = tf.concat([log_diff_1, log_diff_2], 3)
-            log_diff = tf.concat_v2([log_diff_1, log_diff_2], 3)
+            res = tf.concat([res_1, res_2], 3)
-                image = tf.reshape(image, tf.concat_v2([height, width, depth], 0))
+                image = tf.reshape(image, tf.concat([height, width, depth], 0))
-                    z_compressed = tf.concat_v2(
+                    z_compressed = tf.concat(
-                    z_noisy = tf.concat_v2(
+                    z_noisy = tf.concat(
-            z_noisy = tf.concat_v2(z_noisy_list, 0)
+            z_compressed = tf.concat(z_compressed_list, 0)
-  with tf.gfile.GFile(full_path) as fid:
+  with tf.gfile.GFile(full_path, 'rb') as fid:
-    classes_text.append(obj['name'])
+    classes_text.append(obj['name'].encode('utf8'))
-    poses.append(obj['pose'])
+    poses.append(obj['pose'].encode('utf8'))
-      'image/key/sha256': dataset_util.bytes_feature(key),
+      'image/filename': dataset_util.bytes_feature(
-      'image/format': dataset_util.bytes_feature('jpeg'),
+      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),
-  with tf.gfile.GFile(img_path) as fid:
+  with tf.gfile.GFile(img_path, 'rb') as fid:
-    classes_text.append(class_name)
+    classes_text.append(class_name.encode('utf8'))
-    poses.append(obj['pose'])
+    poses.append(obj['pose'].encode('utf8'))
-      'image/key/sha256': dataset_util.bytes_feature(key),
+      'image/filename': dataset_util.bytes_feature(
-      'image/format': dataset_util.bytes_feature('jpeg'),
+      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),
-          logits = net
+          if spatial_squeeze:
-        return logits, end_points
+          end_points['predictions'] = slim.softmax(net, scope='predictions')
-          logits = net
+          if spatial_squeeze:
-        return logits, end_points
+          end_points['predictions'] = slim.softmax(net, scope='predictions')
-        {(key, 'runtime_shapes'): tf.shape(tensor)
+        {(key + '_runtime_shapes'): tf.shape(tensor)
-          shapes[(key[0], i)] = unbatched_tensor
+        if '_runtime_shapes' in key:
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.iteritems():
+    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.iteritems():
+    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
-                  'Convert training set, validation set or merged set.')
+flags.DEFINE_string('set', 'train', 'Convert training set, validation set or '
-                  'Desired challenge year.')
+flags.DEFINE_string('year', 'VOC2007', 'Desired challenge year.')
-                  'Convert training set, validation set or merged set.')
+flags.DEFINE_string('set', 'train', 'Convert training set, validation set or '
-                  'Desired challenge year.')
+flags.DEFINE_string('year', 'VOC2007', 'Desired challenge year.')
-  font = ImageFont.load_default()
+  try:
-  for box, color in box_to_color_map.iteritems():
+  for box, color in six.iteritems(box_to_color_map):
-                                              depth_radius=5,
+                                              depth_radius=2,
-                                              depth_radius=5,
+                                              depth_radius=2,
-            rate *= unit_stride
+            net = block.unit_fn(net, rate=rate, **dict(unit, stride=1))
-            current_stride *= unit_stride
+            net = block.unit_fn(net, rate=1, **unit)
-              spatial_squeeze=True,
+              spatial_squeeze=False,
-        end_points = slim.utils.convert_collection_to_dict(end_points_collection)
+        end_points = slim.utils.convert_collection_to_dict(
-          'block4', bottleneck, [(2048, 512, 1)] * 3)
+      resnet_v1_block('block1', base_depth=64, num_units=3, stride=2),
-          'block4', bottleneck, [(2048, 512, 1)] * 3)
+      resnet_v1_block('block1', base_depth=64, num_units=3, stride=2),
-          'block4', bottleneck, [(2048, 512, 1)] * 3)]
+      resnet_v1_block('block1', base_depth=64, num_units=3, stride=2),
-          'block4', bottleneck, [(2048, 512, 1)] * 3)]
+      resnet_v1_block('block1', base_depth=64, num_units=3, stride=2),
-        end_points = dict(tf.get_collection('end_points'))
+        end_points = slim.utils.convert_collection_to_dict('end_points')
-              resnet_utils.Block('block2', bottleneck, [(8, 2, 1), (8, 2, 1)])]
+    blocks = [
-                                rate=1)
+            net = block.unit_fn(net, rate=1, **unit)
-  def _atrousValues(self, bottleneck):
+  def testAtrousValuesBottleneck(self):
-      bottleneck: The bottleneck function.
+    block = resnet_v1.resnet_v1_block
-        resnet_utils.Block('block4', bottleneck, [(32, 8, 1), (32, 8, 1)])
+        block('block1', base_depth=1, num_units=2, stride=2),
-    bottleneck = resnet_v1.bottleneck
+    block = resnet_v1.resnet_v1_block
-            'block4', bottleneck, [(32, 8, 1)] * 2)]
+        block('block1', base_depth=1, num_units=3, stride=2),
-              spatial_squeeze=True,
+              spatial_squeeze=False,
-        end_points = slim.utils.convert_collection_to_dict(end_points_collection)
+        end_points = slim.utils.convert_collection_to_dict(
-                 spatial_squeeze=True,
+                 spatial_squeeze=False,
-          'block4', bottleneck, [(2048, 512, 1)] * 3)]
+      resnet_v2_block('block1', base_depth=64, num_units=3, stride=2),
-                  spatial_squeeze=True,
+                  spatial_squeeze=False,
-          'block4', bottleneck, [(2048, 512, 1)] * 3)]
+      resnet_v2_block('block1', base_depth=64, num_units=3, stride=2),
-                  spatial_squeeze=True,
+                  spatial_squeeze=False,
-          'block4', bottleneck, [(2048, 512, 1)] * 3)]
+      resnet_v2_block('block1', base_depth=64, num_units=3, stride=2),
-                  spatial_squeeze=True,
+                  spatial_squeeze=False,
-          'block4', bottleneck, [(2048, 512, 1)] * 3)]
+      resnet_v2_block('block1', base_depth=64, num_units=3, stride=2),
-        end_points = dict(tf.get_collection('end_points'))
+        end_points = slim.utils.convert_collection_to_dict('end_points')
-              resnet_utils.Block('block2', bottleneck, [(8, 2, 1), (8, 2, 1)])]
+    blocks = [
-                                rate=1)
+            net = block.unit_fn(net, rate=1, **unit)
-  def _atrousValues(self, bottleneck):
+  def testAtrousValuesBottleneck(self):
-      bottleneck: The bottleneck function.
+    block = resnet_v2.resnet_v2_block
-        resnet_utils.Block('block4', bottleneck, [(32, 8, 1), (32, 8, 1)])
+        block('block1', base_depth=1, num_units=2, stride=2),
-    bottleneck = resnet_v2.bottleneck
+    block = resnet_v2.resnet_v2_block
-            'block4', bottleneck, [(32, 8, 1)] * 2)]
+        block('block1', base_depth=1, num_units=3, stride=2),
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+# Dependency imports
-flags.DEFINE_float('perturb_norm_length', 0.1,
+flags.DEFINE_float('perturb_norm_length', 5.0,
-                   'optimized with validation')
+                   'optimized with validation. '
-flags.DEFINE_float('small_constant_for_finite_diff', 1e-3,
+flags.DEFINE_float('small_constant_for_finite_diff', 1e-1,
-  weights = _end_of_seq_mask(inputs.labels)
+  weights = inputs.eos_weights
-  d = _mask_by_length(tf.random_normal(shape=tf.shape(embedded)), inputs.length)
+  d = tf.random_normal(shape=tf.shape(embedded))
-    d = _scale_l2(d, FLAGS.small_constant_for_finite_diff)
+    d = _scale_l2(
-      _mask_by_length(d, inputs.length), FLAGS.perturb_norm_length)
+  perturb = _scale_l2(d, FLAGS.perturb_norm_length)
-  weights = _end_of_seq_mask(f_inputs.labels)
+  weights = f_inputs.eos_weights
-  ]
+  perturbs = [_scale_l2(d, FLAGS.perturb_norm_length) for d in perturbs]
-  mask = tf.sequence_mask(length, maxlen=maxlen)
+
-                                          keep_dims=True) + 1e-6)
+  l2_norm = alpha * tf.sqrt(
-    p = tf.nn.sigmoid(p_logits)
+    kl = tf.squeeze(kl)
-    kl = tf.reduce_sum(q * (tf.log(q) - tf.log(p)), 1)
+    kl = tf.reduce_sum(
-  kl.get_shape().assert_has_rank(2)
+  kl.get_shape().assert_has_rank(1)
-                     num_labels, name='kl')
+  loss = tf.identity(tf.reduce_sum(weights * kl) / num_labels, name='kl')
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+
-    labels (offset by 1, i.e. predict next token) with weights set to 1.0.
+    labels (offset by 1, i.e. predict next token) with weights set to 1.0,
-
+  for i, timestep in enumerate(seq):
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+# Dependency imports
-      self.assertEqual(ts.weight, 1.0)
+      # For end of sequence, the token and label should be same, and weight
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+# Dependency imports
-                       label=None, add_tokens=False)
+        yield Document(
-  Dataset described at http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm
+  Dataset described at
-                           is_test=False, label=None, add_tokens=False)
+            yield Document(
-              is_validation and not include_validation):
+          if (is_test and dataset != 'test') or (is_validation and
-                         is_test=is_test, label=class_label, add_tokens=True)
+          yield Document(
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+# Dependency imports
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+# Dependency imports
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+# Dependency imports
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+
-flags.DEFINE_float('keep_prob_emb', 1.0, 'keep probability on embedding layer')
+flags.DEFINE_float('keep_prob_emb', 1.0, 'keep probability on embedding layer. '
-                inputs.weights)
+                layers_lib.predictions(logits), inputs.labels, inputs.weights)
-      unroll_steps=FLAGS.num_timesteps)
+      unroll_steps=FLAGS.num_timesteps,
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+# Dependency imports
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+
-  def __init__(self, batch, state_name=None, tokens=None, num_states=0):
+  def __init__(self,
-           unroll_steps=100):
+           unroll_steps=100,
-
+    eos_id: int, id of end of sequence. used for the kl weights on vat
-          forward_batch, state_name=state_name, num_states=num_layers)
+          forward_batch,
-          reverse_batch, state_name=state_name_rev, num_states=num_layers)
+          reverse_batch,
-      return VatxtInput(batch, state_name=state_name, num_states=num_layers)
+      return VatxtInput(
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-import tensorflow as tf
+# Dependency imports
-      subgraph.add(K.layers.Dropout(keep_prob))
+      subgraph.add(K.layers.Dropout(1. - keep_prob))
-      embedded = tf.nn.dropout(embedded, self.keep_prob)
+      shape = embedded.get_shape().as_list()
-          initializer='glorot_uniform')
+          initializer=K.initializers.glorot_uniform())
-          initializer='glorot_uniform')
+          initializer=K.initializers.glorot_uniform())
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-  SyncReplicasOptimizer, that is the total minibatch is 256.
+  2 days to train 100000 steps on 1 layer 1024 hidden units LSTM,
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-  6 hours to train 10000 steps without adversarial or virtual adversarial
+  1.8 hours to train 10000 steps without adversarial or virtual adversarial
-    BP, 64 minibatch and on single GPU.
+    BP, 64 minibatch and on single GPU (Pascal Titan X, cuDNNv5).
-  12 hours to train 10000 steps with adversarial or virtual adversarial
+  4 hours to train 10000 steps with adversarial or virtual adversarial
-# Copyright 2017 Google, Inc. All Rights Reserved.
+# Copyright 2017 Google Inc. All Rights Reserved.
-
+# Dependency imports
-
+  # lrn1
-
+  # lrn2
-    )
+    lrn1 = tf.nn.local_response_normalization(conv1,
-    )
+    lrn2 = tf.nn.local_response_normalization(conv2,
-        device += '/device:GPU:%d' % clone_index
+      device += '/device:GPU:%d' % clone_index
-    self.assertDeviceEqual(deploy_config.clone_device(0), '')
+    self.assertDeviceEqual(deploy_config.clone_device(0), 'GPU:0')
-                           '/job:worker')
+                           '/job:worker/device:GPU:0')
-                           '/job:worker')
+                           '/job:worker/device:GPU:0')
-      self.assertDeviceEqual(clone.device, '')
+      self.assertDeviceEqual(clone.device, 'GPU:0')
-      self.assertDeviceEqual(clone.device, '')
+      self.assertDeviceEqual(clone.device, 'GPU:0')
-      self.assertDeviceEqual(clone.device, '/job:worker')
+      self.assertDeviceEqual(clone.device, '/job:worker/device:GPU:0')
-        self.assertDeviceEqual(g.device, '')
+        self.assertDeviceEqual(g.device, 'GPU:0')
-        self.assertDeviceEqual(g.device, '')
+        self.assertDeviceEqual(g.device, 'GPU:0')
-        self.assertDeviceEqual(g.device, '/job:worker')
+        self.assertDeviceEqual(g.device, '/job:worker/device:GPU:0')
-        self.assertLess(final_loss, initial_loss / 10.0)
+        self.assertLess(final_loss, initial_loss / 5.0)
-      images, labels = batch_queue.dequeue()
+      with tf.device(deploy_config.inputs_device()):
-  with tf.gfile.Open(labels_filename, 'r') as f:
+  with tf.gfile.Open(labels_filename, 'rb') as f:
-import cPickle
+from six.moves import cPickle
-    data = cPickle.load(f)
+  with tf.gfile.Open(filename, 'rb') as f:
-  images = data['data']
+  images = data[b'data']
-  labels = data['labels']
+  labels = data[b'labels']
-            png_string, 'png', _IMAGE_SIZE, _IMAGE_SIZE, label)
+            png_string, b'png', _IMAGE_SIZE, _IMAGE_SIZE, label)
-            image_data = tf.gfile.FastGFile(filenames[i], 'r').read()
+            image_data = tf.gfile.FastGFile(filenames[i], 'rb').read()
-                image_data, 'jpg', height, width, class_id)
+                image_data, b'jpg', height, width, class_id)
-                        scope='InceptionResnetV2'):
+                        scope='InceptionResnetV2',
-  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs], reuse=reuse):
+  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs, num_classes],
-        # Auxiliary tower
+      net, end_points = inception_resnet_v2_base(inputs, scope=scope)
-          aux = slim.avg_pool2d(net, 5, stride=3, padding='VALID',
+          aux = end_points['PreAuxLogits']
-          end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')
+      with tf.variable_scope('Logits'):
-      logits, _ = inception.inception_resnet_v2(inputs, num_classes)
+      logits, endpoints = inception.inception_resnet_v2(inputs, num_classes)
-      pre_pool = end_points['PrePool']
+      pre_pool = end_points['Conv2d_7b_1x1']
-      pre_pool = end_points['PrePool']
+      pre_pool = end_points['Conv2d_7b_1x1']
-    arg_scope = arg_scopes_map[name](weight_decay=weight_decay)
+      'mobilenet_v1': inception_preprocessing,
-      'resnet_v2_152': vgg_preprocessing,
+import sys
-    return f.read().decode("utf-8").replace("\n", "<eos>").split()
+    if sys.version_info[0] >= 3:
-      print('Failed to find: ' % original_filename)
+      print('Failed to find: %s' % original_filename)
-            images, labels = batch_queue.dequeue()
+            image_batch, label_batch = batch_queue.dequeue()
-            loss = tower_loss(scope, images, labels)
+            loss = tower_loss(scope, image_batch, label_batch)
-    labels: Labels. 1D tensor of [batch_size] size.
+    images: Images. 4D tensor of shape [batch_size, height, width, 3].
-  print(FLAGS.batch_size)
+  print(FLAGS.batch_size)
-
+    batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(
-    with tf.device('/CPU:0'):
+    with tf.device('/cpu:0'):
-      images, labels = cifar10.distorted_inputs()
+    images, labels = cifar10.distorted_inputs()
-    # on GPU and resulting in a slow down.
+    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on
-def tower_loss(scope):
+def tower_loss(scope, images, labels):
-  images, labels = cifar10.distorted_inputs()
+
-            loss = tower_loss(scope)
+            loss = tower_loss(scope, images, labels)
-    images, labels = cifar10.distorted_inputs()
+    # Force input pipeline to CPU:0 to avoid opertaios sometimes ending up
-  # TODO(shlens, jiayq): Add a GPU version of local response normalization.
+
-  pool1 = tf.nn.max_pool(conv1,
+  pool1 = tf.nn.max_pool(lrn1,
-  pool2 = tf.nn.max_pool(conv2,
+  pool2 = tf.nn.max_pool(lrn2,
-      combined_message = tf.concat([message, key], 1)
+      combined_message = tf.concat(axis=1, values=[message, key])
-        [tf.contrib.layers.fully_connected, tf.contrib.layers.convolution2d],
+        [tf.contrib.layers.fully_connected, tf.contrib.layers.conv2d],
-      conv = tf.contrib.layers.convolution2d(
+      conv = tf.contrib.layers.conv2d(
-      conv = tf.contrib.layers.convolution2d(
+      conv = tf.contrib.layers.conv2d(
-      conv = tf.contrib.layers.convolution2d(
+      conv = tf.contrib.layers.conv2d(
-      combined_message = tf.concat(1, [message, key])
+      combined_message = tf.concat([message, key], 1)
-        [tf.contrib.layers.fully_connected, tf.contrib.layers.convolution],
+        [tf.contrib.layers.fully_connected, tf.contrib.layers.convolution2d],
-      conv = tf.contrib.layers.convolution(
+      conv = tf.contrib.layers.convolution2d(
-      conv = tf.contrib.layers.convolution(
+      conv = tf.contrib.layers.convolution2d(
-      conv = tf.contrib.layers.convolution(
+      conv = tf.contrib.layers.convolution2d(
-        if node_def.op == 'Variable':
+        if node_def.op.startswith('Variable'):
-      # elipson used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.
+      # epsilon used to be 1e-5. Maybe 0.001 solves NaN problem in deeper net.
-      tf.contrib.deprecated.scalar_summary('Learning Rate', learning_rate)
+      tf.summary.scalar('Learning Rate', learning_rate)
-                                           code_length[-1])
+      tf.summary.scalar('code_length_layer_{:02d}'.format(k), code_length[-1])
-    tf.contrib.deprecated.scalar_summary('loss', self.loss)
+    tf.summary.scalar('loss', self.loss)
-"""A binary to evaluate Inception on the flowers data set.
+"""A binary to evaluate Inception on the ImageNet data set.
-      self.assertEquals(output.op.name, 'Dropout/dropout/mul_1')
+      self.assertEquals(output.op.name, 'Dropout/dropout/mul')
-          output = tf.identity(output)
+        output = tf.identity(output)
-          output = tf.identity(output)
+        output = tf.identity(output)
-          output = tf.identity(output)
+        output = tf.identity(output)
-          train_op = total_loss
+          train_op = tf.identity(total_loss, name='train_op')
-      train_tensor = total_loss
+      train_tensor = tf.identity(total_loss, name='train_op')
-  # TODO(damienv): Possibly use multiple files (instead of just one).
+  # TODO: Possibly use multiple files (instead of just one).
-    # TODO(damienv):
+    # TODO:
-          output = output
+          output = tf.identity(output)
-          output = output
+          output = tf.identity(output)
-          output = output
+          output = tf.identity(output)
-        output = control_flow_ops.with_dependencies([barrier], output)
+        with tf.control_dependencies([barrier]):
-        output = control_flow_ops.with_dependencies([barrier], output)
+        with tf.control_dependencies([barrier]):
-        output = control_flow_ops.with_dependencies([barrier], output)
+        with tf.control_dependencies([barrier]):
-                                                      name='train_op')
+        with tf.control_dependencies([update_op]):
-      tf.stack([crop_height, crop_width, original_shape[2]]))
+  with tf.control_dependencies([rank_assertion]):
-      tf.slice(image, offsets, cropped_shape))
+  with tf.control_dependencies([size_assertion]):
-      tf.shape(image_list[0]))
+  with tf.control_dependencies([rank_assertions[0]]):
-                                               tf.shape(image))
+    with tf.control_dependencies([rank_assertions[i]]):
-      asserts, tf.reshape(image_width - crop_width + 1, []))
+  with tf.control_dependencies(asserts):
-                                                      name='train_op')
+    with tf.control_dependencies([update_op]):
-  If central_fraction is specified it would cropt the central fraction of the
+  If central_fraction is specified it would crop the central fraction of the
-          cell = tf.contrib.rnn.MultiRNNCell([lstm_cell] * nconvs)
+          def lstm_cell():
-"""A library to train Inception using multiple GPU's with synchronous updates.
+"""A library to train Inception using multiple GPUs with synchronous updates.
-  multiple GPU's. For instance, if the batch size = 32 and num_gpus = 2,
+  multiple GPUs. For instance, if the batch size = 32 and num_gpus = 2,
-"""A binary to train CIFAR-10 using multiple GPU's with synchronous updates.
+"""A binary to train CIFAR-10 using multiple GPUs with synchronous updates.
-import six
+from six.moves import xrange
-  for _ in six.moves.range(FLAGS.num_samples):
+  for _ in xrange(FLAGS.num_samples):
-  for i in six.moves.range(vocab.size):
+  for i in xrange(vocab.size):
-  for i in six.moves.range(len(word_ids)):
+  for i in xrange(len(word_ids)):
-the main pathway. Also see [2; Fig. 4e].
+import six
-      (article, abstract) = input_gen.next()
+      (article, abstract) = six.next(input_gen)
-      e = example_gen.next()
+      e = six.next(example_gen)
-    self._cost = cost = tf.reduce_sum(loss) / batch_size
+
-  for _ in range(FLAGS.num_samples):
+  for _ in six.moves.range(FLAGS.num_samples):
-  for i in range(vocab.size):
+  for i in six.moves.range(vocab.size):
-  for i in range(len(word_ids)):
+  for i in six.moves.range(len(word_ids)):
-                   include_root_block=True, reuse=reuse, scope=scope)
+                   include_root_block=True, spatial_squeeze=spatial_squeeze,
-                   include_root_block=True, reuse=reuse, scope=scope)
+                   include_root_block=True, spatial_squeeze=spatial_squeeze,
-                   include_root_block=True, reuse=reuse, scope=scope)
+                   include_root_block=True, spatial_squeeze=spatial_squeeze,
-                   include_root_block=True, reuse=reuse, scope=scope)
+                   include_root_block=True, spatial_squeeze=spatial_squeeze,
-                   include_root_block=True, reuse=reuse, scope=scope)
+                   include_root_block=True, spatial_squeeze=spatial_squeeze,
-                   include_root_block=True, reuse=reuse, scope=scope)
+                   include_root_block=True, spatial_squeeze=spatial_squeeze,
-                   include_root_block=True, reuse=reuse, scope=scope)
+                   include_root_block=True, spatial_squeeze=spatial_squeeze,
-                   include_root_block=True, reuse=reuse, scope=scope)
+                   include_root_block=True, spatial_squeeze=spatial_squeeze,
-#
+Swivel can be run "stand-alone" or "distributed".  The latter involves running
-import math
+import itertools
-import threading
+import random
-                     'Number of GPUs to use, 0 means all available')
+flags.DEFINE_string(
-    tf.logging.info(message, *args, **kwargs)
+class Model(object):
-            if d.device_type == 'GPU']
+def main(_):
-      self.saver = tf.train.Saver(sharded=True)
+    server = tf.train.Server(
-  start_time = time.time()
+    device_setter = tf.train.replica_device_setter(
-    os.makedirs(FLAGS.output_base_path)
+  else:
-  # Create and run model
+  # Build the graph.
-    log("Elapsed: %s", time.time() - start_time)
+    with tf.device(device_setter):
-  return norm_length * x
+  # Divide x by max(abs(x)) for a numerically stable L2 norm.
-  loss = tf.identity(tf.reduce_sum(weights * kl) / num_labels, name='kl')
+  kl.get_shape().assert_has_rank(2)
-    metric_values = dict(zip(metric_names, values))
+    _log_values(sess, value_ops_dict, summary_writer=summary_writer)
-      tf.logging.info('%s = %.3f', name, val)
+def _log_values(sess, value_ops, summary_writer=None):
-    return emb
+    mean = tf.reduce_sum(weights * emb, 0, keep_dims=True)
-    weights: 2-D [timesteps*batch_size] float tensor.
+    weights: 1-D [timesteps*batch_size] float tensor.
-      "--no-preserve=all", os.path.join(base_dir, "dragnn"), os.path.join(
+      "cp", "-r", os.path.join(base_dir, "dragnn"), os.path.join(
-        ["cp", "-r", "--no-preserve=all", tensorflow_dir, tmp_packaging])
+        ["cp", "-r", tensorflow_dir, tmp_packaging])
-    output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])
+    output = tf.reshape(tf.stack(axis=1, values=outputs), [-1, size])
-      s = f.read()
+      s = f.read().decode()
-  for _ in xrange(FLAGS.num_samples):
+  for _ in range(FLAGS.num_samples):
-  for i in xrange(len(word_ids)):
+  for i in range(len(word_ids)):
-      # Update hps.num_residual_units to 9
+      # Update hps.num_residual_units to 4
-      task_index=FLAGS.task_id)
+      task_index=FLAGS.task_id,
-
+# Copyright 2017 Google, Inc. All Rights Reserved.
-      for idx in xrange(4):
+      for idx in range(4):
-      for idx in xrange(7):
+      for idx in range(7):
-      for idx in xrange(3):
+      for idx in range(3):
-        momentum=FLAGS.rmsprop_momentum,
+        momentum=FLAGS.momentum,
-    for name, value in names_to_values.iteritems():
+    for name, value in names_to_values.items():
-  
+
-    ["https://github.com/npapernot/multiple-teachers-for-privacy/blob/master/mnist_250_teachers_labels.npy?raw=true", 
+    ["https://github.com/npapernot/multiple-teachers-for-privacy/blob/master/mnist_250_teachers_labels.npy?raw=true",
-  
+
-    kernel = _variable_with_weight_decay('weights', 
+    kernel = _variable_with_weight_decay('weights',
-                                         stddev=1e-4, 
+                                         stddev=1e-4,
-                         ksize=[1, 3, 3, 1], 
+  pool1 = tf.nn.max_pool(conv1,
-                         padding='SAME', 
+                         padding='SAME',
-  
+
-                    alpha=0.001 / 9.0, 
+  norm1 = tf.nn.lrn(pool1,
-    kernel = _variable_with_weight_decay('weights', 
+    kernel = _variable_with_weight_decay('weights',
-                                         stddev=1e-4, 
+                                         stddev=1e-4,
-                    alpha=0.001 / 9.0, 
+  norm2 = tf.nn.lrn(conv2,
-  
+
-  pool2 = tf.nn.max_pool(norm2, 
+  pool2 = tf.nn.max_pool(norm2,
-                         padding='SAME', 
+                         strides=[1, 2, 2, 1],
-    weights = _variable_with_weight_decay('weights', 
+    weights = _variable_with_weight_decay('weights',
-                                          stddev=0.04, 
+                                          stddev=0.04,
-    weights = _variable_with_weight_decay('weights', 
+    weights = _variable_with_weight_decay('weights',
-                                          stddev=0.04, 
+                                          stddev=0.04,
-    weights = _variable_with_weight_decay('weights', 
+    weights = _variable_with_weight_decay('weights',
-                                          stddev=1/192.0, 
+                                          stddev=1/192.0,
-    biases = _variable_on_cpu('biases', 
+    biases = _variable_on_cpu('biases',
-  
+
-  :param directory: the directory where to download 
+  :param directory: the directory where to download
-    if not gfile.Exists(filepath):
+    if not tf.gfile.Exists(filepath):
-  with gfile.Open(local_url, mode='r') as file_obj:
+  with tf.gfile.Open(local_url, mode='r') as file_obj:
-    
+
-                                         nb_teachers, 
+  data, labels = input.partition_dataset(train_data,
-from . import seq2seq_model
+import data_utils
-from . import data_utils
+import data_utils
-  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)
+    arg_scope = arg_scopes_map[name](weight_decay=weight_decay)
-
+slim = tf.contrib.slim
-          scope='vgg_a'):
+          scope='vgg_a',
-      net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')
+      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')
-           scope='vgg_16'):
+           scope='vgg_16',
-      net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')
+      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')
-           scope='vgg_19'):
+           scope='vgg_19',
-      net = slim.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')
+      net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')
-
+      [inferred_batch_size] + state_shape, dtype=dtype)
-               reuse_scope=None):
+               reuse_scope=None,
-    self.prefix = prefix = tf.placeholder(tf.string, [])
+    if prefix is None:
-    actions = tf.split(axis=1, num_or_size_splits=actions.get_shape()[1], value=actions)
+    actions = tf.split(axis=1, num_or_size_splits=int(actions.get_shape()[1]), value=actions)
-    states = tf.split(axis=1, num_or_size_splits=states.get_shape()[1], value=states)
+    states = tf.split(axis=1, num_or_size_splits=int(states.get_shape()[1]), value=states)
-    images = tf.split(axis=1, num_or_size_splits=images.get_shape()[1], value=images)
+    images = tf.split(axis=1, num_or_size_splits=int(images.get_shape()[1]), value=images)
-  print 'Constructing models and inputs.'
+  print('Constructing models and inputs.')
-    model = Model(images, actions, states, FLAGS.sequence_length)
+    model = Model(images, actions, states, FLAGS.sequence_length,
-                      FLAGS.sequence_length, training_scope)
+                      FLAGS.sequence_length, training_scope, prefix='val')
-  print 'Constructing saver.'
+  print('Constructing saver.')
-                 model.iter_num: np.float32(itr),
+    feed_dict = {model.iter_num: np.float32(itr),
-  with tf.gfile.FastGFile(filename, 'r') as f:
+  with tf.gfile.FastGFile(filename, 'rb') as f:
-  with tf.gfile.FastGFile(filename, 'rb') as f:
+  with tf.gfile.FastGFile(filename, 'r') as f:
-
+      [inferred_batch_size] + state_shape, dtype=dtype)
-               reuse_scope=None):
+               reuse_scope=None,
-    self.prefix = prefix = tf.placeholder(tf.string, [])
+    if prefix is None:
-    actions = tf.split(axis=1, num_or_size_splits=actions.get_shape()[1], value=actions)
+    actions = tf.split(axis=1, num_or_size_splits=int(actions.get_shape()[1]), value=actions)
-    states = tf.split(axis=1, num_or_size_splits=states.get_shape()[1], value=states)
+    states = tf.split(axis=1, num_or_size_splits=int(states.get_shape()[1]), value=states)
-    images = tf.split(axis=1, num_or_size_splits=images.get_shape()[1], value=images)
+    images = tf.split(axis=1, num_or_size_splits=int(images.get_shape()[1]), value=images)
-  print 'Constructing models and inputs.'
+  print('Constructing models and inputs.')
-    model = Model(images, actions, states, FLAGS.sequence_length)
+    model = Model(images, actions, states, FLAGS.sequence_length,
-                      FLAGS.sequence_length, training_scope)
+                      FLAGS.sequence_length, training_scope, prefix='val')
-  print 'Constructing saver.'
+  print('Constructing saver.')
-                 model.iter_num: np.float32(itr),
+    feed_dict = {model.iter_num: np.float32(itr),
-import seq2seq_model
+from . import data_utils
-import data_utils
+from . import data_utils
-        target_semi_images, target_semi_labels = data_provider.provide(
+        target_semi_images, target_semi_labels = provide_batch_fn()(
-"""
+"""Evaluation for Domain Separation Networks (DSNs)."""
-# pylint: enable=line-too-long
+"""Training for Domain Separation Networks (DSNs)."""
-tf.app.flags.DEFINE_string('dataset_dir', '/cns/ok-d/home/konstantinos/cad_learning/',
+tf.app.flags.DEFINE_string('dataset_dir', None,
-    tag_accuracy = 'losses/Domain Accuracy'
+    tag_loss = 'losses/domain_loss'
-        tag_accuracy, domain_accuracy, name='domain_accuracy_summary')
+    tf.summary.scalar(tag_loss, domain_loss)
-      map(normalize_images, [
+      axis=2,
-      ]), 2)
+      ]))
-      map(normalize_images, [
+      axis=2,
-      ]), 2)
+      ]))
-      sess.run(tf.initialize_all_variables())  # global_step = 0
+      sess.run(tf.global_variables_initializer())  # global_step = 0
-    tf.contrib.deprecated.scalar_summary(tag, loss_value)
+    tf.summary.scalar(tag, loss_value)
-    tf.contrib.deprecated.scalar_summary(tag, corr_loss)
+    tf.summary.scalar(tag, corr_loss)
-    samples = tf.concat([source_samples, target_samples], 0)
+    samples = tf.concat(axis=0, values=[source_samples, target_samples])
-        [tf.zeros((batch_size, 1)), tf.ones((batch_size, 1))], 0)
+        axis=0, values=[tf.zeros((batch_size, 1)), tf.ones((batch_size, 1))])
-    tf.contrib.deprecated.scalar_summary(
+    tf.summary.scalar(
-    tf.contrib.deprecated.scalar_summary(
+    tf.summary.scalar(
-  tf.contrib.deprecated.scalar_summary('losses/Difference Loss {}'.format(name),
+  tf.summary.scalar('losses/Difference Loss {}'.format(name),
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-  conv_one_row = tf.concat(conv_summary[0:num_filters_sqrt], 2)
+  conv_one_row = tf.concat(axis=2, values=conv_summary[0:num_filters_sqrt])
-                             2)
+    conv_one_row = tf.concat(axis=2,
-        [tf.squeeze(conv_final), tf.squeeze(conv_one_row)], 1)
+        axis=1, values=[tf.squeeze(conv_final), tf.squeeze(conv_one_row)])
-    'Comma-seperated list of layer names to use MMD regularization on.')
+    'Comma-separated list of layer names to use MMD regularization on.')
-  tf.app.run()
+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
-  with tf.gfile.FastGFile(filename, 'r') as f:
+  with tf.gfile.FastGFile(filename, 'rb') as f:
-      map(normalize_images, [
+      axis=2,
-      ]), 2)
+      ]))
-      map(normalize_images, [
+      axis=2,
-      ]), 2)
+      ]))
-      sess.run(tf.initialize_all_variables())  # global_step = 0
+      sess.run(tf.global_variables_initializer())  # global_step = 0
-    tf.contrib.deprecated.scalar_summary(tag, loss_value)
+    tf.summary.scalar(tag, loss_value)
-    tf.contrib.deprecated.scalar_summary(tag, corr_loss)
+    tf.summary.scalar(tag, corr_loss)
-    samples = tf.concat([source_samples, target_samples], 0)
+    samples = tf.concat(axis=0, values=[source_samples, target_samples])
-        [tf.zeros((batch_size, 1)), tf.ones((batch_size, 1))], 0)
+        axis=0, values=[tf.zeros((batch_size, 1)), tf.ones((batch_size, 1))])
-    tf.contrib.deprecated.scalar_summary(
+    tf.summary.scalar(
-    tf.contrib.deprecated.scalar_summary(
+    tf.summary.scalar(
-  tf.contrib.deprecated.scalar_summary('losses/Difference Loss {}'.format(name),
+  tf.summary.scalar('losses/Difference Loss {}'.format(name),
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-  conv_one_row = tf.concat(conv_summary[0:num_filters_sqrt], 2)
+  conv_one_row = tf.concat(axis=2, values=conv_summary[0:num_filters_sqrt])
-                             2)
+    conv_one_row = tf.concat(axis=2,
-        [tf.squeeze(conv_final), tf.squeeze(conv_one_row)], 1)
+        axis=1, values=[tf.squeeze(conv_final), tf.squeeze(conv_one_row)])
-    'Comma-seperated list of layer names to use MMD regularization on.')
+    'Comma-separated list of layer names to use MMD regularization on.')
-  tf.app.run()
+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
-    #                            initial_state=self._initial_state)
+    # outputs, state = tf.contrib.rnn.static_rnn(
-    # outputs, state = tf.nn.rnn(cell, inputs,
+    # outputs, state = tf.contrib.rnn.static_rnn(cell, inputs,
-      try:
+      # defined in TensorFlow 1.0. To maintain backwards compatibility, we add
-      except TypeError:
+      else:
-          size, forget_bias=0.0, state_is_tuple=True)
+      # With the latest TensorFlow source code (as of Mar 27, 2017),
-  train_directory/train-00127-of-01024
+  train_directory/train-01023-of-01024
-  image/format: string, specifying the format, always'JPEG'
+  image/format: string, specifying the format, always 'JPEG'
-If you data set involves bounding boxes, please look at build_imagenet_data.py.
+If your data set involves bounding boxes, please look at build_imagenet_data.py.
-    ranges.append([spacing[i], spacing[i+1]])
+    ranges.append([spacing[i], spacing[i + 1]])
-  train_directory/train-00127-of-01024
+  train_directory/train-01023-of-01024
-  image/format: string, specifying the format, always'JPEG'
+  image/format: string, specifying the format, always 'JPEG'
-Running this script using 16 threads may take around ~2.5 hours on a HP Z420.
+Running this script using 16 threads may take around ~2.5 hours on an HP Z420.
-    ranges.append([spacing[i], spacing[i+1]])
+    ranges.append([spacing[i], spacing[i + 1]])
-# each replica to check staleness of the gradients in sync_replicas_optimizer.
+# each replica to check staleness of the gradients in SyncReplicasOptimizer.
-# More details can be found in the sync_replicas_optimizer class:
+# More details can be found in the SyncReplicasOptimizer class:
-      # replicas. More details can be found in sync_replicas_optimizer.
+      # replicas. More details can be found in SyncReplicasOptimizer.
-      # More details can be found in sync_replicas_optimizer.
+      # Get chief queue_runners and init_tokens, which is used to synchronize
-      # Get chief queue_runners, init_tokens and clean_up_op, which is used to
+      # Get chief queue_runners and init_tokens, which is used to
-            sess.run(clean_up_op)
+            tf.logging.info('Chief got exception while running!')
-        return self.sess.run(self.reconstruction, feed_dict={self.z_mean: hidden})
+            hidden = self.sess.run(tf.random_normal([1, self.n_hidden]))
-from six.moves.queue import Queue
+from six.moves import queue as Queue
-import Queue
+from six.moves.queue import Queue
-tf.flags.DEFINE_bool('normalize_by_length', True, 'Whether normalize')
+tf.flags.DEFINE_bool('normalize_by_length', True, 'Whether to normalize')
-import tensorflow as tf
+from six.moves import xrange
-
+from six.moves import xrange
-        tf.random_normal_initializer(stddev=1e-4, seed=self._seed)))
+        tf.random_normal_initializer(stddev=1e-4)))
-        tf.random_normal_initializer(stddev=1e-4, seed=self._seed)))
+        tf.random_normal_initializer(stddev=1e-4)))
-        tf.random_normal_initializer(stddev=1e-4, seed=self._seed)))
+        tf.random_normal_initializer(stddev=1e-4)))
-        tf.float32, tf.random_normal_initializer(stddev=1e-4, seed=self._seed)))
+        tf.float32, tf.random_normal_initializer(stddev=1e-4)))
-        tf.random_normal_initializer(stddev=1e-4, seed=self._seed)))
+        tf.random_normal_initializer(stddev=1e-4)))
-        tf.random_normal_initializer(stddev=1e-4, seed=self._seed)))
+        tf.random_normal_initializer(stddev=1e-4)))
-        tf.random_normal_initializer(stddev=1e-4, seed=self._seed)))
+        tf.random_normal_initializer(stddev=1e-4)))
-    return state.handle, cost, 0, 0
+    correct, total = tf.constant(0), tf.constant(0)
-    return state.handle, cost, 0, 0
+    correct, total = tf.constant(0), tf.constant(0)
-    stride = state.current_batch_size * self.training_beam_size
+    with tf.control_dependencies([tf.assert_equal(self.training_beam_size, 1)]):
-    # connections.
+# Copyright 2017 Google Inc. All Rights Reserved.
-"""
+# Copyright 2017 Google Inc. All Rights Reserved.
-        component_total = tf.constant(0)
+        args = (master_state, network_states)
-              comp.build_greedy_training(master_state, network_states))
+
-              master_state, network_states, during_training=True)
+          handle = comp.build_greedy_inference(*args, during_training=True)
-        # In addition, it keeps track of the number of training steps.
+        # Standard training keeps track of the number of training steps.
-def add_embeddings(channel_id, feature_spec, seed):
+def add_embeddings(channel_id, feature_spec, seed=None):
-        scaling_coefficient=1.0)
+        scaling_coefficient=1.0,
-        self._params.append(add_embeddings(channel_id, spec, self._seed))
+        self._params.append(add_embeddings(channel_id, spec))
-                    stddev=1 / spec.embedding_dim**.5, seed=self._seed)))
+                    stddev=1 / spec.embedding_dim**.5)))
-                  stddev=1e-4, seed=self._seed)))
+              initializer=tf.random_normal_initializer(stddev=1e-4)))
-                  stddev=1e-4, seed=self._seed)))
+              initializer=tf.random_normal_initializer(stddev=1e-4)))
-                  stddev=1e-4, seed=self._seed)))
+              initializer=tf.random_normal_initializer(stddev=1e-4)))
-                                                   seed=self._seed))
+          initializer=tf.random_normal_initializer(stddev=1e-4))
-                  stddev=1e-4, seed=self._seed)))
+              initializer=tf.random_normal_initializer(stddev=1e-4)))
-                                                 seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-                                                 seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-                                                 seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-        initializer=tf.random_normal_initializer(stddev=1e-4, seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-                                                 seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-                                                 seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-                                                 seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-        initializer=tf.random_normal_initializer(stddev=1e-4, seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-                                                 seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-                                                 seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-        initializer=tf.random_normal_initializer(stddev=1e-4, seed=self._seed))
+        initializer=tf.random_normal_initializer(stddev=1e-4))
-                                                 seed=self._seed)))
+        initializer=tf.random_normal_initializer(stddev=1e-4)))
-                    stddev=1e-4, seed=self._seed),
+                initializer=tf.random_normal_initializer(stddev=1e-4),
-                    stddev=1e-4, seed=self._seed),
+                initializer=tf.random_normal_initializer(stddev=1e-4),
-  def __init__(self, name, builder='DynamicComponentBuilder'):
+  def __init__(self,
-        backend=self.make_module('SyntaxNetComponent'),
+        backend=self.make_module(backend),
-def _write_summary(summary_writer, label, value, step):
+def write_summary(summary_writer, label, value, step):
-        _write_summary(summary_writer, label, metric, actual_step + step)
+        write_summary(summary_writer, label, metric, actual_step + step)
-  def __init__(self, component):
+  def __init__(self, component, additional_attr_defaults=None):
-        })
+        defaults=attr_defaults)
-                stddev=1e-4, seed=self._seed)))
+            initializer=tf.random_normal_initializer(stddev=1e-4)))
-                             tf.get_variable('bias_softmax'))
+                             self._component.get_variable('weights_softmax'),
-    super(BulkBiLSTMNetwork, self).__init__(component)
+    """Initializes the bulk bi-LSTM.
-flags.DEFINE_integer('num_epochs', 10, 'Number of epochs to train for.')
+# Copyright 2017 Google Inc. All Rights Reserved.
-          embedding_init=self._embedding_init)
+          embedding_init=self._embedding_init,
-      return e.SerializeToString()
+  def _token_embedding(self, token, embedding):
-    records_path = os.path.join(FLAGS.test_tmpdir, 'sstable-00000-of-00001')
+    records_path = os.path.join(FLAGS.test_tmpdir, 'records1')
-    writer.write(_TokenEmbedding('the', [5, 6]))
+    writer.write(self._token_embedding('.', [1, 2]))
-      def sampled_loss(labels, inputs):
+      def sampled_loss(labels, logits):
-        local_inputs = tf.cast(inputs, tf.float32)
+        local_inputs = tf.cast(logits, tf.float32)
-                                              hps.num_softmax_samples, vsize)
+            return tf.nn.sampled_softmax_loss(
-          size, forget_bias=0.0, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)
+          size, forget_bias=0.0, state_is_tuple=True)
-from autoencoder.autoencoder_models.DenoisingAutoencoder import AdditiveGaussianNoiseAutoencoder
+from autoencoder_models.DenoisingAutoencoder import AdditiveGaussianNoiseAutoencoder
-            "cost=", "{:.9f}".format(avg_cost)
+        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
-print "Total cost: " + str(autoencoder.calc_total_cost(X_test))
+print("Total cost: " + str(autoencoder.calc_total_cost(X_test)))
-from autoencoder.autoencoder_models.Autoencoder import Autoencoder
+from autoencoder_models.Autoencoder import Autoencoder
-            "cost=", "{:.9f}".format(avg_cost)
+        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
-print "Total cost: " + str(autoencoder.calc_total_cost(X_test))
+print("Total cost: " + str(autoencoder.calc_total_cost(X_test)))
-from autoencoder.autoencoder_models.DenoisingAutoencoder import MaskingNoiseAutoencoder
+from autoencoder_models.DenoisingAutoencoder import MaskingNoiseAutoencoder
-            "cost=", "{:.9f}".format(avg_cost)
+        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
-print "Total cost: " + str(autoencoder.calc_total_cost(X_test))
+print("Total cost: " + str(autoencoder.calc_total_cost(X_test)))
-from autoencoder.autoencoder_models.VariationalAutoencoder import VariationalAutoencoder
+from autoencoder_models.VariationalAutoencoder import VariationalAutoencoder
-            "cost=", "{:.9f}".format(avg_cost)
+        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
-print "Total cost: " + str(autoencoder.calc_total_cost(X_test))
+print("Total cost: " + str(autoencoder.calc_total_cost(X_test)))
-        all_weights['w1'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))
+        all_weights['w1'] = tf.get_variable("w1", shape=[self.n_input, self.n_hidden],
-            hidden = np.random.normal(size=self.weights["b1"])
+            hidden = self.sess.run(tf.random_normal([1, self.n_hidden]))
-        all_weights['w1'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))
+        all_weights['w1'] = tf.get_variable("w1", shape=[self.n_input, self.n_hidden],
-    def generate(self, hidden = None):
+    def generate(self, hidden=None):
-            hidden = np.random.normal(size = self.weights["b1"])
+            hidden = self.sess.run(tf.random_normal([1, self.n_hidden]))
-        all_weights['w1'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))
+        all_weights['w1'] = tf.get_variable("w1", shape=[self.n_input, self.n_hidden],
-    def generate(self, hidden = None):
+    def generate(self, hidden=None):
-            hidden = np.random.normal(size = self.weights["b1"])
+            hidden = self.sess.run(tf.random_normal([1, self.n_hidden]))
-        all_weights['log_sigma_w1'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))
+        all_weights['w1'] = tf.get_variable("w1", shape=[self.n_input, self.n_hidden],
-      def sampled_loss(inputs, labels):
+      def sampled_loss(labels, inputs):
-        print ('SKIPPED. Unexpected eror while decoding %s' % filename)
+      except Exception as e:
-    raise
+  image = coder.decode_jpeg(image_data)
-      image_buffer, height, width = _process_image(filename, coder)
+      try:
-  image = coder.decode_jpeg(image_data)
+  try:
-    tf.summary.scalar('learning rate', self.lrn_rate)
+    tf.summary.scalar('learning_rate', self.lrn_rate)
-    tf.nn.softmax_cross_entropy_with_logits(logits=y_logits, targets=y))
+    tf.nn.softmax_cross_entropy_with_logits(logits=y_logits, labels=y))
-          (emb_encoder_inputs, fw_state, _) = tf.nn.bidirectional_rnn(
+          (emb_encoder_inputs, fw_state, _) = tf.contrib.rnn.static_bidirectional_rnn(
-        decoder_outputs, self._dec_out_state = tf.nn.seq2seq.attention_decoder(
+        decoder_outputs, self._dec_out_state = tf.contrib.legacy_seq2seq.attention_decoder(
-          self._loss = tf.nn.seq2seq.sequence_loss(
+          self._loss = tf.contrib.legacy_seq2seq.sequence_loss(
-def trace_html(trace, convert_to_unicode=True, height='700px', script=None):
+def trace_html(trace,
-  visualizeToDiv({json}, "{elt_id}");
+  visualizeToDiv({json}, "{elt_id}", {master_spec_json});
-      script=script, json=json_trace, elt_id=elt_id, div_html=div_html)
+      script=script,
-  def show_trace(self, trace):
+  def show_trace(self, trace, master_spec=None):
-    visualizeToDiv({json}, "{elt_id}");
+    visualizeToDiv({json}, "{elt_id}", {master_spec_json});
-        json=parse_trace_json(trace), elt_id=self.elt_id)
+        json=parse_trace_json(trace),
-      name='test_component',)
+      # Google Translate says this is "component" in Chinese. (To test UTF-8).
-def trace_html(trace, convert_to_unicode=True, height='700px', script=None):
+def trace_html(trace,
-  visualizeToDiv({json}, "{elt_id}");
+  visualizeToDiv({json}, "{elt_id}", {master_spec_json});
-      script=script, json=json_trace, elt_id=elt_id, div_html=div_html)
+      script=script,
-  def show_trace(self, trace):
+  def show_trace(self, trace, master_spec=None):
-    visualizeToDiv({json}, "{elt_id}");
+    visualizeToDiv({json}, "{elt_id}", {master_spec_json});
-        json=parse_trace_json(trace), elt_id=self.elt_id)
+        json=parse_trace_json(trace),
-      name='test_component',)
+      # Google Translate says this is "component" in Chinese. (To test UTF-8).
-      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope='on_cpu'):
+      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
-      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope='on_gpu'):
+      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
-    for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope='on_cpu'):
+    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
-    for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope='on_gpu'):
+    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
-      tf.get_collection(tf.GraphKeys.VARIABLES), max_to_keep=0)
+      tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES), max_to_keep=0)
-  #  So adding a float function to avoid  ValueError.
+  # In some XML annotation files, the point values are not integers, but floats.
-    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is
+    spatial_squeeze: if True, logits is of shape [B, C], if false logits is
-    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is
+    spatial_squeeze: if True, logits is of shape [B, C], if false logits is
-    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is
+    spatial_squeeze: if True, logits is of shape [B, C], if false logits is
-    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is
+    spatial_squeeze: if True, logits is of shape [B, C], if false logits is
-    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is
+    spatial_squeeze: if True, logits is of shape [B, C], if false logits is
-        return net, end_points
+          end_points['predictions'] = slim.softmax(logits, scope='predictions')
-        logits = tf.squeeze(net, [1, 2], name='SpatialSqueeze')
+        if spatial_squeeze:
-          end_points['predictions'] = slim.softmax(net, scope='predictions')
+          end_points['predictions'] = slim.softmax(logits, scope='predictions')
-        if self._step % log_frequency == 0:
+        if self._step % FLAGS.log_frequency == 0:
-          sec_per_batch = float(duration / log_frequency)
+          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration
-                            """Log process results per count.""")
+tf.app.flags.DEFINE_integer('log_frequency', 10,
-          self._start_time = time.time()
+        log_frequency = FLAGS.log_frequency
-          loss_value = run_values.results
+        if self._step % log_frequency == 0:
-          sec_per_batch = float(duration / log_steps)
+          loss_value = run_values.results
-      def sampled_loss(inputs,labels):
+      def sampled_loss(inputs, labels):
-      return tf.nn.rnn_cell.GRUCell(size)
+      return tf.contrib.rnn.GRUCell(size)
-        return tf.nn.rnn_cell.BasicLSTMCell(size)
+        return tf.contrib.rnn.BasicLSTMCell(size)
-      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])
+      cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])
-      return tf.nn.seq2seq.embedding_attention_seq2seq(
+      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(
-      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(
+      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
-      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(
+      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
-resnet_v2_50.default_image_size = 224
+resnet_v2_50.default_image_size = resnet_v2.default_image_size
-resnet_v2_101.default_image_size = 224
+resnet_v2_101.default_image_size = resnet_v2.default_image_size
-resnet_v2_152.default_image_size = 224
+resnet_v2_152.default_image_size = resnet_v2.default_image_size
-resnet_v2_200.default_image_size = 224
+resnet_v2_200.default_image_size = resnet_v2.default_image_size
-resnet_v1_200.default_image_size = resnet_v1.default_image_size
+resnet_v1_200.default_image_size = resnet_v1.default_image_size
-  image_and_text = "<p><em>Text:</em> {}</p>{}".format(" ".join(
+  image_and_text = u"<p><em>Text:</em> {}</p>{}".format(" ".join(
-  as_uri = "data:text/html;base64,{}".format(base64.b64encode(new_window_html))
+  new_window_html = (u"<style type='text/css'>svg { max-width: 100%; }</style>"
-  return "{}<p><a target='_blank' href='{}'>Open in new window</a></p>".format(
+  return u"{}<p><a target='_blank' href='{}'>Open in new window</a></p>".format(
-    self.assertIn('text/html;base64', contents)
+    self.assertIn('text/html;charset=utf-8;base64', contents)
-  image_and_text = "<p><em>Text:</em> {}</p>{}".format(" ".join(
+  image_and_text = u"<p><em>Text:</em> {}</p>{}".format(" ".join(
-  as_uri = "data:text/html;base64,{}".format(base64.b64encode(new_window_html))
+  new_window_html = (u"<style type='text/css'>svg { max-width: 100%; }</style>"
-  return "{}<p><a target='_blank' href='{}'>Open in new window</a></p>".format(
+  return u"{}<p><a target='_blank' href='{}'>Open in new window</a></p>".format(
-    self.assertIn('text/html;base64', contents)
+    self.assertIn('text/html;charset=utf-8;base64', contents)
-        'testdata/context.pbtxt')
+    initial_task_context = os.path.join(FLAGS.test_srcdir,
-  indices, ids, weights = gen_parser_ops.unpack_sparse_features(sparse_features)
+  indices, ids, weights = gen_parser_ops.unpack_syntax_net_sparse_features(
-    broadcast_weights_shape = tf.concat(axis=0, values=[tf.shape(weights), [1]])
+    broadcast_weights_shape = tf.concat([tf.shape(weights), [1]], 0)
-    last_layer = tf.concat(axis=1, values=embeddings)
+    last_layer = tf.concat(embeddings, 1)
-        'testdata/context.pbtxt')
+    initial_task_context = os.path.join(FLAGS.test_srcdir,
-CHARS = u'''à¤ à¤ à¤ à¤ à¤ à¤ à¤ à¤¤ à¤¦ à¤¨ à¤ª à¤­ à¤¬ à¤¯ à¤® à¤° à¤² à¤µ à¤¹ à¤¸ à¤¿ à¤¾ à¥ à¥ à¥ à¥ à¥ à¥ à¥ à¥¤ à¤'''
+CHARS = u'''à¤ à¤ à¤ à¤ à¤ à¤ à¤ à¤¤ à¤¦ à¤¨ à¤ª à¤­ à¤¬ à¤¯ à¤® à¤° à¤² à¤µ à¤¹ à¤¸ à¤¿ à¤¾ à¥ à¥ à¥ à¥ à¥ à¥ à¥ à¥¤ à¤'''.split(' ')
-                 'suffix-table', 'tag-to-category', 'char-map'):
+                 'suffix-table', 'tag-to-category', 'char-map',
-    doc_source = gen_parser_ops.document_source(self.context_file, batch_size=1)
+    doc_source = gen_parser_ops.document_source(
-          loaded_map[entries[0]] = entries[1]
+        if len(entries) >= 2:
-    for char in CHARS.split(' '):
+    self.assertEqual(len(char_map), len(CHARS))
-      gen_parser_ops.lexicon_builder(task_context=self.context_file).run()
+      gen_parser_ops.lexicon_builder(
-                 'parser_ops.so'))
+    os.path.join(tf.resource_loader.get_data_files_path(), 'parser_ops.so'))
-  with gfile.FastGFile(task_context) as fin:
+  with gfile.FastGFile(task_context, 'rb') as fin:
-  with gfile.FastGFile(FLAGS.task_context) as fin:
+  with gfile.FastGFile(FLAGS.task_context, 'rb') as fin:
-        'testdata/context.pbtxt')
+    initial_task_context = os.path.join(FLAGS.test_srcdir,
-                 'suffix-table', 'tag-to-category'):
+    for name in ('word-map', 'lcword-map', 'tag-map', 'category-map',
-        self.context_file, batch_size=1)
+        task_context=self.context_file, batch_size=1)
-                       tokenization)
+      self.assertEqual(' '.join([t.word
-        self.context_file, batch_size=1)
+        task_context=self.context_file, batch_size=1)
-                             [0, 3, 6, 9], [2, 5, 8, 11])
+    self.CheckUntokenizedDoc('ä¸ä¸ªæµè¯', ['ä¸', 'ä¸ª', 'æµ', 'è¯'], [0, 3, 6, 9],
-                  'å¥å­	NO_SPACE']
+    doc1_lines = ['æµè¯	NO_SPACE\n', 'ç	NO_SPACE\n', 'å¥å­	NO_SPACE']
-                  '.	NO_SPACE']
+    doc2_lines = [
-                         0, 1]
+    doc2_tokens = [
-        self.context_file, batch_size=1)
+        task_context=self.context_file, batch_size=1)
-    print 'POS %.2f UAS %.2f LAS %.2f' % (pos, uas, las)
+# Copyright 2016 Google Inc. All Rights Reserved.
-        'testdata/context.pbtxt')
+    initial_task_context = os.path.join(FLAGS.test_srcdir,
-  indices, ids, weights = gen_parser_ops.unpack_sparse_features(sparse_features)
+  indices, ids, weights = gen_parser_ops.unpack_syntax_net_sparse_features(
-    broadcast_weights_shape = tf.concat(axis=0, values=[tf.shape(weights), [1]])
+    broadcast_weights_shape = tf.concat([tf.shape(weights), [1]], 0)
-    last_layer = tf.concat(axis=1, values=embeddings)
+    last_layer = tf.concat(embeddings, 1)
-        'testdata/context.pbtxt')
+    initial_task_context = os.path.join(FLAGS.test_srcdir,
-CHARS = u'''à¤ à¤ à¤ à¤ à¤ à¤ à¤ à¤¤ à¤¦ à¤¨ à¤ª à¤­ à¤¬ à¤¯ à¤® à¤° à¤² à¤µ à¤¹ à¤¸ à¤¿ à¤¾ à¥ à¥ à¥ à¥ à¥ à¥ à¥ à¥¤ à¤'''
+CHARS = u'''à¤ à¤ à¤ à¤ à¤ à¤ à¤ à¤¤ à¤¦ à¤¨ à¤ª à¤­ à¤¬ à¤¯ à¤® à¤° à¤² à¤µ à¤¹ à¤¸ à¤¿ à¤¾ à¥ à¥ à¥ à¥ à¥ à¥ à¥ à¥¤ à¤'''.split(' ')
-                 'suffix-table', 'tag-to-category', 'char-map'):
+                 'suffix-table', 'tag-to-category', 'char-map',
-    doc_source = gen_parser_ops.document_source(self.context_file, batch_size=1)
+    doc_source = gen_parser_ops.document_source(
-          loaded_map[entries[0]] = entries[1]
+        if len(entries) >= 2:
-    for char in CHARS.split(' '):
+    self.assertEqual(len(char_map), len(CHARS))
-      gen_parser_ops.lexicon_builder(task_context=self.context_file).run()
+      gen_parser_ops.lexicon_builder(
-                 'parser_ops.so'))
+    os.path.join(tf.resource_loader.get_data_files_path(), 'parser_ops.so'))
-  with gfile.FastGFile(task_context) as fin:
+  with gfile.FastGFile(task_context, 'rb') as fin:
-  with gfile.FastGFile(FLAGS.task_context) as fin:
+  with gfile.FastGFile(FLAGS.task_context, 'rb') as fin:
-        'testdata/context.pbtxt')
+    initial_task_context = os.path.join(FLAGS.test_srcdir,
-                 'suffix-table', 'tag-to-category'):
+    for name in ('word-map', 'lcword-map', 'tag-map', 'category-map',
-        self.context_file, batch_size=1)
+        task_context=self.context_file, batch_size=1)
-                       tokenization)
+      self.assertEqual(' '.join([t.word
-        self.context_file, batch_size=1)
+        task_context=self.context_file, batch_size=1)
-                             [0, 3, 6, 9], [2, 5, 8, 11])
+    self.CheckUntokenizedDoc('ä¸ä¸ªæµè¯', ['ä¸', 'ä¸ª', 'æµ', 'è¯'], [0, 3, 6, 9],
-                  'å¥å­	NO_SPACE']
+    doc1_lines = ['æµè¯	NO_SPACE\n', 'ç	NO_SPACE\n', 'å¥å­	NO_SPACE']
-                  '.	NO_SPACE']
+    doc2_lines = [
-                         0, 1]
+    doc2_tokens = [
-        self.context_file, batch_size=1)
+        task_context=self.context_file, batch_size=1)
-    print "Succesfully downloaded", filename, statinfo.st_size, "bytes"
+    print "Successfully downloaded", filename, statinfo.st_size, "bytes"
-  with tf.op_scope([t, upper_bound], name, "batch_clip_by_l2norm") as name:
+  with tf.name_scope(values=[t, upper_bound], name=name,
-  with tf.op_scope([t, threshold_ratio], name, "soft_thresholding") as name:
+  with tf.name_scope(values=[t, threshold_ratio], name=name,
-  with tf.op_scope([t, sigma], name, "add_gaussian_noise") as name:
+  with tf.name_scope(values=[t, sigma], name=name,
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([image_buffer], scope, 'decode_jpeg'):
+  with tf.name_scope(values=[image_buffer], name=scope,
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([image], scope, 'distort_color'):
+  with tf.name_scope(values=[image], name=scope, default_name='distort_color'):
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([image, height, width, bbox], scope, 'distort_image'):
+  with tf.name_scope(values=[image, height, width, bbox], name=scope,
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([image, height, width], scope, 'eval_image'):
+  with tf.name_scope(values=[image, height, width], name=scope,
-                                  tf.expand_dims(most_recent_hint_idx, 1)], 1)
+      hint_pool_idxs = tf.concat(
-    return tf.concat(hint_pool_idxs, 1)
+    return tf.concat(axis=1, values=hint_pool_idxs)
-    sess.run(tf.initialize_all_variables())
+    sess.run(tf.global_variables_initializer())
-      sigmoid_loss = tf.reduce_mean(tf.concat(sigmoid_losses, 0), 0,
+      l2_loss = tf.reduce_mean(tf.concat(axis=0, values=l2_losses), 0,
-                   'sequence_loss_by_example'):
+  with tf.name_scope(values=inputs + targets + weights, name=name,
-  with tf.op_scope(inputs + targets + weights, name, 'sampled_sequence_loss'):
+  with tf.name_scope(values=inputs + targets + weights, name=name,
-        graph_def=sess.graph.as_graph_def(add_shapes=True))
+        graph=sess.graph)
-    ######################
+    #######################
-    ####################
+    ######################
-    ####################
+    ######################
-          cell_fw = tf.nn.rnn_cell.LSTMCell(
+          cell_fw = tf.contrib.rnn.LSTMCell(
-          cell_bw = tf.nn.rnn_cell.LSTMCell(
+          cell_bw = tf.contrib.rnn.LSTMCell(
-        cell = tf.nn.rnn_cell.LSTMCell(
+        cell = tf.contrib.rnn.LSTMCell(
-    tf.summary.scalar('clone_loss', clone_loss)
+    tf.summary.scalar(clone.scope + '/clone_loss', clone_loss)
-                      name='clone_loss')
+    tf.summary.scalar(clone.scope + '/clone_loss', clone_loss)
-                      name='regularization_loss')
+    tf.summary.scalar('regularization_loss', regularization_loss)
-                                      name='total_loss'))
+      summaries.add(tf.summary.scalar('total_loss', total_loss))
-                                      name='learning_rate'))
+      summaries.add(tf.summary.scalar('learning_rate', learning_rate))
-                                    name='total_loss'))
+    summaries.add(tf.summary.scalar('total_loss', total_loss))
-      tf.summary.scalar('loss', self.loss, name='loss')
+      tf.summary.scalar('loss', self.loss)
-    tf.summary.scalar('learn_rate', learn_rate_dec, name='lr_summ')
+    tf.summary.scalar('learn_rate', learn_rate_dec)
-                                                 ops.conv2d(branch3x3, 384, [3, 1])])
+                                                  ops.conv2d(branch3x3, 384, [3, 1])])
-            branch3x3 = tf.concat(axis=3,values=[ops.conv2d(branch3x3, 384, [1, 3]),
+            branch3x3 = tf.concat(axis=3, values=[ops.conv2d(branch3x3, 384, [1, 3]),
-     train("data/SmallNames.txt", "model/namignizer", SmallConfig)
+if __name__ == "__main__":
-         tf.train.latest_checkpoint("model"), SmallConfig)
+    namignize(["mary", "ida", "gazorbazorb", "mmmhmm", "bob"],
-     namignator(tf.train.latest_checkpoint("model"), SmallConfig)
+    namignator(tf.train.latest_checkpoint("model"), SmallConfig)
-  outputs = tf.split(axis=tf.nn.log_softmax(output), num_or_size_splits=beam_size, value=0)
+  outputs = tf.split(axis=0, num_or_size_splits=beam_size, value=tf.nn.log_softmax(output))
-    gpu_prev_step = tf.split(axis=self.prev_step, num_or_size_splits=num_gpus, value=0)
+    gpu_input = tf.split(axis=0, num_or_size_splits=num_gpus, value=self.input)
-        res = tf.split(axis=res, num_or_size_splits=2, value=3)
+        res = tf.split(axis=3, num_or_size_splits=2, value=res)
-            input_, canvas = tf.split(axis=input_, num_or_size_splits=2, value=3)
+            input_, canvas = tf.split(axis=3, num_or_size_splits=2, value=input_)
-            canvas, input_ = tf.split(axis=input_, num_or_size_splits=2, value=3)
+            canvas, input_ = tf.split(axis=3, num_or_size_splits=2, value=input_)
-        shift, log_rescaling = tf.split(axis=res, num_or_size_splits=2, value=3)
+        shift, log_rescaling = tf.split(axis=3, num_or_size_splits=2, value=res)
-            input_, canvas = tf.split(axis=input_, num_or_size_splits=2, value=3)
+            input_, canvas = tf.split(axis=3, num_or_size_splits=2, value=input_)
-            canvas, input_ = tf.split(axis=input_, num_or_size_splits=2, value=3)
+            canvas, input_ = tf.split(axis=3, num_or_size_splits=2, value=input_)
-                log_diff_1, log_diff_2 = tf.split(axis=log_diff, num_or_size_splits=2, value=3)
+                res_1, res_2 = tf.split(axis=3, num_or_size_splits=2, value=res)
-                log_diff_1, log_diff_2 = tf.split(axis=log_diff, num_or_size_splits=2, value=3)
+                res_1, res_2 = tf.split(axis=3, num_or_size_splits=2, value=res)
-                z_lost, _ = tf.split(axis=z_lost, num_or_size_splits=2, value=3)
+                z_lost, _ = tf.split(axis=3, num_or_size_splits=2, value=z_lost)
-    # train("data/SmallNames.txt", "model/namignizer", SmallConfig)
+ if __name__ == "__main__":
-    #     tf.train.latest_checkpoint("model"), SmallConfig)
+     namignize(["mary", "ida", "gazorbazorb", "mmmhmm", "bob"],
-    # namignator(tf.train.latest_checkpoint("model"), SmallConfig)
+     namignator(tf.train.latest_checkpoint("model"), SmallConfig)
-        tf.concat(axis=initial_state, values=1, name="initial_state")
+        tf.concat(axis=1, values=initial_state, name="initial_state")
-        tf.concat(axis=state_tuple, values=1, name="state")
+        tf.concat(axis=1, values=state_tuple, name="state")
-          net = tf.concat(axis=[branch1x1, branch5x5, branch3x3dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch1x1, branch5x5, branch3x3dbl, branch_pool])
-          net = tf.concat(axis=[branch1x1, branch5x5, branch3x3dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch1x1, branch5x5, branch3x3dbl, branch_pool])
-          net = tf.concat(axis=[branch1x1, branch5x5, branch3x3dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch1x1, branch5x5, branch3x3dbl, branch_pool])
-          net = tf.concat(axis=[branch3x3, branch3x3dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch3x3, branch3x3dbl, branch_pool])
-          net = tf.concat(axis=[branch1x1, branch7x7, branch7x7dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch1x1, branch7x7, branch7x7dbl, branch_pool])
-          net = tf.concat(axis=[branch1x1, branch7x7, branch7x7dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch1x1, branch7x7, branch7x7dbl, branch_pool])
-          net = tf.concat(axis=[branch1x1, branch7x7, branch7x7dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch1x1, branch7x7, branch7x7dbl, branch_pool])
-          net = tf.concat(axis=[branch1x1, branch7x7, branch7x7dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch1x1, branch7x7, branch7x7dbl, branch_pool])
-          net = tf.concat(axis=[branch3x3, branch7x7x3, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch3x3, branch7x7x3, branch_pool])
-                                   ops.conv2d(branch3x3, 384, [3, 1])], values=3)
+            branch3x3 = tf.concat(axis=3,values=[ops.conv2d(branch3x3, 384, [1, 3]),
-                                      ops.conv2d(branch3x3dbl, 384, [3, 1])], values=3)
+            branch3x3dbl = tf.concat(axis=3, values=[ops.conv2d(branch3x3dbl, 384, [1, 3]),
-          net = tf.concat(axis=[branch1x1, branch3x3, branch3x3dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch1x1, branch3x3, branch3x3dbl, branch_pool])
-                                   ops.conv2d(branch3x3, 384, [3, 1])], values=3)
+            branch3x3 = tf.concat(axis=3, values=[ops.conv2d(branch3x3, 384, [1, 3]),
-                                      ops.conv2d(branch3x3dbl, 384, [3, 1])], values=3)
+            branch3x3dbl = tf.concat(axis=3, values=[ops.conv2d(branch3x3dbl, 384, [1, 3]),
-          net = tf.concat(axis=[branch1x1, branch3x3, branch3x3dbl, branch_pool], values=3)
+          net = tf.concat(axis=3, values=[branch1x1, branch3x3, branch3x3dbl, branch_pool])
-    concated = tf.concat(axis=[indices, labels], values=1)
+    concated = tf.concat(axis=1, values=[indices, labels])
-      arg = tf.concat(axis=args, values=3)
+      arg = tf.concat(axis=3, values=args)
-  return tf.concat(axis=[slice1, selected, slice2], values=1)
+  return tf.concat(axis=1, values=[slice1, selected, slice2])
-  return tf.concat(axis=[slice1, selected, slice2], values=1)
+  return tf.concat(axis=1, values=[slice1, selected, slice2])
-  all_beam_idx = tf.reshape(tf.transpose(tf.concat(axis=all_beam_idx, values=1), [1, 0]),
+  all_beam_idx = tf.reshape(tf.transpose(tf.concat(axis=1, values=all_beam_idx), [1, 0]),
-  top_beam, top_beam_idx = tf.nn.top_k(tf.concat(axis=all_beam_vals, values=1), k=beam_size)
+  top_beam, top_beam_idx = tf.nn.top_k(tf.concat(axis=1, values=all_beam_vals), k=beam_size)
-  top_out_idx = tf.concat(axis=top_out_idx, values=0)
+  new_tensors = [tf.concat(axis=0, values=t) for t in reordered]
-      output = tf.concat(axis=[output] * height, values=1)
+      output = tf.concat(axis=1, values=[output] * height)
-        mem = tf.concat(axis=[mem] * height, values=2)
+        mem = tf.concat(axis=2, values=[mem] * height)
-              embedded_targets_tn = tf.concat(axis=[embedded_targets_tn] * height, values=2)
+              embedded_targets_tn = tf.concat(axis=2, values=[embedded_targets_tn] * height)
-              concatenated = tf.reshape(tf.concat(axis=[cell_inp, attn_res], values=1),
+              concatenated = tf.reshape(tf.concat(axis=1, values=[cell_inp, attn_res]),
-                    tf.concat(axis=[output, res], values=1), height * nmaps, name="rnnmem")
+                    tf.concat(axis=1, values=[output, res]), height * nmaps, name="rnnmem")
-            dec_inp = tf.concat(axis=[dec_zero, gpu_targets], values=1)
+            dec_inp = tf.concat(axis=1, values=[dec_zero, gpu_targets])
-            tgts = tf.concat(axis=[embedded_targets_tn] * beam_size, values=1)
+            tgts = tf.concat(axis=1, values=[embedded_targets_tn] * beam_size)
-            step = tf.concat(axis=[step] * beam_size, values=0)
+            step = tf.concat(axis=0, values=[step] * beam_size)
-    self.after_enc_step = tf.concat(axis=self.after_enc_step, values=0)  # Concat GPUs.
+    self.after_enc_step = tf.concat(axis=0, values=self.after_enc_step)  # Concat GPUs.
-    self.out_idx = tf.concat(axis=gpu_out_idx, values=0)
+    self.out_idx = tf.concat(axis=0, values=gpu_out_idx)
-    self.outputs = [tf.concat(axis=[gpu_outputs[g] for g in xrange(num_gpus)], values=1)]
+    self.outputs = [tf.concat(axis=1, values=[gpu_outputs[g] for g in xrange(num_gpus)])]
-                              tf.expand_dims(sparse_local_col, 1)], values=1)
+  sparse_indices = tf.concat(axis=1, values=[tf.expand_dims(sparse_local_row, 1),
-    broadcast_weights_shape = tf.concat(axis=[tf.shape(weights), [1]], values=0)
+    broadcast_weights_shape = tf.concat(axis=0, values=[tf.shape(weights), [1]])
-    last_layer = tf.concat(axis=embeddings, values=1)
+    last_layer = tf.concat(axis=1, values=embeddings)
-    grad = tf.concat(axis=grads, values=0)
+    grad = tf.concat(axis=0, values=grads)
-    output = tf.reshape(tf.concat(axis=outputs, values=1), [-1, size])
+    output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])
-        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))
+        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))
-        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))
+        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))
-        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))
+        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))
-        self.z = tf.add(self.z_mean, tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))
+        eps = tf.random_normal(tf.stack([tf.shape(self.x)[0], self.n_hidden]), 0, 1, dtype = tf.float32)
-        reconstr_loss = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))
+        reconstr_loss = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.reconstruction, self.x), 2.0))
-        logits, tf.one_hot(labels, 10))
+        logits=logits, labels=tf.one_hot(labels, 10))
-      sess.run(tf.initialize_variables([v]))
+      sess.run(tf.variables_initializer([v]))
-    t2 = tf.reshape(t, tf.concat(0, [batch_size, [-1]]))
+    t2 = tf.reshape(t, tf.concat(axis=0, values=[batch_size, [-1]]))
-    t2 = tf.reshape(t, tf.concat(0, [tf.slice(saved_shape, [0], [1]), -1]))
+    t2 = tf.reshape(t, tf.concat(axis=0, values=[tf.slice(saved_shape, [0], [1]), -1]))
-    return tf.mul(x_expanded, z_grads_expanded)
+    return tf.multiply(x_expanded, z_grads_expanded)
-    conv = tf.concat(0, conv_px)
+    conv = tf.concat(axis=0, values=conv_px)
-    return tf.pack(gradients_list)
+    return tf.stack(gradients_list)
-    weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')
+    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')
-  tf.scalar_summary('learning_rate', lr)
+  tf.summary.scalar('learning_rate', lr)
-    tf.histogram_summary(var.op.name, var)
+    tf.summary.histogram(var.op.name, var)
-    saver = tf.train.Saver(tf.all_variables())
+    saver = tf.train.Saver(tf.global_variables())
-    x = tf.mul(binomial, signs)
+    x = tf.multiply(binomial, signs)
-    y = tf.mul(x, tf.exp(exponents))
+    y = tf.multiply(x, tf.exp(exponents))
-        tf.concat(initial_state, 1, name="initial_state")
+        tf.concat(axis=initial_state, values=1, name="initial_state")
-            inputs=tf.squeeze(self.seq_embeddings, squeeze_dims=[1]),
+            inputs=tf.squeeze(self.seq_embeddings, axis=[1]),
-        tf.concat(state_tuple, 1, name="state")
+        tf.concat(axis=state_tuple, values=1, name="state")
-      tf.image_summary('image_with_bounding_boxes', image_with_box)
+      tf.summary.image('image_with_bounding_boxes', image_with_box)
-      tf.image_summary('images_with_distorted_bounding_box',
+      tf.summary.image('images_with_distorted_bounding_box',
-      tf.image_summary('cropped_resized_image',
+      tf.summary.image('cropped_resized_image',
-      tf.image_summary('final_distorted_image',
+      tf.summary.image('final_distorted_image',
-  image = tf.mul(image, 2.0)
+  image = tf.subtract(image, 0.5)
-  bbox = tf.concat(0, [ymin, xmin, ymax, xmax])
+  bbox = tf.concat(axis=0, values=[ymin, xmin, ymax, xmax])
-    tf.image_summary('images', images)
+    tf.summary.image('images', images)
-      tf.scalar_summary('learning_rate', lr)
+      tf.summary.scalar('learning_rate', lr)
-          tf.scalar_summary(loss_name, loss_averages.average(l))
+          tf.summary.scalar(loss_name + ' (raw)', l)
-        tf.histogram_summary(var.op.name, var)
+        tf.summary.histogram(var.op.name, var)
-          tf.histogram_summary(var.op.name + '/gradients', grad)
+          tf.summary.histogram(var.op.name + '/gradients', grad)
-      summary_op = tf.merge_all_summaries()
+      summary_op = tf.summary.merge_all()
-    summary_op = tf.merge_all_summaries()
+    summary_op = tf.summary.merge_all()
-    summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir,
+    summary_writer = tf.summary.FileWriter(FLAGS.eval_dir,
-  concated = tf.concat(1, [indices, sparse_labels])
+  concated = tf.concat(axis=1, values=[indices, sparse_labels])
-  tf.contrib.deprecated.scalar_summary(tensor_name + '/sparsity', tf.nn.zero_fraction(x))
+  tf.summary.histogram(tensor_name + '/activations', x)
-    tf.scalar_summary(loss_name, loss_averages.average(l))
+    tf.summary.scalar(loss_name +' (raw)', l)
-    grad = tf.concat(0, grads)
+    grad = tf.concat(axis=0, values=grads)
-    labels_splits = tf.split(0, FLAGS.num_gpus, labels)
+    images_splits = tf.split(axis=0, num_or_size_splits=FLAGS.num_gpus, value=images)
-    summaries.append(tf.scalar_summary('learning_rate', lr))
+    summaries.append(tf.summary.scalar('learning_rate', lr))
-            tf.histogram_summary(var.op.name + '/gradients', grad))
+            tf.summary.histogram(var.op.name + '/gradients', grad))
-      summaries.append(tf.histogram_summary(var.op.name, var))
+      summaries.append(tf.summary.histogram(var.op.name, var))
-    saver = tf.train.Saver(tf.all_variables())
+    saver = tf.train.Saver(tf.global_variables())
-    summary_op = tf.merge_summary(summaries)
+    summary_op = tf.summary.merge(summaries)
-    summary_writer = tf.train.SummaryWriter(
+    summary_writer = tf.summary.FileWriter(
-          net = tf.concat([branch1x1, branch5x5, branch3x3dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch1x1, branch5x5, branch3x3dbl, branch_pool], values=3)
-          net = tf.concat([branch1x1, branch5x5, branch3x3dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch1x1, branch5x5, branch3x3dbl, branch_pool], values=3)
-          net = tf.concat([branch1x1, branch5x5, branch3x3dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch1x1, branch5x5, branch3x3dbl, branch_pool], values=3)
-          net = tf.concat([branch3x3, branch3x3dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch3x3, branch3x3dbl, branch_pool], values=3)
-          net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch1x1, branch7x7, branch7x7dbl, branch_pool], values=3)
-          net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch1x1, branch7x7, branch7x7dbl, branch_pool], values=3)
-          net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch1x1, branch7x7, branch7x7dbl, branch_pool], values=3)
-          net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch1x1, branch7x7, branch7x7dbl, branch_pool], values=3)
-          net = tf.concat([branch3x3, branch7x7x3, branch_pool], 3)
+          net = tf.concat(axis=[branch3x3, branch7x7x3, branch_pool], values=3)
-                                   ops.conv2d(branch3x3, 384, [3, 1])], 3)
+            branch3x3 = tf.concat(axis=[ops.conv2d(branch3x3, 384, [1, 3]),
-                                      ops.conv2d(branch3x3dbl, 384, [3, 1])], 3)
+            branch3x3dbl = tf.concat(axis=[ops.conv2d(branch3x3dbl, 384, [1, 3]),
-          net = tf.concat([branch1x1, branch3x3, branch3x3dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch1x1, branch3x3, branch3x3dbl, branch_pool], values=3)
-                                   ops.conv2d(branch3x3, 384, [3, 1])], 3)
+            branch3x3 = tf.concat(axis=[ops.conv2d(branch3x3, 384, [1, 3]),
-                                      ops.conv2d(branch3x3dbl, 384, [3, 1])], 3)
+            branch3x3dbl = tf.concat(axis=[ops.conv2d(branch3x3dbl, 384, [1, 3]),
-          net = tf.concat([branch1x1, branch3x3, branch3x3dbl, branch_pool], 3)
+          net = tf.concat(axis=[branch1x1, branch3x3, branch3x3dbl, branch_pool], values=3)
-    concated = tf.concat([indices, labels], 1)
+    concated = tf.concat(axis=[indices, labels], values=1)
-        concated, tf.pack([batch_size, num_classes]), 1.0, 0.0)
+        concated, tf.stack([batch_size, num_classes]), 1.0, 0.0)
-                             initializer=tf.zeros_initializer,
+                             initializer=tf.zeros_initializer(),
-        output = tf.reshape(tf.concat(outputs, 1), [-1, size])
+        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, size])
-      arg = tf.concat(args, 3)
+      arg = tf.concat(axis=args, values=3)
-  return tf.concat([slice1, selected, slice2], 1)
+  return tf.concat(axis=[slice1, selected, slice2], values=1)
-  return tf.concat([slice1, selected, slice2], 1)
+  return tf.concat(axis=[slice1, selected, slice2], values=1)
-  outputs = tf.split(tf.nn.log_softmax(output), beam_size, 0)
+  outputs = tf.split(axis=tf.nn.log_softmax(output), num_or_size_splits=beam_size, value=0)
-  all_beam_idx = tf.reshape(tf.transpose(tf.concat(all_beam_idx, 1), [1, 0]),
+  all_beam_idx = tf.reshape(tf.transpose(tf.concat(axis=all_beam_idx, values=1), [1, 0]),
-  top_beam, top_beam_idx = tf.nn.top_k(tf.concat(all_beam_vals, 1), k=beam_size)
+  top_beam, top_beam_idx = tf.nn.top_k(tf.concat(axis=all_beam_vals, values=1), k=beam_size)
-  top_out_idx = tf.concat(top_out_idx, 0)
+  new_tensors = [tf.concat(axis=t, values=0) for t in reordered]
-    gpu_prev_step = tf.split(self.prev_step, num_gpus, 0)
+    gpu_input = tf.split(axis=self.input, num_or_size_splits=num_gpus, value=0)
-      output = tf.concat([output] * height, 1)
+      output = tf.concat(axis=[output] * height, values=1)
-        mem = tf.concat([mem] * height, 2)
+        mem = tf.concat(axis=[mem] * height, values=2)
-              embedded_targets_tn = tf.concat([embedded_targets_tn] * height, 2)
+              embedded_targets_tn = tf.concat(axis=[embedded_targets_tn] * height, values=2)
-              concatenated = tf.reshape(tf.concat([cell_inp, attn_res], 1),
+              concatenated = tf.reshape(tf.concat(axis=[cell_inp, attn_res], values=1),
-                    tf.concat([output, res], 1), height * nmaps, name="rnnmem")
+                    tf.concat(axis=[output, res], values=1), height * nmaps, name="rnnmem")
-            dec_inp = tf.concat([dec_zero, gpu_targets], 1)
+            dec_inp = tf.concat(axis=[dec_zero, gpu_targets], values=1)
-            tgts = tf.concat([embedded_targets_tn] * beam_size, 1)
+            tgts = tf.concat(axis=[embedded_targets_tn] * beam_size, values=1)
-            step = tf.concat([step] * beam_size, 0)
+            step = tf.concat(axis=[step] * beam_size, values=0)
-    self.after_enc_step = tf.concat(self.after_enc_step, 0)  # Concat GPUs.
+    self.after_enc_step = tf.concat(axis=self.after_enc_step, values=0)  # Concat GPUs.
-    self.out_idx = tf.concat(gpu_out_idx, 0)
+    self.out_idx = tf.concat(axis=gpu_out_idx, values=0)
-    self.outputs = [tf.concat([gpu_outputs[g] for g in xrange(num_gpus)], 1)]
+    self.outputs = [tf.concat(axis=[gpu_outputs[g] for g in xrange(num_gpus)], values=1)]
-    hidden_vectors = tf.concat(0, hidden_vectors)
+    hidden_vectors = tf.concat(axis=0, values=hidden_vectors)
-            tf.concat(1, [hprev, curr_hprev]), self.params[
+            tf.concat(axis=1, values=[hprev, curr_hprev]), self.params[
-      question_number_softmax = tf.nn.softmax(tf.concat(1, [first, second]))
+      question_number_softmax = tf.nn.softmax(tf.concat(axis=1, values=[first, second]))
-        question_number_softmax = tf.select(
+        question_number_softmax = tf.where(
-              1, [self.batch_question_number, self.batch_question_number_one]),
+              axis=1, values=[self.batch_question_number, self.batch_question_number_one]),
-        1, [self.column_hidden_vectors, self.word_column_hidden_vectors])
+        axis=1, values=[self.column_hidden_vectors, self.word_column_hidden_vectors])
-      temp_ans = tf.transpose(tf.concat(0, temp_ans))
+      temp_ans = tf.transpose(tf.concat(axis=0, values=temp_ans))
-    softmax = tf.select(
+    softmax = tf.where(
-      answer = tf.select(select_mask, curr_prob, answer)
+      answer = tf.where(select_mask, curr_prob, answer)
-    select_prev = tf.concat(1, [
+    select_prev = tf.concat(axis=1, values=[
-    select_next = tf.concat(1, [
+    select_next = tf.concat(axis=1, values=[
-    values = tf.concat(1, [count])
+    values = tf.concat(axis=1, values=[count])
-    output = tf.reduce_sum(tf.mul(softmax_content, values), 1)
+    output = tf.reduce_sum(tf.multiply(softmax_content, values), 1)
-        tf.concat(1, select_lists), 1)
+        tf.concat(axis=1, values=select_lists), 1)
-            tf.concat(1, [question_embedding, attention_vector]), self.params[
+            tf.concat(axis=1, values=[question_embedding, attention_vector]), self.params[
-            tf.concat(1, [question_embedding, attention_vector]), self.params[
+            tf.concat(axis=1, values=[question_embedding, attention_vector]), self.params[
-            tf.concat(1, [
+            tf.concat(axis=1, values=[
-    inter = tf.select(
+    inter = tf.where(
-    math_error = 0.5 * tf.square(tf.sub(self.scalar_output, self.batch_answer))
+    math_error = 0.5 * tf.square(tf.subtract(self.scalar_output, self.batch_answer))
-    self.init_print_error = tf.select(
+    self.init_print_error = tf.where(
-      error = tf.select(
+      error = tf.where(
-          tf.select(
+          tf.where(
-      error = tf.select(
+      error = tf.where(
-          tf.select(
+          tf.where(
-      history_input = tf.concat(1, [input_op, input_col])
+      history_input = tf.concat(axis=1, values=[input_op, input_col])
-    correct_add = tf.select(
+    correct_add = tf.where(
-        1, [self.batch_number_column_mask, self.batch_word_column_mask])
+        axis=1, values=[self.batch_number_column_mask, self.batch_word_column_mask])
-    self.full_processed_sorted_index_column = tf.concat(1, [
+        axis=1,
-        1, [self.select_mask, self.select_word_mask])
+        axis=1, values=[self.select_mask, self.select_word_mask])
-        tf.concat(1, [
+        tf.concat(axis=1, values=[
-          1, [image, image + diff_output, image + diff, diff_output])
+          axis=1, values=[image, image + diff_output, image + diff, diff_output])
-    net = tf.concat(3, [image, diff])
+    net = tf.concat(axis=3, values=[image, diff])
-            split_dim=1, num_split=2, value=z)
+            axis=1, num_or_size_splits=2, value=z)
-    kernels = tf.split(split_dim=3, num_split=4, value=self.kernel)
+    kernels = tf.split(axis=3, num_or_size_splits=4, value=self.kernel)
-        cross_conved_images.append(tf.concat(0, conved_image))
+        cross_conved_images.append(tf.concat(axis=0, values=conved_image))
-    net = tf.concat(3, nets)
+    net = tf.concat(axis=3, values=nets)
-        (tf.concat(0, resized_images[:-1]), tf.concat(0, diffs)))
+        (tf.concat(axis=0, values=resized_images[:-1]), tf.concat(axis=0, values=diffs)))
-        res = tf.split(res, 2, 3)
+        res = tf.split(axis=res, num_or_size_splits=2, value=3)
-            input_, canvas = tf.split(input_, 2, 3)
+            input_, canvas = tf.split(axis=input_, num_or_size_splits=2, value=3)
-            canvas, input_ = tf.split(input_, 2, 3)
+            canvas, input_ = tf.split(axis=input_, num_or_size_splits=2, value=3)
-        shift, log_rescaling = tf.split(res, 2, 3)
+        shift, log_rescaling = tf.split(axis=res, num_or_size_splits=2, value=3)
-            input_, canvas = tf.split(input_, 2, 3)
+            input_, canvas = tf.split(axis=input_, num_or_size_splits=2, value=3)
-            canvas, input_ = tf.split(input_, 2, 3)
+            canvas, input_ = tf.split(axis=input_, num_or_size_splits=2, value=3)
-                log_diff_1, log_diff_2 = tf.split(log_diff, 2, 3)
+                res_1, res_2 = tf.split(axis=res, num_or_size_splits=2, value=3)
-                log_diff_1, log_diff_2 = tf.split(log_diff, 2, 3)
+                res_1, res_2 = tf.split(axis=res, num_or_size_splits=2, value=3)
-                z_lost, _ = tf.split(z_lost, 2, 3)
+                z_lost, _ = tf.split(axis=z_lost, num_or_size_splits=2, value=3)
-                res = tf.concat(2, [pad_2, res])
+                res = tf.concat(axis=1, values=[pad_1, res])
-    res = tf.concat(4, [
+        axis=2, values=[res, tf.zeros([batch_size, height, stride - 1, width, 1, channels])])
-    res = tf.split(0, batch_size, input_)
+    res = tf.split(axis=0, num_or_size_splits=batch_size, value=input_)
-    res = tf.concat(0, res)
+    res = tf.concat(axis=0, values=res)
-    indices_input = tf.concat(0, [indices, tf.reshape(input_, [-1])])
+    indices_input = tf.concat(axis=0, values=[indices, tf.reshape(input_, [-1])])
-    tf.scalar_summary(clone.scope + '/clone_loss', clone_loss,
+    tf.summary.scalar(clone.scope + '/clone_loss', clone_loss,
-    tf.scalar_summary('regularization_loss', regularization_loss,
+    tf.summary.scalar('regularization_loss', regularization_loss,
-      summaries.add(tf.scalar_summary('total_loss', total_loss,
+      summaries.add(tf.summary.scalar('total_loss', total_loss,
-      summary_op = tf.merge_summary(list(summaries), name='summary_op')
+      summary_op = tf.summary.merge(list(summaries), name='summary_op')
-      summaries.append(tf.histogram_summary(var.op.name + ':gradient',
+      summaries.append(tf.summary.histogram(var.op.name + ':gradient',
-      summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',
+      summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',
-      op = tf.scalar_summary(summary_name, value, collections=[])
+      op = tf.summary.scalar(summary_name, value, collections=[])
-                          biases_initializer=tf.zeros_initializer,
+                          biases_initializer=tf.zeros_initializer(),
-                                  biases_initializer=tf.zeros_initializer,
+                                  biases_initializer=tf.zeros_initializer(),
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-          branch_1 = tf.concat(3, [
+          branch_1 = tf.concat(axis=3, values=[
-          branch_2 = tf.concat(3, [
+          branch_2 = tf.concat(axis=3, values=[
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          branch_1 = tf.concat(3, [
+          branch_1 = tf.concat(axis=3, values=[
-          branch_2 = tf.concat(3, [
+          branch_2 = tf.concat(axis=3, values=[
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-      return tf.concat(3, [branch_0, branch_1, branch_2])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-      return tf.concat(3, [branch_0, branch_1, branch_2])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-        branch_1 = tf.concat(3, [
+        branch_1 = tf.concat(axis=3, values=[
-        branch_2 = tf.concat(3, [
+        branch_2 = tf.concat(axis=3, values=[
-      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1])
+        net = tf.concat(axis=3, values=[branch_0, branch_1])
-        net = tf.concat(3, [branch_0, branch_1])
+        net = tf.concat(axis=3, values=[branch_0, branch_1])
-        net = tf.concat(3, [branch_0, branch_1])
+        net = tf.concat(axis=3, values=[branch_0, branch_1])
-                      biases_initializer=tf.zeros_initializer):
+                      biases_initializer=tf.zeros_initializer()):
-                          biases_initializer=tf.zeros_initializer,
+                          biases_initializer=tf.zeros_initializer(),
-                      biases_initializer=tf.zeros_initializer):
+                      biases_initializer=tf.zeros_initializer()):
-  tf.image_summary('image', tf.expand_dims(image, 0))
+  tf.summary.image('image', tf.expand_dims(image, 0))
-  tf.image_summary('distorted_image', tf.expand_dims(distorted_image, 0))
+  tf.summary.image('distorted_image', tf.expand_dims(distorted_image, 0))
-  return tf.image.per_image_whitening(distorted_image)
+  return tf.image.per_image_standardization(distorted_image)
-  tf.image_summary('image', tf.expand_dims(image, 0))
+  tf.summary.image('image', tf.expand_dims(image, 0))
-  tf.image_summary('resized_image', tf.expand_dims(resized_image, 0))
+  tf.summary.image('resized_image', tf.expand_dims(resized_image, 0))
-  return tf.image.per_image_whitening(resized_image)
+  return tf.image.per_image_standardization(resized_image)
-    tf.image_summary('image_with_bounding_boxes', image_with_box)
+    tf.summary.image('image_with_bounding_boxes', image_with_box)
-    tf.image_summary('images_with_distorted_bounding_box',
+    tf.summary.image('images_with_distorted_bounding_box',
-    tf.image_summary('cropped_resized_image',
+    tf.summary.image('cropped_resized_image',
-    tf.image_summary('final_distorted_image',
+    tf.summary.image('final_distorted_image',
-    distorted_image = tf.mul(distorted_image, 2.0)
+    distorted_image = tf.subtract(distorted_image, 0.5)
-    image = tf.mul(image, 2.0)
+    image = tf.subtract(image, 0.5)
-  image = tf.sub(image, 128.0)
+  image = tf.subtract(image, 128.0)
-      tf.pack([crop_height, crop_width, original_shape[2]]))
+      tf.stack([crop_height, crop_width, original_shape[2]]))
-  offsets = tf.to_int32(tf.pack([offset_height, offset_width, 0]))
+  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))
-  channels = tf.split(2, num_channels, image)
+  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)
-  return tf.concat(2, channels)
+  return tf.concat(axis=2, values=channels)
-  summaries.append(tf.scalar_summary('training/Learning Rate', learning_rate))
+    summaries.append(tf.summary.histogram(variable.op.name, variable))
-      summaries.add(tf.scalar_summary('sparsity/' + end_point,
+      summaries.add(tf.summary.histogram('activations/' + end_point, x))
-      summaries.add(tf.scalar_summary('losses/%s' % loss.op.name, loss))
+      summaries.add(tf.summary.scalar('losses/%s' % loss.op.name, loss))
-      summaries.add(tf.histogram_summary(variable.op.name, variable))
+      summaries.add(tf.summary.histogram(variable.op.name, variable))
-      summaries.add(tf.scalar_summary('learning_rate', learning_rate,
+      summaries.add(tf.summary.scalar('learning_rate', learning_rate,
-    summaries.add(tf.scalar_summary('total_loss', total_loss,
+    summaries.add(tf.summary.scalar('total_loss', total_loss,
-    summary_op = tf.merge_summary(list(summaries), name='summary_op')
+    summary_op = tf.summary.merge(list(summaries), name='summary_op')
-      out = tf.concat(2, [forward, backward])
+      out = tf.concat(axis=2, values=[forward, backward])
-        inp = tf.reverse(inp, [False, True, False])
+        inp = tf.reverse(inp, [1])
-    prev = tf.reshape(inp, tf.pack([batch_size * num_frames, num_prev]))
+    prev = tf.reshape(inp, tf.stack([batch_size * num_frames, num_prev]))
-            initializer=tf.zeros_initializer,
+            initializer=tf.zeros_initializer(),
-    prev = tf.reshape(prev, tf.pack([batch_size, num_frames, 4, num_nodes]))
+    prev = tf.reshape(prev, tf.stack([batch_size, num_frames, 4, num_nodes]))
-      state = tf.fill(tf.pack([batch_size, num_nodes]), 0.0)
+      state = tf.fill(tf.stack([batch_size, num_nodes]), 0.0)
-      memory = tf.fill(tf.pack([batch_size, num_nodes]), 0.0)
+      memory = tf.fill(tf.stack([batch_size, num_nodes]), 0.0)
-        out = tf.reverse(out, [False, True, False])
+        out = tf.reverse(out, [1])
-  tf.image_summary('Images', images)
+  tf.summary.image('Images', images)
-  image = tf.mul(image, 1 / 100.0)
+  image = tf.subtract(image, 128.0)
-    sw = tf.train.SummaryWriter(eval_dir)
+    sw = tf.summary.FileWriter(eval_dir)
-      tf.scalar_summary('loss', self.loss, name='loss')
+      tf.summary.scalar('loss', self.loss, name='loss')
-    tf.scalar_summary('learn_rate', learn_rate_dec, name='lr_summ')
+    tf.summary.scalar('learn_rate', learn_rate_dec, name='lr_summ')
-      lengths = tf.mul(lengths, tf.cast(factor, tf.float32))
+      lengths = tf.multiply(lengths, tf.cast(factor, tf.float32))
-    return tf.concat(num_dims - 1, layers), index + 1
+    return tf.concat(axis=num_dims - 1, values=layers), index + 1
-        self.reduction_factors[i] = tf.div(tf.mul(factor1, factor2), divisor)
+        self.reduction_factors[i] = tf.div(tf.multiply(factor1, factor2), divisor)
-      return tf.concat(3, [fwd, back], name=name + '_concat'), m.end()
+      return tf.concat(axis=3, values=[fwd, back], name=name + '_concat'), m.end()
-                              tf.expand_dims(sparse_local_col, 1)], 1)
+  sparse_indices = tf.concat(axis=[tf.expand_dims(sparse_local_row, 1),
-    broadcast_weights_shape = tf.concat([tf.shape(weights), [1]], 0)
+    broadcast_weights_shape = tf.concat(axis=[tf.shape(weights), [1]], values=0)
-    last_layer = tf.concat(embeddings, 1)
+    last_layer = tf.concat(axis=embeddings, values=1)
-    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir)
+    summary_writer = tf.summary.FileWriter(FLAGS.train_dir)
-  summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir)
+  summary_writer = tf.summary.FileWriter(FLAGS.eval_dir)
-      loss_weights = tf.unpack(tf.transpose(self._loss_weights))
+      encoder_inputs = tf.unstack(tf.transpose(self._articles))
-        self._enc_top_states = tf.concat(1, encoder_outputs)
+        self._enc_top_states = tf.concat(axis=1, values=encoder_outputs)
-              1, [tf.reshape(x, [hps.batch_size, 1]) for x in best_outputs])
+              axis=1, values=[tf.reshape(x, [hps.batch_size, 1]) for x in best_outputs])
-        tf.scalar_summary('loss', tf.minimum(12.0, self._loss))
+        tf.summary.scalar('loss', tf.minimum(12.0, self._loss))
-    tf.scalar_summary('global_norm', global_norm)
+    tf.summary.scalar('global_norm', global_norm)
-    tf.scalar_summary('learning rate', self._lr_rate)
+    tf.summary.scalar('learning rate', self._lr_rate)
-    self._summaries = tf.merge_all_summaries()
+    self._summaries = tf.summary.merge_all()
-      res = tf.matmul(tf.concat(1, args), matrix)
+      res = tf.matmul(tf.concat(axis=1, values=args), matrix)
-                tf.expand_dims(tf.ones(shape=tf.pack([n_repeats, ])), 1), [1, 0])
+                tf.expand_dims(tf.ones(shape=tf.stack([n_repeats, ])), 1), [1, 0])
-            im_flat = tf.reshape(im, tf.pack([-1, channels]))
+            im_flat = tf.reshape(im, tf.stack([-1, channels]))
-            x_t = tf.matmul(tf.ones(shape=tf.pack([height, 1])),
+            x_t = tf.matmul(tf.ones(shape=tf.stack([height, 1])),
-                            tf.ones(shape=tf.pack([1, width])))
+                            tf.ones(shape=tf.stack([1, width])))
-            grid = tf.concat(0, [x_t_flat, y_t_flat, ones])
+            grid = tf.concat(axis=0, values=[x_t_flat, y_t_flat, ones])
-            grid = tf.reshape(grid, tf.pack([num_batch, 3, -1]))
+            grid = tf.tile(grid, tf.stack([num_batch]))
-            T_g = tf.batch_matmul(theta, grid)
+            T_g = tf.matmul(theta, grid)
-                input_transformed, tf.pack([num_batch, out_height, out_width, num_channels]))
+                input_transformed, tf.stack([num_batch, out_height, out_width, num_channels]))
-    true_logits = tf.reduce_sum(tf.mul(example_emb, true_w), 1) + true_b
+    true_logits = tf.reduce_sum(tf.multiply(example_emb, true_w), 1) + true_b
-    grad = tf.concat(grads, 0)
+    grad = tf.concat(axis=grads, values=0)
-    output = tf.reshape(tf.concat(outputs, 1), [-1, size])
+    output = tf.reshape(tf.concat(axis=outputs, values=1), [-1, size])
-               state_initializer=tf.zeros_initializer,
+               state_initializer=tf.zeros_initializer(),
-      tf.pack([batch_size] + state_shape),
+      tf.stack([batch_size] + state_shape),
-    inputs_h = tf.concat(3, [inputs, h])
+    c, h = tf.split(axis=3, num_or_size_splits=2, value=state)
-    i, j, f, o = tf.split(3, 4, i_j_f_o)
+    i, j, f, o = tf.split(axis=3, num_or_size_splits=4, value=i_j_f_o)
-    return new_h, tf.concat(3, [new_c, new_h])
+    return new_h, tf.concat(axis=3, values=[new_c, new_h])
-  image_seq = tf.concat(0, image_seq)
+  image_seq = tf.concat(axis=0, values=image_seq)
-    action_seq = tf.concat(0, action_seq)
+    state_seq = tf.concat(axis=0, values=state_seq)
-      state_action = tf.concat(1, [action, current_state])
+      state_action = tf.concat(axis=1, values=[action, current_state])
-        enc2 = tf.concat(3, [enc2, smear])
+        enc2 = tf.concat(axis=3, values=[enc2, smear])
-      hidden6 = tf.concat(3, [hidden6, enc1])  # both 16x16
+      hidden6 = tf.concat(axis=3, values=[hidden6, enc1])  # both 16x16
-      hidden7 = tf.concat(3, [hidden7, enc0])  # both 32x32
+      hidden7 = tf.concat(axis=3, values=[hidden7, enc0])  # both 32x32
-      mask_list = tf.split(3, num_masks + 1, masks)
+      mask_list = tf.split(axis=3, num_or_size_splits=num_masks + 1, value=masks)
-  prev_images = tf.split(0, batch_size, prev_image)
+  cdna_kerns = tf.split(axis=0, num_or_size_splits=batch_size, value=cdna_kerns)
-  transformed = tf.split(3, num_masks, transformed)
+  transformed = tf.concat(axis=0, values=transformed)
-  inputs = tf.concat(3, inputs)
+  inputs = tf.concat(axis=3, values=inputs)
-    actions = tf.split(1, actions.get_shape()[1], actions)
+    actions = tf.split(axis=1, num_or_size_splits=actions.get_shape()[1], value=actions)
-    states = tf.split(1, states.get_shape()[1], states)
+    states = tf.split(axis=1, num_or_size_splits=states.get_shape()[1], value=states)
-    images = tf.split(1, images.get_shape()[1], images)
+    images = tf.split(axis=1, num_or_size_splits=images.get_shape()[1], value=images)
-      summaries.append(tf.scalar_summary(prefix + '_psnr' + str(i), psnr_i))
+          tf.summary.scalar(prefix + '_recon_cost' + str(i), recon_cost))
-          tf.scalar_summary(prefix + '_state_cost' + str(i), state_cost))
+          tf.summary.scalar(prefix + '_state_cost' + str(i), state_cost))
-    summaries.append(tf.scalar_summary(prefix + '_psnr_all', psnr_all))
+    summaries.append(tf.summary.scalar(prefix + '_psnr_all', psnr_all))
-    summaries.append(tf.scalar_summary(prefix + '_loss', loss))
+    summaries.append(tf.summary.scalar(prefix + '_loss', loss))
-    self.summ_op = tf.merge_summary(summaries)
+    self.summ_op = tf.summary.merge(summaries)
-  summary_writer = tf.train.SummaryWriter(
+  summary_writer = tf.summary.FileWriter(
-        'Recall@5': slim.metrics.streaming_recall_at_k(
+        'Recall_5': slim.metrics.streaming_recall_at_k(
-            end_points['AuxLogits'], labels,
+            logits=end_points['AuxLogits'], onehot_labels=labels,
-          logits, labels, label_smoothing=FLAGS.label_smoothing, weights=1.0)
+          logits=logits, onehot_labels=labels,
-resnet_v2.default_image_size = 224
+        return logits, end_points
-          tf.squeeze(logits), labels, label_smoothing=FLAGS.label_smoothing, weights=1.0)
+          logits, labels, label_smoothing=FLAGS.label_smoothing, weights=1.0)
-                      name='clone_loss')
+    tf.summary.scalar('clone_loss', clone_loss)
-                      name='regularization_loss')
+    tf.summary.scalar('regularization_loss', regularization_loss)
-                                      name='total_loss'))
+      summaries.add(tf.summary.scalar('total_loss', total_loss))
-      summary_op = tf.merge_summary(list(summaries), name='summary_op')
+      summary_op = tf.summary.merge(list(summaries), name='summary_op')
-      summaries.append(tf.histogram_summary(var.op.name + ':gradient',
+      summaries.append(tf.summary.histogram(var.op.name + ':gradient',
-      summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',
+      summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',
-      op = tf.scalar_summary(summary_name, value, collections=[])
+      op = tf.summary.scalar(summary_name, value, collections=[])
-                          biases_initializer=tf.zeros_initializer,
+                          biases_initializer=tf.zeros_initializer(),
-                                  biases_initializer=tf.zeros_initializer,
+                                  biases_initializer=tf.zeros_initializer(),
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-          branch_1 = tf.concat(3, [
+          branch_1 = tf.concat(axis=3, values=[
-          branch_2 = tf.concat(3, [
+          branch_2 = tf.concat(axis=3, values=[
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-          branch_1 = tf.concat(3, [
+          branch_1 = tf.concat(axis=3, values=[
-          branch_2 = tf.concat(3, [
+          branch_2 = tf.concat(axis=3, values=[
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-      return tf.concat(3, [branch_0, branch_1, branch_2])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-      return tf.concat(3, [branch_0, branch_1, branch_2])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
-        branch_1 = tf.concat(3, [
+        branch_1 = tf.concat(axis=3, values=[
-        branch_2 = tf.concat(3, [
+        branch_2 = tf.concat(axis=3, values=[
-      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
-        net = tf.concat(3, [branch_0, branch_1])
+        net = tf.concat(axis=3, values=[branch_0, branch_1])
-        net = tf.concat(3, [branch_0, branch_1])
+        net = tf.concat(axis=3, values=[branch_0, branch_1])
-        net = tf.concat(3, [branch_0, branch_1])
+        net = tf.concat(axis=3, values=[branch_0, branch_1])
-                      biases_initializer=tf.zeros_initializer):
+                      biases_initializer=tf.zeros_initializer()):
-                          biases_initializer=tf.zeros_initializer,
+                          biases_initializer=tf.zeros_initializer(),
-
+resnet_v2_50.default_image_size = 224
-                      biases_initializer=tf.zeros_initializer):
+                      biases_initializer=tf.zeros_initializer()):
-  tf.image_summary('image', tf.expand_dims(image, 0))
+  tf.summary.image('image', tf.expand_dims(image, 0))
-  tf.image_summary('distorted_image', tf.expand_dims(distorted_image, 0))
+  tf.summary.image('distorted_image', tf.expand_dims(distorted_image, 0))
-  return tf.image.per_image_whitening(distorted_image)
+  return tf.image.per_image_standardization(distorted_image)
-  tf.image_summary('image', tf.expand_dims(image, 0))
+  tf.summary.image('image', tf.expand_dims(image, 0))
-  tf.image_summary('resized_image', tf.expand_dims(resized_image, 0))
+  tf.summary.image('resized_image', tf.expand_dims(resized_image, 0))
-  return tf.image.per_image_whitening(resized_image)
+  return tf.image.per_image_standardization(resized_image)
-    tf.image_summary('image_with_bounding_boxes', image_with_box)
+    tf.summary.image('image_with_bounding_boxes', image_with_box)
-    tf.image_summary('images_with_distorted_bounding_box',
+    tf.summary.image('images_with_distorted_bounding_box',
-    tf.image_summary('cropped_resized_image',
+    tf.summary.image('cropped_resized_image',
-    tf.image_summary('final_distorted_image',
+    tf.summary.image('final_distorted_image',
-    distorted_image = tf.mul(distorted_image, 2.0)
+    distorted_image = tf.subtract(distorted_image, 0.5)
-    image = tf.mul(image, 2.0)
+    image = tf.subtract(image, 0.5)
-  image = tf.sub(image, 128.0)
+  image = tf.subtract(image, 128.0)
-      tf.pack([crop_height, crop_width, original_shape[2]]))
+      tf.stack([crop_height, crop_width, original_shape[2]]))
-  offsets = tf.to_int32(tf.pack([offset_height, offset_width, 0]))
+  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))
-  channels = tf.split(2, num_channels, image)
+  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)
-  return tf.concat(2, channels)
+  return tf.concat(axis=2, values=channels)
-  summaries.append(tf.scalar_summary('training/Learning Rate', learning_rate))
+    summaries.append(tf.summary.histogram(variable.op.name, variable))
-        slim.losses.softmax_cross_entropy(
+        tf.losses.softmax_cross_entropy(
-          logits, labels, label_smoothing=FLAGS.label_smoothing, weight=1.0)
+            label_smoothing=FLAGS.label_smoothing, weights=0.4, scope='aux_loss')
-      summaries.add(tf.scalar_summary('sparsity/' + end_point,
+      summaries.add(tf.summary.histogram('activations/' + end_point, x))
-      summaries.add(tf.scalar_summary('losses/%s' % loss.op.name, loss))
+      summaries.add(tf.summary.scalar('losses/%s' % loss.op.name, loss))
-      summaries.add(tf.histogram_summary(variable.op.name, variable))
+      summaries.add(tf.summary.histogram(variable.op.name, variable))
-                                      name='learning_rate'))
+      summaries.add(tf.summary.scalar('learning_rate', learning_rate))
-                                    name='total_loss'))
+    summaries.add(tf.summary.scalar('total_loss', total_loss))
-    summary_op = tf.merge_summary(list(summaries), name='summary_op')
+    summary_op = tf.summary.merge(list(summaries), name='summary_op')
-    summary_writer: Instance of SummaryWriter.
+    summary_writer: Instance of FileWriter.
-  # Log perplexity to the SummaryWriter.
+  # Log perplexity to the FileWriter.
-    summary_writer: Instance of SummaryWriter.
+    summary_writer: Instance of FileWriter.
-    summary_writer = tf.train.SummaryWriter(eval_dir)
+    summary_op = tf.summary.merge_all()
-          size, forget_bias=0.0, state_is_tuple=True)
+          size, forget_bias=0.0, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)
-        if self._step % 10 == 0:
+        log_steps = FLAGS.log_steps_count
-          sec_per_batch = float(duration)
+          examples_per_sec = num_examples_per_step * FLAGS.log_steps_count / duration
-        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell] * config.num_layers)
+        lstm_cells = []
-import argparse
+from tensorflow.python.client import device_lib
-                   'Fraction of GPU memory to use')
+flags.DEFINE_float('per_process_gpu_memory_fraction', 0,
-      num_threads=4,
+      num_threads=FLAGS.num_readers,
-  sys.stdout.flush()
+  log('Writing row embeddings to: %s', row_embedding_output_path)
-  sys.stdout.flush()
+  log('Writing column embeddings to: %s', col_embedding_output_path)
-    sys.stdout.flush()
+    log('Reading model from: %s', config.input_base_path)
-    sys.stdout.flush()
+    log('Matrix dim: (%d,%d) SubMatrix dim: (%d,%d)',
-    self.saver = tf.train.Saver(sharded=True)
+    log('n_submatrices: %d', self.n_submatrices)
-        per_process_gpu_memory_fraction=FLAGS.per_process_gpu_memory_fraction)
+    gpu_opts = {}
-    n_steps_per_thread = n_total_steps / FLAGS.num_concurrent_steps
+    n_steps_per_thread = n_total_steps / (
-        if (global_step % n_steps_between_status_updates) == 0:
+        _, global_step, loss = sess.run((
-              global_step, n_submatrices_to_train,
+          log(msg, global_step, n_submatrices_to_train,
-          sys.stdout.flush()
+              n_steps_between_status_updates / elapsed, loss)
-    #Shutdown
+    # Shutdown
-  tarfile.open(filepath, 'r:gz').extractall(dest_directory)
+  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')
-      tf.summary.histogram('col_bias', self.col_bias)
+    # embeddings
-      selected_col_bias = tf.nn.embedding_lookup([self.col_bias], global_col)
+    global_row, global_col, count = count_matrix_input(
-  print 'Writing row embeddings to:', row_embedding_output_path
+  print('Writing row embeddings to:', row_embedding_output_path)
-  print 'Writing column embeddings to:', col_embedding_output_path
+  print('Writing column embeddings to:', col_embedding_output_path)
-    print 'Reading model from:', config.input_base_path
+    print('Reading model from:', config.input_base_path)
-        self.n_rows, self.n_cols, config.submatrix_rows, config.submatrix_cols)
+    print('Matrix dim: (%d,%d) SubMatrix dim: (%d,%d) ' % (
-    print 'n_submatrices: %d' % (self.n_submatrices)
+    print('n_submatrices: %d' % (self.n_submatrices))
-      for _ in range(n_steps_per_thread):
+      for _ in range(int(n_steps_per_thread)):
-          print '%d/%d submatrices trained (%.1f%%), %.1f submatrices/sec' % (
+          print('%d/%d submatrices trained (%.1f%%), %.1f submatrices/sec' % (
-              n_steps_between_status_updates / elapsed)
+              n_steps_between_status_updates / elapsed))
-        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(size, forget_bias=0.0)
+        lstm_cell = tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0)
-            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(
+            lstm_cell = tf.contrib.rnn.DropoutWrapper(
-        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)
+        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell] * config.num_layers)
-        output = tf.reshape(tf.concat(1, outputs), [-1, size])
+        output = tf.reshape(tf.concat(outputs, 1), [-1, size])
-        loss = tf.nn.seq2seq.sequence_loss_by_example(
+        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
-        self.saver = tf.train.Saver(tf.all_variables())
+        self.saver = tf.train.Saver(tf.global_variables())
-    broadcast_weights_shape = tf.concat_v2([tf.shape(weights), [1]], 0)
+    broadcast_weights_shape = tf.concat([tf.shape(weights), [1]], 0)
-            ema.average(param), tf.zeros_initializer)
+            ema.average(param), tf.zeros_initializer())
-    last_layer = tf.concat_v2(embeddings, 1)
+    last_layer = tf.concat(embeddings, 1)
-        tf.zeros_initializer,
+        tf.zeros_initializer(),
-                                                        tf.zeros_initializer)
+                                                        tf.zeros_initializer())
-        tf.strided_slice(n['gold_slot'], [beam_id], [beam_id + 1], [1]), [1])
+        tf.strided_slice(n['gold_slot'], [beam_id], [beam_id + 1]), [1])
-          tf.zeros_initializer)
+          tf.zeros_initializer())
-                                                        tf.zeros_initializer)
+                                                        tf.zeros_initializer())
-    print(output_str)
+    tf.logging.info('loss: %.3f, precision: %.3f, best precision: %.3f' %
-    tf.logging.info('image after unit %s', x.get_shape())
+    tf.logging.debug('image after unit %s', x.get_shape())
-            png_string, 'png', _IMAGE_SIZE, _IMAGE_SIZE, labels[j])
+            png_string, 'png'.encode(), _IMAGE_SIZE, _IMAGE_SIZE, labels[j])
-        size = f.Size()
+        size = f.size()
-  int_codes = (int_codes + 1)/2
+  int_codes = (int_codes + 1)//2
-        size = f.Size()
+        size = f.size()
-        init = tf.initialize_all_variables()
+        init = tf.global_variables_initializer()
-        init = tf.initialize_all_variables()
+        init = tf.global_variables_initializer()
-        init = tf.initialize_all_variables()
+        init = tf.global_variables_initializer()
-        init = tf.initialize_all_variables()
+        init = tf.global_variables_initializer()
-    sess.run(tf.initialize_all_variables())
+    sess.run(tf.global_variables_initializer())
-    init = tf.initialize_all_variables()
+    init = tf.global_variables_initializer()
-  be initialized with tf.initialize_all_variables() or similar calls.
+  be initialized with tf.global_variables_initializer() or similar calls.
-      init_op = tf.initialize_all_variables()
+      init_op = tf.global_variables_initializer()
-    init = tf.initialize_all_variables()
+    init = tf.global_variables_initializer()
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-        tf.initialize_all_variables().run()
+        tf.global_variables_initializer().run()
-    sess.run(tf.initialize_all_variables())
+    sess.run(tf.global_variables_initializer())
-    self.init_op = tf.initialize_all_variables()
+    self.init_op = tf.global_variables_initializer()
-        sess.run(tf.initialize_all_variables())
+        sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      tf.initialize_all_variables().run()
+      tf.global_variables_initializer().run()
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      tf.initialize_all_variables().run()
+      tf.global_variables_initializer().run()
-      tf.initialize_all_variables().run()
+      tf.global_variables_initializer().run()
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      tf.initialize_all_variables().run()
+      tf.global_variables_initializer().run()
-      tf.initialize_all_variables().run()
+      tf.global_variables_initializer().run()
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      tf.initialize_all_variables().run()
+      tf.global_variables_initializer().run()
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-              sess.run(tf.initialize_all_variables())
+              sess.run(tf.global_variables_initializer())
-            sess.run(tf.initialize_all_variables())
+            sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-              sess.run(tf.initialize_all_variables())
+              sess.run(tf.global_variables_initializer())
-            sess.run(tf.initialize_all_variables())
+            sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-      tf.initialize_all_variables().run(session=sess)
+      tf.global_variables_initializer().run(session=sess)
-      tf.initialize_all_variables().run(session=sess)
+      tf.global_variables_initializer().run(session=sess)
-      tf.initialize_all_variables().run(session=sess)
+      tf.global_variables_initializer().run(session=sess)
-        tf.initialize_all_variables().run()
+        tf.global_variables_initializer().run()
-sess.run(tf.initialize_all_variables())
+sess.run(tf.global_variables_initializer())
-sess.run(tf.initialize_all_variables())
+sess.run(tf.global_variables_initializer())
-  sess.run(tf.initialize_all_variables())
+  sess.run(tf.global_variables_initializer())
-    mixed = tf.concat(3, [tower_conv, tower_conv1_1, tower_conv2_2])
+    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])
-    mixed = tf.concat(3, [tower_conv, tower_conv1_2])
+    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])
-    mixed = tf.concat(3, [tower_conv, tower_conv1_2])
+    mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])
-          net = tf.concat(3, [tower_conv, tower_conv1_1,
+          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_1,
-          net = tf.concat(3, [tower_conv, tower_conv1_2, tower_pool])
+          net = tf.concat(axis=3, values=[tower_conv, tower_conv1_2, tower_pool])
-          net = tf.concat(3, [tower_conv_1, tower_conv1_1,
+          net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1,
-                                 tf.expand_dims(sparse_local_col, 1)])
+  sparse_indices = tf.concat([tf.expand_dims(sparse_local_row, 1),
-      tf.histogram_summary('col_emb', self.col_embedding)
+      tf.summary.histogram('row_emb', self.row_embedding)
-      tf.histogram_summary('col_bias', self.col_bias)
+      tf.summary.histogram('row_bias', self.row_bias)
-    tf.scalar_summary("loss", self.loss)
+    tf.summary.scalar("l2_loss", l2_loss)
-    sess.run(tf.initialize_all_variables())
+    sess.run(tf.global_variables_initializer())
-                    (loss, precision, best_precision))
+    output_str = 'loss: %.3f, precision: %.3f, best precision: %.3f\n' % \
-      tf.concat_v2(values=[indices, labels], axis=1),
+      tf.concat(values=[indices, labels], axis=1),
-        tf.histogram_summary(variance.op.name, variance)
+        tf.summary.histogram(mean.op.name, mean)
-        # tf.histogram_summary(var.op.name, var)
+        # tf.summary.histogram(var.op.name, var)
-        tf.concat_v2(initial_state, 1, name="initial_state")
+        tf.concat(initial_state, 1, name="initial_state")
-        tf.concat_v2(state_tuple, 1, name="state")
+        tf.concat(state_tuple, 1, name="state")
-        net = slim.avg_pool2d(net, [7, 7], stride=1, scope='MaxPool_0a_7x7')
+        net = slim.avg_pool2d(net, [7, 7], stride=1, scope='AvgPool_0a_7x7')
-      tf.concat_v2(values=[indices, labels], axis=1),
+      tf.concat(values=[indices, labels], axis=1),
-    output = tf.reshape(tf.concat_v2(outputs, 1), [-1, size])
+    output = tf.reshape(tf.concat(outputs, 1), [-1, size])
-                                                         width, height)
+                                                         height, width)
-          tower_grads.append(grads)
+    with tf.variable_scope(tf.get_variable_scope()):
-    tf.scalar_summary("NCE loss", loss)
+    tf.summary.scalar("NCE loss", loss)
-  tf.contrib.deprecated.scalar_summary(tensor_name + '/sparsity',
+  tf.summary.histogram(tensor_name + '/activations', x)
-    tf.contrib.deprecated.scalar_summary(l.op.name, loss_averages.average(l))
+    tf.summary.scalar(l.op.name + ' (raw)', l)
-  tf.contrib.deprecated.scalar_summary('learning_rate', lr)
+  tf.summary.scalar('learning_rate', lr)
-    tf.contrib.deprecated.histogram_summary(var.op.name, var)
+    tf.summary.histogram(var.op.name, var)
-      tf.contrib.deprecated.histogram_summary(var.op.name + '/gradients', grad)
+      tf.summary.histogram(var.op.name + '/gradients', grad)
-  tf.contrib.deprecated.image_summary('images', images)
+  tf.summary.image('images', images)
-    tf.contrib.deprecated.scalar_summary(loss_name, l)
+    tf.summary.scalar(loss_name, l)
-    summaries.append(tf.contrib.deprecated.scalar_summary('learning_rate', lr))
+    summaries.append(tf.summary.scalar('learning_rate', lr))
-                                                    grad))
+        summaries.append(tf.summary.histogram(var.op.name + '/gradients', grad))
-          tf.contrib.deprecated.histogram_summary(var.op.name, var))
+      summaries.append(tf.summary.histogram(var.op.name, var))
-    summary_op = tf.contrib.deprecated.merge_summary(summaries)
+    summary_op = tf.summary.merge(summaries)
-      tf.scalar_summary("Learning Rate", m.lr)
+      tf.summary.scalar("Training Loss", m.cost)
-      tf.scalar_summary("Validation Loss", mvalid.cost)
+      tf.summary.scalar("Validation Loss", mvalid.cost)
-    tf.scalar_summary("NCE loss", loss)
+    tf.summary.scalar("NCE loss", loss)
-      def sampled_loss(labels, inputs):
+      def sampled_loss(inputs,labels):
-      return tf.contrib.rnn.GRUCell(size)
+      return tf.nn.rnn_cell.GRUCell(size)
-        return tf.contrib.rnn.BasicLSTMCell(size)
+        return tf.nn.rnn_cell.BasicLSTMCell(size)
-      cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])
+      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])
-      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(
+      return tf.nn.seq2seq.embedding_attention_seq2seq(
-      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
+      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(
-      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
+      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(
-    grad = tf.concat_v2(grads, 0)
+    grad = tf.concat(grads, 0)
-    candidate = tf.tanh_hard(conv_lin(inpts + [reset * mem], "c", 0.0))
+    candidate = tanh_hard(conv_lin(inpts + [reset * mem], "c", 0.0))
-      adam = tf.train.AdamOptimizer(adam_lr, epsilon=2e-4)
+      adam = tf.train.AdamOptimizer(adam_lr, epsilon=1e-3)
-tf.app.flags.DEFINE_float("dropout", 0.0, "Dropout that much.")
+tf.app.flags.DEFINE_float("dropout", 0.1, "Dropout that much.")
-    dev_set = read_data(en_dev, fr_dev, data.bins)
+    dev_set = {}
-      tf.concat(values=[indices, labels], axis=1),
+      tf.concat_v2(values=[indices, labels], axis=1),
-    for _ in xrange(FLAGS.eval_batch_count):
+    for _ in six.moves.range(FLAGS.eval_batch_count):
-      tf.concat(1, [indices, labels]),
+      tf.concat(values=[indices, labels], axis=1),
-    for i in xrange(1, self.hps.num_residual_units):
+    for i in six.moves.range(1, self.hps.num_residual_units):
-    for i in xrange(1, self.hps.num_residual_units):
+    for i in six.moves.range(1, self.hps.num_residual_units):
-    for i in xrange(1, self.hps.num_residual_units):
+    for i in six.moves.range(1, self.hps.num_residual_units):
-          logits, self.labels)
+          logits=logits, labels=self.labels)
-    return tf.mul(self.hps.weight_decay_rate, tf.add_n(costs))
+    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))
-    return tf.select(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')
+    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')
-"""Convolutional Gated Recurrent Networks for Algorithm Learning."""
+"""Neural GPU -- data generation and batching utilities."""
-from tensorflow.python.platform import gfile
+import program_utils
-bins = [8, 12, 16, 20, 24, 28, 32, 36, 40, 48, 64, 128]
+bins = [2 + bin_idx_i for bin_idx_i in xrange(256)]
-forward_max = 128
+             "badd", "qadd", "search", "progeval", "progsynth"]
-  return forward_max
+  return bins[-1]
-  for case in xrange(nbr_cases):
+
-      train_set[task][len(i)].append([i, t])
+      train_set[task][bin_for(len(i))].append([[[], i, [], []], [t]])
-      test_set[task][len(i)].append([i, t])
+      test_set[task][bin_for(len(i))].append([[[], i, [], []], [t]])
-      train_set[task][len(i)].append([i, t])
+      train_set[task][bin_for(len(i))].append([[i], [t]])
-      test_set[task][len(i)].append([i, t])
+      test_set[task][bin_for(len(i))].append([[i], [t]])
-      train_set[task][len(i)].append([i, t])
+      train_set[task][bin_for(len(i))].append([[i], [t]])
-      test_set[task][len(i)].append([i, t])
+      test_set[task][bin_for(len(i))].append([[i], [t]])
-      train_set[task][len(i)].append([i, t])
+      train_set[task][bin_for(len(i))].append([[i], [t]])
-      test_set[task][len(i)].append([i, t])
+      test_set[task][bin_for(len(i))].append([[i], [t]])
-      train_set[task][len(i)].append([i, t])
+      train_set[task][bin_for(len(i))].append([[i], [t]])
-    else:
+      test_set[task][bin_for(len(i))].append([[i], [t]])
-      train_set[task][l].append([inp, target])
+      train_set[task][bin_for(l)].append([[inp], [target]])
-      test_set[task][l].append([inp, target])
+      test_set[task][bin_for(l)].append([[inp], [target]])
-def get_batch(max_length, batch_size, do_train, task, offset=None, preset=None):
+def get_batch(bin_id, batch_size, data_set, height, offset=None, preset=None):
-  pad_length = pad(length)
+  inputs, targets = [], []
-        elem = cur_set[length][offset + b]
+      elem = random.choice(data_set[bin_id])
-    res_target.append(new_target)
+    inpt, targett, inpl, targetl = elem[0], elem[1], [], []
-      with gfile.GFile(log_filename, mode="a") as f:
+      with tf.gfile.GFile(log_filename, mode="a") as f:
-      sys.stdout.write("Error appending to %s\n" % log_filename)
+      sys.stderr.write("Error appending to %s\n" % log_filename)
-def accuracy(inpt, output, target, batch_size, nprint):
+def accuracy(inpt_t, output, target_t, batch_size, nprint,
-    print_out("    i: " + " ".join([str(i - 1) for i in inp if i > 0]))
+    print_out("    i: " + " ".join([tok(i) for i in inp if i > 0]))
-              " ".join([str(output[l] - 1) for l in xrange(print_len)]))
+              " ".join([tok(output[l]) for l in xrange(print_len)]))
-              " ".join([str(target[l] - 1) for l in xrange(print_len)]))
+              " ".join([tok(target[l]) for l in xrange(print_len)]))
-import data_utils
+from tensorflow.python.framework import function
-def conv_linear(args, kw, kh, nin, nout, do_bias, bias_start, prefix):
+
-    k = tf.get_variable("CvK", [kw, kh, nin, nout])
+    with tf.device("/cpu:0"):
-      res = tf.nn.conv2d(args[0], k, [1, 1, 1, 1], "SAME")
+      arg = args[0]
-      res = tf.nn.conv2d(tf.concat(3, args), k, [1, 1, 1, 1], "SAME")
+      arg = tf.concat(args, 3)
-    return res + bias_term + bias_start
+    with tf.device("/cpu:0"):
-  return tf.minimum(1.0, tf.maximum(0.0, cutoff * y - d))
+  return tf.minimum(1.0, tf.maximum(0.0, cutoff * y - d), name="cutoff_min")
-def conv_gru(inpts, mem, kw, kh, nmaps, cutoff, prefix):
+@function.Defun(tf.float32, noinline=True)
-  gate = sigmoid_cutoff(conv_lin(inpts + [mem], "g", 1.0), cutoff)
+    total_args_len = args_len or len(args) * nmaps
-def make_dense(targets, noclass):
+def autoenc_quantize(x, nbits, nmaps, do_training, layers=1):
-  return tf.reshape(tf.slice(reshaped, [0, 0], [-1, 1]), [-1])
+  low = low_param / float(noclass - 1)
-               learning_rate, pull, pull_incr, min_length, act_noise=0.0):
+  def __init__(self, nmaps, vec_size, niclass, noclass, dropout,
-    self.pull_incr_op = self.pull.assign(self.pull * pull_incr)
+    self.nmaps = nmaps
-    self.updates = []
+    self.input = tf.placeholder(tf.int32, name="inp")
-    batch_size = inp0_shape[0]
+      if beam_size > 0:
-      if length > data_utils.bins[0]:
+      output_w = tf.get_variable("output_w", [nmaps, noclass], tf.float32)
-           get_steps=False):
+    self.updates = []
-    length = len(target)
+    batch_size, height, length = inp.shape[0], inp.shape[1], inp.shape[2]
-    res = sess.run(feed_out, feed_in)
+    if train_mode:
-    return res[offset], outputs, norm, steps
+    if train_mode:
-"""Neural GPU for Learning Algorithms."""
+"""Neural GPU."""
-
+import program_utils
-import neural_gpu
+import neural_gpu as ngpu
-tf.app.flags.DEFINE_float("max_grad_norm", 1.0, "Clip gradients to this norm.")
+tf.app.flags.DEFINE_float("lr", 0.1, "Learning rate.")
-tf.app.flags.DEFINE_float("dropout", 0.15, "Dropout that much.")
+tf.app.flags.DEFINE_float("curriculum_ppx", 9.9, "Move curriculum if ppl < X.")
-tf.app.flags.DEFINE_integer("rx_step", 6, "Relax that many recursive steps.")
+tf.app.flags.DEFINE_integer("steps_per_checkpoint", 100, "Steps per epoch.")
-tf.app.flags.DEFINE_integer("jobid", -1, "Task id when running on borg.")
+tf.app.flags.DEFINE_integer("mem_size", -1, "Memory size (sqrt)")
-tf.app.flags.DEFINE_bool("animate", False, "Whether to produce an animation.")
+tf.app.flags.DEFINE_bool("atrous", False, "Whether to use atrous convs.")
-tf.app.flags.DEFINE_string("task", "rev", "Which task are we learning?")
+tf.app.flags.DEFINE_bool("do_train", True, "If false, only update memory.")
-tf.app.flags.DEFINE_string("ensemble", "", "Model paths for ensemble.")
+tf.app.flags.DEFINE_string("test_file_prefix", "", "Files to test (.en,.fr).")
-EXTRA_EVAL = 12
+EXTRA_EVAL = 10
-def initialize(sess):
+def initialize(sess=None):
-  data.print_out("NN ", newline=False)
+  global MAXLEN_F
-  np.random.seed(seed)
+  if FLAGS.random_seed > 0:
-  min_length = 3
+  while len(data.bins) > 1 and data.bins[-2] >= max_length + EXTRA_EVAL:
-  while len(data.bins) > 1 and data.bins[-2] > max_length + EXTRA_EVAL:
+  while len(data.bins) > 1 and data.bins[-2] >= max_length + EXTRA_EVAL:
-  if not gfile.IsDirectory(checkpoint_dir):
+  if FLAGS.mode == 0 or FLAGS.task < 0:
-    gfile.MkDir(checkpoint_dir)
+    tf.gfile.MkDir(checkpoint_dir)
-  data.print_out("Initialized variables.")
+      tf.orthogonal_initializer(gain=1.8 * init_weight))
-  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):
+  if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path + ".index"):
-      ensemble.append(ckpt.model_checkpoint_path)
+  elif sv is None:
-                offset=None, ensemble=None, get_steps=False):
+  return (model, beam_model, min_length, max_length, checkpoint_dir,
-  errors, total, seq_err = data.accuracy(inpt, res, target, batch_size, nprint)
+  if not dev[p][bin_id]:
-  return errors, seq_err
+    data.print_out("  bin %d (%d)\t%s\tppl %.2f errors %.2f seq-errors %.2f"
-    quant_op = neural_gpu.quantize_weights_op(512, 8)
+  batch_size = FLAGS.batch_size * FLAGS.num_gpus
-    prev_acc_perp = [1000000 for _ in xrange(3)]
+    prev_acc_perp = [1000000 for _ in xrange(5)]
-      acc_grad_norm, step_count, step_time = 0.0, 0, 0.0
+    while not sv.ShouldStop():
-          l = max(l, l1)
+        step_count += 1
-        noise_param = math.sqrt(math.pow(global_step, -0.55) *
+        inp, target = data.get_batch(bucket_id, batch_size, train_set,
-        loss, res, gnorm, _ = model.step(sess, inp, target, True, noise_param)
+        # In multi-step mode, we use best from beam for middle steps.
-          acc_seq_err += seq_err
+        acc_grad_norm += 0.0 if gnorm is None else float(gnorm)
-                     (msg3, max_cur_length, data.safe_exp(acc_loss),
+      t_size = float(sum([len(x) for x in train_set])) / float(1000000)
-      if curriculum > acc_seq_err:
+      is_good = FLAGS.curriculum_ppx > data.safe_exp(acc_loss)
-            if data.train_set[t]: do_incr = False
+        sess.run(model.cur_length_incr_op)
-      # Lower learning rate if we're worse than the last 3 checkpoints.
+      # Lower learning rate if we're worse than the last 5 checkpoints.
-      if acc_perp > max(prev_acc_perp[-3:]):
+      if acc_perp > max(prev_acc_perp[-5:]) and is_chief:
-  animation.save("/tmp/neural_gpu.mp4", writer="mencoder", fps=4*fps, dpi=3*80)
+      if is_chief:
-                   batch_size * 64, 0, ensemble=ensemble)
+  batch_size = FLAGS.batch_size * FLAGS.num_gpus
-    sys.stdout.write("Input to Neural GPU, e.g., 0 1. Use -1 for PAD.\n")
+  with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:
-    inpt = sys.stdin.readline()
+    inpt = sys.stdin.readline(), ""
-      print "  " + " ".join([data.to_symbol(output[0]) for output in res])
+      cures = []
-      inpt = sys.stdin.readline()
+      inpt = sys.stdin.readline(), ""
-    broadcast_weights_shape = tf.concat(0, [tf.shape(weights), [1]])
+    broadcast_weights_shape = tf.concat_v2([tf.shape(weights), [1]], 0)
-    last_layer = tf.concat(1, embeddings)
+    last_layer = tf.concat_v2(embeddings, 1)
-            logits, dense_golden)), batch_size)
+        tf.reduce_sum(
-    beam_gold_slot = tf.reshape(tf.slice(n['gold_slot'], [beam_id], [1]), [1])
+    beam_gold_slot = tf.reshape(
-              tf.sparse_to_dense(beam_gold_slot, num, [1.], 0.), 0))
+          labels=tf.expand_dims(
-  tf.scalar_summary(tensor_name + '/sparsity',
+  tf.contrib.deprecated.histogram_summary(tensor_name + '/activations', x)
-    weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')
+    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')
-    tf.scalar_summary(l.op.name, loss_averages.average(l))
+    tf.contrib.deprecated.scalar_summary(l.op.name + ' (raw)', l)
-  tf.scalar_summary('learning_rate', lr)
+  tf.contrib.deprecated.scalar_summary('learning_rate', lr)
-    tf.histogram_summary(var.op.name, var)
+    tf.contrib.deprecated.histogram_summary(var.op.name, var)
-      tf.histogram_summary(var.op.name + '/gradients', grad)
+      tf.contrib.deprecated.histogram_summary(var.op.name + '/gradients', grad)
-  tf.image_summary('images', images)
+  tf.contrib.deprecated.image_summary('images', images)
-    tf.scalar_summary(loss_name, l)
+    tf.contrib.deprecated.scalar_summary(loss_name, l)
-    summaries.append(tf.scalar_summary('learning_rate', lr))
+    summaries.append(tf.contrib.deprecated.scalar_summary('learning_rate', lr))
-            tf.histogram_summary(var.op.name + '/gradients',
+            tf.contrib.deprecated.histogram_summary(var.op.name + '/gradients',
-          tf.histogram_summary(var.op.name, var))
+          tf.contrib.deprecated.histogram_summary(var.op.name, var))
-    summary_op = tf.merge_summary(summaries)
+    summary_op = tf.contrib.deprecated.merge_summary(summaries)
-  from_vocab_path = os.path.join(data_dir, "vocab%d" % from_vocabulary_size)
+  to_vocab_path = os.path.join(data_dir, "vocab%d.to" % to_vocabulary_size)
-  for i in xrange(1, 16):
+  for i in range(1, 16):
-  return ['loop_{0:02d}/add:0'.format(i) for i in xrange(0, 16)]
+  return ['loop_{0:02d}/add:0'.format(i) for i in range(0, 16)]
-           '--model=residual_gru.pb\n\n')
+    print('\nUsage: python decoder.py --input_codes=output_codes.pkl '
-           'from file.\n')
+    print('\n--iteration must be between 0 and 15 inclusive, or -1 to infer '
-    print '\nInput codes not found.\n'
+    print('\nInput codes not found.\n')
-  for i in xrange(1, 16):
+  for i in range(1, 16):
-           '--model=residual_gru.pb\n\n')
+    print('\nUsage: python encoder.py --input_image=/your/image/here.png '
-    print '\n--iteration must be between 0 and 15 inclusive.\n'
+    print('\n--iteration must be between 0 and 15 inclusive.\n')
-  for _ in xrange(levels):
+  for _ in range(levels):
-           '--compared_image=distorted.png\n\n')
+    print('\nUsage: python msssim.py --original_image=original.png '
-    print '\nCannot find --original_image.\n'
+    print('\nCannot find --original_image.\n')
-    print '\nCannot find --compared_image.\n'
+    print('\nCannot find --compared_image.\n')
-  print MultiScaleSSIM(img1, img2, max_val=255)
+  print((MultiScaleSSIM(img1, img2, max_val=255)))
-  tf.scalar_summary(tensor_name + '/sparsity', tf.nn.zero_fraction(x))
+  tf.contrib.deprecated.histogram_summary(tensor_name + '/activations', x)
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([inputs], scope, 'inception_v3'):
+  with tf.name_scope(scope, 'inception_v3', [inputs]):
-          net = tf.concat(3, [branch1x1, branch5x5, branch3x3dbl, branch_pool])
+          net = tf.concat([branch1x1, branch5x5, branch3x3dbl, branch_pool], 3)
-          net = tf.concat(3, [branch1x1, branch5x5, branch3x3dbl, branch_pool])
+          net = tf.concat([branch1x1, branch5x5, branch3x3dbl, branch_pool], 3)
-          net = tf.concat(3, [branch1x1, branch5x5, branch3x3dbl, branch_pool])
+          net = tf.concat([branch1x1, branch5x5, branch3x3dbl, branch_pool], 3)
-          net = tf.concat(3, [branch3x3, branch3x3dbl, branch_pool])
+          net = tf.concat([branch3x3, branch3x3dbl, branch_pool], 3)
-          net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool])
+          net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)
-          net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool])
+          net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)
-          net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool])
+          net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)
-          net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool])
+          net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)
-          net = tf.concat(3, [branch3x3, branch7x7x3, branch_pool])
+          net = tf.concat([branch3x3, branch7x7x3, branch_pool], 3)
-                                      ops.conv2d(branch3x3, 384, [3, 1])])
+            branch3x3 = tf.concat([ops.conv2d(branch3x3, 384, [1, 3]),
-                                         ops.conv2d(branch3x3dbl, 384, [3, 1])])
+            branch3x3dbl = tf.concat([ops.conv2d(branch3x3dbl, 384, [1, 3]),
-          net = tf.concat(3, [branch1x1, branch3x3, branch3x3dbl, branch_pool])
+          net = tf.concat([branch1x1, branch3x3, branch3x3dbl, branch_pool], 3)
-                                      ops.conv2d(branch3x3, 384, [3, 1])])
+            branch3x3 = tf.concat([ops.conv2d(branch3x3, 384, [1, 3]),
-                                         ops.conv2d(branch3x3dbl, 384, [3, 1])])
+            branch3x3dbl = tf.concat([ops.conv2d(branch3x3dbl, 384, [1, 3]),
-          net = tf.concat(3, [branch1x1, branch3x3, branch3x3dbl, branch_pool])
+          net = tf.concat([branch1x1, branch3x3, branch3x3dbl, branch_pool], 3)
-      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope='on_cpu'):
+      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
-      for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope='on_gpu'):
+      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-    with tf.op_scope([tensor], scope, 'L1Regularizer'):
+    with tf.name_scope(scope, 'L1Regularizer', [tensor]):
-      return tf.mul(l1_weight, tf.reduce_sum(tf.abs(tensor)), name='value')
+      return tf.multiply(l1_weight, tf.reduce_sum(tf.abs(tensor)), name='value')
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-    with tf.op_scope([tensor], scope, 'L2Regularizer'):
+    with tf.name_scope(scope, 'L2Regularizer', [tensor]):
-      return tf.mul(l2_weight, tf.nn.l2_loss(tensor), name='value')
+      return tf.multiply(l2_weight, tf.nn.l2_loss(tensor), name='value')
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-    with tf.op_scope([tensor], scope, 'L1L2Regularizer'):
+    with tf.name_scope(scope, 'L1L2Regularizer', [tensor]):
-      reg_l1 = tf.mul(weight_l1_t, tf.reduce_sum(tf.abs(tensor)),
+      reg_l1 = tf.multiply(weight_l1_t, tf.reduce_sum(tf.abs(tensor)),
-      reg_l2 = tf.mul(weight_l2_t, tf.nn.l2_loss(tensor),
+      reg_l2 = tf.multiply(weight_l2_t, tf.nn.l2_loss(tensor),
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([tensor], scope, 'L1Loss'):
+  with tf.name_scope(scope, 'L1Loss', [tensor]):
-    loss = tf.mul(weight, tf.reduce_sum(tf.abs(tensor)), name='value')
+    loss = tf.multiply(weight, tf.reduce_sum(tf.abs(tensor)), name='value')
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([tensor], scope, 'L2Loss'):
+  with tf.name_scope(scope, 'L2Loss', [tensor]):
-    loss = tf.mul(weight, tf.nn.l2_loss(tensor), name='value')
+    loss = tf.multiply(weight, tf.nn.l2_loss(tensor), name='value')
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([logits, one_hot_labels], scope, 'CrossEntropyLoss'):
+  with tf.name_scope(scope, 'CrossEntropyLoss', [logits, one_hot_labels]):
-                                                            name='xentropy')
+    cross_entropy = tf.contrib.nn.deprecated_flipped_softmax_cross_entropy_with_logits(
-    loss = tf.mul(weight, tf.reduce_mean(cross_entropy), name='value')
+    loss = tf.multiply(weight, tf.reduce_mean(cross_entropy), name='value')
-    scope: Optional scope for variable_op_scope.
+    scope: Optional scope for variable_scope.
-  with tf.variable_op_scope([inputs], scope, 'BatchNorm', reuse=reuse):
+  with tf.variable_scope(scope, 'BatchNorm', [inputs], reuse=reuse):
-    scope: Optional scope for variable_op_scope.
+    scope: Optional scope for variable_scope.
-  with tf.variable_op_scope([inputs], scope, 'Conv', reuse=reuse):
+  with tf.variable_scope(scope, 'Conv', [inputs], reuse=reuse):
-    scope: Optional scope for variable_op_scope.
+    scope: Optional scope for variable_scope.
-  with tf.variable_op_scope([inputs], scope, 'FC', reuse=reuse):
+  with tf.variable_scope(scope, 'FC', [inputs], reuse=reuse):
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([labels], scope, 'OneHotEncoding'):
+  with tf.name_scope(scope, 'OneHotEncoding', [labels]):
-    concated = tf.concat(1, [indices, labels])
+    concated = tf.concat([indices, labels], 1)
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([inputs], scope, 'MaxPool'):
+  with tf.name_scope(scope, 'MaxPool', [inputs]):
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([inputs], scope, 'AvgPool'):
+  with tf.name_scope(scope, 'AvgPool', [inputs]):
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-    with tf.op_scope([inputs], scope, 'Dropout'):
+    with tf.name_scope(scope, 'Dropout', [inputs]):
-    scope: Optional scope for op_scope.
+    scope: Optional scope for name_scope.
-  with tf.op_scope([inputs], scope, 'Flatten'):
+  with tf.name_scope(scope, 'Flatten', [inputs]):
-  with tf.variable_op_scope([inputs], scope, 'RepeatOp'):
+  with tf.variable_scope(scope, 'RepeatOp', [inputs]):
-  candidates = tf.get_collection(tf.GraphKeys.VARIABLES, name)
+  candidates = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, name)
-        tf.GraphKeys.VARIABLES,
+        tf.GraphKeys.GLOBAL_VARIABLES,
-      Note that the variable is always also added to the tf.GraphKeys.VARIABLES
+      Note that the variable is always also added to the tf.GraphKeys.GLOBAL_VARIABLES
-  collections += [tf.GraphKeys.VARIABLES, MODEL_VARIABLES]
+  # Make sure variables are added to tf.GraphKeys.GLOBAL_VARIABLES and MODEL_VARIABLES
-                                            normalize_digits)
+          token_ids = sentence_to_token_ids(tf.compat.as_bytes(line), vocab,
-                                       num_samples, self.target_vocab_size),
+            tf.nn.sampled_softmax_loss(
-      return tf.nn.rnn_cell.GRUCell(size)
+      return tf.contrib.rnn.GRUCell(size)
-        return tf.nn.rnn_cell.BasicLSTMCell(size)
+        return tf.contrib.rnn.BasicLSTMCell(size)
-      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])
+      cell = tf.contrib.rnn.MultiRNNCell([single_cell() for _ in range(num_layers)])
-      return tf.nn.seq2seq.embedding_attention_seq2seq(
+      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(
-      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(
+      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
-      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(
+      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
-        size, forget_bias=0.0, state_is_tuple=True)
+    def lstm_cell():
-          lstm_cell, output_keep_prob=config.keep_prob)
+      def attn_cell():
-        [lstm_cell] * config.num_layers, state_is_tuple=True)
+        [attn_cell() for _ in range(config.num_layers)], state_is_tuple=True)
-    single_cell = tf.nn.rnn_cell.GRUCell(size)
+    def single_cell():
-    cell = single_cell
+      def single_cell():
-      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)
+      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])
-                              initializer=tf.zeros_initializer,
+                              initializer=tf.zeros_initializer(),
-                                initializer=tf.zeros_initializer,
+                                initializer=tf.zeros_initializer(),
-                                     initializer=tf.zeros_initializer,
+                                     initializer=tf.zeros_initializer(),
-    sanitizer: the sanitizer used for acheiving privacy.
+    sanitizer: the sanitizer used for achieving privacy.
-  # Data indpendent bound, as mechanism is
+  # Data independent bound, as mechanism is
-    dropout: Boolean controling whether to use dropout or not
+    dropout: Boolean controlling whether to use dropout or not
-    dropout: Boolean controling whether to use dropout or not
+    dropout: Boolean controlling whether to use dropout or not
-  By distinguishign two cases of wether D < D' or D' < D, we have
+  By distinguishing two cases of whether D < D' or D' < D, we have
-  print("Proccessing captions.")
+  print("Processing captions.")
-  # Number of workers and parameter servers are infered from the workers and ps
+  # Number of workers and parameter servers are inferred from the workers and ps
-      print('Succesfully loaded model from %s at step=%s.' %
+      print('Successfully loaded model from %s at step=%s.' %
-    # Another possiblility is to use tf.slim.get_variables().
+    # Another possibility is to use tf.slim.get_variables().
-   updated after the ops have been computed, for exmaple to update moving means
+   updated after the ops have been computed, for example to update moving means
-    separated by an end of name token. The names are choosen randomly according
+    separated by an end of name token. The names are chosen randomly according
-We use recurrent neural nets to learn complex functions able to recogize and
+We use recurrent neural nets to learn complex functions able to recognize and
-  #computes the most frequently occuring entry in a column
+  #computes the most frequently occurring entry in a column
-      #dot product of operation embedding with hidden state to the left of the number occurence
+      #dot product of operation embedding with hidden state to the left of the number occurrence
-  # Sum the gradients accross clones.
+  # Sum the gradients across clones.
-        # Auxillary tower
+        # Auxiliary tower
-    create_aux_logits: Whether to include the auxilliary logits.
+    create_aux_logits: Whether to include the auxiliary logits.
-    image = tf.image.per_image_whitening(image)
+    image = tf.image.per_image_standardization(image)
-  tf.image_summary('images', images)
+  tf.summary.image('images', images)
-                  tf.summary.scalar('Precision', precision)])
+      summary_op=tf.summary.merge([model.summaries,
-    self.summaries = tf.merge_all_summaries()
+    self.summaries = tf.summary.merge_all()
-      tf.scalar_summary('cost', self.cost)
+      tf.summary.scalar('cost', self.cost)
-    tf.scalar_summary('learning rate', self.lrn_rate)
+    tf.summary.scalar('learning rate', self.lrn_rate)
-  # for version 3 compatibility (__cmp__ is deprecated)
+  # For Python 3 compatibility (__cmp__ is deprecated).
-  # also for version 3 compatibility
+  # Also for Python 3 compatibility.
-    # Softmax bias: [emb_dim].
+    # Softmax bias: [vocab_size].
-def _tower_loss(images, labels, num_classes, scope):
+def _tower_loss(images, labels, num_classes, scope, reuse_variables=None):
-                               scope=scope)
+  with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):
-    
+
-                               scope)
+                               scope, reuse_variables)
-          tf.get_variable_scope().reuse_variables()
+          reuse_variables = True
-    self._assertCollectionSize(376, tf.GraphKeys.VARIABLES)
+    self._assertCollectionSize(376, tf.GraphKeys.GLOBAL_VARIABLES)
-    self._assertCollectionSize(376, tf.GraphKeys.VARIABLES)
+    self._assertCollectionSize(376, tf.GraphKeys.GLOBAL_VARIABLES)
-    self._assertCollectionSize(376, tf.GraphKeys.VARIABLES)
+    self._assertCollectionSize(376, tf.GraphKeys.GLOBAL_VARIABLES)
-    self._assertCollectionSize(376, tf.GraphKeys.VARIABLES)
+    self._assertCollectionSize(376, tf.GraphKeys.GLOBAL_VARIABLES)
-  image = tf.mul(image, 2.0)
+  image = tf.subtract(image, 0.5)
-    input_length = tf.expand_dims(tf.sub(caption_length, 1), 0)
+    input_length = tf.expand_dims(tf.subtract(caption_length, 1), 0)
-    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(
+    lstm_cell = tf.contrib.rnn.BasicLSTMCell(
-      lstm_cell = tf.nn.rnn_cell.DropoutWrapper(
+      lstm_cell = tf.contrib.rnn.DropoutWrapper(
-        tf.concat(1, initial_state, name="initial_state")
+        tf.concat_v2(initial_state, 1, name="initial_state")
-        state_tuple = tf.split(1, 2, state_feed)
+        state_tuple = tf.split(value=state_feed, num_or_size_splits=2, axis=1)
-        tf.concat(1, state_tuple, name="state")
+        tf.concat_v2(state_tuple, 1, name="state")
-      batch_loss = tf.div(tf.reduce_sum(tf.mul(losses, weights)),
+      losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,
-    init = tf.initialize_all_variables()
+    init = tf.global_variables_initializer()
-    for v in tf.all_variables():
+    for v in tf.global_variables():
-      tf.image_summary(name, tf.expand_dims(image, 0))
+      tf.summary.image(name, tf.expand_dims(image, 0))
-  tf.scalar_summary(
+  tf.summary.scalar(
-    tf.scalar_summary("caption_length/batch_mean", tf.reduce_mean(lengths))
+    tf.summary.scalar("caption_length/batch_min", tf.reduce_min(lengths))
-      total_loss = tf.contrib.losses.get_total_loss()
+      tf.losses.add_loss(batch_loss)
-      tf.scalar_summary("total_loss", total_loss)
+      tf.summary.scalar("losses/batch_loss", batch_loss)
-        tf.histogram_summary(var.op.name, var)
+        tf.summary.histogram("parameters/" + var.op.name, var)
-    for v in tf.all_variables():
+    for v in tf.global_variables():
-      sess.run(tf.initialize_all_variables())
+      sess.run(tf.global_variables_initializer())
-        true_logits, tf.ones_like(true_logits))
+        labels=tf.ones_like(true_logits), logits=true_logits)
-        sampled_logits, tf.zeros_like(sampled_logits))
+        labels=tf.zeros_like(sampled_logits), logits=sampled_logits)
-      logits=logits, labels=labels, name='cross_entropy_per_example')
+      labels=labels, logits=logits, name='cross_entropy_per_example')
-          logits, self.labels, name='xent')
+          logits=logits, labels=self.labels, name='xent')
-      logits, labels, name='cross_entropy_per_example')
+      logits=logits, labels=labels, name='cross_entropy_per_example')
-      logits, labels, name='cross_entropy_per_example')
+      logits=logits, labels=labels, name='cross_entropy_per_example')
-                                                            one_hot_labels,
+    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,
-    tf.nn.softmax_cross_entropy_with_logits(y_logits, y))
+    tf.nn.softmax_cross_entropy_with_logits(logits=y_logits, targets=y))
-      logits, train_labels_node))
+      labels=train_labels_node, logits=logits))
-tensorflow/models/slim/data/create_mnist_dataset.py
+tensorflow/models/slim/datasets/download_and_convert_mnist.py
-tensorflow/models/slim/data/create_cifar10_dataset.py
+tensorflow/models/slim/datasets/download_and_convert_cifar10.py
-"""Provides data for the Cifar10 dataset.
+"""Provides data for the flowers dataset.
-tensorflow/models/slim/data/create_cifar10_dataset.py
+tensorflow/models/slim/datasets/download_and_convert_flowers.py
-  """Gets a dataset tuple with instructions for reading cifar10.
+  """Gets a dataset tuple with instructions for reading flowers.
-                                            tokenizer, normalize_digits)
+          token_ids = sentence_to_token_ids(line, vocab, tokenizer,
-  create_vocabulary(en_vocab_path, train_path + ".en", en_vocabulary_size, tokenizer)
+  to_vocab_path = os.path.join(data_dir, "vocab%d" % to_vocabulary_size)
-  data_to_token_ids(train_path + ".en", en_train_ids_path, en_vocab_path, tokenizer)
+  to_train_ids_path = to_train_path + (".ids%d" % to_vocabulary_size)
-          en_vocab_path, fr_vocab_path)
+  to_dev_ids_path = to_dev_path + (".ids%d" % to_vocabulary_size)
-                num_classes=self.target_vocab_size),
+            tf.nn.sampled_softmax_loss(local_w_t, local_b, local_inputs, labels,
-    single_cell = tf.contrib.rnn.GRUCell(size)
+    single_cell = tf.nn.rnn_cell.GRUCell(size)
-      single_cell = tf.contrib.rnn.BasicLSTMCell(size)
+      single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)
-      cell = tf.contrib.rnn.MultiRNNCell([single_cell] * num_layers)
+      cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)
-      return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(
+      return tf.nn.seq2seq.embedding_attention_seq2seq(
-      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
+      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(
-      self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
+      self.outputs, self.losses = tf.nn.seq2seq.model_with_buckets(
-tf.app.flags.DEFINE_integer("fr_vocab_size", 40000, "French vocabulary size.")
+tf.app.flags.DEFINE_integer("from_vocab_size", 40000, "English vocabulary size.")
-      FLAGS.fr_vocab_size,
+      FLAGS.from_vocab_size,
-      FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)
+  from_train = None
-    train_set = read_data(en_train, fr_train, FLAGS.max_train_data_size)
+    dev_set = read_data(from_dev, to_dev)
-                                 "vocab%d.en" % FLAGS.en_vocab_size)
+                                 "vocab%d.from" % FLAGS.from_vocab_size)
-                                 "vocab%d.fr" % FLAGS.fr_vocab_size)
+                                 "vocab%d.to" % FLAGS.to_vocab_size)
-        logging.warning("Sentence truncated: %s", sentence) 
+        logging.warning("Sentence truncated: %s", sentence)
-      'image/colorspace': _bytes_feature(colorspace),
+      'image/colorspace': _bytes_feature(tf.compat.as_bytes(colorspace)),
-      'image/encoded': _bytes_feature(image_buffer)}))
+      'image/class/text': _bytes_feature(tf.compat.as_bytes(text)),
-      tf.image_summary(name, tf.expand_dims(image, 0))
+      tf.summary.image(name, tf.expand_dims(image, 0))
-    rev_vocab = [line.strip() for line in rev_vocab]
+    rev_vocab = [tf.compat.as_bytes(line.strip()) for line in rev_vocab]
-word2vec = tf.load_op_library(os.path.dirname(os.path.realpath(__file__)) + '/word2vec_ops.so')
+word2vec = tf.load_op_library(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'word2vec_ops.so'))
-word2vec = tf.load_op_library(os.path.dirname(os.path.realpath(__file__)) + '/word2vec_ops.so')
+word2vec = tf.load_op_library(os.path.join(os.path.dirname(os.path.realpath(__file__)), 'word2vec_ops.so'))
-import gen_word2vec as word2vec
+word2vec = tf.load_op_library(os.path.dirname(os.path.realpath(__file__)) + '/word2vec_ops.so')
-import gen_word2vec as word2vec
+word2vec = tf.load_op_library(os.path.dirname(os.path.realpath(__file__)) + '/word2vec_ops.so')
-    """Bottleneck resisual unit with 3 sub layers."""
+    """Bottleneck residual unit with 3 sub layers."""
-from tensorflow.models.embedding import gen_word2vec
+import gen_word2vec
-from tensorflow.models.embedding import gen_word2vec as word2vec
+import gen_word2vec as word2vec
-from tensorflow.models.embedding import gen_word2vec as word2vec
+import gen_word2vec as word2vec
-from tensorflow.models.embedding import word2vec_optimized
+import word2vec_optimized
-from tensorflow.models.embedding import word2vec
+import word2vec
-      third_party/tensorflow/models/image/alexnet:alexnet_benchmark
+      models/tutorials/image/alexnet:alexnet_benchmark
-from tensorflow.models.image.cifar10 import cifar10_input
+import cifar10
-from tensorflow.models.image.cifar10 import cifar10_input
+import cifar10_input
-from tensorflow.models.image.cifar10 import cifar10
+import cifar10
-from tensorflow.models.image.cifar10 import cifar10_input
+import cifar10_input
-from tensorflow.models.image.cifar10 import cifar10
+import cifar10
-from tensorflow.models.image.cifar10 import cifar10
+import cifar10
-from tensorflow.models.rnn.ptb import reader
+import reader
-from tensorflow.models.rnn.ptb import reader
+import reader
-    # Simplified version of tensorflow.models.rnn.rnn.py's rnn().
+    # Simplified version of models/tutorials/rnn/rnn.py's rnn().
-"""Tests for tensorflow.models.ptb_lstm.ptb_reader."""
+"""Tests for models.tutorials.rnn.ptb.reader."""
-from tensorflow.models.rnn.ptb import reader
+import reader
-from tensorflow.models.rnn.translate import seq2seq_model
+import data_utils
-from tensorflow.models.rnn.translate import data_utils
+import data_utils
-from tensorflow.models.rnn.translate import seq2seq_model
+import data_utils
-      tf.contrib.deprecated.scalar_summary("Learning Rate", m.lr)
+      tf.scalar_summary("Training Loss", m.cost)
-      tf.contrib.deprecated.scalar_summary("Validation Loss", mvalid.cost)
+      tf.scalar_summary("Validation Loss", mvalid.cost)
-    tf.contrib.deprecated.scalar_summary("NCE loss", loss)
+    tf.scalar_summary("NCE loss", loss)
-    tf.contrib.deprecated.scalar_summary(loss_name, l)
+    tf.scalar_summary(loss_name, l)
-    summaries.append(tf.contrib.deprecated.scalar_summary('learning_rate', lr))
+    summaries.append(tf.scalar_summary('learning_rate', lr))
-            tf.contrib.deprecated.histogram_summary(var.op.name + '/gradients',
+            tf.histogram_summary(var.op.name + '/gradients',
-          tf.contrib.deprecated.histogram_summary(var.op.name, var))
+          tf.histogram_summary(var.op.name, var))
-    summary_op = tf.contrib.deprecated.merge_summary(summaries)
+    summary_op = tf.merge_summary(summaries)
-  tf.contrib.deprecated.image_summary('images', images)
+  tf.image_summary('images', images)
-  tf.contrib.deprecated.scalar_summary(tensor_name + '/sparsity',
+  tf.histogram_summary(tensor_name + '/activations', x)
-    tf.contrib.deprecated.scalar_summary(l.op.name, loss_averages.average(l))
+    tf.scalar_summary(l.op.name + ' (raw)', l)
-  tf.contrib.deprecated.scalar_summary('learning_rate', lr)
+  tf.scalar_summary('learning_rate', lr)
-    tf.contrib.deprecated.histogram_summary(var.op.name, var)
+    tf.histogram_summary(var.op.name, var)
-      tf.contrib.deprecated.histogram_summary(var.op.name + '/gradients', grad)
+      tf.histogram_summary(var.op.name + '/gradients', grad)
-                                 initializer=tf.ones_initializer,
+                                 initializer=tf.ones_initializer(),
-                                         initializer=tf.ones_initializer,
+                                         initializer=tf.ones_initializer(),
-  for s in xrange(num_shards_per_batch):
+  for s in range(num_shards_per_batch):
-  for i in xrange(len(spacing) - 1):
+  for i in range(len(spacing) - 1):
-  for thread_index in xrange(len(ranges)):
+  for thread_index in range(len(ranges)):
-  shuffled_index = range(len(filenames))
+  shuffled_index = list(range(len(filenames)))
-  for s in xrange(num_shards_per_batch):
+  for s in range(num_shards_per_batch):
-  for i in xrange(len(spacing) - 1):
+  for i in range(len(spacing) - 1):
-  for thread_index in xrange(len(ranges)):
+  for thread_index in range(len(ranges)):
-  shuffled_index = range(len(filenames))
+  shuffled_index = list(range(len(filenames)))
-  for i in xrange(len(labels)):
+  for i in range(len(labels)):
-  for index in xrange(num_boxes):
+  for index in range(num_boxes):
-    for i in xrange(FLAGS.num_gpus):
+    for i in range(FLAGS.num_gpus):
-    for step in xrange(FLAGS.max_steps):
+    for step in range(FLAGS.max_steps):
-    image = tf.image.per_image_whitening(image)
+    image = tf.image.per_image_standardization(image)
-import sys
+import sys
-tf.app.flags.DEFINE_string('eval_data_path', '', 'Filepattern for eval data')
+tf.app.flags.DEFINE_string('train_data_path', '',
-  sv.Stop()
+
-  summary_writer = tf.train.SummaryWriter(FLAGS.eval_dir)
+  summary_writer = tf.summary.FileWriter(FLAGS.eval_dir)
-    self.global_step = tf.Variable(0, name='global_step', trainable=False)
+    self.global_step = tf.contrib.framework.get_or_create_global_step()
-
+  def CheckVocab(self, word):
-  assert vocab.WordToId(data.SENTENCE_END) > 0
+  assert vocab.CheckVocab(data.PAD_TOKEN) > 0
-        collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.VARIABLES])
+        collections=[tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES])
-        tf.GraphKeys.VARIABLES, scope="InceptionV3")
+        tf.GraphKeys.GLOBAL_VARIABLES, scope="InceptionV3")
-  return int(GetItem(name, root, index))
+  #  In some XML annotation files, the point value are not int,  but float.
-"""Brings inception_v1, inception_v2 and inception_v3 under one namespace."""
+"""Brings all inception models under one namespace."""
-      return sc
+inception_v1_arg_scope = inception_utils.inception_arg_scope
-      return sc
+inception_v2_arg_scope = inception_utils.inception_arg_scope
-      return sc
+inception_v3_arg_scope = inception_utils.inception_arg_scope
-        # 17 x 17 x 1024
+        # 17 x 17 x 1088
-tf.flags.DEFINE_string("val_captions_file", "/tmp/captions_train2014.json",
+tf.flags.DEFINE_string("val_captions_file", "/tmp/captions_val2014.json",
-from differential_privacy.dp_optimizer import utils
+from differential_privacy.dp_sgd.dp_optimizer import dp_optimizer
-from differential_privacy.per_example_gradients import per_example_gradients
+from differential_privacy.dp_sgd.dp_optimizer import utils
-from differential_privacy.dp_optimizer import sanitizer as san
+from differential_privacy.dp_sgd.dp_optimizer import sanitizer as san
-from differential_privacy.dp_optimizer import utils
+from differential_privacy.dp_sgd.dp_optimizer import utils
-  indirectly consumed by ys.
+  """Maps xs to consumers.
-    assert idx == 1 # We expect weights to be arg 1
+    assert idx == 1  # We expect weights to be arg 1
-    x, w = self.op.inputs
+    x, _ = self.op.inputs
-    x, b = self.op.inputs
+    x, _ = self.op.inputs
-                                    gate_gradients=gate_gradients)
+                                     name=name,
-from differential_privacy.dp_optimizer import utils
+from differential_privacy.dp_sgd.dp_optimizer import utils
-  E(P(x+s)/P(x+s-1) - 1)^i for s = 0 and 1.
+  For GaussianMomentAccountant, it suffices to compute I1, as I1 >= I2,
-              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=123))
+              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=123),
-              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=113))
+              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=113),
-            initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=113))
+            initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=113),
-  sess = sv.prepare_or_wait_for_session()
+  sess = sv.prepare_or_wait_for_session(
-  file_queue = tf.train.string_input_producer([data_path], shuffle=True)
+  data_files = tf.gfile.Glob(data_path)
-tf.app.flags.DEFINE_string('eval_data_path', '', 'Filename for eval data')
+tf.app.flags.DEFINE_string('train_data_path', '', 'Filepattern for training data.')
-    var_def = graph_pb2.NodeDef(name=var_name, op='Variable')
+    var_def = tf.NodeDef(name=var_name, op='Variable')
-      end_points = dict(tf.get_collection(end_points_collection))
+      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-      end_points = dict(tf.get_collection(end_points_collection))
+      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-        end_points = dict(tf.get_collection(end_points_collection))
+        end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-        end_points = dict(tf.get_collection(end_points_collection))
+        end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-      end_points = dict(tf.get_collection(end_points_collection))
+      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-      end_points = dict(tf.get_collection(end_points_collection))
+      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-      end_points = dict(tf.get_collection(end_points_collection))
+      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-        eval_op=names_to_updates.values(),
+        eval_op=list(names_to_updates.values()),
-                       'images.')
+tf.flags.DEFINE_string('output_codes', None, 'File to save output encoding.')
-  with open(image.filename, "r") as f:
+  with tf.gfile.FastGFile(image.filename, "r") as f:
-  with open(filename, 'r') as f:
+  with tf.gfile.FastGFile(filename, 'r') as f:
-  with open(filename, 'r') as f:
+  with tf.gfile.FastGFile(filename, 'r') as f:
-    loaded_codes = np.load(code_file)
+  contents = ''
-    np.savez_compressed(code_file, shape=int_codes.shape, codes=export)
+  output = io.BytesIO()
-    lines = f.read()
+    lines = f.read().decode()
-     Those suffix needs to be removed after the asciitree is rendered.
+
-    token_str.append('%s %s %s @%d' % (token.word, token.tag, token.label, (i+1)))
+    token_str.append('%s %s %s @%d' %
-        pat = re.compile('\s*@\d+$')
+        pat = re.compile(r'\s*@\d+$')
-    def OnesInitializer(shape, dtype=tf.float32):
+    def OnesInitializer(shape, dtype=tf.float32, partition_info=None):
-    def _Initializer(shape, dtype=tf.float32):
+    def _Initializer(shape, dtype=tf.float32, partition_info=None):
-      for _ in range(500):
+      for _ in range(50):
-      for _ in range(500):
+      for _ in range(50):
-          cf.While(Condition, Body, loop_vars, parallel_iterations=1))
+          tf.while_loop(Condition, Body, loop_vars,
-tf.NoGradient('BeamParserOutput')
+tf.NotDifferentiable('BeamParseReader')
-
+  def AddParameter(self, name, value, context):
-
+resnet_v1_50.default_image_size = resnet_v1.default_image_size
-resnet_v1_200.default_image_size = resnet_v1.default_image_size
+resnet_v1_200.default_image_size = resnet_v1.default_image_size
-
+resnet_v1_101.default_image_size = resnet_v1.default_image_size
-
+resnet_v1_152.default_image_size = resnet_v1.default_image_size
-  with tf.gfile.FastGFile(image.filename, "r") as f:
+  with open(image.filename, "r") as f:
-  image_data = tf.gfile.FastGFile(filename, 'r').read()
+  with open(filename, 'r') as f:
-  image_data = tf.gfile.FastGFile(filename, 'r').read()
+  with open(filename, 'r') as f:
-                                             resize_method)
+    distorted_image = tf.image.resize_images(distorted_image, [height, width],
-        lambda x, method: tf.image.resize_images(x, height, width, method),
+        lambda x, method: tf.image.resize_images(x, [height, width], method=method),
-                                   new_width=resize_width,
+                                   size=[resize_height, resize_width],
-
+
-def run_once(model, summary_writer, summary_op):
+def run_once(model, saver, summary_writer, summary_op):
-    model.saver.restore(sess, model_path)
+    saver.restore(sess, model_path)
-      run_once(model, summary_writer, summary_op)
+      run_once(model, saver, summary_writer, summary_op)
-      saver = tf.Saver()
+    self.build_model(model_config)
-    self.setup_saver()
+    # Set up the Saver for saving and restoring model checkpoints.
-      saver=model.saver)
+      saver=saver)
-  """Generates tf.Examples from path of recordio files.
+def ExampleGen(data_path, num_epochs=None):
-    recordio_path: CNS path to tf.Example recordio
+    data_path: path to tf.Example data files.
-    filelist = glob.glob(recordio_path)
+    filelist = glob.glob(data_path)
-    sess = sv.prepare_or_wait_for_session()
+    sess = sv.prepare_or_wait_for_session(config=tf.ConfigProto(
-    self._cur_gpu = (self._cur_gpu + 1) % (self._num_gpus-1)
+    if self._num_gpus > 1:
-from slim.datasets import dataset_utils
+from datasets import dataset_utils
-from slim.datasets import mnist
+from datasets import cifar10
-This script downloads the cifar10 data, uncompresses it, reads the files
+This module downloads the cifar10 data, uncompresses it, reads the files
-FLAGS = tf.app.flags.FLAGS
+from datasets import dataset_utils
-def _get_output_filename(split_name):
+def _get_output_filename(dataset_dir, split_name):
-  return '%s/cifar10_%s.tfrecord' % (FLAGS.dataset_dir, split_name)
+  return '%s/cifar10_%s.tfrecord' % (dataset_dir, split_name)
-    raise ValueError('You must supply the dataset directory with --dataset_dir')
+def run(dataset_dir):
-    tf.gfile.MakeDirs(FLAGS.dataset_dir)
+  Args:
-  _download_and_uncompress_dataset(FLAGS.dataset_dir)
+  training_filename = _get_output_filename(dataset_dir, 'train')
-  with tf.python_io.TFRecordWriter(output_file) as tfrecord_writer:
+  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:
-      filename = os.path.join(FLAGS.dataset_dir,
+      filename = os.path.join(dataset_dir,
-    filename = os.path.join(FLAGS.dataset_dir,
+  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:
-  dataset_utils.write_label_file(labels_to_class_names, FLAGS.dataset_dir)
+  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)
-  _clean_up_temporary_files(FLAGS.dataset_dir)
+  _clean_up_temporary_files(dataset_dir)
-This script downloads the Flowers data, uncompresses it, reads the files
+This module downloads the Flowers data, uncompresses it, reads the files
-FLAGS = tf.app.flags.FLAGS
+from datasets import dataset_utils
-
+def _get_dataset_filename(dataset_dir, split_name, shard_id):
-        output_filename = os.path.join(dataset_dir, output_filename)
+        output_filename = _get_dataset_filename(
-    raise ValueError('You must supply the dataset directory with --dataset_dir')
+def _dataset_exists(dataset_dir):
-    tf.gfile.MakeDirs(FLAGS.dataset_dir)
+  if _dataset_exists(dataset_dir):
-  photo_filenames, class_names = _get_filenames_and_classes(FLAGS.dataset_dir)
+  dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)
-                   FLAGS.dataset_dir)
+                   dataset_dir)
-                   FLAGS.dataset_dir)
+                   dataset_dir)
-  dataset_utils.write_label_file(labels_to_class_names, FLAGS.dataset_dir)
+  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)
-  _clean_up_temporary_files(FLAGS.dataset_dir)
+  _clean_up_temporary_files(dataset_dir)
-This script downloads the MNIST data, uncompresses it, reads the files
+This module downloads the MNIST data, uncompresses it, reads the files
-FLAGS = tf.app.flags.FLAGS
+from datasets import dataset_utils
-def _get_output_filename(split_name):
+def _get_output_filename(dataset_dir, split_name):
-  return '%s/mnist_%s.tfrecord' % (FLAGS.dataset_dir, split_name)
+  return '%s/mnist_%s.tfrecord' % (dataset_dir, split_name)
-    raise ValueError('You must supply the dataset directory with --dataset_dir')
+def run(dataset_dir):
-    tf.gfile.MakeDirs(FLAGS.dataset_dir)
+  Args:
-  _download_dataset(FLAGS.dataset_dir)
+  if tf.gfile.Exists(training_filename) and tf.gfile.Exists(testing_filename):
-    labels_filename = os.path.join(FLAGS.dataset_dir, _TRAIN_LABELS_FILENAME)
+  with tf.python_io.TFRecordWriter(training_filename) as tfrecord_writer:
-    labels_filename = os.path.join(FLAGS.dataset_dir, _TEST_LABELS_FILENAME)
+  with tf.python_io.TFRecordWriter(testing_filename) as tfrecord_writer:
-  dataset_utils.write_label_file(labels_to_class_names, FLAGS.dataset_dir)
+  dataset_utils.write_label_file(labels_to_class_names, dataset_dir)
-  _clean_up_temporary_files(FLAGS.dataset_dir)
+  _clean_up_temporary_files(dataset_dir)
-from slim.datasets import dataset_utils
+from datasets import dataset_utils
-# TODO(nsilberman): Add _LABELS_TO_NAMES
+
-      num_classes=_NUM_CLASSES)
+      num_classes=_NUM_CLASSES,
-from slim.datasets import dataset_utils
+from datasets import dataset_utils
-  config = slim.DeploymentConfig(num_clones=2, clone_on_cpu=True)
+  config = model_deploy.DeploymentConfig(num_clones=2, clone_on_cpu=True)
-  model_dp = slim.deploy(config, model_fn, [inputs_queue], optimizer=optimizer)
+  model_dp = model_deploy.deploy(config, model_fn, [inputs_queue],
-                    kwargs=None):
+                    **kwargs):
-    kwargs: Dict of kwarg to pass to compute_gradients().
+    **kwargs: Dict of kwarg to pass to compute_gradients().
-                    kwargs=None):
+                    **kwargs):
-   kwargs: Optional list of keyword arguments to pass to `compute_gradients`.
+   **kwargs: Optional list of keyword arguments to pass to `compute_gradients`.
-          optimizer, clone, num_clones, regularization_losses, kwargs)
+          optimizer, clone, num_clones, regularization_losses, **kwargs)
-from slim.models import model_deploy
+from deployment import model_deploy
-"""Generic evaluation script that trains a given model a specified dataset."""
+"""Generic evaluation script that evaluates a model using a given dataset."""
-from slim.models import preprocessing_factory
+from datasets import dataset_factory
-    'dataset_split_name', 'train', 'The name of the train/test split.')
+    'dataset_split_name', 'test', 'The name of the train/test split.')
-tf.app.flags.MarkFlagAsRequired('dataset_dir')
+tf.app.flags.DEFINE_integer(
-    model_fn = model_factory.get_model(
+    network_fn = nets_factory.get_network_fn(
-                                   width=model_fn.default_image_size)
+    eval_image_size = FLAGS.eval_image_size or network_fn.default_image_size
-    logits, _ = model_fn(images)
+    logits, _ = network_fn(images)
-        variables_to_restore[tf_global_step.op.name] = tf_global_step
+      variables_to_restore[tf_global_step.op.name] = tf_global_step
-      variables_to_restore = slim.get_variables_to_restore(exclude=exclude)
+      variables_to_restore = slim.get_variables_to_restore()
-        checkpoint_path,
+        master=FLAGS.master,
-  return image
+
-        logits = mnist.Mnist(images, is_training=False)
+
-"""Provides utilities to preprocess images.
+"""Provides utilities to preprocess images in CIFAR-10.
-_PADDING = 2
+_PADDING = 4
-  padded_image = tf.pad(image, [[padding, padding], [padding, padding], [0, 0]])
+  tf.image_summary('image', tf.expand_dims(image, 0))
-  distorted_image = tf.random_crop(padded_image,
+  distorted_image = tf.random_crop(image,
-
+  tf.image_summary('image', tf.expand_dims(image, 0))
-from slim.models import vgg_preprocessing
+from preprocessing import cifarnet_preprocessing
-      'cifar10': cifar10_preprocessing,
+      'cifarnet': cifarnet_preprocessing,
-"""Generic training script that trains a given model a specified dataset."""
+"""Generic training script that trains a model using a given dataset."""
-from slim.models import preprocessing_factory
+from datasets import dataset_factory
-    'log_every_n_steps', 5,
+    'log_every_n_steps', 10,
-    'Comma-separated list of scopes to include when fine-tuning '
+    'Comma-separated list of scopes of variables to exclude when restoring '
-      variables_to_restore)
+      checkpoint_path,
-    # Select the model #
+    # Select the network #
-    model_fn = model_factory.get_model(
+    network_fn = nets_factory.get_network_fn(
-        train_image_size = FLAGS.train_image_size
+      train_image_size = FLAGS.train_image_size or network_fn.default_image_size
-      """Allows data parallelism by creating multiple clones of the model_fn."""
+      """Allows data parallelism by creating multiple clones of network_fn."""
-      logits, end_points = model_fn(images)
+      logits, end_points = network_fn(images)
-    # the updates for the batch_norm variables created by model_fn.
+    # the updates for the batch_norm variables created by network_fn.
-      tf.scalar_summary('losses/%s' % loss.op.name, loss)
+      summaries.add(tf.scalar_summary('losses/%s' % loss.op.name, loss))
-    # TODO(sguada) Refactor into function that takes the clones and optimizer
+    # Variables to train.
-                                                                optimizer)
+    total_loss, clones_gradients = model_deploy.optimize_clones(
-      tf.scalar_summary('cost', moving_avg.average(self.cost))
+      tf.scalar_summary('cost', self.cost)
-    correct_prediction = total_prediction = 0
+    predictions = np.argmax(predictions, axis=1)
-        total_prediction += 1
+      predictions = np.argmax(predictions, axis=1)
-               for token in sentence.token]
+  token_str = list()
-        print tr(d)
+        tr_str = tr(d)
-  """Builds and evaluates a network.
+def RewriteContext(task_context):
-                                      task_context=FLAGS.task_context,
+                                      task_context=task_context,
-    Eval(sess, num_actions, feature_sizes, domain_sizes, embedding_dims)
+    Eval(sess)
-                 'suffix-table', 'tag-to-category'):
+                 'suffix-table', 'tag-to-category', 'char-map'):
-from tensorflow.python.platform import gfile
+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
-  The first call to conv2d will use predefined args:
+  The first call to conv2d will overwrite padding:
-  The second call to Conv will overwrite padding:
+  The second call to Conv will use predefined args:
-y_pred = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
+y_logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
-cross_entropy = -tf.reduce_sum(y * tf.log(y_pred))
+cross_entropy = tf.reduce_mean(
-correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))
+correct_prediction = tf.equal(tf.argmax(y_logits, 1), tf.argmax(y, 1))
-  num_words = max(len(vocab), FLAGS.shard_size)
+from scipy import ndimage
-from scipy import ndimage
+#!/usr/bin/env python
-# ==============================================================================
+# =============================================================================
-from tf_utils import conv2d, linear, weight_variable, bias_variable, dense_to_one_hot
+from tf_utils import weight_variable, bias_variable, dense_to_one_hot
-x = tf.placeholder(tf.float32, [None, 1600]) 
+x = tf.placeholder(tf.float32, [None, 1600])
-# %% We'll setup the two-layer localisation network to figure out the parameters for an affine transformation of the input
+# %% We'll setup the two-layer localisation network to figure out the
-initial = np.array([[1.,0, 0],[0,1.,0]]) # Use identity transformation as starting point
+# Use identity transformation as starting point
-h_trans = transformer(x_tensor, h_fc_loc2, downsample_factor=1)
+# %% We'll create a spatial transformer module to identify discriminative
-indices = np.linspace(0,10000 - 1,iter_per_epoch)
+indices = np.linspace(0, 10000 - 1, iter_per_epoch)
-    	batch_xs = X_train[indices[iter_i]:indices[iter_i+1]]
+        batch_xs = X_train[indices[iter_i]:indices[iter_i+1]]
-                   })
+                            feed_dict={
-    #theta = sess.run(h_fc_loc2, feed_dict={
+
-    #print(theta[0])
+    # print(theta[0])
-# %% Image retrieved from https://raw.githubusercontent.com/skaae/transformer_network/master/cat.jpg
+# %% Image retrieved from:
-x = tf.cast(batch,'float32')
+x = tf.cast(batch, 'float32')
-    n_fc = 6 
+    n_fc = 6
-    initial = np.array([[0.5,0, 0],[0,0.5,0]]) 
+    initial = np.array([[0.5, 0, 0], [0, 0.5, 0]])
-    h_trans = transformer(x, h_fc1, downsample_factor=2)
+    h_fc1 = tf.matmul(tf.zeros([num_batch, 1200 * 1600 * 3]), W_fc1) + b_fc1
-# plt.imshow(y[0])
+# plt.imshow(y[0])
-    
+
-    
+
-    U : float 
+    U : float
-    theta: float   
+        shape [num_batch, height, width, num_channels].
-        The size of the output of the network
+    out_size: tuple of two ints
-            
+
-                             [0., 1., 0.]]) 
+                             [0., 1., 0.]])
-        
+
-    
+
-            rep = tf.transpose(tf.expand_dims(tf.ones(shape=tf.pack([n_repeats,])),1),[1,0])
+            rep = tf.transpose(
-            return tf.reshape(x,[-1])
+            x = tf.matmul(tf.reshape(x, (-1, 1)), rep)
-            out_width = out_size[1] 
+            out_width = out_size[1]
-            x = (x + 1.0)*(width_f) / 2.0 
+            x = (x + 1.0)*(width_f) / 2.0
-            im_flat = tf.reshape(im,tf.pack([-1, channels]))
+            # use indices to lookup pixels in the flat image and restore
-            wd = tf.expand_dims(((x-x0_f) * (y-y0_f)),1)
+            wa = tf.expand_dims(((x1_f-x) * (y1_f-y)), 1)
-    
+
-                        tf.ones(shape=tf.pack([1, width]))) 
+                            tf.transpose(tf.expand_dims(tf.linspace(-1.0, 1.0, width), 1), [1, 0]))
-            y_t_flat = tf.reshape(y_t,(1, -1))
+            x_t_flat = tf.reshape(x_t, (1, -1))
-            width = tf.shape(input_dim)[2]            
+            width = tf.shape(input_dim)[2]
-            out_width = out_size[1] 
+            out_width = out_size[1]
-            
+            grid = tf.expand_dims(grid, 0)
-            y_s_flat = tf.reshape(y_s,[-1])
+            x_s = tf.slice(T_g, [0, 0, 0], [-1, 1, -1])
-                  out_size)
+                input_dim, x_s_flat, y_s_flat,
-            output = tf.reshape(input_transformed, tf.pack([num_batch, out_height, out_width, num_channels]))
+            output = tf.reshape(
-    
+
-    
+
-
+#!/usr/bin/env python
-from tensorflow.python.platform import logging
+from tensorflow.python.platform import tf_logging as logging
-from tensorflow.python.platform import logging
+from tensorflow.python.platform import tf_logging as logging
-from tensorflow.python.platform import logging
+from tensorflow.python.platform import tf_logging as logging
-from tensorflow.python.platform import logging
+from tensorflow.python.platform import tf_logging as logging
-from tensorflow.python.platform import logging
+from tensorflow.python.platform import tf_logging as logging
-from tensorflow.python.platform import logging
+from tensorflow.python.platform import tf_logging as logging
-from tensorflow.python.platform import logging
+from tensorflow.python.platform import tf_logging as logging
-from tensorflow.python.platform import logging
+from tensorflow.python.platform import tf_logging as logging
-def transformer(U, theta, downsample_factor=1, name='SpatialTransformer', **kwargs):
+def transformer(U, theta, out_size, name='SpatialTransformer', **kwargs):
-        
+    out_size: tuple of two floats
-    def _interpolate(im, x, y, downsample_factor):
+    def _interpolate(im, x, y, out_size):
-            out_width = tf.cast(width_f // downsample_factor, 'int32')
+            out_height = out_size[0]
-    def _transform(theta, input_dim, downsample_factor):
+    def _transform(theta, input_dim, out_size):
-            out_width = tf.cast(width_f // downsample_factor, 'int32')
+            out_height = out_size[0]
-                  downsample_factor)
+                  out_size)
-        return output
+        output = _transform(theta, U, out_size)
-            loss = _tower_loss(images_batch, labels_batch, num_classes, scope)
+            loss = _tower_loss(images_splits[i], labels_splits[i], num_classes,
-where the sub-sirectory is the unique label associated with these images.
+where the sub-directory is the unique label associated with these images.
- preprocessed in pararllel across mulitple threads. The resulting images
+ preprocessed in parallel across multiple threads. The resulting images
-# Images are preprocessed asynchronously using multiple threads specifed by
+# Images are preprocessed asynchronously using multiple threads specified by
-
+tf.app.flags.DEFINE_integer('num_readers', 4,
-        num_preprocess_threads=num_preprocess_threads)
+        num_preprocess_threads=num_preprocess_threads,
-        num_preprocess_threads=num_preprocess_threads)
+        num_preprocess_threads=num_preprocess_threads,
-def batch_inputs(dataset, batch_size, train, num_preprocess_threads=None):
+def batch_inputs(dataset, batch_size, train, num_preprocess_threads=None,
-    filename_queue = tf.train.string_input_producer(data_files, capacity=16)
+    # Create filename_queue
-      images_and_labels.append([image, label_index])
+    if num_readers is None:
-          batch_size=batch_size,
+      examples_queue = tf.RandomShuffleQueue(
-          min_after_dequeue=min_queue_examples)
+          min_after_dequeue=min_queue_examples,
-          capacity=min_queue_examples + 3 * batch_size)
+      reader = dataset.reader()
-            scope=scope)
+      logits, endpoints = slim.inception.inception_v3(
-          loss = _tower_loss(images, labels, num_classes, scope)
+          # Split the batch of images and labels.
-
+
-      # Allocate parameters for the beta and gamma of the normalization.
+    # Allocate parameters for the beta and gamma of the normalization.
-        moving_mean = variables.variable('moving_mean',
+                                trainable=trainable,
-                                             initializer=tf.ones)
+                                         initializer=tf.ones_initializer,
-    keep_prob: the probability of dropping each input unit.
+    keep_prob: the probability of keeping each input unit.
-  with tf.device(device):
+  # Get the device for the variable.
-        with scopes.arg_scope([variables.variable], trainable=False,
+        with scopes.arg_scope([variables.variable],
-    axis = range(len(inputs_shape) - 1)
+    axis = list(range(len(inputs_shape) - 1))
-        # 71 x 71 x 80.
+        # 73 x 73 x 80.
-        # 69 x 69 x 192.
+        # 71 x 71 x 192.
-        # mixed_8: 17 x 17 x 1280.
+        # mixed_8: 8 x 8 x 1280.
-
+  def testVariablesSetDevice(self):
-
+def l1_regularizer(weight=1.0, scope=None):
-
+class RegularizersTest(tf.test.TestCase):
-               scope=None):
+               scope=None,
-  with tf.variable_op_scope([inputs], scope, 'BatchNorm'):
+  with tf.variable_op_scope([inputs], scope, 'BatchNorm', reuse=reuse):
-           scope=None):
+           scope=None,
-    stride: the stride in height and width of the convolution.
+    kernel_size: a list of length 2: [kernel_height, kernel_width] of
-
+    reuse: whether or not the layer and its variables should be reused. To be
-  with tf.variable_op_scope([inputs], scope, 'Conv'):
+  with tf.variable_op_scope([inputs], scope, 'Conv', reuse=reuse):
-    weights_shape = [kernel_size[0], kernel_size[1],
+    weights_shape = [kernel_h, kernel_w,
-    l2_regularizer = lambda t: losses.l2_loss(t, weight_decay)
+    l2_regularizer = None
-    conv = tf.nn.conv2d(inputs, weights, [1, stride, stride, 1],
+    conv = tf.nn.conv2d(inputs, weights, [1, stride_h, stride_w, 1],
-       scope=None):
+       scope=None,
-  with tf.variable_op_scope([inputs], scope, 'FC'):
+  with tf.variable_op_scope([inputs], scope, 'FC', reuse=reuse):
-    l2_regularizer = lambda t: losses.l2_loss(t, weight_decay)
+    l2_regularizer = None
-    stride: the stride in height and width of the convolution.
+    kernel_size: a list of length 2: [kernel_height, kernel_width] of the
-    raise ValueError('kernel_size must be a 2-D list.')
+    kernel_h, kernel_w = _two_element_tuple(kernel_size)
-                          strides=[1, stride, stride, 1],
+                          ksize=[1, kernel_h, kernel_w, 1],
-    stride: the stride in height and width of the convolution.
+    kernel_size: a list of length 2: [kernel_height, kernel_width] of the
-    raise ValueError('kernel_size must be a 2-D list.')
+    kernel_h, kernel_w = _two_element_tuple(kernel_size)
-                          strides=[1, stride, stride, 1],
+                          ksize=[1, kernel_h, kernel_w, 1],
-from inception.slim import losses
+  def testCreateSquareConv(self):
-      self.assertEquals(wd.op.name, 'Conv/weights/Regularizer/L2Loss/value')
+      wd = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)[0]
-      self.assertEquals(len(tf.get_collection(losses.LOSSES_COLLECTION)), 1)
+      self.assertEquals(len(variables.get_variables()), 2)
-      self.assertEquals(len(variables.get_variables('conv2/BatchNorm')), 3)
+      images = tf.random_uniform((5, height, width, 32), seed=1)
-      ops.fc(inputs, 32, scope='fc1')
+      ops.fc(inputs, 32, scope='fc1', reuse=True)
-      self.assertEquals(wd.op.name, 'FC/weights/Regularizer/L2Loss/value')
+      wd = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)[0]
-      self.assertEquals(len(tf.get_collection(losses.LOSSES_COLLECTION)), 1)
+      self.assertEquals(len(variables.get_variables()), 2)
-      self.assertEquals(len(tf.get_collection('moving_vars')), 4)
+        net = ops.fc(images, 27)
-      self.assertEquals(len(variables.get_variables('fc2/BatchNorm')), 3)
+  def testCreateSquareMaxPool(self):
-      moving_variance = tf.get_collection('moving_vars')[1]
+      moving_mean = tf.moving_average_variables()[0]
-      ops.batch_norm(images, scale=True, scope='bn')
+      ops.batch_norm(images, scale=True, scope='bn', reuse=True)
-      ops.batch_norm(images, scope='bn')
+      ops.batch_norm(images, scope='bn', reuse=True)
-  with slim.arg_scope(ops.conv2d, padding='SAME',
+  with scopes.arg_scope(ops.conv2d, padding='SAME',
-def arg_scope(list_ops, **kwargs):
+def arg_scope(list_ops_or_scope, **kwargs):
-              list_ops need to be decorated with @add_arg_scope to work.
+    list_ops_or_scope: List or tuple of operations to set argument scope for or
-    _get_arg_stack().pop()
+  if isinstance(list_ops_or_scope, dict):
-
+  def testCurrentArgScope(self):
-"""Contains convenience wrappers for creating Variables in TensorFlow.
+"""Contains convenience wrappers for creating variables in TF-Slim.
-                               device='/cpu:0')
+                              shape=[100],
-  with slim.arg_scope(variables.Variables, restore=False):
+  with slim.arg_scope([variables.variable], restore=False):
-  # Get all variables in the VARIABLES_TO_RESTORE collection
+  # Get all variables to restore.
-  variables_to_restore = tf.get_collection(slim.variables.VARIABLES_TO_RESTORE)
+  variables_to_restore = slim.variables.get_variables_to_restore()
-variables_to_restore = tf.get_collection(slim.variables.VARIABLES_TO_RESTORE)
+variables_to_restore = slim.variables.get_variables_to_restore()
-VARIABLES_COLLECTION = '_variables_'
+MODEL_VARIABLES = '_model_variables_'
-# Collection containing all the slim.variables that are marked to_restore
+# Collection containing the slim.variables that are created with restore=True.
-  """Adds a variable to the default set of collections.
+  """Adds a variable to the MODEL_VARIABLES collection.
-  for collection in default_collections(given_name, restore):
+  collections = [MODEL_VARIABLES]
-  """Gets the list of variables, filtered by prefix and/or suffix.
+def get_variables(scope=None, suffix=None):
-    prefix: an optional prefix for filtering the variables to return.
+    scope: an optional scope for filtering the variables to return.
-    a list of variables with prefix and suffix.
+    a copied list of variables with scope and suffix.
-  candidates = tf.get_collection(VARIABLES_COLLECTION, prefix)
+  candidates = tf.get_collection(MODEL_VARIABLES, scope)[:]
-  """Gets the list of variables were given that name.
+def get_variables_to_restore():
-    prefix: an optional prefix for filtering the variables to return.
+    scope: an optional scope for filtering the variables to return.
-    a list of variables with prefix and suffix.
+    a copied list of variables with the given name and prefix.
-  return tf.get_collection(VARIABLES_COLLECTION + given_name, prefix)
+  return get_variables(scope=scope, suffix=given_name)
-      collection.
+      and MODEL_VARIABLES collections.
-                                                                  restore))
+  collections = list(collections or [])
-
+      self.assertEquals([a, b], variables.get_variables())
-                           tf.get_collection(variables.VARIABLES_TO_RESTORE))
+        b = variables.variable('a', [5])
-  def testGetVariablesToRestorePartial(self):
+  def testNoneGetVariablesToRestore(self):
-        a = variables.variable('a', [5])
+        a = variables.variable('a', [5], restore=False)
-                           tf.get_collection(variables.VARIABLES_TO_RESTORE))
+      with tf.variable_scope('B'):
-                           tf.get_collection(variables.VARIABLES_TO_RESTORE))
+      self.assertListEqual([a, b, c], variables.get_variables_to_restore())
-  assert args
+  assert args is not None
-      if grad:
+      if grad is not None:
-    preprocessor = prep.StandardScaler().fit(X_train)
+def minmax_scale(X_train, X_test):
-X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)
+X_train, X_test = minmax_scale(mnist.train.images, mnist.test.images)
-                                     gaussian_sample_size = 128)
+                                     optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
-                 gaussian_sample_size = 128):
+    def __init__(self, n_input, n_hidden, optimizer = tf.train.AdamOptimizer()):
-        eps = tf.random_normal((self.gaussian_sample_size, n_hidden), 0, 1, dtype = tf.float32)
+        eps = tf.random_normal(tf.pack([tf.shape(self.x)[0], self.n_hidden]), 0, 1, dtype = tf.float32)
-    preprocessor = prep.StandardScaler().fit(X_train)
+def min_max_scale(X_train, X_test):
-X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)
+X_train, X_test = min_max_scale(mnist.train.images, mnist.test.images)
-             "right-shift", "bmul", "dup", "badd", "qadd"]
+bins = [8, 12, 16, 20, 24, 28, 32, 36, 40, 48, 64, 128]
-    elif task in ["bmul"]:
+    elif task in ["mul", "bmul"]:
-      res = [int(x) for x in list(reversed(str(bin(d1n * d2n))))[:-2]]
+      if task == "bmul":
-    if task in ["add", "badd", "qadd", "bmul"]:
+    if task in ["add", "badd", "qadd", "bmul", "mul"]:
-
+def tanh_cutoff(x, cutoff):
-               learning_rate, pull, pull_incr, min_length):
+               learning_rate, pull, pull_incr, min_length, act_noise=0.0):
-      step = [tf.nn.dropout(first, 1.0 - self.do_training * dropout) * mask]
+      keep_prob = 1.0 - self.do_training * (dropout * 8.0 / float(length))
-          cur = tf.nn.dropout(cur, 1.0 - self.do_training * dropout)
+            cur *= mask
-                           for o in list(tf.split(1, length, output))])
+      external_output = [tf.reshape(o, [-1, noclass])
-  def step(self, sess, inp, target, do_backward, noise_param=None):
+  def step(self, sess, inp, target, do_backward, noise_param=None,
-      feed_out.append(self.steps[index][l])
+    if get_steps:
-    steps = res[offset + 1 + length:]
+    steps = res[offset + 1 + length:] if get_steps else None
-tf.app.flags.DEFINE_float("lr", 0.003, "Learning rate.")
+tf.app.flags.DEFINE_float("lr", 0.001, "Learning rate.")
-tf.app.flags.DEFINE_float("max_grad_norm", 0.05, "Clip gradients to this norm.")
+tf.app.flags.DEFINE_float("max_grad_norm", 1.0, "Clip gradients to this norm.")
-tf.app.flags.DEFINE_float("curriculum_bound", 0.08, "Move curriculum < this.")
+tf.app.flags.DEFINE_float("curriculum_bound", 0.15, "Move curriculum < this.")
-tf.app.flags.DEFINE_float("grad_noise_scale", 1.0, "Gradient noise scale.")
+tf.app.flags.DEFINE_float("grad_noise_scale", 0.0, "Gradient noise scale.")
-tf.app.flags.DEFINE_integer("noclass", 14, "Number of classes (0 is padding).")
+tf.app.flags.DEFINE_integer("nmaps", 128, "Number of floats in each cell.")
-  data.forward_max = max(FLAGS.forward_max, data.bins[-1])
+  data.forward_max = max(FLAGS.forward_max, data.bins[-1])
-  return (model, min_length, max_length, checkpoint_dir, curriculum)
+  return (model, min_length, max_length, checkpoint_dir, curriculum, ensemble)
-                offset=None):
+                offset=None, ensemble=None, get_steps=False):
-  _, res, _, steps = model.step(sess, inpt, target, False)
+  _, res, _, steps = model.step(sess, inpt, target, False, get_steps=get_steps)
-def multi_test(l, model, sess, task, nprint, batch_size, offset=None):
+def multi_test(l, model, sess, task, nprint, batch_size, offset=None,
-                                 False, cur_offset)
+                                 False, cur_offset, ensemble=ensemble)
-    model, min_length, max_length, checkpoint_dir, curriculum = initialize(sess)
+    (model, min_length, max_length, checkpoint_dir,
-          sess.run([model.avg_op, model.lr_decay_op])
+          sess.run(model.avg_op)
-    model, min_length, max_length, _, _ = initialize(sess)
+    model, min_length, max_length, _, _, ensemble = initialize(sess)
-        _, seq_err, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)
+        _, seq_err, _ = single_test(l, model, sess, t, FLAGS.nprint,
-      animate(l, test_data, anim_size)
+      if FLAGS.animate:
-                              batch_size * 4)
+                              batch_size * 4, ensemble=ensemble)
-                   batch_size * 64, 0)
+                   batch_size * 64, 0, ensemble=ensemble)
-    model, _, _, _, _ = initialize(sess)
+    model, _, _, _, _, _ = initialize(sess)
-    adam = tf.train.AdamOptimizer(0.01*self.lr, epsilon=1e-4)
+    adam = tf.train.AdamOptimizer(self.lr, epsilon=1e-4)
-tf.app.flags.DEFINE_float("lr", 0.3, "Learning rate.")
+tf.app.flags.DEFINE_float("lr", 0.003, "Learning rate.")
-                                (20 * prev_seq_err)) * FLAGS.grad_noise_scale
+                                prev_seq_err) * FLAGS.grad_noise_scale
-      prev_seq_err = acc_seq_err
+      prev_seq_err = max(0.0, acc_seq_err - 0.02)  # No noise at error < 2%.
-    adam = tf.train.AdamOptimizer(0.01*self.lr, epsilon=1e-5)
+    adam = tf.train.AdamOptimizer(0.01*self.lr, epsilon=1e-4)
-tf.app.flags.DEFINE_float("lr", 0.1, "Learning rate.")
+tf.app.flags.DEFINE_float("lr", 0.3, "Learning rate.")
-tf.app.flags.DEFINE_float("curriculum_bound", 0.06, "Move curriculum < this.")
+tf.app.flags.DEFINE_float("curriculum_bound", 0.08, "Move curriculum < this.")
-tf.app.flags.DEFINE_integer("batch_size", 64, "Batch size.")
+tf.app.flags.DEFINE_integer("batch_size", 32, "Batch size.")
-        if pull < 1:
+        if pull < 0.1:
-        if seq_err < 0.5:  # Run larger test if we're good enough.
+        if seq_err < 0.05:  # Run larger test if we're good enough.
-    length = batch_size * noclass
+    length = tf.expand_dims(batch_size * noclass, 0)
-    dense = tf.sparse_to_dense(indices, 2 * batch_size, 1.0, 0.0)
+    dense = tf.sparse_to_dense(indices, tf.expand_dims(2 * batch_size, 0),
-tf.app.flags.DEFINE_integer("steps_per_checkpoint", 100, "Steps per epoch.")
+tf.app.flags.DEFINE_integer("steps_per_checkpoint", 200, "Steps per epoch.")
-  curriculum = 0.12
+  curriculum = FLAGS.curriculum_bound
-from google3.third_party.tensorflow.python.platform import gfile
+from tensorflow.python.platform import gfile
-from google3.experimental.users.lukaszkaiser.neural_gpu import data_utils
+import data_utils
-import google3.experimental.users.lukaszkaiser.neural_gpu.neural_gpu as ngpu
+from tensorflow.python.platform import gfile
-tf.app.flags.DEFINE_float("dropout", 0.2, "Dropout that much.")
+tf.app.flags.DEFINE_float("dropout", 0.15, "Dropout that much.")
-  while len(data.bins) > 1 and data.bins[-2] > max_length + 12:
+  while len(data.bins) > 1 and data.bins[-2] > max_length + EXTRA_EVAL:
-    for l in xrange(max_length + 11):
+    for l in xrange(max_length + EXTRA_EVAL - 1):
-  data.print_out(tag)
+  msg1 = ("layers %d kw %d h %d kh %d relax %d batch %d noise %.2f task %s"
-  model = ngpu.NeuralGPU(
+  model = neural_gpu.NeuralGPU(
-  seq = float(seq) / batch_size
+  errors, total, seq_err = data.accuracy(inpt, res, target, batch_size, nprint)
-  return errors, seq, (steps, inpt, [np.argmax(o, axis=1) for o in res])
+                   % (task, l, 100*errors, 100*seq_err))
-  seq = 0.0
+  errors, seq_err = 0.0, 0.0
-                             cur_offset)
+    err, sq_err, _ = single_test(l, model, sess, task, to_print, low_batch,
-    seq += sq
+    seq_err += sq_err
-      cur_seq = float(low_batch * seq) / ((mstep+1) * low_batch)
+      cur_seq_err = float(low_batch * seq_err) / ((mstep+1) * low_batch)
-                     % (task, 100*cur_errors, 100*cur_seq))
+                     % (task, 100*cur_errors, 100*cur_seq_err))
-  seq = float(low_batch) * float(seq) / batch_size
+  seq_err = float(low_batch) * float(seq_err) / batch_size
-  return errors, seq
+                 % (task, l, 100*errors, 100*seq_err))
-  """Main training function."""
+  """Train the model."""
-    prev_sq = 1.0
+    prev_seq_err = 1.0
-      acc_loss, acc_total, acc_errors, acc_seq = 0.0, 0, 0, 0
+      acc_loss, acc_total, acc_errors, acc_seq_err = 0.0, 0, 0, 0
-          l = np.random.randint(max_cur_length - min_length+1) + min_length
+
-          l = np.random.randint(max_length - min_length + 1) + min_length
+        # Mixed curriculum learning: in 25% of cases go to any larger length.
-        noise_param = math.sqrt(stepp * 20 * prev_sq) * FLAGS.grad_noise_scale
+        noise_param = math.sqrt(math.pow(global_step, -0.55) *
-                                             batch_size, 0)
+          errors, total, seq_err = data.accuracy(inp, res, target,
-          acc_seq += seq
+          acc_seq_err += seq_err
-      prev_sq = acc_seq
+      acc_seq_err = float(acc_seq_err) / (step_count * batch_size)
-        prev_acc_perp.append(1000000)
+      msg1 = "step %d step-time %.2f" % (global_step, step_time)
-        prev_acc_perp.append(acc_perp)
+
-          _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)
+        while l < max_length + EXTRA_EVAL and l < bound:
-      if should_exit:
+        if seq_err < 0.5:  # Run larger test if we're good enough.
-  fps = 2
+  xf = 12  # Extra frames to slow down at start and end.
-    return str(i-1)
+
-          t.set_text(to_symbol(out[i]))
+          t.set_text(data.to_symbol(out[i]))
-        t.set_text(to_symbol(inpt[i][batch]) if index < xf else "")
+        t.set_text(data.to_symbol(inpt[i][batch]) if index < xf else "")
-        _, sq, _ = single_test(l, model, sess, t, FLAGS.nprint, batch_size)
+      while l < max_length + EXTRA_EVAL and l < bound:
-    if sq < 0.01:  # More tests.
+      _, seq_err = multi_test(data.forward_max, model, sess, t, FLAGS.nprint,
-      ids = [int(c) for c in inpt.strip()]
+      ids = [data.to_id(s) for s in inpt.strip().split()]
-      print " ".join([str(output[0]) for output in res])
+      res = [o for o in res[:len(ids)] if o > 0]
