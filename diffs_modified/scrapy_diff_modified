-        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL']
+        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None
-        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL']
+        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None
-FILES_STORE_GCS_ACL = None
+FILES_STORE_GCS_ACL = ''
-IMAGES_STORE_GCS_ACL = None
+IMAGES_STORE_GCS_ACL = ''
-                               # FilesPipeline.from_settings.
+
-FILES_STORE_GCS_ACL = 'projectPrivate'
+FILES_STORE_GCS_ACL = None
-IMAGES_STORE_GCS_ACL = 'projectPrivate'
+IMAGES_STORE_GCS_ACL = None
-    return content, blob
+    return content, acl, blob
-        content, blob = get_gcs_content_and_delete(u.hostname, u.path[1:]+path)
+        content, acl, blob = get_gcs_content_and_delete(u.hostname, u.path[1:]+path)
-            content_type=self._get_content_type(headers)
+            content_type=self._get_content_type(headers),
-            '|descendant::button[not(@type) or re.test(@type, "^submit$", "i")]',
+            'descendant::input[re:test(@type, "^(submit|image)$", "i")]'
-            '|descendant::button[not(@type)]',
+            'descendant::input[re.test(@type, "^(submit|image)$", "i")]'
-    print("clickables =", clickables)
+    print("form =", form.__dict__)
-             namespaces={"re": "http://exslt.org/regular-expressions"})
+            'descendant::*[(self::input or self::button)'
-        so that the connection is not closed, only logging warnings.
+        except that VerificationError, CertificateError and ValueError
-    from service_identity.exceptions import CertificateError
+    try:
-                except (CertificateError, VerificationError) as e:
+                except verification_errors as e:
-    'm4a', 'm4v',
+    'm4a', 'm4v', 'flv',
-                except VerificationError as e:
+                except (CertificateError, VerificationError) as e:
-        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b'i2v']})
+    
-            namespaces={"re": "http://exslt.org/regular-expressions"})
+             'descendant::*[(self::input or self::button)'
-from sphinx.util.compat import Directive
+from docutils.parsers.rst import Directive
-    # Python>=3.6 raises TypeError
+    # Python <= 3.4 raises pickle.PicklingError here while
-        'parsel>=1.1',
+        'parsel>=1.4',
-        self.assertRaises(ValueError, q.push, sel)
+    # Selectors should fail (lxml.html.HtmlElement objects can't be pickled)
-    except (pickle.PicklingError, AttributeError) as e:
+    # Python<=3.4 raises pickle.PicklingError here while
-        q = self.queue()
+    if version_info.major == 3 and version_info.minor >= 6:
-                     extra={'crawler': self.crawler})
+        logger.info("Telnet console listening on %(host)s:%(port)d",
-            raise IgnoreRequest()
+            raise IgnoreRequest("Forbidden by robots.txt")
-                 to_native_str(self._useragent), request.url):
+        if rp is None:
-        else: # last effort try
+        else:  # last effort try
-                pass
+                self.crawler.stats.inc_value('robotstxt/unicode_error_count')
-    if signame.startswith("SIG"):
+    if signame.startswith('SIG') and not signame.startswith('SIG_'):
-    if hasattr(signal, "SIGBREAK"):
+    if hasattr(signal, 'SIGBREAK'):
-        self.assertEqual(r6.text, u'WORD\ufffd\ufffd')
+        self.assertIn(r6.text, {
-    It delays by 100ms so reactor has a chance to go trough readers and writers
+    It delays by 100ms so reactor has a chance to go through readers and writers
-            # Abort connection inmediately.
+            # Abort connection immediately.
-            self.assertEqual(get_func_args(operator.itemgetter(2)), ['obj'])
+            stripself = not six.PY2  # PyPy3 exposes them as methods
-        'Programming Language :: Python :: 3.3',
+    python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*',
-    pass
+    pass
-                warnings.warn("allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains." % domain, Warning)
+                warnings.warn("allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains." % domain, URLWarning)
-            assert "allowed_domains accepts only domains, not URLs." in str(w[-1].message)
+            assert issubclass(w[-1].category, URLWarning)
-                logger.warn("allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains." % domain)
+                warnings.warn("allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains." % domain, Warning)
-        for domainIndex in range(0, len(allowed_domains)):
+        for domain in allowed_domains:
-                logger.warn("allowed_domains accepts only domains, not URLs. Ignoring URL entry %s in allowed_domains." % allowed_domains[domainIndex])
+            if url_pattern.match(domain):
-import sys, os
+import sys
-USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % import_module('scrapy').__version__
+USER_AGENT = 'Scrapy/%s (+https://scrapy.org)' % import_module('scrapy').__version__
-    # workaround for http://bugs.python.org/issue7904 - Python < 2.7
+    # workaround for https://bugs.python.org/issue7904 - Python < 2.7
-    # workaround for http://bugs.python.org/issue9374 - Python < 2.7.4
+    # workaround for https://bugs.python.org/issue9374 - Python < 2.7.4
-        Quoting http://twistedmatrix.com/documents/current/api/twisted.web.client.Agent.html:
+        Quoting https://twistedmatrix.com/documents/current/api/twisted.web.client.Agent.html:
-            # http://www.openssl.org/docs/ssl/SSL_CTX_set_options.html
+            # https://www.openssl.org/docs/manmaster/man3/SSL_CTX_set_options.html
-            # the original traceback (see http://bugs.python.org/issue7563),
+            # the original traceback (see https://bugs.python.org/issue7563),
-    documented in: http://en.wikipedia.org/wiki/Chunked_transfer_encoding
+    documented in: https://en.wikipedia.org/wiki/Chunked_transfer_encoding
-        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18
+        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18
-    # Workaround for http://bugs.python.org/issue17606
+    # Workaround for https://bugs.python.org/issue17606
-        # Response cacheability - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4
+        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec14.9.1
-        # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#410
+        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706
-        # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#366
+        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658
-    http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9
+    https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9
-                "http://doc.scrapy.org/en/latest/topics/telnetconsole.html",
+                "https://doc.scrapy.org/en/latest/topics/telnetconsole.html",
-        # http://bugs.python.org/issue5103
+        # https://bugs.python.org/issue5103
-        
+
-        .. _deferreds: http://twistedmatrix.com/documents/current/core/howto/defer.html
+        .. _deferreds: https://twistedmatrix.com/documents/current/core/howto/defer.html
-    Taken from: http://jcalderone.livejournal.com/24285.html
+    Taken from: https://jcalderone.livejournal.com/24285.html
-        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks
+        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass
-    
+    """
-    http://en.wikipedia.org/wiki/Chunked_transfer_encoding
+    https://en.wikipedia.org/wiki/Chunked_transfer_encoding
-        http://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/
+        https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/
-    http://code.google.com/web/ajaxcrawling/docs/getting-started.html
+    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started
-    url='http://scrapy.org',
+    url='https://scrapy.org',
-see http://doc.scrapy.org/en/latest/contributing.html#running-tests
+see https://doc.scrapy.org/en/latest/contributing.html#running-tests
-                         "max size (%(maxsize)s).",
+                         "max size (%(maxsize)s) in request %(request)s.",
-                          'maxsize': self._maxsize})
+                          'maxsize': self._maxsize,
-                           "download warn size (%(warnsize)s) in request (%(request)s).",
+                           "download warn size (%(warnsize)s) in request %(request)s.",
-                           {'size': expected_size, 'warnsize': warnsize})
+                           "download warn size (%(warnsize)s) in request (%(request)s).",
-                         "max size (%(maxsize)s) in request %(request)s.",
+                         "max size (%(maxsize)s).",
-                          'request': self._request})
+                          'maxsize': self._maxsize})
-                         "max size (%(maxsize)s).",
+                         "max size (%(maxsize)s) in request %(request)s.",
-                          'maxsize': self._maxsize})
+                          'maxsize': self._maxsize,
-        self.smtppass = to_bytes(smtppass) if smtppass is not None else None
+        self.smtpuser = _to_bytes_or_none(smtpuser)
-        self.smtppass = to_bytes(smtppass)
+        self.smtpuser = to_bytes(smtpuser) if smtpuser is not None else None
-from .utils.misc import arg_to_iter
+from scrapy.utils.misc import arg_to_iter
-        self.smtppass = smtppass
+        self.smtpuser = to_bytes(smtpuser)
-        dfd = self._sendmail(rcpts, msg.as_string())
+        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or 'utf-8'))
-        msg = BytesIO(msg.encode('utf-8'))
+        # Import twisted.mail here because it is not available in python3
-from six.moves import cStringIO as StringIO
+from io import BytesIO
-        msg = StringIO(msg)
+        msg = BytesIO(msg.encode('utf-8'))
-    def test_no_enviroment_proxies(self):
+    def test_no_environment_proxies(self):
-    def test_enviroment_proxies(self):
+    def test_environment_proxies(self):
-                                   'Platform'])
+                                   'cryptography', 'Platform'])
-    'm4a',
+    'm4a', 'm4v',
-            content_type='application/octet-stream'
+            content_type=self._get_content_type(headers)
-from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore
+from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore, GCSFilesStore
-    def __init__(self, settings=None):
+    def __init__(self, settings=None, install_root_handler=True):
-        configure_logging(self.settings)
+        configure_logging(self.settings, install_root_handler)
-                print("%-9s : %s" % (name, version))
+            versions = scrapy_components_versions()
-   ur'Scrapy developers', 'manual'),
+   u'Scrapy developers', 'manual'),
-line_re = re.compile(ur'(.*)\:\d+\:\s\[(.*)\]\s(?:(.*)\sto\s(.*)|(.*))')
+line_re = re.compile(u'(.*)\:\d+\:\s\[(.*)\]\s(?:(.*)\sto\s(.*)|(.*))')
-  ('index', 'Scrapy.tex', ur'Scrapy Documentation',
+  ('index', 'Scrapy.tex', u'Scrapy Documentation',
-    logger.info("Versions: %(versions)s}",
+    logger.info("Versions: %(versions)s",
-from scrapy.settings import Settings
+from scrapy.settings import overridden_settings, Settings
-from scrapy.settings import overridden_settings, Settings
+from scrapy.settings import Settings
-            # scrapy root handler alread installed: update it with new settings
+            # scrapy root handler already installed: update it with new settings
-        self.assertTrue(self.wrapped.info() is self.wrapped)
+        self.assertIs(self.wrapped.info(), self.wrapped)
-            self.assertTrue(isinstance(k, bytes))
+            self.assertIsInstance(k, bytes)
-                self.assertTrue(isinstance(s, bytes))
+                self.assertIsInstance(s, bytes)
-        self.assertTrue(isinstance(proc(['hello', 'world']), six.text_type))
+        self.assertIsInstance(proc(['hello', 'world']), six.text_type)
-        self.assertTrue(l.selector is sel)
+        self.assertIs(l.selector, sel)
-        self.assertTrue(l.selector is sel)
+        self.assertIs(l.selector, sel)
-        self.assertFalse(d is d2)  # shouldn't modify in place
+        self.assertIsNot(d, d2)  # shouldn't modify in place
-        self.assertFalse(d is d2)  # shouldn't modify in place
+        self.assertIsNot(d, d2)  # shouldn't modify in place
-        self.assertFalse(d is d2)  # shouldn't modify in place
+        self.assertIsNot(d, d2)  # shouldn't modify in place
-        self.assertTrue(isinstance(result[0][1], Failure))
+        self.assertIsInstance(result[0][1], Failure)
-        self.assertTrue(b'404 - No Such Resource' in pageData)
+        self.assertIn(b'404 - No Such Resource', pageData)
-        self.assertEquals(200, settingsdict[EXT_PATH])
+        self.assertEqual(200, settingsdict[EXT_PATH])
-            self.assertEquals(response.body, b'0123456789')
+            self.assertEqual(response.url, request.url)
-        d.addCallback(self.assertEquals, b"0123456789")
+        d.addCallback(self.assertEqual, b"0123456789")
-        d.addCallback(self.assertEquals, b'')
+        d.addCallback(self.assertEqual, b'')
-        d.addCallback(self.assertEquals, 302)
+        d.addCallback(self.assertEqual, 302)
-        d.addCallback(self.assertEquals, 302)
+        d.addCallback(self.assertEqual, 302)
-            self.assertEquals(
+            self.assertEqual(
-            self.assertEquals(request.headers, {})
+            self.assertEqual(request.headers, {})
-            self.assertEquals(request.headers.get('Host'), b'example.com')
+            self.assertEqual(response.body, b'example.com')
-        d.addCallback(self.assertEquals, b'example.com')
+        d.addCallback(self.assertEqual, b'example.com')
-            self.assertEquals(response.body, b'0')
+            self.assertEqual(response.body, b'0')
-            self.assertEquals(contentlengths, [b"0"])
+            self.assertEqual(len(contentlengths), 1)
-        d.addCallback(self.assertEquals, body)
+        d.addCallback(self.assertEqual, body)
-        d.addCallback(self.assertEquals, b"0123456789")
+        d.addCallback(self.assertEqual, b"0123456789")
-            self.assertEquals(type(response), TextResponse)
+            self.assertEqual(type(response), TextResponse)
-        d.addCallback(self.assertEquals, b"0123456789")
+        d.addCallback(self.assertEqual, b"0123456789")
-        d.addCallback(self.assertEquals, b"0123456789")
+        d.addCallback(self.assertEqual, b"0123456789")
-        d.addCallback(self.assertEquals, b"chunked content\n")
+        d.addCallback(self.assertEqual, b"chunked content\n")
-            self.assertEquals(response.body, b'http://example.com')
+            self.assertEqual(response.status, 200)
-            self.assertEquals(response.body, b'https://example.com')
+            self.assertEqual(response.status, 200)
-            self.assertEquals(response.body, b'/path/to/resource')
+            self.assertEqual(response.status, 200)
-            self.assertEquals(response.url, uri)
+            self.assertEqual(response.url, uri)
-            self.assertEquals(type(response),
+            self.assertEqual(response.text, 'A brief note')
-            self.assertEquals(response.encoding, "US-ASCII")
+            self.assertEqual(response.encoding, "US-ASCII")
-            self.assertEquals(type(response),
+            self.assertEqual(response.text, u'\u038e\u03a3\u038e')
-            self.assertEquals(response.encoding, "iso-8859-7")
+            self.assertEqual(response.encoding, "iso-8859-7")
-            self.assertEquals(response.encoding, "iso-8859-7")
+            self.assertEqual(response.text, u'\u038e\u03a3\u038e')
-            self.assertEquals(type(response),
+            self.assertEqual(response.text, u'\u038e\u03a3\u038e')
-            self.assertEquals(response.encoding, "utf-8")
+            self.assertEqual(response.encoding, "utf-8")
-            self.assertEquals(response.text, 'Hello, world.')
+            self.assertEqual(response.text, 'Hello, world.')
-        self.assertEquals(req2.headers.get('Cookie'), b"C1=value1")
+        self.assertEqual(req2.headers.get('Cookie'), b"C1=value1")
-        self.assertEquals(req.headers.get('Cookie'), b'C1=value1')
+        self.assertEqual(req.headers.get('Cookie'), b'C1=value1')
-        self.assertEquals(req.headers.get('Cookie'), b'C1=value1')
+        self.assertEqual(req.headers.get('Cookie'), b'C1=value1')
-        self.assertEquals(req.headers.get('Cookie'), b'C2=value2')
+        self.assertEqual(req.headers.get('Cookie'), b'C2=value2')
-        self.assertEquals(req.headers.get('Cookie'), b'galleta=salada')
+        self.assertEqual(req.headers.get('Cookie'), b'galleta=salada')
-        self.assertEquals(req.headers.get('Cookie'), b'galleta=salada')
+        self.assertEqual(req.headers.get('Cookie'), b'galleta=salada')
-        self.assertEquals(req3.headers.get('Cookie'), b'galleta=dulce')
+        self.assertEqual(req3.headers.get('Cookie'), b'galleta=dulce')
-        self.assertEquals(req5_2.headers.get('Cookie'), b'C1=value1')
+        self.assertEqual(req5_2.headers.get('Cookie'), b'C1=value1')
-        self.assertEquals(req5_3.headers.get('Cookie'), b'C1=value1')
+        self.assertEqual(req5_3.headers.get('Cookie'), b'C1=value1')
-        self.assertEquals(req6.headers.get('Cookie'), None)
+        self.assertEqual(req6.headers.get('Cookie'), None)
-        self.assertEquals(req.headers, defaults)
+        self.assertEqual(req.headers, defaults)
-        self.assertEquals(req.headers, bytes_headers)
+        self.assertEqual(req.headers, bytes_headers)
-        self.assertEquals(req.headers, defaults)
+        self.assertEqual(req.headers, defaults)
-        self.assertEquals(req.meta.get('download_timeout'), 180)
+        self.assertEqual(req.meta.get('download_timeout'), 180)
-        self.assertEquals(req.meta.get('download_timeout'), 20.1)
+        self.assertEqual(req.meta.get('download_timeout'), 20.1)
-        self.assertEquals(req.meta.get('download_timeout'), 2)
+        self.assertEqual(req.meta.get('download_timeout'), 2)
-        self.assertEquals(req.meta.get('download_timeout'), 1)
+        self.assertEqual(req.meta.get('download_timeout'), 1)
-        self.assertEquals(req.headers['Authorization'], b'Basic Zm9vOmJhcg==')
+        self.assertEqual(req.headers['Authorization'], b'Basic Zm9vOmJhcg==')
-        self.assertEquals(req.headers['Authorization'], b'Digest 123')
+        self.assertEqual(req.headers['Authorization'], b'Digest 123')
-        self.assertEquals(response.body, b'')
+        self.assertEqual(response.body, b'')
-            self.assertEquals(req.meta, {})
+            self.assertEqual(req.url, url)
-            self.assertEquals(req.meta.get('proxy'), proxy)
+            self.assertEqual(req.url, url)
-        self.assertEquals(req.meta, {'proxy': 'https://new.proxy:3128'})
+        self.assertEqual(req.meta, {'proxy': 'https://new.proxy:3128'})
-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')
+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6cGFzc3dvcmQ=')
+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')
+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcm5hbWU6')
+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic bcOhbjpwYXNz')
+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic w7xzZXI6cGFzcw==')
+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic beFuOnBhc3M=')
+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
-        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic /HNlcjpwYXNz')
+        self.assertEqual(req.meta, {'proxy': 'https://proxy:3128'})
-        self.assertEquals(req.meta, {'proxy': 'http://proxy.com'})
+        self.assertEqual(req.meta, {'proxy': 'http://proxy.com'})
-        self.assertEquals(perc_encoded_utf8_url, req_result.url)
+        self.assertEqual(perc_encoded_utf8_url, req_result.url)
-        self.assertEquals(perc_encoded_utf8_url, req_result.url)
+        self.assertEqual(perc_encoded_utf8_url, req_result.url)
-        self.assertEquals(req.headers['User-Agent'], b'default_useragent')
+        self.assertEqual(req.headers['User-Agent'], b'default_useragent')
-        self.assertEquals(req.headers['User-Agent'], b'spider_useragent')
+        self.assertEqual(req.headers['User-Agent'], b'spider_useragent')
-        self.assertEquals(req.headers['User-Agent'], b'header_useragent')
+        self.assertEqual(req.headers['User-Agent'], b'header_useragent')
-        self.assert_(self.wrapped.info() is self.wrapped)
+        self.assertTrue(self.wrapped.info() is self.wrapped)
-            self.assert_(isinstance(k, bytes))
+            self.assertTrue(isinstance(k, bytes))
-                self.assert_(isinstance(s, bytes))
+                self.assertTrue(isinstance(s, bytes))
-        self.assert_(isinstance(proc(['hello', 'world']), six.text_type))
+        self.assertTrue(isinstance(proc(['hello', 'world']), six.text_type))
-        self.assert_(l.selector is sel)
+        self.assertTrue(l.selector is sel)
-        self.assert_(l.selector is sel)
+        self.assertTrue(l.selector is sel)
-        self.assert_(l.selector)
+        self.assertTrue(l.selector)
-        self.assert_(l.selector)
+        self.assertTrue(l.selector)
-        self.assert_(l.selector)
+        self.assertTrue(l.selector)
-        self.assert_(l.selector)
+        self.assertTrue(l.selector)
-        self.assert_(l.selector)
+        self.assertTrue(l.selector)
-        self.assert_(l.selector)
+        self.assertTrue(l.selector)
-        self.assertEquals(converted.getcolors(), [(10000, COLOUR)])
+        self.assertEqual(converted.mode, 'RGB')
-        self.assertEquals(thumbnail.size, (10, 10))
+        self.assertEqual(thumbnail.mode, 'RGB')
-        self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])
+        self.assertEqual(converted.mode, 'RGB')
-        self.assertEquals(converted.getcolors(), [(10000, (205, 230, 255))])
+        self.assertEqual(converted.mode, 'RGB')
-        self.assertEquals(x.xpath("//span[@id='blank']/text()").extract(),
+        self.assertEqual(x.xpath("//span[@id='blank']/text()").extract(),
-        self.assertEquals([r.url for r in output],
+        self.assertEqual([r.url for r in output],
-        self.assertEquals([r.url for r in output],
+        self.assertEqual([r.url for r in output],
-        self.assertEquals([r.url for r in output],
+        self.assertEqual([r.url for r in output],
-        self.assertEquals(out, result)
+        self.assertEqual(out, result)
-        self.assertEquals(rdc, 1)
+        self.assertEqual(rdc, 1)
-        self.assertEquals(out2, [])
+        self.assertEqual(out2, [])
-        self.assertEquals(rdm, 1)
+        self.assertEqual(rdm, 1)
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals([],
+        self.assertEqual([],
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(None,
+        self.assertEqual(None,
-        self.assertEquals(out, onsite_reqs)
+        self.assertEqual(out, onsite_reqs)
-        self.assertEquals(out, reqs)
+        self.assertEqual(out, reqs)
-      self.assertEquals(out, reqs)
+      self.assertEqual(out, reqs)
-            self.assertEquals(out[0].headers.get('Referer'), referrer)
+            self.assertEqual(out[0].headers.get('Referer'), referrer)
-            self.assertEquals(mw.default_policy, p)
+            self.assertEqual(mw.default_policy, p)
-            self.assertEquals(mw.default_policy, p)
+            self.assertEqual(mw.default_policy, p)
-            self.assertEquals(out[0].headers.get('Referer'), init_referrer)
+            self.assertEqual(out[0].headers.get('Referer'), init_referrer)
-            self.assertEquals(request.headers.get('Referer'), final_referrer)
+            self.assertEqual(request.headers.get('Referer'), final_referrer)
-        self.assertEquals(out, [short_url_req])
+        self.assertEqual(out, [short_url_req])
-        self.assertEquals(p.query, 'param=value')
+        self.assertEqual(p.scheme, 's3')
-        self.assertEquals([], are_not_in)
+        self.assertEqual([], are_not_in)
-        self.assertEquals([11, 12, 14, 15, 17, 18], are_not_in)
+        self.assertEqual([11, 12, 14, 15, 17, 18], are_not_in)
-        self.assertEquals("abfg", chars)
+        self.assertEqual("abfg", chars)
-        self.assertEquals("abfg", chars)
+        self.assertEqual("abfg", chars)
-        self.failIf(errors)
+        self.assertFalse(errors)
-            self.assert_(all((isinstance(v, six.text_type) for v in result_row.values())))
+            self.assertTrue(all((isinstance(k, six.text_type) for k in result_row.keys())))
-        self.assertEquals(set([m.__name__ for m in mods]), set(expected))
+        self.assertEqual(set([m.__name__ for m in mods]), set(expected))
-        self.assertEquals(set([m.__name__ for m in mods]), set(expected))
+        self.assertEqual(set([m.__name__ for m in mods]), set(expected))
-        self.assertEquals(set([m.__name__ for m in mods]), set(expected))
+        self.assertEqual(set([m.__name__ for m in mods]), set(expected))
-            self.assertEquals(set([m.__name__ for m in mods]), set(expected))
+            self.assertEqual(set([m.__name__ for m in mods]), set(expected))
-        self.assertEquals('/absolute/path', data_path('/absolute/path'))
+        self.assertEqual('.scrapy/somepath', data_path('somepath'))
-            self.assertEquals(
+            self.assertEqual(
-            self.assertEquals('/absolute/path', data_path('/absolute/path'))
+            self.assertEqual('/absolute/path', data_path('/absolute/path'))
-        self.failIf(equal_attributes(a, b, []))
+        self.assertFalse(equal_attributes(a, b, []))
-        self.failIf(equal_attributes(a, b, ['x', 'y']))
+        self.assertFalse(equal_attributes(a, b, ['x', 'y']))
-        self.failIf(equal_attributes(a, b, ['x', 'y']))
+        self.assertFalse(equal_attributes(a, b, ['x', 'y']))
-        self.failIf(equal_attributes(a, b, ['x', 'y']))
+        self.assertFalse(equal_attributes(a, b, ['x', 'y']))
-        self.failIf(equal_attributes(a, b, [compare_z, 'x']))
+        self.assertFalse(equal_attributes(a, b, [compare_z, 'x']))
-        self.failIf(any(isinstance(x, six.text_type) for x in d2.values()))
+        self.assertFalse(d is d2)  # shouldn't modify in place
-        self.failIf(any(isinstance(x, six.text_type) for x in d2.values()))
+        self.assertFalse(d is d2)  # shouldn't modify in place
-        self.failIf(any(isinstance(x, six.text_type) for x in d2.keys()))
+        self.assertFalse(d is d2)  # shouldn't modify in place
-        self.assert_(isinstance(result[0][1], Failure))
+        self.assertTrue(isinstance(result[0][1], Failure))
-            self.assertEquals(client._parse(url), test, url)
+            self.assertEqual(client._parse(url), test, url)
-            self.assertEquals, to_bytes(s))
+            self.assertEqual, to_bytes(s))
-                self.assertEquals, to_bytes("127.0.0.1:%d" % self.portno)),
+                self.assertEqual, to_bytes("127.0.0.1:%d" % self.portno)),
-                self.assertEquals, to_bytes("www.example.com"))])
+                self.assertEqual, to_bytes("www.example.com"))])
-        d.addCallback(self.assertEquals, b"0123456789")
+        d.addCallback(self.assertEqual, b"0123456789")
-            self.assertEquals, to_bytes("127.0.0.1:%d" % self.portno))
+            self.assertEqual, to_bytes("127.0.0.1:%d" % self.portno))
-        self.assert_(b'404 - No Such Resource' in pageData)
+        self.assertTrue(b'404 - No Such Resource' in pageData)
-        self.assertEquals(factory.response_headers[b'content-length'], b'10')
+        self.assertEqual(factory.status, b'200')
-        self.assertEquals(pageData,
+        self.assertEqual(pageData,
-        self.assertEquals(
+        self.assertEqual(content_encoding, EncodingResource.out_encoding)
-        raise NotImplementedError('Spider.parse callback is not defined')
+        raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))
-        text = 'Random text response'
+        text = b'Random text'
-        raise NotImplementedError
+        raise NotImplementedError('Spider.parse callback is not defined')
-                    return __class__  # noqa https://github.com/scrapy/scrapy/issues/2836
+                    return __class__  # noqa  https://github.com/scrapy/scrapy/issues/2836
-                    return __class__
+                    return __class__  # noqa https://github.com/scrapy/scrapy/issues/2836
-                for loc in iterloc(s):
+                for loc in iterloc(s, self.sitemap_alternate_links):
-import time
+def _getargspec_py23(func):
-        func_args, _, _, _ = inspect.getargspec(func)
+        func_args, _, _, _ = _getargspec_py23(func)
-        spec = inspect.getargspec(func)
+        spec = _getargspec_py23(func)
-        spec = inspect.getargspec(func.__call__)
+        spec = _getargspec_py23(func.__call__)
-from scrapy.utils.log import scrapy_components_versions
+from scrapy.utils.versions import scrapy_components_versions
-
+import platform
-import OpenSSL
+from scrapy.utils.log import scrapy_components_versions
-            print("Platform  : %s" % platform.platform())
+            for name, version in scrapy_components_versions():
-        return '{} ({})'.format(OpenSSL.version.__version__, openssl)
+def scrapy_components_versions():
-
+    logger.info("Versions: %(versions)s}",
-        allowed_status = (301, 302, 303, 307)
+        allowed_status = (301, 302, 303, 307, 308)
-        if response.status in (301, 307) or request.method == 'HEAD':
+        if response.status in (301, 307, 308) or request.method == 'HEAD':
-            url = 'http://www.example.com/301'
+    def test_redirect_3xx_permanent(self):
-            rsp = Response(url, headers={'Location': url2}, status=301)
+            rsp = Response(url, headers={'Location': url2}, status=status)
-RETRY_HTTP_CODES = [500, 502, 503, 504, 408]
+RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408]
-            urls = [url.replace('localhost', '127.0.0.%d' % (x + 1)) for x in xrange(slots)]
+            urls = [url.replace('localhost', '127.0.0.%d' % (x + 1)) for x in range(slots)]
-            raise TypeError('callback must be a function, got %s' % type(callback).__name__)
+            raise TypeError('callback must be a callable, got %s' % type(callback).__name__)
-            raise TypeError('errback must be a function, got %s' % type(errback).__name__)
+            raise TypeError('errback must be a callable, got %s' % type(errback).__name__)
-        d.addCallback(self._cache_result, name)
+        if dnscache.limit:
-    Shell(spider.crawler).start(response=response)
+    Shell(spider.crawler).start(response=response, spider=spider)
-from setuptools import setup, find_packages
+from pkg_resources import parse_version
-elif hasattr(sys, "pypy_version_info"):
+if hasattr(sys, "pypy_version_info"):
-from scrapy.utils.python import to_bytes, to_unicode
+from scrapy.utils.python import to_bytes, to_unicode, garbage_collect
-import gc
+from scrapy.utils.python import garbage_collect
-        gc.collect()
+        garbage_collect()
-        self.assertEqual(get_func_args(operator.itemgetter(2)), [])
+        if platform.python_implementation() == 'CPython':
-        self.assertRaises(ValueError, q.push, lambda x: x)
+    test_nonserializable_object = nonserializable_object_test
-        self.assertRaises(ValueError, q.push, lambda x: x)
+    test_nonserializable_object = nonserializable_object_test
-            float('$10')
+            float(u'$10')
-    execute()
+    try:
-        * a Selector for ``<a>`` element, e.g.
+        * a Selector for ``<a>`` or ``<link>`` element, e.g.
-        raise ValueError("Only <a> elements are supported; got <%s>" %
+    if sel.root.tag not in ('a', 'link'):
-        raise ValueError("<a> element has no href attribute: %s" % sel)
+        raise ValueError("<%s> element has no href attribute: %s" %
-
+        # select <link> elements
-            errback='handle_error',
+            callback=self.spider.parse_item,
-        self._assert_serializes_ok(r)
+        self._assert_serializes_ok(r, spider=self.spider)
-        request = Request(url, opts.callback)
+        # Request requires callback argument as callable or None, not string
-                if opts.rules and self.first_response == response:
+                if opts.callback:
-    HTTPConnectionPool, ResponseFailed, URI
+    HTTPConnectionPool, ResponseFailed
-        proxyEndpoint = self._getEndpoint(self._proxyURI)
+        if twisted_version >= (15, 0, 0):
-                return self._ProxyAgent(reactor, proxyURI=proxy,
+                return self._ProxyAgent(reactor, proxyURI=to_bytes(proxy, encoding='ascii'),
-    HTTPConnectionPool, ResponseFailed
+    HTTPConnectionPool, ResponseFailed, URI
-    _ProxyAgent = ProxyAgent
+    _ProxyAgent = ScrapyProxyAgent
-                return self._ProxyAgent(endpoint)
+                return self._ProxyAgent(reactor, proxyURI=proxy,
-MEMUSAGE_REPORT = False
+from scrapy.cmdline import execute
-    values.extend(formdata.items())
+    values.extend((k, v) for k, v in formdata.items() if v is not None)
-[
+                'expected': b"""[
-""",
+]""",
-[
+                'expected': b"""[
-""",
+]""",
-[
+                'expected': b"""[
-[
+                'expected': b"""[
-[
+                'expected': b"""[
-<?xml version="1.0" encoding="utf-8"?>
+                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
-<?xml version="1.0" encoding="utf-8"?>
+                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
-<?xml version="1.0" encoding="utf-8"?>
+                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
-<?xml version="1.0" encoding="utf-8"?>
+                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
-<?xml version="1.0" encoding="utf-8"?>
+                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
-<?xml version="1.0" encoding="utf-8"?>
+                'expected': b"""<?xml version="1.0" encoding="utf-8"?>
-            self.assertEqual(row['expected'].strip(), data)
+            self.assertEqual(row['expected'], data)
-def ssl_context_factory(keyfile='keys/cert.pem', certfile='keys/cert.pem'):
+def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt'):
-    certfile = 'keys/cert.pem'
+    keyfile = 'keys/localhost.key'
-            txresponse._transport._producer.loseConnection()
+            # Abort connection inmediately.
-from scrapy.http import Request
+from scrapy.http import Headers, Request
-from tests.mockserver import MockServer, ssl_context_factory
+from tests.mockserver import MockServer, ssl_context_factory, Echo
-from tests import tests_datadir
+    render_POST = render_GET
-        self.putChild(b"files", File(os.path.join(tests_datadir, 'test_site/files/')))
+        try:
-            bodyproducer = _RequestBodyProducer(b'') if method == b'POST' else None
+            bodyproducer = None
-                    obj.__module__ == module.__name__:
+                    obj.__module__ == module.__name__ and \
-                d[cmdname] = cmd()
+            cmdname = cmd.__module__.split('.')[-1]
-            d[cmdname] = cmd()
+            if not cmd.__module__ == module:
-        self._test_retry(req, DNSLookupError('foo'), 5)
+    def test_with_settings_zero(self):
-        self.mw.max_retry_times = 2
+        req = Request(self.invalid_url)
-        # request with meta(max_retry_times) is called first
+    def test_with_metakey_zero(self):
-        req3 = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 4})
+        req = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})
-        self._test_retry(req2, DNSLookupError('foo'), 2)
+    def test_without_metakey(self):
-        # SETINGS: RETRY_TIMES > meta(max_retry_times)
+        # SETTINGS: RETRY_TIMES is NON-ZERO
-        self.mw.max_retry_times = 2
+        req = Request(self.invalid_url)
-        # request with meta(max_retry_times) is called second
+    def test_with_metakey_greater(self):
-        self._test_retry(req1, DNSLookupError('foo'), 3)
+        meta_max_retry_times = 3
-        self._test_retry(req3, DNSLookupError('foo'), 4)
+        meta_max_retry_times = 4
-        self.mw.max_retry_times = 2
+        req1 = Request(self.invalid_url, meta={'max_retry_times': meta_max_retry_times})
-        while max_retry_times > 0:
+        for i in range(0, max_retry_times):
-
+            bodyproducer = _RequestBodyProducer(b'') if method == b'POST' else None
-    def test_different_retry(self):
+
-        req = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 3})
+        req = Request('http://www.scrapytest.org/invalid_url')
-        self._test_retry(req, DNSLookupError('foo'), 3)
+        # SETTINGS: RETRY_TIMES is NON-ZERO
-        req2 = Request('http://www.scrapytest.org/invalid_url')
+        # SETTINGS: RETRY_TIMES = 0
-        self._test_retry(req2, DNSLookupError('foo'), 0)
+        # SETINGS: RETRY_TIMES > meta(max_retry_times)
-        self._test_retry(req2, DNSLookupError('foo'), 4)
+        self.mw.max_retry_times = 5
-        retry_times = request.meta.get('max_retry_times') or self.max_retry_times
+        retry_times = self.max_retry_times
-    def test_init(self):
+    def test_init_dict(self):
-    ( 'python', _embed_standard_shell),
+    ('ptpython', _embed_ptpython_shell),
-        self._test_retry(req2, DNSLookupError('foo'))
+        req = Request('http://www.scrapytest.org/invalid_url', meta={'max_retry_times': 3})
-        assert stats.get_value('retry/count') == 3
+        # SETINGS: meta(max_retry_times) = 3, RETRY_TIMES = 2
-    def _test_retry(self, req, exception):
+        req2 = Request('http://www.scrapytest.org/invalid_url')
-        assert isinstance(req, Request)
+        # SETINGS: RETRY_TIMES < meta(max_retry_times)
-        retry_times = req.meta.get('max_retry_times') or self.mw.max_retry_times
+        # SETINGS: RETRY_TIMES = 0
-        while req.meta['retry_times'] != retry_times:
+    def _test_retry(self, req, exception, max_retry_times):
-        self.assertEqual(req.meta['retry_times'], retry_times)
+            if req.meta['retry_times'] == max_retry_times:
-            retry_times = request.meta['max_retry_times']
+        retry_times = request.meta.get('max_retry_times') or self.max_retry_times
-from collections import OrderedDict
+from collections import OrderedDict, Mapping
-        seq = seq.items() if isinstance(seq, dict) else seq
+        seq = seq.items() if isinstance(seq, Mapping) else seq
-        except:  # FIXME: catching everything!
+        except os.error:
-            self.max_retry_times = request.meta['max_retry_times']
+            retry_times = request.meta['max_retry_times']
-        if retries <= self.max_retry_times:
+        if retries <= retry_times:
-            'image_urls': [
+            self.media_key: [],
-class MediaDownloadCrawlTestCase(TestCase):
+class FileDownloadCrawlTestCase(TestCase):
-            'IMAGES_STORE': self.tmpmediastore,
+            'ITEM_PIPELINES': {self.pipeline_class: 1},
-        crawler = self.runner.create_crawler(spider_class)
+    def _create_crawler(self, spider_class, **kwargs):
-        self.assertIn('images', items[0])
+        self.assertIn(self.media_key, items[0])
-        self.assertEqual(checksums, self.expected_checksums)
+        # check that the images/files checksums are what we know they should be
-            for i in item['images']:
+            for i in item[self.media_key]:
-        # check that the item does NOT have the "images" field populated
+        # check that the item does NOT have the "images/files" field populated
-        self.assertFalse(items[0]['images'])
+        self.assertIn(self.media_key, items[0])
-            yield crawler.crawl("http://localhost:8998/files/images/")
+            yield crawler.crawl("http://localhost:8998/files/images/",
-            yield crawler.crawl("http://localhost:8998/files/images/")
+            yield crawler.crawl("http://localhost:8998/files/images/",
-            yield crawler.crawl("http://localhost:8998/files/images/")
+            yield crawler.crawl("http://localhost:8998/files/images/",
-            yield crawler.crawl("http://localhost:8998/files/images/")
+            yield crawler.crawl("http://localhost:8998/files/images/",
-                        'SPIDER_LOADER_WARN_ONLY': True}
+    default_settings = {'LOG_ENABLED': False}
-            SpiderLoader.from_settings(settings)
+        self.assertRaises(ImportError, SpiderLoader.from_settings, settings)
-    default_settings = {'LOG_ENABLED': False}
+    default_settings = {'LOG_ENABLED': False,
-    default_settings = {'LOG_ENABLED': False}
+    default_settings = {'LOG_ENABLED': False,
-    default_settings = {'LOG_ENABLED': False}
+    default_settings = {'LOG_ENABLED': False,
-    
+
-    default_settings = {'LOG_ENABLED': False}
+    default_settings = {'LOG_ENABLED': False,
-                warnings.warn(msg, RuntimeWarning)
+                if self.warn_only:
-            settings = Settings({'SPIDER_MODULES': [module]})
+            settings = Settings({'SPIDER_MODULES': [module],
-
+        # there is a small difference between the behaviour or JsonItemExporter.indent
-        if self.indent is not None:
+    def _beautify_newline(self, new_item=False):
-        self._beautify_newline()
+        self._beautify_newline(new_item=True)
-        self._beautify_newline()
+        self._beautify_newline(new_item=True)
-FEED_EXPORT_INDENT = None
+FEED_EXPORT_INDENT = 0
-            settings = {'FEED_FORMAT': fmt, 'FEED_STORE_EMPTY': True}
+            settings = {'FEED_FORMAT': fmt, 'FEED_STORE_EMPTY': True, 'FEED_EXPORT_INDENT': None}
-        items = [dict({'foo': ['bar']}), dict({'key': 'value'})]
+        items = [
-        output = [
+        test_cases = [
-}
+{"foo": ["bar"]},
-}
+{"foo": ["bar"]},
-                'expected': b'<?xml version="1.0" encoding="utf-8"?>\n<items><item><foo><value>bar</value></foo></item><item><key>value</key></item></items>',
+                'expected': b"""
-</item>
+<item><foo><value>bar</value></foo></item>
-</item>
+<item><foo><value>bar</value></foo></item>
-        for row in output:
+        for row in test_cases:
-        self.indent_width = options.pop('indent_width', None)
+        self.indent = options.pop('indent', None)
-        self.encoder = ScrapyJSONEncoder(indent=self.indent_width, **kwargs)
+        kwargs.setdefault('indent', self.indent)
-        self.file.write(b"[\n")
+        self.file.write(b"[")
-        self.file.write(b"\n]")
+        self._beautify_newline()
-            self.file.write(b',\n')
+            self.file.write(b',')
-        if self.indent_width:
+        if self.indent is not None:
-            self._xg_characters(' ' * self.indent_width * depth)
+        if self.indent:
-        self.indent_width = settings.getint('FEED_EXPORT_INDENT_WIDTH') or None
+        self.indent = None
-            encoding=self.export_encoding, indent_width=self.indent_width)
+            encoding=self.export_encoding, indent=self.indent)
-FEED_EXPORT_INDENT_WIDTH = None
+FEED_EXPORT_INDENT = None
-            ('json', b'[\n\n]'),
+            ('json', b'[]'),
-            'json': u'[\n{"foo": "Test\\u00d6"}\n]'.encode('utf-8'),
+            'json': u'[{"foo": "Test\\u00d6"}]'.encode('utf-8'),
-            settings = {'FEED_FORMAT': format, 'FEED_EXPORT_INDENT_WIDTH': None}
+            settings = {'FEED_FORMAT': format, 'FEED_EXPORT_INDENT': None}
-            'json': u'[\n{"foo": "Test\xd6"}\n]'.encode('latin-1'),
+            'json': u'[{"foo": "Test\xd6"}]'.encode('latin-1'),
-        settings = {'FEED_EXPORT_INDENT_WIDTH': None, 'FEED_EXPORT_ENCODING': 'latin-1'}
+        settings = {'FEED_EXPORT_INDENT': None, 'FEED_EXPORT_ENCODING': 'latin-1'}
-        items = [dict({'foo': ['bar']})]
+        items = [dict({'foo': ['bar']}), dict({'key': 'value'})]
-                'expected': b'[\n{"foo": ["bar"]}\n]',
+                'indent': None,
-                'indent_width': 2,
+                'indent': -1,
-                'indent_width': 4,
+                'indent': 4,
-                'indent_width': 5,
+                'indent': 5,
-                'expected': b'<?xml version="1.0" encoding="utf-8"?>\n<items><item><foo><value>bar</value></foo></item></items>',
+                'indent': None,
-                'indent_width': 2,
+                'indent': -1,
-                'indent_width': 4,
+                'indent': 4,
-                'indent_width': 5,
+                'indent': 5,
-            settings = {'FEED_FORMAT': row['format'], 'FEED_EXPORT_INDENT_WIDTH': row['indent_width']}
+            settings = {'FEED_FORMAT': row['format'], 'FEED_EXPORT_INDENT': row['indent']}
-        self.encoder = ScrapyJSONEncoder(**kwargs)
+        self.encoder = ScrapyJSONEncoder(indent=self.indent_width, **kwargs)
-            self._export_xml_field(name, value)
+            self._export_xml_field(name, value, depth=2)
-    def _export_xml_field(self, name, serialized_value):
+    def _export_xml_field(self, name, serialized_value, depth):
-                self._export_xml_field(subname, value)
+                self._export_xml_field(subname, value, depth=depth+1)
-                self._export_xml_field('value', value)
+                self._export_xml_field('value', value, depth=depth+1)
-            encoding=self.export_encoding)
+            encoding=self.export_encoding, indent_width=self.indent_width)
-            settings = {'FEED_FORMAT': format}
+        for format, expected in formats.items():
-            self.assertEqual(formats[format], data)
+            self.assertEqual(expected, data)
-            settings = {'FEED_FORMAT': format, 'FEED_EXPORT_ENCODING': 'latin-1'}
+        settings = {'FEED_EXPORT_INDENT_WIDTH': None, 'FEED_EXPORT_ENCODING': 'latin-1'}
-            self.assertEqual(formats[format], data)
+            self.assertEqual(expected, data)
-                warnings.warn(msg, UserWarning)
+        self._check_name_duplicates()
-            self.assertIn("several spiders with the same name 'spider3'", str(w[0].message))
+            msg = str(w[0].message)
-            self.assertIn("several spiders with the same name 'spider2'", msgs[1])
+            self.assertEqual(len(w), 1)
-            self.assertIn("several spiders with the same name (spider3)", str(w[0].message))
+            self.assertIn("several spiders with the same name 'spider3'", str(w[0].message))
-            self.assertIn("several spiders with the same name (spider2)", msgs[1])
+            self.assertIn("several spiders with the same name 'spider1'", msgs[0])
-                              "), this can cause unexpected behavior", UserWarning)
+                msg = ("There are several spiders with the same name {!r}:\n"
-        assert 'Content-Encoding' not in newresponse.headers
+        self.assertIsNot(newresponse, response)
-        assert 'Content-Encoding' not in newresponse.headers
+        self.assertIsNot(newresponse, response)
-        assert 'Content-Encoding' not in newresponse.headers
+        self.assertIsNot(newresponse, response)
-        crawler = self.runner.create_crawler(BrokenStartRequestsSpider)
+            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)
-        crawler = self.runner.create_crawler(BrokenStartRequestsSpider)
+            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)
-    return response.body[:2] == b'\x1f\x8b'
+    return response.body[:3] == b'\x1f\x8b\x08'
-from scrapy.utils.gz import gunzip, is_gzipped
+from scrapy.utils.gz import gunzip
-                pass
+            return gunzip(response.body)
-            if content_encoding and not is_gzipped(response):
+            if content_encoding:
-from scrapy.utils.gz import gunzip, is_gzipped
+from scrapy.utils.gz import gunzip, gzip_magic_number
-        elif response.url.endswith('.xml'):
+        elif gzip_magic_number(response):
-            return gunzip(response.body)
+
-        self.assertEqual(response.headers['Content-Type'], b'application/gzip')
+        assert newresponse is not response
-        self.assertEqual(response.headers['Content-Type'], b'application/octet-stream')
+        assert newresponse is not response
-        self.assertEqual(response.headers['Content-Type'], b'binary/octet-stream')
+        assert newresponse is not response
-                    url=response.url)
+                    url=response.url, body=decoded_body)
-                    resp_or_url.headers.get('Referrer-Policy', '').decode('latin1'))
+                policy_header = resp_or_url.headers.get('Referrer-Policy')
-        d.addBoth(self._downloaded, slot, request, spider)
+        d.addBoth(self._downloaded, self.slot, request, spider)
-
+
-                                    base_class_name="MediaPipeline")
+                                    base_class_name="MediaPipeline",
-from scrapy.utils.log import LogCounterHandler, configure_logging, log_scrapy_info
+from scrapy.utils.log import (
-        handler = LogCounterHandler(self, level=settings.get('LOG_LEVEL'))
+        handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))
-        logging.root.addHandler(handler)
+        install_scrapy_root_handler(settings)
-            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)
+        crawler = self.runner.create_crawler(BrokenStartRequestsSpider)
-            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)
+import logging
-            request = self._modify_media_request(request)
+            self._modify_media_request(request)
-        assert self.pipe._modify_media_request(request).meta == {'handle_httpstatus_all': True}
+        self.pipe._modify_media_request(request)
-        self.assertIn('handle_httpstatus_list', meta)
+        self.pipe._modify_media_request(request)
-                self.assertIn(status, meta['handle_httpstatus_list'])
+                self.assertIn(status, request.meta['handle_httpstatus_list'])
-                self.assertNotIn(status, meta['handle_httpstatus_list'])
+                self.assertNotIn(status, request.meta['handle_httpstatus_list'])
-from scrapy.downloadermiddlewares.redirect import RedirectMiddleware
+from scrapy.utils.datatypes import SequenceExclude
-        )
+        self._handle_statuses(self.allow_redirects)
-                                   if i not in RedirectMiddleware.allowed_status]
+    def _handle_statuses(self, allow_redirects):
-            request.meta['handle_httpstatus_list'] = self.httpstatus_list
+        if self.handle_httpstatus_list:
-                                        settings=Settings())
+                                        settings=Settings(self.settings))
-        self.pipe.handle_httpstatus_list = None
+class MediaPipelineAllowRedirectsTestCase(BaseMediaPipelineTestCase):
-        self.pipe.allow_redirects = False
+        meta = self.pipe._modify_media_request(request).meta
-        if 'Location' not in response.headers or response.status not in self.allowed_status:
+        allowed_status = (301, 302, 303, 307)
-            resolve('MEDIA_ALLOW_REDIRECTS'), self.ALLOW_REDIRECTS
+            resolve('MEDIA_ALLOW_REDIRECTS'), False
-            request.meta['handle_httpstatus_list'] = httpstatus_list
+        if self.httpstatus_list:
-        correct = {'handle_httpstatus_list': list(range(300)) + list(range(400,1000))}
+        correct = {'handle_httpstatus_list': [i for i in range(1000)
-        self.allow_httpstatus_list = settings.getlist(
+        self.handle_httpstatus_list = settings.getlist(
-        elif self.allow_redirects:
+        if self.handle_httpstatus_list:
-                httpstatus_list = list(range(0, 300)) + list(range(400, 1000))
+                httpstatus_list = [i for i in range(1000)
-                        pass
+                httpstatus_list = [i for i in httpstatus_list
-        self.pipe.allow_httpstatus_list = list(range(100))
+        self.pipe.handle_httpstatus_list = list(range(100))
-        self.pipe.allow_httpstatus_list = None
+        self.pipe.handle_httpstatus_list = None
-        if 'Location' not in response.headers or response.status not in allowed_status:
+        if 'Location' not in response.headers or response.status not in self.allowed_status:
-        self.pipe = self.pipeline_class(download_func=_mocked_download_func)
+        self.pipe = self.pipeline_class(download_func=_mocked_download_func,
-        super(FilesPipeline, self).__init__(download_func=download_func)
+        super(FilesPipeline, self).__init__(download_func=download_func, settings=settings)
-    def __init__(self, download_func=None):
+    def __init__(self, download_func=None, settings=None):
-
+        resolve = functools.partial(self._key_for_pipe,
-            request.meta['handle_httpstatus_all'] = True
+            request = self._modify_media_request(request)
-        pass
+        logger.debug("Using filesystem cache storage in %(cachedir)s" % {'cachedir': self.cachedir},
-
+        logger.debug("Using DBM cache storage in %(cachepath)s" % {'cachepath': dbpath}, extra={'spider': spider})
-
+        logger.debug("Using LevelDB cache storage in %(cachepath)s" % {'cachepath': dbpath}, extra={'spider': spider})
-    def referrer(self, response, request):
+    def referrer(self, response_url, request_url):
-    def referrer(self, response, request):
+    def referrer(self, response_url, request_url):
-            return self.stripped_referrer(response)
+    def referrer(self, response_url, request_url):
-            return self.stripped_referrer(response)
+    def referrer(self, response_url, request_url):
-        return self.origin_referrer(response)
+    def referrer(self, response_url, request_url):
-            return self.origin_referrer(response)
+    def referrer(self, response_url, request_url):
-            return self.stripped_referrer(response)
+    def referrer(self, response_url, request_url):
-            return self.origin_referrer(response)
+    def referrer(self, response_url, request_url):
-        return self.stripped_referrer(response)
+    def referrer(self, response_url, request_url):
-            return self.strip_url(r)
+    def stripped_referrer(self, url):
-            return self.origin(r)
+    def origin_referrer(self, url):
-    def origin(self, r):
+    def origin(self, url):
-        return self.strip_url(r, origin_only=True)
+        return self.strip_url(url, origin_only=True)
-        return response.url
+        return response
-
+import logging
-            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3']))
+            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))
-            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3']))
+            self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))
-        self._spiders[spcls.name] = spcls
+            self._spiders[spcls.name] = spcls
-            self._spiders[spcls.name] = spcls
+            if spcls.name in self._spiders.keys():
-
+    AWS_ENDPOINT_URL = None
-                aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY)
+                's3',
-            })
+        })
-        
+
-            self.assertEqual(strip_url(urlparse(i), origin_only=True), o)
+import warnings
-        with self.assertRaises(NotConfigured):
+        with self.assertRaises(RuntimeError):
-                        raise NotConfigured("Unknown referrer policy name %r" % policy)
+            self.default_policy = _load_policy_class(
-        return cls()
+        if policy_name is None:
-        cls = _policy_classes.get(policy_name.lower(), self.default_policy)
+        cls = _policy_classes.get(policy_name.lower()) if policy_name else self.default_policy
-                    request).referrer(orig_url, request.url)
+                # the request's referrer header value acts as a surrogate
-class TestRefererMiddlewareSettingsNoReferrer(MixinNoReferrer, TestRefererMiddleware):
+class TestSettingsNoReferrer(MixinNoReferrer, TestRefererMiddleware):
-class TestRefererMiddlewareSettingsNoReferrerWhenDowngrade(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):
+class TestSettingsNoReferrerWhenDowngrade(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):
-class TestRefererMiddlewareSettingsSameOrigin(MixinSameOrigin, TestRefererMiddleware):
+class TestSettingsSameOrigin(MixinSameOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareSettingsOrigin(MixinOrigin, TestRefererMiddleware):
+class TestSettingsOrigin(MixinOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareSettingsStrictOrigin(MixinStrictOrigin, TestRefererMiddleware):
+class TestSettingsStrictOrigin(MixinStrictOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareSettingsOriginWhenCrossOrigin(MixinOriginWhenCrossOrigin, TestRefererMiddleware):
+class TestSettingsOriginWhenCrossOrigin(MixinOriginWhenCrossOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareSettingsStrictOriginWhenCrossOrigin(MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware):
+class TestSettingsStrictOriginWhenCrossOrigin(MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareSettingsUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):
+class TestSettingsUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):
-class TestRefererMiddlewareSettingsCustomPolicy(TestRefererMiddleware):
+class TestSettingsCustomPolicy(TestRefererMiddleware):
-class TestRefererMiddlewareDefaultMeta(MixinDefault, TestRefererMiddleware):
+class TestRequestMetaDefault(MixinDefault, TestRefererMiddleware):
-class TestRefererMiddlewareNoReferrer(MixinNoReferrer, TestRefererMiddleware):
+class TestRequestMetaNoReferrer(MixinNoReferrer, TestRefererMiddleware):
-class TestRefererMiddlewareNoReferrerWhenDowngrade(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):
+class TestRequestMetaNoReferrerWhenDowngrade(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):
-class TestRefererMiddlewareSameOrigin(MixinSameOrigin, TestRefererMiddleware):
+class TestRequestMetaSameOrigin(MixinSameOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareOrigin(MixinOrigin, TestRefererMiddleware):
+class TestRequestMetaOrigin(MixinOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareSrictOrigin(MixinStrictOrigin, TestRefererMiddleware):
+class TestRequestMetaSrictOrigin(MixinStrictOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareOriginWhenCrossOrigin(MixinOriginWhenCrossOrigin, TestRefererMiddleware):
+class TestRequestMetaOriginWhenCrossOrigin(MixinOriginWhenCrossOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareStrictOriginWhenCrossOrigin(MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware):
+class TestRequestMetaStrictOriginWhenCrossOrigin(MixinStrictOriginWhenCrossOrigin, TestRefererMiddleware):
-class TestRefererMiddlewareUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):
+class TestRequestMetaUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):
-class TestRefererMiddlewareMetaPredecence001(MixinUnsafeUrl, TestRefererMiddleware):
+class TestRequestMetaPredecence001(MixinUnsafeUrl, TestRefererMiddleware):
-class TestRefererMiddlewareMetaPredecence002(MixinNoReferrer, TestRefererMiddleware):
+class TestRequestMetaPredecence002(MixinNoReferrer, TestRefererMiddleware):
-class TestRefererMiddlewareMetaPredecence003(MixinUnsafeUrl, TestRefererMiddleware):
+class TestRequestMetaPredecence003(MixinUnsafeUrl, TestRefererMiddleware):
-class TestRefererMiddlewareSettingsPolicyByName(TestCase):
+class TestSettingsPolicyByName(TestCase):
-class TestRefererMiddlewarePolicyHeaderPredecence001(MixinUnsafeUrl, TestRefererMiddleware):
+class TestPolicyHeaderPredecence001(MixinUnsafeUrl, TestRefererMiddleware):
-class TestRefererMiddlewarePolicyHeaderPredecence002(MixinNoReferrer, TestRefererMiddleware):
+class TestPolicyHeaderPredecence002(MixinNoReferrer, TestRefererMiddleware):
-class TestRefererMiddlewarePolicyHeaderPredecence003(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):
+class TestPolicyHeaderPredecence003(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):
-class TestReferrerPolicyOnRedirect(TestRefererMiddleware):
+class TestReferrerOnRedirect(TestRefererMiddleware):
-
+        (   'http://scrapytest.org/1',      # parent
-
+        ),
-    def test_(self):
+        self.referrermw = RefererMiddleware(settings)
-            request = self.get_request(target_chain.pop())
+    def test(self):
-            out = list(self.mw.process_spider_output(response, [request], self.spider))
+            out = list(self.referrermw.process_spider_output(response, [request], self.spider))
-            request.meta['redirected_urls'] = target_chain
+            for status, url in redirections:
-        scheme = urlparse_cached(request).scheme
+        scheme = urlparse(request).scheme
-        return self.strip_url(r)
+        if urlparse(r).scheme not in self.NOREFERRER_SCHEMES:
-        return self.strip_url(r, origin_only=True)
+        if urlparse(r).scheme not in self.NOREFERRER_SCHEMES:
-    def strip_url(self, r, origin_only=False):
+    def strip_url(self, url, origin_only=False):
-        if r is None or not r.url:
+        if not url:
-                             origin_only=origin_only)
+        return strip_url(url,
-    def potentially_trustworthy(self, r):
+    def potentially_trustworthy(self, url):
-        parsed_url = urlparse_cached(r)
+        parsed_url = urlparse(url)
-        return self.tls_protected(r)
+        return self.tls_protected(url)
-        return urlparse_cached(r).scheme in ('https', 'ftps')
+    def tls_protected(self, url):
-    def policy(self, response, request):
+    def policy(self, resp_or_url, request):
-                response.headers.get('Referrer-Policy', '').decode('latin1'))
+            if isinstance(resp_or_url, Response):
-                referrer = self.policy(response, r).referrer(response, r)
+                referrer = self.policy(response, r).referrer(response.url, r.url)
-                    request).referrer(faked_response, request)
+                initial_url = redirected_urls[0]
-        parsed_url = url
+    parsed_url = urlparse(url)
-REFERER_POLICY = 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'
+REFERRER_POLICY = 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'
-            policy = settings.get('REFERER_POLICY')
+            policy = settings.get('REFERRER_POLICY')
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerPolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.OriginPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginPolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.StrictOriginPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.StrictOriginPolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.UnsafeUrlPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.UnsafeUrlPolicy'}
-    settings = {'REFERER_POLICY': 'tests.test_spidermiddleware_referer.CustomPythonOrgPolicy'}
+    settings = {'REFERRER_POLICY': 'tests.test_spidermiddleware_referer.CustomPythonOrgPolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}
-            settings = Settings({'REFERER_POLICY': s})
+            settings = Settings({'REFERRER_POLICY': s})
-            settings = Settings({'REFERER_POLICY': s.upper()})
+            settings = Settings({'REFERRER_POLICY': s.upper()})
-        settings = Settings({'REFERER_POLICY': 'some-custom-unknown-policy'})
+        settings = Settings({'REFERRER_POLICY': 'some-custom-unknown-policy'})
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}
-    settings = {'REFERER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}
+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}
-
+    """
-        if ((self.tls_protected(response) and self.potentially_trustworthy(request))
+        if ((self.tls_protected(response) and
-        elif ((urlparse_cached(response).scheme in ('https', 'ftps') and
+        elif ((self.tls_protected(response) and
-              or urlparse_cached(response).scheme == 'http'):
+              or not self.tls_protected(response)):
-        ('ftp://example4.com/urls.zip',     'http://example4.com/not-page.html',    None),
+
-             or urlparse_cached(response).scheme == 'http'):
+        if ((self.tls_protected(response) and self.potentially_trustworthy(request))
-    and requests from request clients which are not TLS-protected to any origin.
+    The "no-referrer-when-downgrade" policy sends a full URL along with requests
-    Requests from TLS-protected request clients to non-a priori authenticated URLs,
+    Requests from TLS-protected clients to non-potentially trustworthy URLs,
-        return self.stripped_referrer(response)
+        if not self.tls_protected(response) or self.tls_protected(request):
-        return self.strip_url(req_or_resp)
+    def stripped_referrer(self, r):
-        return self.strip_url(req_or_resp, origin_only=True)
+    def origin_referrer(self, r):
-    def strip_url(self, req_or_resp, origin_only=False):
+    def strip_url(self, r, origin_only=False):
-        if req_or_resp.url is None or not req_or_resp.url:
+        if r is None or not r.url:
-        parsed_url = urlparse_cached(req_or_resp)
+        parsed_url = urlparse_cached(r)
-    def origin(self, req_or_resp):
+    def origin(self, r):
-        return self.strip_url(req_or_resp, origin_only=True)
+        return self.strip_url(r, origin_only=True)
-        if urlparse_cached(response).scheme == urlparse_cached(request).scheme:
+        if ((urlparse_cached(response).scheme == 'https' and
-            return origin
+        elif ((urlparse_cached(response).scheme in ('https', 'ftps') and
-
+class MixinStrictOrigin(object):
-from scrapy.utils.url import strip_url_credentials
+from scrapy.utils.url import strip_url
-            return strip_url_credentials(parsed_url, origin_only=origin_only)
+            return strip_url(parsed_url,
-def strip_url_credentials(url, origin_only=False, keep_fragments=False):
+def strip_url(url, strip_credentials=True, strip_default_port=True, origin_only=False, strip_fragment=True):
-    if parsed_url.username or parsed_url.password:
+    if (strip_credentials or origin_only) and (parsed_url.username or parsed_url.password):
-        if (parsed_url.scheme, parsed_url.port) in (('http', 80), ('https', 443)):
+    if strip_default_port and parsed_url.port:
-        '' if not keep_fragments else parsed_url.fragment
+        '' if strip_fragment else parsed_url.fragment
-                              parse_url, strip_url_credentials)
+                              parse_url, strip_url)
-class StripUrlCredentials(unittest.TestCase):
+class StripUrl(unittest.TestCase):
-        self.assertEqual(strip_url_credentials(
+        self.assertEqual(strip_url(
-        self.assertEqual(strip_url_credentials(
+        self.assertEqual(strip_url(
-            'http://www.example.com/index.html?somekey=somevalue#section', keep_fragments=True),
+        self.assertEqual(strip_url(
-            'http://www.example.com/index.html?somekey=somevalue')
+    def test_path(self):
-            'https://www.example.com/index.html?somekey=somevalue')
+            ('http://www.example.com',
-            'ftp://www.example.com/index.html?somekey=somevalue')
+            ('http://www.example.com',
-            'http://www.example.com/index.html')
+    def test_credentials(self):
-            'http://www.example.com:8080/index.html')
+            ('http://username:password@www.example.com:443/index.html',
-            'https://www.example.com/index.html')
+            ('https://username:password@www.example.com:443/index.html',
-            'https://www.example.com:442/index.html')
+            ('https://username:password@www.example.com:442/index.html',
-            'https://www.example.com/')
+        for i, o in [
-
+from scrapy.utils.url import strip_url_credentials
-            return urlunparse(stripped)
+        return self.strip_url(req_or_resp)
-            return urlunparse(stripped)
+        return self.strip_url(req_or_resp, origin_only=True)
-                           '')
+        parsed_url = urlparse_cached(req_or_resp)
-        return tuple(self.strip_url(req_or_resp, origin_only=True)[:3])
+        """Return serialized origin (scheme, host, path) for a request or response URL."""
-            return urlunparse(origin + ('', '', ''))
+            return origin
-from six.moves.urllib.parse import (ParseResult, urldefrag, urlparse)
+from six.moves.urllib.parse import (ParseResult, urldefrag, urlparse, urlunparse)
-                              add_http_if_no_scheme, guess_scheme, parse_url)
+                              add_http_if_no_scheme, guess_scheme,
-from scrapy.http import Request
+from scrapy.http import Request, Response
-        return cls(crawler.settings)
+        mw = cls(crawler.settings)
-    SameOriginPolicy, UnsafeUrlPolicy
+    SameOriginPolicy, UnsafeUrlPolicy, ReferrerPolicy
-            except ValueError:
+    def __init__(self, settings=None):
-            self.default_policy = DefaultReferrerPolicy
+                    self.default_policy = load_object(policy)
-                    self.default_policy = _policy_classes[policy]
+                    self.default_policy = _policy_classes[policy.lower()]
-    POLICY_UNSAFE_URL
+    POLICY_SCRAPY_DEFAULT, POLICY_UNSAFE_URL, \
-            request = Request(target)
+        settings = Settings(self.settings)
-            self.assertEquals(out[0].headers.get('Referer'), referrer)
+    def get_request(self, target):
-    def test_policy_no_referrer(self):
+    def get_response(self, origin):
-            request = Request(target, meta={'referrer_policy': POLICY_NO_REFERRER})
+    def test(self):
-            self.assertEquals(out[0].headers.get('Referer'), referrer)
+class MixinDefault(object):
-            request = Request(target, meta={'referrer_policy': POLICY_SAME_ORIGIN})
+        # Different port: send origin as referrer
-            self.assertEquals(out[0].headers.get('Referer'), referrer)
+        # Different protocols: send origin as referrer
-    def test_policy_origin(self):
+        # test for user/password stripping
-            request = Request(target, meta={'referrer_policy': POLICY_ORIGIN})
+class MixinUnsafeUrl(object):
-            self.assertEquals(out[0].headers.get('Referer'), referrer)
+        # TLS to non-TLS: send referrer (yes, it's unsafe)
-            request = Request(target, meta={'referrer_policy': POLICY_ORIGIN_WHEN_CROSS_ORIGIN})
+        # non-TLS to TLS or non-TLS: send referrer (yes, it's unsafe)
-            self.assertEquals(out[0].headers.get('Referer'), referrer)
+        # test for user/password stripping
-            self.assertEquals(out[0].headers.get('Referer'), referrer)
+class TestRefererMiddlewareDefault(MixinDefault, TestRefererMiddleware):
-        self.default_policy = policy_class
+    def __init__(self, settings={}):
-        return cls()
+        return cls(crawler.settings)
-            return urlunparse(stripped)
+        return self.stripped_referrer(response)
-                return urlunparse(stripped)
+            return self.stripped_referrer(response)
-            return urlunparse(origin)
+        return self.origin_referrer(response)
-                return urlunparse(stripped)
+            return self.stripped_referrer(response)
-            return urlunparse(stripped)
+        return self.stripped_referrer(response)
-from six.moves.urllib.parse import urlparse, urlunparse, ParseResult
+from six.moves.urllib.parse import ParseResult, urlunparse
-    def strip_url_parsed(self, req_or_resp, origin_only=False):
+    def strip_url(self, req_or_resp, origin_only=False):
-    def origin_parsed(self, req_or_resp):
+    def origin(self, req_or_resp):
-        return tuple(self.strip_url_parsed(req_or_resp, origin_only=True)[:3])
+        return tuple(self.strip_url(req_or_resp, origin_only=True)[:3])
-        stripped = self.strip_url_parsed(response)
+        stripped = self.strip_url(response)
-            stripped = self.strip_url_parsed(response)
+        if self.origin(response) == self.origin(request):
-        origin = self.strip_url_parsed(response, origin_only=True)
+        origin = self.strip_url(response, origin_only=True)
-            stripped = self.strip_url_parsed(response)
+        origin = self.origin(response)
-        return self.strip_url(referrer_source)
+        stripped = self.strip_url(response)
-            return urlunparse(stripped)
+        origin = self.strip_url_parsed(response, origin_only=True)
-            return self.strip_url(referrer_source, origin_only=False)
+        origin = self.origin_parsed(response)
-
+            return urlunparse(origin + ('', '', ''))
-        return self.strip_url(response.url, origin_only=True)
+        stripped = self.strip_url_parsed(response, origin_only=True)
-from six.moves.urllib.parse import urlsplit, urlunsplit
+from six.moves.urllib.parse import urlparse, urlunparse, ParseResult
-        parsed = urlsplit(url, allow_fragments=True)
+        parsed = urlparse(url, allow_fragments=True)
-        return urlunsplit((
+        return urlunparse((
-            urlsplit(target_url).scheme in ('http',):
+        if urlparse_cached(response).scheme in ('https', 'ftps') and \
-        return referrer_url
+        stripped = self.strip_url_parsed(response)
-            return None
+        if self.origin_parsed(response) == self.origin_parsed(request):
-            netloc = parsed.netloc
+            netloc = netloc.replace('{p.username}:{p.password}@'.format(p=parsed), '')
-            '' if origin_only else parsed.path,
+            '/' if origin_only else parsed.path,
-    name = "no-referrer"
+    name = POLICY_NO_REFERRER
-    name = "no-referrer-when-downgrade"
+    name = POLICY_NO_REFERRER_WHEN_DOWNGRADE
-    name = "same-origin"
+    name = POLICY_SAME_ORIGIN
-        if urlsplit(referrer_source).netloc == urlsplit(target_url).netloc:
+        if self.origin(referrer_source) == self.origin(target_url):
-    name = "origin"
+    name = POLICY_ORIGIN
-        return self.strip_url(referrer_source, origin_only=True)
+        return self.strip_url(response.url, origin_only=True)
-    name = "origin-when-cross-origin"
+    name = POLICY_ORIGIN_WHEN_CROSS_ORIGIN
-    name = "unsafe-url"
+    name = POLICY_UNSAFE_URL
-_policies = {p.name: p for p in (
+_policy_classes = {p.name: p for p in (
-            policy_name = to_native_str(response.headers.get('Referrer-Policy', '').decode('latin1'))
+            policy_name = to_native_str(
-        return policy_class()
+        cls = _policy_classes.get(policy_name.lower(), self.default_policy)
-from scrapy.spidermiddlewares.referer import RefererMiddleware
+from scrapy.spidermiddlewares.referer import RefererMiddleware, \
-        self.assertEquals(out[0].headers.get('Referer'), None)
+        for origin, target, referrer in [
-                r.headers.setdefault('Referer', response.url)
+                referrer = self.policy(response, r).referrer(response, r)
-from twisted.web.http import PotentialDataLoss
+from twisted.web.http import _DataLoss, PotentialDataLoss
-    HTTPConnectionPool
+    HTTPConnectionPool, ResponseFailed
-            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize))
+            warnsize=getattr(spider, 'download_warnsize', self._default_warnsize),
-                 maxsize=0, warnsize=0):
+                 maxsize=0, warnsize=0, fail_on_dataloss=True):
-        txresponse.deliverBody(_ResponseReader(d, txresponse, request, maxsize, warnsize))
+        txresponse.deliverBody(_ResponseReader(
-    def __init__(self, finished, txresponse, request, maxsize, warnsize):
+    def __init__(self, finished, txresponse, request, maxsize, warnsize,
-        elif reason.check(PotentialDataLoss):
+            return
-            self._finished.errback(reason)
+            return
-        PayloadResource, BrokenDownloadResource
+        PayloadResource
-        self.body = open(filename, "w") if filename else BytesIO()
+        self.body = open(filename, "wb") if filename else BytesIO()
-        return respcls(url=request.url, status=200, body=body, headers=headers)
+        return respcls(url=request.url, status=200, body=to_bytes(body), headers=headers)
-                return Response(url=request.url, status=httpcode, body=message)
+                return Response(url=request.url, status=httpcode, body=to_bytes(message))
-        fp.child('file with spaces.txt').setContent("Moooooooooo power!")
+        fp.child('file.txt').setContent(b"I have the power!")
-            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': ['17']})
+            self.assertEqual(r.body, b'I have the power!')
-            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': ['18']})
+            self.assertEqual(r.body, b'Moooooooooo power!')
-        local_fname = "/tmp/file.txt"
+        local_fname = b"/tmp/file.txt"
-            self.assertEqual(r.headers, {'Local Filename': ['/tmp/file.txt'], 'Size': ['17']})
+            self.assertEqual(r.headers, {b'Local Filename': [b'/tmp/file.txt'], b'Size': [b'17']})
-                self.assertEqual(f.read(), "I have the power!")
+            with open(local_fname, "rb") as f:
-        fp.child('file with spaces.txt').setContent("Moooooooooo power!")
+        fp.child('file.txt').setContent(b"I have the power!")
-from scrapy.core.downloader.handlers.data import DataURIDownloadHandler
+from scrapy.core.downloader.handlers.datauri import DataURIDownloadHandler
-    from urllib.parse import unquote_to_bytes as unquote
+from w3lib.url import parse_data_uri
-        respcls = responsetypes.from_mimetype(media_type)
+        uri = parse_data_uri(request.url)
-        return respcls(url=request.url, body=data, **resp_kwargs)
+        return respcls(url=request.url, body=uri.data, **resp_kwargs)
-from six.moves.urllib.parse import unquote
+
-_mediatype_pattern = re.compile(r'{token}/{token}'.format(token=_token))
+_mediatype_pattern = re.compile(
-                                                    quoted=_quoted_string)
+                                                    quoted=_quoted_string
-            media_type = m.group()
+            media_type = m.group().decode()
-                media_type_params[attribute] = value
+                    value = re.sub(br'\\(.)', r'\1', value_quoted)
-        is_base64, data = url.split(',', 1)
+        is_base64, data = url.split(b',', 1)
-            if is_base64 != ";base64":
+            if is_base64 != b";base64":
-import shutil
+from scrapy.core.downloader.handlers.data import DataURIDownloadHandler
-        self.spider = Spider('foo')
+        crawler = get_crawler(Spider)
-                self.mw.process_spider_exception(self.res404, \
+                self.mw.process_spider_exception(self.res404,
-                self.mw.process_spider_exception(self.res404, \
+                self.mw.process_spider_exception(self.res404,
-                Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),
+                Link(url=page4_url, text=u'href with whitespaces'),
-                Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),
+                Link(url=page4_url, text=u'href with whitespaces'),
-                Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),
+                Link(url=page4_url, text=u'href with whitespaces'),
-from w3lib.url import safe_url_string
+from w3lib.url import safe_url_string, canonicalize_url
-                 strip=True):
+                 strip=True, canonicalized=False):
-        return links
+        return unique_list(links, key=self.link_key) if self.unique else links
-                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True,
+                 tags=('a', 'area'), attrs=('href',), canonicalize=False, unique=True,
-                unique=unique, process_value=process_value, strip=strip)
+                unique=unique, process_value=process_value, strip=strip,
-                 strip=True):
+                 strip=True, canonicalized=False):
-            return unique_list(links, key=lambda link: link.url)
+            return unique_list(links, key=self.link_key)
-                 tags=('a', 'area'), attrs=('href',), canonicalize=True,
+                 tags=('a', 'area'), attrs=('href',), canonicalize=False,
-            unique=unique, process=process_value, strip=strip)
+        lx = LxmlParserLinkExtractor(
-                link.url = canonicalize_url(urlparse(link.url))
+                link.url = canonicalize_url(link.url)
-try:
+from scrapy import twisted_version
-except ImportError:
+else:
-        self._connectBuffer = b''
+        self._connectBuffer = bytearray()
-                 dont_filter=False, errback=None):
+                 dont_filter=False, errback=None, flags=None):
-CRAWLEDMSG = u"Crawled (%(status)s) %(request)s (referer: %(referer)s)%(flags)s"
+CRAWLEDMSG = u"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s"
-        flags = ' %s' % str(response.flags) if response.flags else ''
+        request_flags = ' %s' % str(request.flags) if request.flags else ''
-                'flags': flags,
+                'response_flags': response_flags,
-        dont_filter=d['dont_filter'])
+        dont_filter=d['dont_filter'],
-            meta={'a': 'b'})
+            meta={'a': 'b'},
-FTP_PASSWORD = 'anonymous@example.com'
+FTP_PASSWORD = 'guest'
-        self.anonymous_password = settings['FTP_ANONYMOUS_PASSWORD']
+        self.default_user = settings['FTP_USER']
-            password = self.anonymous_password
+        user = request.meta.get("ftp_user", self.default_user)
-            passive=request.meta.get("ftp_passive", 1))
+            passive=passive_mode)
-FTP_ANONYMOUS_PASSWORD = 'anonymous@example.com'
+FTP_USER = 'anonymous'
-from six.moves.urllib.parse import urlparse, unquote
+from six.moves.urllib.parse import unquote
-        pass
+    def __init__(self, settings):
-                                    passive=request.meta.get("ftp_passive", 1))
+        parsed_url = urlparse_cached(request)
-class FTPTestCase(unittest.TestCase):
+class BaseFTPTestCase(unittest.TestCase):
-                meta={"ftp_user": self.username, "ftp_password": self.password})
+                          meta=self.req_meta)
-            meta={"ftp_user": self.username, "ftp_password": self.password}
+            meta=self.req_meta
-                meta={"ftp_user": self.username, "ftp_password": self.password})
+                          meta=self.req_meta)
-                meta={"ftp_user": self.username, "ftp_password": self.password, "ftp_local_filename": local_fname})
+                          meta=meta)
-                meta={"ftp_user": self.username, "ftp_password": 'invalid'})
+                          meta=meta)
-        if cls.make_requests_from_url is not Spider.make_requests_from_url:
+        if method_is_overridden(cls, Spider, 'make_requests_from_url'):
-        if self.make_requests_from_url is not Spider.make_requests_from_url:
+        cls = self.__class__
-                "Please override start_requests method instead."
+                "Spider.make_requests_from_url method is deprecated; it "
-class BaseSpiderDeprecationTest(unittest.TestCase):
+class DeprecationTest(unittest.TestCase):
-    unittest.main()
+        class MySpider5(Spider):
-    def test_Encoding(self):
+    def test_encoding(self):
-        fd.close()
+        with open(self.tmpname + '^', 'w') as f:
-        r = static.File(name)
+        self.tmpname = self.mktemp()
-            self.assertEqual(fp.read(), b"content")
+        try:
-            self.assertEqual(fp.read(), b"new content")
+        try:
-        ss2.spider_closed(spider2)
+        try:
-from six.moves.urllib.parse import urlparse
+import shutil
-        r = static.File(name)
+        self.tmpname = self.mktemp()
-        return self.port.stopListening()
+        yield self.port.stopListening()
-            self.assertEquals(expected, data_path('somepath'))
+            self.assertEquals(
-            yield self.make_requests_from_url(url)
+        if self.make_requests_from_url is not Spider.make_requests_from_url:
-        return Request(url, dont_filter=self.dont_filter)
+                yield Request(url, dont_filter=self.dont_filter)
-        return Request(url)  # dont_filter=False
+    def start_requests(self):
-                        url = trim_href_attribute(url)
+                        value = strip_html5_whitespace(value)
-from six.moves.urllib.parse import urlparse, urljoin
+from six.moves.urllib.parse import urljoin
-                    attr_val = trim_href_attribute(attr_val)
+                    attr_val = strip_html5_whitespace(attr_val)
-from scrapy.utils.url import trim_href_attribute
+                    if self.strip and value is not None:
-
+from scrapy.utils.url import trim_href_attribute
-    def __init__(self, tag="a", attr="href", process=None, unique=False):
+    def __init__(self, tag="a", attr="href", process=None, unique=False,
-from scrapy.linkextractors import FilteringLinkExtractor
+from scrapy.utils.url import trim_href_attribute
-    def __init__(self, tag="a", attr="href", process=None, unique=False):
+    def __init__(self, tag="a", attr="href", process=None, unique=False,
-                continue # skipping bogus links
+                continue  # skipping bogus links
-                 unique=True, process_value=None, deny_extensions=None, restrict_css=()):
+                 unique=True, process_value=None, deny_extensions=None, restrict_css=(),
-            unique=unique, process=process_value)
+            unique=unique, process=process_value, strip=strip)
-from scrapy.selector import Selector
+from scrapy.utils.url import trim_href_attribute
-    def __init__(self, tag="a", attr="href", unique=False, process_value=None):
+    def __init__(self, tag="a", attr="href", unique=False, process_value=None,
-                 process_value=None, deny_extensions=None, restrict_css=()):
+                 process_value=None, deny_extensions=None, restrict_css=(),
-                unique=unique, process_value=process_value)
+                unique=unique, process_value=process_value, strip=strip)
-                          Link(url='http://example.com/innertag.html', text=u'inner tag'),])
+        self.assertEqual(lx.extract_links(self.response), [
-    return link_text.strip("\t\r\n '\"")
+    return link_text.strip("\t\r\n '\"\x0c")
-import six
+from w3lib.html import strip_html5_whitespace
-        return urljoin(form.base_url, form.action)
+        action = form.get('action')
-                    <base href="http://b.com/">
+                    <base href=" http://b.com/">
-        * scrapy.link.Link object.
+        Return a :class:`~.Request` instance to follow a link ``url``.
-        * attribute Selector (not SelectorList) - e.g.
+        Return a :class:`~.Request` instance to follow a link ``url``.
-                               resp.follow, resp.css('a'))
+        self.assertRaisesRegexp(ValueError, 'SelectorList',
-            body='<html><body><a href="foo?">click me</a></body></html>'.encode('utf8')
+            body=u'<html><body><a href="foo?">click me</a></body></html>'.encode('utf8')
-            body='<html><body><a href="foo?">click me</a></body></html>'.encode('cp1251')
+            body=u'<html><body><a href="foo?">click me</a></body></html>'.encode('cp1251')
-        elif isinstance(url, parsel.Selector):
+        if isinstance(url, parsel.Selector):
-                       errback=errback)
+        return super(TextResponse, self).follow(url, callback,
-                self.assert_followed_url(sel, url, response=resp)
+                self._assert_followed_url(sel, url, response=resp)
-                self.assert_followed_url(sel, url, response=resp)
+                self._assert_followed_url(sel, url, response=resp)
-                                 'http://example.com/foo%20')
+            self._assert_followed_url(src, 'http://example.com/sample2.jpg')
-        self.assert_followed_url(resp.css('a')[0],
+        self._assert_followed_url(resp.css('a')[0],
-        self.assert_followed_url(resp.css('a::attr(href)')[0],
+                                  response=resp)
-                                 response=resp)
+                                  response=resp)
-        req = self.assert_followed_url(
+        req = self._assert_followed_url(
-        req = self.assert_followed_url(
+        req = self._assert_followed_url(
-            url = _url_from_selector(url).strip()
+            url = _url_from_selector(url)
-        return sel.root
+        return strip_html5_whitespace(sel.root)
-    return href
+    return strip_html5_whitespace(href)
-tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data')
+tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)),
-    return open(path, 'rb').read()
+    with open(path, 'rb') as f:
-            url = _url_from_selector(url)
+            url = _url_from_selector(url).strip()
-
+            raise ValueError("SelectorList is not supported")
-        'w3lib>=1.15.0',
+        'w3lib>=1.17.0',
-        settings = crawler.settings
+    def __init__(self, settings):
-        return cls(crawler)
+        return cls(crawler.settings)
-            self.stats.inc_value('retry/reason_count/%s' % reason)
+            stats.inc_value('retry/count')
-            self.stats.inc_value('retry/max_reached')
+            stats.inc_value('retry/max_reached')
-    def __init__(self, settings):
+    def __init__(self, crawler):
-        return cls(crawler.settings)
+        return cls(crawler)
-        ex_class = "%s.%s" % (exception.__class__.__module__, exception.__class__.__name__)
+        ex_class = global_object_name(exception.__class__)
-    
+
-    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), "Unknown Status")))
+    message = http.RESPONSES.get(int(status), "Unknown Status")
-        self.mw = RetryMiddleware.from_crawler(crawler)
+        self.crawler = get_crawler(Spider)
-        self.assertEqual(str(log).count('TunnelError'), 1)
+        print(log)
-                r.meta.update(rule=n, link_text=link.text)
+                r = self._build_request(n, link)
-try:
+if twisted_version >= (14, 0, 0):
-                                             _maybeSetHostNameIndication,
+    if twisted_version < (17, 0, 0):
-                _maybeSetHostNameIndication(connection, self._hostnameBytes)
+                set_tlsext_host_name(connection, self._hostnameBytes)
-
+    def _basic_auth_header(self, username, password):
-            creds = base64.b64encode(user_pass).strip()
+            creds = self._basic_auth_header(user, password)
-
+    def test_proxy_precedence_meta(self):
-        os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'
+        os.environ['http_proxy'] = 'https://proxy.for.http:3128'
-        os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'
+        os.environ['http_proxy'] = 'https://proxy.for.http:3128'
-from scrapy.item import Item, Field
+from scrapy.item import ABCMeta, Item, ItemMeta, Field
-from scrapy import twisted_version
+from __future__ import absolute_import
-    HTTPDownloadHandler = HTTP10DownloadHandler
+from .http11 import HTTP11DownloadHandler as HTTPDownloadHandler
-            d.errback(Exception())
+from twisted.web.test.test_webclient import PayloadResource
-    from twisted.internet.task import deferLater
+from scrapy.utils.python import to_bytes, to_unicode
-            self.putChild(b"xpayload", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))
+        self.putChild(b"payload", PayloadResource())
-        skip = 'HTTP1.1 not supported in twisted < 11.1.0'
+        crawler = get_crawler(SingleRequestSpider)
-            request = Request('http://localhost:8998/payload', method='POST', body=body, meta={'download_maxsize': 50})
+        if six.PY2:
-                raise unittest.SkipTest("xpayload only enabled for PY2")
+            # download_maxsize = 50 is enough for the gzipped response
-            raise unittest.SkipTest("xpayload and payload endpoint only enabled for twisted > 12.3.0")
+            # See issue https://twistedmatrix.com/trac/ticket/8175
-            exceptions.append(ResponseFailed)
+                ConnectError, ConnectionLost, ResponseFailed]
-
+    """
-                return self._redirect(redirected, request, spider, 'meta refresh')
+        interval, url = get_meta_refresh(response)
-MEMUSAGE_ENABLED = False
+MEMUSAGE_ENABLED = True
-_matches = lambda url, regexs: any((r.search(url) for r in regexs))
+_matches = lambda url, regexs: any(r.search(url) for r in regexs)
-        denied = [regex.search(url) for regex in self.deny_res] if self.deny_res else []
+        allowed = (regex.search(url) for regex in self.allow_res) if self.allow_res else [True]
-                                   ",".join(ACCEPTED_ENCODINGS))
+                                   b",".join(ACCEPTED_ENCODINGS))
-from unittest import TestCase
+from unittest import TestCase, SkipTest
-from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware
+from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, \
-                         b'gzip,deflate,br')
+                         b','.join(ACCEPTED_ENCODINGS))
-import brotli
+ACCEPTED_ENCODINGS = [b'gzip', b'deflate']
-        request.headers.setdefault('Accept-Encoding', 'gzip,deflate,br')
+        request.headers.setdefault('Accept-Encoding',
-        if encoding == b"br":
+        if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:
-from os.path import join, abspath, dirname
+from os.path import join
-        request.headers.setdefault('Accept-Encoding', 'gzip,deflate')
+        request.headers.setdefault('Accept-Encoding', 'gzip,deflate,br')
-        self.assertEqual(request.headers.get('Accept-Encoding'), b'gzip,deflate')
+        self.assertEqual(request.headers.get('Accept-Encoding'),
-            yield crawler.crawl("http://dns.resolution.invalid/")
+            yield crawler.crawl("http://dns.resolution.invalid./")
-            self._assert_retried(l)
+        crawler = self.runner.create_crawler(SimpleSpider)
-        'parsel>=0.9.5',
+        'parsel>=1.1',
-        return self.selector.xpath(query)
+    def xpath(self, query, **kwargs):
-        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(rcvd_bytes)
+        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(self._connectBuffer)
-            timeout = self.timeout
+        # in Twisted<=16.6, getHostByName() is always called with
-        ``False`` and ``None`` return ``False``.
+        ``1``, ``'1'``, `True`` and ``'True'`` return ``True``,
-        return bool(int(self.get(name, default)))
+        got = self.get(name, default)
-            help="use this spider")
+        parser.remove_option("--headers")
-    return Request(
+    request_cls = load_object(d['_class']) if '_class' in d else Request
-from scrapy.http import Request
+from scrapy.http import Request, FormRequest
-        ScrapyCommand.add_options(self, parser)
+        super(Command, self).add_options(parser)
-import sys
+import sys, os
-        return "Edit a spider using the editor defined in EDITOR setting"
+        return ("Edit a spider using the editor defined in the EDITOR environment"
-        EDITOR = 'vi'
+EDITOR = 'vi'
-            "FILES_EXPIRES": random.randint(1, 1000),
+            "FILES_EXPIRES": random.randint(100, 1000),
-            ).format(url=request.url, size=expected_size, maxsize=maxsize)
+            error_msg = ("Cancelling download of %(url)s: expected response "
-            logger.error(error_message)
+            logger.error(error_msg, error_args)
-            raise defer.CancelledError(error_message)
+            raise defer.CancelledError(error_msg % error_args)
-        'parsel>=0.9.3',
+        'parsel>=0.9.5',
-            request.meta['handle_httpstatus_list'] = SequenceExclude(six.moves.range(300, 400))
+            request.meta['handle_httpstatus_list'] = SequenceExclude(range(300, 400))
-from scrapy.utils.datatypes import CaselessDict
+from scrapy.utils.datatypes import CaselessDict, SequenceExclude
-                       "(exception: {})".format(name, str(e)))
+                msg = ("\n{tb}Could not load spiders from module '{modname}'. "
-    def proc(self, *new_args, **kwargs):
+    def proc(self, *new_args, **popen_kwargs):
-                             **kwargs)
+                             **popen_kwargs)
-    def runspider(self, code, name='myspider.py'):
+    def runspider(self, code, name='myspider.py', args=()):
-import scrapy
+            return self.proc('runspider', fname, *args)
-        log = to_native_str(p.stderr.read())
+    def get_log(self, code, name='myspider.py', args=()):
-        log = to_native_str(p.stderr.read())
+        log = self.get_log("from scrapy.spiders import Spider\n")
-        log = to_native_str(p.stderr.read())
+        log = self.get_log('', name='myspider.txt')
-        p = self.runspider("""
+        log = self.get_log("""
-        log = to_native_str(p.stderr.read())
+
-    handler.addFilter(TopLevelFormatter(['scrapy']))
+    if settings.getbool('LOG_SHORT_NAMES'):
-                     "update local objects")
+            b.append("  fetch(url[, redirect=True]) "
-            
+
-                self._load_spiders(module)
+            try:
-            default=False, help="do not handle status codes like redirects and print response as-is")
+        parser.add_option("--no-redirect", dest="no_redirect", action="store_true", \
-        if opts.no_status_aware:
+        # by default, let the framework handle redirects,
-            default=False, help="do not transparently handle status codes like redirects")
+        parser.add_option("--no-redirect", dest="no_redirect", action="store_true", \
-        shell.start(url=url, handle_statuses=opts.no_status_aware)
+        shell.start(url=url, redirect=not opts.no_redirect)
-    def start(self, url=None, request=None, response=None, spider=None, handle_statuses=True):
+    def start(self, url=None, request=None, response=None, spider=None, redirect=True):
-            self.fetch(url, spider, handle_statuses=handle_statuses)
+            self.fetch(url, spider, redirect=redirect)
-    def fetch(self, request_or_url, spider=None, handle_statuses=False, **kwargs):
+    def fetch(self, request_or_url, spider=None, redirect=True, **kwargs):
-            if handle_statuses:
+            if redirect:
-        _, out, err = yield self.execute(['--no-status-aware', self.url('/redirect-no-meta-refresh')])
+        _, out, err = yield self.execute(['--no-redirect', self.url('/redirect-no-meta-refresh')])
-            "IMAGES_EXPIRES": random.randint(1, 1000),
+            "IMAGES_EXPIRES": random.randint(100, 1000),
-    client = endpoints = _Mock()
+from __future__ import absolute_import
-    HTTPConnectionPool, TCP4ClientEndpoint
+from twisted.web.client import Agent, ProxyAgent, ResponseDone, \
-from scrapy.xlib.tx import ResponseFailed
+from twisted.web.client import ResponseFailed
-from scrapy.xlib.tx import ResponseFailed
+from twisted.web.client import ResponseFailed
-from twisted.web.http_headers import Headers
+#from twisted.protocols.basic import LineReceiver
-    TransportProxyProducer,
+    RequestGenerationFailed, RequestTransmissionFailed, ConnectionAborted,
-}}} '''
+
-''' {{{
+
-}}} '''
+
-''' {{{
+
-}}} '''
+
-''' {{{
+
-}}} '''
+
-from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer
+from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IResponse
-    PartialDownloadError,
+    PartialDownloadError, FileBodyProducer,
-}}} '''
+
-
+}}} '''
-    ResponseNeverReceived)
+#from twisted.web.error import SchemeNotSupported
-
+}}} '''
-        ClientFactory, Protocol, Factory)
+#from twisted.internet.protocol import (
-from twisted.internet.interfaces import IStreamClientEndpointStringParser
+#from twisted.internet.interfaces import IStreamClientEndpointStringParser
-from twisted.python.components import proxyForInterface
+#from twisted.python.components import proxyForInterface
-from .interfaces import IFileDescriptorReceiver
+from twisted.internet.endpoints import (
-
+''' {{{
-                      IFileDescriptorReceiver]:
+                      interfaces.IFileDescriptorReceiver]:
-
+}}} '''
-    IReactorUDP, IReactorMulticast, IReactorProcess,
+    IReactorWin32Events, IReactorUDP, IReactorMulticast, IReactorProcess,
-    IReactorPluggableResolver, IReactorFDSet,
+    IReactorPluggableResolver, IReactorDaemonize, IReactorFDSet,
-    IProtocolFactory, ITransport, IProcessTransport, IServiceCollection,
+    IFileDescriptorReceiver, IProtocolFactory, ITransport, ITCPTransport,
-    IMulticastTransport,
+    IMulticastTransport, IStreamClientEndpoint, IStreamServerEndpoint,
-}}} '''
+
-''' {{{
+
-}}} '''
+
-''' {{{
+
-}}} '''
+
-''' {{{
+
-}}} '''
+
-''' {{{
+
-}}} '''
+
-''' {{{
+
-}}} '''
+
-''' {{{
+
-}}} '''
+
-    UNKNOWN_LENGTH,
+    IRequest, ICredentialFactory, IBodyProducer, IRenderable, ITemplateLoader,
-#)
+''' {{{
-''' {{{
+
-}}} '''
+
-''' {{{
+
-        'Twisted>=10.0.0',
+        'Twisted>=13.1.0',
-TCP4ClientEndpoint = endpoints.TCP4ClientEndpoint
+Agent = client.Agent  # since < 11.1
-from .iweb import IResponse, UNKNOWN_LENGTH
+from twisted.web._newclient import (
-
+''' {{{
-
+}}} '''
-
+''' {{{
-
+}}} '''
-
+''' {{{
-
+}}} '''
-
+''' {{{
-
+}}} '''
-from .iweb import IResponse, UNKNOWN_LENGTH, IBodyProducer
+from twisted.web.client import (
-
+}}} '''
-from ._newclient import RequestNotSent, RequestTransmissionFailed
+from twisted.web._newclient import ResponseDone
-    ResponseNeverReceived, PotentialDataLoss, _WrapperException)
+    ResponseNeverReceived)
-    'HTTPClientFactory', 'HTTPDownloader', 'getPage', 'downloadPage',
+    'PartialDownloadError',
-import socket
+#import socket
-from twisted.internet import interfaces, defer, error, fdesc, threads
+from twisted.internet import interfaces, defer, error, fdesc
-        ClientFactory, Protocol, ProcessProtocol, Factory)
+        ClientFactory, Protocol, Factory)
-from twisted.python import log
+#from twisted.python.failure import Failure
-from twisted.internet import stdio
+#from twisted.internet import stdio
-
+from twisted.internet.interfaces import (
-
+}}} '''
-
+''' {{{
-
+}}} '''
-
+''' {{{
-
+}}} '''
-
+''' {{{
-
+}}} '''
-
+''' {{{
-
+}}} '''
-
+''' {{{
-
+}}} '''
-
+''' {{{
-
+}}} '''
-from twisted.internet.interfaces import IPushProducer
+#from twisted.internet.interfaces import IPushProducer
-
+''' {{{
-
+}}} '''
-
+''' {{{
-
+}}} '''
-        os.mkdir(self.tmpdir)
+        self.tmpdir = tempfile.mkdtemp()
-            set(['spider1', 'spider2', 'spider3']))
+            set(['spider1', 'spider2', 'spider3', 'spider4']))
-    def test_load_spider_module(self):
+    def test_load_spider_module_multiple(self):
-            for url in sitemap_urls_from_robots(response.text):
+            for url in sitemap_urls_from_robots(response.text, base_url=response.url):
-def sitemap_urls_from_robots(robots_text):
+def sitemap_urls_from_robots(robots_text, base_url=None):
-            yield line.split(':', 1)[1].strip()
+            url = line.split(':', 1)[1].strip()
-                          'http://example.com/sitemap-product-index.xml'])
+                          'http://example.com/sitemap-product-index.xml',
-             ['http://example.com/sitemap.xml', 'http://example.com/sitemap-product-index.xml'])
+        self.assertEqual(list(sitemap_urls_from_robots(robots, base_url='http://example.com')),
-              "is deprecated, chunked transfers are supported by default.",
+warnings.warn("Module `scrapy.downloadermiddlewares.chunked` is deprecated, "
-    'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware': 830,
+class NoMetaRefreshRedirect(util.Redirect):
-        request.meta['handle_httpstatus_all'] = True
+        if opts.no_status_aware:
-        shell.start(url=url)
+        shell.start(url=url, handle_statuses=opts.no_status_aware)
-    def start(self, url=None, request=None, response=None, spider=None):
+    def start(self, url=None, request=None, response=None, spider=None, handle_statuses=True):
-            self.fetch(url, spider)
+            self.fetch(url, spider, handle_statuses=handle_statuses)
-    def fetch(self, request_or_url, spider=None):
+    def fetch(self, request_or_url, spider=None, handle_statuses=False, **kwargs):
-            request.meta['handle_httpstatus_all'] = True
+            request = Request(url, dont_filter=True, **kwargs)
-                                   'Python', 'pyOpenSSL', 'Platform'])
+        self.assertEqual(headers, ['Scrapy', 'lxml', 'libxml2',
-        if rp is not None and not rp.can_fetch(self._useragent, request.url):
+        if rp is not None and not rp.can_fetch(
-            if isinstance(link.url, unicode):
+            if isinstance(link.url, six.text_type):
-            if isinstance(link.url, unicode):
+            if isinstance(link.url, six.text_type):
-        rp.parse(body.splitlines())
+        # stdlib's robotparser expects native 'str' ;
-        ROBOTS = re.sub(b'^\s+(?m)', b'', b'''
+        ROBOTS = re.sub(b'^\s+(?m)', b'', u'''
-        ''')
+
-            self.assertIgnored(Request('http://site.local/static/'), middleware)
+            self.assertIgnored(Request('http://site.local/static/'), middleware),
-Some pipelines used for testing and benchmarking
+Some pipelines used for testing
-                self.heartbeat.stop()
+                if self.heartbeat.running:
-        if self.task.running:
+        if self.task and self.task.running:
-        """Handle the downloaded response or failure trough the spider
+        """Handle the downloaded response or failure through the spider
-        location = to_native_str(response.headers['location'].decode('latin1'))
+        location = safe_url_string(response.headers['location'])
-        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'
+        perc_encoded_utf8_url = 'http://scrapytest.org/a%E7%E3o'
-    def test_location_with_wrong_encoding(self):
+    def test_utf8_location(self):
-        utf8_location = u'/ao'  # header with wrong encoding (utf-8)
+        utf8_location = u'/ao'.encode('utf-8')  # header using UTF-8 encoding
-        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%83%C2%A7%C3%83%C2%A3o'
+        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'
-    ('${project_name}', 'middleware.py.tmpl'),
+    ('${project_name}', 'middlewares.py.tmpl'),
-from utils.misc import arg_to_iter
+from .utils.misc import arg_to_iter
-        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body', _callback=self._catch_mail_sent)
+        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body',
-        mailsender.send(to=['test@scrapy.org'], subject='subject', body='<p>body</p>', mimetype='text/html', _callback=self._catch_mail_sent)
+        mailsender.send(to=['test@scrapy.org'], subject='subject',
-                        attachs=attachs, charset='utf-8', _callback=self._catch_mail_sent)
+                        attachs=attachs, charset='utf-8',
-        self.assertEqual(msg.get('Content-Type'), 'multipart/mixed; charset="utf-8"')
+        self.assertEqual(msg.get('Content-Type'),
-    from scrapy.core.downloader.tls import ScrapyClientTLSOptions
+    from scrapy.core.downloader.tls import ScrapyClientTLSOptions, DEFAULT_CIPHERS
-                                       getattr(self, '_ssl_method', None)))
+                                       getattr(self, '_ssl_method', None)),
-    as relative to the project data dir, otherwise return it unmodified
+    """
-    as relative the project data dir, otherwise return it unmodified
+    as relative to the project data dir, otherwise return it unmodified
-        path = join(project_data_dir(), path)
+    if not isabs(path):
-    otherwise return the path unmodified
+    """If inside the project and path is relative, return the given path
-    if not isabs(path):
+    if inside_project() and not isabs(path):
-            logger.debug(
+            logger.info(
-                             exc_info=True, extra={'spider': self.spider})
+                logger.warning(msg, {'request': request, 'reason': e},
-        pass
+        for h in self.logger.handlers:
-        exporter.start_exporting()
+        if self.store_empty:
-        slot.exporter.finish_exporting()
+        if self._exporting:
-        
+
-        
+
-            
+
-        
+
-                                    base_class_name=cls_name)
+                                    base_class_name=cls_name,
-                                    base_class_name="ImagesPipeline")
+                                    base_class_name="ImagesPipeline",
-    def _key_for_pipe(self, key, base_class_name=None):
+    def _key_for_pipe(self, key, base_class_name=None,
-        if class_name == base_class_name or not base_class_name:
+        formatted_key = "{}_{}".format(class_name.upper(), key)
-        return "{}_{}".format(class_name.upper(), key)
+        return formatted_key
-        should be preserved.
+        If file settings are defined but they are not defined for subclass
-            self.assertEqual(value, getattr(pipeline, pipe_attr))
+            self.assertEqual(value, setting_value)
-        should be preserved.
+        If image settings are defined but they are not defined for subclass default
-            # Instance attribute (lowercase) must be equal to class attribute (uppercase).
+            # Instance attribute (lowercase) must be equal to
-            self.assertEqual(value, getattr(pipeline, pipe_attr))
+            setings_value = settings.get(settings_attr)
-
+    def test_user_defined_subclass_default_key_names(self):
-        shell = InteractiveShellEmbed(
+        # Always use .instace() to ensure _instance propagation to all parents
-                    return rule.callback
+                if rule.link_extractor.matches(response.url):
-        # ignore if proxy is already seted
+        # ignore if proxy is already set
-    canonicalize_url, url_is_from_any_domain, url_has_any_extension,
+    url_is_from_any_domain, url_has_any_extension,
-from scrapy.utils.url import canonicalize_url
+from w3lib.url import canonicalize_url
-    from urllib.parse import unquote_to_bytes
+from six.moves.urllib.parse import (ParseResult, urldefrag, urlparse)
-from scrapy.utils.python import to_bytes, to_native_str, to_unicode
+from w3lib.url import _safe_chars, _unquotepath
-        'w3lib>=1.14.2',
+        'w3lib>=1.15.0',
-                              guess_scheme, parse_url)
+                              add_http_if_no_scheme, guess_scheme, parse_url)
-                    output += f.extrabuf
+                    output += f.extrabuf[-f.extrasize:]
-                                 {'callback': callback, 'spider': spider.name})
+                                 {'callback': cb, 'spider': spider.name})
-"""Helper functions which doesn't fit anywhere else"""
+"""Helper functions which don't fit anywhere else"""
-        except ValueError as e: # non serializable request
+        except ValueError as e:  # non serializable request
-                       " (stats being collected)")
+                msg = ("Unable to serialize request: %(request)s - reason:"
-            self.stats.inc_value('scheduler/unserializable', spider=self.spider)
+            self.stats.inc_value('scheduler/unserializable',
-        logunser = settings.getbool('SCHEDULER_DEBUG')
+        logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS', settings.getbool('SCHEDULER_DEBUG'))
-        logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS')
+        logunser = settings.getbool('SCHEDULER_DEBUG')
-                             {'request': request, 'reason': e},
+                msg = ("Unable to serialize request: %(request)s - reason: %(reason)s"
-LOG_UNSERIALIZABLE_REQUESTS = False
+SCHEDULER_DEBUG = False
-        if isinstance(o, datetime.datetime):
+        if isinstance(o, set):
-                              (dec, decs), (['foo', d], ['foo', ds])]:
+                              (dec, decs), (['foo', d], ['foo', ds]), (s, ss),
-        EXPIRES=0,
+        EXPIRES=90,
-    EXPIRES = 0
+    EXPIRES = 90
-                return
+
-        super(ImagesPipeline, self).__init__(store_uri, settings=settings, download_func=download_func)
+        super(ImagesPipeline, self).__init__(store_uri, settings=settings,
-        cls_name = "ImagesPipeline"
+        resolve = functools.partial(self._key_for_pipe,
-            self._key_for_pipe('IMAGES_EXPIRES', cls_name), self.EXPIRES
+            resolve("IMAGES_EXPIRES"), self.EXPIRES
-        # and doesn't close the connection
+        """
-                            self._hostnameASCII, e))
+                        'Ignoring error while verifying certificate '
-                        'SSL/TLS verification failed for hostname "{}"; {}'.format(
+                        'Ignoring remote certificate verification failure for hostname "{}"; {}'.format(
-        self.encoding = options.pop('encoding', 'utf-8')
+        kwargs.setdefault('ensure_ascii', not self.encoding)
-        self.file.write(to_bytes(self.encoder.encode(itemdict) + '\n'))
+        data = self.encoder.encode(itemdict) + '\n'
-        self.file.write(to_bytes(self.encoder.encode(itemdict)))
+        data = self.encoder.encode(itemdict)
-        self.csv_writer = csv.writer(file, **kwargs)
+        self.stream = io.TextIOWrapper(
-                yield to_native_str(s)
+                yield to_native_str(s, self.encoding)
-        exporter = self._get_exporter(file, fields_to_export=self.export_fields)
+        exporter = self._get_exporter(file, fields_to_export=self.export_fields,
-        >>> assert result == "IMAGES"
+        >>> MediaPipeline()._key_for_pipe("IMAGES")
-        >>> assert other_key == "MYPIPE_IMAGES"
+        >>> MyPipe()._key_for_pipe("IMAGES", base_class_name="MediaPipeline")
-
+import functools
-            self._key_for_pipe('FILES_EXPIRES', cls_name), self.EXPIRES
+            resolve('FILES_EXPIRES'), self.EXPIRES
-            self._key_for_pipe('FILES_URLS_FIELD', cls_name), self.FILES_URLS_FIELD
+            resolve('FILES_URLS_FIELD'), self.FILES_URLS_FIELD
-            self._key_for_pipe('FILES_RESULT_FIELD', cls_name), self.FILES_RESULT_FIELD
+            resolve('FILES_RESULT_FIELD'), self.FILES_RESULT_FIELD
-
+import functools
-        default_images_urls_field = getattr(self, "IMAGES_URLS_FIELD", "DEFAULT_IMAGES_URLS_FIELD")
+        resolve = functools.partial(self._key_for_pipe, base_class_name=cls_name)
-            self._key_for_pipe('IMAGES_URLS_FIELD', cls_name), default_images_urls_field
+            resolve('IMAGES_URLS_FIELD'),
-            self._key_for_pipe('IMAGES_RESULT_FIELD', cls_name), default_images_result_field
+            resolve('IMAGES_RESULT_FIELD'),
-            self._key_for_pipe('IMAGES_MIN_WIDTH', cls_name), self.MIN_WIDTH
+            resolve('IMAGES_MIN_WIDTH'), self.MIN_WIDTH
-            self._key_for_pipe('IMAGES_MIN_HEIGHT', cls_name), self.MIN_HEIGHT
+            resolve('IMAGES_MIN_HEIGHT'), self.MIN_HEIGHT
-            self._key_for_pipe('IMAGES_THUMBS', cls_name), self.THUMBS
+            resolve('IMAGES_THUMBS'), self.THUMBS
-    def _key_for_pipe(self, key, base_class_name):
+    def _key_for_pipe(self, key, base_class_name=None):
-        and it will override default settings and class attributes.
+        >>> result = MediaPipeline()._key_for_pipe("IMAGES")
-        if class_name == base_class_name:
+        if class_name == base_class_name or not base_class_name:
-_is_octetstream_re = re.compile(br'^(application|binary)/octet-stream\b', re.I)
+_is_gzipped = re.compile(br'^application/(x-)?gzip\b', re.I).search
-             cenc in (b'gzip', b'x-gzip')))
+    return (_is_gzipped(ctype) or
-    requires_project = True
+    requires_project = False
-        spiders_dir = abspath(dirname(spiders_module.__file__))
+        if self.settings.get('NEWSPIDER_MODULE'):
-        print("  %s.%s" % (spiders_module.__name__, module))
+        print("Created spider %r using template %r " % (name, \
-def tunnel_request_data(host, port, proxy_auth_header=None, host_header=True):
+def tunnel_request_data(host, port, proxy_auth_header=None):
-        tunnel_req += b'Host: ' + host_value + b'\r\n'
+    tunnel_req += b'Host: ' + host_value + b'\r\n'
-    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 550,
+    'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,
-        return request.uri
+        # Note: this is an ugly hack for CONNECT request timeout test.
-    _responseMatcher = re.compile(b'HTTP/1\.. 200')
+    _responseMatcher = re.compile(b'HTTP/1\.. (?P<status>\d{3})(?P<reason>.{,32})')
-    def processProxyResponse(self, bytes):
+    def processProxyResponse(self, rcvd_bytes):
-        if  TunnelingTCP4ClientEndpoint._responseMatcher.match(bytes):
+        respm = TunnelingTCP4ClientEndpoint._responseMatcher.match(rcvd_bytes)
-                    self._host, self._port)))
+                TunnelError('Could not open CONNECT tunnel with proxy %s:%s [%r]' % (
-def tunnel_request_data(host, port, proxy_auth_header=None):
+def tunnel_request_data(host, port, proxy_auth_header=None, host_header=True):
-    'CONNECT example.com:8080 HTTP/1.1\r\n\r\n'
+    'CONNECT example.com:8080 HTTP/1.1\r\nHost: example.com:8080\r\n\r\n'
-    'CONNECT example.com:8080 HTTP/1.1\r\nProxy-Authorization: 123\r\n\r\n'
+    'CONNECT example.com:8080 HTTP/1.1\r\nHost: example.com:8080\r\nProxy-Authorization: 123\r\n\r\n'
-    'CONNECT example.com:8090 HTTP/1.1\r\n\r\n'
+    'CONNECT example.com:8090 HTTP/1.1\r\nHost: example.com:8090\r\n\r\n'
-        b' HTTP/1.1\r\n')
+    host_value = to_bytes(host, encoding='ascii') + b':' + to_bytes(str(port))
-    return ctype in (b'application/x-gzip', b'application/gzip')
+    ctype = response.headers.get('Content-Type', b'').lower()
-            self._key_for_pipe('FILES_URLS_FIELD', cls_name), self.DEFAULT_FILES_URLS_FIELD
+            self._key_for_pipe('FILES_URLS_FIELD', cls_name), self.FILES_URLS_FIELD
-            self._key_for_pipe('FILES_RESULT_FIELD', cls_name), self.DEFAULT_FILES_RESULT_FIELD
+            self._key_for_pipe('FILES_RESULT_FIELD', cls_name), self.FILES_RESULT_FIELD
-    IMAGES_RESULT_FIELD = 'images'
+    DEFAULT_IMAGES_URLS_FIELD = 'image_urls'
-            self._key_for_pipe('IMAGES_URLS_FIELD', cls_name), self.IMAGES_URLS_FIELD
+            self._key_for_pipe('IMAGES_URLS_FIELD', cls_name), default_images_urls_field
-            self._key_for_pipe('IMAGES_RESULT_FIELD', cls_name), self.IMAGES_RESULT_FIELD
+            self._key_for_pipe('IMAGES_RESULT_FIELD', cls_name), default_images_result_field
-        "DEFAULT_FILES_RESULT_FIELD": "files"
+        "FILES_URLS_FIELD": "file_urls",
-        ("DEFAULT_FILES_RESULT_FIELD", "FILES_RESULT_FIELD", "files_result_field")
+        ("FILES_URLS_FIELD", "FILES_URLS_FIELD", "files_urls_field"),
-            DEFAULT_FILES_RESULT_FIELD = "beta"
+            EXPIRES = 1001
-        
+
-        self.files_result_field = settings.get('FILES_RESULT_FIELD', self.DEFAULT_FILES_RESULT_FIELD)
+        self.expires = settings.getint(
-        self.thumbs = settings.get(key_for_pipe('IMAGES_THUMBS'), self.THUMBS)
+        cls_name = "ImagesPipeline"
-        ("DEFAULT_FILES_RESULT_FIELD", "FILES_RESULT_FIELD")
+        ("EXPIRES", "FILES_EXPIRES", "expires"),
-        return {prefix.upper() + "_" + k: v for k, v in settings.items()}
+        return {prefix.upper() + "_" + k if k != "FILES_STORE" else k: v for k, v in settings.items()}
-        for pipe_attr, settings_attr in self.file_cls_attr_settings_map:
+        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:
-            self.assertEqual(getattr(another_pipeline, pipe_attr_lower), custom_value)
+            self.assertEqual(getattr(another_pipeline, pipe_ins_attr), custom_value)
-            self.assertEqual(getattr(pipe, attr_lower), getattr(pipe, pipe_attr))
+        for pipe_attr, settings_attr, pipe_ins_attr in self.file_cls_attr_settings_map:
-FILES_RESULT_FIELD = 'files'
+    def _generate_fake_pipeline(self):
-        self.assertEqual(another_pipeline.files_result_field, 'funny_field')
+    def _generate_fake_settings(self, prefix=None):
-    # ImagesPipeline. They may be overriden by settings.
+    # ImagesPipeline. They may be overridden by settings.
-            MIN_WIDTH = 1000
+    def _generate_fake_settings(self, prefix=None):
-            MIN_WIDTH = 1000
+        def random_string():
-            "IMAGES_MIN_WIDTH": 90
+            "IMAGES_RESULT_FIELD": random_string(),
-            pass
+        return {prefix.upper() + "_" + k if k != "IMAGES_STORE" else k: v for k, v in settings.items()}
-        self.assertEqual(user_pipeline.min_width, 1999)
+    def _generate_fake_pipeline_subclass(self):
-            MIN_WIDTH = 200
+    def test_different_settings_for_different_instances(self):
-        self.assertEqual(user_pipeline.min_width, 1999)
+        prefix = UserDefinedImagePipeline.__name__.upper()
-    def test_is_gzipped_right(self):
+    def test_is_x_gzipped_right(self):
-        self.assertEqual(another_pipeline.thumbs, custom_thumbs)
+    def test_different_settings_for_different_instances(self):
-        # If image settings are defined they override class attributes.
+        # Class attributes for subclass of ImagePipeline override default setting keys.
-    return not _is_gzipped_re.search(ctype) is None
+    return _is_gzipped_re.search(ctype) is not None
-
+import re
-    return b'application/x-gzip' in ctype or b'application/gzip' in ctype
+    return not _is_gzipped_re.search(ctype) is None
-        self.assertTrue(not is_gzipped(r1))
+        self.assertFalse(is_gzipped(r1))
-        self.assertTrue(not is_gzipped(r1))
+        self.assertFalse(is_gzipped(r1))
-
+    def test_response_class_choosing_request(self):
-    def test_is_gzipped(self):
+    def test_is_gzipped_right(self):
-        self.assertTrue(not is_gzipped(r2))
+        hdrs = Headers({"Content-Type": "application/gzip"})
-        self.assertTrue(not is_gzipped(r3))
+        r1 = Response("http://www.example.com", headers=hdrs)
-    return ctype in (b'application/x-gzip', b'application/gzip')
+    return b'application/x-gzip' in ctype or b'application/gzip' in ctype
-from scrapy.utils.gz import gunzip
+from scrapy.utils.gz import gunzip, is_gzipped
-        self.thumbs = settings.get('IMAGES_THUMBS', self.THUMBS)
+        def key_for_pipe(key):
-    def test_class_attrs_not_preserved_if_settings_defined(self):
+    def test_class_attrs_preserved_if_only_global_settings_defined(self):
-        self.assertEqual(pipeline.min_width, 90)
+        self.assertEqual(pipeline.min_width, 1000)
-        to_native_str(parts.netloc.encode('idna')),
+        to_native_str(netloc),
-        self.assertEquals(response.body, '')
+        self.assertEquals(response.body, b'')
-        print("    %s\n" % abspath(project_name))
+        print("    %s\n" % abspath(project_dir))
-        print("    cd %s" % project_name)
+        print("    cd %s" % project_dir)
-            print('Error: Directory %r already exists' % project_name)
+        self.assertEqual(2, self.call('startproject'))
-    def _copytree(self, src, dst, symlinks=False, ignore=None):
+    def _copytree(self, src, dst):
-            ignored_names = set()
+        ignored_names = ignore(src, names)
-        errors = []
+
-                pass
+            if os.path.isdir(srcname):
-            raise EnvironmentError(errors)
+                copy2(srcname, dstname)
-        self._copytree(self.templates_dir, abspath(project_dir), ignore=IGNORE)
+        self._copytree(self.templates_dir, abspath(project_dir))
-            except EnvironmentError, err:
+            except EnvironmentError as err:
-            except EnvironmentError, why:
+            except EnvironmentError as why:
-        except OSError, why:
+        except OSError as why:
-from shutil import copytree, ignore_patterns, move
+from shutil import ignore_patterns, move, copy2, copystat
-        return "<project_name>"
+        return "<project_name> [project_dir]"
-        if len(args) != 1:
+        if len(args) not in (1, 2):
-        move(join(project_name, 'module'), join(project_name, project_name))
+        self._copytree(self.templates_dir, abspath(project_dir), ignore=IGNORE)
-            tplfile = join(project_name,
+            tplfile = join(project_dir,
-    
+    
-        respcls = responsetypes.from_args(headers=headers, url=url)
+        respcls = responsetypes.from_args(headers=headers, url=url, body=body)
-copyright = u'2008-2015, Scrapy developers'
+copyright = u'2008-2016, Scrapy developers'
-        self.files_result_field = settings.get('FILES_RESULT_FIELD')
+        self.expires = settings.getint('FILES_EXPIRES', self.EXPIRES)
-        
+
-        self.thumbs = settings.get('IMAGES_THUMBS')
+        self.expires = settings.getint('IMAGES_EXPIRES', self.EXPIRES)
-        self.assertEqual(self.pipeline.images_urls_field, self.default_settings.get('IMAGES_URLS_FIELD'))
+        default = self.pipeline.IMAGES_URLS_FIELD
-        self.assertEqual(self.pipeline.images_result_field, self.default_settings.get('IMAGES_RESULT_FIELD'))
+        default = self.pipeline.IMAGES_RESULT_FIELD
-        self.assertEqual(self.pipeline.min_width, self.default_settings.getint('IMAGES_MIN_WIDTH'))
+        default = self.pipeline.MIN_WIDTH
-        self.assertEqual(self.pipeline.thumbs, self.default_settings.get('IMAGES_THUMBS'))
+        default = self.pipeline.THUMBS
-                    logger.warning(e)
+                    logger.warning(
-                           IOError)
+                           IOError, TunnelError)
-        'w3lib>=1.13.0',
+        'w3lib>=1.14.2',
-if six.PY3:
+if not six.PY2:
-if six.PY3:
+if not six.PY2:
-            raise
+    except UnicodeEncodeError as e:
-    if not six.PY2:
+    if six.PY2:
-        # for non-UTF-8 percent-escaped characters, they get lost.
+    if six.PY2:
-        self.file.write(b"[")
+        self.file.write(b"[\n")
-        self.file.write(b"]")
+        self.file.write(b"\n]")
-                                    unquote)
+                                    quote, unquote)
-from scrapy.utils.python import to_native_str
+from scrapy.utils.python import to_bytes, to_native_str, to_unicode
-      percent-encoded using UTF-8 (RFC-3986)
+    - percent encode paths ; non-ASCII characters are percent-encoded
-    - remove fragments (unless keep_fragments is True)
+    - remove query arguments with blank values (unless `keep_blank_values` is True)
-    str.
+    The url passed can be bytes or unicode, while the url returned is
-    keyvals = parse_qsl(query, keep_blank_values)
+    # 1. decode query-string as UTF-8 (or keep raw bytes),
-    # path = moves.urllib.parse.quote(path, _safe_chars, encoding='latin1') or '/'
+    # 2. decode percent-encoded sequences in path as UTF-8 (or keep raw bytes)
-    path = safe_url_string(_unquotepath(path)) or '/'
+
-    return unquote(path)
+
-    return urlparse(to_native_str(url, encoding))
+    return urlparse(to_unicode(url, encoding))
-                              guess_scheme)
+                              guess_scheme, parse_url)
-    @unittest.skipUnless(six.PY2, "TODO")
+    def test_canonicalize_url_unicode_path(self):
-                                          "http://www.example.com/a%A3do"),
+                                          "http://www.example.com/a%A3do")
-    @unittest.skipUnless(six.PY2, "TODO")
+        self.assertEqual(canonicalize_url("http://www.example.com/do?k=r%c3%a9sum%c3%a9"),
-        self.assertEqual(canonicalize_url(u"http://www.example.com/do?price=\xa3500&a=5&z=3"),
+        self.assertEqual(canonicalize_url(u"http://www.example.com/do?price=500&a=5&z=3"),
-    @unittest.skipUnless(six.PY2, "TODO")
+    def test_canonicalize_idns(self):
-def ssl_context_factory():
+def ssl_context_factory(keyfile='keys/cert.pem', certfile='keys/cert.pem'):
-         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),
+         os.path.join(os.path.dirname(__file__), keyfile),
-                0, self.wrapper, ssl_context_factory(), interface=self.host)
+                0, self.wrapper, ssl_context_factory(self.keyfile, self.certfile),
-                self._tunneledHost, self._tunneledPort)
+            try:
-            self._protocol.transport.startTLS(self._contextFactory,
+            # this set proper Server Name Indication extension
-    from twisted.internet._sslverify import ClientTLSOptions
+    from twisted.internet.ssl import (optionsForClientTLS,
-            pass
+    from scrapy.core.downloader.tls import ScrapyClientTLSOptions
-            return ClientTLSOptions(hostname.decode("ascii"), self.getContext())
+            return ScrapyClientTLSOptions(hostname.decode("ascii"), self.getContext())
-from scrapy.utils.python import to_native_str, to_bytes
+from scrapy.utils.python import to_bytes
-        self._url = escape_ajax(safe_url_string(url))
+        s = safe_url_string(url, self.encoding)
-from six.moves.urllib.parse import urlparse
+from six.moves.urllib.parse import urlparse, parse_qs, unquote
-        self.assertEqual(r2.url, "http://www.scrapy.org/price/%A3")
+        r = self.request_class(url=u"http://www.scrapy.org/price/")
-    def test_default_encoding(self):
+    def test_default_encoding_bytes(self):
-        data = {'one': 'two', 'price': '\xc2\xa3 100'}
+        data = {b'one': b'two', b'price': b'\xc2\xa3 100'}
-        r3 = self.request_class("http://www.example.com", formdata=data, encoding='latin1')
+    def test_default_encoding_textual_data(self):
-        self.assertEqual(set(fs[b"one"]), {b"two", b"three"})
+        self.assertEqual(set(fs[b'test']), {b'val1', b'val2'})
-        self.assertEqual(fs['six'], ['seven'])
+        self.assertEqual(set(fs[b'test']), set([b'val1', b'val2']))
-        self.assertFalse('clickable2' in fs, fs)  # xpath in _get_clickable()
+        self.assertEqual(fs[b'clickable1'], [b'clicked1'])
-        self.assertEqual(fs['two'], ['2'])
+        self.assertEqual(fs[b'clickable1'], [b'clicked1'])
-        self.assertEqual(fs['two'], ['2'])
+        self.assertEqual(fs[b'clickable2'], [b'clicked2'])
-        self.assertEqual(fs, {'i1': ['i1v']})
+        self.assertEqual(fs, {b'i1': [b'i1v']})
-        self.assertEqual(fs, {'i1': ['i1v'], 'i2': ['i2v']})
+        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b'i2v']})
-                clickdata={'name': 'clickable', 'value': 'clicked2'})
+                clickdata={u'name': u'clickable', u'value': u'clicked2'})
-        self.assertEqual(fs['two'], ['clicked2'])
+        self.assertEqual(fs[b'clickable'], [b'clicked2'])
-        self.assertTrue(fs[to_native_str(u'price in \u00a3')])
+                clickdata={u'name': u'price in \u00a3'})
-                clickdata={'name': 'clickable'})
+                clickdata={u'name': u'clickable'})
-        self.assertFalse('field1' in fs, fs)
+        self.assertEqual(fs[b'clickable'], [b'clicked2'])
-        self.assertEqual(fs['clickme'], ['two'])
+        self.assertEqual(fs[b'clickme'], [b'two'])
-        self.assertFalse('clickable2' in fs, fs)
+        self.assertFalse(b'clickable1' in fs, fs)
-        self.assertNotIn('clickable1', fs)
+        self.assertIn(b'clickable2', fs)
-        self.assertEqual(fs, {'foo': ['xxx'], 'bar': ['buz']})
+        self.assertEqual(fs, {b'foo': [b'xxx'], b'bar': [b'buz']})
-        fs = _qs(req)
+        fs = _qs(req, to_unicode=True)
-        self.assertEqual(fs, {'i1': ['iv2'], 'i2': ['on']})
+        self.assertEqual(fs, {b'i1': [b'iv2'], b'i2': [b'on']})
-        self.assertEqual(fs, {'i1': ['iv2'], 'i2': ['on']})
+        self.assertEqual(fs, {b'i1': [b'iv2'], b'i2': [b'on']})
-        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': [''], 'i4': ['i4v1']})
+        self.assertEqual(fs, {b'i1': [b'i1v1'], b'i2': [b''], b'i4': [b'i4v1']})
-        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': ['']})
+        self.assertEqual(fs, {b'i1': [b'i1v1'], b'i2': [b'']})
-        self.assertEqual(fs, {'i1': ['i1v'], 'i2': [''], 'i3': ['']})
+        self.assertEqual(fs, {b'i1': [b'i1v'], b'i2': [b''], b'i3': [b'']})
-        self.assertEqual(set(fs), set(['h2', 'i2', 'i1', 'i3', 'h1', 'i5', 'i4']))
+        self.assertEqual(set(fs), set([b'h2', b'i2', b'i1', b'i3', b'h1', b'i5', b'i4']))
-def _qs(req):
+def _qs(req, encoding='utf-8', to_unicode=False):
-    return cgi.parse_qs(qs, True)
+    if six.PY2:
-        self.assertEqual(r.body, to_bytes(xmlrpclib.dumps(**kwargs)))
+        self.assertEqual(r.body,
-        self._test_request(params=(u'pas\xa3',), encoding='utf-8')
+        self._test_request(params=(u'pas',), encoding='utf-8')
-        self._test_request(params=(u'pas\xa3',), encoding='latin')
+        self._test_request(params=(u'pas',), encoding='latin1')
-                TunnelError('Could not open CONNECT tunnel.'))
+                TunnelError('Could not open CONNECT tunnel with proxy %s:%s' % (
-        # 'sample_\xc3\xb1.html'
+        # b'sample_\xc3\xb1.html'
-        # 'sample_\xc3\xa1.html'
+        # b'sample_\xc3\xa1.html'
-            Link(url='http://example.com/sample_%E1.html', text='sample \xe1 text'.decode('latin1')),
+            Link(url='http://example.com/sample_%C3%B1.html', text=''),
-        if line.lstrip().startswith('Sitemap:'):
+        if line.lstrip().lower().startswith('sitemap:'):
-from tempfile import TemporaryFile
+from tempfile import NamedTemporaryFile
-        return TemporaryFile(prefix='feed-')
+        path = spider.crawler.settings['FEED_TEMPDIR']
-from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete
+    S3FeedStorage, StdoutFeedStorage,
-        return cls(store_uri)
+        return cls(store_uri, settings=settings)
-        if width < self.MIN_WIDTH or height < self.MIN_HEIGHT:
+        if width < self.min_width or height < self.min_height:
-                                 (width, height, self.MIN_WIDTH, self.MIN_HEIGHT))
+                                 (width, height, self.min_width, self.min_height))
-        for thumb_id, size in six.iteritems(self.THUMBS):
+        for thumb_id, size in six.iteritems(self.thumbs):
-        return [Request(x) for x in item.get(self.IMAGES_URLS_FIELD, [])]
+        return [Request(x) for x in item.get(self.images_urls_field, [])]
-            item[self.IMAGES_RESULT_FIELD] = [x for ok, x in results if ok]
+        if isinstance(item, dict) or self.images_result_field in item.fields:
-    def __init__(self, store_uri, download_func=None):
+    def __init__(self, store_uri, download_func=None, settings=None):
-        return cls(store_uri)
+        return cls(store_uri, settings=settings)
-            if age_days > self.EXPIRES:
+            if age_days > self.expires:
-        return [Request(x) for x in item.get(self.FILES_URLS_FIELD, [])]
+        return [Request(x) for x in item.get(self.files_urls_field, [])]
-            item[self.FILES_RESULT_FIELD] = [x for ok, x in results if ok]
+        if isinstance(item, dict) or self.files_result_field in item.fields:
-                'last_modified': time.time() - (FilesPipeline.EXPIRES * 60 * 60 * 24 * 2)}),
+                'last_modified': time.time() - (self.pipeline.expires * 60 * 60 * 24 * 2)}),
-        cls.EXPIRES = settings.getint('FILES_EXPIRES', 90)
+        cls.FILES_URLS_FIELD = settings.get('FILES_URLS_FIELD')
-        cls.THUMBS = settings.get('IMAGES_THUMBS', {})
+        cls.MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH')
-        cls.IMAGES_RESULT_FIELD = settings.get('IMAGES_RESULT_FIELD', cls.DEFAULT_IMAGES_RESULT_FIELD)
+        cls.IMAGES_URLS_FIELD = settings.get('IMAGES_URLS_FIELD')
-from scrapy.utils.python import isbinarytext, to_bytes, to_native_str
+from scrapy.utils.python import binary_is_text, to_bytes, to_native_str
-        if isbinarytext(chunk):
+        if not binary_is_text(chunk):
-
+@deprecated("scrapy.utils.python.binary_is_text")
-    otherwise, by looking for binary bytes at their chars
+    """ This function is deprecated.
-    return any(c in _BINARYCHARS for c in text)
+    if not isinstance(data, bytes):
-    memoizemethod_noargs, isbinarytext, equal_attributes,
+    memoizemethod_noargs, binary_is_text, equal_attributes,
-        assert not isbinarytext(b"hello")
+class BinaryIsTextTest(unittest.TestCase):
-        assert not isbinarytext(u"hello".encode('utf-16'))
+        assert binary_is_text(u"hello".encode('utf-16'))
-        assert not isbinarytext(b"<div>Price \xa3</div>")
+        assert binary_is_text(b"<div>Price \xa3</div>")
-        assert isbinarytext(b"\x02\xa3")
+        assert not binary_is_text(b"\x02\xa3")
-    text = html.remove_comments(html.replace_entities(text))
+    text = html.remove_tags_with_content(text, ('script', 'noscript'))
-            response.encoding)
+            response.encoding, ignore_tags=('script', 'noscript'))
-        'w3lib>=1.8.0',
+        'w3lib>=1.13.0',
-        entry_type, info, refid, _ = node['entries'][0]
+        entry_type, info, refid = node['entries'][0][:3]
-    entry_type, info, refid, _ = node['entries'][0]
+    entry_type, info, refid = node['entries'][0][:3]
-        mtime = os.stat(rpath).st_mtime
+        mtime = os.stat(metapath).st_mtime
-        self.store.persist_file(path, buf, info)
+        buf.seek(0)
-                Metadata={k: str(v) for k, v in six.iteritems(meta)},
+                Metadata={k: str(v) for k, v in six.iteritems(meta or {})},
-    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))
+    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), "Unknown Status")))
-                                   get_meta_refresh, get_base_url)
+                                   get_meta_refresh, get_base_url, response_status_message)
-    'xls', 'xlsx', 'ppt', 'pptx', 'doc', 'docx', 'odt', 'ods', 'odg', 'odp',
+    'xls', 'xlsx', 'ppt', 'pptx', 'pps', 'doc', 'docx', 'odt', 'ods', 'odg',
-    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False, stats=None):
+    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None,
-        return cls(dupefilter, job_dir(settings), dqclass, mqclass, logunser, crawler.stats)
+        return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,
-        self.mqs = PriorityQueue(self._newmq)
+        self.mqs = self.pqclass(self._newmq)
-        q = PriorityQueue(self._newdq, startprios=prios)
+        q = self.pqclass(self._newdq, startprios=prios)
-        tunnelReq += b'\r\n'
+        tunnelReq = tunnel_request_data(self._tunneledHost, self._tunneledPort,
-
+        self._load_all_spiders()
-                'test_timeout_download_from_spider skipped under https')
+    def test_timeout_download_from_spider_nodata_rcvd(self):
-        # client connects but no data is received
+
-    Please upgrade your context factory class to handle or ignore it."""
+ '%s' does not accept `method` argument (type OpenSSL.SSL method,\
-            warnings.warn("""
+            msg = """
-    Please upgrade your context factory class to handle or ignore it.""")
+    Please upgrade your context factory class to handle or ignore it."""
-    POLICY = 'private'  # Overriden from settings.S3_STORE_ACL in
+    POLICY = 'private'  # Overriden from settings.FILES_STORE_S3_ACL in
-        s3store.POLICY = settings['S3_STORE_ACL']
+        s3store.POLICY = settings['FILES_STORE_S3_ACL']
-    POLICY = 'public-read'
+    POLICY = 'private'  # Overriden from settings.S3_STORE_ACL in
-from scrapy.core.downloader.tls import openssl_methods, METHOD_TLS
+from scrapy.core.downloader.tls import openssl_methods
-            # use defaults
+            # use context factory defaults
-
+            warnings.warn("""
-            return CertificateOptions(verify=False, method=self._ssl_method)
+
-    METHOD_SSLv3: SSL.SSLv3_METHOD,                     # SSL 3 (NOT recommended)
+    METHOD_TLS:    SSL.SSLv23_METHOD,                   # protocol negotiation (recommended)
-                self._tunneledHost, self._tunneledPort), encoding='ascii')
+        tunnelReq = (
-        raise SkipTest(str(e))
+        raise SkipTest(e)
-        raise SkipTest(e.message)
+        raise SkipTest(str(e))
-            # server require this otherwise returing HTTP 411
+            # servers require this, otherwise returning HTTP 411 Length required
-        bodyproducer = _RequestBodyProducer(request.body) if request.body else None
+        if request.body:
-            super(BrowserLikePolicyForHTTPS, self).__init__(*args, **kwargs)
+            super(ScrapyClientContextFactory, self).__init__(*args, **kwargs)
-if twisted_version >= (14, 0, 0):
+try:
-else:
+except ImportError:
-         os.path.join(os.path.dirname(__file__), 'keys/server.pem'),
+         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),
-    from twisted.internet.ssl import optionsForClientTLS
+    from twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust
-        Using Twisted recommended context factory for twisted.web.client.Agent
+        Non-peer-certificate verifying HTTPS context factory
-        so unless you have special requirements you can leave this as-is."
+        Default OpenSSL method is TLS_METHOD (also called SSLv23_METHOD)
-        See http://twistedmatrix.com/documents/current/api/twisted.web.client.Agent.html
+        'A TLS/SSL connection established with [this method] may
-    class OpenSSLMethodContextFactory(ScrapyClientContextFactory):
+    class BrowserLikeContextFactory(ScrapyClientContextFactory):
-        openssl_method = SSL.SSLv23_METHOD
+        Quoting http://twistedmatrix.com/documents/current/api/twisted.web.client.Agent.html:
-                                       trustRoot=self._trustRoot,
+                                       trustRoot=platformTrust(),
-                                            'method': self.openssl_method
+                                            'method': self._ssl_method,
-    class OpenSSLMethodContextFactory(ClientContextFactory):
+    class ScrapyClientContextFactory(ClientContextFactory):
-            self.method = self.openssl_method
+        def __init__(self, method=SSL.SSLv23_METHOD):
-    openssl_method = SSL.TLSv1_METHOD
+from scrapy.core.downloader.tls import openssl_methods, METHOD_TLS
-        self._contextFactory = self._contextFactoryClass()
+        # try method-aware context factory
-         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),
+         os.path.join(os.path.dirname(__file__), 'keys/server.pem'),
-    it hasn't been already scheduled since the last time it run.
+    it hasn't been already scheduled since the last time it ran.
-    object can be a class, function, variable o instance.
+    object can be a class, function, variable or an instance.
-    """Loads a module and all its submodules from a the given module path and
+    """Loads a module and all its submodules from the given module path and
-                raise six.reraise(*exc_info)
+                six.reraise(*exc_info)
-    from twisted.web.client import BrowserLikePolicyForHTTPS
+    from twisted.web.client import BrowserLikePolicyForHTTPS
-        return ctx
+
-        p = self.runspider("", "myspider.txt")
+        p = self.runspider('', 'myspider.txt')
-        self.assertIn("Unable to load", log)
+        self.assertIn('Unable to load', log)
-                ACL=self.POLICY)
+                ACL=self.POLICY,
-def get_s3_content_and_delete(bucket, path):
+def get_s3_content_and_delete(bucket, path, with_key=False):
-    return content
+    return (content, key) if with_key else content
-        yield store.persist_file(path, buf, info=None, meta=meta)
+        yield store.persist_file(
-        content = get_s3_content_and_delete(u.hostname, u.path[1:])
+        content, key = get_s3_content_and_delete(
-                headers=request.headers.to_native_string_dict(),
+                headers=request.headers.to_unicode_dict(),
-    def to_native_string_dict(self):
+    def to_unicode_dict(self):
-            exc = defer.fail()
+            # In Python 2 reraising an exception after yield discards
-            yield exc
+
-import six
+from contextlib import contextmanager
-    def test_runspider(self):
+    @contextmanager
-        fname = abspath(join(tmpdir, 'myspider.py'))
+        fname = abspath(join(tmpdir, name))
-            f.write("""
+            f.write(content)
-        p = self.proc('runspider', fname)
+"""
-        p = self.proc('runspider', fname)
+        p = self.runspider("from scrapy.spiders import Spider\n")
-        p = self.proc('runspider', fname)
+        p = self.runspider("", "myspider.txt")
-            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]
+            filename = to_native_str(content_disposition,
-            ('attachment; filename=data.xml', XmlResponse),
+            (b'attachment; filename="data.xml"', XmlResponse),
-            TypeError, S3DownloadHandler, Settings(), extra_kw=True)
+        try:
-            S3DownloadHandler(Settings(), extra_kw=True)
+        self.assertRaises(
-        self.assertEqual(s['checksum'], b'3187896a9657a28163abb31667df64c8')
+        self.assertEqual(s['checksum'], '3187896a9657a28163abb31667df64c8')
-        try:
+        self.is_botocore = is_botocore()
-            modified_stamp = int(mktime_tz(modified_tuple))
+            if self.is_botocore:
-        return threads.deferToThread(b.get_key, key_name)
+        if self.is_botocore:
-                                     headers=h, policy=self.POLICY)
+        if self.is_botocore:
-from scrapy.utils.boto import is_botocore
+from __future__ import absolute_import
-from scrapy.utils.test import assert_aws_environ
+from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete
-        uri = os.environ.get('FEEDTEST_S3_URI')
+        uri = os.environ.get('S3_TEST_FILE_URI')
-        content = self._get_content_and_delete(u.hostname, u.path[1:])
+        content = get_s3_content_and_delete(u.hostname, u.path[1:])
-
+from six.moves.urllib.parse import urlparse
-from scrapy.pipelines.files import FilesPipeline, FSFilesStore
+from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore
-        file.write("content")
+        expected_content = b"content: \xe2\x98\x83"
-        self.assertEqual(content, "content")
+        self.assertEqual(content, expected_content)
-from scrapy.utils.python import to_unicode
+from scrapy.utils.boto import is_botocore
-        return None
+def _get_boto_connection():
-        try:
+        if is_botocore():
-        else:
+        else:
-        self.keyname = u.path
+        self.is_botocore = is_botocore()
-        key.close()
+        if self.is_botocore:
-import six
+from scrapy.exceptions import NotConfigured
-            raise SkipTest('missing botocore library')
+        is_botocore()
-        bucket.delete_key(u.path)
+        content = self._get_content_and_delete(u.hostname, u.path[1:])
-        key = connect_s3().get_bucket(u.hostname, validate=False).get_key(u.path)
+        bucket = connect_s3().get_bucket(u.hostname, validate=False)
-
+    skip_if_no_boto()
-from scrapy.utils.test import get_crawler
+from scrapy.utils.test import get_crawler, skip_if_no_boto
-class S3AnonTestCase(BaseS3TestCase):
+class S3AnonTestCase(unittest.TestCase):
-class S3TestCase(BaseS3TestCase):
+class S3TestCase(unittest.TestCase):
-                b'AWS 0PN5J17HBGZHT7JJ3X82:otYM2krxnuHhAofO4oqIV7wcfdU='])
+        self.assertEqual(httpreq.headers['Authorization'],
-                aws_access_key_id, aws_secret_access_key))
+            if not self.anon:
-        except ImportError:
+    try:
-class S3TestCase(unittest.TestCase):
+class S3TestCase(BaseS3TestCase):
-        httpreq = self.download_request(req, self.spider)
+        date ='Tue, 27 Mar 2007 19:36:42 +0000'
-            'Date': 'Tue, 27 Mar 2007 21:15:45 +0000',
+            'Date': date,
-        httpreq = self.download_request(req, self.spider)
+        with self._mocked_date(date):
-                    'Date': 'Tue, 27 Mar 2007 19:42:41 +0000',
+                    'Date': date,
-        httpreq = self.download_request(req, self.spider)
+        with self._mocked_date(date):
-        httpreq = self.download_request(req, self.spider)
+        date = 'Tue, 27 Mar 2007 19:44:46 +0000'
-                    'Date': 'Tue, 27 Mar 2007 21:20:27 +0000',
+                    'Date': date,
-                b'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')
+        with self._mocked_date(date):
-                    'Date': 'Tue, 27 Mar 2007 21:06:08 +0000',
+                    'Date': date,
-        httpreq = self.download_request(req, self.spider)
+        with self._mocked_date(date):
-        httpreq = self.download_request(req, self.spider)
+            headers={'Date': date},
-                    for key, value in request.headers.items()),
+                headers=request.headers.to_native_string_dict(),
-        anon = kw.get('anon', None)
+        anon = kw.get('anon')
-        if self._signer is not None:
+        if self.anon:
-        self.assertEqual(self.s3reqh.conn.anon, True)
+        self.assertEqual(hasattr(self.s3reqh, 'anon'), True)
-
+        self._signer = None
-            raise NotConfigured(str(ex))
+            import botocore.auth
-        signed_headers = self.conn.make_request(
+        if self._signer is not None:
-                headers=request.headers,
+                url='%s://s3.amazonaws.com/%s%s' % (scheme, bucket, path),
-        return self._download_http(httpreq, spider)
+            self._signer.add_auth(awsrequest)
-        skip = 'missing boto library'
+class BaseS3TestCase(unittest.TestCase):
-        skip = 'S3 not supported on Py3'
+        try:
-                'AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')
+                b'AWS 0PN5J17HBGZHT7JJ3X82:xXjDGYUmKxnwqr5KXNPGldn5LbA=')
-                'AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')
+                b'AWS 0PN5J17HBGZHT7JJ3X82:hcicpDDvL9SsO6AkvxqmIWkmOuQ=')
-                'AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')
+                b'AWS 0PN5J17HBGZHT7JJ3X82:jsRt/rhG+Vtp88HrYL706QhE4w4=')
-                'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')
+                b'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')
-                'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')
+                b'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')
-                'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')
+                b'AWS 0PN5J17HBGZHT7JJ3X82:C0FlOtU8Ylb9KDTpZqYkZPX91iI=')
-            'AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=')
+            b'AWS 0PN5J17HBGZHT7JJ3X82:+CfvG8EZ3YccOrRVMXNaK2eKZmM=')
-    # python3 uses request.unverifiable
+    def get_origin_req_host(self):
-        return urlparse_cached(self.request).hostname
+    @property
-    def __init__(self, uri, _stdout=sys.stdout):
+    def __init__(self, uri, _stdout=None):
-            for url in sitemap_urls_from_robots(response.body):
+            for url in sitemap_urls_from_robots(response.text):
-    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', _callback=None):
+    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None, _callback=None):
-            msg.attach(MIMEText(body))
+            msg.attach(MIMEText(body, 'plain', charset or 'us-ascii'))
-        return idle
+        if not self.scraper.slot.is_idle():
-        return defer.DeferredList([c.stop() for c in self.crawlers])
+        return defer.DeferredList([c.stop() for c in list(self.crawlers)])
-        else:
+        elif isinstance(serialized_value, six.text_type):
-        for key, slot in self.slots.items():
+        for key, slot in list(self.slots.items()):
-            print(settings.get(opts.get))
+            s = settings.get(opts.get)
-        self.assertIn('value=200', settingsdict[EXT_PATH])
+        self.assertEquals(200, settingsdict[EXT_PATH])
-        return str(self.attributes)
+    def _to_dict(self):
-        return "<%s %s>" % (self.__class__.__name__, self.attributes)
+        Modifications to the returned dict won't be reflected on the original
-from scrapy.utils.url import guess_scheme
+from datetime import datetime
-        }
+    def test_nonstring_types_item(self):
-        }
+    def test_nonstring_types_item(self):
-            item=item,
+            item=self._get_nonstring_types_item(),
-    """
+    """Add an URL scheme if missing: file:// for filepath-like input or http:// otherwise."""
-                              canonicalize_url, add_http_if_no_scheme)
+                              canonicalize_url, add_http_if_no_scheme,
-    
+
-    
+
-        return add_http_if_no_scheme(url)
+from scrapy.utils.url import guess_scheme
-from scrapy.commands.shell import guess_scheme
+from scrapy.utils.url import guess_scheme
-                             robotsreq, spider)
+            dfd = self.crawler.engine.download(robotsreq, spider)
-        self._parsers.pop(netloc).callback(None)
+        rp_dfd = self._parsers[netloc]
-        # check for self.encoding before _cached_ubody just in
+        return self.text
-                yield to_native_str(str(s))
+                yield s
-                yield to_native_str(repr(s))
+                yield to_native_str(str(s))
-            return to_unicode(value, encoding=self.encoding)
+        encode_func = to_bytes if self.binary else to_unicode
-            dfd = self.crawler.engine.download(robotsreq, spider)
+            # engine.download() can return an already-called deferred, e.g. if a
-        body = response.body_as_unicode()[:self.lookup_bytes]
+        body = response.text[:self.lookup_bytes]
-            body = response.body_as_unicode()
+        if hasattr(response, 'text'):
-    root = create_root_node(text, lxml.html.HTMLParser, base_url=get_base_url(response))
+    root = create_root_node(response.text, lxml.html.HTMLParser,
-            text = response.body_as_unicode()
+            text = response.text
-            return obj.body_as_unicode()
+            return obj.text
-        text = response.body_as_unicode()[0:4096]
+        text = response.text[0:4096]
-        text = response.body_as_unicode()[0:4096]
+        text = response.text[0:4096]
-        m = self.name_re.search(body)
+        m = self.name_re.search(response.text)
-        m = self.price_re.search(body)
+        m = self.price_re.search(response.text)
-        self.assertEqual(r6.body_as_unicode(), u'WORD\ufffd\ufffd')
+        self.assertEqual(r6.text, u'WORD\ufffd\ufffd')
-        # response.body_as_unicode() in indistint order doesn't affect final
+        # response.text in indistint order doesn't affect final
-        self.assertEqual(response.body_as_unicode(), u'WORD')
+        self.assertEqual(response.text, u'WORD')
-        self.assertEqual(response.body_as_unicode(), u'WORD')
+        self.assertEqual(response.text, u'WORD')
-        self.assertEqual(response.body_as_unicode(), u'WORD')
+        self.assertEqual(response.text, u'WORD')
-        self.assertEqual(response.body_as_unicode(), u'WORD')
+        self.assertEqual(response.text, u'WORD')
-        assert u'SUFFIX' in r.body_as_unicode(), repr(r.body_as_unicode())
+        assert u'\ufffd' in r.text, repr(r.text)
-        assert u'<span>value</span>' in r.body_as_unicode(), repr(r.body_as_unicode())
+        assert u'<span>value</span>' in r.text, repr(r.text)
-        #assert u'\ufffd' in r.body_as_unicode(), repr(r.body_as_unicode())
+        #r = self.response_class("http://www.example.com", body=b'PREFIX\xe3\xabSUFFIX')
-        enabled = [x.__class__.__name__ for x in middlewares]
+from scrapy.exceptions import NotConfigured
-        obj = cls(job_dir(crawler.settings))
+        jobdir = job_dir(crawler.settings)
-                     'enabledlist': pprint.pformat(mwlist)},
+                     'enabledlist': pprint.pformat(enabled)},
-        return dict(name='foo', allowed_domains=['scrapytest.org', 'scrapy.org'])
+        return dict(name='foo', allowed_domains=['scrapytest.org', 'scrapy.org', 'scrapy.test.org'])
-                       Request('http://offsite.tld/letmepass', dont_filter=True)]
+                       Request('http://offsite.tld/letmepass', dont_filter=True),
-                       Request('http://roguescrapytest.org')]
+                       Request('http://roguescrapytest.org'),
-    pass
+    def __init__(self, *args, **kwargs):
-    else:
+    else:  # pragma: no cover
-    XmlItemExporter, JsonLinesItemExporter, JsonItemExporter, PythonItemExporter
+    XmlItemExporter, JsonLinesItemExporter, JsonItemExporter,
-        values = [to_native_str(x) for _, x in fields]
+        values = list(self._build_row(x for _, x in fields))
-            row = [to_native_str(s) for s in self.fields_to_export]
+            row = list(self._build_row(self.fields_to_export))
-    parts = urlparse(url)
+        parts = urlparse(url)
-        self.assertEqual(attach.get_payload(decode=True), 'content')
+        self.assertEqual(text.get_payload(decode=True), b'body')
-import six
+    def assertExportedXml(self, items, rows, settings=None):
-                PendingDeprecationWarning)
+                ScrapyDeprecationWarning)
-        expected_value = u'<?xml version="1.0" ?>\n<items><item><age>22</age><name>John\xa3</name></item></items>'
+        expected_value = b'<?xml version="1.0" encoding="utf-8"?>\n<items><item><age>22</age><name>John\xc2\xa3</name></item></items>'
-            u'<?xml version="1.0" ?>\n<items><item><name><value>John\xa3</value><value>Doe</value></name></item></items>'
+            b'<?xml version="1.0" encoding="utf-8"?>\n<items><item><name><value>John\xc2\xa3</value><value>Doe</value></name></item></items>'
-            u'</items>'
+            b'<?xml version="1.0" encoding="utf-8"?>\n'
-            u'</items>'
+            b'<?xml version="1.0" encoding="utf-8"?>\n'
-
+class ShellURLTest(unittest.TestCase):
-            url, guessed_url, scheme)
+import pytest
-
+
-
+import re
-    return url
+    if parts.scheme:
-        self.assertEquals(url, iurl)
+    pass
-from six.moves.urllib.parse import urlparse, urlunparse
+from six.moves.urllib.parse import urlparse
-import warnings
+import six
-                url = add_http_if_no_scheme(url)
+            # first argument may be a local file
-            assert b'{}' in out
+    def test_local_file(self):
-            self.assertIn(b'DNS lookup failed', err)
+    def test_dns_failures(self):
-
+            key = to_bytes(key) if self.binary else key
-        return dict(self._get_serialized_fields(item))
+        result = dict(self._get_serialized_fields(item))
-        reader = csv.DictReader(data.splitlines())
+        reader = csv.DictReader(to_native_str(data).splitlines())
-        parsed = [json.loads(line) for line in data.splitlines()]
+        parsed = [json.loads(to_native_str(line)) for line in data.splitlines()]
-from scrapy.utils.python import to_bytes, to_unicode, is_listlike
+from scrapy.utils.python import to_bytes, to_unicode, to_native_str, is_listlike
-        serializer = field.get('serializer', self._to_str_if_unicode)
+        serializer = field.get('serializer', self._join_if_needed)
-    def _to_str_if_unicode(self, value):
+    def _join_if_needed(self, value):
-                value = self._join_multivalued.join(value)
+                return self._join_multivalued.join(value)
-        return value.encode(self.encoding) if isinstance(value, six.text_type) else value
+        return value
-        values = [x[1] for x in fields]
+        values = [to_native_str(x) for _, x in fields]
-            self.csv_writer.writerow(self.fields_to_export)
+            row = [to_native_str(s) for s in self.fields_to_export]
-
+class CsvItemExporterTest(BaseItemExporterTest):
-        self.assertCsvEqual(self.output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n')
+        self.assertCsvEqual(to_unicode(self.output.getvalue()), u'age,name\r\n22,John\xa3\r\n')
-            expected='age,name\r\n22,John\xc2\xa3\r\n',
+            expected=b'age,name\r\n22,John\xc2\xa3\r\n',
-            expected='age,name\r\n22,John\xc2\xa3\r\n',
+            expected=b'age,name\r\n22,John\xc2\xa3\r\n',
-                expected='age\r\n22\r\n',
+                expected=b'age\r\n22\r\n',
-            self.assertCsvEqual(output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n22,John\xc2\xa3\r\n')
+            self.assertCsvEqual(output.getvalue(),
-                expected='22,John\xc2\xa3\r\n',
+                expected=b'22,John\xc2\xa3\r\n',
-import os
+from os.path import join
-            relpath(test_file_path),
+            # relpath(test_file_path),
-              for value in serialized_value:
+            for value in serialized_value:
-from scrapy.utils.python import to_bytes, to_unicode
+from scrapy.utils.python import to_bytes, to_unicode, is_listlike
-              and not isinstance(serialized_value, six.string_types)):
+        elif is_listlike(serialized_value):
-                and not isinstance(value, six.string_types):
+        if is_listlike(value):
-        self.file.write(self.encoder.encode(itemdict) + '\n')
+        self.file.write(to_bytes(self.encoder.encode(itemdict) + '\n'))
-        self.file.write("[")
+        self.file.write(b"[")
-        self.file.write("]")
+        self.file.write(b"]")
-            self.file.write(',\n')
+            self.file.write(b',\n')
-        self.file.write(self.encoder.encode(itemdict))
+        self.file.write(to_bytes(self.encoder.encode(itemdict)))
-            for value in serialized_value:
+        elif (hasattr(serialized_value, '__iter__')
-            if not isinstance(serialized_value, unicode):
+            if not isinstance(serialized_value, six.text_type):
-        return value.encode(self.encoding) if isinstance(value, unicode) else value
+        return value.encode(self.encoding) if isinstance(value, six.text_type) else value
-        self.file.write(pprint.pformat(itemdict) + '\n')
+        self.file.write(to_bytes(pprint.pformat(itemdict) + '\n'))
-        if hasattr(value, '__iter__'):
+        if hasattr(value, '__iter__') \
-        return value.encode(self.encoding) if isinstance(value, unicode) else value
+        if self.binary:
-class MidRefactoringBaseItemExporterTest(BaseItemExporterTest):
+class IntermediateRefactoringBaseItemExporterTest(BaseItemExporterTest):
-class PythonItemExporterTest(MidRefactoringBaseItemExporterTest):
+class PythonItemExporterTest(BaseItemExporterTest):
-        return PythonItemExporter(**kwargs)
+        return PythonItemExporter(binary=False, **kwargs)
-class CsvItemExporterTest(MidRefactoringBaseItemExporterTest):
+@unittest.skipUnless(six.PY2, "TODO")
-        expected_value = '<?xml version="1.0" encoding="utf-8"?>\n<items><item><age>22</age><name>John\xc2\xa3</name></item></items>'
+        expected_value = u'<?xml version="1.0" ?>\n<items><item><age>22</age><name>John\xa3</name></item></items>'
-            '<?xml version="1.0" encoding="utf-8"?>\n<items><item><name><value>John\xc2\xa3</value><value>Doe</value></name></item></items>'
+            u'<?xml version="1.0" ?>\n<items><item><name><value>John\xa3</value><value>Doe</value></name></item></items>'
-            '</items>'
+            u'<?xml version="1.0" ?>\n'
-            '</items>'
+            u'<?xml version="1.0" ?>\n'
-        exported = json.loads(self.output.getvalue().strip())
+        exported = json.loads(to_unicode(self.output.getvalue().strip()))
-        exported = json.loads(self.output.getvalue())
+        exported = json.loads(to_unicode(self.output.getvalue()))
-        exported = json.loads(self.output.getvalue().strip())
+        exported = json.loads(to_unicode(self.output.getvalue().strip()))
-        exported = json.loads(self.output.getvalue())
+        exported = json.loads(to_unicode(self.output.getvalue()))
-        exported = json.loads(self.output.getvalue())
+        exported = json.loads(to_unicode(self.output.getvalue()))
-        exported = json.loads(self.output.getvalue())
+        exported = json.loads(to_unicode(self.output.getvalue()))
-        serializer = field.get('serializer', self._to_str_if_unicode)
+        serializer = field.get('serializer', lambda x: x)
-class JsonItemExporter(JsonLinesItemExporter):
+class JsonItemExporter(BaseItemExporter):
-        return super(CsvItemExporter, self)._to_str_if_unicode(value)
+        return value.encode(self.encoding) if isinstance(value, unicode) else value
-        return self._to_str_if_unicode(value)
+        return value.encode(self.encoding) if isinstance(value, unicode) else value
-        self.i = TestItem(name=u'John\xa3', age='22')
+        self.i = TestItem(name=u'John\xa3', age=u'22')
-class PythonItemExporterTest(BaseItemExporterTest):
+class PythonItemExporterTest(MidRefactoringBaseItemExporterTest):
-class CsvItemExporterTest(BaseItemExporterTest):
+class CsvItemExporterTest(MidRefactoringBaseItemExporterTest):
-def getarg(request, name, default=None, type=str):
+def getarg(request, name, default=None, type=None):
-        return type(request.args[name][0])
+        value = request.args[name][0]
-        for s in xrange(100):
+        for s in range(100):
-from scrapy.utils.python import to_bytes
+from scrapy.utils.python import to_bytes, to_unicode
-            'body': request.content.read(),
+            'headers': dict(
-        echo0 = json.loads(crawler.spider.meta['responses'][2].body)
+        echo0 = json.loads(to_unicode(crawler.spider.meta['responses'][2].body))
-        echo1 = json.loads(crawler.spider.meta['responses'][1].body)
+        echo1 = json.loads(to_unicode(crawler.spider.meta['responses'][1].body))
-        echo2 = json.loads(crawler.spider.meta['responses'][2].body)
+        echo2 = json.loads(to_unicode(crawler.spider.meta['responses'][2].body))
-        echo3 = json.loads(crawler.spider.meta['responses'][3].body)
+        echo3 = json.loads(to_unicode(crawler.spider.meta['responses'][3].body))
-        if order == "rand":
+        total = getarg(request, b"total", 100, type=int)
-            args["n"] = [str(nl)]
+            args[b"n"] = [to_bytes(str(nl))]
-        request.write(s)
+        request.write(to_bytes(s))
-        b = getarg(request, "b", 1, type=int)
+        n = getarg(request, b"n", 1, type=float)
-        request.write("Response delayed for %0.3f seconds\n" % n)
+        request.write(to_bytes("Response delayed for %0.3f seconds\n" % n))
-        n = getarg(request, "n", 200, type=int)
+        n = getarg(request, b"n", 200, type=int)
-        return ""
+        return b""
-        raw = getarg(request, 'raw', 'HTTP 1.1 200 OK\n')
+        raw = getarg(request, b'raw', b'HTTP 1.1 200 OK\n')
-        return json.dumps(output)
+        return to_bytes(json.dumps(output))
-        abort = getarg(request, "abort", 0, type=int)
+        abort = getarg(request, b"abort", 0, type=int)
-    def __init__(self):
+    def __init__(self, auth_encoding='latin-1'):
-            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))
+            user_pass = to_bytes(
-
+from scrapy.utils.python import to_bytes
-            user_pass = '%s:%s' % (unquote(user), unquote(password))
+            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))
-            request.headers['Proxy-Authorization'] = 'Basic ' + creds
+            request.headers['Proxy-Authorization'] = b'Basic ' + creds
-        self.assertEquals(req.headers.get('Proxy-Authorization'), 'Basic dXNlcjpwYXNz')
+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')
-        self.assertEquals(req.headers.get('Proxy-Authorization'), 'Basic dXNlcjo=')
+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')
-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)
+        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
-        rsp = Response('http://www.scrapytest.org/404', body='', status=404)
+        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)
-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)
+        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)
+        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
-        show = _getarg(request, 'show', 10, int)
+        total = _getarg(request, b'total', 100, int)
-        request.write("<html><head></head><body>")
+        request.write(b"<html><head></head><body>")
-        return ''
+                          .format(argstr, nl).encode('utf8'))
-    
+
-        
+
-                      clickdata=None, dont_click=False, formxpath=None, **kwargs):
+                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):
-            
+
-            self._print_headers(response.request.headers, '>')
+            self._print_headers(response.request.headers, b'>')
-            self._print_headers(response.headers, '<')
+            self._print_headers(response.headers, b'<')
-from scrapy.utils.python import to_unicode
+        prefix = prefix.encode()
-                      prefix, to_unicode(key), to_unicode(value)))
+                self._print_bytes(prefix + b' ' + key + b': ' + value)
-            print(to_unicode(response.body))
+            self._print_bytes(response.body)
-                print('%s %s: %s' % (prefix, key, value))
+                print('%s %s: %s' % (
-            print(response.body)
+            print(to_unicode(response.body))
-    r.putChild("redirected", static.Data("Redirected here", "text/plain"))
+    r.putChild(b"text", static.Data(b"Works", "text/plain"))
-    
+
-        self.assertEqual(out.strip(), 'Works')
+        self.assertEqual(out.strip(), b'Works')
-        assert 'Content-Type: text/plain' in out
+        out = out.replace(b'\r', b'') # required on win32
-        assert '{}' in out
+        assert b'{}' in out
-        assert 'Works' in out
+        assert b'Works' in out
-        assert 'TextResponse' in out
+        assert b'TextResponse' in out
-        assert 'HtmlResponse' in out
+        assert b'HtmlResponse' in out
-        self.assertEqual(out.strip(), 'Works')
+        self.assertEqual(out.strip(), b'Works')
-        self.assertEqual(out.strip(), 'gb18030')
+        self.assertEqual(out.strip(), b'gb18030')
-        assert out.strip().endswith('/redirected')
+        assert out.strip().endswith(b'/redirected')
-        e = ExecutionEngine(get_crawler(TestSpider), lambda: None)
+        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
-        e = ExecutionEngine(get_crawler(TestSpider), lambda: None)
+        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
-        e = ExecutionEngine(get_crawler(TestSpider), lambda: None)
+        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)
-        m = self.name_re.search(response.body)
+        body = response.body_as_unicode()
-        m = self.price_re.search(response.body)
+        m = self.price_re.search(body)
-    r.putChild("redirected", static.Data("Redirected here", "text/plain"))
+    r.putChild(b"redirect", util.Redirect(b"/redirected"))
-if six.PY3:
+if six.PY2:
-        return gzf.read1(size)
+        return gzf.read(size)
-        return gzf.read(size)
+        return gzf.read1(size)
-            b' HTTP/1.1\r\n')
+        tunnelReq = to_bytes(
-    skip = 'boto' not in optional_features and 'missing boto library'
+    try:
-    except NotConfigured:
+        import boto
-            return self.read(size)
+import six
-    f = ReadOneGzipFile(fileobj=BytesIO(data))
+    f = GzipFile(fileobj=BytesIO(data))
-            chunk = f.readone(8196)
+            chunk = read1(f, 8196)
-            omitConnectTunnel = proxyParams.find(b'noconnect') >= 0
+            omitConnectTunnel = b'noconnect' in proxyParams
-    if opts.profile or opts.lsprof:
+    if opts.profile:
-    except pickle.PicklingError as e:
+    # Python>=3.5 raises AttributeError here while
-                # See issue https://twistedmatrix.com/trac/ticket/8175
+                # See issue https://twistedmatrix.com/trac/ticket/8175
-
+                request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')
-                raise unittest.SkipTest("xpayload only enabled for PY2")
+            else:
-        if isinstance(url, six.string_types):
+        if isinstance(url, str):
-        self.assertRaises((ValueError, AttributeError), q.push, lambda x: x)
+        self.assertRaises(ValueError, q.push, lambda x: x)
-        self.assertRaises((ValueError, AttributeError), q.push, lambda x: x)
+        self.assertRaises(ValueError, q.push, lambda x: x)
-    
+
-    
+
-        if encoding == 'gzip' or encoding == 'x-gzip':
+        if encoding == b'gzip' or encoding == b'x-gzip':
-        if encoding == 'deflate':
+        if encoding == b'deflate':
-        self.assertEqual(request.headers.get('Accept-Encoding'), 'gzip,deflate')
+        self.assertEqual(request.headers.get('Accept-Encoding'), b'gzip,deflate')
-        self.assertEqual(response.headers['Content-Encoding'], 'gzip')
+        self.assertEqual(response.headers['Content-Encoding'], b'gzip')
-        assert newresponse.body.startswith('<!DOCTYPE')
+        assert newresponse.body.startswith(b'<!DOCTYPE')
-        self.assertEqual(response.headers['Content-Encoding'], 'deflate')
+        self.assertEqual(response.headers['Content-Encoding'], b'deflate')
-        assert newresponse.body.startswith('<!DOCTYPE')
+        assert newresponse.body.startswith(b'<!DOCTYPE')
-        self.assertEqual(response.headers['Content-Encoding'], 'deflate')
+        self.assertEqual(response.headers['Content-Encoding'], b'deflate')
-        assert newresponse.body.startswith('<!DOCTYPE')
+        assert newresponse.body.startswith(b'<!DOCTYPE')
-        response = Response('http://scrapytest.org', body='<!DOCTYPE...')
+        response = Response('http://scrapytest.org', body=b'<!DOCTYPE...')
-        assert newresponse.body.startswith('<!DOCTYPE')
+        assert newresponse.body.startswith(b'<!DOCTYPE')
-        self.assertEqual(newresponse.headers.getlist('Content-Encoding'), ['uuencode'])
+        self.assertEqual(newresponse.headers.getlist('Content-Encoding'), [b'uuencode'])
-        self.assertEqual(response.headers['Content-Type'], 'application/gzip')
+        self.assertEqual(response.headers['Content-Encoding'], b'gzip')
-        urlparse(badInput)
+        if not six.PY2:
-        self.assertTrue(isinstance(path, bytes))
+        self.assertTrue(isinstance(scheme, str))
-def getPage(url, contextFactory=None, r_transform=None, *args, **kwargs):
+def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):
-        f.deferred.addCallback(r_transform or (lambda r: r.body))
+        f.deferred.addCallback(response_transform or (lambda r: r.body))
-            self.getURL('encoding'), body=body, r_transform=lambda r: r)\
+            self.getURL('encoding'), body=body, response_transform=lambda r: r)\
-
+    @defer.inlineCallbacks
-        url = to_bytes(urldefrag(request.url)[0])
+        url = urldefrag(request.url)[0]
-        d = agent.request(method, url, headers, bodyproducer)
+        d = agent.request(
-            url=url, status=status, headers=headers, body=body, flags=flags)
+        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)
-    _responseMatcher = re.compile('HTTP/1\.. 200')
+    _responseMatcher = re.compile(b'HTTP/1\.. 200')
-                                                  self._tunneledPort)
+        tunnelReq = (
-        tunnelReq += '\r\n'
+            tunnelReq += \
-            self.assertEquals(response.body, to_bytes('127.0.0.1:%d' % self.portno))
+            self.assertEquals(
-        self.host = '127.0.0.1'
+        self.host = 'localhost'
-    host = to_bytes(parsed.hostname)
+    path = b(path)
-    netloc = to_bytes(parsed.netloc)
+    scheme = b(parsed.scheme)
-        self.url = to_bytes(self._url)
+        self.url = to_bytes(self._url, encoding='ascii')
-                      (self.project_name, join(self.tmpl, 'project')), out)
+        self.assertIn("New Scrapy project %r, using template directory" % self.project_name, out)
-from twisted.web import server, static, error, util
+from twisted.web import server, static, util, resource
-def getPage(url, contextFactory=None, *args, **kwargs):
+def getPage(url, contextFactory=None, r_transform=None, *args, **kwargs):
-        f.deferred.addCallback(lambda r: r.body)
+        f.deferred.addCallback(r_transform or (lambda r: r.body))
-            self.factory.page('')
+            self.factory.page(b'')
-        if len(file_path) > 0 and file_path[0] != '#':
+        if file_path and file_path[0] != '#':
-        nodetext = etree.tostring(node, encoding=six.text_type)
+        nodetext = etree.tostring(node, encoding='unicode')
-
+from io import UnsupportedOperation
-    f = GzipFile(fileobj=BytesIO(data))
+    f = ReadOneGzipFile(fileobj=BytesIO(data))
-            chunk = f.read(8196)
+            chunk = f.readone(8196)
-                break
+            if output or getattr(f, 'extrabuf', None):
-        self.assertRaises(ValueError, q.push, lambda x: x)
+        self.assertRaises((ValueError, AttributeError), q.push, lambda x: x)
-        self.assertRaises(ValueError, q.push, lambda x: x)
+        self.assertRaises((ValueError, AttributeError), q.push, lambda x: x)
-            to_bytes, settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS'))
+        self.ignore_response_cache_controls = [to_bytes(cc) for cc in
-from scrapy.utils.python import to_bytes
+from scrapy.utils.python import to_bytes, to_unicode
-         )
+    contextFactory = ssl_context_factory()
-from tests.mockserver import MockServer
+from tests.mockserver import MockServer, ssl_context_factory
-        self.port = reactor.listenTCP(0, self.wrapper, interface='127.0.0.1')
+        self.host = '127.0.0.1'
-        return "http://127.0.0.1:%d/%s" % (self.portno, path)
+        return "%s://%s:%d/%s" % (self.scheme, self.host, self.portno, path)
-        self.ignore_response_cache_controls = settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')
+        self.ignore_response_cache_controls = map(
-            res0b = res0.replace(body='foo')
+            res0b = res0.replace(body=b'foo')
-                res0b = res0a.replace(body='bar')
+                res0b = res0a.replace(body=b'bar')
-            f.write(repr(metadata))
+            f.write(to_bytes(repr(metadata)))
-        batch.Put('%s_time' % key, str(time()))
+        batch.Put(key + b'_data', pickle.dumps(data, protocol=2))
-            ts = self.db.Get('%s_time' % key)
+            ts = self.db.Get(key + b'_time')
-            data = self.db.Get('%s_data' % key)
+            data = self.db.Get(key + b'_data')
-        return request_fingerprint(request)
+        return to_bytes(request_fingerprint(request))
-                                 body='test body',
+                                 body=b'test body',
-        assert any(h in request2.headers for h in ('If-None-Match', 'If-Modified-Since'))
+        assert not b'If-None-Match' in request1.headers
-            cch = r.headers.get('Cache-Control', '')
+            cch = r.headers.get(b'Cache-Control', b'')
-        if 'no-store' in cc:
+        if b'no-store' in cc:
-        if 'no-store' in cc:
+        if b'no-store' in cc:
-        elif 'max-age' in cc or 'Expires' in response.headers:
+        elif b'max-age' in cc or b'Expires' in response.headers:
-            return 'Last-Modified' in response.headers or 'ETag' in response.headers
+            return b'Last-Modified' in response.headers or b'ETag' in response.headers
-        if 'no-cache' in cc or 'no-cache' in ccreq:
+        if b'no-cache' in cc or b'no-cache' in ccreq:
-        if 'max-stale' in ccreq and 'must-revalidate' not in cc:
+        if b'max-stale' in ccreq and b'must-revalidate' not in cc:
-            staleage = ccreq['max-stale']
+            staleage = ccreq[b'max-stale']
-            if 'must-revalidate' not in cc:
+            if b'must-revalidate' not in cc:
-            request.headers['If-Modified-Since'] = cachedresponse.headers['Last-Modified']
+        if b'Last-Modified' in cachedresponse.headers:
-            request.headers['If-None-Match'] = cachedresponse.headers['ETag']
+        if b'ETag' in cachedresponse.headers:
-            return max(0, int(cc['max-age']))
+            return max(0, int(cc[b'max-age']))
-        date = rfc1123_to_epoch(response.headers.get('Date')) or now
+        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now
-            expires = rfc1123_to_epoch(response.headers['Expires'])
+        if b'Expires' in response.headers:
-        lastmodified = rfc1123_to_epoch(response.headers.get('Last-Modified'))
+        lastmodified = rfc1123_to_epoch(response.headers.get(b'Last-Modified'))
-        date = rfc1123_to_epoch(response.headers.get('Date')) or now
+        date = rfc1123_to_epoch(response.headers.get(b'Date')) or now
-        if 'Age' in response.headers:
+        if b'Age' in response.headers:
-                age = int(response.headers['Age'])
+                age = int(response.headers[b'Age'])
-    ...                                                'max-age': '3600'}
+    >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,
-    >>> parse_cachecontrol('') == {}
+    >>> parse_cachecontrol(b'') == {}
-        key, sep, val = directive.strip().partition('=')
+    for directive in header.split(b','):
-HTTPCACHE_DBM_MODULE = 'anydbm'
+HTTPCACHE_DBM_MODULE = 'anydbm' if six.PY2 else 'dbm'
-        body = '<p>You are being redirected</p>'
+        body = b'<p>You are being redirected</p>'
-        self.assertEqual(ret.url, resp.headers['Location'],
+        self.assertEqual(to_bytes(ret.url), resp.headers['Location'],
-        body = '<p>You are being redirected</p>'
+        body = b'<p>You are being redirected</p>'
-        if six.PY2 and twisted_version > (12, 3, 0):
+        if twisted_version > (12, 3, 0):
-            self.putChild("xpayload", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))
+            self.putChild(b"payload", PayloadResource())
-        if six.PY2 and twisted_version > (12, 3, 0):
+        if twisted_version > (12, 3, 0):
-            body = '1'*100 # PayloadResource requires body length to be 100
+            body = b'1'*100 # PayloadResource requires body length to be 100
-            request.headers.setdefault('Accept-Encoding', 'gzip,deflate')
+            request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')
-            self.assertTrue(reason, 'finished')
+            if six.PY2:
-            raise unittest.SkipTest("xpayload and payload endpoint only enabled for twisted > 12.3.0 and python 2.x")
+            raise unittest.SkipTest("xpayload and payload endpoint only enabled for twisted > 12.3.0")
-        request.setHeader("Content-Length", "1024")
+        request.setHeader(b"Content-Length", b"1024")
-        request.write("partial content\n")
+        request.write(b"partial content\n")
-        request.write("this connection will be dropped\n")
+        request.write(b"this connection will be dropped\n")
-        self.putChild("echo", Echo())
+        self.putChild(b"status", Status())
-        return 'Scrapy mock HTTP server\n'
+        return b'Scrapy mock HTTP server\n'
-            omitConnectTunnel = proxyParams.find('noconnect') >= 0
+            proxyHost = to_unicode(proxyHost)
-                             request.headers.get('Proxy-Authorization', None))
+                             request.headers.get(b'Proxy-Authorization', None))
-            headers.removeHeader('Proxy-Authorization')
+            headers.removeHeader(b'Proxy-Authorization')
-            self.assertEquals(response.body, 'http://example.com')
+            self.assertEquals(response.body, b'http://example.com')
-            self.assertEquals(response.body, 'https://example.com')
+            self.assertEquals(response.body, b'https://example.com')
-            self.assertEquals(response.body, '/path/to/resource')
+            self.assertEquals(response.body, b'/path/to/resource')
-        if factory.scheme == 'https':
+        host, port = to_unicode(factory.host), factory.port
-        d.addCallback(self.assertEquals, "0123456789")
+        d.addCallback(self.assertEquals, b"0123456789")
-        d.addCallback(self.assertEquals, "0123456789")
+        d.addCallback(self.assertEquals, b"0123456789")
-        d.addCallback(self.assertEquals, "0123456789")
+        d.addCallback(self.assertEquals, b"0123456789")
-            if  scheme == 'https' and not omitConnectTunnel:
+            if  scheme == b'https' and not omitConnectTunnel:
-            return txresponse, '', None
+            return txresponse, b'', None
-import twisted
+from scrapy.utils.python import to_bytes
-        d.addCallback(self.assertEquals, '')
+        d.addCallback(self.assertEquals, b'')
-            self.assertEquals(response.body, '127.0.0.1:%d' % self.portno)
+            self.assertEquals(response.body, to_bytes('127.0.0.1:%d' % self.portno))
-            self.assertEquals(request.headers.get('Host'), 'example.com')
+            self.assertEquals(response.body, b'example.com')
-        d.addCallback(self.assertEquals, 'example.com')
+        d.addCallback(self.assertEquals, b'example.com')
-        body = '1'*100 # PayloadResource requires body length to be 100
+        body = b'1'*100 # PayloadResource requires body length to be 100
-        method = request.method
+        url = to_bytes(urldefrag(request.url)[0])
-        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)
+        return respcls(
-        if isinstance(url, str):
+        if isinstance(url, six.string_types):
-            self.assertEquals(response.body, '0123456789')
+            self.assertEquals(response.body, b'0123456789')
-        FilePath(name).child("file").setContent("0123456789")
+        FilePath(name).child("file").setContent(b"0123456789")
-        r.putChild("broken", BrokenDownloadResource())
+        r.putChild(b"redirect", util.Redirect(b"/file"))
-        d.addCallback(self.assertEquals, "0123456789")
+        d.addCallback(self.assertEquals, b"0123456789")
-from zope.interface import implements
+from zope.interface import implementer
-from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler
+        from twisted.protocols.ftp import FTPRealm, FTPFactory
-        if self.factory.method.upper() == 'HEAD':
+        if self.factory.method.upper() == b'HEAD':
-        elif self.method == 'POST':
+        elif self.method == b'POST':
-        self.assertTrue(isinstance(path, str))
+        self.assertTrue(isinstance(scheme, bytes))
-        scheme, netloc, host, port, path = client._parse(url)
+        _, _, host, port, _ = client._parse(url)
-        reactor.connectTCP(host, port, factory)
+        reactor.connectTCP(to_unicode(host), port, factory)
-    host = b(parsed.hostname)  # FIXME
+    path = to_bytes(path)
-    netloc = b(parsed.netloc)  # FIXME - host + port
+    scheme = to_bytes(parsed.scheme, encoding='ascii')
-        self.url = to_bytes(self._url)  # FIXME
+        self.url = to_bytes(self._url)
-    host = parsed.hostname
+    path = to_bytes(path)  # FIXME
-    netloc = parsed.netloc
+    scheme = b(parsed.scheme)
-        port = 443 if scheme == 'https' else 80
+        port = 443 if scheme == b'https' else 80
-            to_bytes(self.factory.path))
+        self.sendCommand(self.factory.method, self.factory.path)
-        self.method = request.method
+        self._url = urldefrag(request.url)[0]
-        respcls = responsetypes.from_args(headers=headers, url=self.url)
+        respcls = responsetypes.from_args(headers=headers, url=self._url)
-        return respcls(url=self.url, status=status, headers=headers, body=body)
+        return respcls(url=self._url, status=status, headers=headers, body=body)
-        r.putChild(b"redirect", util.Redirect("/file"))
+        r.putChild(b"redirect", util.Redirect(b"/file"))
-            connected = self.wrapper.protocols.keys()
+            connected = list(six.iterkeys(self.wrapper.protocols))
-        self.assertEquals(factory.response_headers['content-length'], '10')
+        self.assertEquals(factory.status, b'200')
-        return getPage(self.getURL("payload"), body=s).addCallback(self.assertEquals, s)
+        return getPage(self.getURL("payload"), body=s).addCallback(
-            getPage(self.getURL("host"), headers={"Host": "www.example.com"}).addCallback(self.assertEquals, "www.example.com")])
+            getPage(self.getURL("host")).addCallback(
-        d.addCallback(self.assertEquals, "127.0.0.1:%d" % self.portno)
+        d.addCallback(
-        self.assert_('404 - No Such Resource' in pageData)
+        self.assert_(b'404 - No Such Resource' in pageData)
-                '<a href="/file">click here</a>\n    </body>\n</html>\n')
+                b'\n<html>\n    <head>\n        <meta http-equiv="refresh" content="0;URL=/file">\n'
-        r.putChild("broken", BrokenDownloadResource())
+        r.putChild(b"redirect", util.Redirect("/file"))
-            "some data")
+            b"GET /bar HTTP/1.0\r\n"
-            "\r\n")
+            b"GET /bar HTTP/1.0\r\n"
-            "name=value")
+            b"POST /bar HTTP/1.0\r\n"
-                   "\r\n")
+            b"POST /bar HTTP/1.0\r\n"
-            "\r\n")
+            b"GET /bar HTTP/1.0\r\n"
-            "\r\n")
+            b"GET /bar HTTP/1.0\r\n"
-        protocol.dataReceived("\n")
+        protocol.dataReceived(b"HTTP/1.0 200 OK\n")
-        goodInput = badInput.encode('ascii')
+        goodInput = u'http://example.com/path'
-            _getPage("HEAD").addCallback(self.assertEqual, "")])
+            _getPage("head").addCallback(self.assertEqual, b""),
-    def _clientfactory(*args, **kwargs):
+    def _clientfactory(url, *args, **kwargs):
-        f = client.ScrapyHTTPClientFactory(Request(*args, **kwargs), timeout=timeout)
+        f = client.ScrapyHTTPClientFactory(
-    return _makeGetterFactory(url, _clientfactory,
+    return _makeGetterFactory(to_bytes(url), _clientfactory,
-        FilePath(name).child("file").setContent("0123456789")
+        FilePath(name).child("file").setContent(b"0123456789")
-        d.addCallback(self.assertEquals, "0123456789")
+        d.addCallback(self.assertEquals, b"0123456789")
-    delimiter = '\n'
+    delimiter = b'\n'
-        self.sendCommand(self.factory.method, self.factory.path)
+        self.sendCommand(
-                          ([u'27'], [u'A'], [u'27'])])
+        for r in (
-        nodetext = etree.tostring(node, encoding='unicode')
+        nodetext = etree.tostring(node, encoding=six.text_type)
-        "obj must be Response or basestring, not %s" % type(obj).__name__
+    expected_types = (Response, six.text_type, six.binary_type)
-        body = """<?xml version="1.0" encoding="UTF-8"?>
+        body = u"""<?xml version="1.0" encoding="UTF-8"?>
-        response = XmlResponse(url="http://example.com", body=body)
+
-
+from io import StringIO
-    assert isinstance(obj, (Response, six.string_types, six.binary_type)), \
+    assert isinstance(obj, (Response, six.string_types, bytes)), \
-       given tha name of the node to iterate. Useful for parsing XML feeds.
+       given the name of the node to iterate. Useful for parsing XML feeds.
-    r = re.compile(r"<{0}[\s>].*?</{0}>".format(nodename_patt), re.DOTALL)
+    r = re.compile(r'<%(np)s[\s>].*?</%(np)s>' % {'np': nodename_patt}, re.DOTALL)
-        nodetext = etree.tostring(node)
+        nodetext = etree.tostring(node, encoding='unicode')
-    
+
-    assert isinstance(obj, (Response, six.string_types)), \
+    assert isinstance(obj, (Response, six.string_types, bytes)), \
-        
+
-        self._is_unicode = isinstance(self._text, unicode)
+        self._is_unicode = isinstance(self._text, six.text_type)
-    
+
-    lines = BytesIO(_body_or_str(obj, unicode=False))
+    # Python 3 csv reader input object needs to return strings
-    assert isinstance(obj, (Response, six.string_types)), \
+    assert isinstance(obj, (Response, six.string_types, six.binary_type)), \
-            self.xmliter(response, 'item').next().extract(),
+            next(self.xmliter(response, 'item')).extract(),
-            self.assert_(all((isinstance(v, unicode) for v in result_row.values())))
+            self.assert_(all((isinstance(k, six.text_type) for k in result_row.keys())))
-        body = get_testdata('feeds', 'feed-sample3.csv').replace(',', '\t')
+        body = get_testdata('feeds', 'feed-sample3.csv').replace(b',', b'\t')
-        
+        body2 = get_testdata('feeds', 'feed-sample6.csv').replace(b',', b'|')
-        body = get_testdata('feeds', 'feed-sample3.csv').replace(',', '\t')
+        body = get_testdata('feeds', 'feed-sample3.csv').replace(b',', b'\t')
-        headers, body = sample[0].split(','), '\n'.join(sample[1:])
+        headers, body = sample[0].split(b','), b'\n'.join(sample[1:])
-        csv = csviter(response, headers=headers)
+        csv = csviter(response, headers=[h.decode('utf-8') for h in headers])
-        body = '\n'.join((body, 'a,b', 'a,b,c,d'))
+        body = b'\n'.join((body, b'a,b', b'a,b,c,d'))
-             return self._retry(request, exception, spider)
+            return self._retry(request, exception, spider)
-        os.mknod(join(self.tmpl_proj, 'root_template'))
+        with open(join(self.tmpl_proj, 'root_template'), 'w'):
-                            'warnsize': self._warnsize})
+        if self._warnsize and self._bytes_received > self._warnsize and not self._reached_warnsize:
-            and getattr(v, '__module__', '').startswith('twisted'):
+    if not str(getattr(k, '__module__', '')).startswith('twisted') \
-            )
+            error_message = ("Cancelling download of {url}: expected response "
-                         {'size': expected_size, 'maxsize': maxsize})
+            error_message = (
-            raise defer.CancelledError()
+            raise defer.CancelledError(error_message)
-from twisted.internet import defer
+from twisted.internet import defer, task
-            slot.nextcall.schedule(5)
+        slot.heartbeat.start(5)
-from shutil import copytree, ignore_patterns
+from shutil import copytree, ignore_patterns, move
-        shutil.copy(join(TEMPLATES_PATH, 'scrapy.cfg'), project_name)
+        copytree(self.templates_dir, project_name, ignore=IGNORE)
-        print("New Scrapy project %r created in:" % project_name)
+        print("New Scrapy project %r, using template directory %r, created in:" % \
-from shutil import rmtree
+from shutil import rmtree, copytree
-                        '  and translate(., "RADIO", "radio") != "radio"))]]')
+                        ' not(re:test(., "^(?:submit|image|reset)$", "i"))'
-            '|descendant::button[not(@type)]')]
+            ' and re:test(@type, "^submit$", "i")]'
-        raise ValueError('No <form> element found with %s' % formxpath)
+        encoded = formxpath if six.PY3 else formxpath.encode('unicode_escape')
-                        '|descendant::input[@type['
+                        '|descendant::input[not(@type) or @type['
-        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': ['']})
+        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': [''], 'i4': ['i4v1']})
-                        'and ((@type!="checkbox" and @type!="radio") or @checked)]')
+                        '|descendant::input[@type['
-                                          '|descendant::button[not(@type)]')]
+    clickables = [
-        return value
+        if opt_name not in self:
-        return prio
+        if name not in self:
-                self.priority = priority
+        if priority >= self.priority:
-import shutil
+import os
-import tempfile
+import shutil
-                                     '100, "tests.test_cmdline.extensions.DummyExtension": 200}'))
+                                    'EXTENSIONS=' + json.dumps(EXTENSIONS))
-        self.assertIn('value=100', settingsdict['tests.test_cmdline.extensions.TestExtension'])
+        six.assertCountEqual(self, settingsdict.keys(), EXTENSIONS.keys())
-            BaseSettings({'one': 10, 'two': 20}, 0), 0)
+    def test_overwrite_basesettings(self):
-        new_dict = {'one': 11, 'two': 21}
+        new_dict = {'three': 11, 'four': 21}
-        self.assertEqual(attribute.value['two'], 21)
+        self.assertIsInstance(attribute.value, BaseSettings)
-        contracts = build_component_list(self.settings._getcomposite('SPIDER_CONTRACTS'))
+        contracts = build_component_list(self.settings.getwithbase('SPIDER_CONTRACTS'))
-            feed_exporters = without_none_values(self.settings._getcomposite('FEED_EXPORTERS'))
+            feed_exporters = without_none_values(
-            feed_exporters = without_none_values(self.settings._getcomposite('FEED_EXPORTERS'))
+            feed_exporters = without_none_values(self.settings.getwithbase('FEED_EXPORTERS'))
-        handlers = without_none_values(crawler.settings._getcomposite('DOWNLOAD_HANDLERS'))
+        handlers = without_none_values(
-        return build_component_list(settings._getcomposite('DOWNLOADER_MIDDLEWARES'))
+        return build_component_list(
-        return build_component_list(settings._getcomposite('SPIDER_MIDDLEWARES'))
+        return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))
-        return build_component_list(settings._getcomposite('EXTENSIONS'))
+        return build_component_list(settings.getwithbase('EXTENSIONS'))
-        conf = without_none_values(self.settings._getcomposite(setting_prefix))
+        conf = without_none_values(self.settings.getwithbase(setting_prefix))
-        return build_component_list(settings._getcomposite('ITEM_PIPELINES'))
+        return build_component_list(settings.getwithbase('ITEM_PIPELINES'))
-        return self[name]
+    def getwithbase(self, name):
-        s = BaseSettings({'TEST_BASE': {1: 1, 2: 2},
+    def test_getwithbase(self):
-        self.assertIsNone(cs)
+                          'HASNOBASE': BaseSettings({3: 3000}, 'default')})
-def build_component_list(compdict, convert=update_classpath):
+def build_component_list(compdict, custom=None, convert=update_classpath):
-        return type(compdict)(convert(c) for c in compdict)
+    # BEGIN Backwards compatibility for old (base, custom) call signature
-        self.assertEqual(build_component_list(d, lambda x: x),
+        self.assertEqual(build_component_list(d, convert=lambda x: x),
-        self.assertEqual(build_component_list(custom, lambda x: x), custom)
+        self.assertEqual(build_component_list(None, custom,
-        self.assertEqual(build_component_list(custom, lambda x: x.upper()),
+        self.assertEqual(build_component_list({}, custom,
-        self.assertEqual(build_component_list(custom, lambda x: x.upper()),
+        self.assertEqual(build_component_list(None, custom,
-                          build_component_list, duplicate_dict, lambda x: x.lower())
+        self.assertRaises(ValueError, build_component_list, {}, duplicate_dict,
-                          build_component_list, duplicate_list, lambda x: x)
+        self.assertRaises(ValueError, build_component_list, None,
-        self.assertEqual(build_component_list(duplicate_bs, convert=lambda x: x.lower()),
+        self.assertEqual(build_component_list(duplicate_bs,
-        self.assertEqual(build_component_list(duplicate_bs, convert=lambda x: x.lower()),
+        self.assertEqual(build_component_list(duplicate_bs,
-                          build_component_list, duplicate_bs, convert=lambda x: x.lower())
+        self.assertRaises(ValueError, build_component_list, duplicate_bs,
-DOWNLOAD_HANDLERS = {
+DOWNLOAD_HANDLERS = {}
-DOWNLOADER_MIDDLEWARES = {
+DOWNLOADER_MIDDLEWARES = {}
-EXTENSIONS = {
+EXTENSIONS = {}
-FEED_STORAGES = {
+FEED_STORAGES = {}
-FEED_EXPORTERS = {
+FEED_EXPORTERS = {}
-SPIDER_MIDDLEWARES = {
+SPIDER_MIDDLEWARES = {}
-SPIDER_CONTRACTS = {
+SPIDER_CONTRACTS = {}
-            parts = urlparse.urlsplit(url)
+            parts = urlparse(url)
-from urlparse import urlsplit, urlunsplit
+import re
-    ))
+    match = re.match(r"^\w+://", url, flags=re.I)
-            url = any_to_uri(url)
+            parts = urlparse.urlsplit(url)
-    return url
+    parts = urlsplit(url)
-            url = add_http_if_no_scheme(url)
+            url = any_to_uri(url)
-    runner = CrawlerRunner(Settings(settings_dict))
+    runner = CrawlerRunner(settings_dict)
-    return runner._create_crawler(spidercls or Spider)
+    return runner.create_crawler(spidercls or Spider)
-from scrapy.utils.test import get_crawler
+from scrapy.http import Request
-from scrapy.http import Request
+        self.runner = CrawlerRunner()
-        crawler = get_crawler(FollowAllSpider)
+        crawler = self.runner.create_crawler(FollowAllSpider)
-        crawler = get_crawler(FollowAllSpider, settings)
+        crawler = CrawlerRunner(settings).create_crawler(FollowAllSpider)
-        crawler = get_crawler(DelaySpider)
+        crawler = self.runner.create_crawler(DelaySpider)
-        crawler = get_crawler(DelaySpider, {"DOWNLOAD_TIMEOUT": 0.35})
+        crawler = CrawlerRunner({"DOWNLOAD_TIMEOUT": 0.35}).create_crawler(DelaySpider)
-        crawler = get_crawler(SimpleSpider)
+        crawler = self.runner.create_crawler(SimpleSpider)
-        crawler = get_crawler(SimpleSpider)
+        crawler = self.runner.create_crawler(SimpleSpider)
-            crawler = get_crawler(SimpleSpider)
+            crawler = self.runner.create_crawler(SimpleSpider)
-            crawler = get_crawler(BrokenStartRequestsSpider)
+            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)
-            crawler = get_crawler(BrokenStartRequestsSpider)
+            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)
-        crawler = get_crawler(BrokenStartRequestsSpider, settings)
+        crawler = CrawlerRunner(settings).create_crawler(BrokenStartRequestsSpider)
-        crawler = get_crawler(DuplicateStartRequestsSpider, settings)
+        crawler = CrawlerRunner(settings).create_crawler(DuplicateStartRequestsSpider)
-        crawler = get_crawler(SimpleSpider)
+        crawler = self.runner.create_crawler(SimpleSpider)
-        crawler = get_crawler(SimpleSpider)
+        crawler = self.runner.create_crawler(SimpleSpider)
-        crawler = get_crawler(SimpleSpider)
+        crawler = self.runner.create_crawler(SimpleSpider)
-        crawler = get_crawler(SingleRequestSpider)
+        crawler = self.runner.create_crawler(SingleRequestSpider)
-        crawler = get_crawler(SingleRequestSpider)
+        crawler = self.runner.create_crawler(SingleRequestSpider)
-        crawler = get_crawler(FaultySpider)
+        crawler = self.runner.create_crawler(FaultySpider)
-            crawler = self._create_crawler(crawler_or_spidercls)
+        crawler = self.create_crawler(crawler_or_spidercls)
-                        0)
+            BaseSettings({'one': 10, 'two': 20}, 0), 0)
-        myattr = SettingsAttribute(0, 30) # Note priority 30
+        myattr = SettingsAttribute(0, 30)  # Note priority 30
-        custom_settings = BaseSettings({'key_lowprio': 1, 'key_highprio': 11}, priority=30)
+        custom_settings = BaseSettings({'key_lowprio': 1, 'key_highprio': 11},
-                             CrawlerSettings)
+                             CrawlerSettings, SETTINGS_PRIORITIES,
-
+    def test_repr(self):
-            raise
+            if self.engine is not None:
-from w3lib.html import remove_tags, replace_entities, replace_escape_chars
+from w3lib.html import remove_tags, replace_entities, replace_escape_chars, get_base_url
-            base_url = urljoin(response_url, self.base_url) if self.base_url else response_url
+            base_url = get_base_url(response_text, response_url, response_encoding)
-        return form.action or form.base_url
+        return urljoin(form.base_url, form.action)
-    root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)
+    root = create_root_node(text, lxml.html.HTMLParser, base_url=get_base_url(response))
-        
+
-        ``False`` and ``None`` return ``False``. 
+        ``False`` and ``None`` return ``False``.
-            compsett.update(self[name])
+            # When users defined a _BASE setting, they explicitly don't want to
-            return self[name]
+        return self[name]
-                          'HASNOBASE': BaseSettings({1: 1})})
+                          'TEST': BaseSettings({1: 10, 3: 30}, 'default'),
-        self.assertEqual(cs[1], 10)
+        self.assertEqual(len(cs), 3)
-from scrapy.utils.conf import arglist_to_dict, remove_none_values
+from scrapy.utils.conf import arglist_to_dict
-            feed_exporters = remove_none_values(self.settings._getcomposite('FEED_EXPORTERS'))
+            feed_exporters = without_none_values(self.settings._getcomposite('FEED_EXPORTERS'))
-from scrapy.utils.conf import arglist_to_dict, remove_none_values
+from scrapy.utils.conf import arglist_to_dict
-            feed_exporters = remove_none_values(self.settings._getcomposite('FEED_EXPORTERS'))
+            feed_exporters = without_none_values(self.settings._getcomposite('FEED_EXPORTERS'))
-from scrapy.utils.conf import remove_none_values
+from scrapy.utils.python import without_none_values
-        handlers = remove_none_values(crawler.settings._getcomposite('DOWNLOAD_HANDLERS'))
+        handlers = without_none_values(crawler.settings._getcomposite('DOWNLOAD_HANDLERS'))
-from scrapy.utils.conf import remove_none_values
+from scrapy.utils.python import without_none_values
-        headers = remove_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])
+        headers = without_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])
-from scrapy.utils.conf import remove_none_values
+from scrapy.utils.python import without_none_values
-        conf = remove_none_values(self.settings._getcomposite(setting_prefix))
+        conf = without_none_values(self.settings._getcomposite(setting_prefix))
-import warnings
+from scrapy.utils.python import without_none_values
-    compdict = remove_none_values(_map_keys(compdict))
+    compdict = without_none_values(_map_keys(compdict))
-
+
-                               remove_none_values)
+from scrapy.utils.conf import build_component_list, arglist_to_dict
-    WeakKeyCache, stringify_dict, get_func_args, to_bytes, to_unicode)
+    WeakKeyCache, stringify_dict, get_func_args, to_bytes, to_unicode,
-    __repr__ = __str__
+    def __repr__(self):
-        # will be removed along with _BASE support in a future release
+        # It's for internal use in the transition away from the _BASE settings
-from collections import Mapping, MutableMapping
+from collections import MutableMapping
-        False is: 0, '0', False, None
+        Get a setting value as a boolean.
-            val = self[name]
+        for name, val in six.iteritems(self):
-        )
+        contracts = build_component_list(self.settings._getcomposite('SPIDER_CONTRACTS'))
-from scrapy.utils.conf import arglist_to_dict
+from scrapy.utils.conf import arglist_to_dict, remove_none_values
-            )
+            feed_exporters = remove_none_values(self.settings._getcomposite('FEED_EXPORTERS'))
-from scrapy.utils.conf import arglist_to_dict
+from scrapy.utils.conf import arglist_to_dict, remove_none_values
-            )
+            feed_exporters = remove_none_values(self.settings._getcomposite('FEED_EXPORTERS'))
-        handlers.update(crawler.settings.get('DOWNLOAD_HANDLERS', {}))
+        handlers = remove_none_values(crawler.settings._getcomposite('DOWNLOAD_HANDLERS'))
-            settings['DOWNLOADER_MIDDLEWARES'])
+        return build_component_list(settings._getcomposite('DOWNLOADER_MIDDLEWARES'))
-            settings['SPIDER_MIDDLEWARES'])
+        return build_component_list(settings._getcomposite('SPIDER_MIDDLEWARES'))
-        return cls(crawler.settings.get('DEFAULT_REQUEST_HEADERS').items())
+        headers = remove_none_values(crawler.settings['DEFAULT_REQUEST_HEADERS'])
-            settings['EXTENSIONS'])
+        return build_component_list(settings._getcomposite('EXTENSIONS'))
-        conf.update(self.settings[setting_prefix])
+        conf = remove_none_values(self.settings._getcomposite(setting_prefix))
-        return build_component_list(settings['ITEM_PIPELINES_BASE'], item_pipelines)
+        return build_component_list(settings._getcomposite('ITEM_PIPELINES'))
-        self.priority = priority
+        if isinstance(self.value, BaseSettings):
-            self.priority = priority
+        if isinstance(self.value, BaseSettings):
-        Settings.__init__(self, **kw)
+        Settings.__init__(self, **kw)
-DOWNLOAD_HANDLERS_BASE = {
+DOWNLOAD_HANDLERS = {
-DOWNLOADER_MIDDLEWARES_BASE = {
+DOWNLOADER_MIDDLEWARES = {
-EXTENSIONS_BASE = {
+EXTENSIONS = {
-FEED_STORAGES_BASE = {
+FEED_STORAGES = {
-FEED_EXPORTERS_BASE = {
+FEED_EXPORTERS = {
-SPIDER_MIDDLEWARES_BASE = {
+SPIDER_MIDDLEWARES = {
-SPIDER_CONTRACTS_BASE = {
+SPIDER_CONTRACTS = {
-    """
+def build_component_list(compdict, convert=update_classpath):
-
+        if isinstance(compdict, BaseSettings):
-        return {convert(k): v for k, v in six.iteritems(compdict)}
+        return type(compdict)(convert(c) for c in compdict)
-    return [x[0] for x in sorted(items, key=itemgetter(1))]
+def remove_none_values(compdict):
-class SettingsTest(unittest.TestCase):
+        new_dict = {'one': 11, 'two': 21}
-        assertItemsEqual = unittest.TestCase.assertCountEqual
+        new_settings = BaseSettings()
-from scrapy.utils.conf import build_component_list, arglist_to_dict
+from scrapy.settings import BaseSettings
-                         ['one', 'four', 'five', 'three'])
+        d = {'one': 1, 'two': None, 'three': 8, 'four': 4}
-                         custom)
+        self.assertEqual(build_component_list(custom, lambda x: x), custom)
-        self.assertEqual(build_component_list({}, custom, lambda x: x.upper()),
+        self.assertEqual(build_component_list(custom, lambda x: x.upper()),
-        self.assertEqual(build_component_list(None, custom, lambda x: x.upper()),
+        self.assertEqual(build_component_list(custom, lambda x: x.upper()),
-                          build_component_list, {}, duplicate_dict, lambda x: x.lower())
+                          build_component_list, duplicate_dict, lambda x: x.lower())
-                          build_component_list, None, duplicate_list, lambda x: x)
+                          build_component_list, duplicate_list, lambda x: x)
-from collections import MutableMapping
+from collections import Mapping, MutableMapping
-class Settings(object):
+class BaseSettings(MutableMapping):
-            self.setdict(values, priority)
+        self.update(values, priority)
-        if opt_name in self.attributes:
+        if opt_name in self:
-            self.attributes[name] = SettingsAttribute(value, priority)
+        priority = get_settings_priority(priority)
-            self.set(name, value, priority)
+        self.update(values, priority)
-from scrapy.settings import Settings, SettingsAttribute, CrawlerSettings
+from scrapy.settings import (BaseSettings, Settings, SettingsAttribute,
-        self.assertEqual(attr.priority, 10)
+class BaseSettingsTest(unittest.TestCase):
-        self.assertIn('TEST_OPTION', settings.attributes)
+    if six.PY3:
-        self.assertEqual(attr.priority, 10)
+    def setUp(self):
-        self.settings.attributes = {}
+    def test_set_settingsattribute(self):
-                mock.patch.object(SettingsAttribute, 'set') as mock_set:
+        attr = SettingsAttribute('value', 10)
-            mock_setattr.reset_mock()
+    def test_setitem(self):
-            mock_set.assert_called_with('TEST_DEFAULT', 'defvalue', 10)
+            mock_set.assert_any_call('TEST_DEFAULT', 'defvalue', 10)
-                                   six.itervalues(ctrl_attributes)):
+        for key in six.iterkeys(ctrl_attributes):
-                print(open(template_file, 'r').read())
+                with open(template_file, "r") as f:
-        parser.add_option("--spider", dest="spider", default=None, \
+        parser.add_option("--spider", dest="spider", default=None,
-        parser.add_option("-a", dest="spargs", action="append", default=[], metavar="NAME=VALUE", \
+        parser.add_option("-a", dest="spargs", action="append", default=[], metavar="NAME=VALUE",
-        parser.add_option("--pipelines", action="store_true", \
+        parser.add_option("--pipelines", action="store_true",
-        parser.add_option("--nolinks", dest="nolinks", action="store_true", \
+        parser.add_option("--nolinks", dest="nolinks", action="store_true",
-        parser.add_option("--noitems", dest="noitems", action="store_true", \
+        parser.add_option("--noitems", dest="noitems", action="store_true",
-        parser.add_option("--nocolour", dest="nocolour", action="store_true", \
+        parser.add_option("--nocolour", dest="nocolour", action="store_true",
-        parser.add_option("-r", "--rules", dest="rules", action="store_true", \
+        parser.add_option("-r", "--rules", dest="rules", action="store_true",
-        parser.add_option("-c", "--callback", dest="callback", \
+        parser.add_option("-c", "--callback", dest="callback",
-        parser.add_option("-d", "--depth", dest="depth", type="int", default=1, \
+        parser.add_option("-d", "--depth", dest="depth", type="int", default=1,
-        parser.add_option("-v", "--verbose", dest="verbose", action="store_true", \
+        parser.add_option("-v", "--verbose", dest="verbose", action="store_true",
-        else: return 0
+        levels = list(self.items.keys()) + list(self.requests.keys())
-            levels = self.requests.keys()
+            levels = list(self.requests.keys())
-            for level in xrange(1, self.max_level+1):
+            for level in range(1, self.max_level+1):
-
+
-        raw = file.read()
+    with open(path, 'rb') as fp:
-        file.write(content)
+    with open(render_path, 'wb') as fp:
-        out = retry_on_eintr(p.stdout.read)
+        out = to_native_str(retry_on_eintr(p.stdout.read))
-        out = retry_on_eintr(p.stdout.read)
+        out = to_native_str(retry_on_eintr(p.stdout.read))
-        log = p.stderr.read()
+        log = to_native_str(p.stderr.read())
-        log = p.stderr.read()
+        log = to_native_str(p.stderr.read())
-        log = p.stderr.read()
+        log = to_native_str(p.stderr.read())
-        log = p.stderr.read()
+        log = to_native_str(p.stderr.read())
-        self.assertIn("DEBUG: It Works!", stderr)
+        self.assertIn("DEBUG: It Works!", to_native_str(stderr))
-        self.assertIn("INFO: It Works!", stderr)
+        self.assertIn("INFO: It Works!", to_native_str(stderr))
-        self.assertIn("""[{}, {'foo': 'bar'}]""", out)
+        self.assertIn("""[{}, {'foo': 'bar'}]""", to_native_str(out))
-        log = p.stderr.read()
+        log = to_native_str(p.stderr.read())
-    HEADER_END_RE = re.compile(r'<\s*/%s\s*>' % nodename, re.S)
+    nodename_patt = re.escape(nodename)
-    r = re.compile(r"<%s[\s>].*?</%s>" % (nodename, nodename), re.DOTALL)
+    r = re.compile(r"<{0}[\s>].*?</{0}>".format(nodename_patt), re.DOTALL)
-    'scrapy.telnet.TelnetConsole': 0,
+    'scrapy.extensions.telnet.TelnetConsole': 0,
-Scrapy Telnet Console extension
+import warnings
-        return telnet_vars
+from scrapy.extensions.telnet import *
-
+class AddHttpIfNoScheme(unittest.TestCase):
-    parser = parse_url(url)
+    """Add http as the default scheme if it is missing from the url."""
-    elif not parser.scheme or not parser.netloc:
+        return url
-from scrapy.utils.url import add_scheme_if_missing
+from scrapy.utils.url import add_http_if_no_scheme
-            url = add_scheme_if_missing(url)
+            url = add_http_if_no_scheme(url)
-def add_scheme_if_missing(url):
+def add_http_if_no_scheme(url):
-    return parser.geturl() 
+    if url.startswith('//'):
-                              canonicalize_url, add_scheme_if_missing)
+                              canonicalize_url, add_http_if_no_scheme)
-        self.assertEqual(add_scheme_if_missing('http://www.example.com'),
+    def test_add_http_if_no_scheme(self):
-        self.assertEqual(add_scheme_if_missing('http://www.example.com/some/page.html'),
+        self.assertEqual(add_http_if_no_scheme('http://www.example.com/some/page.html'),
-        self.assertEqual(add_scheme_if_missing('http://example.com'),
+        self.assertEqual(add_http_if_no_scheme('http://example.com'),
-        self.assertEqual(add_scheme_if_missing('www.example.com'),
+        self.assertEqual(add_http_if_no_scheme('www.example.com'),
-        self.assertEqual(add_scheme_if_missing('example.com'),
+        self.assertEqual(add_http_if_no_scheme('example.com'),
-        self.assertEqual(add_scheme_if_missing('//example.com'),
+        self.assertEqual(add_http_if_no_scheme('//example.com'),
-        self.assertEqual(add_scheme_if_missing('https://www.example.com'),
+        self.assertEqual(add_http_if_no_scheme('//www.example.com/some/page.html'),
-                              canonicalize_url)
+                              canonicalize_url, add_scheme_if_missing)
-__all__ = ['__version__', 'version_info', 'optional_features', 'twisted_version',
+__all__ = ['__version__', 'version_info', 'twisted_version',
-from scrapy import optional_features
+from scrapy import twisted_version
-if 'http11' in optional_features:
+if twisted_version >= (11, 1, 0):
-    if 'http11' not in optional_features:
+    if twisted_version < (11, 1, 0):
-    if 'http11' not in optional_features:
+    if twisted_version < (11, 1, 0):
-    if 'http11' not in optional_features:
+    if twisted_version < (11, 1, 0):
-from scrapy import optional_features
+from scrapy import twisted_version
-        if 'http11' in optional_features:
+        if twisted_version >= (11, 1, 0): # http11 available
-                .format(interval, url)
+        html = u"""<html><head><meta http-equiv="refresh" content="{0};url={1}"/></head></html>"""
-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
+        rsp = HtmlResponse(req.url, body=self._body())
-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
+        rsp = HtmlResponse(req.url, body=self._body())
-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
+        rsp = HtmlResponse(req.url, body=self._body())
-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
+        rsp = HtmlResponse(req.url, body=self._body())
-        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
+        rsp = HtmlResponse(req.url, body=self._body())
-        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'), encoding='utf-8')
+        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'))
-        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'), encoding='utf-8')
+        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'))
-            location = to_native_str(response.headers['location'].decode('latin1'))
+        allowed_status = (301, 302, 303, 307)
-            redirected_url = urljoin(request.url, location)
+        # HTTP header is ascii or latin1, redirected url will be percent-encoded utf-8
-                return self._redirect(redirected, request, spider, response.status)
+        redirected_url = urljoin(request.url, location)
-                return self._redirect(redirected, request, spider, response.status)
+        if response.status in (301, 307) or request.method == 'HEAD':
-        return response
+        redirected = self._redirect_request_using_get(request, redirected_url)
-            location = response.headers['location'].decode('latin1')
+            # HTTP header is ascii or latin1, redirected url will be percent-encoded utf-8
-        resp = Response('http://scrapytest.org/first', headers={'Location': latin1_path}, status=302)
+        latin1_location = u'/ao'.encode('latin1')  # HTTP historically supports latin1
-            location = response.headers['location']
+            location = response.headers['location'].decode('latin1')
-        rsp = HtmlResponse(req.url, body=self._body())
+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
-        rsp = HtmlResponse(req.url, body=self._body())
+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
-        rsp = HtmlResponse(url='http://example.org', body=self._body(interval=1000))
+        rsp = HtmlResponse(url='http://example.org',
-        rsp = HtmlResponse(req.url, body=self._body())
+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
-        rsp = HtmlResponse(req.url, body=self._body())
+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
-        rsp = HtmlResponse(req.url, body=self._body())
+        rsp = HtmlResponse(req.url, body=self._body(), encoding='utf-8')
-        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'))
+        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'), encoding='utf-8')
-        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'))
+        rsp2 = HtmlResponse(req2.url, body=self._body(url='/redirected2'), encoding='utf-8')
-               request.meta.get('handle_httpstatus_all', False)):
+                response.status in getattr(spider, 'handle_httpstatus_list', []) or
-                redirected_url = urljoin(request.url, response.headers['location'])
+        location = None
-            return self._redirect(redirected, request, spider, response.status)
+
-import logging
+import logging
-        logger.info("Enabled %(componentname)ss: %(enabledlist)s",
+        logger.info("Enabled %(componentname)ss:\n%(enabledlist)s",
-                     'enabledlist': ', '.join(enabled)},
+                     'enabledlist': pprint.pformat(mwlist)},
-        return MarshalFifoDiskQueue(self.qdir, chunksize=self.chunksize)
+        return MarshalFifoDiskQueue(self.qpath, chunksize=self.chunksize)
-        return PickleFifoDiskQueue(self.qdir, chunksize=self.chunksize)
+        return PickleFifoDiskQueue(self.qpath, chunksize=self.chunksize)
-        return MarshalLifoDiskQueue(self.path)
+        return MarshalLifoDiskQueue(self.qpath)
-        return PickleLifoDiskQueue(self.path)
+        return PickleLifoDiskQueue(self.qpath)
-        conman = ContractsManager([load_object(c) for c in contracts])
+        conman = ContractsManager(load_object(c) for c in contracts)
-                for s in module.split('_')])
+            'classname': '%sSpider' % ''.join(s.capitalize() \
-        return defer.DeferredList([c.stop() for c in list(self.crawlers)])
+        return defer.DeferredList([c.stop() for c in self.crawlers])
-            value = flatten([extract_regex(regex, x) for x in value])
+            value = flatten(extract_regex(regex, x) for x in value)
-        return flatten([self.selector.xpath(xpath).extract() for xpath in xpaths])
+        return flatten(self.selector.xpath(xpath).extract() for xpath in xpaths)
-        return flatten([self.selector.css(css).extract() for css in csss])
+        return flatten(self.selector.css(css).extract() for css in csss)
-        return "\n".join(["[s] %s" % l for l in b])
+        return "\n".join("[s] %s" % l for l in b)
-    return defer.DeferredList([coop.coiterate(work) for i in range(count)])
+    return defer.DeferredList([coop.coiterate(work) for _ in range(count)])
-                                 for h in sorted(include_headers)])
+        include_headers = tuple(to_bytes(h.lower())
-            selector = self.selector.css(css)
+    def nested_xpath(self, xpath, **context):
-            item=self.item, selector=selector, parent=self
+            item=self.item, parent=self, **context
-        nl = l.nested_loader(xpath="//header")
+        nl = l.nested_xpath("//header")
-        nl = l.nested_loader(css="header")
+        nl = l.nested_css("header")
-        nl2 = nl1.nested_loader(xpath='a')
+        nl1 = l.nested_xpath('//footer')
-        nl2 = nl1.nested_loader(xpath='a')
+        nl1 = l.nested_xpath('//footer')
-        nl2 = nl1.nested_loader(xpath='img')
+        nl1 = l.nested_xpath('//footer')
-
+    def test_nested_bad_arguments(self):
-    def __init__(self, item=None, selector=None, response=None, **context):
+    def __init__(self, item=None, selector=None, response=None, parent=None, **context):
-        self._values = defaultdict(list)
+        self.parent = parent
-
+class TestNestedItem(Item):
-from scrapy.utils.python import to_bytes
+from scrapy.utils.python import to_bytes, to_native_str
-    return '%s %s' % (status, http.responses.get(int(status)))
+    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))
-        to_bytes(RESPONSES.get(response.status, b'')) + b"\r\n"
+        to_bytes(http.RESPONSES.get(response.status, b'')) + b"\r\n"
-            if not getattr(obj1, attr) == getattr(obj2, attr):
+            if attr(obj1) != attr(obj2):
-            for el_ in flatten(el):
+            for el_ in iflatten(el):
-                return Response(request.url)
+                return resp
-        self.assertIsInstance(results[0], Response)
+        self.assertIs(results[0], resp)
-from scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware
+from scrapy.downloadermiddlewares.robotstxt import (RobotsTxtMiddleware,
-        middleware._logerror = mock.MagicMock(side_effect=lambda fail, req, spider: fail)
+        middleware._logerror = mock.MagicMock(side_effect=middleware._logerror)
-        if rp and not rp.can_fetch(self._useragent, request.url):
+        d = maybeDeferred(self.robot_parser, request, spider)
-            raise IgnoreRequest
+            raise IgnoreRequest()
-            self._parsers[netloc] = None
+            self._parsers[netloc] = Deferred()
-            dfd.addCallback(self._parse_robots)
+            dfd.addCallback(self._parse_robots, netloc)
-        return self._parsers[netloc]
+            dfd.addErrback(self._robots_error, netloc)
-    def _parse_robots(self, response):
+    def _parse_robots(self, response, netloc):
-        self._parsers[urlparse_cached(response).netloc] = rp
+
-from twisted.internet.defer import Deferred
+from twisted.internet.defer import Deferred, DeferredList, maybeDeferred
-            self.assertIgnored(Request('http://site.local/admin/main'), middleware)
+        return DeferredList([
-        return deferred
+        ], fireOnOneErrback=True)
-            self.assertNotIgnored(Request('http://site.local/admin/main', meta=meta), middleware)
+        return DeferredList([
-        return deferred
+        ], fireOnOneErrback=True)
-            self.assertNotIgnored(Request('http://site.local/admin/main'), middleware)
+        deferred = DeferredList([
-        reactor.callFromThread(deferred.callback, None)
+        ], fireOnOneErrback=True)
-            self.assertNotIgnored(Request('http://site.local/admin/main'), middleware)
+        return DeferredList([
-        return deferred
+        ], fireOnOneErrback=True)
-        reactor.callFromThread(deferred.callback, None)
+        middleware._logerror = mock.MagicMock(side_effect=lambda fail, req, spider: fail)
-        self.assertIsNone(middleware.process_request(request, spider))
+        dfd = maybeDeferred(middleware.process_request, request, spider)
-        self.assertRaises(IgnoreRequest, middleware.process_request, request, spider)
+        return self.assertFailure(maybeDeferred(middleware.process_request, request, spider),
-                response = method(request=request, spider=spider)
+                response = yield method(request=request, spider=spider)
-            return download_func(request=request, spider=spider)
+                    defer.returnValue(response)
-                return response
+                defer.returnValue(response)
-                response = method(request=request, response=response, spider=spider)
+                response = yield method(request=request, response=response,
-            return response
+                    defer.returnValue(response)
-                response = method(request=request, exception=exception, spider=spider)
+                response = yield method(request=request, exception=exception,
-            return _failure
+                    defer.returnValue(response)
-        return (el.get('name'), el.get('value'))
+        return (el.get('name'), el.get('value') or '')
-            return (el.get('name'), el.get('value'))
+            return (el.get('name'), el.get('value') or '')
-        return (el[0].get('name'), el[0].get('value'))
+        return (el[0].get('name'), el[0].get('value') or '')
-            request.headers.setdefault('User-Agent', self.user_agent)
+            request.headers.setdefault(b'User-Agent', self.user_agent)
-        self.assertEquals(req.headers['User-Agent'], 'default_useragent')
+        self.assertEquals(req.headers['User-Agent'], b'default_useragent')
-        self.assertEquals(req.headers['User-Agent'], 'spider_useragent')
+        self.assertEquals(req.headers['User-Agent'], b'spider_useragent')
-        req = Request('http://scrapytest.org/', headers={'User-Agent': 'header_useragent'})
+        req = Request('http://scrapytest.org/',
-        self.assertEquals(req.headers['User-Agent'], 'header_useragent')
+        self.assertEquals(req.headers['User-Agent'], b'header_useragent')
-            spider=self.spider), 1)
+        self.assertStatsEqual('downloader/request_count', 1)
-            spider=self.spider), 1)
+        self.assertStatsEqual('downloader/response_count', 1)
-            spider=self.spider), 1)
+        self.mw.process_exception(self.req, MyException(), self.spider)
-            request.headers['Authorization'] = auth
+        if auth and b'Authorization' not in request.headers:
-        self.assertEquals(req.headers['Authorization'], 'Basic Zm9vOmJhcg==')
+        self.assertEquals(req.headers['Authorization'], b'Basic Zm9vOmJhcg==')
-        req = Request('http://scrapytest.org/', headers=dict(Authorization='Digest 123'))
+        req = Request('http://scrapytest.org/',
-    unittest.main()
+        self.assertEquals(req.headers['Authorization'], b'Digest 123')
-import six
+from scrapy.utils.python import to_bytes
-            six.iteritems(crawler.settings.get('DEFAULT_REQUEST_HEADERS'))])
+        defaults = {
-        self.assertEquals(req.headers, headers)
+        self.assertEquals(req.headers, bytes_headers)
-        defaults.update(headers)
+        defaults.update(bytes_headers)
-        return '<html><head><meta name="fragment" content="!"/></head><body></body></html>'
+        return b'<html><head><meta name="fragment" content="!"/></head><body></body></html>'
-        req, resp = self._req_resp('http://example.com/', {}, {'body': '<html></html>'})
+        req, resp = self._req_resp('http://example.com/', {}, {'body': b'<html></html>'})
-    clickables = [el for el in form.xpath('.//input[@type="submit"]')]
+    clickables = [el for el in form.xpath('descendant::input[@type="submit"]'
-        return (el.name, el.value)
+        return (el.get('name'), el.get('value'))
-            return (el.name, el.value)
+            return (el.get('name'), el.get('value'))
-        return (el[0].name, el[0].value)
+        return (el[0].get('name'), el[0].get('value'))
-
+        def test_extract_all_links(self):
-
+        def test_extract_filter_allow_and_deny(self):
-from scrapy.utils.python import to_native_str
+import warnings
-            url = to_native_str(url)
+            if six.PY2:
-        l3 = Link(b"http://www.example.com")
+        l1 = Link("http://www.example.com")
-        l6 = Link(b"http://www.example.com", text="test")
+        l4 = Link("http://www.example.com", text="test")
-        l10 = Link(b"http://www.example.com", text="test", fragment='other', nofollow=False)
+        l7 = Link("http://www.example.com", text="test", fragment='something', nofollow=False)
-        l1 = Link(b"http://www.example.com", text="test", fragment='something', nofollow=True)
+        l1 = Link("http://www.example.com", text="test", fragment='something', nofollow=True)
-            if six.PY2:
+    def test_non_str_url_py2(self):
-        assert len(w) == 1, "warning not issued"
+            assert len(w) == 1, "warning not issued"
-                                 extra={'spider': spider})
+                    logger.debug(
-                    self.stats.max_value('request_depth_max', depth, spider=spider)
+                        self.stats.inc_value('request_depth_count/%s' % depth,
-                          'http://scrapytest.org')
+                          b'http://scrapytest.org')
-            links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]
+            links = [lnk for lnk in rule.link_extractor.extract_links(response)
-            elif isinstance(method, basestring):
+            elif isinstance(method, six.string_types):
-            if isinstance(c, basestring):
+            if isinstance(c, six.string_types):
-        return (Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls)
+        for url in self.sitemap_urls:
-        response is not a sitemap.
+        """Return the sitemap body contained in the given response,
-    if isinstance(x, basestring):
+    if isinstance(x, six.string_types):
-    return ctype in ('application/x-gzip', 'application/gzip')
+    ctype = response.headers.get('Content-Type', b'')
-    def test_get_sitemap_body(self):
+    def assertSitemapBody(self, response, body):
-        self.assertEqual(spider._get_sitemap_body(r), self.BODY)
+        self.assertSitemapBody(r, self.BODY)
-        self.assertEqual(spider._get_sitemap_body(r), None)
+        self.assertSitemapBody(r, None)
-        self.assertEqual(spider._get_sitemap_body(r), None)
+        self.assertSitemapBody(r, None)
-        self.assertEqual(spider._get_sitemap_body(r), self.BODY)
+    def test_get_sitemap_body_gzip_headers(self):
-        self.assertEqual(spider._get_sitemap_body(r), self.BODY)
+        self.assertSitemapBody(r, self.BODY)
-        self.assertEqual(spider._get_sitemap_body(r), self.BODY)
+        self.assertSitemapBody(r, self.BODY)
-class GzTest(unittest.TestCase):
+class GunzipTest(unittest.TestCase):
-        if isinstance(url, six.text_type):
+        if not isinstance(url, str):
-            url = url.encode('utf-8')
+            warnings.warn("Link urls must be str objects.")
-_is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'file'])
+_is_valid_url = lambda url: url.split('://', 1)[0] in {'http', 'https', 'file'}
-        self.deny_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(deny)]
+        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x)
-        self.deny_extensions = set(['.' + e for e in arg_to_iter(deny_extensions)])
+        self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)}
-import re
+import six
-from scrapy.utils.python import unique as unique_list
+from scrapy.utils.python import unique as unique_list, to_native_str
-    if isinstance(tag, basestring):
+    if isinstance(tag, six.string_types):
-                url = url.encode(response_encoding)
+            url = to_native_str(url, encoding=response_encoding)
-                if self.unique else links
+        return self._deduplicate_if_needed(links)
-        links = unique_list(links, key=lambda link: link.url) if self.unique else links
+        return self._deduplicate_if_needed(links)
-
+import six
-    def test_unicode_url(self):
+    def test_non_str_url(self):
-            assert len(w) == 1, "warning not issued"
+            if six.PY2:
-        <body>
+# a hack to skip base class tests in pytest
-class LxmlLinkExtractorTestCase(BaseLinkExtractorTestCase):
+    </body>
-        html = """
+        html = b"""
-        ])
+import unittest
-class LinkExtractorTestCase(unittest.TestCase):
+class BaseSgmlLinkExtractorTestCase(unittest.TestCase):
-    extractor_cls = SgmlLinkExtractor
+class BaseLinkExtractorTestCase(unittest.TestCase):
-        '''Test that the resulting urls are regular strings and not a unicode objects'''
+        ''' Test that the resulting urls are str objects '''
-        self.assertTrue(all(isinstance(link.url, str) for link in lx.extract_links(self.response)))
+        self.assertTrue(all(isinstance(link.url, str)
-        links = SgmlLinkExtractor(restrict_xpaths='//p').extract_links(response)
+        links = self.extractor_cls(restrict_xpaths='//p').extract_links(response)
-    def test_deny_extensions(self):
+    def test_ignored_extensions(self):
-        lx = SgmlLinkExtractor(deny_extensions="jpg")
+        # override denied extensions
-            Link(url='http://example.org/page.html', text=u'asd'),
+            Link(url='http://example.org/photo.jpg'),
-class LxmlLinkExtractorTestCase(SgmlLinkExtractorTestCase):
+class LxmlLinkExtractorTestCase(BaseLinkExtractorTestCase):
-from scrapy.utils.misc import arg_to_iter
+from scrapy.utils.misc import arg_to_iter, rel_has_nofollow
-                nofollow=True if el.get('rel') == 'nofollow' else False)
+                        nofollow=rel_has_nofollow(el.get('rel')))
-from scrapy.utils.misc import arg_to_iter
+from scrapy.utils.misc import arg_to_iter, rel_has_nofollow
-                        link = Link(url=url, nofollow=True if dict(attrs).get('rel') == 'nofollow' else False)
+                        link = Link(url=url, nofollow=rel_has_nofollow(dict(attrs).get('rel')))
-                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False)]
+                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False),
-                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False)]
+                          Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False),
-    with open(path.rstrip('.tmpl'), 'wb') as file:
+    render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path
-        assert all(isinstance(x, unicode) for x in lines)
+        assert all(isinstance(x, six.text_type) for x in lines)
-        assert all(isinstance(x, unicode) for x in lines)
+        assert all(isinstance(x, six.text_type) for x in lines)
-            {'request': request, 'referer': referer},
+            {'request': request, 'referer': referer_str(request)},
-                'referer': request.headers.get('Referer'),
+                'referer': referer_str(request),
-        src = response.getErrorMessage() if isinstance(response, Failure) else response
+        if isinstance(response, Failure):
-from scrapy.utils.python import to_bytes, to_native_str
+from scrapy.utils.python import to_bytes
-            referer = _get_referer(request)
+            referer = referer_str(request)
-            referer = _get_referer(request)
+            referer = referer_str(request)
-        referer = _get_referer(request)
+        referer = referer_str(request)
-from twisted.internet.defer import Deferred
+
-               response.status in getattr(spider, 'handle_httpstatus_list', [])):
+               response.status in getattr(spider, 'handle_httpstatus_list', []) or
-            else: # try all by default
+            else:  # try all by default
-            start_python_console(self.vars, shells=shells)
+            start_python_console(self.vars, shells=shells,
-            self.print_help()
+            self.vars['banner'] = self.get_help()
-        self.p("Available Scrapy objects:")
+        print(self.get_help())
-        self.p("  shelp()           Shell help (print this help)")
+                b.append("  %-10s %s" % (k, v))
-        self.p("  view(response)    View response in a browser")
+            b.append("  fetch(req_or_url) Fetch request (or URL) and "
-        print("[s] %s" % line)
+        return "\n".join(["[s] %s" % l for l in b])
-            start_python_console(self.vars)
+            """
-    is True. Also, tab completion will be used on Unix systems.
+def _embed_ipython_shell(namespace={}, banner=''):
-            code.interact(banner=banner, local=namespace)
+        shell = get_shell_embed_func(shells)
-            link.url = urljoin(base_url, link.url)
+            try:
-                continue
+            try:
-            link.url = urljoin(base_url, link.url)
+            try:
-            httpdownloadhandler=HTTPDownloadHandler):
+            httpdownloadhandler=HTTPDownloadHandler, **kw):
-            self.conn = _S3Connection(aws_access_key_id, aws_secret_access_key)
+            self.conn = _S3Connection(aws_access_key_id, aws_secret_access_key, **kw)
-                self.AWS_SECRET_ACCESS_KEY, \
+        s3reqh = S3DownloadHandler(Settings(), self.AWS_ACCESS_KEY_ID,
-        self.assertEqual(out.strip().decode(encoding), "Scrapy %s" % scrapy.__version__)
+        self.assertEqual(
-            print("Platform: %s" % platform.platform())
+            print("Scrapy    : %s" % scrapy.__version__)
-from scrapy.utils.url import canonicalize_url, url_is_from_any_domain, url_has_any_extension
+from scrapy.utils.url import (
-    _csstranslator = ScrapyHTMLTranslator()
+    _csstranslator = HTMLTranslator()
-from parsel import Selector as ParselSelector, SelectorList as ParselSelectorList
+from parsel import Selector as _ParselSelector
-class SelectorList(ParselSelectorList, object_ref):
+class SelectorList(_ParselSelector.selectorlist_cls, object_ref):
-class Selector(ParselSelector, object_ref):
+class Selector(_ParselSelector, object_ref):
-from parsel import Selector as ParselSelector, SelectorList
+from parsel import Selector as ParselSelector, SelectorList as ParselSelectorList
-        body = open(filepath, 'rb').read()
+        with open(filepath, 'rb') as fo:
-        body = u"<p><input name='a'value='1'/><input name='b'value='2'/></p>"
+        body = b"<p><input name='a'value='1'/><input name='b'value='2'/></p>"
-        text = u'<div><img src="a.jpg"><p>Hello</div>'
+        text = b'<div><img src="a.jpg"><p>Hello</div>'
-                          body=u'<html><p>an Jos\xe9 de</p><html>', \
+                          body=b'<html><p>an Jos\xe9 de</p><html>', \
-    text = u'<div><img src="a.jpg"><p>Hello</div>'
+    text = '<div><img src="a.jpg"><p>Hello</div>'
-            self.assertTrue(isinstance(usel, XPathSelector))
+            self.assertTrue(isinstance(usel, XPathSelector))
-        sel = self.sscls(response)
+        body = u"<p><input name='a'value='1'/><input name='b'value='2'/></p>"
-            assert isinstance(x, self.sscls)
+            assert isinstance(x, Selector)
-            sel = self.sscls(_root=root)
+            sel = Selector(_root=root)
-            sel = self.sscls(_root=_root, root=root)
+            sel = Selector(_root=_root, root=root)
-        sel = self.sscls(XmlResponse('http://example.com', body=text))
+        text = u'<div><img src="a.jpg"><p>Hello</div>'
-        sel = self.sscls(HtmlResponse('http://example.com', body=text))
+        sel = Selector(HtmlResponse('http://example.com', body=text, encoding='utf-8'))
-        x = self.sscls(response)
+        x = Selector(response)
-                          body='<html><p>an Jos\xe9 de</p><html>', \
+                          body=u'<html><p>an Jos\xe9 de</p><html>', \
-    test_nested_select_on_text_nodes.skip = "Text nodes lost parent node reference in lxml"
+        Selector(r1).xpath('//text()').extract()
-        x = self.sscls(text='')
+        x = Selector(text='')
-    text = '<div><img src="a.jpg"><p>Hello</div>'
+    text = u'<div><img src="a.jpg"><p>Hello</div>'
-                         [u'url', u'name', u'startDate', u'location', u'offers'])
+            self.assertTrue(isinstance(usel, XPathSelector))
-from cssselect.xpath import ExpressionError
+from scrapy.selector.csstranslator import (
-'''
+class DeprecatedClassesTest(unittest.TestCase):
-                            for f in sel.xpath(x).extract()
+                            for f in response.xpath(x).extract()
-        if root is None and _root is not None:
+        if _root is not None:
-            root = _root
+            if root is None:
-            mock.ANY, stacklevel=2)
+    def test_deprecated_root_argument(self):
-from lxml import etree
+from parsel.selector import create_root_node
-    root = _create_parser_from_response(response, lxml.html.HTMLParser)
+    text = response.body_as_unicode()
-        'parsel>=0.9.2',
+        'parsel>=0.9.3',
-        for el, attr, attr_val in self._iter_links(selector._root):
+        for el, attr, attr_val in self._iter_links(selector.root):
-        'parsel>=0.9.1',
+        'parsel>=0.9.2',
-        self.assertIs(root, sel._root)
+        self.assertIs(root, sel.root)
-        raise ValueError('Invalid type: %s' % st)
+    return st
-        LxmlDocument(response)
+from lxml import etree
-    root = LxmlDocument(response, lxml.html.HTMLParser)
+    root = _create_parser_from_response(response, lxml.html.HTMLParser)
-        return self._extract_links(html, response.url, response.encoding, base_url)
+        return self._extract_links(response.selector, response.url, response.encoding, base_url)
-                    for subdoc in html.xpath(x)]
+                    for subdoc in response.xpath(x)]
-            docs = [html]
+            docs = [response.selector]
-            sel = Selector(response)
+            sel = response.selector
-)
+from parsel.csstranslator import XPathExpr, GenericTranslator, HTMLTranslator
-        'parsel>=0.9.0',
+        'parsel>=0.9.1',
-        self.assertEqual(root, sel._root)
+        self.assertIs(root, sel._root)
-from .lxmldocument import LxmlDocument
+from scrapy.exceptions import ScrapyDeprecationWarning
-    def __init__(self, response=None, text=None, type=None, root=None, **kwargs):
+    def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):
-        self._parser = _ctgroup[st]['_parser']
+        if root is None and _root is not None:
-            root = LxmlDocument(response, self._parser)
+            text = response.body_as_unicode()
-
+from lxml import etree
-
+from parsel.csstranslator import (
-        root = kwargs.get('root', root)
+
-from scrapy.utils.decorators import deprecated
+from scrapy.utils.python import to_bytes
-class Selector(object_ref):
+class Selector(ParselSelector, object_ref):
-                 '__weakref__', '_parser', '_csstranslator', '_tostring_method']
+    __slots__ = ['response']
-        "re": "http://exslt.org/regular-expressions",
+    def __init__(self, response=None, text=None, type=None, root=None, **kwargs):
-            _root = LxmlDocument(response, self._parser)
+            root = LxmlDocument(response, self._parser)
-    __repr__ = __str__
+        text = response.body_as_unicode() if response else None
-        return self.xpath(xpath)
+        'parsel>=0.9.0',
-        x = self.sscls()
+        x = self.sscls(text='')
-        self._crawler_settings = crawler.settings
+        self._crawler = crawler
-            dh = dhcls(self._crawler_settings)
+            dh = dhcls(self._crawler.settings)
-                             .format(path, scheme))
+            logger.error('Loading "%(clspath)s" for scheme "%(scheme)s"',
-        self._notconfigured = {} # remembers failed handlers
+        self._schemes = {}  # stores acceptable schemes on instancing
-                    'no handler available for that scheme'
+            self._notconfigured[scheme] = 'no handler available for that scheme'
-            logger.exception('Loading "{}" for scheme "{}" handler'\
+            logger.exception('Loading "{}" for scheme "{}" handler'
-                    (scheme, self._notconfigured[scheme]))
+                               (scheme, self._notconfigured[scheme]))
-            dhcls = load_object(self._schemes[scheme])
+            dhcls = load_object(path)
-            logger.exception()
+            logger.exception('Loading "{}" for scheme "{}" handler'\
-        dhcls = load_object(self._schemes[scheme])
+            dhcls = load_object(self._schemes[scheme])
-        self._notconfigured = {}
+        self._crawler_settings = crawler.settings
-                self._handlers[scheme] = dh
+            self._schemes[scheme] = clspath
-        return handler(request, spider)
+        handler = self._get_handler(scheme)
-        self.assertNotIn('scheme', dh._notconfigured)
+        self.assertIn('scheme', dh._notconfigured)
-        for slot in self.slots.itervalues():
+        for slot in six.itervalues(self.slots):
-from collections import defaultdict
+import weakref
-NoneType = type(None)
+NoneType = type(None)
-    s = "Live References" + os.linesep + os.linesep
+    s = "Live References\n\n"
-    for cls, wdict in six.iteritems(live_refs):
+    for cls, wdict in sorted(six.iteritems(live_refs),
-            now-oldest) + os.linesep
+        oldest = min(six.itervalues(wdict))
-                return min(six.iteritems(wdict), key=itemgetter(1))[0]
+            if not wdict:
-    s += b"Host: " + to_bytes(parsed.hostname) + b"\r\n"
+    s += b"Host: " + to_bytes(parsed.hostname or b'') + b"\r\n"
-from scrapy.utils.python import to_bytes
+from scrapy.utils.python import to_bytes, to_native_str
-            referer = request.headers.get('Referer')
+            referer = _get_referer(request)
-            referer = request.headers.get('Referer')
+            referer = _get_referer(request)
-        referer = request.headers.get('Referer')
+        referer = _get_referer(request)
-HTMLBODY = '''
+HTMLBODY = b'''
-        r1 = HtmlResponse('http://www.example.com', body='<html><head></head><body></body></html>')
+        r1 = HtmlResponse('http://www.example.com', body=b'<html><head></head><body></body></html>')
-        body = 'test problematic \x00 body'
+        body = b'test problematic \x00 body'
-        body = """<?xml version="1.0" encoding="UTF-8"?>\
+        body = b"""<?xml version="1.0" encoding="UTF-8"?>\
-        body = """\
+        body = b"""\
-        body = '<?xml version="1.0" encoding="ISO-8859-9"?>\n<xml>\n    <item>Some Turkish Characters \xd6\xc7\xde\xdd\xd0\xdc \xfc\xf0\xfd\xfe\xe7\xf6</item>\n</xml>\n\n'
+        body = b'<?xml version="1.0" encoding="ISO-8859-9"?>\n<xml>\n    <item>Some Turkish Characters \xd6\xc7\xde\xdd\xd0\xdc \xfc\xf0\xfd\xfe\xe7\xf6</item>\n</xml>\n\n'
-        body = """\
+        body = b"""\
-        body = """\
+        body = b"""\
-            body="some body",
+            body=b"some body",
-        self.assertEqual(u'FOO', il.get_value([u'foo', u'bar'], TakeFirst(), unicode.upper))
+        self.assertEqual(u'FOO', il.get_value([u'foo', u'bar'], TakeFirst(), six.text_type.upper))
-            name_in = MapCompose(TestItemLoader.name_in, unicode.swapcase)
+            name_in = MapCompose(TestItemLoader.name_in, six.text_type.swapcase)
-            name_in = MapCompose(DefaultedItemLoader.default_input_processor, unicode.swapcase)
+            name_in = MapCompose(DefaultedItemLoader.default_input_processor, six.text_type.swapcase)
-        self.assert_(isinstance(proc(['hello', 'world']), unicode))
+        self.assert_(isinstance(proc(['hello', 'world']), six.text_type))
-        proc = MapCompose(filter_world, unicode.upper)
+        proc = MapCompose(filter_world, six.text_type.upper)
-    response = HtmlResponse(url="", body="""
+    response = HtmlResponse(url="", encoding='utf-8', body=b"""
-        image_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation
+        image_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation
-        thumb_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation
+        thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation
-        image_guid = hashlib.sha1(url).hexdigest()
+        image_guid = hashlib.sha1(to_bytes(url)).hexdigest()
-        thumb_guid = hashlib.sha1(url).hexdigest()
+        thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()
-        media_guid = hashlib.sha1(url).hexdigest()  # change to request.url after deprecation
+        media_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation
-        media_guid = hashlib.sha1(url).hexdigest()
+        media_guid = hashlib.sha1(to_bytes(url)).hexdigest()
-from urlparse import unquote
+from six.moves.urllib.parse import unquote
-
+import six
-                        (method.im_self.__class__.__name__, response.__class__.__name__)
+                        (six.get_method_self(method).__class__.__name__, response.__class__.__name__)
-                    (method.im_self.__class__.__name__, type(response))
+                    (six.get_method_self(method).__class__.__name__, type(response))
-                    (method.im_self.__class__.__name__, type(response))
+                    (six.get_method_self(method).__class__.__name__, type(response))
-
+import six
-        fname = lambda f:'%s.%s' % (f.im_self.__class__.__name__, f.im_func.__name__)
+        fname = lambda f:'%s.%s' % (
-from HTMLParser import HTMLParser
+from six.moves.html_parser import HTMLParser
-        from urllib import urlencode
+        from six.moves.urllib.parse import urlencode
-        for obj in vars(module).itervalues():
+        for obj in vars(module).values():
-               obj.__module__ == module.__name__:
+                    issubclass(obj, ScrapyCommand) and \
-        self.err = ''
+        self.out = b''
-        return comm[0].strip()
+        comm = proc.communicate()[0].strip()
-        self.assertEqual(out.strip(), "Scrapy %s" % scrapy.__version__)
+        self.assertEqual(out.strip().decode(encoding), "Scrapy %s" % scrapy.__version__)
-        'url': request.url.decode('ascii'), # urls should be safe (safe_string_url)
+        'url': to_unicode(request.url),  # urls should be safe (safe_string_url)
-        url=d['url'].encode('ascii'),
+        url=to_native_str(d['url']),
-        raise ValueError("Function %s is not a method of: %s" % (func, obj))
+    if obj:
-            cookies={'currency': 'usd'},
+            cookies={'currency': u''},
-        r = Request("http://www.example.com", body="\xa3")
+        r = Request("http://www.example.com", body=b"\xa3")
-        r = Request("http://www.example.com", body="\xc2\xa3")
+        r = Request("http://www.example.com", body=b"\xc2\xa3")
-            errback=self.spider.handle_error)
+        r = Request("http://www.example.com", callback=self.spider.parse_item,
-            cl = request.headers.getlist('Cookie')
+            cl = [to_native_str(c, errors='replace')
-                msg += os.linesep.join("Cookie: %s" % c for c in cl)
+                cookies = "\n".join("Cookie: {}\n".format(c) for c in cl)
-            cl = response.headers.getlist('Set-Cookie')
+            cl = [to_native_str(c, errors='replace')
-                msg += os.linesep.join("Set-Cookie: %s" % c for c in cl)
+                cookies = "\n".join("Set-Cookie: {}\n".format(c) for c in cl)
-from twisted.mail.smtp import ESMTPSenderFactory
+        # Import twisted.mail here because it is not available in python3
-from unittest import TestCase
+import logging
-        return to_native_str(self.request.headers.get(name, default))
+        return to_native_str(self.request.headers.get(name, default),
-            (to_native_str(k), [to_native_str(x) for x in v])
+            (to_native_str(k, errors='replace'),
-        return [to_native_str(v) for v in self.response.headers.getlist(name)]
+        return [to_native_str(v, errors='replace')
-        headers = {'Set-Cookie': 'C1=value1; path=/'}
+        headers = {'Set-Cookie': 'C1=value1; path=/'}
-
+    def test_do_not_break_on_non_utf8_header(self):
-from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE
+from six.moves.http_cookiejar import (
-        return self.request.headers.get(name, default)
+        return to_native_str(self.request.headers.get(name, default))
-        return self.request.headers.items()
+        return [
-        return self.response.headers.getlist(name)
+    # python3 cookiejars calls get_all
-        cookievaleq = lambda cv: re.split(';\s*', cv)
+        cookievaleq = lambda cv: re.split(';\s*', cv.decode('latin1'))
-        self.assertEquals(req2.headers.get('Cookie'), "C1=value1")
+        self.assertEquals(req2.headers.get('Cookie'), b"C1=value1")
-        self.assertEquals(req.headers.get('Cookie'), 'C1=value1')
+        self.assertEquals(req.headers.get('Cookie'), b'C1=value1')
-        self.assertEquals(req.headers.get('Cookie'), 'C1=value1')
+        self.assertEquals(req.headers.get('Cookie'), b'C1=value1')
-        assert req.headers.get('Cookie') in ('C1=value1; C3=value3', 'C3=value3; C1=value1')
+        assert req.headers.get('Cookie') in (b'C1=value1; C3=value3', b'C3=value3; C1=value1')
-        self.assertEquals(req.headers.get('Cookie'), 'C2=value2')
+        self.assertEquals(req.headers.get('Cookie'), b'C2=value2')
-        self.assertEquals(req.headers.get('Cookie'), 'galleta=salada')
+        self.assertEquals(req.headers.get('Cookie'), b'galleta=salada')
-        self.assertCookieValEqual(req2.headers.get('Cookie'), "C1=value1; galleta=salada")
+        self.assertCookieValEqual(req2.headers.get('Cookie'), b"C1=value1; galleta=salada")
-        self.assertEquals(req.headers.get('Cookie'), 'galleta=salada')
+        self.assertEquals(req.headers.get('Cookie'), b'galleta=salada')
-        self.assertCookieValEqual(req2.headers.get('Cookie'),'C1=value1; galleta=salada')
+        self.assertCookieValEqual(req2.headers.get('Cookie'), b'C1=value1; galleta=salada')
-        self.assertEquals(req3.headers.get('Cookie'), 'galleta=dulce')
+        self.assertEquals(req3.headers.get('Cookie'), b'galleta=dulce')
-        self.assertCookieValEqual(req4.headers.get('Cookie'), 'C2=value2; galleta=dulce')
+        self.assertCookieValEqual(req4.headers.get('Cookie'), b'C2=value2; galleta=dulce')
-        self.assertEquals(req5_2.headers.get('Cookie'), 'C1=value1')
+        self.assertEquals(req5_2.headers.get('Cookie'), b'C1=value1')
-        self.assertEquals(req5_3.headers.get('Cookie'), 'C1=value1')
+        self.assertEquals(req5_3.headers.get('Cookie'), b'C1=value1')
-
+        self.assertEqual(b'currencyCookie=USD', request.headers['Cookie'])
-            headers={"Content-Type": "text/html"})
+        self.request = Request("http://www.example.com/page.html",
-        self.assertEqual(self.wrapped.header_items(), [('Content-Type', ['text/html'])])
+        self.assertEqual(self.wrapped.header_items(),
-        self.assertEqual(self.request.headers['hello'], 'world')
+        self.assertEqual(self.request.headers['hello'], b'world')
-            headers={"Content-TYpe": "text/html"})
+        self.response = Response("http://www.example.com/page.html",
-        return headers
+def get_s3_connection():
-        return http_request.headers
+    class _v19_S3Connection(S3Connection):
-    _S3Connection = _v20_S3Connection
+    class _v20_S3Connection(S3Connection):
-        if 'boto' not in optional_features:
+
-        c = S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)
+        c = self.S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)
-    skip = 'boto' not in optional_features and 'missing boto library'
+    download_handler_cls = S3DownloadHandler
-    """Return the raw HTTP representation (as string) of the given request.
+    """Return the raw HTTP representation (as bytes) of the given request.
-
+from scrapy.utils.python import to_bytes
-        _baseurl_cache[response] = html.get_base_url(text, response.url, \
+        _baseurl_cache[response] = html.get_base_url(text, response.url,
-        _metaref_cache[response] = html.get_meta_refresh(text, response.url, \
+        _metaref_cache[response] = html.get_meta_refresh(text, response.url,
-    """Return raw HTTP representation (as string) of the given response. This
+    """Return raw HTTP representation (as bytes) of the given response. This
-    s = "HTTP/1.1 %d %s\r\n" % (response.status, RESPONSES.get(response.status, ''))
+    s = b"HTTP/1.1 " + to_bytes(str(response.status)) + b" " + \
-    s += "\r\n"
+        s += response.headers.to_string() + b"\r\n"
-            body = body.replace('<head>', '<head><base href="%s">' % response.url)
+        if b'<base' not in body:
-            response.__class__.__name__)
+        raise TypeError("Unsupported response type: %s" %
-from scrapy.utils.response import response_httprepr, open_in_browser, get_meta_refresh
+from scrapy.utils.python import to_bytes
-    dummy_response = TextResponse(url='http://example.org/', body='dummy_response')
+    dummy_response = TextResponse(url='http://example.org/', body=b'dummy_response')
-        self.assertEqual(response_httprepr(r1), 'HTTP/1.1 200 OK\r\n\r\n')
+        self.assertEqual(response_httprepr(r1), b'HTTP/1.1 200 OK\r\n\r\n')
-        self.assertEqual(response_httprepr(r1), 'HTTP/1.1 404 Not Found\r\nContent-Type: text/html\r\n\r\nSome body')
+        r1 = Response("http://www.example.com", status=404, headers={"Content-type": "text/html"}, body=b"Some body")
-        self.assertEqual(response_httprepr(r1), 'HTTP/1.1 6666 \r\nContent-Type: text/html\r\n\r\nSome body')
+        r1 = Response("http://www.example.com", status=6666, headers={"Content-type": "text/html"}, body=b"Some body")
-        body = "<html> <head> <title>test page</title> </head> <body>test body</body> </html>"
+        body = b"<html> <head> <title>test page</title> </head> <body>test body</body> </html>"
-            assert '<base href="%s">' % url in bbody, "<base> tag not added"
+            with open(path, "rb") as f:
-            debug=True)
+
-        r1 = HtmlResponse("http://www.example.com", body="""
+        r1 = HtmlResponse("http://www.example.com", body=b"""
-        r2 = HtmlResponse("http://www.example.com", body="""
+        r2 = HtmlResponse("http://www.example.com", body=b"""
-        r3 = HtmlResponse("http://www.example.com", body="""
+        r3 = HtmlResponse("http://www.example.com", body=b"""
-    unittest.main()
+    def test_get_base_url(self):
-
+from __future__ import absolute_import
-            cls = self.from_content_disposition(headers['Content-Disposition'])
+        if b'Content-Type' in headers:
-        """Guess the most appropriate Response class based on the given arguments"""
+        """Guess the most appropriate Response class based on
-        self.assertEqual(fs, {'four': ['4'], 'three': ['3']})
+        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})
-        self.assertEqual(fs, {'four': ['4'], 'three': ['3']})
+        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})
-        self.assertEqual(fs, {'one': ['1']})
+        self.assertEqual(fs, {b'one': [b'1']})
-            ('<?xml version="1.0" encoding="utf-8"', XmlResponse),
+            (b'\x03\x02\xdf\xdd\x23', Response),
-sys.path.append(path.join(path.dirname(path.dirname(__file__)), "scrapy"))
+        chunk = to_bytes(chunk)
-        mimetype = content_type.split(';')[0].strip().lower()
+        mimetype = to_native_str(content_type).split(';')[0].strip().lower()
-from scrapy.utils.python import isbinarytext
+from scrapy.utils.python import isbinarytext, to_bytes, to_native_str
-            filename = content_disposition.split(';')[1].split('=')[1]
+            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]
-        elif "<html>" in chunk.lower():
+        elif b"<html>" in chunk.lower():
-        elif "<?xml" in chunk.lower():
+        elif b"<?xml" in chunk.lower():
-        rsp = Response(url='http://test.com', body='')
+        rsp = Response(url='http://test.com', body=b'')
-        
+
-        meta={'response': Response(item_url, status=200, body='data')})
+        meta={'response': Response(item_url, status=200, body=b'data')})
-        response = Response('http://url', body='')
+        response = Response('http://url', body=b'')
-            [M1, M3])
+        if six.PY2:
-import rfc822
+from email.utils import parsedate_tz, mktime_tz
-            modified_stamp = int(rfc822.mktime_tz(modified_tuple))
+            modified_tuple = parsedate_tz(last_modified)
-    while 1:
+    while True:
-    while 1:
+    while True:
-        rp.parse(response.body.splitlines())
+        body = ''
-from scrapy.http import Request, Response
+from scrapy.http import Request, Response, TextResponse
-        middleware = self._get_middleware()
+        middleware = RobotsTxtMiddleware(self._get_successful_crawler())
-        crawler.settings = Settings()
+    def _get_garbage_crawler(self):
-        crawler.engine.download = mock.MagicMock()
+        response = Response('http://site.local/robots.txt')
-        crawler.engine.download.side_effect = return_failure
+        self.crawler.engine.download.side_effect = return_failure
-        middleware = RobotsTxtMiddleware(crawler)
+        middleware = RobotsTxtMiddleware(self.crawler)
-        return RobotsTxtMiddleware(crawler)
+            self.file.seek(0)
-                fp.update(request.url.lower())
+                fp.update(to_bytes(request.url.lower()))
-from scrapy.xlib.pydispatch import dispatcher
+from pydispatch import dispatcher
-from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \
+from pydispatch.dispatcher import Any, Anonymous, liveReceivers, \
-from scrapy.xlib.pydispatch.robustapply import robustApply
+from pydispatch.robustapply import robustApply
-    dont_log = named.pop('dont_log', None)
+    dont_log = named.pop('dont_log', _IgnoredException)
-        return None
+        'PyDispatcher>=2.0.5',
-from scrapy.xlib.pydispatch import dispatcher
+from pydispatch import dispatcher
-    return defer.DeferredList([coop.coiterate(work) for i in xrange(count)])
+    return defer.DeferredList([coop.coiterate(work) for i in range(count)])
-        include_headers = tuple([h.lower() for h in sorted(include_headers)])
+        include_headers = tuple([to_bytes(h.lower())
-        fp.update(request.body or '')
+        fp.update(to_bytes(request.method))
-    s += "Host: %s\r\n" % parsed.hostname
+    s = to_bytes(request.method) + b" " + to_bytes(path) + b" HTTP/1.1\r\n"
-    s += "\r\n"
+        s += request.headers.to_string() + b"\r\n"
-        r2.headers['SESSIONID'] = "somehash"
+        r2.headers['SESSIONID'] = b"somehash"
-        r2.headers['Accept-Language'] = 'en'
+        r2.headers['Accept-Language'] = b'en'
-        r3.headers['SESSIONID'] = "somehash"
+        r3.headers['Accept-Language'] = b'en'
-        r3 = Request("http://www.example.com", method='POST', body='request body')
+        r3 = Request("http://www.example.com", method='POST', body=b'request body')
-        r2 = r1.replace(url = "http://www.example.com/other")
+        r2 = r1.replace(url="http://www.example.com/other")
-        self.assertEqual(r.headers['Authorization'], 'Basic c29tZXVzZXI6c29tZXBhc3M=')
+        self.assertEqual(r.headers['Authorization'], b'Basic c29tZXVzZXI6c29tZXBhc3M=')
-        self.assertEqual(request_httprepr(r1), 'GET / HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
+        self.assertEqual(request_httprepr(r1), b'GET / HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
-        self.assertEqual(request_httprepr(r1), 'GET /some/page.html?arg=1 HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
+        self.assertEqual(request_httprepr(r1), b'GET /some/page.html?arg=1 HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
-        self.assertEqual(request_httprepr(r1), 'POST / HTTP/1.1\r\nHost: www.example.com\r\nContent-Type: text/html\r\n\r\nSome body')
+        r1 = Request("http://www.example.com", method='POST', headers={"Content-type": b"text/html"}, body=b"Some body")
-        else:
+        if not isinstance(url, six.string_types):
-            self._body = ''
+        if body is None:
-            raise TypeError("Request body must either str or unicode. Got: '%s'" % type(body).__name__)
+            self._body = to_bytes(body, self.encoding)
-from scrapy.utils.python import to_bytes
+from scrapy.utils.python import to_bytes, is_listlike
-                self.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')
+                self.headers.setdefault(b'Content-Type', b'application/x-www-form-urlencoded')
-              for v in (vs if hasattr(vs, '__iter__') else [vs])]
+              for v in (vs if is_listlike(vs) else [vs])]
-    def __init__(self, url, status=200, headers=None, body='', flags=None, request=None):
+    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None):
-                "is not tied to any request")
+            raise AttributeError(
-            raise TypeError('%s url must be str, got %s:' % (type(self).__name__, \
+            raise TypeError('%s url must be str, got %s:' % (type(self).__name__,
-            self._body = ''
+        if body is None:
-                % type(body).__name__)
+            self._body = body
-from scrapy.utils.python import memoizemethod_noargs
+from scrapy.utils.python import memoizemethod_noargs, to_native_str
-            self._url = url.encode(self.encoding)
+        if isinstance(url, six.text_type):
-            if self.encoding is None:
+        self._body = b''  # used by encoding detection
-        return http_content_type_encoding(content_type)
+        content_type = self.headers.get(b'Content-Type', b'')
-                    auto_detect_fun=self._auto_detect_fun, \
+            content_type = to_native_str(self.headers.get(b'Content-Type', b''))
-                                  encoding=unicode,
+                                  encoding="unicode",
-                return unicode(self._root)
+                return six.text_type(self._root)
-from scrapy.utils.python import flatten
+from scrapy.utils.python import flatten, to_unicode
-    if isinstance(regex, basestring):
+    if isinstance(regex, six.string_types):
-    if isinstance(text, unicode):
+    if isinstance(text, six.text_type):
-        return [replace_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]
+        return [replace_entities(to_unicode(s, encoding), keep=['lt', 'amp'])
-        headers = {"caca": "coco"}
+        headers = {b"caca": b"coco"}
-        self.assertEqual(r.headers["caca"], "coco")
+        self.assertEqual(r.headers[b"caca"], b"coco")
-        headers = {'Accept':'gzip', 'Custom-Header':'nothing to tell you'}
+        headers = {b'Accept':'gzip', b'Custom-Header':'nothing to tell you'}
-            self.assert_(isinstance(k, str))
+            self.assert_(isinstance(k, bytes))
-                self.assert_(isinstance(s, str))
+                self.assert_(isinstance(s, bytes))
-        # url quoting on creation
+    def test_url_quoting(self):
-        # url encoding
+    @unittest.skipUnless(six.PY2, "TODO")
-        assert r1.body == ''
+        assert r1.body == b''
-        assert isinstance(r2.body, str)
+        r2 = self.request_class(url="http://www.example.com/", body=b"")
-        self.assertEqual(r3.body, "Price: \xc2\xa3100")
+        assert isinstance(r3.body, bytes)
-        self.assertEqual(r4.body, "Price: \xa3100")
+        assert isinstance(r4.body, bytes)
-        hdrs = Headers(dict(r1.headers, key='value'))
+        hdrs = Headers(r1.headers)
-        self.assertEqual((r1.body, r2.body), ('', "New body"))
+        self.assertEqual((r1.body, r2.body), (b'', b"New body"))
-        r4 = r3.replace(url="http://www.example.com/2", body='', meta={}, dont_filter=False)
+        r4 = r3.replace(url="http://www.example.com/2", body=b'', meta={}, dont_filter=False)
-        self.assertEqual(r4.body, '')
+        self.assertEqual(r4.body, b'')
-    def assertSortedEqual(self, first, second, msg=None):
+    def assertQueryEqual(self, first, second, msg=None):
-        self.assertEqual(r1.body, '')
+        self.assertEqual(r1.body, b'')
-        self.assertEqual(r2.headers['Content-Type'], 'application/x-www-form-urlencoded')
+        self.assertQueryEqual(r2.body, b'price=%C2%A3+100&one=two')
-        self.assertEqual(r3.body, 'price=%A3+100')
+        self.assertEqual(r3.body, b'price=%A3+100')
-            'colours=red&colours=blue&colours=green&price=%C2%A3+100'.split('&'))
+        self.assertQueryEqual(r3.body,
-            """<form action="post.php" method="POST">
+            b"""<form action="post.php" method="POST">
-        self.assertEqual(req.headers['Content-type'], 'application/x-www-form-urlencoded')
+        self.assertEqual(req.headers[b'Content-type'], b'application/x-www-form-urlencoded')
-        self.assertEqual(fs['six'], ['seven'])
+        self.assertEqual(set(fs[b"test"]), {b"val1", b"val2"})
-        self.assertEqual(req.headers['Accept-Encoding'], 'gzip,deflate')
+        self.assertEqual(req.headers['Content-type'], b'application/x-www-form-urlencoded')
-        self.assertEqual(fs['two'], ['2'])
+        self.assertEqual(fs[b'one'], [b'1'])
-        self.assertTrue(fs[u'price in \u00a3'.encode('utf-8')])
+        self.assertTrue(fs[to_native_str(u'price in \u00a3')])
-        self.assertEqual(r1.headers['Content-type'], 'application/x-www-form-urlencoded')
+        self.assertEqual(r1.headers['Content-type'], b'application/x-www-form-urlencoded')
-        self.assertEqual(fs, {'one': ['1'], 'two': ['3']})
+        self.assertEqual(fs, {b'one': [b'1'], b'two': [b'3']})
-        self.assertEqual(fs, {'four': ['4'], 'three': ['3']})
+        self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})
-        self.assertEqual(fs, {'one': ['1']})
+        self.assertEqual(fs, {b'one': [b'1']})
-        self.assertEqual(fs['one'], ['1'])
+        self.assertEqual(fs[b'one'], [b'1'])
-        self.assertEqual(fs['three'], ['3'])
+        self.assertEqual(fs[b'three'], [b'3'])
-    default_headers = {'Content-Type': ['text/xml']}
+    default_headers = {b'Content-Type': [b'text/xml']}
-        self.assertEqual(r.body, xmlrpclib.dumps(**kwargs))
+        self.assertEqual(r.headers[b'Content-Type'], b'text/xml')
-        self._test_request(params=(u'pas\xa3',), encoding='latin')
+    @unittest.skipUnless(six.PY2, "TODO")
-from scrapy.http import Request, Response, TextResponse, HtmlResponse, XmlResponse, Headers
+
-        self.assertTrue(isinstance(self.response_class('http://example.com/', body='body'), self.response_class))
+        self.assertTrue(isinstance(self.response_class('http://example.com/', body=b''), self.response_class))
-        self.assertTrue(isinstance(self.response_class('http://example.com/', headers={}, status=200, body=''), self.response_class))
+        self.assertTrue(isinstance(self.response_class('http://example.com/', body=b'', headers={}, status=200), self.response_class))
-        body = "a body"
+        headers = {"foo": "bar"}
-        self.assertEqual(r.headers["caca"], "coco")
+        self.assertEqual(r.headers[b"foo"], b"bar")
-        r1 = self.response_class("http://www.example.com", body="Some body")
+        r1 = self.response_class("http://www.example.com", body=b"Some body")
-        r1 = self.response_class("http://www.example.com", body="Some body", request=req)
+        r1 = self.response_class("http://www.example.com", body=b"Some body", request=req)
-        assert r1.body == ''
+        r2 = r1.replace(status=301, body=b"New body", headers=hdrs)
-        self.assertEqual((r1.body, r2.body), ('', "New body"))
+        self.assertEqual((r1.body, r2.body), (b'', b"New body"))
-        self.assertEqual(r4.body, '')
+        r4 = r3.replace(body=b'', flags=[])
-        if isinstance(body, unicode):
+        if isinstance(body, six.text_type):
-            body_str = body.encode(encoding)
+            body_bytes = body.encode(encoding)
-            body_str = body
+            body_bytes = body
-        assert isinstance(response.body, str)
+        assert isinstance(response.body, bytes)
-        self.assertEqual(response.body, body_str)
+        self.assertEqual(response.body, body_bytes)
-        self.assertEqual(resp.url, 'http://www.example.com/price/\xc2\xa3')
+        self.assertEqual(resp.url, to_native_str(b'http://www.example.com/price/\xc2\xa3'))
-        self.assertEqual(resp.url, 'http://www.example.com/price/\xc2\xa3')
+        self.assertEqual(resp.url, to_native_str(b'http://www.example.com/price/\xc2\xa3'))
-        self.assertTrue(isinstance(r1.body_as_unicode(), unicode))
+        self.assertTrue(isinstance(r1.body_as_unicode(), six.text_type))
-        r1 = self.response_class("http://www.example.com", headers={"Content-type": ["text/html; charset=utf-8"]}, body="\xc2\xa3")
+        r1 = self.response_class("http://www.example.com", headers={"Content-type": ["text/html; charset=utf-8"]}, body=b"\xc2\xa3")
-        r7 = self.response_class("http://www.example.com", headers={"Content-type": ["text/html; charset=gbk"]}, body="\xa8D")
+        r3 = self.response_class("http://www.example.com", headers={"Content-type": ["text/html; charset=iso-8859-1"]}, body=b"\xa3")
-                                body="\xc2\xa3")
+                                body=b"\xc2\xa3")
-                                body='\xff\xfeh\x00i\x00',
+                                body=b'\xff\xfeh\x00i\x00',
-                                 body="\xef\xbb\xbfWORD\xe3\xab")
+                                 body=b"\xef\xbb\xbfWORD\xe3\xab")
-        body = "\xef\xbb\xbfWORD"
+        body = b"\xef\xbb\xbfWORD"
-        r = self.response_class("http://www.example.com", encoding='utf-8', body='PREFIX\xe3\xabSUFFIX')
+        r = self.response_class("http://www.example.com", encoding='utf-8', body=b'PREFIX\xe3\xabSUFFIX')
-                body='\xf0<span>value</span>')
+                body=b'\xf0<span>value</span>')
-        body = "<html><head><title>Some page</title><body></body></html>"
+        body = b"<html><head><title>Some page</title><body></body></html>"
-        body = "<html><head><title>Some page</title><body></body></html>"
+        body = b"<html><head><title>Some page</title><body></body></html>"
-        body = '<html><body><base href="https://example.net"></body></html>'
+        body = b'<html><body><base href="https://example.net"></body></html>'
-        body = '<html><body><base href="/elsewhere"></body></html>'
+        body = b'<html><body><base href="/elsewhere"></body></html>'
-        body = '<html><body><base href="/elsewhere/"></body></html>'
+        body = b'<html><body><base href="/elsewhere/"></body></html>'
-        body = """<html><head><title>Some page</title><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+        body = b"""<html><head><title>Some page</title><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-        body = """<?xml version="1.0" encoding="iso-8859-1"?>
+        body = b"""<?xml version="1.0" encoding="iso-8859-1"?>
-        body = """<html><head><title>Some page</title><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
+        body = b"""<html><head><title>Some page</title><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
-        body = "New body \xa3"
+        body = b"New body \xa3"
-        body = """<html><head><meta charset="gb2312" /><title>Some page</title><body>bla bla</body>"""
+        body = b"""<html><head><meta charset="gb2312" /><title>Some page</title><body>bla bla</body>"""
-        body = "<xml></xml>"
+        body = b"<xml></xml>"
-        body = """<?xml version="1.0" encoding="iso-8859-1"?><xml></xml>"""
+        body = b"""<?xml version="1.0" encoding="iso-8859-1"?><xml></xml>"""
-        body = """<?xml version="1.0" encoding="iso-8859-1"?><xml></xml>"""
+        body = b"""<?xml version="1.0" encoding="iso-8859-1"?><xml></xml>"""
-        body2 = "New body"
+        body2 = b"New body"
-        body2 = """<?xml version="1.0" encoding="utf-8"?><xml></xml>"""
+        body = b"""<?xml version="1.0" encoding="iso-8859-1"?><xml></xml>"""
-        body = '<?xml version="1.0" encoding="utf-8"?><xml><elem>value</elem></xml>'
+        body = b'<?xml version="1.0" encoding="utf-8"?><xml><elem>value</elem></xml>'
-        body = '<?xml version="1.0" encoding="utf-8"?><xml><elem>value</elem></xml>'
+        body = b'<?xml version="1.0" encoding="utf-8"?><xml><elem>value</elem></xml>'
-    unittest.main()
+from six.moves import xrange
-        self.assertEqual(out, range(10))
+        self.assertEqual(out, list(range(10)))
-# scrapy.utils.url was moved to w3lib.url and import * ensures this move doesn't break old code
+# scrapy.utils.url was moved to w3lib.url and import * ensures this
-from scrapy.utils.python import to_bytes
+from w3lib.url import _safe_chars
-    else:
+    if not host:
-        encoding=None):
+                     encoding=None):
-    return urlparse(to_bytes(url, encoding))
+    return urlparse(to_native_str(url, encoding))
-from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url
+from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,
-        # always return a str
+    def test_return_str(self):
-        # append missing path
+    def test_append_missing_path(self):
-        # typical usage
+
-        # sorting by argument values
+    def test_sorting(self):
-        # using keep_blank_values
+    def test_keep_blank_values(self):
-        # spaces
+    def test_spaces(self):
-        # normalize percent-encoding case (in paths)
+    @unittest.skipUnless(six.PY2, "TODO")
-        # normalize percent-encoding case (in query arguments)
+
-        # non-ASCII percent-encoding in paths
+    def test_non_ascii_percent_encoding_in_paths(self):
-        self.assertEqual(canonicalize_url("http://www.example.com/a do\xc2\xa3.html?a=1"),
+        self.assertEqual(canonicalize_url(u"http://www.example.com/a do.html?a=1"),
-        # non-ASCII percent-encoding in query arguments
+
-        self.assertEqual(canonicalize_url("http://www.example.com/do?price=\xc2\xa3500&a=5&z=3"),
+        self.assertEqual(canonicalize_url(b"http://www.example.com/do?price=\xc2\xa3500&a=5&z=3"),
-        self.assertEqual(canonicalize_url("http://www.example.com/do?price(\xc2\xa3)=500&a=1"),
+        self.assertEqual(canonicalize_url(b"http://www.example.com/do?price(\xc2\xa3)=500&a=1"),
-        # urls containing auth and ports
+    def test_urls_with_auth_and_ports(self):
-        # remove fragments
+    def test_remove_fragments(self):
-        # domains are case insensitive
+    def test_domains_are_case_insensitive(self):
-        # quoted slash and question sign
+    def test_quoted_slash_and_question_sign(self):
-        self.assertIn(u'\ufffd', to_unicode(b'a\xedb', 'utf-8', errors='replace'))
+    def test_errors_argument(self):
-    def test_converting_a_regular_string_to_string_should_return_the_same_object(self):
+    def test_converting_a_regular_bytes_to_bytes_should_return_the_same_object(self):
-        self.assertIn(b'?', to_bytes(u'a\ufffdb', 'latin-1', errors='replace'))
+    def test_errors_argument(self):
-    def utf_16_strings_contain_null_bytes(self):
+    def test_utf_16_strings_contain_null_bytes(self):
-    def test_finally_some_real_binary_bytes(self):
+    def test_real_binary_bytes(self):
-    pattern = re.compile(pattern) if isinstance(pattern, basestring) else pattern
+    if isinstance(pattern, six.string_types):
-            return (offset + matches[-1].span()[0], offset + matches[-1].span()[1])
+            start, end = matches[-1].span()
-_BINARYCHARS = set(map(chr, range(32))) - set(["\0", "\t", "\n", "\r"])
+_BINARYCHARS = {six.b(chr(i)) for i in range(32)} - {b"\0", b"\t", b"\n", b"\r"}
-    """Return True if the given text is considered binary, or false
+    """Return True if the given text is considered binary, or False
-    assert isinstance(text, str), "text must be str, got '%s'" % type(text).__name__
+    if not isinstance(text, bytes):
-    """Return a (new) dict with the unicode keys (and values if, keys_only is
+    """Return a (new) dict with unicode keys (and values when "keys_only" is
-        k = k.encode(encoding) if isinstance(k, unicode) else k
+        k = k.encode(encoding) if isinstance(k, six.text_type) else k
-            v = v.encode(encoding) if isinstance(v, unicode) else v
+            v = v.encode(encoding) if isinstance(v, six.text_type) else v
-class UtilsPythonTestCase(unittest.TestCase):
+class MemoizedMethodTest(unittest.TestCase):
-        assert not isbinarytext("hello")
+class IsBinaryTextTest(unittest.TestCase):
-        # utf-16 strings contain null bytes
+    def utf_16_strings_contain_null_bytes(self):
-        assert not isbinarytext("<div>Price \xa3</div>")
+    def test_one_with_encoding(self):
-        assert isbinarytext("\x02\xa3")
+
-        d = {'a': 123, u'b': 'c', u'd': u'e', object(): u'e'}
+        d = {'a': 123, u'b': b'c', u'd': u'e', object(): u'e'}
-        self.failIf(any(isinstance(x, unicode) for x in d2.values()))
+        self.failIf(d is d2)  # shouldn't modify in place
-        self.failIf(any(isinstance(x, unicode) for x in d2.values()))
+        self.failIf(d is d2)  # shouldn't modify in place
-        self.failIf(any(isinstance(x, unicode) for x in d2.keys()))
+        self.failIf(d is d2)  # shouldn't modify in place
-        self.assertEqual(get_func_args(unicode.split), [])
+        self.assertEqual(get_func_args(six.text_type.split), [])
-        self.assertEqual(to_unicode('lel\xc3\xb1e'), u'lel\xf1e')
+        self.assertEqual(to_unicode(b'lel\xc3\xb1e'), u'lel\xf1e')
-        self.assertEqual(to_unicode('lel\xf1e', 'latin-1'), u'lel\xf1e')
+        self.assertEqual(to_unicode(b'lel\xf1e', 'latin-1'), u'lel\xf1e')
-        self.assertIn(u'\ufffd', to_unicode('a\xedb', 'utf-8', errors='replace'))
+        self.assertIn(u'\ufffd', to_unicode(b'a\xedb', 'utf-8', errors='replace'))
-        self.assertEqual(to_bytes(u'\xa3 49'), '\xc2\xa3 49')
+        self.assertEqual(to_bytes(u'\xa3 49'), b'\xc2\xa3 49')
-        self.assertEqual(to_bytes(u'\xa3 49', 'latin-1'), '\xa3 49')
+        self.assertEqual(to_bytes(u'\xa3 49', 'latin-1'), b'\xa3 49')
-        self.assertEqual(to_bytes('lel\xf1e'), 'lel\xf1e')
+        self.assertEqual(to_bytes(b'lel\xf1e'), b'lel\xf1e')
-        self.assertIn('?', to_bytes(u'a\ufffdb', 'latin-1', errors='replace'))
+        self.assertIn(b'?', to_bytes(u'a\ufffdb', 'latin-1', errors='replace'))
-
+    [1, 2, 3, 42, None, 4, 5, 6, 7, 8, 9, 10]
-        if hasattr(el, "__iter__"):
+        if is_listlike(el):
-from scrapy.utils.python import unicode_to_str
+from scrapy.utils.python import to_bytes
-    values = [(unicode_to_str(k, enc), unicode_to_str(v, enc))
+    values = [(to_bytes(k, enc), to_bytes(v, enc))
-from scrapy.utils.python import unique as unique_list, str_to_unicode
+from scrapy.utils.python import unique as unique_list
-from scrapy.utils.python import unique as unique_list, str_to_unicode
+from scrapy.utils.python import unique as unique_list, to_unicode
-            link.text = str_to_unicode(link.text, response_encoding, errors='replace').strip()
+            link.text = to_unicode(link.text, response_encoding, errors='replace').strip()
-from scrapy.utils.python import unicode_to_str, flatten, iflatten
+from scrapy.utils.python import to_bytes, flatten, iflatten
-              body=unicode_to_str(text, 'utf-8'))
+              body=to_bytes(text, 'utf-8'))
-from scrapy.utils.python import re_rsearch, str_to_unicode
+from scrapy.utils.python import re_rsearch, to_unicode
-        return [str_to_unicode(field, encoding) for field in next(csv_r)]
+        return [to_unicode(field, encoding) for field in next(csv_r)]
-
+from scrapy.utils.decorators import deprecated
-    """
+    """ This function is deprecated.
-        raise TypeError('str_to_unicode must receive a str or unicode object, got %s' % type(text).__name__)
+@deprecated("scrapy.utils.python.to_bytes")
-    """
+    """ This function is deprecated. Please use scrapy.utils.python.to_bytes """
-    elif isinstance(text, str):
+    return text.decode(encoding, errors)
-        raise TypeError('unicode_to_str must receive a unicode or str object, got %s' % type(text).__name__)
+    if not isinstance(text, six.string_types):
-from scrapy.utils.python import unicode_to_str
+from scrapy.utils.python import to_bytes
-        urlparse(unicode_to_str(url, encoding))
+    if isinstance(url, ParseResult):
-from scrapy.utils.python import str_to_unicode
+from scrapy.utils.python import to_unicode
-            exported_dict[k] = str_to_unicode(v)
+            exported_dict[k] = to_unicode(v)
-    WeakKeyCache, stringify_dict, get_func_args
+from scrapy.utils.python import (
-        self.assertEqual(str_to_unicode('lel\xf1e', 'latin-1'), u'lel\xf1e')
+class ToUnicodeTest(unittest.TestCase):
-        self.assertEqual(str_to_unicode(u'\xf1e\xf1e\xf1e'), u'\xf1e\xf1e\xf1e')
+    def test_converting_a_strange_object_should_raise_TypeError(self):
-        self.assertRaises(TypeError, str_to_unicode, 423)
+    def test_check_errors_argument_works(self):
-        self.assertEqual(unicode_to_str(u'\xa3 49'), '\xc2\xa3 49')
+class ToBytesTest(unittest.TestCase):
-        self.assertEqual(unicode_to_str(u'\xa3 49', 'latin-1'), '\xa3 49')
+    def test_converting_a_unicode_object_to_a_latin_1_encoded_string(self):
-        self.assertEqual(unicode_to_str('lel\xf1e'), 'lel\xf1e')
+    def test_converting_a_regular_string_to_string_should_return_the_same_object(self):
-        self.assertRaises(TypeError, unicode_to_str, unittest)
+    def test_converting_a_strange_object_should_raise_TypeError(self):
-        assert '?' in unicode_to_str(u'a\ufffdb', 'latin-1', errors='replace')
+    def test_check_errors_argument_works(self):
-        req = Request(url, meta={'dont_redirect': True})
+        req = Request(url)
-        if request.meta.get('dont_redirect', False):
+        if (request.meta.get('dont_redirect', False) or
-        self.mw = RedirectMiddleware.from_crawler(crawler)
+        self.crawler = get_crawler(Spider)
-                self.assertEqual(mock_object, mock_copy.return_value)
+        frozencopy = self.settings.frozencopy()
-        new_delay = (slot.delay + latency) / 2.0
+        # If a server needs `latency` seconds to respond then
-        # If latency is bigger than old delay, then use latency instead of mean.
+        # Adjust the delay to make it closer to target_delay
-        new_delay = max(latency, new_delay)
+        new_delay = max(target_delay, new_delay)
-        return getattr(spider, 'download_delay', 0.0) or s.getfloat('DOWNLOAD_DELAY')
+        return getattr(spider, 'download_delay', s.getfloat('DOWNLOAD_DELAY'))
-            s.getfloat('DOWNLOAD_DELAY')
+        return getattr(spider, 'download_delay', 0.0) or s.getfloat('DOWNLOAD_DELAY')
-        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY', 60.0)
+        return self.crawler.settings.getfloat('AUTOTHROTTLE_MAX_DELAY')
-        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY', 5.0))
+        return max(self.mindelay, self.crawler.settings.getfloat('AUTOTHROTTLE_START_DELAY'))
-        new_delay = min(max(self.mindelay, latency, (slot.delay + latency) / 2.0), self.maxdelay)
+        # It works better with problematic sites.
-            slot.delay = new_delay
+        if response.status != 200 and new_delay <= slot.delay:
-        :param str value: a string with JSON data to extract from
+        :param value: a data structure (dict, list) to extract from
-#html_theme_path = []
+# Add path to the RTD explicitly to robustify builds (otherwise might
-from scrapy.exceptions import ScrapyDeprecationWarning
+    def __repr__(self):
-    def __init__(self, concurrency, delay, settings):
+    def __init__(self, concurrency, delay, randomize_delay):
-        self.randomize_delay = settings.getbool('RANDOMIZE_DOWNLOAD_DELAY')
+        self.randomize_delay = randomize_delay
-            self.slots[key] = Slot(conc, delay, self.settings)
+            self.slots[key] = Slot(conc, delay, self.randomize_delay)
-        if user and password:
+        if user:
-    default_settings = {'KEEP_ALIVE': True, 'LOGSTATS_INTERVAL': 0}
+    default_settings = {
-    main()
+        # Do compactation each time to save space and also recreate files to
-        Like :meth:`send_catch_log` but supports returning `deferred`_ from
+        Like :meth:`send_catch_log` but supports returning `deferreds`_ from
-        Returns a `deferred`_ that gets fired once all signal handlers
+        Returns a Deferred that gets fired once all signal handlers
-    """Initialize and configure default loggers
+    """
-      - Route stdout to log if LOG_STDOUT setting is True
+
-      - Creates a handler for the root logger according to given settings
+    creates a handler for the root logger according to given settings
-        with warnings.catch_warnings(record=True):
+        with warnings.catch_warnings():
-        with warnings.catch_warnings(record=True):
+        with warnings.catch_warnings():
-        with warnings.catch_warnings(record=True):
+        with warnings.catch_warnings():
-        with warnings.catch_warnings(record=True):
+        with warnings.catch_warnings():
-        with warnings.catch_warnings(record=True):
+        with warnings.catch_warnings():
-        with warnings.catch_warnings(record=True):
+        with warnings.catch_warnings():
-        with warnings.catch_warnings(record=True):
+        with warnings.catch_warnings():
-        with warnings.catch_warnings(record=True):
+        with warnings.catch_warnings():
-        with warnings.catch_warnings(record=True):
+        with warnings.catch_warnings():
-        parser.add_option("--get", dest="get", metavar="SETTING", \
+        parser.add_option("--get", dest="get", metavar="SETTING",
-            help="print setting value, intepreted as an float")
+        parser.add_option("--getbool", dest="getbool", metavar="SETTING",
-from scrapy.utils import signal
+from scrapy.utils import signal as _signal
-        return dispatcher.connect(*a, **kw)
+    def connect(self, receiver, signal, **kwargs):
-        return dispatcher.disconnect(*a, **kw)
+        The keyword arguments are passed to the signal handlers (connected
-        return signal.send_catch_log(*a, **kw)
+        .. _deferreds: http://twistedmatrix.com/documents/current/core/howto/defer.html
-        return signal.send_catch_log_deferred(*a, **kw)
+    def disconnect_all(self, signal, **kwargs):
-        return signal.disconnect_all(*a, **kw)
+        :param signal: the signal to disconnect from
-def configure_logging(settings=None):
+def configure_logging(settings=None, install_root_handler=True):
-      - Create a handler for the root logger according to given settings
+      - Route stdout to log if LOG_STDOUT setting is True
-    if isinstance(settings, dict):
+    if isinstance(settings, dict) or settings is None:
-        logging.root.setLevel(logging.NOTSET)
+    if settings.getbool('LOG_STDOUT'):
-        handler.addFilter(TopLevelFormatter(['scrapy']))
+    if install_root_handler:
-RETRY_HTTP_CODES = [500, 502, 503, 504, 400, 408]
+RETRY_HTTP_CODES = [500, 502, 503, 504, 408]
-        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS', 60.0)
+        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS')
-        if isinstance(settings, dict):
+    def __init__(self, spidercls, settings=None):
-        if isinstance(settings, dict):
+    def __init__(self, settings=None):
-    def __init__(self, settings):
+    def __init__(self, settings=None):
-class CrawlerTestCase(unittest.TestCase):
+class BaseCrawlerTest(unittest.TestCase):
-        self.assertIsInstance(crawler.settings, Settings)
+        self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')
-class CrawlerRunnerTestCase(unittest.TestCase):
+class CrawlerRunnerTestCase(BaseCrawlerTest):
-        self.assertIsInstance(runner.settings, Settings)
+        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')
-class CrawlerProcessTest(unittest.TestCase):
+class CrawlerProcessTest(BaseCrawlerTest):
-        self.assertIsInstance(runner.settings, Settings)
+        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')
-        reactor.callFromThread(self.stop)
+        reactor.callFromThread(self._graceful_stop_reactor)
-            d.addBoth(lambda _: self._stop_reactor())
+            d.addBoth(self._stop_reactor)
-                                                            self.settings.getfloat('DNS_TIMEOUT')))
+        reactor.installResolver(self._get_dns_resolver())
-        tsk.start(60.0, now=True)
+        tsk.start(self.check_interval, now=True)
-            tsk.start(60.0, now=True)
+            tsk.start(self.check_interval, now=True)
-            tsk.start(60.0, now=True)
+            tsk.start(self.check_interval, now=True)
-LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S%z'
+LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S'
-
+import scrapy
-def build_component_list(base, custom):
+
-    compdict.update(custom)
+        _check_components(custom)
-    def test_build_component_list(self):
+class BuildComponentListTest(unittest.TestCase):
-        self.assertEqual(build_component_list(base, custom),
+        self.assertEqual(build_component_list(base, custom, lambda x: x),
-        self.assertEqual(build_component_list(base, custom), custom)
+        self.assertEqual(build_component_list(None, custom, lambda x: x.upper()),
-from scrapy.utils.deprecate import create_deprecated_class
+from scrapy.utils.deprecate import create_deprecated_class, update_classpath
-                pass
+        maxage = self._get_max_age(cc)
-from scrapy.http import Headers
+from scrapy.http import Headers, Response
-            self._cc_parsed[r] = parse_cachecontrol(cch)
+            parsed = parse_cachecontrol(cch)
-        log_scrapy_info(settings)
+        configure_logging(self.settings)
-from scrapy.crawler import Crawler, CrawlerRunner
+from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess
-                             signals.engine_stopped)
+        # lambda is assigned to Crawler attribute because this way it is not
-from six.moves.urllib.parse import urlparse
+from six.moves.urllib.parse import urlparse, unquote
-                                request, parsed_url.path)
+                                request, unquote(parsed_url.path))
-        FilePath(userdir).child('file.txt').setContent("I have the power!")
+        fp = FilePath(userdir)
-    """Same as twisted.internet.defer.fail, but delay calling errback until
+    """Same as twisted.internet.defer.fail but delay calling errback until
-    """Same as twsited.internet.defer.succed, but delay calling callback until
+    """Same as twisted.internet.defer.succeed but delay calling callback until
-    reactor.callLater(0, d.errback, _failure)
+    reactor.callLater(0.1, d.errback, _failure)
-    reactor.callLater(0, d.callback, result)
+    reactor.callLater(0.1, d.callback, result)
-# Imports kept for backwards-compatibility
+# Imports and level_names variable kept for backwards-compatibility
-        logfmt = "%%s %(format)s feed (%(itemcount)d items) in: %(uri)s"
+        logfmt = "%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s"
-
+        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None
-        # but it is possible to override fields using FEED_EXPORT_FIELDS
+        # edge case: FEED_EXPORT_FIELDS==[] means the same as default None
-        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None
+
-        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS')
+        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None
-        # by default, Scrapy uses fields of the first Item
+        # by default, Scrapy uses fields of the first Item for CSV and
-        rows = [
+        rows_csv = [
-        yield self.assertExported(items, header, rows, ordered=False)
+        rows_jl = [dict(row) for row in items]
-        # a header.
+        # a header for CSV, and all fields are used for JSON Lines.
-        rows = [
+        rows_csv = [
-        yield self.assertExported(items, ['egg', 'foo'], rows, ordered=False)
+        rows_jl = items
-    def test_export_csv_items(self):
+    def assertExportedJsonLines(self, items, rows, settings=None):
-        yield self.assertExportedCsv(items, header, rows, ordered=False)
+        yield self.assertExported(items, header, rows, ordered=False)
-    def test_export_csv_multiple_item_classes(self):
+    def test_export_multiple_item_classes(self):
-        yield self.assertExportedCsv(items, header, rows, ordered=False)
+        yield self.assertExported(items, header, rows, ordered=False)
-                                     settings=settings, ordered=True)
+        yield self.assertExported(items, header, rows,
-    def test_export_csv_dicts(self):
+    def test_export_dicts(self):
-        yield self.assertExportedCsv(items, ['egg', 'foo'], rows, ordered=False)
+        yield self.assertExported(items, ['egg', 'foo'], rows, ordered=False)
-    def test_export_csv_feed_export_fields(self):
+    def test_export_feed_export_fields(self):
-                                         settings=settings, ordered=True)
+            yield self.assertExported(items, ['foo', 'baz', 'egg'], rows,
-                                         settings=settings, ordered=True)
+            yield self.assertExported(items, ['egg', 'baz'], rows,
-        return (failure.type, failure.value, failure.tb)
+        return (failure.type, failure.value, failure.getTracebackObject())
-from scrapy.utils.log import logformatter_adapter
+from scrapy.utils.log import logformatter_adapter, failure_to_exc_info
-                                           extra={'spider': spider, 'failure': f}))
+                                           exc_info=failure_to_exc_info(f),
-                                           extra={'spider': spider, 'failure': f}))
+                                           exc_info=failure_to_exc_info(f),
-                                           extra={'spider': spider, 'failure': f}))
+                                           exc_info=failure_to_exc_info(f),
-                                            extra={'spider': spider, 'failure': f}))
+                                            exc_info=failure_to_exc_info(f),
-                logger.error(msg, extra={'spider': spider, 'failure': failure})
+                logger.error(
-from scrapy.utils.log import logformatter_adapter
+from scrapy.utils.log import logformatter_adapter, failure_to_exc_info
-                                   extra={'spider': spider, 'failure': f}))
+                                   exc_info=failure_to_exc_info(f),
-            extra={'spider': spider, 'failure': _failure}
+            exc_info=failure_to_exc_info(_failure),
-                             extra={'spider': spider, 'failure': download_failure})
+                             exc_info=failure_to_exc_info(download_failure),
-                             extra={'spider': spider, 'failure': output})
+                             exc_info=failure_to_exc_info(output),
-                         extra={'spider': spider, 'failure': failure})
+                         exc_info=failure_to_exc_info(failure),
-                                            extra={'spider': spider, 'failure': f}))
+                                            exc_info=failure_to_exc_info(f),
-    logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})
+    logger.log(level, message, *[kw] if kw else [], exc_info=failure_to_exc_info(failure))
-                         extra={'spider': info.spider, 'failure': f})
+                         exc_info=failure_to_exc_info(f),
-            f.value, extra={'spider': info.spider, 'failure': f})
+            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})
-                        extra={'spider': info.spider, 'failure': value}
+                        exc_info=failure_to_exc_info(value),
-        return True
+def failure_to_exc_info(failure):
-            'filters': ['failure_formatter'],
+from scrapy.utils.log import failure_to_exc_info
-                         extra={'spider': spider, 'failure': failure})
+                         exc_info=failure_to_exc_info(failure),
-        assert record.failure is fail
+        self.assertTupleEqual(record.exc_info, failure_to_exc_info(fail))
-from scrapy.utils.log import (FailureFormatter, TopLevelFormatter,
+from scrapy.utils.log import (failure_to_exc_info, TopLevelFormatter,
-class FailureFormatterTest(unittest.TestCase):
+class FailureToExcInfoTest(unittest.TestCase):
-        self.assertTupleEqual(failure_record.exc_info, exc_record.exc_info)
+    def test_failure(self):
-            self.logger.error('test log msg', extra={'failure': 3})
+        self.assertTupleEqual(exc_info, failure_to_exc_info(failure))
-                                  'test log msg' + os.linesep + '3')
+    def test_non_failure(self):
-                                 extra={'spider': spider})
+                logger.error('Error while obtaining start requests',
-                                 extra={'spider': self.spider})
+                logger.error("Unable to serialize request: %(request)s - reason: %(reason)s",
-            logger.exception(
+            logger.error(
-                extra={'spider': info.spider}
+                exc_info=True, extra={'spider': info.spider}
-                             {'receiver': receiver}, extra={'spider': spider})
+            logger.error("Error caught on signal handler: %(receiver)s",
-                self.logger.exception('test log msg')
+                self.logger.error('test log msg', exc_info=True)
-        fields = {}
+        fields = getattr(_class, 'fields', {})
-        self.assertEqual(D.fields, {'save': {'default': 'A'}})
+        item = D(save='X', load='Y')
-        self.assertEqual(E.fields, {'save': {'default': 'C'}})
+        self.assertEqual(E(load='X')['load'], 'X')
-            'load': {'default': 'D'}})
+            'load': {'default': 'D'}, 'update': {'default': 'D'}})
-            'load': {'default': 'E'}})
+            'load': {'default': 'E'}, 'update': {'default': 'C'}})
-        self.assertEqual(D.fields, {'save': {'default': 'A'}})
+        self.assertEqual(D.fields, {'save': {'default': 'A'},
-        self.assertEqual(E.fields, {'save': {'default': 'A'}})
+        self.assertEqual(E.fields, {'save': {'default': 'A'},
-    return p
+    p += refnode
-                           for d in env.scrapy_all_settings if fromdocname != d['docname']])
+        settings_list = nodes.bullet_list()
-    return node.tagname == 'pending_xref' and node['reftype'] == 'setting'
+def is_setting_index(node):
-            node.replace_self([targetnode, node])
+    for node in doctree.traverse(is_setting_index):
-            'target': targetnode,
+            'setting_name': setting_name,
-                           setting_data['docname'], targetid, text)
+                           todocname=setting_data['docname'],
-                           for d in env.scrapy_all_settings])
+                           for d in env.scrapy_all_settings if fromdocname != d['docname']])
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-            :class:`~scrapy.spider.Spider` subclass or string
+            :class:`~scrapy.spiders.Spider` subclass or string
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-        
+        (of the last initialization request) must be self.initialized.
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-    from scrapy.spider import Spider
+    from scrapy.spiders import Spider
-    from scrapy.spider import Spider
+    from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spiders import CrawlSpider, Rule, XMLFeedSpider, \
+from scrapy.spiders import Spider, BaseSpider, CrawlSpider, Rule, XMLFeedSpider, \
-        with mock.patch('scrapy.spider.Spider.logger') as mock_logger:
+        with mock.patch('scrapy.spiders.Spider.logger') as mock_logger:
-# ugly hack to avoid cyclic imports of scrapy.spider when running this test
+# ugly hack to avoid cyclic imports of scrapy.spiders when running this test
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-        from scrapy.spider import Spider
+        from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spider import Spider
+from scrapy.spiders import Spider
-from scrapy.spiders.sitemap import SitemapSpider
+    "scrapy/linkextractor.py",
-scrapy.linkextractors).
+scrapy.linkextractors
-from scrapy.linkextractor import FilteringLinkExtractor
+from scrapy.linkextractors import FilteringLinkExtractor
-from scrapy.linkextractor import FilteringLinkExtractor
+from scrapy.linkextractors import FilteringLinkExtractor
-from .lxmlhtml import LxmlLinkExtractor as LinkExtractor
+    "scrapy/command.py",
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.command import ScrapyCommand
+from scrapy.commands import ScrapyCommand
-from scrapy.commands import fetch
+from scrapy.commands import fetch, ScrapyCommand
-            ScrapyDeprecationWarning
+            ScrapyDeprecationWarning, stacklevel=2,
-            ScrapyDeprecationWarning
+            ScrapyDeprecationWarning, stacklevel=2,
-            ScrapyDeprecationWarning
+            ScrapyDeprecationWarning, stacklevel=2,
-            return x.extract() or default
+            return x.extract()
-collect_ignore = ["scrapy/stats.py", "scrapy/project.py"]
+
-#html_theme = 'sphinx_rtd_theme'
+html_theme = 'sphinx_rtd_theme'
-html_theme = 'sphinx_rtd_theme'
+#html_theme = 'sphinx_rtd_theme'
-from twisted.internet import reactor
+from twisted.internet import reactor, error
-    """A class to run multiple scrapy crawlers in a process simultaneously"""
+    """
-        self.crawlers = set()
+        self._crawlers = set()
-        """Wait for all managed crawlers to complete"""
+        """
-extensions = ['scrapydocs']
+extensions = [
-from scrapy.settings import overridden_settings
+from scrapy.settings import overridden_settings, Settings
-STATS_CLASS = 'scrapy.statscol.MemoryStatsCollector'
+STATS_CLASS = 'scrapy.statscollectors.MemoryStatsCollector'
-from scrapy.statscol import StatsCollector
+from scrapy.statscollectors import StatsCollector
-from scrapy.statscol import StatsCollector, DummyStatsCollector
+from scrapy.statscollectors import StatsCollector, DummyStatsCollector
-    'scrapy.contrib.corestats.CoreStats': 0,
+    'scrapy.extensions.corestats.CoreStats': 0,
-    'scrapy.contrib.throttle.AutoThrottle': 0,
+    'scrapy.extensions.memusage.MemoryUsage': 0,
-    'ftp': 'scrapy.contrib.feedexport.FTPFeedStorage',
+    '': 'scrapy.extensions.feedexport.FileFeedStorage',
-HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.FilesystemCacheStorage'
+HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
-HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.DummyPolicy'
+HTTPCACHE_POLICY = 'scrapy.extensions.httpcache.DummyPolicy'
-    policy_class = 'scrapy.contrib.httpcache.RFC2616Policy'
+    storage_class = 'scrapy.extensions.httpcache.DbmCacheStorage'
-    storage_class = 'scrapy.contrib.httpcache.DbmCacheStorage'
+    storage_class = 'scrapy.extensions.httpcache.DbmCacheStorage'
-    storage_class = 'scrapy.contrib.httpcache.FilesystemCacheStorage'
+    storage_class = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
-    storage_class = 'scrapy.contrib.httpcache.LeveldbCacheStorage'
+    storage_class = 'scrapy.extensions.httpcache.LeveldbCacheStorage'
-    policy_class = 'scrapy.contrib.httpcache.DummyPolicy'
+    policy_class = 'scrapy.extensions.httpcache.DummyPolicy'
-    policy_class = 'scrapy.contrib.httpcache.RFC2616Policy'
+    policy_class = 'scrapy.extensions.httpcache.RFC2616Policy'
-from scrapy.contrib.feedexport import (
+from scrapy.extensions.feedexport import (
-from scrapy.contrib.spiderstate import SpiderState
+from scrapy.extensions.spiderstate import SpiderState
-DUPEFILTER_CLASS = 'scrapy.dupefilter.RFPDupeFilter'
+DUPEFILTER_CLASS = 'scrapy.dupefilters.RFPDupeFilter'
-from scrapy.dupefilter import RFPDupeFilter
+from scrapy.dupefilters import RFPDupeFilter
-from scrapy.contrib.spiders.sitemap import SitemapSpider
+from scrapy.spiders.crawl import CrawlSpider, Rule
-from scrapy.contrib.spiders import CrawlSpider, Rule, XMLFeedSpider, \
+from scrapy.spiders.init import InitSpider
-from scrapy.contrib.spiders import CrawlSpider
+from scrapy.spiders import CrawlSpider
-SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'
+SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'
-from scrapy.squeue import MarshalFifoDiskQueue, MarshalLifoDiskQueue, PickleFifoDiskQueue, PickleLifoDiskQueue
+from scrapy.squeues import MarshalFifoDiskQueue, MarshalLifoDiskQueue, PickleFifoDiskQueue, PickleLifoDiskQueue
-from scrapy.utils.decorator import defers
+from scrapy.utils.decorators import defers
-from scrapy.utils.decorator import deprecated
+from scrapy.utils.decorators import deprecated
-from scrapy.utils.decorator import deprecated
+from scrapy.utils.decorators import deprecated
-from scrapy.utils.decorator import deprecated
+from scrapy.utils.decorators import deprecated
-              "use `scrapy.loader.processor` instead",
+              "use `scrapy.loader.processors` instead",
-from scrapy.loader.processor import *
+from scrapy.loader.processors import *
-from .processor import Identity
+from .processors import Identity
-from scrapy.loader.processor import Join, Identity, TakeFirst, \
+from scrapy.loader.processors import Join, Identity, TakeFirst, \
-    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware': 900,
+    'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': 50,
-from scrapy.contrib.spidermiddleware.depth import DepthMiddleware
+from scrapy.spidermiddlewares.depth import DepthMiddleware
- 
+
-from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpError
+from scrapy.spidermiddlewares.httperror import HttpErrorMiddleware, HttpError
-from scrapy.contrib.spidermiddleware.offsite import OffsiteMiddleware
+from scrapy.spidermiddlewares.offsite import OffsiteMiddleware
-from scrapy.contrib.spidermiddleware.referer import RefererMiddleware
+from scrapy.spidermiddlewares.referer import RefererMiddleware
-from scrapy.contrib.spidermiddleware.urllength import UrlLengthMiddleware
+from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware
-from scrapy.contrib.pipeline.media import MediaPipeline
+from scrapy.pipelines.media import MediaPipeline
-from scrapy.contrib.pipeline.files import FileException, FilesPipeline
+#TODO: from scrapy.pipelines.media import MediaPipeline
-ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'
+ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'
-from scrapy.contrib.pipeline.files import FilesPipeline, FSFilesStore
+from scrapy.pipelines.files import FilesPipeline, FSFilesStore
-from scrapy.contrib.pipeline.images import ImagesPipeline
+from scrapy.pipelines.images import ImagesPipeline
-from scrapy.contrib.pipeline.media import MediaPipeline
+from scrapy.pipelines.media import MediaPipeline
-from scrapy.contrib.loader.processor import Join, Identity, TakeFirst, \
+from scrapy.loader import ItemLoader
-from scrapy.contrib.loader import ItemLoader
+from scrapy.loader import ItemLoader
-from scrapy.contrib.linkextractors import LinkExtractor
+from scrapy.linkextractors import LinkExtractor
-For actual link extractors implementation see scrapy.contrib.linkextractor, or
+For actual link extractors implementation see scrapy.linkextractors, or
-scrapy.contrib.linkextractor).
+scrapy.linkextractors).
-scrapy.contrib.linkextractors
+scrapy.linkextractors
-            "Please use scrapy.contrib.linkextractors.LinkExtractor",
+            "Please use scrapy.linkextractors.LinkExtractor",
-            "Please use scrapy.contrib.linkextractors.LinkExtractor",
+            "Please use scrapy.linkextractors.LinkExtractor",
-from scrapy.contrib.linkextractors import LinkExtractor
+from scrapy.linkextractors import LinkExtractor
-from scrapy.contrib.linkextractors import LinkExtractor
+from scrapy.linkextractors import LinkExtractor
-from scrapy.contrib.linkextractors.regex import RegexLinkExtractor
+from scrapy.linkextractors.regex import RegexLinkExtractor
-from scrapy.contrib.linkextractors.lxmlhtml import LxmlLinkExtractor
+from scrapy.linkextractors.htmlparser import HtmlParserLinkExtractor
-from scrapy.contrib.linkextractors import LinkExtractor
+from scrapy.linkextractors import LinkExtractor
-    'pickle': 'scrapy.contrib.exporter.PickleItemExporter',
+    'json': 'scrapy.exporters.JsonItemExporter',
-from scrapy.contrib.exporter import (
+from scrapy.exporters import (
-              "use `scrapy.contrib.downloadermiddleware.decompression` instead",
+              "use `scrapy.downloadermiddlewares.decompression` instead",
-from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware
+from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware
-    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,
+    'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,
-    path ie: 'scrapy.contrib.downloadermiddelware.redirect.RedirectMiddleware'
+    path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'
-from scrapy.contrib.downloadermiddleware.ajaxcrawl import AjaxCrawlMiddleware
+from scrapy.downloadermiddlewares.ajaxcrawl import AjaxCrawlMiddleware
-__doctests__ = ['scrapy.contrib.downloadermiddleware.ajaxcrawl']
+__doctests__ = ['scrapy.downloadermiddlewares.ajaxcrawl']
-from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware
+from scrapy.downloadermiddlewares.cookies import CookiesMiddleware
-from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware
+from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware
-    
+
-from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware
+from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware
-from scrapy.contrib.downloadermiddleware.downloadtimeout import DownloadTimeoutMiddleware
+from scrapy.downloadermiddlewares.downloadtimeout import DownloadTimeoutMiddleware
-from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware
+from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware
-from scrapy.contrib.downloadermiddleware.httpcache import HttpCacheMiddleware
+from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware
-from scrapy.contrib.downloadermiddleware.httpcompression import HttpCompressionMiddleware
+from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware
-from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware
+from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware
-from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware, MetaRefreshMiddleware
+from scrapy.downloadermiddlewares.redirect import RedirectMiddleware, MetaRefreshMiddleware
-from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware
+from scrapy.downloadermiddlewares.retry import RetryMiddleware
-from scrapy.contrib.downloadermiddleware.robotstxt import RobotsTxtMiddleware
+from scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware
-from scrapy.contrib.downloadermiddleware.stats import DownloaderStats
+from scrapy.downloadermiddlewares.stats import DownloaderStats
-        
+
-from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware
+from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware
-Scrapy - a web crawling and screen scraping framework written for Python
+Scrapy - a web crawling and web scraping framework written for Python
-    description='A high-level Web Crawling and Screen Scraping framework',
+    description='A high-level Web Crawling and Web Scraping framework',
-from zope.interface.verify import verifyClass
+from zope.interface.verify import verifyClass, DoesNotImplement
-    verifyClass(ISpiderLoader, loader_cls)
+    try:
-        with self.assertRaises(DoesNotImplement):
+        with warnings.catch_warnings(record=True) as w, \
-                logger._log(extra={'spider': spider}, **logkws)
+                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
-                logger._log(extra={'spider': spider}, **logkws)
+                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
-            logger._log(extra={'spider': spider}, **logkws)
+            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
-def msg(message, _level=logging.INFO, **kw):
+def msg(message=None, _level=logging.INFO, **kw):
-    logger.log(level, message, kw)
+    message = kw.pop('format', message)
-    logger.log(level, message, kw, extra={'failure': failure})
+    logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})
-from scrapy.utils.log import FailureFormatter, LogCounterHandler, StreamLogger
+from scrapy.utils.log import (FailureFormatter, TopLevelFormatter,
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-        self.logger = logging.getLogger('scrapy')
+        self.logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-logger = logging.getLogger('scrapy')
+logger = logging.getLogger(__name__)
-            datefmt='%Y-%m-%d %H:%M:%S%z'
+            fmt=settings.get('LOG_FORMAT'),
-See documentation in docs/topics/logging.rst
+This module is kept to provide a helpful warning about its removal.
-    return start_from_settings(crawler.settings, crawler)
+import warnings
-    unittest.main()
+        if settings.getbool('LOG_STDOUT'):
-from scrapy.utils.log import FailureFormatter, LogCounterHandler
+from scrapy.utils.log import FailureFormatter, LogCounterHandler, StreamLogger
-        method to send log messages from your spider
+    @property
-        log.msg(message, spider=self, level=level, **kw)
+        self.logger.log(level, message, **kw)
-        self.log("Got response %d" % response.status)
+        self.logger.info("Got response %d" % response.status)
-        self.log("It Works!")
+        self.logger.debug("It Works!")
-            self.log('It Works!')
+            self.logger.debug('It Works!')
-from scrapy.utils.log import configure_logging, log_scrapy_info
+from scrapy.utils.log import LogCounterHandler, configure_logging, log_scrapy_info
-
+
-from scrapy.utils.log import FailureFormatter
+from scrapy.utils.log import FailureFormatter, LogCounterHandler
-        log.scrapy_info(settings)
+        configure_logging(settings)
-
+# -*- coding: utf-8 -*-
-from scrapy import log, signals
+from scrapy import signals
-                log.msg(spider=spider, **logkws)
+                logger._log(extra={'spider': spider}, **logkws)
-                log.msg(spider=spider, **logkws)
+                logger._log(extra={'spider': spider}, **logkws)
-            log.msg(spider=spider, **logkws)
+            logger._log(extra={'spider': spider}, **logkws)
-from scrapy import log
+SCRAPEDMSG = u"Scraped from %(src)s" + os.linesep + "%(item)s"
-    timestamp
+    """Class for generating log messages for different actions.
-            'flags': flags,
+            'level': logging.DEBUG,
-            'item': item,
+            'level': logging.DEBUG,
-            'item': item,
+            'level': logging.WARNING,
-        logline = logkws['format'] % logkws
+        logline = logkws['msg'] % logkws['args']
-        logline = logkws['format'] % logkws
+        logline = logkws['msg'] % logkws['args']
-        logline = logkws['format'] % logkws
+        logline = logkws['msg'] % logkws['args']
-        logline = logkws['format'] % logkws
+        logline = logkws['msg'] % logkws['args']
-        self.flushLoggedErrors(crawler.spider.exception_cls)
+import logging
-from scrapy.utils.test import get_crawler, get_testlog
+
-        self._assert_retried()
+        with LogCapture() as l:
-        self._assert_retried()
+        with LogCapture() as l:
-            self._assert_retried()
+            with LogCapture() as l:
-        self.assertEqual(len(errors), 1)
+        with LogCapture('scrapy', level=logging.ERROR) as l:
-        self.assertEqual(len(errors), 1)
+        with LogCapture('scrapy', level=logging.ERROR) as l:
-        self.assertEqual(log.count("Got response 200"), 1)
+        with LogCapture() as l:
-        self._assert_retried()
+        with LogCapture() as l:
-        self._assert_retried()
+        with LogCapture() as l:
-        self.assertEqual(log.count("Gave up retrying"), 1)
+    def _assert_retried(self, log):
-        self.flushLoggedErrors()
+        with LogCapture() as l:
-        assert events[0]['failure'] is fail
+        assert len(l.records) == 1
-        self.flushLoggedErrors()
+        with LogCapture() as l:
-        assert len(events) == 0
+        assert len(l.records) == 0
-from scrapy.utils.test import get_testlog, get_crawler
+from scrapy.utils.test import get_crawler
-        self._assert_got_response_code(200)
+        with LogCapture() as l:
-        self._assert_got_response_code(200)
+        with LogCapture() as l:
-        self._assert_got_tunnel_error()
+        with LogCapture() as l:
-        yield crawler.crawl("https://localhost:8999/status?n=200")
+        with LogCapture() as l:
-        self._assert_got_tunnel_error()
+        self._assert_got_tunnel_error(l)
-        self._assert_got_response_code(200)
+        with LogCapture() as l:
-        self._assert_got_response_code(407)
+        with LogCapture() as l:
-        self.assertEqual(log.count('Crawled (%d)' % code), 1)
+    def _assert_got_response_code(self, code, log):
-        self.assertEqual(log.count('TunnelError'), 1)
+    def _assert_got_tunnel_error(self, log):
-from scrapy.utils.test import get_crawler, get_testlog
+from scrapy.utils.test import get_crawler
-        # print(get_testlog())
+        with LogCapture() as log:
-        self.assertNotIn('Ignoring response <402', log)
+        self.assertIn('Ignoring response <404', str(log))
-        self.flushLoggedErrors()
+from testfixtures import LogCapture
-from scrapy import log
+
-            handlers_called=handlers_called)
+        with LogCapture() as l:
-        assert log_received in handlers_called
+        self.assertEqual(len(l.records), 1)
-        self.flushLoggedErrors()
+        with LogCapture() as l:
-from scrapy import log
+
-                    level=log.ERROR, spider=spider.name)
+            logger.error('No CrawlSpider rules found in spider %(spider)r, '
-                        level=log.ERROR, spider=opts.spider)
+                logger.error('Unable to find spider: %(spider)s',
-                        level=log.ERROR, url=url)
+                logger.error('Unable to find spider for: %(url)s',
-                    level=log.ERROR, url=url)
+            logger.error('No response downloaded for: %(url)s',
-                            callback=callback, spider=spider.name, level=log.ERROR)
+                    logger.error('Cannot find callback %(callback)r in spider: %(spider)s',
-from scrapy import log
+import logging
-from scrapy import log
+
-        log.msg(msg)
+        log_args = {
-from scrapy import log
+
-                ajax_crawl_request=ajax_crawl_request, request=request)
+        logger.debug("Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s",
-from scrapy import log
+
-                log.msg(msg, spider=spider, level=log.DEBUG)
+                logger.debug(msg, extra={'spider': spider})
-                log.msg(msg, spider=spider, level=log.DEBUG)
+                logger.debug(msg, extra={'spider': spider})
-and extract the potentially compressed responses that may arrive. 
+and extract the potentially compressed responses that may arrive.
-from scrapy import log
+logger = logging.getLogger('scrapy')
-                        level=log.DEBUG, spider=spider, responsefmt=fmt)
+                logger.debug('Decompressed response with format: %(responsefmt)s',
-from scrapy import log
+logger = logging.getLogger('scrapy')
-                    redirected=redirected, reason=reason)
+            logger.debug("Redirecting (%(reason)s) to %(redirected)s from %(request)s",
-                    level=log.DEBUG, spider=spider, request=request)
+            logger.debug("Discarding %(request)s: max redirections reached",
-from scrapy import log
+logger = logging.getLogger('scrapy')
-                    level=log.DEBUG, spider=spider, request=request, retries=retries, reason=reason)
+            logger.debug("Retrying %(request)s (failed %(retries)d times): %(reason)s",
-                    level=log.DEBUG, spider=spider, request=request, retries=retries, reason=reason)
+            logger.debug("Gave up retrying %(request)s (failed %(retries)d times): %(reason)s",
-from scrapy import signals, log
+logger = logging.getLogger('scrapy')
-                    level=log.DEBUG, request=request)
+            logger.debug("Forbidden by robots.txt: %(request)s",
-                    level=log.ERROR, request=request, spider=spider)
+            logger.error("Error downloading %(request)s: %(f_exception)s",
-import sys, os, posixpath
+import os
-from scrapy import log, signals
+from scrapy import signals
-            slot.itemcount, slot.uri)
+        logfmt = "%%s %(format)s feed (%(itemcount)d items) in: %(uri)s"
-        d.addErrback(log.err, logfmt % "Error storing", spider=spider)
+        d.addCallback(lambda _: logger.info(logfmt % "Stored", log_args,
-        log.msg("Unknown feed format: %s" % format, log.ERROR)
+        logger.error("Unknown feed format: %(format)s", {'format': format})
-                log.msg("Disabled feed storage scheme: %s" % scheme, log.ERROR)
+                logger.error("Disabled feed storage scheme: %(scheme)s",
-            log.msg("Unknown feed storage scheme: %s" % scheme, log.ERROR)
+            logger.error("Unknown feed storage scheme: %(scheme)s",
-from scrapy import log, signals
+from scrapy import signals
-        log.msg(msg, spider=spider)
+
-from scrapy import signals, log
+from scrapy import signals
-                    level=log.ERROR, memusage=mem)
+            logger.error("Memory usage exceeded %(memusage)dM. Shutting down Scrapy...",
-                    level=log.WARNING, memusage=mem)
+            logger.warning("Memory usage reached %(memusage)dM",
-from scrapy import log
+logger = logging.getLogger('scrapy')
-                    medianame=self.MEDIA_NAME, request=request, referer=referer)
+            logger.debug(
-        dfd.addErrback(log.err, self.__class__.__name__ + '.store.stat_file')
+        dfd.addErrback(
-                    medianame=self.MEDIA_NAME, request=request, referer=referer)
+            logger.warning(
-                    status=response.status, request=request, referer=referer)
+            logger.warning(
-                    request=request, referer=referer)
+            logger.warning(
-                status=status, request=request, referer=referer)
+        logger.debug(
-                    request=request, referer=referer, errormsg=str(exc))
+            logger.warning(
-            log.err(None, whyfmt % {'request': request, 'referer': referer}, spider=info.spider)
+            logger.exception(
-from scrapy import log
+logger = logging.getLogger('scrapy')
-        dfd.addErrback(log.err, spider=info.spider)
+        dfd.addErrback(lambda f: logger.error(
-                    log.err(value, msg, spider=info.spider)
+                    logger.error(
-from scrapy import log
+import logging
-                            maxdepth=self.maxdepth, requrl=request.url)
+                    logger.debug("Ignoring link (depth > %(maxdepth)d): %(requrl)s ",
-from scrapy import log
+
-                response=response
+            logger.debug(
-from scrapy import log
+
-                                level=log.DEBUG, spider=spider, domain=domain, request=x)
+                        logger.debug("Filtered offsite request to %(domain)r: %(request)s",
-from scrapy import log
+import logging
-                        maxlength=self.maxlength, url=request.url)
+                logger.debug("Ignoring link (url length > %(maxlength)d): %(url)s ",
-from scrapy import log
+
-                        level=log.WARNING, spider=self, response=response)
+                logger.warning("Ignoring invalid sitemap: %(response)s",
-            spider.log(msg, level=logging.INFO)
+            logger.info(
-
+import logging
-from scrapy import log, twisted_version
+from scrapy import twisted_version
-                    logLevel=log.ERROR)
+            logger.error("Expected response size (%(size)s) larger than "
-                    logLevel=log.WARNING)
+            logger.warning("Expected response size (%(size)s) larger than "
-                    logLevel=log.ERROR)
+            logger.error("Received (%(bytes)s) bytes larger than download "
-                    logLevel=log.WARNING)
+            logger.warning("Received (%(bytes)s) bytes larger than download "
-            except Exception as exc:
+            except Exception:
-                        spider=spider)
+                logger.exception('Error while obtaining start requests',
-        d.addErrback(log.msg, spider=spider)
+        d.addErrback(lambda f: logger.info('Error while handling downloader output',
-        d.addErrback(log.msg, spider=spider)
+        d.addErrback(lambda f: logger.info('Error while removing request from slot',
-        d.addErrback(log.msg, spider=spider)
+        d.addErrback(lambda f: logger.info('Error while scheduling new request',
-        d.addErrback(log.err, spider=spider)
+        d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',
-        log.msg("Spider opened", spider=spider)
+        logger.info("Spider opened", extra={'spider': spider})
-        log.msg(format="Closing spider (%(reason)s)", reason=reason, spider=spider)
+        logger.info("Closing spider (%(reason)s)",
-        dfd.addErrback(log.err, spider=spider)
+        dfd.addErrback(log_failure('Downloader close failure'))
-        dfd.addErrback(log.err, spider=spider)
+        dfd.addErrback(log_failure('Scraper close failure'))
-        dfd.addErrback(log.err, spider=spider)
+        dfd.addErrback(log_failure('Scheduler close failure'))
-        dfd.addErrback(log.err, spider=spider)
+        dfd.addErrback(log_failure('Error while sending spider_close signal'))
-        dfd.addErrback(log.err, spider=spider)
+        dfd.addErrback(log_failure('Stats close failure'))
-        dfd.addBoth(lambda _: log.msg(format="Spider closed (%(reason)s)", reason=reason, spider=spider))
+        dfd.addBoth(lambda _: logger.info("Spider closed (%(reason)s)",
-        dfd.addErrback(log.err, spider=spider)
+        dfd.addErrback(log_failure('Error while unassigning slot'))
-        dfd.addErrback(log.err, spider=spider)
+        dfd.addErrback(log_failure('Error while unassigning spider'))
-from scrapy import log
+
-                        request=request, reason=e)
+                logger.exception("Unable to serialize request: %(request)s - reason: %(reason)s",
-                    spider=self.spider, queuesize=len(q))
+            logger.info("Resuming crawl (%(queuesize)d requests scheduled)",
-            log.err, 'Scraper bug processing %s' % request, spider=spider)
+            lambda f: logger.error('Scraper bug processing %(request)s',
-            spider=spider
+        logger.error(
-                    level=log.ERROR, spider=spider, request=request, typename=typename)
+            logger.error('Spider must return Request, BaseItem, dict or None, '
-                        spider=spider)
+                logger.error('Error downloading %(request)s',
-                            errmsg=errmsg)
+                    logger.error('Error downloading %(request)s: %(errmsg)s',
-                log.err(output, 'Error processing %s' % item, spider=spider)
+                logger.error('Error processing %(item)s', {'item': item},
-from scrapy import log, signals
+from scrapy import signals
-                level=log.INFO, signame=signame)
+        logger.info("Received %(signame)s, shutting down gracefully. Send again to force ",
-                level=log.INFO, signame=signame)
+        logger.info('Received %(signame)s twice, forcing unclean shutdown',
-from scrapy import log
+        self.logger = logging.getLogger('scrapy')
-            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)
+            msg = "Filtered duplicate request: %(request)s"
-            fmt = ("Filtered duplicate request: %(request)s"
+            msg = ("Filtered duplicate request: %(request)s"
-            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)
+            self.logger.debug(msg, {'request': request}, extra={'spider': spider})
-from scrapy import log
+logger = logging.getLogger('scrapy')
-                    level=log.DEBUG, mailto=to, mailcc=cc, mailsubject=subject, mailattachs=len(attachs))
+            logger.debug('Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s '
-                mailto=to, mailcc=cc, mailsubject=subject, mailattachs=nattachs)
+        logger.info('Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '
-                mailattachs=nattachs, mailerr=errstr)
+        logger.error('Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '
-from scrapy import log
+logger = logging.getLogger('scrapy')
-                            level=log.WARNING, clsname=clsname, eargs=e.args[0])
+                    logger.warning("Disabled %(clsname)s: %(eargs)s",
-                componentname=cls.component_name, enabledlist=', '.join(enabled))
+        logger.info("Enabled %(componentname)ss: %(enabledlist)s",
-                spider=spider)
+            logger.info("Dumping Scrapy stats:\n" + pprint.pformat(self._stats),
-from scrapy import log, signals
+from scrapy import signals
-                level=log.DEBUG, host=h.host, port=h.port)
+        logger.debug("Telnet console listening on %(host)s:%(port)d",
-import re, csv, six
+import re
-from scrapy import log
+logger = logging.getLogger('scrapy')
-                    level=log.WARNING, csvlnum=csv_r.line_num, csvrow=len(row), csvheader=len(headers))
+            logger.warning("ignoring row %(csvlnum)d (length: %(csvrow)d, "
-from scrapy import log
+logger = logging.getLogger('scrapy')
-                        level=log.ERROR, spider=spider, receiver=receiver)
+                logger.error("Cannot return deferreds from signal handler: %(receiver)s",
-                spider=spider)
+            logger.exception("Error caught on signal handler: %(receiver)s",
-                spider=spider)
+            logger.error("Error caught on signal handler: %(receiver)s",
-from scrapy import log
+logger = logging.getLogger('scrapy')
-                level=log.ERROR, request=request, snames=', '.join(snames))
+        logger.error('More than one spider can handle: %(request)s - %(snames)s',
-                level=log.ERROR, request=request)
+        logger.error('Unable to find spider that handles: %(request)s',
-        self.assertIn("[myspider] INFO: Spider closed (finished)", log)
+        self.assertIn("DEBUG: It Works!", log)
-from scrapy import log
+import logging
-        log.msg('It Works!')
+        logging.info('It Works!')
-        self.assertIn("[parse_spider] DEBUG: It Works!", stderr)
+        self.assertIn("DEBUG: It Works!", stderr)
-        self.assertIn("[scrapy] INFO: It Works!", stderr)
+        self.assertIn("INFO: It Works!", stderr)
-        for n, v in six.iteritems(attrs):
+        for n in dir(_class):
-                new_attrs[n] = v
+            elif n in attrs:
-        return cls
+        new_attrs['fields'] = fields
-                            settings.get('SPIDER_MANAGER_CLASS'))
+    cls_path = settings.get('SPIDER_MANAGER_CLASS',
-        for field_name in self._values:
+        for field_name in tuple(self._values):
-        spiders = self.crawler_process.spiders
+        spider_loader = self.crawler_process.spider_loader
-            spidercls = spiders.load(spidername)
+        for spidername in args or spider_loader.list():
-            spidercls = self.crawler_process.spiders.load(args[0])
+            spidercls = self.crawler_process.spider_loader.load(args[0])
-        spiders = self.crawler_process.spiders
+        spider_loader = self.crawler_process.spider_loader
-            spidercls = spiders.load(opts.spider)
+            spidercls = spider_loader.load(opts.spider)
-            spidercls = spidercls_for_request(spiders, request, spidercls)
+            spidercls = spidercls_for_request(spider_loader, request, spidercls)
-            spidercls = self.crawler_process.spiders.load(name)
+            spidercls = self.crawler_process.spider_loader.load(name)
-        for s in sorted(self.crawler_process.spiders.list()):
+        for s in sorted(self.crawler_process.spider_loader.list()):
-        spiders = self.crawler_process.spiders
+        spider_loader = self.crawler_process.spider_loader
-                self.spidercls = spiders.load(opts.spider)
+                self.spidercls = spider_loader.load(opts.spider)
-            self.spidercls = spidercls_for_request(spiders, Request(url))
+            self.spidercls = spidercls_for_request(spider_loader, Request(url))
-        spiders = self.crawler_process.spiders
+        spider_loader = self.crawler_process.spider_loader
-            spidercls = spiders.load(opts.spider)
+            spidercls = spider_loader.load(opts.spider)
-            spidercls = spidercls_for_request(spiders, Request(url),
+            spidercls = spidercls_for_request(spider_loader, Request(url),
-                          "CrawlerRunner.spiders or instantiate "
+                          "CrawlerRunner.spider_loader or instantiate "
-        self.spiders = _get_spider_loader(settings)
+        self.spider_loader = _get_spider_loader(settings)
-            spidercls = self.spiders.load(spidercls)
+            spidercls = self.spider_loader.load(spidercls)
-        Return the list of spiders names that can handle the given request.
+        Return the list of spider names that can handle the given request.
-def spidercls_for_request(spiderloader, request, default_spidercls=None,
+def spidercls_for_request(spider_loader, request, default_spidercls=None,
-    snames = spiderloader.find_by_request(request)
+    snames = spider_loader.find_by_request(request)
-        return spiderloader.load(snames[0])
+        return spider_loader.load(snames[0])
-            self.assertIsInstance(spiders, sm_cls)
+            sl_cls = load_object(self.crawler.settings['SPIDER_LOADER_CLASS'])
-        self.spiderloader = SpiderLoader.from_settings(settings)
+        self.spider_loader = SpiderLoader.from_settings(settings)
-        del self.spiderloader
+        del self.spider_loader
-        verifyObject(ISpiderLoader, self.spiderloader)
+        verifyObject(ISpiderLoader, self.spider_loader)
-        self.assertEqual(set(self.spiderloader.list()),
+        self.assertEqual(set(self.spider_loader.list()),
-        spider1 = self.spiderloader.load("spider1")
+        spider1 = self.spider_loader.load("spider1")
-        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy1.org/test')),
+        self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy1.org/test')),
-        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy2.org/test')),
+        self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy2.org/test')),
-        self.assertEqual(set(self.spiderloader.find_by_request(Request('http://scrapy3.org/test'))),
+        self.assertEqual(set(self.spider_loader.find_by_request(Request('http://scrapy3.org/test'))),
-        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy999.org/test')),
+        self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy999.org/test')),
-        self.assertEqual(self.spiderloader.find_by_request(Request('http://spider3.com')),
+        self.assertEqual(self.spider_loader.find_by_request(Request('http://spider3.com')),
-        self.assertEqual(self.spiderloader.find_by_request(Request('http://spider3.com/onlythis')),
+        self.assertEqual(self.spider_loader.find_by_request(Request('http://spider3.com/onlythis')),
-        assert len(self.spiderloader._spiders) == 1
+        self.spider_loader = SpiderLoader.from_settings(settings)
-        assert len(self.spiderloader._spiders) == 2
+        self.spider_loader = SpiderLoader.from_settings(settings)
-        assert len(self.spiderloader._spiders) == 0
+        self.spider_loader = SpiderLoader.from_settings(settings)
-from scrapy.interfaces import ISpiderManager
+from scrapy.interfaces import ISpiderLoader
-                          "scrapy.spidermanager.SpiderManager with your "
+                          "scrapy.spiderloader.SpiderLoader with your "
-            self._spiders = spman_cls.from_settings(self.settings)
+            self._spiders = _get_spider_loader(self.settings.frozencopy())
-        self.spiders = smcls.from_settings(settings.frozencopy())
+        self.spiders = _get_spider_loader(settings)
-class ISpiderManager(Interface):
+class ISpiderLoader(Interface):
-        """Returns an instance of the class for the given settings"""
+        """Return an instance of the class for the given settings"""
-        """Returns the Spider class for the given spider name. If the spider
+        """Return the Spider class for the given spider name. If the spider
-        """Returns the list of spiders names that can handle the given request"""
+        """Return the list of spiders names that can handle the given request"""
-SPIDER_MANAGER_CLASS = 'scrapy.spidermanager.SpiderManager'
+SPIDER_LOADER_CLASS = 'scrapy.spiderloader.SpiderLoader'
-""")
+spiders = ObsoleteClass(
-spiders
+Backwards compatibility shim. Use scrapy.spiderloader instead.
-        return list(self._spiders.keys())
+SpiderManager = create_deprecated_class('SpiderManager', SpiderLoader)
-def spidercls_for_request(spidermanager, request, default_spidercls=None,
+def spidercls_for_request(spiderloader, request, default_spidercls=None,
-    the spider manager) and return a Spider class if (and only if) there is
+    the spider loader) and return a Spider class if (and only if) there is
-    snames = spidermanager.find_by_request(request)
+    snames = spiderloader.find_by_request(request)
-        return spidermanager.load(snames[0])
+        return spiderloader.load(snames[0])
-            sm_cls = load_object(self.crawler.settings['SPIDER_MANAGER_CLASS'])
+            sm_cls = load_object(self.crawler.settings['SPIDER_LOADER_CLASS'])
-def SpiderManagerWithWrongInterface(object):
+class SpiderLoaderWithWrongInterface(object):
-            'SPIDER_MANAGER_CLASS': 'tests.test_crawler.SpiderManagerWithWrongInterface'
+            'SPIDER_LOADER_CLASS': 'tests.test_crawler.SpiderLoaderWithWrongInterface'
-from scrapy.spidermanager import SpiderManager
+from scrapy.interfaces import ISpiderLoader
-class SpiderManagerTest(unittest.TestCase):
+
-        self.spiderman = SpiderManager.from_settings(settings)
+        self.spiderloader = SpiderLoader.from_settings(settings)
-        del self.spiderman
+        del self.spiderloader
-        verifyObject(ISpiderManager, self.spiderman)
+        verifyObject(ISpiderLoader, self.spiderloader)
-        self.assertEqual(set(self.spiderman.list()),
+        self.assertEqual(set(self.spiderloader.list()),
-        spider1 = self.spiderman.load("spider1")
+        spider1 = self.spiderloader.load("spider1")
-        self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy1.org/test')),
+        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy1.org/test')),
-        self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy2.org/test')),
+        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy2.org/test')),
-        self.assertEqual(set(self.spiderman.find_by_request(Request('http://scrapy3.org/test'))),
+        self.assertEqual(set(self.spiderloader.find_by_request(Request('http://scrapy3.org/test'))),
-        self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy999.org/test')),
+        self.assertEqual(self.spiderloader.find_by_request(Request('http://scrapy999.org/test')),
-        self.assertEqual(self.spiderman.find_by_request(Request('http://spider3.com')),
+        self.assertEqual(self.spiderloader.find_by_request(Request('http://spider3.com')),
-        self.assertEqual(self.spiderman.find_by_request(Request('http://spider3.com/onlythis')),
+        self.assertEqual(self.spiderloader.find_by_request(Request('http://spider3.com/onlythis')),
-        module = 'tests.test_spidermanager.test_spiders.spider1'
+        module = 'tests.test_spiderloader.test_spiders.spider1'
-        assert len(self.spiderman._spiders) == 1
+        self.spiderloader = SpiderLoader.from_settings(settings)
-        prefix = 'tests.test_spidermanager.test_spiders.'
+        prefix = 'tests.test_spiderloader.test_spiders.'
-        assert len(self.spiderman._spiders) == 2
+        self.spiderloader = SpiderLoader.from_settings(settings)
-        module = 'tests.test_spidermanager.test_spiders.spider0'
+        module = 'tests.test_spiderloader.test_spiders.spider0'
-        assert len(self.spiderman._spiders) == 0
+        self.spiderloader = SpiderLoader.from_settings(settings)
-See documentation in topics/images.rst
+See documentation in topics/media-pipeline.rst
-    def test_from_response_formid_notexists_fallback_formname(self):
+    def test_from_response_formname_notexists_fallback_formid(self):
-        r1 = self.request_class.from_response(response, formid="form3", formname="form2")
+        r1 = self.request_class.from_response(response, formname="form3", formid="form2")
-from w3lib import html
+
-_ajax_crawlable_re = re.compile(ur'<meta\s+name=["\']fragment["\']\s+content=["\']!["\']/?>')
+_ajax_crawlable_re = re.compile(six.u(r'<meta\s+name=["\']fragment["\']\s+content=["\']!["\']/?>'))
-from twisted.conch.insults import insults
+try:
-copyright = u'2008-2014, Scrapy developers'
+copyright = u'2008-2015, Scrapy developers'
-html_style = 'scrapydoc.css'
+# html_style = 'scrapydoc.css'
-        exporter = self._get_exporter(file)
+        exporter = self._get_exporter(file, fields_to_export=self.export_fields)
-        return self.exporters[self.format](*a, **kw)
+    def _get_exporter(self, *args, **kwargs):
-from scrapy.contrib.feedexport import IFeedStorage, FileFeedStorage, FTPFeedStorage, S3FeedStorage, StdoutFeedStorage
+import scrapy
-        spider = Spider("default")
+        spider = scrapy.Spider("default")
-        spider = Spider("default")
+        spider = scrapy.Spider("default")
-        file = storage.open(Spider("default"))
+        file = storage.open(scrapy.Spider("default"))
-        file = storage.open(Spider("default"))
+        file = storage.open(scrapy.Spider("default"))
-from scrapy.settings import Settings
+from scrapy.settings import Settings, default_settings
-            o = cls(crawler.settings)
+        o = cls(crawler.settings)
-    cwd = os.getcwd() # trial chdirs to temp dir
+    cwd = os.getcwd()  # trial chdirs to temp dir
-        self.assert_(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))
+        self.assertIn("Created spider %r using template %r in module" % (spname, tplname), out)
-        self.assert_("Spider %r already exists in module" % spname in out)
+        self.assertIn("Spider %r already exists in module" % spname, out)
-        self.assert_("[myspider] INFO: Spider closed (finished)" in log, log)
+        self.assertIn("[myspider] DEBUG: It Works!", log)
-        self.assert_("No spider found in file" in log)
+        self.assertIn("No spider found in file", log)
-        self.assert_("File not found: some_non_existent_file" in log)
+        self.assertIn("File not found: some_non_existent_file", log)
-        self.assert_("Unable to load" in log)
+        self.assertIn("Unable to load", log)
-        self.assert_("[parse_spider] DEBUG: It Works!" in stderr, stderr)
+        self.assertIn("[parse_spider] DEBUG: It Works!", stderr)
-        self.assert_("[scrapy] INFO: It Works!" in stderr, stderr)
+        self.assertIn("[scrapy] INFO: It Works!", stderr)
-        self.assert_('INFO: Crawled' in log, log)
+        self.assertIn('INFO: Crawled', log)
-]
+EXTENSIONS = {
-def _get_form(response, formname, formnumber, formxpath):
+def _get_form(response, formname, formid, formnumber, formxpath):
-            raise ValueError("Invalid XPath: %s" % query)
+            msg = u"Invalid XPath: %s" % query
-            raise AssertionError("A invalid XPath does not raise an exception")
+        self.assertRaisesRegexp(ValueError, re.escape(xpath), x.xpath, xpath)
-        self._setup_crawler_logging(crawler)
+    def crawl(self, crawler_or_spidercls, *args, **kwargs):
-    def extract_first(self):
+    def extract_first(self, default=None):
-            return x.extract()
+            return x.extract() or default
-def _add_auth_header(request, target):
+def _add_auth_header(req, target):
-        request.add_header('Authorization', basic_auth_header(u, p))
+        req.add_header('Authorization', basic_auth_header(u, p))
-            request.add_header('Authorization', basic_auth_header(a[0], a[2]))
+            req.add_header('Authorization', basic_auth_header(a[0], a[2]))
-def _http_post(request):
+def _http_post(req):
-        f = request.urlopen(request)
+        f = request.urlopen(req)
-"""
+import warnings
-        return response
+from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware
-from scrapy.selector import Selector
+import warnings
-        return self._text[s:e].encode('utf-8')
+from scrapy.utils.iterators import xmliter_lxml
-from scrapy.contrib_exp.downloadermiddleware.decompression import DecompressionMiddleware
+from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware
-from scrapy.contrib_exp.iterators import xmliter_lxml
+from scrapy.utils.iterators import csviter, xmliter, _body_or_str, xmliter_lxml
-    def from_response(cls, response, formname=None, formnumber=0, formdata=None,
+    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,
-        form = _get_form(response, formname, formnumber, formxpath)
+        form = _get_form(response, formname, formid, formnumber, formxpath)
-
+        cache_size = self.settings.getint('DNSCACHE_SIZE') if self.settings.getbool('DNSCACHE_ENABLED') else 0
-# TODO: make cache size a setting
+    def __init__(self, reactor, cache_size, timeout):
-    def getHostByName(self, name, timeout = (1, 3, 11, 45)):
+    def getHostByName(self, name, timeout=None):
-        self.settings = settings
+        self.settings = settings.copy()
-        return Crawler(spidercls, crawler_settings)
+        return Crawler(spidercls, self.settings)
-                                             priority='project')
+        settings = Settings()
-        yield d
+
-import urllib2
+from six.moves.urllib import request
-        urllib2.install_opener(urllib2.build_opener(HTTPRedirectHandler))
+        request.install_opener(request.build_opener(HTTPRedirectHandler))
-            req = urllib2.Request(_url(target, 'listprojects.json'))
+            req = request.Request(_url(target, 'listprojects.json'))
-            f = urllib2.urlopen(req)
+            f = request.urlopen(req)
-    req = urllib2.Request(url, body, headers)
+    req = request.Request(url, body, headers)
-        f = urllib2.urlopen(request)
+        f = request.urlopen(request)
-    except urllib2.HTTPError as e:
+    except HTTPError as e:
-    except urllib2.URLError as e:
+    except URLError as e:
-class HTTPRedirectHandler(urllib2.HTTPRedirectHandler):
+class HTTPRedirectHandler(request.HTTPRedirectHandler):
-            return urllib2.Request(newurl,
+            return request.Request(newurl,
-            return urllib2.Request(newurl,
+            return request.Request(newurl,
-            raise urllib2.HTTPError(req.get_full_url(), code, msg, headers, fp)
+            raise HTTPError(req.get_full_url(), code, msg, headers, fp)
-from urllib2 import _parse_proxy
+from six.moves.urllib.request import getproxies, proxy_bypass
-from six.moves.urllib.parse import urljoin
+from six.moves.urllib.parse import urljoin, urlencode
-    return urllib.urlencode(values, doseq=1)
+    return urlencode(values, doseq=1)
-import urllib
+                                    urlparse, parse_qsl, urlencode,
-    query = urllib.urlencode(keyvals)
+    query = urlencode(keyvals)
-    return urllib.unquote(path)
+    return unquote(path)
-import sys, time, random, urllib, os, json
+import sys, time, random, os, json
-            argstr = urllib.urlencode(args, doseq=True)
+            argstr = urlencode(args, doseq=True)
-from pdb import set_trace
+from pdb import set_trace
-            raise TypeError("Response body must either str or unicode. Got: '%s'" \
+            raise TypeError("Response body must either be str or unicode. Got: '%s'" \
-from scrapy.http import HtmlResponse, TextResponse
+    from scrapy.http import HtmlResponse, TextResponse
-
+        
-
+        # Content-Length must be specified in POST method even with no body
-        for el in iflatten((x.re(regex) for x in self)):
+        for el in iflatten(x.re(regex) for x in self):
-from scrapy.utils.python import unicode_to_str, flatten
+from scrapy.utils.python import unicode_to_str, flatten, iflatten
-    result = []
+    return list(iflatten(x))
-            result.extend(flatten(el))
+            for el_ in flatten(el):
-    return result
+            yield el
-            return x
+        for x in self:
-Scrapy - a screen scraping framework written in Python
+Scrapy - a web crawling and screen scraping framework written for Python
-    description='A high-level Python Screen Scraping framework',
+    description='A high-level Web Crawling and Screen Scraping framework',
-            if isinstance(x, BaseItem):
+            if isinstance(x, (BaseItem, dict)):
-        'items': BaseItem,
+        'item': (BaseItem, dict),
-            if isinstance(x, BaseItem):
+            if isinstance(x, (BaseItem, dict)):
-        serialized_value)
+        """Return the fields to export as an iterable of tuples
-            if include_empty:
+            if include_empty and not isinstance(item, dict):
-                              nonempty_fields)
+                field_iter = (x for x in self.fields_to_export if x in item)
-                field = item.fields[field_name]
+                field = {} if isinstance(item, dict) else item.fields[field_name]
-                self.fields_to_export = item.fields.keys()
+                if isinstance(item, dict):
-        if self.FILES_RESULT_FIELD in item.fields:
+        if isinstance(item, dict) or self.FILES_RESULT_FIELD in item.fields:
-        if self.IMAGES_RESULT_FIELD in item.fields:
+        if isinstance(item, dict) or self.IMAGES_RESULT_FIELD in item.fields:
-        elif isinstance(output, BaseItem):
+        elif isinstance(output, (BaseItem, dict)):
-            log.msg(format='Spider must return Request, BaseItem or None, '
+            log.msg(format='Spider must return Request, BaseItem, dict or None, '
-class MySpider(Spider):
+class MySpider(scrapy.Spider):
-from scrapy.item import Item
+import scrapy
-class MySpider(Spider):
+class MySpider(scrapy.Spider):
-        return [Item()]
+        return [scrapy.Item(), dict(foo='bar')]
-import unittest, json
+from __future__ import absolute_import
-    JsonItemExporter, PythonItemExporter
+from scrapy.contrib.exporter import (
-    def test_export_item(self):
+    def assertItemExportWorks(self, item):
-            self.ie.export_item(self.i)
+            self.ie.export_item(item)
-            self.ie.serialize_field(self.i.fields['age'], 'age', self.i['age']), '22')
+        res = self.ie.serialize_field(self.i.fields['name'], 'name', self.i['name'])
-        i2 = TestItem(name=u'Maria', age=i1)
+        i2 = dict(name=u'Maria', age=i1)
-        ie = CsvItemExporter(output, fields_to_export=self.i.fields.keys())
+    def assertExportResult(self, item, expected, **kwargs):
-        ie.export_item(self.i)
+        ie.export_item(item)
-        self.assertCsvEqual(output.getvalue(), '22,John\xc2\xa3\r\n')
+        self.assertCsvEqual(fp.getvalue(), expected)
-        self.assertCsvEqual(output.getvalue(), '"Mary,Paul",John\r\n')
+        for cls in TestItem2, dict:
-        self.assertXmlEquivalent(output.getvalue(), expected_value)
+        self.assertExportResult(
-        i2 = TestItem(name=u'bar', age=i1)
+        i2 = dict(name=u'bar', age=i1)
-        self.assertXmlEquivalent(output.getvalue(), expected_value)
+
-        i2 = TestItem(name=u'bar')
+        i2 = dict(name=u'bar', v2={"egg": ["spam"]})
-        self.assertXmlEquivalent(output.getvalue(), expected_value)
+
-        i2 = TestItem(name=u'Maria', age=i1)
+        i2 = dict(name=u'Maria', age=i1)
-    def test_two_items(self):
+    def assertTwoItemsExported(self, item):
-        self.ie.export_item(self.i)
+        self.ie.export_item(item)
-        self.assertEqual(exported, [dict(self.i), dict(self.i)])
+        self.assertEqual(exported, [dict(item), dict(item)])
-                        name, value)
+                    return super(CustomItemExporter, self).serialize_field(field, name, value)
-            ie.serialize_field(i.fields['age'], 'age', i['age']), '23')
+        self.assertEqual(ie.serialize_field(i.fields['name'], 'name', i['name']), 'John')
-        item = TestItem()
+        item = self.item_cls()
-    def __init__(self, with_dupefilter=False):
+    def __init__(self, spider_class):
-            TestDupeFilterSpider
+        self.spider_class = spider_class
-        self.run = CrawlerRun(with_dupefilter=True)
+
-        self.assertEqual(item['files'], [results[0][1]])
+
-        self.assertEqual(item['stored_file'], [results[0][1]])
+
-        self.assertEqual(item['images'], [results[0][1]])
+
-        self.assertEqual(item['stored_image'], [results[0][1]])
+
-            msg = '%s found errors proessing %s' % (self.__class__.__name__, item)
+            msg = '%s found errors processing %s' % (self.__class__.__name__, item)
-from scrapy.http import Request
+
-                raise TypeError('You cannot return an "%s" object from a spider' % type(ret).__name__)
+            ret = iterate_spider_output(self.parse_row(response, row))
-    return [result] if isinstance(result, BaseItem) else arg_to_iter(result)
+    return arg_to_iter(result)
-		See documentation of the _Anonymous class.
+    Any -- Singleton used to signal either "Any Sender" or
-		vs. the original code.)
+    WEAKREF_TYPES -- tuple of types/classes which represent
-		return self.__class__.__name__
+    """Used to represent default parameter values."""
-	"""Singleton used to signal either "Any Sender" or "Any Signal"
+    """Singleton used to signal either "Any Sender" or "Any Signal"
-	"""
+
-	"""
+    """Singleton used to signal "Anonymous Sender"
-
+    """Connect receiver to sender for signal
-		return []
+    """Disconnect receiver from sender for signal
-					pass
+    """Filter sequence of receivers to get resolved, live receivers
-	
+    """Send signal from sender to all connected receivers.
-					_cleanupConnections(senderkey, signal)
+    """Remove receiver from connections."""
-					_removeSender(senderkey)
+    """Delete any empty signals for senderkey. Delete senderkey if empty."""
-			_killBackref( receiver, senderkey )
+    """Remove senderkey from connections."""
-	return True
+    """Kill old sendersBack references from receiver
-	"""Base class for all Dispatcher errors"""
+    """Base class for all Dispatcher errors"""
-	"""Error raised when unknown (sender,signal) set specified"""
+    """Error raised when unknown (sender,signal) set specified"""
-	"""Error raised when inappropriate signal-type specified (None)"""
+    """Error raised when inappropriate signal-type specified (None)"""
-	return responses
+
-
+
-           hasattr(receiver.__call__, 'im_code'):
+                hasattr(receiver.__call__, 'im_code'):
-    if hasattr( receiver, 'im_func' ):
+    if hasattr(receiver, 'im_func'):
-        raise ValueError('unknown receiver type %s %s'%(receiver, type(receiver)))
+        raise ValueError(
-    for name in codeObject.co_varnames[startIndex:startIndex+len(arguments)]:
+    acceptable = codeObject.co_varnames[
-                """Argument %r specified both positionally and as a keyword for calling %r"""% (
+                """Argument %r specified both positionally and as a keyword for calling %r""" % (
-        # fc does not have a **kwds type parameter, therefore 
+        # fc does not have a **kwds type parameter, therefore
-		return weakref.ref( target )
+
-		return None
+    """'Safe' and reusable weak references to instance methods
-import mock
+from tests import mock
-import mock
+from tests import mock
-import mock
+from tests import mock
-    import mock
+from tests import mock
-    import mock
+from tests import mock
-import mock
+from tests import mock
-    Compose, MapCompose
+    Compose, MapCompose, SelectJmes
-
+class SelectJmesTestCase(unittest.TestCase):
-from scrapy import log
+from scrapy import log, twisted_version
-            self._bindAddress)
+    if twisted_version >= (15, 0, 0):
-            collect_ignore.append(filePath)
+        file_path = line.strip()
-            collect_ignore.append(fn.strip())
+    for line in open('tests/py3-ignores.txt'):
-This module implements the FormRequest class which is a more covenient class
+This module implements the FormRequest class which is a more convenient class
-        with open(os.path.join(rpath, 'response_body'), 'rb') as f:
+        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:
-        with open(os.path.join(rpath, 'response_headers'), 'rb') as f:
+        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:
-        with open(os.path.join(rpath, 'meta'), 'wb') as f:
+        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:
-        with open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:
+        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:
-        with open(os.path.join(rpath, 'response_headers'), 'wb') as f:
+        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:
-        with open(os.path.join(rpath, 'response_body'), 'wb') as f:
+        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:
-        with open(os.path.join(rpath, 'request_headers'), 'wb') as f:
+        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:
-        with open(os.path.join(rpath, 'request_body'), 'wb') as f:
+        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:
-        with open(metapath, 'rb') as f:
+        with self._open(metapath, 'rb') as f:
-class ParseCommandTest(CommandTest):
+class ParseCommandTest(ProcessTest, SiteTest, CommandTest):
-        self.assert_("[parse_spider] DEBUG: It Works!" in log, log)
+        _, _, stderr = yield self.execute(['--spider', self.spider_name,
-        self.assert_("[scrapy] INFO: It Works!" in log, log)
+        _, _, stderr = yield self.execute(['--spider', self.spider_name,
-        return self._pool.closeCachedConnections()
+        d = self._pool.closeCachedConnections()
-        d = ThreadedResolver.getHostByName(self, name, timeout)
+        d = super(CachingThreadedResolver, self).getHostByName(name, timeout)
-def xmliter_lxml(obj, nodename, namespace=None):
+def xmliter_lxml(obj, nodename, namespace=None, prefix='x'):
-    selxpath = '//' + ('x:%s' % nodename if namespace else nodename)
+    selxpath = '//' + ('%s:%s' % (prefix, nodename) if namespace else nodename)
-            xs.register_namespace('x', namespace)
+            xs.register_namespace(prefix, namespace)
-                 unique=True, process_value=None, deny_extensions=None):
+                 tags=('a', 'area'), attrs=('href',), canonicalize=True,
-            canonicalize, deny_extensions)
+        super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,
-                 process_value=None, deny_extensions=None):
+                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True,
-            canonicalize, deny_extensions)
+        super(SgmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,
-                 restrict_xpaths, restrict_css, canonicalize, deny_extensions):
+                 restrict_xpaths, canonicalize, deny_extensions, restrict_css):
-                 deny_extensions=None):
+                 restrict_css=(), tags=('a', 'area'), attrs=('href',), canonicalize=True,
-            deny_extensions)
+            allow_domains, deny_domains, restrict_xpaths, restrict_css,
-                 deny_extensions=None):
+                 restrict_css=(), tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True,
-            deny_extensions)
+            allow_domains, deny_domains, restrict_xpaths, restrict_css,
-                 restrict_xpaths, canonicalize, deny_extensions):
+                 restrict_xpaths, restrict_css, canonicalize, deny_extensions):
-                query_args=p.query,
+                key=unquote(p.path),
-    def __init__(self):
+    def __init__(self, with_dupefilter=False):
-        start_urls = [self.geturl("/"), self.geturl("/redirect")]
+        start_urls = [self.geturl("/"), self.geturl("/redirect"),
-        self.crawler = get_crawler(TestSpider)
+        self.crawler = get_crawler(self.spider_class)
-        self._assert_scheduled_requests()
+        self._assert_scheduled_requests(urls_to_visit=8)
-        self.assertEqual(6, len(self.run.reqplug))
+    def _assert_scheduled_requests(self, urls_to_visit=None):
-        self.assertEqual(6, len(self.run.respplug))
+        self.assertEqual(8, len(self.run.respplug))
-        return self.slot.scheduler.enqueue_request(request)
+        if not self.slot.scheduler.enqueue_request(request):
-            return
+            return False
-from twisted.web.resource import Resource, EncodingResourceWrapper
+from twisted.web.server import Site, NOT_DONE_YET
-        self.putChild("xpayload", EncodingResourceWrapper(PayloadResource(), [GzipEncoderFactory()]))
+
-        self.assertTrue(reason, 'finished')
+        if six.PY2 and twisted_version > (12, 3, 0):
-        o = cls(crawler.settings['DOWNLOAD_TIMEOUT'])
+        o = cls(crawler.settings.getfloat('DOWNLOAD_TIMEOUT'))
-        crawler = get_crawler(Spider)
+    def get_request_spider_mw(self, settings=None):
-DOWNLOAD_WARNSIZE = 33554432    # 32m
+DOWNLOAD_MAXSIZE = 1024*1024*1024   # 1024m
-from twisted.web.resource import Resource
+from twisted.web.server import Site, NOT_DONE_YET, GzipEncoderFactory
-from twisted.web.iweb import IBodyProducer
+from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH
-        agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool)
+        agent = ScrapyAgent(contextFactory=self._contextFactory, pool=self._pool,
-    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None):
+    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None,
-        txresponse.deliverBody(_ResponseReader(d, txresponse, request))
+        txresponse.deliverBody(_ResponseReader(d, txresponse, request, maxsize, warnsize))
-    def __init__(self, finished, txresponse, request):
+    def __init__(self, finished, txresponse, request, maxsize, warnsize):
-copyright = u'2008-2013, Scrapy developers'
+copyright = u'2008-2014, Scrapy developers'
-            spider=spider, reason=reason, spider_stats=self.crawler.stats.get_stats()))
+        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(
-from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning
+from scrapy.exceptions import DontCloseSpider
-from six.moves.urllib.parse import ParseResult, urlunparse, urldefrag, urlparse
+from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,
-    keyvals = cgi.parse_qsl(query, keep_blank_values)
+    keyvals = parse_qsl(query, keep_blank_values)
-        raise ImportError("Error loading object '%s': %s" % (path, e))
+    mod = import_module(module)
-        useragent = self._useragent
+        if request.meta.get('dont_obey_robotstxt'):
-        if rp and not rp.can_fetch(useragent, request.url):
+        if rp and not rp.can_fetch(self._useragent, request.url):
-            robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)
+            robotsreq = Request(
-    def test(self):
+    def test_robotstxt(self):
-        return deferred
+        return crawler
-            if req_host.find(".") == -1:
+            if '.' not in req_host:
-
+
-    You can set some options regarding the CSV file, such as the delimiter
+    You can set some options regarding the CSV file, such as the delimiter, quotechar
-        for row in csviter(response, self.delimiter, self.headers):
+        for row in csviter(response, self.delimiter, self.headers, self.quotechar):
-def csviter(obj, delimiter=None, headers=None, encoding=None):
+def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):
-    delimiter is the character used to separate field on the given obj.
+    delimiter is the character used to separate fields on the given obj.
-        csv_r = csv.reader(lines)
+
-        self.spider_modules = settings['SPIDER_MODULES']
+        self.spider_modules = settings.getlist('SPIDER_MODULES')
-        self.log_observer.stop()
+        if self.log_observer:
-        return defer.DeferredList(c.stop() for c in self.crawlers)
+        return defer.DeferredList([c.stop() for c in list(self.crawlers)])
-        crawler = self.crawler_runner.crawlers.pop()
+        d = self.crawler_runner.crawl(CustomSettingsSpider)
-        crawler = self.crawler_process._create_logged_crawler(spidercls)
+        crawler = self.crawler_process._create_crawler(spidercls)
-        self.crawl_deferreds = set()
+        self._active = set()
-        crawler = self._create_logged_crawler(spidercls)
+        crawler = self._create_crawler(spidercls)
-        return d
+        self._active.add(d)
-        return crawler
+        def _done(result):
-        return crawler
+    def _setup_crawler_logging(self, crawler):
-            d = defer.DeferredList(self.crawl_deferreds)
+            d = self.join()
-                # Don't start the reactor if the deferreds are already fired
+
-        t = Thread(target=self.crawler_process._start_reactor,
+        t = Thread(target=self.crawler_process.start,
-    def _start_reactor(self, stop_after_crawl=True):
+    def start(self, stop_after_crawl=True):
-                hosts += req_host + ".local"
+                hosts += [req_host + ".local"]
-def walk_modules(path, load=False):
+def walk_modules(path):
-from scrapy.crawler import Crawler
+from twisted.internet import defer
-    custom_settings = {}
+    custom_settings = None
-        settings.setdict(cls.custom_settings, priority='spider')
+        settings.setdict(cls.custom_settings or {}, priority='spider')
-        crawler = Crawler(spidercls, self.settings.frozencopy())
+
-subclass form object_ref (instead of object).
+subclass from object_ref (instead of object).
-                sh = embed.InteractiveShellEmbed(banner1=banner)
+                from IPython.terminal.embed import InteractiveShellEmbed
-                sh = IPShellEmbed(banner=banner)
+                from IPython.frontend.terminal.embed import InteractiveShellEmbed
-            sh(global_ns={}, local_ns=namespace)
+            config = load_default_config()
-import re
+import json
-        return o
+    pass
-
+import json
-from scrapy.spider import Spider
+from scrapy.utils.serialize import ScrapyJSONEncoder
-class BaseTestCase(unittest.TestCase):
+class JsonEncoderTestCase(unittest.TestCase):
-        assert sp2 is self.spref.get_spider_from_reference('spider::name2')
+        self.encoder = ScrapyJSONEncoder()
-            self.assertEqual(self.decoder.decode(json.dumps(refs)), spiders)
+        for input, output in [('foo', 'foo'), (d, ds), (t, ts), (dt, dts),
-        assert 'Deferred' in self.encoder.encode(defer.Deferred())
+    def test_encode_deferred(self):
-        assert r.url in rs
+        self.assertIn(r.method, rs)
-
+        self.assertIn(r.url, rs)
-        if 'dont_merge_cookies' in request.meta:
+        if request.meta.get('dont_merge_cookies', False):
-        if 'dont_merge_cookies' in request.meta:
+        if request.meta.get('dont_merge_cookies', False):
-        if 'dont_redirect' in request.meta:
+        if request.meta.get('dont_redirect', False):
-        if 'dont_redirect' in request.meta or request.method == 'HEAD' or \
+        if request.meta.get('dont_redirect', False) or request.method == 'HEAD' or \
-        if 'dont_retry' in request.meta:
+        if request.meta.get('dont_retry', False):
-            return self._retry(request, exception, spider)
+                and not request.meta.get('dont_retry', False):
-        self.crawler_process._start_logging()
+        self.crawler_process.start(start_reactor=False)
-    def _start_logging(self):
+    def start(self, stop_after_crawl=True, start_reactor=True):
-        assert not self.frozen, "Trying to modify an immutable Settings object"
+        self._assert_mutability()
-        assert not self.frozen, "Trying to modify an immutable Settings object"
+        self._assert_mutability()
-        assert not self.frozen, "Trying to modify an immutable Settings object"
+        self._assert_mutability()
-        with self.assertRaises(AssertionError) as cm:
+        with self.assertRaises(TypeError) as cm:
-
+    @property
-collect_ignore = ["scrapy/stats.py"]
+collect_ignore = ["scrapy/stats.py", "scrapy/project.py"]
-
+
-extension, middleware or pipeline implement the `from_crawler` class method.
+raise ImportError("""scrapy.project usage has become obsolete.
-"""
+        return cls(crawler)""")
-def inspect_response(response, spider=None):
+def inspect_response(response, spider):
-    Shell(crawler).start(response=response, spider=spider)
+    Shell(spider.crawler).start(response=response)
-"from scrapy.spider import spiders" no longer works - use "from scrapy.project import crawler" and then access crawler.spiders attribute"
+"from scrapy.spider import spiders" no longer works - use "from scrapy.spidermanager import SpiderManager" and instantiate it with your project settings"
-    ScrapyDeprecationWarning, stacklevel=2)
+"""
-from scrapy.utils.spider import create_spider_for_request
+from scrapy.utils.spider import spidercls_for_request, DefaultSpider
-        spider = None
+        spidercls = DefaultSpider
-            spider = crawler.spiders.create(opts.spider)
+            spidercls = spiders.load(opts.spider)
-        crawler.crawl(spider, [request])
+            spidercls = spidercls_for_request(spiders, request, spidercls)
-from scrapy.utils.spider import iterate_spider_output, create_spider_for_request
+from scrapy.utils.spider import iterate_spider_output, spidercls_for_request
-            for rule in self.spider.rules:
+    def get_callback_from_rules(self, spider, response):
-                    level=log.ERROR, spider=self.spider.name)
+                    level=log.ERROR, spider=spider.name)
-    def set_spider(self, url, opts):
+    def set_spidercls(self, url, opts):
-                self.spider = self.pcrawler.spiders.create(opts.spider, **opts.spargs)
+                self.spidercls = spiders.load(opts.spider)
-            if not self.spider:
+            self.spidercls = spidercls_for_request(spiders, Request(url))
-        request = self.prepare_request(request, opts)
+        _start_requests = lambda s: [self.prepare_request(s, request, opts)]
-        self.pcrawler.crawl(self.spider, [request])
+    def start_parsing(self, url, opts):
-                    level=log.ERROR, request=request)
+            log.msg(format='No response downloaded for: %(url)s',
-    def prepare_request(self, request, opts):
+    def prepare_request(self, spider, request, opts):
-                    cb = self.get_callback_from_rules(response)
+                    cb = self.get_callback_from_rules(spider, response)
-                cb_method = getattr(self.spider, cb, None)
+                cb_method = getattr(spider, cb, None)
-                            callback=callback, spider=self.spider.name, level=log.ERROR)
+                            callback=callback, spider=spider.name, level=log.ERROR)
-                    itemproc.process_item(item, self.spider)
+                    itemproc.process_item(item, spider)
-        self.set_spider(url, opts)
+        # prepare spidercls
-        if self.spider and opts.depth > 0:
+        if self.spidercls and opts.depth > 0:
-        self.crawler_process.start_crawling()
+        spiders = self.crawler_process.spiders
-        shell.start(url=url, spider=spider)
+        shell.start(url=url)
-        t = Thread(target=self.crawler_process.start_reactor)
+        t = Thread(target=self.crawler_process._start_reactor,
-        spider.set_crawler(self.crawler)
+            spider = self.crawler.spider or self.crawler._create_spider()
-    """Create a spider to handle the given Request.
+def spidercls_for_request(spidermanager, request, default_spidercls=None,
-    the spider manager) and return a (new) Spider if (and only if) there is
+    the spider manager) and return a Spider class if (and only if) there is
-    default_spider passed. It can optionally log if multiple or no spiders
+    default_spidercls passed. It can optionally log if multiple or no spiders
-        return spidermanager.create(snames[0], **spider_kwargs)
+        return spidermanager.load(snames[0])
-    return default_spider
+    return default_spidercls
-        ev = _adapt_eventdict(eventDict, self.level, self.encoding)
+        ev = _adapt_eventdict(eventDict, self.level, self.encoding,
-def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=True):
+def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8',
-            level=INFO)
+    msg("Scrapy %s started (bot: %s)" % (scrapy.__version__,
-        msg(format="Overridden settings: %(settings)r", settings=d, level=INFO)
+    msg("Optional features available: %s" % ", ".join(scrapy.optional_features),
-        log_observer.stop()
+    d = dict(overridden_settings(settings))
-    def test_msg_spider(self):
+    def test_msg_ignore_spider(self):
-        self.assertEqual(self.logged(), "[myspider] INFO: Hello")
+        self.failIf(self.logged())
-
+class CrawlerScrapyFileLogObserverTest(unittest.TestCase):
-            crawler.crawl(spider)
+            self.crawler_process.crawl(_BenchSpider, total=100000)
-        spiders = spman_cls.from_settings(self.settings)
+        spiders = self.crawler_process.spiders
-            contract_reqs[spider.name] = []
+        for spidername in args or spiders.list():
-                crawler.crawl(spider, requests)
+                for method in tested_methods:
-        crawler.crawl(spider)
+        self.crawler_process.crawl(spname, **opts.spargs)
-        editor = crawler.settings['EDITOR']
+        editor = self.settings['EDITOR']
-            spider = crawler.spiders.create(args[0])
+            spidercls = self.crawler_process.spiders.load(args[0])
-        sfile = sys.modules[spider.__module__].__file__
+        sfile = sys.modules[spidercls.__module__].__file__
-            spider = crawler.spiders.create(name)
+            spidercls = self.crawler_process.spiders.load(name)
-                print("  %s" % spider.__module__)
+                print("  %s" % spidercls.__module__)
-        for s in sorted(crawler.spiders.list()):
+        for s in sorted(self.crawler_process.spiders.list()):
-        spider = spclasses.pop()(**opts.spargs)
+        spidercls = spclasses.pop()
-        crawler.crawl(spider)
+        self.crawler_process.crawl(spidercls, **opts.spargs)
-from scrapy.exceptions import UsageError, ScrapyDeprecationWarning
+from scrapy.exceptions import UsageError
-def get_crawler(settings_dict=None):
+def get_crawler(spidercls=None, settings_dict=None):
-    from scrapy.crawler import Crawler
+    from scrapy.crawler import CrawlerRunner
-    return Crawler(Settings(settings_dict))
+    runner = CrawlerRunner(Settings(settings_dict))
-from scrapy.utils.test import docrawl
+from scrapy.utils.test import get_crawler
-        reason = spider.meta['close_reason']
+        crawler = get_crawler(ItemSpider, {'CLOSESPIDER_ITEMCOUNT': close_on})
-        itemcount = spider.crawler.stats.get_value('item_scraped_count')
+        itemcount = crawler.stats.get_value('item_scraped_count')
-        reason = spider.meta['close_reason']
+        crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_PAGECOUNT': close_on})
-        pagecount = spider.crawler.stats.get_value('response_received_count')
+        pagecount = crawler.stats.get_value('response_received_count')
-        reason = spider.meta['close_reason']
+        crawler = get_crawler(ErrorSpider, {'CLOSESPIDER_ERRORCOUNT': close_on})
-        errorcount = spider.crawler.stats.get_value(key)
+                .format(name=crawler.spider.exception_cls.__name__)
-        reason = spider.meta['close_reason']
+        crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_TIMEOUT': close_on})
-        stats = spider.crawler.stats
+        stats = crawler.stats
-from scrapy.utils.test import docrawl, get_testlog
+from scrapy.utils.test import get_crawler, get_testlog
-        self.assertEqual(len(spider.urls_visited), 11)  # 10 + start_url
+        crawler = get_crawler(FollowAllSpider)
-        t = spider.times
+        crawler = get_crawler(FollowAllSpider, settings)
-        self.assertTrue(spider.t2 > spider.t1)
+        crawler = get_crawler(DelaySpider)
-        self.assertTrue(spider.t2_err > spider.t1)
+        crawler = get_crawler(DelaySpider, {"DOWNLOAD_TIMEOUT": 0.35})
-        self.assertTrue(spider.t2_err > spider.t1)
+        yield crawler.crawl(n=0.5, b=1)
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-            yield docrawl(spider)
+            crawler = get_crawler(SimpleSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(BrokenStartRequestsSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(BrokenStartRequestsSpider)
-        #                spider.seedsseen)
+        crawler = get_crawler(BrokenStartRequestsSpider, settings)
-        self.assertEqual(spider.visited, 6)
+        crawler = get_crawler(DuplicateStartRequestsSpider, settings)
-        self.assertEqual(spider.visited, 3)
+        yield crawler.crawl(dont_filter=False, distinct_urls=3, dupe_factor=4)
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(SingleRequestSpider)
-        self.assertNotIn('failures', spider.meta)
+        self.assertIn('responses', crawler.spider.meta)
-        echo0 = json.loads(spider.meta['responses'][2].body)
+        echo0 = json.loads(crawler.spider.meta['responses'][2].body)
-        echo1 = json.loads(spider.meta['responses'][1].body)
+        echo1 = json.loads(crawler.spider.meta['responses'][1].body)
-        echo2 = json.loads(spider.meta['responses'][2].body)
+        echo2 = json.loads(crawler.spider.meta['responses'][2].body)
-        echo3 = json.loads(spider.meta['responses'][3].body)
+        echo3 = json.loads(crawler.spider.meta['responses'][3].body)
-            est.append(get_engine_status(spider.crawler.engine))
+            est.append(get_engine_status(crawler.engine))
-        yield docrawl(spider)
+        crawler = get_crawler(SingleRequestSpider)
-        self.assertEqual(s['engine.spider.name'], spider.name)
+        self.assertEqual(s['engine.spider.name'], crawler.spider.name)
-        dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))
+        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})
-        dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))
+        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})
-        dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))
+        crawler = get_crawler(settings_dict={'DOWNLOAD_HANDLERS': handlers})
-        self.spider.set_crawler(self.crawler)
+        self.crawler = get_crawler(Spider, self.settings_dict)
-        crawler = get_crawler({'AJAXCRAWL_ENABLED': True})
+        crawler = get_crawler(Spider, {'AJAXCRAWL_ENABLED': True})
-        spider.set_crawler(crawler)
+        crawler = get_crawler(Spider)
-        spider.set_crawler(crawler)
+        crawler = get_crawler(Spider)
-        self.spider = Spider('example.com')
+        self.crawler = get_crawler(Spider)
-        self.spider = Spider('foo')
+        crawler = get_crawler(Spider)
-        self.spider = Spider('foo')
+        crawler = get_crawler(Spider)
-        self.spider = Spider('foo')
+        crawler = get_crawler(Spider)
-        self.spider = Spider('scrapytest.org')
+        self.crawler = get_crawler(Spider)
-        spider.set_crawler(crawler)
+        crawler = get_crawler(Spider, {'USER_AGENT': default_useragent})
-        self.crawler = get_crawler()
+        self.crawler = get_crawler(TestSpider)
-        self.crawler.start()
+        self.crawler.crawl(start_urls=start_urls)
-from scrapy.utils.test import get_testlog, docrawl
+from scrapy.utils.test import get_testlog, get_crawler
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(SingleRequestSpider)
-        echo = json.loads(spider.meta['responses'][0].body)
+        echo = json.loads(crawler.spider.meta['responses'][0].body)
-        yield docrawl(spider)
+        crawler = get_crawler(SimpleSpider)
-        crawler.settings.set('CRAWLSPIDER_FOLLOW_LINKS', False)
+        settings_dict = {'CRAWLSPIDER_FOLLOW_LINKS': False}
-        spider.set_crawler(get_crawler({'CRAWLSPIDER_FOLLOW_LINKS': False}))
+        settings_dict = {'CRAWLSPIDER_FOLLOW_LINKS': False}
-        self.spider = Spider('scrapytest.org')
+        crawler = get_crawler(Spider)
-        self.stats = StatsCollector(get_crawler())
+        self.stats = StatsCollector(crawler)
-from scrapy.utils.test import docrawl, get_testlog
+from scrapy.utils.test import get_crawler, get_testlog
-        self.assertEqual(spider.failed, {'404', '402', '500'})
+        crawler = get_crawler(_HttpErrorSpider)
-        yield docrawl(spider)
+        crawler = get_crawler(_HttpErrorSpider)
-        self.assertEqual(spider.failed, {'404', '500'})
+        self.assertEqual(crawler.spider.parsed, {'200', '402'})
-        crawler = get_crawler()
+        crawler = get_crawler(Spider)
-        return Spider('foo', allowed_domains=['scrapytest.org', 'scrapy.org'])
+    def _get_spiderargs(self):
-        return Spider('foo', allowed_domains=None)
+    def _get_spiderargs(self):
-      return Spider('foo', allowed_domains=['scrapytest.org', None, bad_hostname])
+      return dict(name='foo', allowed_domains=['scrapytest.org', None, bad_hostname])
-        self.spider = Spider('foo')
+        self.crawler = get_crawler(Spider)
-    """ A class to run multiple scrapy crawlers in a process sequentially"""
+class CrawlerRunner(object):
-        self._started = None
+        smcls = load_object(settings['SPIDER_MANAGER_CLASS'])
-            self.crawlers[name] = Crawler(self.settings)
+    def crawl(self, spidercls, *args, **kwargs):
-        return self.crawlers[name]
+        crawler.install()
-            self.start_reactor()
+        d = crawler.crawl(*args, **kwargs)
-            yield self._active_crawler.stop()
+        return defer.DeferredList(c.stop() for c in self.crawlers)
-    def start_crawling(self):
+    def start(self, stop_after_crawl=True):
-    def start_reactor(self):
+    def _start_reactor(self, stop_after_crawl=True):
-        self.configured = False
+    def __init__(self, spidercls, settings):
-        # TODO: move SpiderManager to CrawlerProcess
+        self.stats = load_object(self.settings['STATS_CLASS'])(self)
-        self.spiders = spman_cls.from_crawler(self)
+        self.spiders = spman_cls.from_settings(self.settings)
-        self.engine = ExecutionEngine(self, self._spider_closed)
+    @defer.inlineCallbacks
-            self._start_requests = lambda: requests
+        try:
-            self.stop()
+    def _create_spider(self, *args, **kwargs):
-        yield defer.maybeDeferred(self.engine.start)
+    def _create_engine(self):
-        if self.configured and self.engine.running:
+        if self.crawling:
-        raise a KeyError."""
+    def from_settings(settings):
-        self.spider_modules = spider_modules
+    def __init__(self, settings):
-        return cls(settings.getlist('SPIDER_MODULES'))
+        return cls(settings)
-    def create(self, spider_name, **spider_kwargs):
+    def load(self, spider_name):
-            spcls = self._spiders[spider_name]
+            return self._spiders[spider_name]
-            return spcls(**spider_kwargs)
+            raise KeyError("Spider not found: {}".format(spider_name))
-            return closed(reason)
+        return list(self._spiders.keys())
-        self.spiderman = SpiderManager(['test_spiders_xxx'])
+        settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})
-            set(['spider1', 'spider2', 'spider3', 'spider4']))
+            set(['spider1', 'spider2', 'spider3']))
-        self.assertEqual(spider2.foo, 'bar')
+    def test_load(self):
-        self.spiderman = SpiderManager(['tests.test_spidermanager.test_spiders.spider1'])
+        module = 'tests.test_spidermanager.test_spiders.spider1'
-        self.spiderman = SpiderManager(['tests.test_spidermanager.test_spiders.spider0'])
+        module = 'tests.test_spidermanager.test_spiders.spider0'
-            return str(value).split(',')
+        value = self.get(name, default or [])
-            return default or {}
+        value = self.get(name, default or {})
-        raise ValueError("Cannot convert value for setting '%s' to dict: '%s'" % (name, value))
+        return dict(value)
-        return self._crawler
+    @classmethod
-        return self.crawler.settings
+    def set_crawler(self, crawler):
-
+try:
-        'w3lib>=1.2',
+        'w3lib>=1.8.0',
-    text = html.remove_comments(html.remove_entities(text))
+    text = html.remove_comments(html.replace_entities(text))
-from w3lib.html import remove_tags, remove_entities, replace_escape_chars
+from w3lib.html import remove_tags, replace_entities, replace_escape_chars
-        clean_url = lambda u: urljoin(base_url, remove_entities(clean_link(u.decode(response_encoding))))
+        clean_url = lambda u: urljoin(base_url, replace_entities(clean_link(u.decode(response_encoding))))
-from w3lib.html import remove_entities
+from w3lib.html import replace_entities
-        return [remove_entities(s, keep=['lt', 'amp']) for s in strings]
+        return [replace_entities(s, keep=['lt', 'amp']) for s in strings]
-        return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]
+        return [replace_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]
-    packages=find_packages(exclude=['tests']),
+    packages=find_packages(exclude=('tests', 'tests.*')),
-    
+
-        'Operating System :: OS Independent',
+from os.path import dirname, join
-        'Intended Audience :: Developers',
+        'Intended Audience :: Developers',
-    setup_args['install_requires'] = [
+    ],
-setup(**setup_args)
+    ],
-    from scrapy.item import Item, Field
+from scrapy.spider import Spider
-        if isinstance(url, unicode):
+        if isinstance(url, six.text_type):
-            warnings.warn("Do not instantiate Link objects with unicode urls. " \
+            warnings.warn("Do not instantiate Link objects with unicode urls. "
-   
+
-        l3 = Link("http://www.example.com")
+        l1 = Link(b"http://www.example.com")
-        l6 = Link("http://www.example.com", text="test")
+        l4 = Link(b"http://www.example.com", text="test")
-        l10 = Link("http://www.example.com", text="test", fragment='other', nofollow=False)
+        l7 = Link(b"http://www.example.com", text="test", fragment='something', nofollow=False)
-        l1 = Link("http://www.example.com", text="test", fragment='something', nofollow=True)
+        l1 = Link(b"http://www.example.com", text="test", fragment='something', nofollow=True)
-            assert l.url == 'http://www.example.com/\xc2\xa3'
+            link = Link(u"http://www.example.com/\xa3")
-        self.failUnless(os.path.exists(path))
+        self.assertTrue(os.path.exists(path))
-            self.failUnlessEqual(fp.read(), b"content")
+            self.assertEqual(fp.read(), b"content")
-        self.failUnless(os.path.exists(path))
+        self.assertTrue(os.path.exists(path))
-            self.failUnlessEqual(fp.read(), b"content")
+            self.assertEqual(fp.read(), b"content")
-            self.failUnlessEqual(fp.read(), b"new content")
+            self.assertEqual(fp.read(), b"new content")
-        self.failUnlessEqual(key.get_contents_as_string(), "content")
+        self.assertEqual(key.get_contents_as_string(), "content")
-        self.failUnless('ZeroDivisionError' in self.logged())
+        self.assertIn('Traceback', self.logged())
-        self.failUnless('bad type' in self.logged())
+        self.assertIn('TypeError', self.logged())
-        self.failUnless('bad type' in self.logged())
+        self.assertIn('TypeError', self.logged())
-        self.failUnless('bad type' in self.logged())
+        self.assertIn('Unhandled Error', self.logged())
-        self.failUnless('bad type' in self.logged())
+        self.assertIn('Unhandled Error', self.logged())
-        self.failUnlessEqual(mwman.middlewares, (m1, m2, m3))
+        self.assertEqual(mwman.middlewares, (m1, m2, m3))
-        self.failUnlessEqual(classes, [M1, M3])
+        self.assertEqual(classes, [M1, M3])
-        self.failUnless(gotexc)
+        self.assertTrue(gotexc)
-        self.failUnlessEqual(out, range(10))
+        self.assertEqual(out, range(10))
-        self.failUnless(isinstance(errors[0].value, ZeroDivisionError))
+        self.assertEqual(out, [0, 1, 2, 3, 4])
-        self.failUnless(equal_attributes(a, b, ['x']))
+        self.assertTrue(equal_attributes(a, b, ['x']))
-        self.failUnless(equal_attributes(a, b, ['x', 'y']))
+        self.assertTrue(equal_attributes(a, b, ['x', 'y']))
-        self.failUnless(equal_attributes(a, b, ['meta']))
+        self.assertTrue(equal_attributes(a, b, ['meta']))
-        self.failUnless(equal_attributes(a, b, [compare_z, 'x']))
+        self.assertTrue(equal_attributes(a, b, [compare_z, 'x']))
-        self.failUnlessEqual(d, d2)
+        self.assertEqual(d, d2)
-        self.failUnlessEqual(d, d2)
+        self.assertEqual(d, d2)
-        self.failUnlessEqual(d, d2)
+        self.assertEqual(d, d2)
-        self.failUnless("Cannot return deferreds from signal handler" in str(log_events))
+        self.assertTrue(log_events)
-import string
+import string
-            print("    scrapy genspider example example.com")
+
-        print("    scrapy genspider example example.com")
+        try:
-from zope.interface import Interface, implements
+from zope.interface import Interface, implementer
-
+@implementer(IFeedStorage)
-    implements(IFeedStorage)
+@implementer(IFeedStorage)
-        file.write("content")
+        file.write(b"content")
-        self.failUnlessEqual(open(path).read(), "content")
+        with open(path, 'rb') as fp:
-        self.failUnlessEqual(open(path).read(), b"content")
+        with open(path, 'rb') as fp:
-        self.failUnlessEqual(open(path).read(), b"new content")
+        with open(path, 'rb') as fp:
-        collect_ignore.append(fn.strip())
+        if fn.strip():
-
+import six
-    elif not isinstance(arg, (dict, BaseItem)) and hasattr(arg, '__iter__'):
+    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):
-    >>> md5sum(StringIO('file content to hash'))
+    >>> from io import BytesIO
-from zope.interface import implements
+from zope.interface import implementer
-
+import six
-    for obj in vars(module).itervalues():
+    for obj in six.itervalues(vars(module)):
-        self.assertEqual(set(it), set([MySpider1, MySpider2]))
+        self.assertEqual(set(it), {MySpider1, MySpider2})
-        s = Sitemap("""<?xml version="1.0" encoding="UTF-8"?>
+        s = Sitemap(b"""<?xml version="1.0" encoding="UTF-8"?>
-        s = Sitemap("""<?xml version="1.0" encoding="UTF-8"?>
+        s = Sitemap(b"""<?xml version="1.0" encoding="UTF-8"?>
-        s = Sitemap("""<?xml version="1.0" encoding="UTF-8"?>
+        s = Sitemap(b"""<?xml version="1.0" encoding="UTF-8"?>
-        s = Sitemap("""<?xml version="1.0" encoding="UTF-8"?>
+        s = Sitemap(b"""<?xml version="1.0" encoding="UTF-8"?>
-        s = Sitemap("""<?xml version="1.0" encoding="UTF-8"?>
+        s = Sitemap(b"""<?xml version="1.0" encoding="UTF-8"?>
-        s = Sitemap("""\
+        s = Sitemap(b"""\
-        s = Sitemap("""<?xml version="1.0" encoding="UTF-8"?>
+        s = Sitemap(b"""<?xml version="1.0" encoding="UTF-8"?>
-        s = Sitemap("""<?xml version="1.0" encoding="UTF-8"?>
+        s = Sitemap(b"""<?xml version="1.0" encoding="UTF-8"?>
-        s = Sitemap("""<?xml version="1.0" encoding="utf-8"?>
+        s = Sitemap(b"""<?xml version="1.0" encoding="utf-8"?>
-from scrapy.utils.decorator import deprecated
+
-    def __init__(self, url, callback=None, method='GET', headers=None, body=None, 
+    def __init__(self, url, callback=None, method='GET', headers=None, body=None,
-                    type(self).__name__)
+                                type(self).__name__)
-                    type(self).__name__)
+                                type(self).__name__)
-                'encoding', 'priority', 'dont_filter', 'callback', 'errback']:
+        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta',
-                                (formnumber, response))
+                             (formnumber, response))
-              for k, v in (_value(e) for e in inputs) \
+    values = [(k, u'' if v is None else v)
-        elif isinstance(url, unicode):
+        elif isinstance(url, six.text_type):
-        elif isinstance(body, unicode):
+        elif isinstance(body, six.text_type):
-from tests import tests_datadir
+from tests import tests_datadir
-            assert text.endswith('</html')
+            assert text.endswith(b'</html')
-            assert text.endswith('</html>')
+            assert text.endswith(b'</html>')
-from six.moves.configparser import SafeConfigParser
+import sys
-        if v is not None]
+    items = (x for x in six.iteritems(compdict) if x[1] is not None)
-        os.path.expanduser('~/.scrapy.cfg')]
+    sources = ['/etc/scrapy.cfg', r'c:\scrapy\scrapy.cfg',
-            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': [17]})
+            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': ['17']})
-            self.assertEqual(r.headers, {'Local Filename': ['/tmp/file.txt'], 'Size': [17]})
+            self.assertEqual(r.headers, {'Local Filename': ['/tmp/file.txt'], 'Size': ['17']})
-        return key.title()
+        """Normalize key to bytes"""
-        """Headers must not be unicode"""
+        """Normalize values to bytes"""
-            for x in value]
+
-        self.assertEqual(h.getlist('Accept', ['text/html', 'images/jpeg']), ['text/html','images/jpeg'])
+        self.assertEqual(h.get('Accept', '*/*'), b'*/*')
-        self.assertEqual(h.getlist('Content-Type'), ['text/html'])
+        self.assertEqual(h['Content-Type'], b'text/html')
-        self.assertEqual(h.getlist('X-Forwarded-For'), hlist)
+        self.assertEqual(h['X-Forwarded-For'], b'ip2')
-        self.assertEqual(val[0], '\xc2\xa3')
+        key, val = dict(h).popitem()
-        self.assertEqual(val[0], '\xa3')
+        key, val = dict(h).popitem()
-        self.assertEqual(val[0], '\xc2\xa3')
+        key, val = dict(h).popitem()
-        self.assertEqual(h.getlist('X-Forwarded-For'), ['ip1'])
+        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1'])
-        self.assertSortedEqual(h.values(), ['ip2', 'text/html'])
+                             {b'Content-Type': [b'text/html'],
-        self.assertEqual(h.getlist('X-Forwarded-For'), ['ip1', 'ip2'])
+        h.update({'Content-Type': 'text/html',
-        self.assertEqual(h1.getlist('header1'), ['value1', 'value3'])
+        self.assertEqual(h1.getlist('header1'), [b'value1', b'value3'])
-        self.assertEqual(h1.getlist('header1'), ['value1', 'value3'])
+        self.assertEqual(h1.getlist('header1'), [b'value1', b'value3'])
-        self.assertEqual(h1.getlist('header1'), ['value2', 'value3'])
+        self.assertEqual(h1.getlist('header1'), [b'value1'])
-        self.assertEqual(h1.getlist('header2'), ['value2', 'value3'])
+        self.assertEqual(h1.getlist('header1'), [b'value1'])
-from tests.spiders import SimpleSpider
+from scrapy.http import Request
-
+    def test_https_tunnel_without_leak_proxy_authorization_header(self):
-from io import BytesIO
+from io import StringIO
-        self.mimetypes.readfp(BytesIO(mimedata))
+        mimedata = get_data('scrapy', 'mime.types').decode('utf8')
-If you're in windows use runtests.bat instead.
+see http://doc.scrapy.org/en/latest/contributing.html#running-tests
-
+import six
-from urllib import urlencode
+from six.moves.urllib.parse import urlencode
-from tests.mockserver import MockServer
+from scrapy.contrib.linkextractors import LinkExtractor
-            spider = FollowAllSpider(total=100000)
+        with _BenchServer():
-collect_ignore = ["stats.py"]
+collect_ignore = ["scrapy/stats.py"]
-from scrapy.tests.mockserver import MockServer
+from tests.spiders import FollowAllSpider
-    For examples see the tests in scrapy.tests.test_utils_url
+    For examples see the tests in tests/test_utils_url.py
-scrapy.tests: this package contains all Scrapy unittests
+tests: this package contains all Scrapy unittests
-        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'],
+        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver'],
-from scrapy.tests.mockserver import MockServer
+from tests.spiders import FollowAllSpider, ItemSpider, ErrorSpider
-        self.env['SCRAPY_SETTINGS_MODULE'] = 'scrapy.tests.test_cmdline.settings'
+        self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'
-from scrapy.tests import get_testdata
+from tests import get_testdata
-from scrapy.tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \
+from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \
-from scrapy.tests.mockserver import MockServer
+from tests.mockserver import MockServer
-os.environ['DJANGO_SETTINGS_MODULE'] = 'scrapy.tests.test_djangoitem.settings'
+os.environ['DJANGO_SETTINGS_MODULE'] = 'tests.test_djangoitem.settings'
-        handlers = {'scheme': 'scrapy.tests.test_downloader_handlers.DummyDH'}
+        handlers = {'scheme': 'tests.test_downloader_handlers.DummyDH'}
-        handlers = {'scheme': 'scrapy.tests.test_downloader_handlers.OffDH'}
+        handlers = {'scheme': 'tests.test_downloader_handlers.OffDH'}
-from scrapy.tests import get_testdata
+from tests import get_testdata
-    dbm_module = 'scrapy.tests.mocks.dummydbm'
+    dbm_module = 'tests.mocks.dummydbm'
-from scrapy.tests import tests_datadir
+from tests import tests_datadir
-from scrapy.tests import tests_datadir
+from tests import tests_datadir
-        return ['scrapy.tests.test_middleware.%s' % x for x in ['M1', 'MOff', 'M3']]
+        return ['tests.test_middleware.%s' % x for x in ['M1', 'MOff', 'M3']]
-from scrapy.tests.mockserver import MockServer
+from tests.spiders import SimpleSpider
-            'scrapy.tests.test_settings.default_settings', 10)
+            'tests.test_settings.default_settings', 10)
-        self.spiderman = SpiderManager(['scrapy.tests.test_spidermanager.test_spiders.spider1'])
+        self.spiderman = SpiderManager(['tests.test_spidermanager.test_spiders.spider1'])
-        self.spiderman = SpiderManager(['scrapy.tests.test_spidermanager.test_spiders.spider0'])
+        self.spiderman = SpiderManager(['tests.test_spidermanager.test_spiders.spider0'])
-from scrapy.tests.mockserver import MockServer
+from tests.mockserver import MockServer
-            "please inherit from scrapy.tests.test_utils_deprecate.NewName."
+            "tests.test_utils_deprecate.UserClass inherits from "
-            "instantiate scrapy.tests.test_utils_deprecate.NewName instead."
+            "tests.test_utils_deprecate.Deprecated is deprecated, "
-        self.assertIn("scrapy.tests.test_utils_deprecate.Deprecated", msg)
+        self.assertIn("tests.test_utils_deprecate.NewName", msg)
-from scrapy.tests import tests_datadir
+from tests import tests_datadir
-from scrapy.tests import get_testdata
+from tests import get_testdata
-from scrapy.tests.test_utils_serialize import CrawlerMock
+from tests.test_utils_serialize import CrawlerMock
-        mods = walk_modules('scrapy.tests.test_utils_misc.test_walk_modules')
+        mods = walk_modules('tests.test_utils_misc.test_walk_modules')
-            'scrapy.tests.test_utils_misc.test_walk_modules.mod1',
+            'tests.test_utils_misc.test_walk_modules',
-        mods = walk_modules('scrapy.tests.test_utils_misc.test_walk_modules.mod')
+        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod')
-            'scrapy.tests.test_utils_misc.test_walk_modules.mod.mod0',
+            'tests.test_utils_misc.test_walk_modules.mod',
-        mods = walk_modules('scrapy.tests.test_utils_misc.test_walk_modules.mod1')
+        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod1')
-            'scrapy.tests.test_utils_misc.test_walk_modules.mod1',
+            'tests.test_utils_misc.test_walk_modules.mod1',
-        it = iter_spider_classes(scrapy.tests.test_utils_spider)
+        import tests.test_utils_spider
-            yield i
+        return iter(self._values)
-            return 1
+    __hash__ = BaseItem.__hash__
-class Item(six.with_metaclass(ItemMeta, DictItem)):
+@six.add_metaclass(ItemMeta)
-from UserDict import DictMixin
+from collections import MutableMapping
-class ItemMeta(type):
+class ItemMeta(ABCMeta):
-class DictItem(DictMixin, BaseItem):
+class DictItem(MutableMapping, BaseItem):
-    __metaclass__ = ItemMeta
+class Item(six.with_metaclass(ItemMeta, DictItem)):
-                         "{'name': u'John Doe', 'number': 123}")
+
-        self.assertEqual(i.values(), ['John'])
+        self.assertEqual(list(i.keys()), ['name'])
-        self.assertSortedEqual(i.values(), [u'Keys', u'Values', u'John'])
+        self.assertSortedEqual(list(i.keys()), ['keys', 'values', 'name'])
-        self.assertEqual(i.values(), [3])
+        self.assertEqual(list(i.keys()), ['keys'])
-        DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, \
+from twisted.internet import defer
-    EXCEPTIONS_TO_RETRY = (ServerTimeoutError, UserTimeoutError, DNSLookupError,
+    EXCEPTIONS_TO_RETRY = (defer.TimeoutError, TimeoutError, DNSLookupError,
-    ConnectionLost
+from twisted.internet import defer
-                      ConnectionLost]
+        exceptions = [defer.TimeoutError, TCPTimedOutError, TimeoutError,
-            'Content-Length': len(body),
+            'Content-Length': str(len(body)),
-            'Content-Length': len(body),
+            'Content-Length': str(len(body)),
-        obj = cls(crawler.settings.get('JOBDIR'))
+        obj = cls(job_dir(crawler.settings))
-from tempfile import mkdtemp
+from tempfile import mkdtemp, TemporaryFile
-    buf = BytesIO()
+    buf = TemporaryFile()
-    elif type(obj) is six.text_type:
+    elif isinstance(obj, six.text_type):
-        buf = six.BytesIO(response.body)
+        buf = BytesIO(response.body)
-        orig_image = Image.open(six.BytesIO(response.body))
+        orig_image = Image.open(BytesIO(response.body))
-        buf = six.BytesIO()
+        buf = BytesIO()
-        archive = six.BytesIO(response.body)
+        archive = BytesIO(response.body)
-        archive = six.BytesIO(response.body)
+        archive = BytesIO(response.body)
-        archive = six.BytesIO(response.body)
+        archive = BytesIO(response.body)
-import six
+from io import BytesIO
-        self.body = open(filename, "w") if filename else six.BytesIO()
+        self.body = open(filename, "w") if filename else BytesIO()
-import six
+from io import BytesIO
-        self._bodybuf = six.BytesIO()
+        self._bodybuf = BytesIO()
-        self.mimetypes.readfp(six.BytesIO(mimedata))
+        self.mimetypes.readfp(BytesIO(mimedata))
-import six
+from io import BytesIO
-        self.output = six.BytesIO()
+        self.output = BytesIO()
-        f = six.BytesIO()
+        f = BytesIO()
-        output = six.BytesIO()
+        output = BytesIO()
-        output = six.BytesIO()
+        output = BytesIO()
-        output = six.BytesIO()
+        output = BytesIO()
-        output = six.BytesIO()
+        output = BytesIO()
-        output = six.BytesIO()
+        output = BytesIO()
-        output = six.BytesIO()
+        output = BytesIO()
-        output = six.BytesIO()
+        output = BytesIO()
-        output = six.BytesIO()
+        output = BytesIO()
-import six
+from io import BytesIO
-        yield storage.store(six.BytesIO(b"new content"))
+        yield storage.store(BytesIO(b"new content"))
-        out = six.BytesIO()
+        out = BytesIO()
-import six
+from io import BytesIO
-        f = six.BytesIO()
+        f = BytesIO()
-        f = six.BytesIO()
+        f = BytesIO()
-import six
+from io import BytesIO
-        self.f = six.BytesIO()
+        self.f = BytesIO()
-        attach = six.BytesIO()
+        attach = BytesIO()
-import six
+from io import BytesIO
-    buf = six.BytesIO()
+    buf = BytesIO()
-import six
+from io import BytesIO
-    f = six.BytesIO()
+    f = BytesIO()
-import unittest, json, six
+import unittest, json
-        return six.BytesIO(self.response)
+        return BytesIO(self.response)
-import six
+
-    f = GzipFile(fileobj=six.BytesIO(data))
+    f = GzipFile(fileobj=BytesIO(data))
-    lines = six.BytesIO(_body_or_str(obj, unicode=False))
+    lines = BytesIO(_body_or_str(obj, unicode=False))
-        self.body.close() if self.filename else self.body.reset()
+        self.body.close() if self.filename else self.body.seek(0)
-        f.reset()
+        f.seek(0)
-        buf = StringIO(response.body)
+        buf = six.BytesIO(response.body)
-        orig_image = Image.open(StringIO(response.body))
+        orig_image = Image.open(six.BytesIO(response.body))
-        buf = StringIO()
+        buf = six.BytesIO()
-        archive = StringIO(response.body)
+        archive = six.BytesIO(response.body)
-        archive = StringIO(response.body)
+        archive = six.BytesIO(response.body)
-        archive = StringIO(response.body)
+        archive = six.BytesIO(response.body)
-        self.body = open(filename, "w") if filename else StringIO()
+        self.body = open(filename, "w") if filename else six.BytesIO()
-        self._bodybuf = StringIO()
+        self._bodybuf = six.BytesIO()
-from cStringIO import StringIO
+from six.moves import cStringIO as StringIO
-        self.mimetypes.readfp(StringIO(mimedata))
+        self.mimetypes.readfp(six.BytesIO(mimedata))
-        self.output = StringIO()
+        self.output = six.BytesIO()
-        f = StringIO()
+        f = six.BytesIO()
-        output = StringIO()
+        output = six.BytesIO()
-        output = StringIO()
+        output = six.BytesIO()
-        output = StringIO()
+        output = six.BytesIO()
-        output = StringIO()
+        output = six.BytesIO()
-        output = StringIO()
+        output = six.BytesIO()
-        output = StringIO()
+        output = six.BytesIO()
-        output = StringIO()
+        output = six.BytesIO()
-        output = StringIO()
+        output = six.BytesIO()
-        file.write("content")
+        file.write(b"content")
-        self.failUnlessEqual(open(path).read(), "content")
+        self.failUnlessEqual(open(path).read(), b"content")
-        self.failUnlessEqual(open(path).read(), "new content")
+        yield storage.store(six.BytesIO(b"new content"))
-        out = StringIO()
+        out = six.BytesIO()
-        file.write("content")
+        file.write(b"content")
-        self.assertEqual(out.getvalue(), "content")
+        self.assertEqual(out.getvalue(), b"content")
-        plainbody = """<html><head><title>Some page</title><meta http-equiv="Content-Type" content="text/html; charset=gb2312">"""
+        f = six.BytesIO()
-        plainbody = """<html><head><title>Some page</title><meta http-equiv="Content-Type" content="text/html; charset=gb2312">"""
+        f = six.BytesIO()
-from cStringIO import StringIO
+import six
-        self.f = StringIO()
+        self.f = six.BytesIO()
-from cStringIO import StringIO
+import six
-        attach.write('content')
+        attach = six.BytesIO()
-from cStringIO import StringIO
+import six
-    buf = StringIO()
+    buf = six.BytesIO()
-from cStringIO import StringIO
+import six
-        body = """<?xml version="1.0" encoding="UTF-8"?>
+        body = b"""<?xml version="1.0" encoding="UTF-8"?>
-    test_body = """<html><head><title>Page title<title>
+    test_body = b"""<html><head><title>Page title<title>
-    f = StringIO()
+    BODY = b"SITEMAP"
-from cStringIO import StringIO
+import unittest, json, six
-        return StringIO(self.response)
+        return six.BytesIO(self.response)
-from cStringIO import StringIO
+import six
-    chunk = '.'
+    f = GzipFile(fileobj=six.BytesIO(data))
-from cStringIO import StringIO
+import re, csv, six
-    lines = StringIO(_body_or_str(obj, unicode=False))
+    lines = six.BytesIO(_body_or_str(obj, unicode=False))
-    assert isinstance(obj, (Response, basestring)), \
+    assert isinstance(obj, (Response, six.string_types)), \
-    elif type(obj) is type(u''):
+    elif type(obj) is six.text_type:
-import xmlrpclib
+from six.moves import xmlrpc_client as xmlrpclib
-from email import Encoders
+import six
-import xmlrpclib
+from six.moves import xmlrpc_client as xmlrpclib
-from urlparse import urljoin
+from six.moves.urllib.parse import urljoin
-from urlparse import urlparse, urljoin
+from six.moves.urllib.parse import urlparse, urljoin
-from urlparse import urlunparse
+from six.moves.urllib.parse import urlunparse
-from urlparse import urljoin
+from six.moves.urllib.parse import urljoin
-import robotparser
+from six.moves.urllib import robotparser
-from urlparse import urlparse
+from six.moves.urllib.parse import urlparse
-from urlparse import urljoin
+from six.moves.urllib.parse import urljoin
-from urlparse import urlparse, urljoin
+from six.moves.urllib.parse import urlparse, urljoin
-from urlparse import urljoin
+from six.moves.urllib.parse import urljoin
-from urlparse import urljoin
+from six.moves.urllib.parse import urljoin
-import urlparse
+from six.moves.urllib.parse import urlparse
-            scheme = urlparse.urlparse(uri).scheme
+            scheme = urlparse(uri).scheme
-from urlparse import urlparse
+from six.moves.urllib.parse import urlparse
-from urlparse import urldefrag
+from six.moves.urllib.parse import urldefrag
-from urlparse import urlparse, urlunparse, urldefrag
+from six.moves.urllib.parse import urlparse, urlunparse, urldefrag
-import urllib, urlparse
+import urllib
-    return urlparse.urljoin(form.base_url, url)
+    return urljoin(form.base_url, url)
-from urlparse import urlparse
+from six.moves.urllib.parse import urlparse
-import os, urlparse
+import os
-        u = urlparse.urlparse(uri)
+        u = urlparse(uri)
-import sys, os, re, urlparse
+import sys, os, re
-        u = urlparse.urlparse(url)
+        u = urlparse(url)
-from urlparse import urlparse
+from six.moves.urllib.parse import urlparse
-from urlparse import urlparse
+from six.moves.urllib.parse import urlparse
-from urlparse import urlparse
+from six.moves.urllib.parse import urlparse
-from urlparse import urlparse
+from six.moves.urllib.parse import urlparse
-import urlparse
+from six.moves.urllib.parse import urlparse
-        urlp = urlparse.urlparse(url)
+        urlp = urlparse(url)
-import urlparse
+from six.moves.urllib.parse import urlparse
-            path = urlparse.urlparse(burl).path
+            path = urlparse(burl).path
-from urlparse import urlparse
+from six.moves.urllib.parse import urlparse
-from urlparse import urlparse
+from six.moves.urllib.parse import urlparse
-from urlparse import urlunparse
+from six.moves.urllib.parse import urlunparse
-import urlparse
+from six.moves.urllib.parse import urljoin
-        return urlparse.urljoin(self.baseurl, path)
+        return urljoin(self.baseurl, path)
-import urlparse
+from six.moves.urllib.parse import ParseResult, urlunparse, urldefrag, urlparse
-    return urlparse.urlunparse((scheme, netloc.lower(), path, params, query, fragment))
+    return urlunparse((scheme, netloc.lower(), path, params, query, fragment))
-        urlparse.urlparse(unicode_to_str(url, encoding))
+    return url if isinstance(url, ParseResult) else \
-    defrag, frag = urlparse.urldefrag(url)
+    defrag, frag = urldefrag(url)
-            env=self.env, **kwargs)
+        with tempfile.TemporaryFile() as out:
-    for cmdname, cmdclass in sorted(cmds.iteritems()):
+    for cmdname, cmdclass in sorted(cmds.items()):
-            for spider, methods in sorted(contract_reqs.iteritems()):
+            for spider, methods in sorted(contract_reqs.items()):
-                    request.cookies.iteritems()]
+                    six.iteritems(request.cookies)]
-                field_iter = item.fields.iterkeys()
+                field_iter = six.iterkeys(item.fields)
-                field_iter = item.iterkeys()
+                field_iter = six.iterkeys(item)
-        for key, val in value.iteritems():
+        for key, val in six.iteritems(value):
-            for k,v in value.iteritems():
+            for k, v in six.iteritems(value):
-            for k,v in value.iteritems():
+            for k, v in six.iteritems(value):
-        for cls, wdict in live_refs.iteritems():
+        for cls, wdict in six.iteritems(live_refs):
-            for metakey, metavalue in meta.iteritems():
+            for metakey, metavalue in six.iteritems(meta):
-        for thumb_id, size in self.THUMBS.iteritems():
+        for thumb_id, size in six.iteritems(self.THUMBS):
-        for fmt, func in self._formats.iteritems():
+        for fmt, func in six.iteritems(self._formats):
-        for scheme, clspath in handlers.iteritems():
+        for scheme, clspath in six.iteritems(handlers):
-            items = formdata.iteritems() if isinstance(formdata, dict) else formdata
+            items = formdata.items() if isinstance(formdata, dict) else formdata
-    values.extend(formdata.iteritems())
+    values.extend(formdata.items())
-            u''.join(u'[@%s="%s"]' % c for c in clickdata.iteritems())
+            u''.join(u'[@%s="%s"]' % c for c in six.iteritems(clickdata))
-        for n, v in attrs.iteritems():
+        for n, v in six.iteritems(attrs):
-            for k, v in dict(*args, **kwargs).iteritems():
+            for k, v in six.iteritems(dict(*args, **kwargs)):
-        for mimetype, cls in self.CLASSES.iteritems():
+        for mimetype, cls in six.iteritems(self.CLASSES):
-        for k, v in sorted(self.vars.iteritems()):
+        for k, v in sorted(self.vars.items()):
-        return [name for name, cls in self._spiders.iteritems()
+        return [name for name, cls in six.iteritems(self._spiders)
-        out = os.tmpfile()
+        out = tempfile.TemporaryFile()
-            crawler.settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])
+            six.iteritems(crawler.settings.get('DEFAULT_REQUEST_HEADERS'))])
-        for key, value in kwargs.iteritems():
+        for key, value in six.iteritems(kwargs):
-        seq = seq.iteritems() if isinstance(seq, dict) else seq
+        seq = seq.items() if isinstance(seq, dict) else seq
-    for k, v in dict(dct_or_tuples).iteritems():
+    for k, v in six.iteritems(dict(dct_or_tuples)):
-import weakref, os
+import weakref, os, six
-    for cls, wdict in live_refs.iteritems():
+    for cls, wdict in six.iteritems(live_refs):
-    for cls, wdict in live_refs.iteritems():
+    for cls, wdict in six.iteritems(live_refs):
-                return min(wdict.iteritems(), key=itemgetter(1))[0]
+                return min(six.iteritems(wdict), key=itemgetter(1))[0]
-    for cls, wdict in live_refs.iteritems():
+    for cls, wdict in six.iteritems(live_refs):
-            return wdict.iterkeys()
+            return six.iterkeys(wdict)
-import types, weakref
+import types, weakref, six
-			for sig,recs in connections.get(signal,{}).iteritems():
+			for sig, recs in six.iteritems(connections.get(signal,{})):
-from ConfigParser import SafeConfigParser
+from six.moves.configparser import SafeConfigParser
-	if connections.has_key(senderkey):
+	if senderkey in connections:
-	if signals.has_key(signal):
+	if signal in signals:
-					if not receivers.has_key( receiver ):
+					if receiver not in receivers:
-        if named.has_key(name):
+        if name in named:
-import cPickle as pickle
+from six.moves import cPickle as pickle
-import cPickle as pickle
+from six.moves import cPickle as pickle
-import os, cPickle as pickle
+import os
-import marshal, cPickle as pickle
+import marshal
-import unittest, json, cPickle as pickle
+import unittest, json
-import cPickle as pickle
+from six.moves import cPickle as pickle
-            spider=spider)
+        dfd.addErrback(
-                request_result, request, spider)
+            return self.spidermw.scrape_response(
-                request_result, request, spider)
+            return dfd.addErrback(
-            spider=spider)
+        referer = request.headers.get('Referer')
-                and not download_failure.check(IgnoreRequest):
+        if (isinstance(download_failure, Failure) and
-                    item=item, response=response, spider=spider, exception=output.value)
+                return self.signals.send_catch_log_deferred(
-                item=output, response=response, spider=spider)
+            return self.signals.send_catch_log_deferred(
-            "BaseSgmlLinkExtractor is deprecated and will be removed in future releases!!! "
+            "BaseSgmlLinkExtractor is deprecated and will be removed in future releases. "
-            if not self.scan_tag(el.tag):
+            if not self.scan_tag(_nons(el.tag)):
-                links.append(link)
+            # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)
-from scrapy.http import HtmlResponse
+from scrapy.http import HtmlResponse, XmlResponse
-            self._overrides = o = _DictProxy()
+            self._overrides = o = _DictProxy(self, 'cmdline')
-            self._defaults = o = _DictProxy()
+            self._defaults = o = _DictProxy(self, 'default')
-        tunnelReq = 'CONNECT %s:%s HTTP/1.1\n' % (self._tunneledHost,
+        tunnelReq = 'CONNECT %s:%s HTTP/1.1\r\n' % (self._tunneledHost,
-            tunnelReq += '\n'
+            tunnelReq += 'Proxy-Authorization: %s\r\n' % self._proxyAuthHeader
-    def test_representation(self):
+    def test_representation_slice(self):
-            [u"<Selector xpath='//input/@name' data=u'{}'>".format(40 * 'b')]
+            ["<Selector xpath='//input/@name' data=u'{}'>".format(40 * 'b')]
-
+from sgmllib import SGMLParser
-from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_unicode
+from scrapy.utils.python import unique as unique_list, str_to_unicode
-class BaseSgmlLinkExtractor(FixedSGMLParser):
+class BaseSgmlLinkExtractor(SGMLParser):
-            FixedSGMLParser.__init__(self)
+        SGMLParser.__init__(self)
-        FixedSGMLParser.reset(self)
+        SGMLParser.reset(self)
-
+        Warning: this class is deprecated and will be removed in future
-from urlparse import urlparse, urljoin
+from urlparse import urljoin
-        FixedSGMLParser.__init__(self)
+        warnings.warn(
-            unique=unique, process_value=process_value)
+
-from sgmllib import SGMLParser
+import six
-    method. This is the same class with the bug fixed"""
+from scrapy.exceptions import ScrapyDeprecationWarning
-        return self.convert_codepoint(n)
+
-import _monkeypatches
+from . import _monkeypatches
-from scrapy.item import Item, Field
+if sys.version_info[0] == 2:
-        sflo = log.start_from_crawler(crawler)
+        log_observer = log.start_from_crawler(crawler)
-            crawler.signals.connect(sflo.stop, signals.engine_stopped)
+        if log_observer:
-    sflo = ScrapyFileLogObserver(file, loglevel, logencoding, crawler)
+    log_observer = ScrapyFileLogObserver(file, loglevel, logencoding, crawler)
-    log.startLoggingWithObserver(sflo.emit, setStdout=logstdout)
+    log.startLoggingWithObserver(log_observer.emit, setStdout=logstdout)
-    return sflo
+    return log_observer
-        msg("Scrapy %s started (bot: %s)" % (scrapy.__version__, \
+    log_observer = start_from_settings(settings)
-        sflo.stop()
+        log_observer.stop()
-        self.sflo.start()
+        self.log_observer = log.ScrapyFileLogObserver(self.f, self.level, self.encoding)
-        self.sflo.stop()
+        self.log_observer.stop()
-from scrapy.settings import Settings, SettingsAttribute
+from scrapy.settings import Settings, SettingsAttribute, CrawlerSettings
-    def test_deprecated_attribute(self):
+    def test_deprecated_attribute_overrides(self):
-
+        links = []
-                if self.unique else self.links
+                links.append(link)
-        return links
+        return unique_list(links, key=lambda link: link.url) \
-This package contains a collection of Link Extractors. 
+This package contains a collection of Link Extractors.
-from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
+from scrapy.contrib.linkextractors import LinkExtractor
-    link_extractor = SgmlLinkExtractor()
+    link_extractor = LinkExtractor()
-from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
+from scrapy.contrib.linkextractors import LinkExtractor
-        xlink = SgmlLinkExtractor()
+        xlink = LinkExtractor()
-from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
+from scrapy.contrib.linkextractors import LinkExtractor
-                Rule(SgmlLinkExtractor(), process_links="dummy_process_links"),
+                Rule(LinkExtractor(), process_links="dummy_process_links"),
-                Rule(SgmlLinkExtractor(), process_links="filter_process_links"),
+                Rule(LinkExtractor(), process_links="filter_process_links"),
-                Rule(SgmlLinkExtractor(), process_links="dummy_process_links"),
+                Rule(LinkExtractor(), process_links="dummy_process_links"),
-
+        self._contextFactory = contextFactory
-        'Twisted>=10.0.0,<14.0.0',
+        'Twisted>=10.0.0',
-import lxml.html
+import re
-from scrapy.utils.python import unique as unique_list
+from scrapy.utils.misc import arg_to_iter
-                    self.links.append(link)
+    def _iter_links(self, document):
-        return self._extract_links(response.body, response.url)
+        html = Selector(response)
-from scrapy.linkextractor import IGNORED_EXTENSIONS
+from scrapy.linkextractor import FilteringLinkExtractor
-class SgmlLinkExtractor(BaseSgmlLinkExtractor):
+class SgmlLinkExtractor(FilteringLinkExtractor):
-                                       process_value=process_value)
+        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))
-        return any(allowed) and not any(denied)
+import re
-        lx = SgmlLinkExtractor()
+        lx = self.extractor_cls()
-        lx = SgmlLinkExtractor()
+        lx = self.extractor_cls()
-        lx = SgmlLinkExtractor(allow=('sample', ))
+        lx = self.extractor_cls(allow=('sample', ))
-        lx = SgmlLinkExtractor(allow=('sample', ), unique=False)
+        lx = self.extractor_cls(allow=('sample', ), unique=False)
-        lx = SgmlLinkExtractor(allow=('sample', ))
+        lx = self.extractor_cls(allow=('sample', ))
-        lx = SgmlLinkExtractor(allow=('sample', ), deny=('3', ))
+        lx = self.extractor_cls(allow=('sample', ), deny=('3', ))
-        lx = SgmlLinkExtractor(allow_domains=('google.com', ))
+        lx = self.extractor_cls(allow_domains=('google.com', ))
-        lx = SgmlLinkExtractor(allow='sample')
+        lx = self.extractor_cls(allow='sample')
-        lx = SgmlLinkExtractor(allow='sample', deny='3')
+        lx = self.extractor_cls(allow='sample', deny='3')
-        lx = SgmlLinkExtractor(allow_domains='google.com')
+        lx = self.extractor_cls(allow_domains='google.com')
-        lx = SgmlLinkExtractor(deny_domains='example.com')
+        lx = self.extractor_cls(deny_domains='example.com')
-        lx = SgmlLinkExtractor(allow=(r'stuff1', ))
+        lx = self.extractor_cls(allow=(r'stuff1', ))
-        lx = SgmlLinkExtractor(deny=(r'uglystuff', ))
+        lx = self.extractor_cls(deny=(r'uglystuff', ))
-        lx = SgmlLinkExtractor(allow_domains=('evenmorestuff.com', ))
+        lx = self.extractor_cls(allow_domains=('evenmorestuff.com', ))
-        lx = SgmlLinkExtractor(deny_domains=('lotsofstuff.com', ))
+        lx = self.extractor_cls(deny_domains=('lotsofstuff.com', ))
-        lx = SgmlLinkExtractor(allow=('blah1',), deny=('blah2',),
+        lx = self.extractor_cls(allow=('blah1',), deny=('blah2',),
-        lx = SgmlLinkExtractor(restrict_xpaths=('//div[@id="subwrapper"]', ))
+        lx = self.extractor_cls(restrict_xpaths=('//div[@id="subwrapper"]', ))
-        lx = SgmlLinkExtractor(restrict_xpaths="//div[@class='links']")
+        lx = self.extractor_cls(restrict_xpaths="//div[@class='links']")
-        lx = SgmlLinkExtractor(restrict_xpaths="//div")
+        lx = self.extractor_cls(restrict_xpaths="//div")
-        lx = SgmlLinkExtractor()
+        lx = self.extractor_cls()
-        lx = SgmlLinkExtractor(restrict_xpaths="//div")
+        lx = self.extractor_cls(restrict_xpaths="//div")
-        lx = SgmlLinkExtractor()
+        lx = self.extractor_cls()
-        lx = SgmlLinkExtractor(process_value=process_value)
+        lx = self.extractor_cls(process_value=process_value)
-        lx = SgmlLinkExtractor(restrict_xpaths="//p")
+        lx = self.extractor_cls(restrict_xpaths="//p")
-        lx = SgmlLinkExtractor(attrs="href")
+        lx = self.extractor_cls(attrs="href")
-        lx = SgmlLinkExtractor(attrs=("href","src"), tags=("a","area","img"), deny_extensions=())
+        lx = self.extractor_cls(attrs=("href","src"), tags=("a","area","img"), deny_extensions=())
-        lx = SgmlLinkExtractor(attrs=None)
+        lx = self.extractor_cls(attrs=None)
-        lx = SgmlLinkExtractor(tags=None)
+        lx = self.extractor_cls(tags=None)
-        lx = SgmlLinkExtractor()
+        lx = self.extractor_cls()
-        lx = SgmlLinkExtractor(tags="area")
+        lx = self.extractor_cls(tags="area")
-        lx = SgmlLinkExtractor(tags="a")
+        lx = self.extractor_cls(tags="a")
-        lx = SgmlLinkExtractor(tags=("a","img"), attrs=("href", "src"), deny_extensions=())
+        lx = self.extractor_cls(tags=("a","img"), attrs=("href", "src"), deny_extensions=())
-                          help="print all contract hooks")
+        parser.add_option("-v", "--verbose", dest="verbose", default=False, action='store_true',
-        runner = TextTestRunner(verbosity=opts.verbose)
+        runner = TextTestRunner(verbosity=2 if opts.verbose else 1)
-        regex = r'^(.*\.)?(%s)$' % '|'.join(re.escape(d) for d in allowed_domains)
+        regex = r'^(.*\.)?(%s)$' % '|'.join(re.escape(d) for d in allowed_domains if d is not None)
-        settings.overrides['TEST1'] = "%s + %s" % (settings['TEST1'], 'started')
+        settings.set('TEST1', "%s + %s" % (settings['TEST1'], 'started'))
-from scrapy.settings import CrawlerSettings
+from scrapy.settings import Settings
-        crawler.settings.overrides['USER_AGENT'] = 'CustomAgent'
+        crawler.settings = Settings()
-        crawler.settings.overrides['ROBOTSTXT_OBEY'] = True
+        crawler.settings.set('ROBOTSTXT_OBEY', True)
-            valid_output_formats = self.settings['FEED_EXPORTERS'].keys() + self.settings['FEED_EXPORTERS_BASE'].keys()
+            valid_output_formats = (
-            valid_output_formats = self.settings['FEED_EXPORTERS'].keys() + self.settings['FEED_EXPORTERS_BASE'].keys()
+            valid_output_formats = (
-            self.settings.overrides.update(arglist_to_dict(opts.set))
+            self.settings.setdict(arglist_to_dict(opts.set),
-            self.settings.overrides['LOG_FILE'] = opts.logfile
+            self.settings.set('LOG_ENABLED', True, priority='cmdline')
-            self.settings.overrides['LOG_LEVEL'] = opts.loglevel
+            self.settings.set('LOG_ENABLED', True, priority='cmdline')
-            self.settings.overrides['LOG_ENABLED'] = False
+            self.settings.set('LOG_ENABLED', False, priority='cmdline')
-                self.settings.overrides['FEED_URI'] = 'stdout:'
+                self.settings.set('FEED_URI', 'stdout:', priority='cmdline')
-                self.settings.overrides['FEED_URI'] = opts.output
+                self.settings.set('FEED_URI', opts.output, priority='cmdline')
-            self.settings.overrides['FEED_FORMAT'] = opts.output_format
+            self.settings.set('FEED_FORMAT', opts.output_format, priority='cmdline')
-                self.settings.overrides['FEED_URI'] = 'stdout:'
+                self.settings.set('FEED_URI', 'stdout:', priority='cmdline')
-                self.settings.overrides['FEED_URI'] = opts.output
+                self.settings.set('FEED_URI', opts.output, priority='cmdline')
-            self.settings.overrides['FEED_FORMAT'] = opts.output_format
+            self.settings.set('FEED_FORMAT', opts.output_format, priority='cmdline')
-    settings.defaults.update(cmd.default_settings)
+    settings.setdict(cmd.default_settings, priority='command')
-from scrapy.settings import CrawlerSettings
+from scrapy.settings import Settings
-    settings = CrawlerSettings(settings_module)
+        settings.setmodule(settings_module_path, priority='project')
-    settings.overrides = pickle.loads(pickled_settings) if pickled_settings else {}
+    if pickled_settings:
-            settings.overrides[k[7:]] = v
+    env_overrides = {k[7:]: v for k, v in os.environ.items() if
-    CrawlerSettings.
+    will be used to populate the crawler settings with a project level
-    from scrapy.settings import CrawlerSettings
+    from scrapy.settings import Settings
-    return Crawler(settings)
+    return Crawler(Settings(settings_dict))
-            self.set(name, defvalue, 'default')
+        self.setmodule(default_settings, priority='default')
-
+import six
-        super(CrawlerSettings, self).__init__(**kw)
+        Settings.__init__(self, **kw)
-        return super(CrawlerSettings, self).__getitem__(opt_name)
+        return Settings.__getitem__(self, opt_name)
-        self.global_defaults = default_settings
+    def __init__(self, values=None, priority='project'):
-        return getattr(self.global_defaults, opt_name, None)
+        value = None
-        if isinstance(value, basestring):
+        if isinstance(value, six.string_types):
-        for s in crawler.spiders.list():
+        for s in sorted(crawler.spiders.list()):
-                sh = InteractiveShellEmbed(banner1=banner)
+                try:
-from unittest import TextTestRunner
+from unittest import TextTestResult
-        self.results.stream = None
+        self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)
-        self.assertEqual([type(x) for x in output], [TestItem])
+        request.callback(response)
-        self.assertEqual([type(x) for x in output], [Request])
+        request.callback(response)
-        self.assertEqual([type(x) for x in output], [TestItem])
+        request.callback(response)
-from unittest import TextTestRunner
+from unittest import TextTestRunner, TextTestResult as _TextTestResult
-    return wrapper
+class TextTestResult(_TextTestResult):
-            help="only list contracts, without checking them")
+                          help="only list contracts, without checking them")
-            help="print all contract hooks")
+                          help="print all contract hooks")
-        self.results = TextTestRunner(verbosity=opts.verbose)._makeResult()
+        conman = ContractsManager([load_object(c) for c in contracts])
-            requests = self.get_requests(spider)
+            requests = self.get_requests(spider, conman, result)
-            self.exitcode = 0 if self.results.wasSuccessful() else 1
+            stop = time.time()
-    def get_requests(self, spider):
+    def get_requests(self, spider, conman, result):
-                request = self.conman.from_method(bound_method, self.results)
+                request = conman.from_method(bound_method, result)
-                    request.callback = _generate(request.callback)
+                self._clean_req(request, method, results)
-        self.testcase_post = self.create_testcase(method, 'post-hook')
+        self.testcase_pre = _create_testcase(method, '@%s pre-hook' % self.name)
-
+
-        self.assertEqual(r.url, "http://www.example.com/ajax.html?_escaped_fragment_=key=value")
+        self.assertEqual(r.url, "http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue")
-        self.assertEqual(r.url, "http://www.example.com/ajax.html?_escaped_fragment_=key=value")
+        self.assertEqual(r.url, "http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue")
-    'www.example.com/ajax.html?_escaped_fragment_=key=value'
+    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'
-    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key=value'
+    'www.example.com/ajax.html?k1=v1&k2=v2&_escaped_fragment_=key%3Dvalue'
-    'www.example.com/ajax.html?_escaped_fragment_=key=value'
+    'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'
-        'Twisted>=10.0.0',
+        'Twisted>=10.0.0,<14.0.0',
-        self.downloader = Downloader(crawler)
+        downloader_cls = load_object(self.settings['DOWNLOADER'])
-        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html#l259
+        # Reference nsHttpResponseHead::ComputeFreshnessLifetime
-        # http://dxr.mozilla.org/mozilla-central/netwerk/protocol/http/nsHttpResponseHead.cpp.html
+        # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#366
-        cls = type.__new__(mcs, class_name, bases, new_attrs)
+        cls = super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)
-                    item=item, spider=spider, exception=output.value)
+                    item=item, response=response, spider=spider, exception=output.value)
-        idle = scraper_idle and not (pending or downloading)
+        pending_start_requests = self.slot.start_requests is not None
-from scrapy.http import Request
+from scrapy.http import Request
-        filter.open()
+        dupefilter = RFPDupeFilter()
-        assert filter.request_seen(r1)
+        assert not dupefilter.request_seen(r1)
-        assert filter.request_seen(r3)
+        assert not case_insensitive_dupefilter.request_seen(r1)
-        filter.close('finished')
+        case_insensitive_dupefilter.close('finished')
-    BrokenStartRequestsSpider, SingleRequestSpider
+    BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider
-from scrapy.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning
-                        Selector, Settings)
+                        Settings)
-        xpath = 'sel.xpath("//p[@class=\'one\']/text()").extract()[0]'
+        xpath = 'response.xpath("//p[@class=\'one\']/text()").extract()[0]'
-        )
+        self._cached_selector = None
-        
+
-TELNETCONSOLE_HOST = '0.0.0.0'
+TELNETCONSOLE_HOST = '127.0.0.1'
-WEBSERVICE_HOST = '0.0.0.0'
+WEBSERVICE_HOST = '127.0.0.1'
-    host = parse_url(url).netloc
+    host = parse_url(url).netloc.lower()
-        return any(((host == d) or (host.endswith('.%s' % d)) for d in domains))
+        return any(((host == d.lower()) or (host.endswith('.%s' % d.lower())) for d in domains))
-version_info = tuple(int(v) for v in __version__.split('.')[:3])
+version_info = tuple(int(v) if v.isdigit() else v
-                        'instantiate scrapy.selector.Selector '
+                        'instantiate scrapy.Selector '
-    new_class_path='scrapy.selector.Selector',
+    new_class_path='scrapy.Selector',
-    new_class_path='scrapy.selector.Selector',
+    new_class_path='scrapy.Selector',
-    new_class_path='scrapy.selector.Selector',
+    new_class_path='scrapy.Selector',
-            self.assertIn('scrapy.selector.Selector', str(w[0].message))
+            self.assertIn('scrapy.Selector', str(w[0].message))
-            self.assertIn('scrapy.selector.Selector', str(w[1].message))
+            self.assertIn('scrapy.Selector', str(w[1].message))
-            self.assertIn('scrapy.selector.Selector', str(w[0].message))
+            self.assertIn('scrapy.Selector', str(w[0].message))
-            self.assertIn('scrapy.selector.Selector', str(w[1].message))
+            self.assertIn('scrapy.Selector', str(w[1].message))
-            self.assertIn('scrapy.selector.Selector', str(w[0].message))
+            self.assertIn('scrapy.Selector', str(w[0].message))
-            self.assertIn('scrapy.selector.Selector', str(w[1].message))
+            self.assertIn('scrapy.Selector', str(w[1].message))
-__all__ = ['__version__', 'version_info', 'optional_features', 'twisted_version']
+__all__ = ['__version__', 'version_info', 'optional_features', 'twisted_version',
-from __future__ import print_function
+
-    __version__ = __version__.decode('ascii')
+__version__ = pkgutil.get_data(__package__, 'VERSION').decode('ascii').strip()
-
+# Check minimum required Python version
-# ignore noisy twisted deprecation warnings
+# Ignore noisy twisted deprecation warnings
-from scrapy.xlib import urlparse_monkeypatches
+# Apply monkey patches to fix issues in external libraries
-
+    del boto
-
+    del django
-import inspect
+
-    {}
+    >>> parse_cachecontrol('public, max-age=3600') == {'public': None,
-def test_processor(x):
+def _test_procesor(x):
-    name_out = staticmethod(test_processor)
+    name_out = staticmethod(_test_procesor)
-version_info = tuple(__version__.split('.')[:3])
+version_info = tuple(int(v) for v in __version__.split('.')[:3])
-        super(SafeXMLParser, self).__init__(*args, resolve_entities=False, **kwargs)
+        kwargs.setdefault('resolve_entities', False)
-        
+
-    'xml': {'_parser': etree.XMLParser,
+    'xml': {'_parser': SafeXMLParser,
-        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True)
+        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True, resolve_entities=False)
-                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None,
+                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True, process_value=None,
-        attr_func = lambda x: x in attrs
+        self.deny_extensions = {'.' + e for e in arg_to_iter(deny_extensions)}
-                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))
+                raise UsageError("Unrecognized output format '%s', set one"
-                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))
+                raise UsageError("Unrecognized output format '%s', set one"
-from scrapy.utils.gz import gunzip
+from scrapy.utils.gz import gunzip, is_gzipped
-            if content_encoding:
+            if content_encoding and not is_gzipped(response):
-                          help="format to use for dumping items with -o (default: %default)")
+        parser.add_option("-t", "--output-format", metavar="FORMAT",
-                          help="format to use for dumping items with -o (default: %default)")
+        parser.add_option("-t", "--output-format", metavar="FORMAT",
-            opts.output_format = os.path.splitext(opts.output)[1].replace(".", "")
+            if not opts.output_format:
-            opts.output_format = os.path.splitext(opts.output)[1].replace(".", "")
+            if not opts.output_format:
-            help="format to use for dumping items with -o (default: %default)")
+        parser.add_option("-a", dest="spargs", action="append", default=[], metavar="NAME=VALUE",
-            help="format to use for dumping items with -o (default: %default)")
+        parser.add_option("-a", dest="spargs", action="append", default=[], metavar="NAME=VALUE",
-                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format,valid_output_formats))
+                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))
-        if "[-] -->" in l:
+    for line in loglines[::-1]:
-        if 200 <= response.status < 300: # common case
+        if 200 <= response.status < 300:  # common case
-                            status_code=response.status)
+            log.msg(
-from scrapy.utils.test import get_crawler
+from scrapy.utils.test import docrawl
-from scrapy.utils.test import get_crawler, get_testlog
+from scrapy.utils.test import docrawl, get_testlog
-from scrapy.utils.test import get_crawler, get_testlog
+from scrapy.utils.test import get_testlog, docrawl
-    return crawler.start()
+
-        self.res404.request = self.req
+        self.res200, self.res404 = _responses(self.req, [200, 404])
-        self.res402.request = self.req
+        self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])
-        self.res402.request = self.req
+        self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])
-            raise IgnoreRequest
+            raise IgnoreRequest("max redirections reached")
-    def __init__(self, settings):
+    def __init__(self, crawler):
-            # component (extension, middlware, etc).
+            # component (extension, middleware, etc).
-        self.assertFalse('clickable1' in fs, fs)
+        self.assertIn('clickable2', fs)
-        occurences = 0
+        occurrences = 0
-                occurences += 1
+                occurrences += 1
-        assertion = (self.min_bound <= occurences <= self.max_bound)
+        assertion = (self.min_bound <= occurrences <= self.max_bound)
-                (occurences, self.obj_name, expected))
+                (occurrences, self.obj_name, expected))
-        The subclass should override it if neccessary
+        The subclass should override it if necessary
-            # stdlib's resource module is only availabe on unix platforms.
+            # stdlib's resource module is only available on unix platforms.
-        """Handle the diferent cases of request's result been a Response or a
+        """Handle the different cases of request's result been a Response or a
-based on different criterias.
+This module implements a class which returns the appropriate Response class
-        """Return the most appropiate Response class for the given mimetype"""
+        """Return the most appropriate Response class for the given mimetype"""
-        """Return the most appropiate Response class from an HTTP Content-Type
+        """Return the most appropriate Response class from an HTTP Content-Type
-        """Return the most appropiate Response class by looking at the HTTP
+        """Return the most appropriate Response class by looking at the HTTP
-        """Return the most appropiate Response class from a file name"""
+        """Return the most appropriate Response class from a file name"""
-        """Try to guess the appropiate response based on the body content.
+        """Try to guess the appropriate response based on the body content.
-        """Guess the most appropiate Response class based on the given arguments"""
+        """Guess the most appropriate Response class based on the given arguments"""
-        raise ValueError('unknown reciever type %s %s'%(receiver, type(receiver)))
+        raise ValueError('unknown receiver type %s %s'%(receiver, type(receiver)))
-        # weren't limited to issueing HTTP/1.1 requests.
+        # weren't limited to issuing HTTP/1.1 requests.
-    without backwards compatability. Suggestions are welcome.
+    without backwards compatibility. Suggestions are welcome.
-            log.msg(format='File (code: %(status)s): Error downloading image from %(request)s referred in <%(referer)s>',
+            log.msg(format='File (code: %(status)s): Error downloading file from %(request)s referred in <%(referer)s>',
-            log.msg(format='File (empty-content): Empty image from %(request)s referred in <%(referer)s>: no-content',
+            log.msg(format='File (empty-content): Empty file from %(request)s referred in <%(referer)s>: no-content',
-        log.msg(format='File (%(status)s): Downloaded image from %(request)s referred in <%(referer)s>',
+        log.msg(format='File (%(status)s): Downloaded file from %(request)s referred in <%(referer)s>',
-            whyfmt = 'File (error): Error processing image from %(request)s referred in <%(referer)s>: %(errormsg)s'
+            whyfmt = 'File (error): Error processing file from %(request)s referred in <%(referer)s>: %(errormsg)s'
-            whyfmt = 'File (unknown-error): Error processing image from %(request)s referred in <%(referer)s>'
+            whyfmt = 'File (unknown-error): Error processing file from %(request)s referred in <%(referer)s>'
-
+from scrapy import log
-    def __init__(self, path=None, verbose_log=False):
+    def __init__(self, path=None, debug=False):
-        self.verbose_log = verbose_log
+        self.debug = debug
-        return cls(job_dir(settings), verbose_log)
+        debug = settings.getbool('DUPEFILTER_DEBUG')
-        if self.verbose_log:
+        if self.debug:
-    def send(self, to, subject, body, cc=None, attachs=(), _callback=None):
+    def send(self, to, subject, body, cc=None, attachs=(), mimetype='text/plain', _callback=None):
-            msg = MIMENonMultipart('text', 'plain')
+            msg = MIMENonMultipart(*mimetype.split('/', 1))
-from cStringIO import StringIO
+from cStringIO import StringIO
-            log.msg(format=fmt, request=request, level=self._log_level, spider=spider)
+            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)
-            log.msg(format=fmt, request=request, level=self._log_level, spider=spider)
+            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)
-    def __init__(self, path=None):
+    _log_level = log.DEBUG
-        return cls(job_dir(settings))
+        verbose_log = settings.getbool('DUPEFILTER_DEBUG')
-            log.msg(format=fmt, request=request, level=log.DEBUG, spider=spider)
+        if self.verbose_log:
-    global_tests = [
+    tests = [
-        "engine.spider_is_idle(spider)",
+        "engine.spider.name",
-    for test in global_tests:
+    checks = []
-            status['global'] += [(test, eval(test))]
+            checks += [(test, eval(test))]
-    return status
+            checks += [(test, "%s (exception)" % type(e).__name__)]
-    status = get_engine_status(engine)
+    checks = get_engine_status(engine)
-    for test, result in status['global']:
+    for test, result in checks:
-            s += "  %-50s : %s\n" % (test, result)
+
-
+import socket
-        self._assert_retried()
+        with mock.patch('socket.gethostbyname',
-            return any(c in candidates for c in mro)
+            return any(c in {cls, new_class} for c in mro)
-        deprecated_cls.__module__ = parent_module.__name__
+
-        self.current_link = None
+        if self.scan_tag(tag):
-            self.current_link.text = data.strip()
+        if self.current_link:
-                          Link(url='http://www.google.com/something', text=u''),])
+                          Link(url='http://www.google.com/something', text=u''),
-                          Link(url='http://www.google.com/something', text=u''),])
+                          Link(url='http://www.google.com/something', text=u''),
-        parser.add_option("-t", "--template", dest="template", default="crawl",
+        parser.add_option("-t", "--template", dest="template", default="basic",
-from os.path import join, exists
+from os.path import join, exists, abspath
-            seen = seen.union(links)
+                seen.add(link)
-from scrapy.http import Response, TextResponse, XmlResponse, HtmlResponse
+from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse
-from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider, SitemapSpider
+from scrapy.contrib.spiders import CrawlSpider, Rule, XMLFeedSpider, \
-        return [Link(url, text) for url, text in urlstext]
+        return [Link(clean_url(url).encode(response_encoding),
-        self.assertEqual(self.output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n')
+        self.assertCsvEqual(self.output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n')
-        self.assertEqual(output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n')
+        self.assertCsvEqual(output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n')
-        self.assertEqual(output.getvalue(), 'age\r\n22\r\n')
+        self.assertCsvEqual(output.getvalue(), 'age\r\n22\r\n')
-        self.assertEqual(output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n22,John\xc2\xa3\r\n')
+        self.assertCsvEqual(output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n22,John\xc2\xa3\r\n')
-        self.assertEqual(output.getvalue(), '22,John\xc2\xa3\r\n')
+        self.assertCsvEqual(output.getvalue(), '22,John\xc2\xa3\r\n')
-        self.assertEqual(output.getvalue(), '"Mary,Paul",John\r\n')
+        self.assertCsvEqual(output.getvalue(), '"Mary,Paul",John\r\n')
-        self.assertEqual(self.output.getvalue(), expected_value)
+        self.assertXmlEquivalent(self.output.getvalue(), expected_value)
-        self.assertEqual(output.getvalue(), expected_value)
+        self.assertXmlEquivalent(output.getvalue(), expected_value)
-        self.assertEqual(output.getvalue(), expected_value)
+        self.assertXmlEquivalent(output.getvalue(), expected_value)
-        self.assertEqual(output.getvalue(), expected_value)
+        self.assertXmlEquivalent(output.getvalue(), expected_value)
-        ])
+        self.assertEqual(lx.extract_links(self.response),
-        ])
+        self.assertEqual(lx.extract_links(self.response),
-        self.assertEqual(i.fields.keys(), ['age', 'name'])
+        self.assertSortedEqual(i.fields.keys(), ['age', 'name'])
-        self.assertEqual(i.fields.keys(), ['age', 'other', 'name'])
+        self.assertSortedEqual(i.fields.keys(), ['age', 'other', 'name'])
-        self.assertEqual(i.fields.keys(), ['age', 'name'])
+        self.assertSortedEqual(i.fields.keys(), ['age', 'name'])
-        self.assertEqual(i.fields.keys(), ['age', 'identifier', 'name'])
+        self.assertSortedEqual(i.fields.keys(), ['age', 'identifier', 'name'])
-        self.assertEqual(i.fields.keys(), ['age', 'name'])
+        self.assertSortedEqual(i.fields.keys(), ['age', 'name'])
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-        self.assertEquals(req2.headers.get('Cookie'), "C1=value1; galleta=salada")
+
-
+        self.assertCookieValEqual(req2.headers.get('Cookie'),'C1=value1; galleta=salada')
-        self.assertEquals(req4.headers.get('Cookie'), 'C2=value2; galleta=dulce')
+        self.assertCookieValEqual(req4.headers.get('Cookie'), 'C2=value2; galleta=dulce')
-                [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])
+        self.assertDictEqual(dict(h),
-        self.assertEqual(h.values(), ['ip2', 'text/html'])
+        self.assertSortedEqual(h.values(), ['ip2', 'text/html'])
-        self.assertEqual(r2.body, 'price=%C2%A3+100&one=two')
+        self.assertSortedEqual(r2.body.split('&'),
-        self.assertEqual(r3.body, 'colours=red&colours=blue&colours=green&price=%C2%A3+100')
+        self.assertSortedEqual(r3.body.split('&'),
-        self.assertEqual(i.values(), [u'Keys', u'Values', u'John'])
+        self.assertSortedEqual(i.keys(), ['keys', 'values', 'name'])
-        self.assertEqual(transport.value(), testvalue)
+        self.assertEqual(
-        self.stats_enabled = stats
+    def __init__(self, stats):
-        o = cls(stats=True)
+        o = cls(crawler.stats)
-
+                        self.stats.inc_value('offsite/domains', spider=spider)
-        self.mw = OffsiteMiddleware()
+        crawler = get_crawler()
-        o = cls()
+        o = cls(stats=True)
-                            ).encode(response.encoding)
+                            ).encode(response.encoding, errors='xmlcharrefreplace')
-        "<a\s.*?href=(\"[.#]+?\"|\'[.#]+?\'|[^\s]+?)(>|\s.*?>)(.*?)<[/ ]?a>", 
+        "<a\s.*?href=(\"[.#]+?\"|\'[.#]+?\'|[^\s]+?)(>|\s.*?>)(.*?)<[/ ]?a>",
-        urlstext = set([(clean_url(url), clean_text(text)) for url, _, text in links_text])
+        urlstext = set([(clean_url(url).encode(response_encoding), clean_text(text))
-Scrapy Shell
+"""Scrapy Shell
-from scrapy.utils.spider import create_spider_for_request
+from scrapy.settings import Settings
-from scrapy.exceptions import IgnoreRequest
+from scrapy.utils.spider import create_spider_for_request
-    relevant_classes = (Spider, Request, Response, BaseItem,
+    relevant_classes = (Crawler, Spider, Request, Response, BaseItem,
-Item Loader
+"""Item Loader
-"""
+"""
-from scrapy.utils.python import flatten
+from scrapy.utils.misc import arg_to_iter, extract_regex
-            item[field_name] = self.get_output_value(field_name)
+            value = self.get_output_value(field_name)
-            False)
+        self.assertFalse(any(map(lambda e: hasattr(e._root, 'getparent'), li_text)))
-            False)
+        self.assertFalse(any(map(lambda e: hasattr(e._root, 'getparent'), div_class)))
-            True)
+        self.assertTrue(all(map(lambda e: hasattr(e._root, 'getparent'), li_text)))
-            True)
+        self.assertTrue(all(map(lambda e: hasattr(e._root, 'getparent'), div_class)))
-                             smart_strings=False)
+                             smart_strings=self._lxml_smart_strings)
-            result = xpathev(query, namespaces=self.namespaces)
+            result = xpathev(query, namespaces=self.namespaces,
-HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmCacheStorage'
+HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.FilesystemCacheStorage'
-class AjaxCrawlableMiddleware(object):
+class AjaxCrawlMiddleware(object):
-        if not settings.getbool('AJAXCRAWLABLE_ENABLED'):
+        if not settings.getbool('AJAXCRAWL_ENABLED'):
-        self.lookup_bytes = settings.getint('AJAXCRAWLABLE_MAXSIZE', 32768)
+        self.lookup_bytes = settings.getint('AJAXCRAWL_MAXSIZE', 32768)
-                request=request)
+        ajax_crawl_request = request.replace(url=request.url+'#!')
-        return ajax_crawlable
+        ajax_crawl_request.meta['ajax_crawlable'] = True
-AJAXCRAWLABLE_ENABLED = False
+AJAXCRAWL_ENABLED = False
-    'scrapy.contrib.downloadermiddleware.ajaxcrawlable.AjaxCrawlableMiddleware': 560,
+    'scrapy.contrib.downloadermiddleware.ajaxcrawl.AjaxCrawlMiddleware': 560,
-from scrapy.contrib.downloadermiddleware.ajaxcrawlable import AjaxCrawlableMiddleware
+from scrapy.contrib.downloadermiddleware.ajaxcrawl import AjaxCrawlMiddleware
-__doctests__ = ['scrapy.contrib.downloadermiddleware.ajaxcrawlable']
+__doctests__ = ['scrapy.contrib.downloadermiddleware.ajaxcrawl']
-class AjaxCrawlableMiddlewareTest(unittest.TestCase):
+class AjaxCrawlMiddlewareTest(unittest.TestCase):
-        self.mw = AjaxCrawlableMiddleware.from_crawler(crawler)
+        crawler = get_crawler({'AJAXCRAWL_ENABLED': True})
-    def test_ajax_crawlable(self):
+    def test_ajaxcrawl(self):
-    def test_ajax_crawlable_loop(self):
+    def test_ajaxcrawl_loop(self):
-        super(XPathSelectorList, self).__init__(*a, **kw)
+def _xpathselector_css(self, *a, **kw):
-                assert 'deprecated' in str(w[-1].message)
+    def test_warnings_xpathselector(self):
-                _ = UserClass()  # subclass instances don't warn
+            _, lineno = Deprecated(), inspect.getlineno(inspect.currentframe())
-        # hostanme can be None for wrong urls (like javascript links)
+        # hostname can be None for wrong urls (like javascript links)
-        "regexp": "http://exslt.org/regular-expressions",
+        "re": "http://exslt.org/regular-expressions",
-        # regexp:test()
+        # re:test()
-            [x.extract() for x in sel.xpath('//input[regexp:test(@name, "[A-Z]+", "i")]')])
+                '//input[re:test(@name, "[A-Z]+", "i")]').extract(),
-                 '//a[regexp:test(@href, "\.html$")]/text()')],
+                 '//a[re:test(@href, "\.html$")]/text()')],
-                 '//a[regexp:test(@href, "first")]/text()')],
+                 '//a[re:test(@href, "first")]/text()')],
-                 '//a[regexp:test(@href, "second")]/text()')],
+                 '//a[re:test(@href, "second")]/text()')],
-        # regexp:match() is rather special: it returns a node-set of <match> nodes
+        # re:match() is rather special: it returns a node-set of <match> nodes
-            sel.xpath('regexp:match(//a[regexp:test(@href, "\.xml$")]/@href,'
+            sel.xpath('re:match(//a[re:test(@href, "\.xml$")]/@href,'
-        # regexp:replace()
+        # re:replace()
-            sel.xpath('regexp:replace(//a[regexp:test(@href, "\.xml$")]/@href,'
+            sel.xpath('re:replace(//a[re:test(@href, "\.xml$")]/@href,'
-        "str": "http://exslt.org/strings",
+        "set": "http://exslt.org/sets"
-                         [u'second link'])
+        self.assertEqual(
-                             u'/xml/index.xml?/xml/utils/rechecker.xml'])
+        self.assertEqual(
-                         [u'https://www.bayes.co.uk/xml/index.xml?/xml/utils/rechecker.html'])
+        self.assertEqual(
-)
+        )
-            [u'url', u'name', u'startDate', u'location', u'offers'])
+                         [u'url', u'name', u'startDate', u'location', u'offers'])
-from scrapy.utils.job import job_dir
+
-    def close(self, reason): # can return a deferred
+    def close(self, reason):  # can return a deferred
-    def log(self, request, spider): # log that a request has been filtered
+    def log(self, request, spider):  # log that a request has been filtered
-        fp = request_fingerprint(request)
+        fp = self.request_fingerprint(request)
-        self.cmd.run(args, opts)
+        'six>=1.5.2',
-        crawler.signals.connect(o.engine_stopped, signals.engine_stopped)
+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)
-    def engine_stopped(self):
+    def spider_closed(self, spider, reason):
-        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage))
+        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage), spider=spider)
-            self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict))
+            self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict), spider=spider)
-
+import sys
-    
+
-        return self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss * 1024
+        size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss
-        self.namespaces = ns
+        self.namespaces = dict(self._default_namespaces)
-        self.namespaces = namespaces
+        ns = copy.copy(self._default_namespaces)
-from scrapy.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest, NotConfigured
-        ROBOTS = ''
+        ROBOTS = re.sub(r'^\s+(?m)', '', '''
-        reactor.callLater(0, deferred.callback, response)
+        def return_response(request, spider):
-        self.assertRaises(IgnoreRequest, middleware.process_request, Request('http://site.local/forbidden'), spider)
+        spider = None  # not actually used
-            old = meta.deprecated_class
+            old = DeprecatedClass.deprecated_class
-                                                   new=_clspath(new_class))
+                                                   old=_clspath(old, old_class_path),
-                                                   new=_clspath(new_class))
+                msg = instance_warn_message.format(cls=_clspath(cls, old_class_path),
-def _clspath(cls):
+def _clspath(cls, forced=None):
-    class DeprecatedClass(type):
+    class DeprecatedClass(new_class.__class__):
-                                                 warn_category=MyWarning)
+    def test_subclassing_warning_message(self):
-
+        w = self._mywarnings(w)
-            str(msg.message),
+            str(w[0].message),
-        self.assertEqual(msg.lineno, inspect.getsourcelines(UserClass)[1])
+        self.assertEqual(w[0].lineno, inspect.getsourcelines(UserClass)[1])
-                                                 warn_category=MyWarning)
+    def test_subclassing_warns_only_on_direct_childs(self):
-            _ = UserClass()
+            class NoWarnOnMe(UserClass):
-        self.assertEqual(msg.lineno, lineno)
+        w = self._mywarnings(w)
-    def test_warning_shown_everytime(self):
+    def test_subclassing_warns_once_by_default(self):
-            class U1(Deprecated):
+            class UserClass(Deprecated):
-            class U2(Deprecated):
+            class FooClass(Deprecated):
-            _i2 = Deprecated()
+            class BarClass(Deprecated):
-        assert issubclass(w[1].category, MyWarning)
+        w = self._mywarnings(w)
-    def test_warning_shown_once(self):
+    def test_warning_on_instance(self):
-            class U2(Deprecated):
+        # ignore subclassing warnings
-        # warns once instantations in the same lineno
+            # warns only once on instantations in the same lineno
-                _i2 = Deprecated()
+                _, lineno = Deprecated(), inspect.getlineno(inspect.currentframe())
-        assert issubclass(w[0].category, MyWarning)
+        w = self._mywarnings(w)
-                            warn_once=False,
+                            warn_once=True,
-            if (cls is not old) and not (warn_once and meta.warned_on_subclass):
+            if old in bases and not (warn_once and meta.warned_on_subclass):
-            _i2 = Deprecated()
+            for _ in range(10):
-        self.assertEqual(len(w), 1)
+        self.assertEqual(len(w), 2)
-                meta.warned_on_instance = True
+            if cls is old:
-            if cls is not old:
+            meta = cls.__class__
-            if cls is cls.__class__.deprecated_class:
+            meta = cls.__class__
-        super(XPathItemLoader, self).__init__(*a, **kw)
+XPathItemLoader = create_deprecated_class('XPathItemLoader', ItemLoader)
-            level=DEBUG)
+            level=INFO)
-        msg(format="Overridden settings: %(settings)r", settings=d, level=DEBUG)
+        msg(format="Overridden settings: %(settings)r", settings=d, level=INFO)
-        log.msg(format="Enabled %(componentname)ss: %(enabledlist)s", level=log.DEBUG,
+        log.msg(format="Enabled %(componentname)ss: %(enabledlist)s", level=log.INFO,
-        warnings.warn('%s is deprecated, instanciate scrapy.contrib.loader.ItemLoader '
+        warnings.warn('%s is deprecated, instantiate scrapy.contrib.loader.ItemLoader '
-        warnings.warn('%s is deprecated, instanciate scrapy.selector.Selector '
+        warnings.warn('%s is deprecated, instantiate scrapy.selector.Selector '
-                           'instanciate scrapy.selector.Selector '
+                           'instantiate scrapy.selector.Selector '
-        warnings.warn('XPathSelectorList is deprecated, instanciate '
+        warnings.warn('XPathSelectorList is deprecated, instantiate '
-            "instanciate scrapy.tests.test_utils_deprecate.NewName instead."
+            "instantiate scrapy.tests.test_utils_deprecate.NewName instead."
-                                    "instanciate {new} instead."):
+                                    "instantiate {new} instead."):
-    its subclasses are instanciated.
+    It also warns when the deprecated class is instantiated, but do not when
-                                         "please inherit from {new}."):
+                            subclass_warn_message="{cls} inherits from "\
-                                        new=_clspath(new_class))
+            old = cls.__class__.deprecated_class
-            "Please inherit from scrapy.tests.test_utils_deprecate.NewName."
+            "scrapy.tests.test_utils_deprecate.UserClass inherits from "
-                            warn_message=None):
+                            warn_message="{cls} inherits from deprecated class {old}, "\
-    deprecated = {}
+        deprecated_class = None
-                            .format(_clspath(deprecated['cls']), _clspath(cls), _clspath(new_class))
+            if cls is not cls.__class__.deprecated_class:
-
+    deprecated_cls = DeprecatedClass(name, (new_class,), clsdict or {})
-import urllib
+import urllib, urlparse
-        url = form.action or form.base_url
+        url = _get_form_url(form, kwargs.pop('url', None))
-        return cls(url, method=method, formdata=formdata, **kwargs)
+        return cls(url=url, method=method, formdata=formdata, **kwargs)
-            Deprecated = create_deprecated_class('Deprecated', NewName, MyWarning)
+            Deprecated = create_deprecated_class('Deprecated', NewName,
-def create_deprecated_class(name, new_class, warn_category=ScrapyDeprecationWarning, message=None):
+def create_deprecated_class(name, new_class, clsdict=None,
-        def __init__(cls, name, bases, clsdict):
+        def __init__(cls, name, bases, clsdict_):
-                    msg = message
+                if warn_message is not None:
-            super(DeprecatedClass, cls).__init__(name, bases, clsdict)
+            super(DeprecatedClass, cls).__init__(name, bases, clsdict_)
-    deprecated_cls = DeprecatedClass(name, (new_class,), {})
+    clsdict = clsdict if clsdict is not None else {}
-from scrapy.utils.deprecate import deprecated_base_class
+from scrapy.utils.deprecate import create_deprecated_class
-    __metaclass__ = deprecated_base_class(Spider, "scrapy.spider.BaseSpider was deprecated. Please inherit from scrapy.spider.Spider.")
+BaseSpider = create_deprecated_class('BaseSpider', Spider)
-from scrapy.utils.deprecate import deprecated_base_class
+from scrapy.utils.deprecate import create_deprecated_class
-                __metaclass__ = deprecated_base_class(NewName, "message")
+            Deprecated = create_deprecated_class('Deprecated', NewName)
-                __metaclass__ = deprecated_base_class(NewName, "message", MyWarning)
+            Deprecated = create_deprecated_class('Deprecated', NewName, MyWarning)
-        self.assertEqual(str(msg.message), "message")
+        self.assertEqual(
-                __metaclass__ = deprecated_base_class(NewName)
+            Deprecated = create_deprecated_class('Deprecated', NewName)
-        # self.assertIn("scrapy.tests.test_utils_deprecate.Deprecated", msg)
+        self.assertIn("scrapy.tests.test_utils_deprecate.Deprecated", msg)
-                __metaclass__ = deprecated_base_class(NewName, "message", MyWarning)
+            DeprecatedName = create_deprecated_class('DeprecatedName', NewName)
-                __metaclass__ = deprecated_base_class(NewName, "message", MyWarning)
+            DeprecatedName = create_deprecated_class('DeprecatedName', NewName)
-        self.assertRaises(ValueError, define_invalid)
+
-def deprecated_base_class(new_class, message=None, category=ScrapyDeprecationWarning):
+def create_deprecated_class(name, new_class, warn_category=ScrapyDeprecationWarning, message=None):
-    of a class this metaclass is applied to.
+    Return a "deprecated" class that causes its subclasses to issue a warning.
-    It can be used to rename a base class of some user classes, e.g. if we
+    It can be used to rename a base class in a library. For example, if we
-            __metaclass__ = deprecated_base_class(NewName, "OldName is deprecated. Please inherit from NewName.")
+        OldName = create_deprecated_class('OldName', NewName)
-                warn_message = "Base class of %s was deprecated. Please inherit from %s." % (cls_name, new_name)
+    deprecated = {}
-            super(Metaclass, cls).__init__(name, bases, clsdict)
+        def __init__(cls, name, bases, clsdict):
-    return Metaclass
+    deprecated_cls = DeprecatedClass(name, (new_class,), {})
-from scrapy.utils.deprecate import warn_when_subclassed
+from scrapy.utils.deprecate import deprecated_base_class
-    )
+    __metaclass__ = deprecated_base_class(Spider, "scrapy.spider.BaseSpider was deprecated. Please inherit from scrapy.spider.Spider.")
-            class MySpider(BaseSpider):
+            class MySpider1(BaseSpider):
-            self.assertEqual(w[0].lineno, inspect.getsourcelines(MySpider)[1])
+            self.assertEqual(w[0].lineno, inspect.getsourcelines(MySpider1)[1])
-from scrapy.utils.deprecate import warn_when_subclassed
+from scrapy.utils.deprecate import deprecated_base_class
-                __metaclass__ = warn_when_subclassed(NewName, "message")
+                __metaclass__ = deprecated_base_class(NewName, "message")
-                __metaclass__ = warn_when_subclassed(NewName, "message", MyWarning)
+                __metaclass__ = deprecated_base_class(NewName, "message", MyWarning)
-
+import inspect
-def warn_when_subclassed(superclass, message, category=ScrapyDeprecationWarning):
+def deprecated_base_class(new_class, message=None, category=ScrapyDeprecationWarning):
-    issue a warning when they are subclassed.
+    Return a metaclass that causes classes to issue a warning when
-                warnings.warn(message, category, stacklevel=2)
+
-        "Please inherit from scrapy.spider.Spider."
+        superclass=Spider,
-from scrapy.spider import Spider
+from scrapy.spider import Spider, BaseSpider
-def warn_when_subclassed(mro_len, message, category=ScrapyDeprecationWarning):
+def warn_when_subclassed(superclass, message, category=ScrapyDeprecationWarning):
-            if len(cls.mro()) > mro_len:
+            if len(cls.mro()) > len(superclass.mro()) + 1:
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class QPSSpider(BaseSpider):
+class QPSSpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-                default_spider=BaseSpider('default'))
+                default_spider=Spider('default'))
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class CrawlSpider(BaseSpider):
+class CrawlSpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class XMLFeedSpider(BaseSpider):
+class XMLFeedSpider(Spider):
-class CSVFeedSpider(BaseSpider):
+class CSVFeedSpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class InitSpider(BaseSpider):
+class InitSpider(Spider):
-    
+
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class SitemapSpider(BaseSpider):
+class SitemapSpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-    relevant_classes = (BaseSpider, Request, Response, BaseItem,
+    relevant_classes = (Spider, Request, Response, BaseItem,
-                                               BaseSpider('default'),
+                                               Spider('default'),
-
+from scrapy.utils.deprecate import warn_when_subclassed
-class BaseSpider(object_ref):
+class Spider(object_ref):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class MetaSpider(BaseSpider):
+class MetaSpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class MySpider(BaseSpider):
+class MySpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class MySpider(BaseSpider):
+class MySpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class TestSpider(BaseSpider):
+class TestSpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        spider = BaseSpider("default")
+        spider = Spider("default")
-        spider = BaseSpider("default")
+        spider = Spider("default")
-        file = storage.open(BaseSpider("default"))
+        file = storage.open(Spider("default"))
-        file = storage.open(BaseSpider("default"))
+        file = storage.open(Spider("default"))
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        spider = BaseSpider(name='default')
+        spider = Spider(name='default')
-        spider2 = BaseSpider(name='default')
+        spider2 = Spider(name='default')
-        spider = BaseSpider(name='default')
+        # consistent interface
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, Spider('foo')).addCallback(_test)
-        d = self.download_request(request, BaseSpider('foo'))
+        d = self.download_request(request, Spider('foo'))
-        d = self.download_request(request, BaseSpider('foo'))
+        d = self.download_request(request, Spider('foo'))
-        d = self.download_request(request, BaseSpider('foo'))
+        d = self.download_request(request, Spider('foo'))
-        d = self.download_request(request, BaseSpider('foo'))
+        d = self.download_request(request, Spider('foo'))
-        d = self.download_request(request, BaseSpider('foo'))
+        d = self.download_request(request, Spider('foo'))
-        spider = BaseSpider('foo')
+        spider = Spider('foo')
-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, Spider('foo')).addCallback(_test)
-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, Spider('foo')).addCallback(_test)
-        d = self.download_request(request, BaseSpider('foo'))
+        d = self.download_request(request, Spider('foo'))
-        d = self.download_request(request, BaseSpider('foo'))
+        d = self.download_request(request, Spider('foo'))
-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, Spider('foo')).addCallback(_test)
-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, Spider('foo')).addCallback(_test)
-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, Spider('foo')).addCallback(_test)
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        spider = BaseSpider('foo')
+        spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        spider = BaseSpider('foo')
+        spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class TestSpider(BaseSpider):
+class TestSpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('example.com')
+        self.spider = Spider('example.com')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-spider = BaseSpider('foo')
+spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('scrapytest.org')
+        self.spider = Spider('scrapytest.org')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        spider = BaseSpider('foo')
+        spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class TestSpider(BaseSpider):
+class TestSpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        spider = BaseSpider("myspider")
+        spider = Spider("myspider")
-        spider = BaseSpider("myspider")
+        spider = Spider("myspider")
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('default')
+        self.spider = Spider('default')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('media.com')
+        self.spider = Spider('media.com')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class BaseSpiderTest(unittest.TestCase):
+class SpiderTest(unittest.TestCase):
-    spider_class = BaseSpider
+    spider_class = Spider
-class InitSpiderTest(BaseSpiderTest):
+class InitSpiderTest(SpiderTest):
-class XMLFeedSpiderTest(BaseSpiderTest):
+class XMLFeedSpiderTest(SpiderTest):
-class CSVFeedSpiderTest(BaseSpiderTest):
+class CSVFeedSpiderTest(SpiderTest):
-class CrawlSpiderTest(BaseSpiderTest):
+class CrawlSpiderTest(SpiderTest):
-class SitemapSpiderTest(BaseSpiderTest):
+class SitemapSpiderTest(SpiderTest):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class Spider0(BaseSpider):
+class Spider0(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class Spider1(BaseSpider):
+class Spider1(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class Spider2(BaseSpider):
+class Spider2(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class Spider3(BaseSpider):
+class Spider3(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class Spider4(BaseSpider):
+class Spider4(Spider):
-from scrapy.contrib.spidermiddleware.depth import DepthMiddleware 
+from scrapy.contrib.spidermiddleware.depth import DepthMiddleware
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('scrapytest.org')
+        self.spider = Spider('scrapytest.org')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        return BaseSpider('foo', allowed_domains=['scrapytest.org', 'scrapy.org'])
+        return Spider('foo', allowed_domains=['scrapytest.org', 'scrapy.org'])
-        return BaseSpider('foo', allowed_domains=None)
+        return Spider('foo', allowed_domains=None)
-        return BaseSpider('foo')
+        return Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        reqs = [short_url_req, long_url_req] 
+        reqs = [short_url_req, long_url_req]
-        spider = BaseSpider('foo')
+        spider = Spider('foo')
-import unittest 
+import unittest
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider = BaseSpider('foo')
+        self.spider = Spider('foo')
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-class TestSpider(BaseSpider):
+class TestSpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        self.spider2 = BaseSpider('name2')
+        self.spider1 = Spider('name1')
-        assert isinstance(sp1, BaseSpider)
+        assert isinstance(sp1, Spider)
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        spider = BaseSpider(name='example.com')
+        spider = Spider(name='example.com')
-        class MySpider(BaseSpider):
+        class MySpider(Spider):
-        spider = BaseSpider(name='example.com', allowed_domains=['example.org', 'example.net'])
+        spider = Spider(name='example.com', allowed_domains=['example.org', 'example.net'])
-        spider = BaseSpider(name='example.com', allowed_domains=set(('example.com', 'example.net')))
+        spider = Spider(name='example.com', allowed_domains=set(('example.com', 'example.net')))
-        spider = BaseSpider(name='example.com', allowed_domains=('example.com', 'example.net'))
+        spider = Spider(name='example.com', allowed_domains=('example.com', 'example.net'))
-        class MySpider(BaseSpider):
+        class MySpider(Spider):
-from scrapy.spider import BaseSpider
+from scrapy.spider import Spider
-        if isinstance(obj, BaseSpider):
+        if isinstance(obj, Spider):
-    from scrapy.spider import BaseSpider
+    from scrapy.spider import Spider
-           issubclass(obj, BaseSpider) and \
+           issubclass(obj, Spider) and \
-        assert not isbinarytext(u"hello".encode('utf-16')) 
+        assert not isbinarytext(u"hello".encode('utf-16'))
-        partial_f1 = functools.partial(f1, None)
+        partial_f1 = functools.partial(f1, None)
-        self.assertEqual(get_func_args(partial_f1), ['a', 'b', 'c'])
+        self.assertEqual(get_func_args(partial_f1), ['b', 'c'])
-        return get_func_args(func.func)
+        return [x for x in get_func_args(func.func)[len(func.args):]
-from functools import wraps
+from functools import partial, wraps
-    'xls', 'ppt', 'doc', 'docx', 'odt', 'ods', 'odg', 'odp',
+    'xls', 'xlsx', 'ppt', 'pptx', 'doc', 'docx', 'odt', 'ods', 'odg', 'odp',
-    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar',
+    'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar',
-        if not hasattr(value, '__iter__'):
+        if value is None:
-import sys, time, random, urllib, os
+import sys, time, random, urllib, os, json
-    BrokenStartRequestsSpider
+    BrokenStartRequestsSpider, SingleRequestSpider
-        if not settings.getbool(self.enabled_setting):
+        if not settings.getbool('AJAXCRAWLABLE_ENABLED'):
-    seen = {}
+    seen = set()
-        if seenkey in seen: 
+        if seenkey in seen:
-        seen[seenkey] = 1
+        seen.add(seenkey)
-    
+
-from scrapy.http import Request, HtmlResponse
+from scrapy.http import Request, HtmlResponse, Response
-        assert resp2 is resp
+        self.assertIs(resp, resp2)
-
+        # XXX: Google parses at least first 100k bytes; scrapy's redirect
-        body = response.body_as_unicode()[:self._lookup_bytes]
+        body = response.body_as_unicode()[:self.lookup_bytes]
-        path = self.file_path(request)
+        path = self.file_path(request, response=response, info=info)
-from scrapy.http import Request
+from scrapy.http import Request, Response
-        self.assertEqual(image_path(Request("https://dev.mydeco.com/mydeco.gif")),
+    def test_file_path(self):
-        self.assertEqual(image_path(Request("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg")),
+        self.assertEqual(file_path(Request("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg")),
-        self.assertEqual(image_path(Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif")),
+        self.assertEqual(file_path(Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif")),
-        self.assertEqual(image_path(Request("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg")),
+        self.assertEqual(file_path(Request("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg")),
-        self.assertEqual(image_path(Request("http://www.dorma.co.uk/images/product_details/2532/")),
+        self.assertEqual(file_path(Request("http://www.dorma.co.uk/images/product_details/2532/")),
-        self.assertEqual(image_path(Request("http://www.dorma.co.uk/images/product_details/2532")),
+        self.assertEqual(file_path(Request("http://www.dorma.co.uk/images/product_details/2532")),
-        from scrapy.contrib.pipeline.images import ImagesPipeline
+class DeprecatedImagesPipeline(ImagesPipeline):
-        self.assertEqual(image_path(Request("https://dev.mydeco.com/mydeco.pdf")),
+        file_path = self.pipeline.file_path
-        self.assertEqual(image_path(Request("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt")),
+        self.assertEqual(file_path(Request("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt")),
-        self.assertEqual(image_path(Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc")),
+        self.assertEqual(file_path(Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc")),
-        self.assertEqual(image_path(Request("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg")),
+        self.assertEqual(file_path(Request("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg")),
-        self.assertEqual(image_path(Request("http://www.dorma.co.uk/images/product_details/2532/")),
+        self.assertEqual(file_path(Request("http://www.dorma.co.uk/images/product_details/2532/")),
-        self.assertEqual(image_path(Request("http://www.dorma.co.uk/images/product_details/2532")),
+        self.assertEqual(file_path(Request("http://www.dorma.co.uk/images/product_details/2532")),
-        self.assertEqual(self.pipeline.store._get_filesystem_path(key), path)
+        path = 'some/image/key.jpg'
-        self.assertEqual(image_path("https://dev.mydeco.com/mydeco.pdf"),
+        image_path = self.pipeline.file_path
-        self.assertEqual(image_path("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt"),
+        self.assertEqual(image_path(Request("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.txt")),
-        self.assertEqual(image_path("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc"),
+        self.assertEqual(image_path(Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.doc")),
-        self.assertEqual(image_path("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg"),
+        self.assertEqual(image_path(Request("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg")),
-        self.assertEqual(image_path("http://www.dorma.co.uk/images/product_details/2532/"),
+        self.assertEqual(image_path(Request("http://www.dorma.co.uk/images/product_details/2532/")),
-        self.assertEqual(image_path("http://www.dorma.co.uk/images/product_details/2532"),
+        self.assertEqual(image_path(Request("http://www.dorma.co.uk/images/product_details/2532")),
-        
+
- 
+
-        self.assertEqual(image_path("https://dev.mydeco.com/mydeco.gif"),
+        image_path = self.pipeline.file_path
-        self.assertEqual(image_path("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg"),
+        self.assertEqual(image_path(Request("http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg")),
-        self.assertEqual(image_path("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif"),
+        self.assertEqual(image_path(Request("https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif")),
-        self.assertEqual(image_path("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg"),
+        self.assertEqual(image_path(Request("http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg")),
-        self.assertEqual(image_path("http://www.dorma.co.uk/images/product_details/2532/"),
+        self.assertEqual(image_path(Request("http://www.dorma.co.uk/images/product_details/2532/")),
-        self.assertEqual(image_path("http://www.dorma.co.uk/images/product_details/2532"),
+        self.assertEqual(image_path(Request("http://www.dorma.co.uk/images/product_details/2532")),
-        thumbnail_name = self.pipeline.thumb_key
+        thumb_path = self.pipeline.thumb_path
-                         'thumbs/50/92dac2a6a2072c5695a5dff1f865b3cb70c657bb.jpg')
+        self.assertEqual(thumb_path(Request("file:///tmp/foo.jpg"), name),
-        
+
- 
+
-        if isinstance(request, unicode) or isinstance(request, str):
+        if not isinstance(request, Request):
-        if isinstance(request, unicode) or isinstance(request, str):
+        if not isinstance(request, Request):
-        if isinstance(request, unicode) or isinstance(request, str):
+        if not isinstance(request, Request):
-        if not hasattr(self.file_key, '_overridden'):
+        if not hasattr(self.file_key, '_base'):
-    file_key._overridden = False
+    file_key._base = True
-        if not hasattr(self.file_key, '_overridden'):
+        if not hasattr(self.file_key, '_base'):
-        elif not hasattr(self.image_key, '_overridden'):
+        elif not hasattr(self.image_key, '_base'):
-        if not hasattr(self.thumb_key, '_overridden'):
+        if not hasattr(self.thumb_key, '_base'):
-    file_key._overridden = False
+    file_key._base = True
-    image_key._overridden = False
+    image_key._base = True
-    thumb_key._overridden = False
+    thumb_key._base = True
-        return self.file_key(request.url)
+        ## start of deprecation warning block (can be removed in the future)
-        return 'full/%s%s' % (media_guid, media_ext)
+        return self.file_path(url)
-        return self.file_key(request.url)
+        ## start of deprecation warning block (can be removed in the future)
-        return self.thumb_key(request.url)
+        ## start of deprecation warning block (can be removed in the future)
-        return 'full/%s.jpg' % (image_guid)
+        return self.file_path(url)
-        return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)
+        return self.thumb_path(url, thumb_id)
-        absolute_path = self._get_filesystem_path(key)
+    def persist_file(self, path, buf, info, meta=None, headers=None):
-        absolute_path = self._get_filesystem_path(key)
+    def stat_file(self, path, info):
-        path_comps = key.split('/')
+    def _get_filesystem_path(self, path):
-    def stat_file(self, key, info):
+    def stat_file(self, path, info):
-        return self._get_boto_key(key).addCallback(_onsuccess)
+        return self._get_boto_key(path).addCallback(_onsuccess)
-    def _get_boto_key(self, key):
+    def _get_boto_key(self, path):
-        key_name = '%s%s' % (self.prefix, key)
+        key_name = '%s%s' % (self.prefix, path)
-    def persist_file(self, key, buf, info, meta=None, headers=None):
+    def persist_file(self, path, buf, info, meta=None, headers=None):
-        key_name = '%s%s' % (self.prefix, key)
+        key_name = '%s%s' % (self.prefix, path)
-            return {'url': request.url, 'path': key, 'checksum': checksum}
+            return {'url': request.url, 'path': path, 'checksum': checksum}
-        dfd = defer.maybeDeferred(self.store.stat_file, key, info)
+        path = self.file_path(request, info=info)
-            key = self.file_key(request.url)
+            path = self.file_path(request, response=response, info=info)
-        return {'url': request.url, 'path': key, 'checksum': checksum}
+        return {'url': request.url, 'path': path, 'checksum': checksum}
-        key = self.file_key(request.url)
+        path = self.file_path(request)
-        self.store.persist_file(key, buf, info)
+        self.store.persist_file(path, buf, info)
-        for key, image, buf in self.get_images(response, request, info):
+        for path, image, buf in self.get_images(response, request, info):
-                key, buf, info,
+                path, buf, info,
-        key = self.file_key(request.url)
+        path = self.file_path(request, response=response, info=info)
-        yield key, image, buf
+        yield path, image, buf
-            thumb_key = self.thumb_key(request.url, thumb_id)
+            thumb_path = self.thumb_path(request, thumb_id, response=response, info=info)
-            yield thumb_key, thumb_image, thumb_buf
+            yield thumb_path, thumb_image, thumb_buf
-
+
-            link.text = str_to_unicode(link.text, response_encoding, errors='replace')
+            link.text = str_to_unicode(link.text, response_encoding, errors='replace').strip()
-            self.current_link.text = self.current_link.text + data.strip()
+            self.current_link.text = self.current_link.text + data
-        self.current_link = None
+        if self.scan_tag(tag):
-            cacert = os.path.expanduser("~/.mitmproxy/mitmproxy-ca.pem")),
+            cacert = cert_path),
-            self.assertEquals(response.body, 'https://example.com')
+            self.assertEquals(response.body, 'http://example.com')
-        request = Request('https://example.com', meta={'proxy': http_proxy})
+        request = Request('http://example.com', meta={'proxy': http_proxy})
-from re import match
+    _responseMatcher = re.compile('HTTP/1\.. 200')
-        if match('HTTP/1\.. 200', bytes):
+        if  TunnelingTCP4ClientEndpoint._responseMatcher.match(bytes):
-            if  scheme == 'https' and not skipConnectTunnel:
+            omitConnectTunnel = proxyParams.find('noconnect') >= 0
-            _, _, proxyHost, proxyPort, _ = _parse(proxy)
+            _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)
-            if  scheme == 'https':
+            skipConnectTunnel = proxyParams.find('noconnect') >= 0
-        proxyHost, proxyPort = proxyConf
+        proxyHost, proxyPort, self._proxyAuthHeader = proxyConf
-                                                   self._tunneledPort)
+        tunnelReq = 'CONNECT %s:%s HTTP/1.1\n' % (self._tunneledHost,
-                proxyConf = (proxyHost, proxyPort)
+                proxyConf = (proxyHost, proxyPort,
-from twisted.internet import defer, reactor, protocol, ssl
+from twisted.internet import defer, reactor, protocol
-from twisted.internet.error import TimeoutError, SSLError
+from twisted.internet.error import TimeoutError
-    L{TCP4ClientEndpoint}.
+    accomplish that, this endpoint sends an HTTP CONNECT to the proxy.
-                contextFactory, timeout=30, bindAddress=None):
+    def __init__(self, reactor, host, port, proxyConf, contextFactory,
-        # temporarily to intercept the response from the proxy.
+        tunnelReq = 'CONNECT %s:%s HTTP/1.1\n\n' % (self._tunneledHost,
-        created, notifies the client that we are ready to send requests.
+        created, notifies the client that we are ready to send requests. If not
-            # The tunnel is ready, switch transport to TLS.
+        if match('HTTP/1\.. 200', bytes):
-            self._protocol.connectionLost = self.connectionLost
+            self._tunnelReadyDeferred.errback(
-        # Store the protocol factory as we will need it to switch to TLS.
+    def connect(self, protocolFactory):
-            return connectDeferred
+        connectDeferred.addCallback(self.requestTunnel)
-    def __init__(self, reactor, proxyHost, proxyPort, contextFactory=None,
+    def __init__(self, reactor, proxyConf, contextFactory=None,
-        self._proxyPort = proxyPort
+        self._proxyConf = proxyConf
-            self._connectTimeout, self._bindAddress)
+            self._proxyConf, self._contextFactory, self._connectTimeout,
-                 pool=None):
+    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None):
-                return self._TunnelingAgent(reactor, proxyHost, proxyPort,
+                proxyConf = (proxyHost, proxyPort)
-    HTTP CONNECT is always sent when using this endpoint, I think this could
+    accomplish that, this endpoint sends an HTTP CONNECT to the proxy. If the
-            raise SSLError
+            # The proxy could not open the tunnel and will drop the connection;
-    def connect(self, protocolFactory):
+    def connectionLost(self, reason):
-        return self._tunnelReadyDeferred
+        if openTunnel:
-    def download_request(self, request):    
+    def download_request(self, request):
-from twisted.internet import defer, reactor, protocol
+from twisted.internet import defer, reactor, protocol, ssl
-from twisted.internet.error import TimeoutError
+from twisted.internet.error import TimeoutError, SSLError
-    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None, pool=None):
+    def __init__(self, contextFactory=None, connectTimeout=10, bindAddress=None,
-            return self._ProxyAgent(endpoint)
+            _, _, proxyHost, proxyPort, _ = _parse(proxy)
-    def download_request(self, request):
+    def download_request(self, request):    
-        ev['system'] = spider.name
+        ev['system'] = unicode_to_str(spider.name, encoding)
-        regex = r'^(.*\.)?(%s)$' % '|'.join(domains)
+        regex = r'^(.*\.)?(%s)$' % '|'.join(re.escape(d) for d in allowed_domains)
-                       Request('http://offsite.tld/')]
+                       Request('http://offsite.tld/'),
-        request.headers.setdefault('Accept-Encoding', 'x-gzip,gzip,deflate')
+        request.headers.setdefault('Accept-Encoding', 'gzip,deflate')
-        self.assertEqual(request.headers.get('Accept-Encoding'), 'x-gzip,gzip,deflate')
+        self.assertEqual(request.headers.get('Accept-Encoding'), 'gzip,deflate')
-    def __init__(self, item=None, **context):
+    def __init__(self, item=None, selector=None, response=None, **context):
-        super(XPathItemLoader, self).__init__(item, **context)
+    def _check_selector_method(self):
-        values = self._get_values(xpath, **kw)
+        values = self._get_xpathvalues(xpath, **kw)
-        values = self._get_values(xpath, **kw)
+        values = self._get_xpathvalues(xpath, **kw)
-        values = self._get_values(xpath, **kw)
+        values = self._get_xpathvalues(xpath, **kw)
-from scrapy.contrib.loader import ItemLoader, XPathItemLoader
+from scrapy.contrib.loader import ItemLoader
-class ItemLoaderTest(unittest.TestCase):
+class BasicItemLoaderTest(unittest.TestCase):
-
+class SelectortemLoaderTest(unittest.TestCase):
-    response = HtmlResponse(url="", body='<html><body><div id="id">marta</div><p>paragraph</p></body></html>')
+    def test_constructor(self):
-        self.assertRaises(RuntimeError, XPathItemLoader)
+        l = TestItemLoader()
-        l = TestXPathItemLoader(selector=sel)
+        l = TestItemLoader(selector=sel)
-        l = TestXPathItemLoader(response=self.response)
+        l = TestItemLoader(response=self.response)
-        l = TestXPathItemLoader(response=self.response)
+        l = TestItemLoader(response=self.response)
-        l = TestXPathItemLoader(response=self.response)
+        l = TestItemLoader(response=self.response)
-        l = TestXPathItemLoader(response=self.response)
+        l = TestItemLoader(response=self.response)
-        l = TestXPathItemLoader(response=self.response)
+        l = TestItemLoader(response=self.response)
-        l = TestXPathItemLoader(response=self.response)
+        l = TestItemLoader(response=self.response)
-        self.handle_httpstatus_list = settings.getlist('HANDLE_HTTPSTATUS_LIST')
+        self.handle_httpstatus_all = settings.getbool('HTTPERROR_ALLOW_ALL')
-        self.mw = HttpErrorMiddleware(Settings({'HANDLE_HTTPSTATUS_LIST': (402,)}))
+        self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOWED_CODES': (402,)}))
-        self.mw = HttpErrorMiddleware(Settings({'HANDLE_HTTPSTATUS_ALL': True}))
+        self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOW_ALL': True}))
-            allowed_statuses = getattr(spider, 'handle_httpstatus_list', ())
+            allowed_statuses = getattr(spider, 'handle_httpstatus_list', self.handle_httpstatus_list)
-        self.mw = HttpErrorMiddleware()
+        self.mw = HttpErrorMiddleware(Settings({}))
-from scrapy.utils.iterators import csviter, xmliter
+from scrapy.utils.iterators import csviter, xmliter, _body_or_str
-
+class TestHelper(unittest.TestCase):
-    get_meta_refresh
+from scrapy.utils.response import response_httprepr, open_in_browser, get_meta_refresh
-    else:
+        if not unicode:
-    def wrapped(func):
+    def deco(func):
-        def new_func(*args, **kwargs):
+        def wrapped(*args, **kwargs):
-    return wrapped
+        return wrapped
-from scrapy.http import TextResponse
+from scrapy.http import TextResponse, Response
-    text = body_or_str(obj)
+    text = _body_or_str(obj)
-    lines = StringIO(body_or_str(obj, unicode=False))
+    lines = StringIO(_body_or_str(obj, unicode=False))
-from scrapy.http import Response, HtmlResponse, TextResponse
+from scrapy.http import HtmlResponse, TextResponse
-            'slot': slot,
+            'spider': self.crawler.engine.spider,
-    d.addErrback(lambda _: _.value.subFailure)
+    d = defer.DeferredList(dfds, fireOnOneErrback=1, consumeErrors=1)
-
+        self.assertEqual(set(i.errors), set(['age', 'name']))
-            self.resource = __import__('resource')
+            # stdlib's resource module is only availabe on unix platforms.
-USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % __import__('scrapy').__version__
+USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % import_module('scrapy').__version__
-            module = __import__('OpenSSL')
+            module = import_module('OpenSSL')
-    scrapy_path = __import__('scrapy').__path__[0]
+    scrapy_path = import_module('scrapy').__path__[0]
-        spiders_module = __import__(self.settings['NEWSPIDER_MODULE'], {}, {}, [''])
+        spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])
-        module = __import__(fname, {}, {}, [''])
+        module = import_module(fname)
-        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'], {}, {}, [''])
+        self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])
-            module = __import__('OpenSSL', {}, {}, [''])
+            module = __import__('OpenSSL')
-        mod = __import__(module, {}, {}, [''])
+        mod = import_module(module)
-    mod = __import__(path, {}, {}, [''])
+    mod = import_module(path)
-                submod = __import__(fullpath, {}, {}, [''])
+                submod = import_module(fullpath)
-from os.path import join, dirname, abspath, isabs, exists
+from importlib import import_module
-            __import__(scrapy_module)
+            import_module(scrapy_module)
-        settings_module = __import__(settings_module_path, {}, {}, [''])
+        settings_module = import_module(settings_module_path)
-
+        self.assertEqual(set(i.errors), set(['age', 'name']))
-            yield crawler.stop()
+        if self._active_crawler:
-            crawler.start()
+        if not self.crawlers or self.stopping:
-            return name, crawler
+        name, crawler = self.crawlers.popitem()
-from types import NoneType
+
-from urlparse import urlparse
+import sys
-    uses_netloc.append('s3')
+if sys.version_info[0] == 2:
-    uses_query.append('s3')
+    # workaround for http://bugs.python.org/issue7904 - Python < 2.7
-    print("Scrapy %s requires Python 2.6 or above" % __version__)
+if sys.version_info < (2, 7):
-from scrapy.utils.py27 import OrderedDict
+from collections import OrderedDict
-        if memo is None: 
+        if memo is None:
-        'Programming Language :: Python :: 2.6',
+from importlib import import_module
-            self.resource = __import__('resource')
+            # stdlib's resource module is only availabe on unix platforms.
-USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % __import__('scrapy').__version__
+USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % import_module('scrapy').__version__
-            module = __import__('OpenSSL')
+            module = import_module('OpenSSL')
-    scrapy_path = __import__('scrapy').__path__[0]
+    scrapy_path = import_module('scrapy').__path__[0]
-        spiders_module = __import__(self.settings['NEWSPIDER_MODULE'], {}, {}, [''])
+        spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])
-        module = __import__(fname, {}, {}, [''])
+        module = import_module(fname)
-        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'], {}, {}, [''])
+        self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])
-            module = __import__('OpenSSL', {}, {}, [''])
+            module = __import__('OpenSSL')
-        mod = __import__(module, {}, {}, [''])
+        mod = import_module(module)
-    mod = __import__(path, {}, {}, [''])
+    mod = import_module(path)
-                submod = __import__(fullpath, {}, {}, [''])
+                submod = import_module(fullpath)
-from os.path import join, dirname, abspath, isabs, exists
+from importlib import import_module
-            __import__(scrapy_module)
+            import_module(scrapy_module)
-        settings_module = __import__(settings_module_path, {}, {}, [''])
+        settings_module = import_module(settings_module_path)
-        print >> out_file, 'events: Ticks'
+        print('events: Ticks', file=out_file)
-        print >> self.out_file, 'summary: %d' % (max_cost,)
+        print('summary: %d' % (max_cost,), file=self.out_file)
-            print >> out_file, 'fi=~'
+            print('fi=~', file=out_file)
-        print >> out_file, 'fn=%s' % (label(code),)
+            print('fi=%s' % (code.co_filename,), file=out_file)
-            print >> out_file, '0 ', inlinetime
+            print('0 ', inlinetime, file=out_file)
-            print >> out_file, '%d %d' % (code.co_firstlineno, inlinetime)
+            print('%d %d' % (code.co_firstlineno, inlinetime), file=out_file)
-        print >> out_file
+        print(file=out_file)
-        print >> out_file, 'cfn=%s' % (label(code),)
+        print('cfn=%s' % (label(code),), file=out_file)
-            print >> out_file, 'calls=%d 0' % (subentry.callcount,)
+            print('cfi=~', file=out_file)
-                subentry.callcount, code.co_firstlineno)
+            print('cfi=%s' % (code.co_filename,), file=out_file)
-        print >> out_file, '%d %d' % (lineno, totaltime)
+        print('%d %d' % (lineno, totaltime), file=out_file)
-                request = slot.start_requests.next()
+                request = next(slot.start_requests)
-        node = my_iter.next()
+        node = next(my_iter)
-        iter.next()
+        next(iter)
-        self.assertRaises(StopIteration, iter.next)
+        self.assertRaises(StopIteration, next, iter)
-        node = namespace_iter.next()
+        node = next(namespace_iter)
-        node = namespace_iter.next()
+        node = next(namespace_iter)
-        iter.next()
+        next(iter)
-        self.assertRaises(StopIteration, iter.next)
+        self.assertRaises(StopIteration, next, iter)
-        wk = WeakKeyCache(lambda k: _values.next())
+        wk = WeakKeyCache(lambda k: next(_values))
-            yield it.next()
+            yield next(it)
-        return [str_to_unicode(field, encoding) for field in csv_r.next()]
+        return [str_to_unicode(field, encoding) for field in next(csv_r)]
-        return not self == other
+# Copyright (c) 2009 Raymond Hettinger
-            current += description.next()
+            current += next(description)
-        cookies = map(self._format_cookie, cookie_list)
+        cookies = [self._format_cookie(x) for x in cookie_list]
-        self.ignore_http_codes = map(int, settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES'))
+        self.ignore_http_codes = [int(x) for x in settings.getlist('HTTPCACHE_IGNORE_HTTP_CODES')]
-        self.portrange = map(int, crawler.settings.getlist('TELNETCONSOLE_PORT'))
+        self.portrange = [int(x) for x in crawler.settings.getlist('TELNETCONSOLE_PORT')]
-        self.assertEqual(frozenset(map(type, contracts)),
+        self.assertEqual(frozenset(type(x) for x in contracts),
-        self.assertEqual(map(type, output), [TestItem])
+        self.assertEqual([type(x) for x in output], [TestItem])
-        self.assertEqual(map(type, output), [Request])
+        self.assertEqual([type(x) for x in output], [Request])
-        self.assertEqual(map(type, output), [TestItem])
+        self.assertEqual([type(x) for x in output], [TestItem])
-            installed_version = map(int, module.__version__.split('.')[:2])
+            installed_version = [int(x) for x in module.__version__.split('.')[:2]]
-        map(lambda p: p.start(), patchers)
+        for p in patchers:
-        map(lambda p: p.stop(), patchers)
+        for p in patchers:
-        map(lambda p: p.start(), patchers)
+        for p in patchers:
-        map(lambda p: p.stop(), patchers)
+        for p in patchers:
-        self.portrange = map(int, crawler.settings.getlist('WEBSERVICE_PORT'))
+        self.portrange = [int(x) for x in crawler.settings.getlist('WEBSERVICE_PORT')]
-        return "Start crawling from a spider"
+        return "Run a spider"
-        return "Start crawling from a spider or URL"
+        return "Start crawling from a spider"
-    print "Available commands:"
+    print("Available commands:")
-        print "  ", func.__doc__
+        print("  ", func.__doc__)
-        print x
+        print(x)
-        print x
+        print(x)
-        print x
+        print(x)
-        print "%-40s %s" % (name, value)
+        print("%-40s %s" % (name, value))
-        print "%-40s %s" % (name, value)
+        print("%-40s %s" % (name, value))
-        print str(e)
+        print(cmd.__doc__)
-            print e.data
+            print("Server Traceback below:")
-    print "Scrapy %s requires Python 2.6 or above" % __version__
+    print("Scrapy %s requires Python 2.6 or above" % __version__)
-            settings['BOT_NAME'])
+        print("Scrapy %s - project: %s\n" % (scrapy.__version__, \
-        print "Scrapy %s - no active project\n" % scrapy.__version__
+        print("Scrapy %s - no active project\n" % scrapy.__version__)
-    print "Available commands:"
+    print("Usage:")
-        print "  %-13s %s" % (cmdname, cmdclass.short_desc())
+        print("  %-13s %s" % (cmdname, cmdclass.short_desc()))
-    print 'Use "scrapy <command> -h" to see more info about a command'
+        print()
-    print 'Use "scrapy" to see available commands'
+    print("Unknown command: %s\n" % cmdname)
-    except UsageError, e:
+    except UsageError as e:
-                print spider
+                print(spider)
-                    print '  * %s' % method
+                    print('  * %s' % method)
-                print "%-20s %s" % (name, target['url'])
+                print("%-20s %s" % (name, target['url']))
-            print os.linesep.join(projects)
+            print(os.linesep.join(projects))
-        print f.read()
+        print(f.read())
-    except urllib2.HTTPError, e:
+    except urllib2.HTTPError as e:
-    except urllib2.URLError, e:
+        print(e.read())
-                print '%s %s: %s' % (prefix, key, value)
+                print('%s %s: %s' % (prefix, key, value))
-            print '>'
+            print('>')
-            print response.body
+            print(response.body)
-                print open(template_file, 'r').read()
+                print(open(template_file, 'r').read())
-            print "Cannot create a spider with the same name as your project"
+            print("Cannot create a spider with the same name as your project")
-                print "  %s" % spider.__module__
+                print("Spider %r already exists in module:" % name)
-        print "  %s.%s" % (spiders_module.__name__, module)
+        print("Created spider %r using template %r in module:" % (name, \
-        print 'Use "scrapy genspider --list" to see all available templates.'
+        print("Unable to find template: %s\n" % template)
-        print "Available templates:"
+        print("Available templates:")
-                print "  %s" % splitext(filename)[0]
+                print("  %s" % splitext(filename)[0])
-            print s
+            print(s)
-        print "# Scraped Items ", "-"*60
+        print("# Scraped Items ", "-"*60)
-        print "# Requests ", "-"*65
+        print("# Requests ", "-"*65)
-                print '\n>>> DEPTH LEVEL: %s <<<' % level
+                print('\n>>> DEPTH LEVEL: %s <<<' % level)
-            print '\n>>> STATUS DEPTH LEVEL %s <<<' % self.max_level
+            print('\n>>> STATUS DEPTH LEVEL %s <<<' % self.max_level)
-        except (ImportError, ValueError), e:
+        except (ImportError, ValueError) as e:
-            print settings.get(opts.get)
+            print(settings.get(opts.get))
-            print settings.getbool(opts.getbool)
+            print(settings.getbool(opts.getbool))
-            print settings.getint(opts.getint)
+            print(settings.getint(opts.getint))
-            print settings.getfloat(opts.getfloat)
+            print(settings.getfloat(opts.getfloat))
-            print settings.getlist(opts.getlist)
+            print(settings.getlist(opts.getlist))
-                'letters, numbers and underscores'
+            print('Error: Project names must begin with a letter and contain only\n' \
-            print "Error: directory %r already exists" % project_name
+            print("Error: directory %r already exists" % project_name)
-            print "Platform: %s" % platform.platform()
+            print("Scrapy  : %s" % scrapy.__version__)
-            print "Scrapy %s" % scrapy.__version__
+            print("Scrapy %s" % scrapy.__version__)
-        except ValidationError, e:
+        except ValidationError as e:
-        except ValidationError, e:
+        except ValidationError as e:
-        except Exception, e:
+        except Exception as e:
-            except NotConfigured, ex:
+            except NotConfigured as ex:
-        except Exception, ex:
+        except Exception as ex:
-            except Exception, exc:
+            except Exception as exc:
-        except ValueError, e: # non serializable request
+        except ValueError as e: # non serializable request
-            except NotConfigured, e:
+            except NotConfigured as e:
-            print eval(self.code, globals(), self.vars)
+            print(eval(self.code, globals(), self.vars))
-        print "[s] %s" % line
+        print("[s] %s" % line)
-    except pickle.PicklingError, e:
+    except pickle.PicklingError as e:
-def open(file, flag='r', mode=0666):
+def open(file, flag='r', mode=0o666):
-            httpHost.host, httpHost.port, httpsHost.host, httpsHost.port)
+        print("Mock server running at http://%s:%d and https://%s:%d" % (
-        except Exception, e:
+        except Exception as e:
-        except Exception, e:
+        except Exception as e:
-        except ImportError, ex:
+        except ImportError as ex:
-            print 'Result', result
+            print('Request', request)
-            % port.getHost().port
+        print("Test server running at http://localhost:%d/ - hit Ctrl-C to finish." \
-except ImportError, e:
+except ImportError as e:
-        except ValueError, e:
+        except ValueError as e:
-        except TypeError, e:
+        except TypeError as e:
-        except JsonRpcError, e:
+        except JsonRpcError as e:
-            raise MultiValueDictKeyError, "Key %r not found in %r" % (key, self)
+            raise MultiValueDictKeyError("Key %r not found in %r" % (key, self))
-            raise TypeError, "update expected at most 1 arguments, got %d" % len(args)
+            raise TypeError("update expected at most 1 arguments, got %d" % len(args))
-                    raise ValueError, "MultiValueDict.update() takes either a MultiValueDict or dictionary"
+                    raise ValueError("MultiValueDict.update() takes either a MultiValueDict or dictionary")
-    except IgnoreRequest, e:
+    except IgnoreRequest as e:
-    print pformat(obj, *args, **kwargs)
+    print(pformat(obj, *args, **kwargs))
-        except Exception, e:
+        except Exception as e:
-            except Exception, e:
+            except Exception as e:
-    print format_engine_status(engine)
+    print(format_engine_status(engine))
-    except Exception, e:
+    except Exception as e:
-    except Exception, e:
+    except Exception as e:
-        raise ValueError, "Error loading object '%s': not a full path" % path
+        raise ValueError("Error loading object '%s': not a full path" % path)
-        raise ImportError, "Error loading object '%s': %s" % (path, e)
+    except ImportError as e:
-        raise NameError, "Module '%s' doesn't define any object named '%s'" % (module, name)
+        raise NameError("Module '%s' doesn't define any object named '%s'" % (module, name))
-        except IOError, e:
+        except IOError as e:
-    except ImportError, e:
+    except ImportError as e:
-    print "http://localhost:%d/" % port.getHost().port
+    print("http://localhost:%d/" % port.getHost().port)
-    print format_live_refs(*a, **kw)
+    print(format_live_refs(*a, **kw))
-	except KeyError, err:
+	except KeyError as err:
-			except KeyError,err:
+			except KeyError as err:
-						except Exception, err:
+						except Exception as err:
-		except Exception, err:
+		except Exception as err:
-				except Exception, e:
+				except Exception as e:
-						print '''Exception during saferef %s cleanup function %s: %s'''%(
+					except AttributeError as err:
-						)
+						))
-        except ValueError, e:
+        except ValueError as e:
-
+
-        self.settings = None # set in scrapy.cmdline
+        self.settings = None  # set in scrapy.cmdline
-        group.add_option("--logfile", metavar="FILE", \
+        group.add_option("--logfile", metavar="FILE",
-            default=None, \
+        group.add_option("-L", "--loglevel", metavar="LEVEL", default=None,
-        group.add_option("--nolog", action="store_true", \
+        group.add_option("--nolog", action="store_true",
-        group.add_option("--profile", metavar="FILE", default=None, \
+        group.add_option("--profile", metavar="FILE", default=None,
-        group.add_option("--lsprof", metavar="FILE", default=None, \
+        group.add_option("--lsprof", metavar="FILE", default=None,
-        group.add_option("--pidfile", metavar="FILE", \
+        group.add_option("--pidfile", metavar="FILE",
-        group.add_option("-s", "--set", action="append", default=[], metavar="NAME=VALUE", \
+        group.add_option("-s", "--set", action="append", default=[], metavar="NAME=VALUE",
-from w3lib.url import add_or_replace_parameter, safe_url_string
+# scrapy.utils.url was moved to w3lib.url and import * ensures this move doesn't break old code
-from itertools import chain
+        super(CloseSpider, self).__init__()
-            raise MultiValueDictKeyError, "Key %r not found in %r" % (key, self)
+            raise MultiValueDictKeyError("Key %r not found in %r" % (key, self))
-            raise TypeError, "update expected at most 1 arguments, got %d" % len(args)
+            raise TypeError("update expected at most 1 arguments, got %d" % len(args))
-                    raise ValueError, "MultiValueDict.update() takes either a MultiValueDict or dictionary"
+                    raise ValueError("MultiValueDict.update() takes either a MultiValueDict or dictionary")
-from w3lib.url import *
+from w3lib.url import add_or_replace_parameter, safe_url_string
-    return os.path.splitext(f)[1] not in ['.py', '.pyc', '.pyo']
+    return os.path.splitext(filename)[1] not in ['.py', '.pyc', '.pyo']
-    setup_args['install_requires'] = ['Twisted>=10.0.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL', 'cssselect>0.8']
+    setup_args['install_requires'] = [
-            selector = Selector(response, contenttype='xml')
+            selector = Selector(response, type='xml')
-            selector = Selector(response, contenttype='html')
+            selector = Selector(response, type='html')
-        xs = Selector(text=nodetext, contenttype='xml')
+        xs = Selector(text=nodetext, type='xml')
-    default_contenttype = 'html'
+    _default_type = 'html'
-    default_contenttype = 'xml'
+    _default_type = 'xml'
-    default_contenttype = 'html'
+    _default_type = 'html'
-    if ct is None:
+def _st(response, st):
-        return ct
+    elif st in ('xml', 'html'):
-        raise ValueError('Invalid contenttype: %s' % ct)
+        raise ValueError('Invalid type: %s' % st)
-    rt = XmlResponse if ct == 'xml' else HtmlResponse
+def _response_from_text(text, st):
-    __slots__ = ['response', 'text', 'namespaces', 'contenttype', '_expr', '_root',
+    __slots__ = ['response', 'text', 'namespaces', 'type', '_expr', '_root',
-    default_contenttype = None
+    _default_type = None
-    def __init__(self, response=None, text=None, namespaces=None, contenttype=None,
+    def __init__(self, response=None, text=None, type=None, namespaces=None,
-        self._tostring_method = _ctgroup[ct]['_tostring_method']
+        self.type = st = _st(response, type or self._default_type)
-            response = _response_from_text(text, ct)
+            response = _response_from_text(text, st)
-                                 contenttype=self.contenttype)
+                                 type=self.type)
-        hs = self.sscls(text=text, contenttype='html')
+        hs = self.sscls(text=text, type='html')
-        xs = self.sscls(text=text, contenttype='xml')
+        xs = self.sscls(text=text, type='xml')
-        self.assertEqual(sel.contenttype, 'xml')
+        self.assertEqual(sel.type, 'xml')
-        self.assertEqual(sel.contenttype, 'html')
+        self.assertEqual(sel.type, 'html')
-        xs = self.sscls(text='<root>lala</root>', contenttype='xml')
+        xs = self.sscls(text='<root>lala</root>', type='xml')
-        yield Selector(text=nodetext, contenttype='xml').xpath('//' + nodename)[0]
+        yield Selector(text=nodetext, type='xml').xpath('//' + nodename)[0]
-            ss = Selector(response)
+            sel = Selector(response)
-                            for f in ss.xpath(x).extract()
+                            for f in sel.xpath(x).extract()
-from scrapy.http import Request, Response, HtmlResponse, XmlResponse
+from scrapy.http import Request, Response
-        self.vars['ss'] = Selector(response)
+        self.vars['sel'] = Selector(response)
-        xpath = 'ss.xpath("//p[@class=\'one\']/text()").extract()[0]'
+        xpath = 'sel.xpath("//p[@class=\'one\']/text()").extract()[0]'
-        ss = self.sscls(response)
+        sel = self.sscls(response)
-        xl = ss.xpath('//input')
+        xl = sel.xpath('//input')
-                         [x.extract() for x in ss.xpath('//input')])
+        self.assertEqual(sel.xpath('//input').extract(),
-        self.assertEqual([x.extract() for x in ss.xpath("//input[@name='a']/@name")],
+        self.assertEqual([x.extract() for x in sel.xpath("//input[@name='a']/@name")],
-        self.assertEqual([x.extract() for x in ss.xpath("number(concat(//input[@name='a']/@value, //input[@name='b']/@value))")],
+        self.assertEqual([x.extract() for x in sel.xpath("number(concat(//input[@name='a']/@value, //input[@name='b']/@value))")],
-        self.assertEqual(ss.xpath("concat('xpath', 'rules')").extract(),
+        self.assertEqual(sel.xpath("concat('xpath', 'rules')").extract(),
-        self.assertEqual([x.extract() for x in ss.xpath("concat(//input[@name='a']/@value, //input[@name='b']/@value)")],
+        self.assertEqual([x.extract() for x in sel.xpath("concat(//input[@name='a']/@value, //input[@name='b']/@value)")],
-        self.assertEqual(ss.xpath(u'//input[@name="\xa9"]/@value').extract(), [u'1'])
+        sel = self.sscls(response)
-        self.assertEqual(ss.xpath("//div").extract(),
+        sel = self.sscls(XmlResponse('http://example.com', body=text))
-        self.assertEqual(ss.xpath("//div").extract(),
+        sel = self.sscls(HtmlResponse('http://example.com', body=text))
-        self.assertEqual(ss.css('#1').xpath('./span/text()').extract(), [u'me'])
+        sel = self.sscls(text=body)
-        self.assertEqual(hxs.xpath("//text()").extract(), [u'fff: ', u'zzz'])
+        sel = self.sscls(text='<div>fff: <a href="#">zzz</a></div>')
-        self.assertEqual(len(xxs.xpath("//link")), 2)
+        sel = self.sscls(XmlResponse("http://example.com/feed.atom", body=xml))
-        self.assertEqual(len(xxs.xpath("//link/@type")), 2)
+        sel = self.sscls(XmlResponse("http://example.com/feed.atom", body=xml))
-        self.ss = self.sscls(self.htmlresponse)
+        self.sel = self.sscls(self.htmlresponse)
-        return [v.strip() for v in self.ss.css(*a, **kw).extract() if v.strip()]
+        return [v.strip() for v in self.sel.css(*a, **kw).extract() if v.strip()]
-                         [x.extract() for x in self.ss.css('input')])
+        for x in self.sel.css('input'):
-        self.assertEqual(self.ss.css('p').css('b::text').extract(),
+        self.assertEqual(self.sel.css('p').css('b::text').extract(),
-        self.assertEqual(self.ss.css('div').css('area:last-child').extract(),
+        self.assertEqual(self.sel.css('div').css('area:last-child').extract(),
-class Libxml2DocumentTest(unittest.TestCase):
+class LxmlDocumentTest(unittest.TestCase):
-from scrapy.selector import HtmlXPathSelector
+from scrapy.selector import Selector
-            hxs = HtmlXPathSelector(response)
+            ss = Selector(response)
-                            for f in hxs.select(x).extract()
+                            for f in ss.xpath(x).extract()
-from scrapy.selector import HtmlXPathSelector
+from scrapy.selector import Selector
-    default_selector_class = HtmlXPathSelector
+    default_selector_class = Selector
-
+        return flatten([self.selector.xpath(xpath).extract() for xpath in xpaths])
-from scrapy.selector import XmlXPathSelector, HtmlXPathSelector
+from scrapy.selector import Selector
-        (itertag). Receives the response and an XPathSelector for each node.
+        (itertag). Receives the response and an Selector for each node.
-            selector = XmlXPathSelector(response)
+            selector = Selector(response, contenttype='xml')
-            nodes = selector.select('//%s' % self.itertag)
+            nodes = selector.xpath('//%s' % self.itertag)
-            selector = HtmlXPathSelector(response)
+            selector = Selector(response, contenttype='html')
-            nodes = selector.select('//%s' % self.itertag)
+            nodes = selector.xpath('//%s' % self.itertag)
-from scrapy.selector import XmlXPathSelector
+from scrapy.selector import Selector
-        xs = XmlXPathSelector(text=nodetext)
+        xs = Selector(text=nodetext, contenttype='xml')
-        yield xs.select(selxpath)[0]
+        yield xs.xpath(selxpath)[0]
-from scrapy.selector import HtmlXPathSelector
+from scrapy.selector import Selector
-        sel = HtmlXPathSelector(text=u"<html><body><div>marta</div></body></html>")
+        sel = Selector(text=u"<html><body><div>marta</div></body></html>")
-                    'custom': selector.select('other/@b:custom').extract(),
+                    'loc': selector.xpath('a:loc/text()').extract(),
-            attrs.append((x.select("@id").extract(), x.select("name/text()").extract(), x.select("./type/text()").extract()))
+            attrs.append((x.xpath("@id").extract(), x.xpath("name/text()").extract(), x.xpath("./type/text()").extract()))
-        self.assertEqual([x.select("text()").extract() for x in self.xmliter(body, 'product')],
+        self.assertEqual([x.xpath("text()").extract() for x in self.xmliter(body, 'product')],
-        self.assertEqual(node.select('price/text()').extract(), [])
+        self.assertEqual(node.xpath('title/text()').extract(), ['Item 1'])
-        self.assertEqual(node.select('text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])
+        self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item1.jpg'])
-        self.assertEqual(node.select('text()').extract(), ['http://www.mydummycompany.com/images/item2.jpg'])
+        self.assertEqual(node.xpath('text()').extract(), ['http://www.mydummycompany.com/images/item2.jpg'])
-from scrapy.selector import XmlXPathSelector
+from scrapy.selector import Selector
-    """Return a iterator of XPathSelector's over all nodes of a XML document,
+    """Return a iterator of Selector's over all nodes of a XML document,
-        yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]
+        yield Selector(text=nodetext, contenttype='xml').xpath('//' + nodename)[0]
-        warnings.warn('XPathSelectorList is deprecated, use '
+        warnings.warn('XPathSelectorList is deprecated, instanciate '
-
+import warnings
-
+from scrapy.exceptions import ScrapyDeprecationWarning
-    XPathSelector
+from scrapy.selector import Selector
-class XPathSelectorTestCase(unittest.TestCase):
+class SelectorTestCase(unittest.TestCase):
-    xxs_cls = XmlXPathSelector
+    sscls = Selector
-    def test_selector_simple(self):
+    def test_simple_selection(self):
-        xpath = self.hxs_cls(response)
+        ss = self.sscls(response)
-        xl = xpath.select('//input')
+        xl = ss.xpath('//input')
-            assert isinstance(x, self.hxs_cls)
+            assert isinstance(x, self.sscls)
-                         [x.extract() for x in xpath.select('//input')])
+        self.assertEqual(ss.xpath('//input').extract(),
-        self.assertEqual([x.extract() for x in xpath.select("//input[@name='a']/@name")],
+        self.assertEqual([x.extract() for x in ss.xpath("//input[@name='a']/@name")],
-        self.assertEqual([x.extract() for x in xpath.select("number(concat(//input[@name='a']/@value, //input[@name='b']/@value))")],
+        self.assertEqual([x.extract() for x in ss.xpath("number(concat(//input[@name='a']/@value, //input[@name='b']/@value))")],
-        self.assertEqual(xpath.select("concat('xpath', 'rules')").extract(),
+        self.assertEqual(ss.xpath("concat('xpath', 'rules')").extract(),
-        self.assertEqual([x.extract() for x in xpath.select("concat(//input[@name='a']/@value, //input[@name='b']/@value)")],
+        self.assertEqual([x.extract() for x in ss.xpath("concat(//input[@name='a']/@value, //input[@name='b']/@value)")],
-    def test_selector_unicode_query(self):
+    def test_select_unicode_query(self):
-        self.assertEqual(xpath.select(u'//input[@name="\xa9"]/@value').extract(), [u'1'])
+        ss = self.sscls(response)
-        """Test XPathSelector returning the same type in x() method"""
+    def test_list_elements_type(self):
-                          self.hxs_cls)
+        assert isinstance(self.sscls(text=text).xpath("//p")[0], self.sscls)
-    def test_selector_boolean_result(self):
+    def test_boolean_result(self):
-        """Test that XML and HTML XPathSelector's behave differently"""
+        xs = self.sscls(response)
-        self.assertEqual(self.xxs_cls(text=text).select("//div").extract(),
+        xs = self.sscls(text=text, contenttype='xml')
-        self.assertEqual(self.hxs_cls(text=text).select("//div").extract(),
+        ss = self.sscls(HtmlResponse('http://example.com', body=text))
-    def test_selector_nested(self):
+    def test_nested_selectors(self):
-        self.assertEqual(map(unicode.strip, divtwo.select("//li").extract()),
+        x = self.sscls(response)
-        self.assertEqual(map(unicode.strip, divtwo.select("./ul/li").extract()),
+        self.assertEqual(divtwo.xpath("./ul/li").extract(),
-        self.assertEqual(map(unicode.strip, divtwo.select(".//li").extract()),
+        self.assertEqual(divtwo.xpath(".//li").extract(),
-                         [])
+        self.assertEqual(divtwo.xpath("./li").extract(), [])
-            [u'fff: ', u'zzz'])
+        hxs = self.sscls(text='<div>fff: <a href="#">zzz</a></div>')
-    def test_selector_namespaces_simple(self):
+    def test_namespaces_simple(self):
-        x = self.xxs_cls(response)
+        x = self.sscls(response)
-        self.assertEqual(x.select("//somens:a/text()").extract(),
+        self.assertEqual(x.xpath("//somens:a/text()").extract(),
-    def test_selector_namespaces_multiple(self):
+    def test_namespaces_multiple(self):
-
+        x = self.sscls(response)
-    def test_selector_re(self):
+        self.assertEqual(len(x.xpath("//xmlns:TestTag")), 1)
-               """
+                  </div>"""
-        x = self.hxs_cls(response)
+        x = self.sscls(response)
-        self.assertEqual(x.select("//ul/li").re(name_re),
+        self.assertEqual(x.xpath("//ul/li").re(name_re),
-        self.assertEqual(x.select("//ul/li").re("Age: (\d+)"),
+        self.assertEqual(x.xpath("//ul/li").re("Age: (\d+)"),
-    def test_selector_re_intl(self):
+    def test_re_intl(self):
-        self.assertEqual(x.select("//div").re("Evento: (\w+)"), [u'cumplea\xf1os'])
+        x = self.sscls(response)
-                         u'<root>lala</root>')
+        hs = self.sscls(text='<root>lala</root>')
-    def test_selector_invalid_xpath(self):
+    def test_invalid_xpath(self):
-        x = self.hxs_cls(response)
+        x = self.sscls(response)
-            x.select(xpath)
+            x.xpath(xpath)
-        self.assertEquals(x.select("//span[@id='blank']/text()").extract(),
+        x = self.sscls(response)
-        self.xxs_cls(r1).select('//text()').extract()
+        self.sscls(r1).xpath('//text()').extract()
-        self.xxs_cls(r1).select('//text()').extract()
+        self.sscls(r1).xpath('//text()').extract()
-        self.xxs_cls(r1).select('//text()').extract()
+        self.sscls(r1).xpath('//text()').extract()
-        r = self.hxs_cls(text=u'<span class="big">some text</span>')
+        r = self.sscls(text=u'<span class="big">some text</span>')
-        x1 = r.select('//text()')
+        x1 = r.xpath('//text()')
-        self.assertEquals(x1.select('.//b').extract(), [])
+        self.assertEquals(x1.xpath('.//b').extract(), [])
-        x1 = r.select('//span/@class')
+        x1 = r.xpath('//span/@class')
-        self.assertEquals(x1.select('.//text()').extract(), [])
+        self.assertEquals(x1.xpath('.//text()').extract(), [])
-        x1 = r.select("//div/descendant::text()[preceding-sibling::b[contains(text(), 'Options')]]")
+        r = self.sscls(text=u'<div><b>Options:</b>opt1</div><div><b>Other</b>opt2</div>')
-        x1 = r.select("//div/descendant::text()/preceding-sibling::b[contains(text(), 'Options')]")
+        x1 = r.xpath("//div/descendant::text()/preceding-sibling::b[contains(text(), 'Options')]")
-
+        r = self.sscls(text=u'<div><b>Options:</b>opt1</div><div><b>Other</b>opt2</div>')
-    test_nested_select_on_text_nodes.skip = True
+    test_nested_select_on_text_nodes.skip = "Text nodes lost parent node reference in lxml"
-                x.__class__.__name__
+        x = self.sscls()
-class HTMLCSSSelectorTest(unittest.TestCase):
+class CSSSelectorTest(unittest.TestCase):
-    hcs_cls = Selector
+    sscls = Selector
-        self.hcs = self.hcs_cls(self.htmlresponse)
+        self.ss = self.sscls(self.htmlresponse)
-        return [v.strip() for v in self.hcs.css(*a, **kw).extract() if v.strip()]
+        return [v.strip() for v in self.ss.css(*a, **kw).extract() if v.strip()]
-                         [x.extract() for x in self.hcs.css('input')])
+        for x in self.ss.css('input'):
-        self.assertEqual(self.hcs.css('p').css('b::text').extract(),
+        self.assertEqual(self.ss.css('p').css('b::text').extract(),
-        self.assertEqual(self.hcs.css('div').css('area:last-child').extract(),
+        self.assertEqual(self.ss.css('div').css('area:last-child').extract(),
-        LxmlDocument(response)
+import unittest
-    default_contenttype = 'xml'
+    default_contenttype = 'html'
-from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector, XmlCSSSelector, HtmlCSSSelector
+from scrapy.selector import Selector
-        XPathSelector, Settings)
+                        Selector, Settings)
-            if isinstance(response, HtmlResponse) else None
+        self.vars['ss'] = Selector(response)
-        xpath = 'hxs.select("//p[@class=\'one\']/text()").extract()[0]'
+        xpath = 'ss.xpath("//p[@class=\'one\']/text()").extract()[0]'
-        self.contenttype = ct = self.default_contenttype or _ct(contenttype)
+        self.contenttype = ct = _ct(response, contenttype or self.default_contenttype)
-        return self.__class__(flatten([x.select(xpath) for x in self]))
+    def xpath(self, xpath):
-    @deprecated(use_instead='SelectorList.extract')
+    @deprecated(use_instead='.extract()')
-    @deprecated(use_instead='SelectorList.select')
+    @deprecated(use_instead='.xpath()')
-from scrapy.selector import CSSSelector, XmlCSSSelector, HtmlCSSSelector
+from scrapy.http import HtmlResponse
-    hcs_cls = HtmlCSSSelector
+    hcs_cls = Selector
-        return [v.strip() for v in self.hcs.select(*a, **kw).extract() if v.strip()]
+        return [v.strip() for v in self.hcs.css(*a, **kw).extract() if v.strip()]
-        for x in self.hcs.select('input'):
+        for x in self.hcs.css('input'):
-                         [x.extract() for x in self.hcs.select('input')])
+        self.assertEqual(self.hcs.css('input').extract(),
-        self.assertEqual(self.hcs.select('p').select('b::text').extract(),
+        self.assertEqual(self.hcs.css('p').css('b::text').extract(),
-        self.assertEqual(self.hcs.select('div').select('area:last-child').extract(),
+        self.assertEqual(self.hcs.css('div').css('area:last-child').extract(),
-from scrapy.selector import XPathSelector, HtmlXPathSelector, XmlXPathSelector
+from .unified import Selector
-class ScrapyXPathExpr(XPathExpr):
+class CSSSelector(Selector):
-class CSSSelectorMixin(object):
+    default_contenttype = 'html'
-    translator = ScrapyHTMLTranslator()
+        return self.css(css)
-    translator = ScrapyHTMLTranslator()
+HtmlCSSSelector = CSSSelector
-    translator = ScrapyGenericTranslator()
+class XmlCSSSelector(CSSSelector):
-        return self.select(xpath)
+from .unified import Selector, SelectorList
-from .list import SelectorList
+__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector',
-        return self.extract()
+class XPathSelector(Selector):
-    _tostring_method = 'xml'
+    default_contenttype = 'xml'
-    _tostring_method = 'html'
+    default_contenttype = 'html'
-from scrapy.selector.csssel import ScrapyHTMLTranslator
+from scrapy.selector.csstranslator import ScrapyHTMLTranslator
-try:
+
-                libxml2_version = ".".join(map(str, lxml.etree.LIBXML_VERSION))
+            import lxml.etree
-
+Selectors
-
+from scrapy.selector.lxmlsel import *
-from scrapy.utils.test import libxml2debug
+
-        assert isinstance(self.hxs_cls(text=text).select("//p")[0], 
+        assert isinstance(self.hxs_cls(text=text).select("//p")[0],
-    unittest.main()
+
-        return testfunction
+    @deprecated(use_instead='SelectorList.extract')
-        return self.xpath_attribute_function(xpath, arguments)
+    def xpath_attr_functional_pseudo_element(self, xpath, function):
-        return self.xpath_text_pseudo(xpath)
+        """Support selecting text nodes using ::text pseudo-element"""
-    def test_attribute_function(self):
+    def test_attr_function(self):
-            ('a > :attribute(name)', u'descendant-or-self::a/*/@name'),
+            ('::attr(name)', u'descendant-or-self::*/@name'),
-    def test_attribute_function2(self):
+    def test_attr_function_exception(self):
-            ('a > ::attribute(name)', u'descendant-or-self::a/*/@name'),
+            ('::attr(12)', ExpressionError),
-            self.assertEqual(self.c2x(css), xpath, css)
+        for css, exc in cases:
-            ('p:text, a::text', u"descendant-or-self::p/text() | descendant-or-self::a/text()"),
+            ('p::text, a::text', u"descendant-or-self::p/text() | descendant-or-self::a/text()"),
-        self.assertEqual(self.x('p :text'), [u'lorem ipsum text', u'hi', u'there', u'guy'])
+        self.assertEqual(self.x('#p-b2::text'), [u'guy'])
-        self.assertEqual(self.x('map[name="dummymap"] :attribute(shape)'), [u'circle', u'default'])
+        self.assertEqual(self.x('#p-b2::attr(id)'), [u'p-b2'])
-        self.assertEqual(self.hcs.select('p').select('b:text').extract(),
+        self.assertEqual(self.hcs.select('p').select('b::text').extract(),
-from cssselect.xpath import XPathExpr, ExpressionError
+from cssselect.xpath import _unicode_safe_getattr, XPathExpr, ExpressionError
-from .list import SelectorList
+from cssselect.xpath import XPathExpr, ExpressionError
-        return self.__class__(flatten([x.xpath(xpath) for x in self]))
+class ScrapyXPathExpr(XPathExpr):
-        return self.__class__(flatten([x.get(attr) for x in self]))
+    textnode = False
-        return self.__class__(flatten([x.text(all) for x in self]))
+    @classmethod
-        return CSSSelectorList(super(CSSSelectorMixin, self).select(self.translator.css_to_xpath(css)))
+        xpath = self._css2xpath(css)
-        return CSSSelectorList(super(CSSSelectorMixin, self).select(xpath))
+    def _css2xpath(self, css):
-        return self.xpath('@' + attr)
+class CSSSelector(CSSSelectorMixin, XPathSelector):
-    translator = GenericTranslator()
+class HtmlCSSSelector(CSSSelectorMixin, HtmlXPathSelector):
-    translator = HTMLTranslator()
+class XmlCSSSelector(CSSSelectorMixin, XmlXPathSelector):
-backend = os.environ.get('SCRAPY_SELECTORS_BACKEND')
+backend = os.environ.get('SCRAPY_SELECTORS_BACKEND')
-from scrapy.selector import HtmlXPathSelector, XmlXPathSelector, XPathSelectorList
+from scrapy.selector import HtmlXPathSelector, XmlXPathSelector
-class CSSSelectorList(XPathSelectorList):
+
-from .list import XPathSelectorList
+from .list import SelectorList
-                return XPathSelectorList([self.__class__(node=node, parent=self, \
+                return SelectorList([self.__class__(node=node, parent=self, \
-                return XPathSelectorList([self.__class__(node=xpath_result, \
+                return SelectorList([self.__class__(node=xpath_result, \
-            return XPathSelectorList([])
+            return SelectorList([])
-            elif isinstance(self.xmlNode, libxml2.xmlAttr): 
+            elif isinstance(self.xmlNode, libxml2.xmlAttr):
-class XPathSelectorList(list):
+
-    @deprecated(use_instead='XPathSelectorList.select')
+    @deprecated(use_instead='SelectorList.select')
-from .list import XPathSelectorList
+from .list import SelectorList
-           'XPathSelectorList']
+__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector']
-                body=unicode_to_str(text, 'utf-8'), encoding='utf-8')
+            response = TextResponse(url='about:blank', encoding='utf-8',
-            return XPathSelectorList([])
+            return SelectorList([])
-        return XPathSelectorList(result)
+        return SelectorList(result)
-
+
-from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector
+from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector, XmlCSSSelector, HtmlCSSSelector
-    setup_args['install_requires'] = ['Twisted>=10.0.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL']
+    setup_args['install_requires'] = ['Twisted>=10.0.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL', 'cssselect>0.8']
-def start_python_console(namespace=None, noipython=False):
+def start_python_console(namespace=None, noipython=False, banner=''):
-            import IPython
+                raise ImportError()
-                shell()
+                from IPython.frontend.terminal.embed import InteractiveShellEmbed
-            code.interact(banner='', local=namespace)
+            code.interact(banner=banner, local=namespace)
-from twisted.internet import reactor, threads
+from twisted.internet import reactor, threads, defer
-        d = request_deferred(request)
+        d = _request_deferred(request)
-                BaseSpider('default'), log_multiple=True)
+            spider = create_spider_for_request(self.crawler.spiders,
-    
+
-    
+
-    
+
-    
+    the fingerprint.
-        settings = crawler.settings
+        settings = self.crawler_process.settings
-
+        self._start_requests = lambda: ()
-        self._scheduled = {}
+        # TODO: remove together with scrapy.project.crawler usage
-            self._scheduled[spider] = None
+        if requests is None:
-        return self.engine.open_spider(spider, requests)
+            self._start_requests = lambda: requests
-
+        if self._spider:
-        if self.engine.running:
+        if self.configured and self.engine.running:
-    """
+class CrawlerProcess(object):
-    def __init__(self, *a, **kw):
+    def __init__(self, settings):
-
+    @defer.inlineCallbacks
-            pass
+        self.stopping = True
-            self.crawlers[name] = Crawler(self.settings)
+        reactor.callFromThread(self._stop_reactor)
-        return self.crawlers[name]
+    def start_reactor(self):
-    def start_crawler(self):
+    def _start_crawler(self):
-            crawler.signals.connect(self.check_done, signals.engine_stopped)
+            crawler.signals.connect(self._check_done, signals.engine_stopped)
-        self.stopping = True
+    def _check_done(self, **kwargs):
-                yield crawler.stop()
+    def _stop_reactor(self, _=None):
-            'default + started')
+                         'default')
-            'override + started')
+                         'override')
-            'override + started')
+                         'override')
-from scrapy.tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider
+from scrapy.tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \
-    crawler.configure()
+    def test_start_requests_bug_before_yield(self):
-        self.slots = {}
+        self.slot = None
-        except KeyError:
+        slot = self.slot
-        slot = self.slots[spider]
+        slot = self.slot
-        slot = self.slots[spider]
+        slot = self.slot
-        pending = self.slots[spider].scheduler.has_pending_requests()
+        pending = self.slot.scheduler.has_pending_requests()
-        return self.slots.keys()
+        return [self.spider] if self.spider else []
-        return len(self.slots) < self._concurrent_spiders
+        return not bool(self.slot)
-        self.slots[spider].nextcall.schedule()
+        self.slot.nextcall.schedule()
-        return self.slots[spider].scheduler.enqueue_request(request)
+        return self.slot.scheduler.enqueue_request(request)
-        slot = self.slots[spider]
+        slot = self.slot
-        slot = self.slots[spider]
+        slot = self.slot
-        assert self.has_capacity(), "No free spider slots when opening %r" % \
+        assert self.has_capacity(), "No free spider slot when opening %r" % \
-        self.slots[spider] = slot
+        self.slot = slot
-            self.slots[spider].nextcall.schedule(5)
+            self.slot.nextcall.schedule(5)
-        slot = self.slots[spider]
+        slot = self.slot
-        dfd.addBoth(lambda _: self.slots.pop(spider))
+        dfd.addBoth(lambda _: setattr(self, 'slot', None))
-        "len(engine.slots[spider].scheduler.mqs)",
+        "engine.slot.closing",
-        self._settings = settings
+    def __init__(self, headers):
-        return self._settings.get('DEFAULT_REQUEST_HEADERS').items()
+        return cls(crawler.settings.get('DEFAULT_REQUEST_HEADERS').items())
-        for k, v in self._headers[spider]:
+        for k, v in self._headers:
-from scrapy.utils.python import WeakKeyCache
+
-        return cls(crawler.settings['DOWNLOAD_TIMEOUT'])
+        o = cls(crawler.settings['DOWNLOAD_TIMEOUT'])
-        return self._timeout
+    def spider_opened(self, spider):
-            request.meta.setdefault('download_timeout', timeout)
+        if self._timeout:
-from scrapy.utils.python import WeakKeyCache
+from scrapy import signals
-        return cls(crawler.settings['USER_AGENT'])
+        o = cls(crawler.settings['USER_AGENT'])
-        return self.user_agent
+    def spider_opened(self, spider):
-            request.headers.setdefault('User-Agent', ua)
+        if self.user_agent:
-from scrapy.utils.python import WeakKeyCache
+
-        self._cache = WeakKeyCache(self._authorization)
+    @classmethod
-    def _authorization(self, spider):
+    def spider_opened(self, spider):
-            return basic_auth_header(usr, pwd)
+            self.auth = basic_auth_header(usr, pwd)
-        auth = self._cache[spider]
+        auth = getattr(self, 'auth', None)
-        crawler.signals.connect(self.spider_closed, signals.spider_closed)
+        self._spider_netlocs = set()
-        useragent = self._useragents[spider]
+        useragent = self._useragent
-            self._spider_netlocs[spider].add(netloc)
+            self._spider_netlocs.add(netloc)
-        del self._useragents[spider]
+            # loop on element attributes also
-        self.scheduled = {}
+        self._scheduled = {}
-            return self.schedule(spider, requests)
+            assert not self._scheduled
-            requests.extend(batch)
+            self._scheduled.setdefault(spider, []).append(requests)
-            yield self.schedule(spider, batches)
+        for spider, batches in self._scheduled.iteritems():
-import sys, time, random, urllib
+import sys, time, random, urllib, os
-from twisted.internet import reactor, defer
+from twisted.internet import reactor, defer, ssl
-    port = reactor.listenTCP(8998, factory)
+    httpPort = reactor.listenTCP(8998, factory)
-        print "Mock server running at http://%s:%d" % (h.host, h.port)
+        httpHost = httpPort.getHost()
-        return [Request(x) for x in item.get('file_urls', [])]
+        return [Request(x) for x in item.get(self.FILES_URLS_FIELD, [])]
-            item['files'] = [x for ok, x in results if ok]
+        if self.FILES_RESULT_FIELD in item.fields:
-        return [Request(x) for x in item.get('image_urls', [])]
+        return [Request(x) for x in item.get(self.IMAGES_URLS_FIELD, [])]
-            item['images'] = [x for ok, x in results if ok]
+        if self.IMAGES_RESULT_FIELD in item.fields:
-        self.pipeline = FilesPipeline(self.tempdir, download_func=_mocked_download_func)
+        self.pipeline = FilesPipeline.from_settings(Settings({'FILES_STORE': self.tempdir}))
-
+class FilesPipelineTestCaseFields(unittest.TestCase):
-    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL']
+    setup_args['install_requires'] = ['Twisted>=10.0.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL']
-from twisted.web import server, error
+from twisted.web import server, resource
-            return error.NoResource("No such child resource.")
+            return resource.ErrorPage(404, "No Such Resource", "No such child resource.")
-        if isinstance(item_pipelines, (tuple, list)):
+        if isinstance(item_pipelines, (tuple, list, set, frozenset)):
-            warnings.warn('ITEM_PIPELINES defined as a list is deprecated, switch to a dict',
+            warnings.warn('ITEM_PIPELINES defined as a list or a set is deprecated, switch to a dict',
-            settings['ITEM_PIPELINES'])
+        return build_component_list(settings['ITEM_PIPELINES_BASE'], item_pipelines)
-        return settings.getlist('ITEM_PIPELINES')
+        item_pipelines = settings['ITEM_PIPELINES']
-ITEM_PIPELINES = []
+ITEM_PIPELINES = {}
-""".format(self.project_name))
+ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}
-                for loc in iterloc(s, self.use_alternate_links):
+                for loc in iterloc(s, self.sitemap_alternate_links):
-from scrapy.exceptions import CloseSpider, DropItem
+from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest
-        if isinstance(download_failure, Failure):
+        if isinstance(download_failure, Failure) \
-        return spider_failure
+        if isinstance(download_failure, Failure):
-                        '|descendant::input[@type!="submit" and @type!="image" '
+                        '|descendant::input[@type!="submit" and @type!="image" and @type!="reset"'
-                for loc in iterloc(s, self._alternate):
+                for loc in iterloc(s, self.use_alternate_links):
-        if alt == True and 'alternate' in d:
+        if alt and 'alternate' in d:
-        xmlp = lxml.etree.XMLParser(recover=True)
+        xmlp = lxml.etree.XMLParser(recover=True, remove_comments=True)
-                    d.setdefault('alternate', []).append(el.get('href'))
+                    if 'href' in el.attrib:
-            
+
-            if waited > 5:
+            if waited > 15:
-    'JsonItemExporter', 'MarshalItemExporter']
+__all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',
-                    nonempty_fields)
+                field_iter = (x for x in self.fields_to_export if x in
-            except TypeError: # list in value may not contain strings
+            except TypeError:  # list in value may not contain strings
-            include_empty=True)
+        fields = self._get_serialized_fields(item, default_value='',
-        self._configure(kwargs)
+        self._configure(kwargs, dont_fail=True)
-        self._configure(kwargs)
+        self._configure(kwargs, dont_fail=True)
-    
+
-  
+
-        self.start_reactor()
+        if self.start_crawling():
-        name, crawler = self.crawlers.popitem()
+        if self.crawlers and not self.stopping:
-            crawler.signals.connect(sflo.stop, signals.engine_stopped)
+            sflo = log.start_from_crawler(crawler)
-        crawler.start()
+            crawler.signals.connect(self.check_done, signals.engine_stopped)
-        return name, crawler
+            return name, crawler
-        else:
+        if not self.start_crawler():
-        self.start_crawler()
+        return self.start_crawler() is not None
-        return threads.deferToThread(k.set_contents_from_file, buf,
+        return threads.deferToThread(k.set_contents_from_string, buf.getvalue(),
-from twisted.internet.task import deferLater
+from twisted.internet import reactor, defer
-class DeferMixin(object):
+class LeafResource(Resource):
-    isLeaf = True
+class Follow(LeafResource):
-    isLeaf = True
+class Delay(LeafResource):
-    isLeaf = True
+class Status(LeafResource):
-    isLeaf = True
+class Raw(LeafResource):
-    isLeaf = True
+class Partial(LeafResource):
-            request.channel.transport.loseConnection()
+        tr = request.channel.transport
-        spider = SimpleSpider("http://localhost:8998/raw?{}".format(query))
+        spider = SimpleSpider("http://localhost:8998/raw?{0}".format(query))
-if sys.version_info < (2,6):
+if sys.version_info < (2, 6):
-                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format,valid_output_formats))
+                raise UsageError('Invalid/unrecognized output format: %s, Expected %s' % (opts.output_format, valid_output_formats))
-        self.file =file
+        self.file = file
-    HTTPConnectionPool, TCP4ClientEndpoint, ResponseFailed
+    HTTPConnectionPool, TCP4ClientEndpoint
-        dfd.addCallback(self.assertEqual, [1,2]) # it is [1] with maybeDeferred
+        dfd.addCallback(self.assertEqual, [1, 2]) # it is [1] with maybeDeferred
-        dfd.addCallback(self.assertEqual, [1,2]) # it is [1] with maybeDeferred
+        dfd.addCallback(self.assertEqual, [1, 2]) # it is [1] with maybeDeferred
-        assert hasattr(arg_to_iter([1,2,3]), '__iter__')
+        assert hasattr(arg_to_iter([1, 2, 3]), '__iter__')
-        self.assertEqual(list(arg_to_iter([1,2,3])), [1,2,3])
+        self.assertEqual(list(arg_to_iter([1, 2, 3])), [1, 2, 3])
-def deprecated_setter(setter, attrname):
+def obsolete_setter(setter, attrname):
-        return setter(self, value)
+        msg = "%s.%s is not modifiable, use %s.replace() instead" % (c, attrname, c)
-from scrapy.http.common import deprecated_setter
+from scrapy.http.common import obsolete_setter
-    url = property(_get_url, deprecated_setter(_set_url, 'url'))
+    url = property(_get_url, obsolete_setter(_set_url, 'url'))
-    body = property(_get_body, deprecated_setter(_set_body, 'body'))
+    body = property(_get_body, obsolete_setter(_set_body, 'body'))
-from scrapy.http.common import deprecated_setter
+from scrapy.http.common import obsolete_setter
-    url = property(_get_url, deprecated_setter(_set_url, 'url'))
+    url = property(_get_url, obsolete_setter(_set_url, 'url'))
-    body = property(_get_body, deprecated_setter(_set_body, 'body'))
+    body = property(_get_body, obsolete_setter(_set_body, 'body'))
-            checksum = self.process_downloaded_media(response, request, info)
+            checksum = self.file_downloaded(response, request, info)
-    def process_downloaded_media(self, response, request, info):
+    def file_downloaded(self, response, request, info):
-    def process_downloaded_media(self, response, request, info):
+    def file_key(self, url):
-    def file_key(self, url):
+    # backwards compatibility
-            mock.patch.object(FSFilesStore, 'stat_image', return_value={
+            mock.patch.object(FSFilesStore, 'stat_file', return_value={
-            mock.patch.object(FSFilesStore, 'stat_image', return_value={
+            mock.patch.object(FSFilesStore, 'stat_file', return_value={
-    ### Overradiable Interface
+    ### Overridable Interface
-    ### Overradiable Interface
+    ### Overridable Interface
-        with open(absolute_path, 'w') as f:
+        with open(absolute_path, 'wb') as f:
-    def stat_image(self, key, info):
+    def stat_file(self, key, info):
-            checksum = md5sum(imagefile)
+        with open(absolute_path, 'rb') as f:
-    def stat_image(self, key, info):
+    def stat_file(self, key, info):
-        dfd = defer.maybeDeferred(self.store.stat_image, key, info)
+        dfd = defer.maybeDeferred(self.store.stat_file, key, info)
-        dfd.addErrback(log.err, self.__class__.__name__ + '.store.stat_image')
+        dfd.addErrback(log.err, self.__class__.__name__ + '.store.stat_file')
-
+from scrapy import optional_features
-                    ResponseFailed):
+        exceptions = [ServerTimeoutError, DNSLookupError,
-from scrapy.xlib.tx._newclient import ResponseFailed
+from twisted.internet.error import TimeoutError as ServerTimeoutError, \
-from twisted.web.http import PotentialDataLoss
+from twisted.web.http import PotentialDataLoss
-        ResponseFailed, HTTPConnectionPool, TCP4ClientEndpoint
+    HTTPConnectionPool, TCP4ClientEndpoint, ResponseFailed
-                bindAddress=bindaddress)
+                                          bindAddress=bindaddress)
-            self._finished.callback((self._txresponse, body, ['partial']))
+        abort = getarg(request, "abort", 0, type=int)
-        request.finish()
+        if abort:
-        spider = SimpleSpider("http://localhost:8998/drop")
+    def test_retry_conn_lost(self):
-                                   ConnectionLost
+from twisted.internet.error import TimeoutError as ServerTimeoutError, \
-        for exc in (ServerTimeoutError, DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, ConnectionLost):
+        for exc in (ServerTimeoutError, DNSLookupError, ConnectionRefusedError,
-            self._test_retry_exception(req, exc())
+            self._test_retry_exception(req, exc('foo'))
-                           ConnectionLost, TCPTimedOutError,
+                           ConnectionLost, TCPTimedOutError, ResponseFailed,
-        request.channel.transport.loseConnection()
+        request.channel.transport.abortConnection()
-            self.xg.characters(serialized_value)
+            self._xg_characters(serialized_value)
-def start_from_crawler(crawler, print_headers=False):
+def start_from_crawler(crawler):
-        shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)
+        self.crawler_process.start_crawling()
-        t = Thread(target=self.crawler_process.start, kwargs={'headers': False})
+        t = Thread(target=self.crawler_process.start_reactor)
-    def _stop_reactor(self, _=None):
+    def stop_reactor(self, _=None):
-        reactor.callFromThread(self._stop_reactor)
+        reactor.callFromThread(self.stop_reactor)
-    def start_crawling(self):
+    def start_crawler(self):
-            self.start_crawling()
+            self.start_crawler()
-            self._stop_reactor()
+            self.stop_reactor()
-    def print_headers(self):
+    def start_crawling(self):
-        return super(CrawlerProcess, self).start()
+        self.start_crawler()
-            return self.engine.open_spider(spider, requests)
+            return self.schedule(spider, requests)
-            self.scheduled.setdefault(spider, []).extend(requests)
+            self.scheduled.setdefault(spider, []).append(requests)
-            yield self.engine.open_spider(spider, requests)
+        for spider, batches in self.scheduled.iteritems():
-            self.crawlers[name] = crawler
+            self.crawlers[name] = Crawler(self.settings)
-            self.store.persist_image(
+            self.store.persist_file(
-            crawler = self.crawler_process.create_crawler('default')
+            crawler = self.crawler_process.create_crawler()
-            self.crawler.start()
+            crawler = self.crawler_process.create_crawler()
-        self.crawler.start()
+
-        editor = self.crawler.settings['EDITOR']
+
-            spider = self.crawler.spiders.create(args[0])
+            spider = crawler.spiders.create(args[0])
-            spider = self.crawler.spiders.create(opts.spider)
+            spider = crawler.spiders.create(opts.spider)
-            spider = create_spider_for_request(self.crawler.spiders, request, \
+            spider = create_spider_for_request(crawler.spiders, request, \
-
+        crawler.crawl(spider, [request])
-            spider = self.crawler.spiders.create(name)
+            crawler = self.crawler_process.create_crawler()
-        for s in self.crawler.spiders.list():
+        crawler = self.crawler_process.create_crawler()
-                self.spider = self.crawler.spiders.create(opts.spider, **opts.spargs)
+                self.spider = self.pcrawler.spiders.create(opts.spider, **opts.spargs)
-            self.spider = create_spider_for_request(self.crawler.spiders, Request(url), **opts.spargs)
+            self.spider = create_spider_for_request(self.pcrawler.spiders, Request(url), **opts.spargs)
-        self.crawler.start()
+        self.pcrawler.crawl(self.spider, [request])
-                itemproc = self.crawler.engine.scraper.itemproc
+                itemproc = self.pcrawler.engine.scraper.itemproc
-        self.crawler.start()
+        crawler = self.crawler_process.create_crawler()
-        settings = self.crawler.settings
+        crawler = self.crawler_process.create_crawler()
-from scrapy import log
+
-        shell = Shell(self.crawler, update_vars=self.update_vars, code=opts.code)
+        spider = crawler.spiders.create(opts.spider) if opts.spider else None
-        t = Thread(target=self.crawler.start)
+        self.crawler_process.print_headers()
-    def create_crawler(self, name):
+    def create_crawler(self, name=None):
-    def start(self):
+    def print_headers(self):
-from scrapy.crawler import CrawlerProcess, MultiCrawlerProcess
+from scrapy.crawler import CrawlerProcess
-
+    cmd.crawler_process = CrawlerProcess(settings)
-from scrapy.exceptions import UsageError
+from scrapy.exceptions import UsageError, ScrapyDeprecationWarning
-    multi_crawlers = False
+    crawler_process = None
-            self._crawler.configure()
+        warnings.warn("Command's default `crawler` is deprecated and will be removed. "
-                crawler = self.process.create_crawler(spider.name)
+                crawler = self.crawler_process.create_crawler(spider.name)
-            self.process.start()
+            self.crawler_process.start()
-from scrapy.settings import overridden_settings
+
-        log.msg(format="Overridden settings: %(settings)r", settings=d, level=log.DEBUG)
+        self.start_crawling()
-class MultiCrawlerProcess(ProcessMixin):
+class CrawlerProcess(ProcessMixin):
-        super(MultiCrawlerProcess, self).__init__(settings)
+        super(CrawlerProcess, self).__init__(settings)
-            self.crawlers[name] = Crawler(self.settings)
+            crawler = Crawler(self.settings)
-    def start_crawler(self):
+    def start_crawling(self):
-            crawler.signals.connect(crawler.sflo.stop, signals.engine_stopped)
+        sflo = log.start_from_crawler(crawler)
-            self.start_crawler()
+            self.start_crawling()
-        super(MultiCrawlerProcess, self).start()
+        log.scrapy_info(self.settings)
-        return
+def start_from_settings(settings, crawler=None):
-        settings['LOG_ENCODING'], crawler)
+def scrapy_info(settings):
-        level=DEBUG)
+        msg("Optional features available: %s" % ", ".join(scrapy.optional_features),
-    return sflo
+        d = dict(overridden_settings(settings))
-        return dfd.addBoth(lambda _: wad) # it must return wad at last
+        return dfd.addBoth(lambda _: wad)  # it must return wad at last
-        info.downloaded[fp] = result # cache result
+        info.downloaded[fp] = result  # cache result
-
+"""
-from scrapy.contrib.pipeline.media import MediaPipeline
+from scrapy.exceptions import DropItem
-class ImageException(Exception):
+class ImageException(FileException):
-        refresh it in case of change.
+class ImagesPipeline(FilesPipeline):
-    def image_downloaded(self, response, request, info):
+    def process_downloaded_media(self, response, request, info):
-            self.store.persist_image(key, image, buf, info)
+            width, height = image.size
-        key = self.image_key(request.url)
+        key = self.file_key(request.url)
-
+    def file_key(self, url):
-        image_path = self.pipeline.image_key
+        image_path = self.pipeline.file_key
-        assert isinstance(self.pipeline.store, FSImagesStore)
+        from scrapy.contrib.pipeline.files import FSFilesStore
-from twisted.internet import defer, reactor
+from twisted.internet import defer, reactor, ssl
-            smtpuser=None, smtppass=None, smtpport=25, debug=False):
+            smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):
-            settings['MAIL_PASS'], settings.getint('MAIL_PORT'))
+            settings['MAIL_PASS'], settings.getint('MAIL_PORT'),
-            requireTransportSecurity=False)
+            requireTransportSecurity=self.smtptls)
-        reactor.connectTCP(self.smtphost, self.smtpport, factory)
+
-
+from scrapy.item import BaseItem
-
+
-    JsonItemExporter
+    JsonItemExporter, PythonItemExporter
-
+        self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')
-        except (IOError, struct.error):
+        except (IOError, EOFError, struct.error):
-                for loc in iterloc(s):
+                for loc in iterloc(s, self._alternate):
-def iterloc(it):
+def iterloc(it, alt=False):
-                d[name] = el.text.strip() if el.text else ''
+
-
+    
-            req_host = req_host.split(':')[0]
+        req_host = urlparse_cached(request).hostname
-class Follow(Resource):
+class DeferMixin(object):
-        return d
+        request.write(s)
-    def __init__(self, total=10, show=20, order="rand", *args, **kwargs):
+    def __init__(self, total=10, show=20, order="rand", maxlatency=0.0, *args, **kwargs):
-        url = "http://localhost:8998/follow?total=%s&show=%s&order=%s" % (total, show, order)
+        qargs = {'total': total, 'show': show, 'order': order, 'maxlatency': maxlatency}
-        self.assertEqual(len(spider.urls_visited), 11) # 10 + start_url
+        self.assertEqual(len(spider.urls_visited), 11)  # 10 + start_url
-            t = t2
+        # short to long delays
-from twisted.protocols.ftp import FTPClient, ConnectionLost 
+from twisted.protocols.ftp import FTPClient, ConnectionLost
-    
+
-# Forums 
+# Forums
-        self.assertEqual(list(sitemap_urls_from_robots(robots)), 
+        self.assertEqual(list(sitemap_urls_from_robots(robots)),
-        self._root = tree.getroot()
+        xmlp = lxml.etree.XMLParser(recover=True)
-            yield d
+            if 'loc' in d:
-
+    >>> escape_ajax("www.example.com/ajax.html?k1=v1&k2=v2#!key=value")
-    return url.replace('#!', '?_escaped_fragment_=')
+    defrag, frag = urlparse.urldefrag(url)
-    default_settings = {'LOG_ENABLED': True}
+    default_settings = {'LOG_ENABLED': False}
-        crawler.signals.connect(crawler.sflo.stop, signals.engine_stopped)
+        if crawler.sflo:
-from scrapy.crawler import CrawlerProcess
+from scrapy.crawler import CrawlerProcess, MultiCrawlerProcess
-    print 'Use "scrapy" to see available commands' 
+    print 'Use "scrapy" to see available commands'
-    cmd.set_crawler(crawler)
+
-        if not self.configured:
+        if not self.multi_crawlers and not self._crawler.configured:
-            self.configured = True
+
-        
+
-    default_settings = {'LOG_ENABLED': False}
+    multi_crawlers = True
-            spider = self.crawler.spiders.create(spider)
+        spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])
-                self.crawler.crawl(spider, requests)
+            elif requests:
-            self.crawler.start()
+            self.process.start()
-        return self.engine.open_spider(spider, requests)
+
-    signals for shutting down the crawl.
+class ProcessMixin(object):
-        reactor.run(installSignalHandlers=False) # blocking call
+        reactor.run(installSignalHandlers=False)  # blocking call
-        return d
+        raise NotImplementedError
-        except RuntimeError: # raised if already stopped or in shutdown stage
+        except RuntimeError:  # raised if already stopped or in shutdown stage
-""" 
+"""
- 
+
-    start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],
+    sflo = start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],
-        self.slots[spider] = SpiderSlot(file, exporter, storage, uri)
+        self.slot = SpiderSlot(file, exporter, storage, uri)
-        slot = self.slots.pop(spider)
+        slot = self.slot
-        slot = self.slots[spider]
+        slot = self.slot
-        del self.spiderinfo[spider]
+        self.spiderinfo = self.SpiderInfo(spider)
-        info = self.spiderinfo[spider]
+        info = self.spiderinfo
-        self.info = self.pipe.spiderinfo[self.spider]
+        self.info = self.pipe.spiderinfo
-                        self.domains_seen[spider].add(domain)
+                    if domain and domain not in self.domains_seen:
-        regex = self.host_regexes[spider]
+        regex = self.host_regex
-        del self.domains_seen[spider]
+        self.host_regex = self.get_host_regex(spider)
-        yield docrawl(spider, {"DOWNLOAD_DELAY": 0.3})
+        yield docrawl(spider, {"DOWNLOAD_DELAY": 1})
-            self.assertTrue(t2-t > 0.15, "download delay too small: %s" % (t2-t))
+            self.assertTrue(t2-t > 0.45, "download delay too small: %s" % (t2-t))
-        return spcls(**spider_kwargs)
+        if hasattr(self, 'crawler') and hasattr(spcls, 'from_crawler'):
-            set(['spider1', 'spider2', 'spider3']))
+            set(['spider1', 'spider2', 'spider3', 'spider4']))
-request_received = object()
+request_scheduled = object()
-item_passed = item_scraped # for backwards compatibility
+item_passed = item_scraped
-        self.crawler.signals.connect(self.request_received, signals.request_received)
+        self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)
-    def request_received(self, request, spider):
+    def request_scheduled(self, request, spider):
-        self._assert_received_requests()
+        self._assert_scheduled_requests()
-        must_be_visited = ["/", "/redirect", "/redirected", 
+        must_be_visited = ["/", "/redirect", "/redirected",
-        self.assertEqual(3, len(self.run.reqplug))
+    def _assert_scheduled_requests(self):
-from .http import HttpDownloadHandler
+from .http import HTTPDownloadHandler
-            httpdownloadhandler=HttpDownloadHandler):
+            httpdownloadhandler=HTTPDownloadHandler):
-        d.addBoth(self._cb_timeout, request, url, timeout)
+        d.addBoth(self._cb_timeout, request, url, timeout)
-    def _mexe(self, http_request, *args):
+    def _mexe(self, http_request, *args, **kwargs):
-    skip = True
+    skip = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'
-    skip = not encoders.issubset(set(Image.core.__dict__))
+    if not encoders.issubset(set(Image.core.__dict__)):
-        self.assertEqual(thumbnail_name("/tmp/foo.jpg", name),  
+        self.assertEqual(thumbnail_name("/tmp/foo.jpg", name),
-        d.addBoth(self._cb_timeout, request, url, timeout)
+        # check download timeout
-        return finished
+        def _cancel(_):
-            nlist = range(n, max(n-show, 0), -1)
+        else:  # order == "desc"
-class Delay(Resource):
+
-        d.addCallback(self._delayedRender)
+        b = getarg(request, "b", 1, type=int)
-    def _delayedRender(self, (request, n)):
+    def _delayedRender(self, request, n):
-class Partial(Resource):
+
-        d.addCallback(self._delayedRender)
+        self.deferRequest(request, 0, self._delayedRender, request)
-            stdout=PIPE, env=get_testenv())
+                          stdout=PIPE, env=get_testenv())
-    def __init__(self, n=1, *args, **kwargs):
+    def __init__(self, n=1, b=0, *args, **kwargs):
-            callback=self.parse, errback=self.errback)
+        url = "http://localhost:8998/delay?n=%s&b=%s" % (self.n, self.b)
-        return self.assertFailure(d, defer.TimeoutError, error.TimeoutError)
+        spider = BaseSpider('foo')
-        agent = self._get_agent(request, timeout)
+
-        d.addCallback(self._downloaded, request)
+        # check download timeout
-        request.meta['download_latency'] = time() - start_time
+    def _cb_timeout(self, result, request, url, timeout):
-            connectTimeout=timeout, bindAddress=bindaddress, pool=self._pool)
+    def _cb_latency(self, result, request, start_time):
-    def _downloaded(self, txresponse, request):
+    def _cb_bodyready(self, txresponse, request):
-            return self._build_response(('', None), txresponse, request)
+            return txresponse, '', None
-        txresponse.deliverBody(_ResponseReader(finished))
+        txresponse.deliverBody(_ResponseReader(finished, txresponse, request))
-        url = urldefrag(request.url)[0]
+    def _cb_bodydone(self, result, request, url):
-        return respcls(url=url, status=status, headers=headers, body=body)
+        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)
-    def __init__(self, finished):
+    def __init__(self, finished, txresponse, request):
-            self._finished.callback((body, None))
+            self._finished.callback((self._txresponse, body, None))
-            self._finished.callback((body, 'partial_download'))
+            self._finished.callback((self._txresponse, body, ['partial']))
-HttpDownloadHandler = HTTP10DownloadHandler
+class HttpDownloadHandler(HTTP10DownloadHandler):
-from scrapy.utils.misc import load_object
+from scrapy import optional_features
-            return reactor.connectTCP(host, port, factory)
+# backwards compatibility
-class Http11DownloadHandler(object):
+class HTTP11DownloadHandler(object):
-    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',
+    'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',
-from scrapy.core.downloader.handlers.http11 import Http11DownloadHandler
+from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownloadHandler
-    download_handler_cls = HttpDownloadHandler
+    download_handler_cls = HTTPDownloadHandler
-    if twisted.__version__.split('.') < (11, 1, 0):
+    download_handler_cls = HTTP11DownloadHandler
-    download_handler_cls = HttpDownloadHandler
+    download_handler_cls = HTTPDownloadHandler
-    download_handler_cls = Http11DownloadHandler
+class DeprecatedHttpProxyTestCase(unittest.TestCase):
-    if twisted.__version__.split('.') < (11, 1, 0):
+class Http10ProxyTestCase(HttpProxyTestCase):
-if txver > (13, 0, 0):
+from scrapy import twisted_version
-if txver >= (11, 1, 0):
+if twisted_version >= (11, 1, 0):
-    'https': 'scrapy.core.downloader.handlers.http11.Http11DownloadHandler',
+    'http': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',
-else:
+txver = twisted.__version__.split('.')
-from twisted.internet.endpoints import TCP4ClientEndpoint
+from scrapy.xlib.tx import Agent, ProxyAgent, ResponseDone, \
-        agent = self._get_agent(request)
+        agent = self._get_agent(request, timeout)
-        d.addErrback(self._agentrequest_failed, request)
+        d.addBoth(self._both_cb, request, start_time, url, timeout)
-        timeout = request.meta.get('download_timeout') or self._connectTimeout
+    def _both_cb(self, result, request, start_time, url, timeout):
-    def _agentrequest_downloaded(self, txresponse, request):
+    def _downloaded(self, txresponse, request):
-        request = Request(self.getURL('wait'), meta=dict(download_timeout=0.000001))
+        request = Request(self.getURL('wait'), meta=dict(download_timeout=0.1))
-from scrapy import signals, log
+from scrapy import signals
-            txlog.addObserver(self.catch_log)
+            crawler.signals.connect(self.error_count, signal=signals.spider_error)
-                    self.crawler.engine.close_spider(spider, 'closespider_errorcount')
+    def error_count(self, failure, response, spider):
-        self.assertTrue(diff.total_seconds() >= close_on)
+        total_seconds = diff.seconds + diff.microseconds
-        self.tasks = {}
+        self.close_on = {
-        if self.errorcount:
+        if self.close_on.get('errorcount'):
-        if self.pagecount:
+        if self.close_on.get('pagecount'):
-        if self.timeout:
+        if self.close_on.get('timeout'):
-        if self.itemcount:
+        if self.close_on.get('itemcount'):
-                if self.errorcounts[spider] == self.errorcount:
+                self.counter['errorcount'] += 1
-        if self.pagecounts[spider] == self.pagecount:
+        self.counter['pagecount'] += 1
-            self.crawler.engine.close_spider, spider=spider, \
+        self.task = reactor.callLater(self.close_on['timeout'], \
-        if self.counts[spider] == self.itemcount:
+        self.counter['itemcount'] += 1
-            tsk.cancel()
+        task = getattr(self, 'task', False)
-class FollowAllSpider(BaseSpider):
+
-    def __init__(self, total=10, show=20, order="rand"):
+    def __init__(self, total=10, show=20, order="rand", *args, **kwargs):
-class DelaySpider(BaseSpider):
+
-    def __init__(self, n=1):
+    def __init__(self, n=1, *args, **kwargs):
-class SimpleSpider(BaseSpider):
+
-    def __init__(self, url="http://localhost:8998"):
+    def __init__(self, url="http://localhost:8998", *args, **kwargs):
-import sys, time, json, random, urllib
+import sys, time, random, urllib
-    def getChild(self, request, name):
+    def getChild(self, name, request):
-import os, sys
+import os
-    return os.path.dirname(scrapy_path) + sep + os.environ.get('PYTHONPATH', '')
+    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')
-    def __init__(self, interval=60.0):
+    def __init__(self, stats, interval=60.0):
-        crawler.signals.connect(o.response_received, signal=signals.response_received)
+        o = cls(crawler.stats, interval)
-            self.tsk.stop()
+        self.pagesprev = 0
-        self.timeout = crawler.settings.getint('CLOSESPIDER_TIMEOUT')
+        self.timeout = crawler.settings.getfloat('CLOSESPIDER_TIMEOUT')
-copyright = u'2008-2012, Scrapinghub'
+copyright = u'2008-2013, Scrapy developers'
-   ur'Scrapinghub', 'manual'),
+   ur'Scrapy developers', 'manual'),
-from time import time
+import sys, time, json, random, urllib
-    d = {"time": time()}
+    d = {"time": time.time()}
-from scrapy.utils.test import get_crawler, get_testenv, get_testlog
+from scrapy.utils.test import get_crawler, get_testlog
-        self.proc.stdout.readline()
+        self.mockserver = MockServer()
-        time.sleep(0.2)
+        self.mockserver.__exit__(None, None, None)
-            'manager': self.crawler,
+            'crawler': self.crawler,
-from scrapy.utils.test import get_crawler, get_testenv
+from scrapy.utils.test import get_crawler, get_testenv, get_testlog
-            if errmsg:
+            if spider_failure.frames:
-from scrapy.utils.test import get_pythonpath
+from scrapy.utils.test import get_testenv
-        self.env['PYTHONPATH'] = get_pythonpath()
+        self.env = get_testenv()
-from scrapy.utils.test import get_pythonpath
+from scrapy.utils.test import get_testenv
-        self.env['PYTHONPATH'] = get_pythonpath()
+        self.env = get_testenv()
-from scrapy.utils.test import get_crawler
+from scrapy.utils.test import get_crawler, get_testenv
-        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'], stdout=PIPE)
+        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'],
-from twisted.web.server import Site
+from twisted.web.server import Site, NOT_DONE_YET
-from twisted.web.http_headers import Headers
+from twisted.web.http_headers import Headers as TxHeaders
-from scrapy.http import Headers as ScrapyHeaders
+from scrapy.http import Headers
-        self._contextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])
+        self._contextFactoryClass = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])
-        headers = Headers(request.headers)
+        headers = TxHeaders(request.headers)
-        headers = ScrapyHeaders(txresponse.headers.getAllRawHeaders())
+        headers = Headers(txresponse.headers.getAllRawHeaders())
-        self.handlers = DownloadHandlers(crawler.settings)
+        self.handlers = DownloadHandlers(crawler)
-    def __init__(self, settings):
+    def __init__(self, crawler):
-        handlers.update(settings.get('DOWNLOAD_HANDLERS', {}))
+        handlers = crawler.settings.get('DOWNLOAD_HANDLERS_BASE')
-                dh = cls(settings)
+                dh = cls(crawler.settings)
-                self._handlers[scheme] = dh.download_request
+                self._handlers[scheme] = dh
-            handler = self._handlers[scheme]
+            handler = self._handlers[scheme].download_request
-    def __init__(self):
+    def __init__(self, settings):
-        self._contextFactory = ClientContextFactory()
+        self._contextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])
-        bodyproducer = _RequestBodyProducer(request.body or '')
+        bodyproducer = _RequestBodyProducer(request.body) if request.body else None
-        self.download_request = HttpDownloadHandler(Settings()).download_request
+        self.download_handler = self.download_handler_cls(Settings())
-        return self.port.stopListening()
+        yield self.port.stopListening()
-        self.download_request = Http11DownloadHandler().download_request
+    """HTTP 1.1 test case"""
-        self.download_request = HttpDownloadHandler(Settings()).download_request
+        self.download_handler = self.download_handler_cls(Settings())
-        return self.port.stopListening()
+        yield self.port.stopListening()
-        self.download_request = Http11DownloadHandler().download_request
+    download_handler_cls = Http11DownloadHandler
-
+"""Download handlers for http and https schemes
-
+
-                    self.ClientContextFactory())
+            return reactor.connectSSL(host, port, factory,
-    serUrl method to make use of our Url object that cache the parse 
+    serUrl method to make use of our Url object that cache the parse
-DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.webclient.ScrapyClientContextFactory'
+DOWNLOADER_CLIENTCONTEXTFACTORY = 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'
-        self._pool = HTTPConnectionPool(reactor, persistent=False)
+        self._pool = HTTPConnectionPool(reactor, persistent=True)
-from twisted.web.client import Agent, ProxyAgent, ResponseDone, ResponseFailed
+from twisted.web.client import Agent, ProxyAgent, ResponseDone, \
-    def __init__(self, contextFactory=None, connectTimeout=180, bindAddress=None):
+    _Agent = Agent
-        request_timeout = request.meta.get('download_timeout') or self._connectTimeout
+    def download_request(self, request):
-        if proxy is not None and proxy != '':
+        if proxy:
-                bindAddress=self._bindAddress)
+            endpoint = TCP4ClientEndpoint(reactor, host, port, timeout=timeout,
-        )
+    def _agentrequest_downloaded(self, txresponse, request):
-        return failure
+<<<<<<< HEAD
-
+from time import time
-
+from twisted.web.client import Agent, ProxyAgent, ResponseDone, ResponseFailed
-    return _parsed_url_args(parsed)
+from scrapy.http import Headers as ScrapyHeaders
-        self._reactor = reactor
+
-        proxy = self._scrapyrequest.meta.get('proxy')
+        proxy = request.meta.get('proxy')
-            endpoint = TCP4ClientEndpoint(self._reactor,
+            endpoint = TCP4ClientEndpoint(reactor,
-            agent = Agent(self._reactor,
+            agent = Agent(reactor,
-        return d
+        request._tw_start_time = time()
-class ScrapyAgentRequestBodyProducer(object):
+class _RequestBodyProducer(object):
-        self.debug = debug
+class _ResponseReader(protocol.Protocol):
-        # finished is the deferred that will be fired
+    def __init__(self, finished):
-        self.bodyBuffer = StringIO()
+        self._bodybuf = StringIO()
-        self.bodyBuffer.write(bodyBytes)
+        self._bodybuf.write(bodyBytes)
-            body=self.bodyBuffer.getvalue())
+        body = self._bodybuf.getvalue()
-    def __init__(self, httpclientfactory=None):
+    def __init__(self):
-        agent = ScrapyAgent(reactor, self._httpclientfactory)
+        agent = ScrapyAgent(reactor)
-    def _agent_callback(self, response, request):
+    def _agent_callback(self, txresponse, request):
-
+        finished.addCallback(self._build_response, txresponse, request)
-            raise defer.TimeoutError
+        #log.err(failure, 'HTTP11 failure: %s' % request)
-    'https': 'scrapy.core.downloader.handlers.http.HttpDownloadHandler',
+    'http': 'scrapy.core.downloader.handlers.http11.Http11DownloadHandler',
-from twisted.internet import reactor, defer
+from twisted.internet import reactor, defer, error
-        return self.assertFailure(d, defer.TimeoutError)
+        return self.assertFailure(d, defer.TimeoutError, error.TimeoutError)
-"""Download handlers for http scheme"""
+"""Download handlers for http and https schemes"""
-        port = 80
+        port = 443 if scheme == 'https' else 80
-"""Download handlers for http and https schemes"""
+"""Download handlers for http scheme"""
-        port = 443 if scheme == 'https' else 80
+        port = 80
-            bindAddress=bindAddress)
+        self._contextFactory = contextFactory
-    def bindRequest(self, request):
+    def launchRequest(self, request):
-            self._agent._proxyEndpoint._timeout = request.meta.get('download_timeout') or self._agent._proxyEndpoint._timeout
+            endpoint = TCP4ClientEndpoint(self._reactor,
-            self._agent._connectTimeout = request.meta.get('download_timeout') or self._agent._connectTimeout
+            agent = Agent(self._reactor,
-        d = self._agent.request(
+        d = agent.request(
-class ScrapyAgentResponseBodyReader(protocol.Protocol):
+class ScrapyAgentResponseReader(protocol.Protocol):
-        # body
+        # twisted.web._newclient.HTTPClientParser already decodes chunked response bodies,
-class HttpDownloadHandler(object):
+class Http11DownloadHandler(object):
-        d = agent.launch()
+        agent = ScrapyAgent(reactor, self._httpclientfactory)
-        response.deliverBody(reader)
+        reader = ScrapyAgentResponseReader(finished, response, request, debug = 0)
-        if request.method != 'HEAD':
+        # is a response body expected?
-from scrapy.core.downloader.handlers.http11 import HttpDownloadHandler as Http11DownloadHandler
+from scrapy.core.downloader.handlers.http11 import Http11DownloadHandler
-        # deletes an object from the 'johnsmith' bucket using the 
+        # deletes an object from the 'johnsmith' bucket using the
-from twisted.internet import reactor, defer
+from twisted.internet import reactor, defer, task
-from twisted.trial.unittest import TestCase, SkipTest
+from twisted.trial.unittest import TestCase
-    def __init__(self):
+    def __init__(self, total=10, show=20, order="rand"):
-        yield docrawl(spider)
+        yield docrawl(spider, {"DOWNLOAD_DELAY": 0.3})
-            self.assertTrue(y-t > 0.5, "download delay too small: %s" % (y-t))
+        for t2 in spider.times[1:]:
-    port = reactor.listenTCP(8998, factory, interface="127.0.0.1")
+    port = reactor.listenTCP(8998, factory)
-    >>> cachecontrol_directives('public, max-age=3600')
+    >>> parse_cachecontrol('public, max-age=3600')
-    >>> cachecontrol_directives('')
+    >>> parse_cachecontrol('')
-    ['www.example.com', 'example.com']
+    ['www.example.com', 'example.com', '.www.example.com', '.example.com']
-    return matches
+    return matches + ['.' + d for d in matches]
-
+
-        
+
-    
+
-    
+
-from scrapy.utils.pqueue import PriorityQueue
+from queuelib import PriorityQueue
-from scrapy.utils import queue
+from queuelib import queue
-from scrapy.tests import test_utils_queue as t
+from queuelib.tests import test_queue as t
-    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.2', 'lxml', 'pyOpenSSL']
+    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.2', 'queuelib', 'lxml', 'pyOpenSSL']
-from copy import deepcopy
+from copy import deepcopy
-            values = Field() 
+            keys = Field()
-            values = Field() 
+            keys = Field()
-reactor.run()
+if __name__ == "__main__":
-from subprocess import Popen
+from subprocess import Popen, PIPE
-        time.sleep(0.5)
+        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'], stdout=PIPE)
-        print log
+        self.assert_("[scrapy] INFO: It Works!" in log, log)
-        time.sleep(0.2)
+        time.sleep(0.5)
-        spider_name = 'parse_spider'
+    def setUp(self):
-        if self.test_arg:
+        if getattr(self, 'test_arg', None):
-""".format(spider_name))
+        return [Item()]
-        p = self.proc('parse', '--spider', spider_name, '-a', 'test_arg=1', '-c', 'parse', 'http://scrapinghub.com')
+    def process_item(self, item, spider):
-    
+
-    
+
-                self.spider = self.crawler.spiders.create(opts.spider)
+                self.spider = self.crawler.spiders.create(opts.spider, **opts.spargs)
-            self.spider = create_spider_for_request(self.crawler.spiders, Request(url))
+            self.spider = create_spider_for_request(self.crawler.spiders, Request(url), **opts.spargs)
-        settings = get_project_settings()
+        from scrapy.conf import settings
-        from scrapy.conf import settings
+        settings = get_project_settings()
-# optional_features is a set containing Scrapy optional features
+# WARNING: optional_features set is deprecated and will be removed soon. Do not use.
-    optional_features.add('ssl')
+# TODO: backwards compatibility, remove for Scrapy 0.20
-            raise NotSupported("HTTPS not supported: install pyopenssl library")
+            return reactor.connectSSL(host, port, factory, \
-
+import json
-        return r
+        return r
-            or self.scraper.slots[spider].needs_backout()
+            or self.scraper.slot.needs_backout()
-            and self.scraper.slots[spider].is_idle()
+        scraper_idle = self.scraper.slot.is_idle()
-        self.slots = {}
+        self.slot = None
-        self.slots[spider] = Slot()
+        self.slot = Slot()
-        slot = self.slots[spider]
+        slot = self.slot
-        return not self.slots
+        return not self.slot
-        slot = self.slots[spider]
+        slot = self.slot
-            self.slots[spider].itemproc_size += 1
+            self.slot.itemproc_size += 1
-        self.slots[spider].itemproc_size -= 1
+        self.slot.itemproc_size -= 1
-        "len(engine.scraper.slots)",
+        "1",
-        "engine.scraper.slots[spider].needs_backout()",
+        "len(engine.scraper.slot.queue)",
-            _upload_egg(target, egg, project, version)
+            if not _upload_egg(target, egg, project, version):
-    _http_post(req)
+    return _http_post(req)
-        return cls(url, method=form.method, formdata=formdata, **kwargs)
+        method = kwargs.pop('method', form.method)
-
+            self.df.log(request, self.spider)
-            allowed_domains = ['example.org', 'example.net']
+            allowed_domains = ('example.org', 'example.net')
-
+
-from scrapy.linkextractor import IGNORED_EXTENSIONS
+from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url
-
+        spider = BaseSpider(name='example.com', allowed_domains=set(('example.com', 'example.net')))
-                _log("Building egg of %s-%s" % (project, version))
+                _log("Packing version %s" % version)
-            shutil.rmtree(tmpdir)
+            if opts.debug:
-        return 'r%s' % p.communicate()[0]
+        d = 'r%s' % p.communicate()[0]
-        return '%s' % p.communicate()[0].strip('\n')
+        d = p.communicate()[0].strip('\n')
-    _log("Deploying %s-%s to %s" % (project, version, url))
+    _log('Deploying to project "%s" in %s' % (project, url))
-    retry_on_eintr(check_call, [sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=f)
+    d = tempfile.mkdtemp(prefix="scrapydeploy-")
-
+
-        getattr(spider, 'allowed_domains', []))
+    return url_is_from_any_domain(url,
-def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \
+
-
+from scrapy.exceptions import IgnoreRequest
-    relevant_classes = (BaseSpider, Request, Response, BaseItem, \
+    relevant_classes = (BaseSpider, Request, Response, BaseItem,
-            spider = create_spider_for_request(self.crawler.spiders, request, \
+            spider = create_spider_for_request(self.crawler.spiders, request,
-            self._schedule, request, spider)
+        try:
-    def __init__(self, sender=dispatcher.Any):
+    def __init__(self, sender=dispatcher.Anonymous):
-    def __init__(self, sender=dispatcher.Anonymous):
+    def __init__(self, sender=dispatcher.Any):
-    elif not isinstance(arg, dict) and not isinstance(arg, BaseItem) and hasattr(arg, '__iter__'):
+    elif not isinstance(arg, (dict, BaseItem)) and hasattr(arg, '__iter__'):
-    elif not isinstance(arg, dict) and hasattr(arg, '__iter__'):
+    elif not isinstance(arg, dict) and not isinstance(arg, BaseItem) and hasattr(arg, '__iter__'):
-RETRY_HTTP_CODES = [500, 503, 504, 400, 408]
+RETRY_HTTP_CODES = [500, 502, 503, 504, 400, 408]
-        if args or kwargs: # avoid creating dict for most common case
+        if args or kwargs:  # avoid creating dict for most common case
-            raise KeyError("%s does not support field: %s" % \
+            raise KeyError("%s does not support field: %s" %
-            raise AttributeError("Use item[%r] = %r to set field value" % \
+            raise AttributeError("Use item[%r] = %r to set field value" %
-
+
-    return posixpath.splitext(parse_url(url).path)[1].replace('.', '').lower() in extensions
+    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions
-from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url
+from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url, url_has_any_extension
-    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions
+    return posixpath.splitext(parse_url(url).path)[1].replace('.', '').lower() in extensions
-    def log(self, message, level=log.DEBUG):
+    def log(self, message, level=log.DEBUG, **kw):
-        log.msg(message, spider=self, level=level)
+        log.msg(message, spider=self, level=level, **kw)
-    suffix = '%s.%s' % __import__('scrapy').version_info[:2]
+    suffix = '%s.%s' % version_info[:2]
-__version__ = "0.17.0"
+import pkgutil
-version = __import__('scrapy').__version__
+    with open('scrapy/VERSION', 'w+') as f:
-    for ifn in glob.glob("debian/scrapy.*") + glob.glob("debian/scrapyd.*"):
+    for ifn in glob.glob("debian/scrapy.*"):
-            glob.glob("debian/scrapyd%s*" % suffix):
+    for f in glob.glob("debian/python-scrapy%s*" % suffix):
-# Scrapy setup.py script 
+# Scrapy setup.py script
-for scrapy_dir in ['scrapy', 'scrapyd']:
+for scrapy_dir in ['scrapy']:
- 
+
-    pass
+error = KeyError
-                                   {}, {}, [''])
+        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'], {}, {}, [''])
-        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'])
+        self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'],
-    path = safe_url_string(urllib.unquote(path)) or '/'
+    path = safe_url_string(_unquotepath(path)) or '/'
-
+
-    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(), 
+    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),
-            unique=unique, process_value=process_value)
+        BaseSgmlLinkExtractor.__init__(self,
-
+
-                         [Link(url='http://example.org/somepage/item/12.html', text='Item 12'), 
+                         [Link(url='http://example.org/somepage/item/12.html', text='Item 12'),
-                          Link(url='http://example.org/othercat.html', text='Other category'), 
+                          Link(url='http://example.org/othercat.html', text='Other category'),
-             [Link(url='http://www.example.com/item/12.html', text=u'Wrong: \ufffd')])
+        self.assertEqual(lx.extract_links(response), [
-              Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \xe2\x82\xac text'.decode('utf-8')) ])
+        self.assertEqual(lx.extract_links(response_utf8), [
-              Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \xe2\x82\xac text'.decode('utf-8')) ])
+        self.assertEqual(lx.extract_links(response_noenc), [
-              Link(url='http://example.com/sample_%E1.html', text='sample \xe1 text'.decode('latin1')) ])
+        self.assertEqual(lx.extract_links(response_latin1), [
-              Link(url='http://example.org/about.html', text=u'About us', nofollow=False) ])
+        self.assertEqual([link for link in lx.extract_links(response)], [
-              Link(url='http://www.google.com/something', text=u'') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-              Link(url='http://example.com/sample3.html', text=u'sample 3 text') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-              Link(url='http://example.com/sample3.html', text=u'sample 3 repetition') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-              ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-              Link(url='http://example.com/sample2.html', text=u'sample 2') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-            [ Link(url='http://www.google.com/something', text=u'') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-              Link(url='http://example.com/sample3.html', text=u'sample 3 text') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-              Link(url='http://example.com/sample2.html', text=u'sample 2') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-            [ Link(url='http://www.google.com/something', text=u'') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-            [ Link(url='http://www.google.com/something', text=u'') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-            allow_domains=('blah1.com', ), deny_domains=('blah2.com', ))
+        lx = SgmlLinkExtractor(allow=('blah1',), deny=('blah2',),
-              Link(url='http://example.com/sample2.html', text=u'sample 2') ])
+        self.assertEqual([link for link in lx.extract_links(self.response)], [
-        lx = SgmlLinkExtractor(restrict_xpaths="//div[@class='links']") 
+        lx = SgmlLinkExtractor(restrict_xpaths="//div[@class='links']")
-            [Link(url='http://example.org/page.html', text=u'asd')])
+        self.assertEqual(lx.extract_links(response), [
-        lx = SgmlLinkExtractor(restrict_xpaths="//p") 
+        lx = SgmlLinkExtractor(restrict_xpaths="//p")
-                for xpath_expr in self.restrict_xpaths)
+            body = u''.join(f
-            html = response.body
+            body = response.body
-        links = self._extract_links(html, response.url, response.encoding, base_url)
+        links = self._extract_links(body, response.url, response.encoding, base_url)
-
+from scrapy.utils.spider import iterate_spider_output
-        if hasattr(self, 'parse_item'): # backward compatibility
+        if hasattr(self, 'parse_item'):  # backward compatibility
-        
+
-                raise TypeError('You cannot return an "%s" object from a spider' % type(ret).__name__)
+            ret = iterate_spider_output(self.parse_node(response, selector))
-            nodes = xmliter(response, self.itertag)
+            nodes = self._iternodes(response)
-import gzip, warnings, inspect
+import gzip
-    if not root.forms:
+    forms = root.xpath('//form')
-    
+
-            form = root.forms[formnumber]
+            form = forms[formnumber]
-        
+
-        if hasattr(serialized_value, '__iter__'):
+        if hasattr(serialized_value, 'items'):
-            return self.init_request()
+    def start_requests(self):
-        return reqs
+        return self.__dict__.pop('_postinit_reqs')
-        super(_FilesystemCacheStorage, self).__init__(*args, **kwargs)
+        super(FilesystemCacheStorage, self).__init__(*args, **kwargs)
-from scrapy.http import TextResponse, HtmlResponse
+from scrapy.http import TextResponse, HtmlResponse, XmlResponse
-        self.assertDictEqual(
+        self.assertEqual(
-        self.assertDictEqual({}, i.errors)
+        self.assertEqual({}, i.errors)
-HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.RFC2616Policy'
+HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.DummyPolicy'
-
+from scrapy import optional_features
-                if model_field.auto_created == False:
+                if not model_field.auto_created:
-if django:
+if 'django' in optional_features:
-    django_model = Person
+    class BasePersonItem(DjangoItem):
-    other = Field()
+    class NewFieldPersonItem(BasePersonItem):
-    django_model = IdentifiedPerson
+    class IdentifiedPersonItem(DjangoItem):
-        if not django:
+        if 'django' not in optional_features:
-from time import time
+from time import time
-        if not db.has_key(tkey):
+        if tkey not in db:
-This module contains the default values for all settings used by Scrapy. 
+This module contains the default values for all settings used by Scrapy.
-HTTPCACHE_POLICY = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'
+HTTPCACHE_POLICY = 'scrapy.contrib.httpcache.RFC2616Policy'
-    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'
+    policy_class = 'scrapy.contrib.httpcache.RFC2616Policy'
-class DefaultMiddlewaretest(_BaseTest):
+class DummyPolicyTest(_BaseTest):
-class RFC2616MiddlewareTest(DefaultStorageTest):
+class RFC2616PolicyTest(DefaultStorageTest):
-    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.RFC2616Policy'
+    policy_class = 'scrapy.contrib.httpcache.RFC2616Policy'
-        
+
-    
+
-            if value:
+            if value is not None and value != '':
-# test items
+# test items
-# test item loaders
+# test item loaders
-# test processors
+# test processors
-
+
-import sys, os
+import os
-FEED_URI_PARAMS = None # a function to extend uri arguments
+FEED_URI_PARAMS = None  # a function to extend uri arguments
-RETRY_TIMES = 2 # initial response + 2 retries = 3 requests
+RETRY_TIMES = 2  # initial response + 2 retries = 3 requests
-    'scrapy.contracts.default.UrlContract' : 1,
+    'scrapy.contracts.default.UrlContract': 1,
-        if not settings.getbool('REDIRECT_ENABLED'):
+        if not settings.getbool(self.enabled_setting):
-        self.max_metarefresh_delay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY')
+        self._maxdelay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY',
-            if url and interval < self.max_metarefresh_delay:
+            if url and interval < self._maxdelay:
-REDIRECT_MAX_TIMES = 20 # uses Firefox default setting
+REDIRECT_MAX_TIMES = 20  # uses Firefox default setting
-    """Handle redirection of requests based on response status and meta-refresh html tag"""
+class BaseRedirectMiddleware(object):
-        self.max_metarefresh_delay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY')
+
-
+class RedirectMiddleware(BaseRedirectMiddleware):
-This module contains the default values for all settings used by Scrapy. 
+This module contains the default values for all settings used by Scrapy.
-        stats.open_spider(self.spider)
+        self.crawler.stats.open_spider(self.spider)
-        stats.close_spider(self.spider, '')
+        self.crawler.stats.close_spider(self.spider, '')
-    def _download(self,request, response=None):
+    def _download(self, request, response=None):
-            response = Response(request.url, request=request)
+            response = Response(request.url)
-        resp = Response(req.url, status=200, request=req)
+        resp = Response(req.url, status=200)
-    request.
+        self.assertTrue(isinstance(ret, Response), "Non-response returned")
-    'Content-Encoding: gzip' giving as result the error below:
+    def test_3xx_and_invalid_gzipped_body_must_redirect(self):
-        exceptions.IOError: Not a gzipped file
+        This happens when httpcompression middleware is executed before redirect
-    """
+            exceptions.IOError: Not a gzipped file
-    def test_gzipped_redirection(self):
+        """
-        resp = Response(req.url, status=302, body=body, request=req, headers={
+        resp = Response(req.url, status=302, body=body, headers={
-from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware
+from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware, MetaRefreshMiddleware
-from scrapy.http import Request, Response, HtmlResponse, Headers
+from scrapy.http import Request, Response, HtmlResponse
-        req = Request(url, method='POST', body='test', 
+        req = Request(url, method='POST', body='test',
-        rsp = HtmlResponse(url='http://example.org', body=body)
+        rsp = HtmlResponse(req.url, body=self._body())
-        rsp = HtmlResponse(url='http://example.org', body=body)
+        rsp = HtmlResponse(url='http://example.org', body=self._body(interval=1000))
-        rsp = HtmlResponse(url='http://example.org', body=body)
+                      headers={'Content-Type': 'text/plain', 'Content-length': '4'})
-        rsp = Response('http://scrapytest.org/302', headers={'Location': '/redirected'}, status=302)
+        req = Request('http://scrapytest.org/max')
-        rsp = Response('http://www.scrapytest.org/302', headers={'Location': '/redirected'}, status=302)
+        rsp = HtmlResponse(req.url, body=self._body())
-        rsp1 = Response('http://scrapytest.org/first', headers={'Location': '/redirected'}, status=302)
+        rsp1 = HtmlResponse(req1.url, body=self._body(url='/redirected'))
-        rsp2 = Response('http://scrapytest.org/redirected', headers={'Location': '/redirected2'}, status=302)
+        assert isinstance(req2, Request), req2
-
+        assert isinstance(req3, Request), req3
-        if 'no-cache' in cc:
+        ccreq = self._parse_cachecontrol(request)
-
+from email.utils import formatdate
-from scrapy.utils.project import data_path
+from scrapy.contrib.httpcache import rfc1123_to_epoch, parse_cachecontrol
-        return response.status not in self.ignore_http_codes
+    def should_cache_response(self, response, request):
-        super(RFC2616Policy, self).__init__(settings)
+    def is_cached_response_fresh(self, response, request):
-        retval = super(RFC2616Policy, self).should_cache_response(response)
+    def is_cached_response_valid(self, cachedresponse, response, request):
-        return retval
+    def _parse_cachecontrol(self, r):
-        return retval
+        if urlparse_cached(request).scheme in self.ignore_schemes:
-        self.policy = load_object(settings['HTTPCACHE_POLICY'])(settings)
+        # Skip uncacheable requests
-        if response and self.policy.should_cache_response(response):
+        # Look for cached response and check if expired
-            raise IgnoreRequest("Ignored request not in cache: %s" % request)
+            return cachedresponse
-class FilesystemCacheStorage(object):
+        # Keep a reference to cached response to avoid a second cache lookup on
-        response = respcls(url=url, headers=headers, status=status, body=body)
+    def process_response(self, request, response, spider):
-            return pickle.load(f)
+    def _cache_response(self, spider, response, request, cachedresponse):
-from scrapy import log
+from email.utils import mktime_tz, parsedate_tz
-            return # not cached
+            return  # not cached
-            return # not found
+            return  # not found
-            return # expired
+            return  # expired
-class DbmRealCacheStorage(DbmCacheStorage, BaseRealCacheStorage):
+class FilesystemCacheStorage(object):
-    
+        self.cachedir = data_path(settings['HTTPCACHE_DIR'])
-            return self.retrieve_cache(spider, request, data['headers'], data['status'], data['url'], data['body'])
+        """Return response if present in cache, or None otherwise."""
-from scrapy.contrib.httpcache import DbmRealCacheStorage
+from scrapy.contrib.downloadermiddleware.httpcache import HttpCacheMiddleware
-class HttpCacheMiddlewareTest(unittest.TestCase):
+class _BaseTest(unittest.TestCase):
-    tomorrow = email.utils.formatdate(time.time() + 1 * 24 * 60 * 60)
+    storage_class = 'scrapy.contrib.httpcache.DbmCacheStorage'
-            {'Content-Type': 'text/html'}, body='test body', status=202)
+                               headers={'User-Agent': 'test'})
-            'HTTPCACHE_POLICY': self.dummy_policy
+            'HTTPCACHE_POLICY': self.policy_class,
-            storage.close_spider(self.spider)
+        with self._middleware(**new_settings) as mw:
-            assert mw.process_request(self.request, self.spider) is None
+class DummyMiddlewareTest(DefaultStorageTest):
-            assert mw.process_request(self.request, self.spider) is None
+    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'
-            assert storage.retrieve_response(self.spider, self.request) is None
+class RFC2616MiddlewareTest(DefaultStorageTest):
-            assert storage.retrieve_response(self.spider, self.request) is None
+    policy_class = 'scrapy.contrib.downloadermiddleware.httpcache.RFC2616Policy'
-        self.assertEqual(request1.body, request2.body)
+    def _process_requestresponse(self, mw, request, response):
-        return response
+from scrapy.contrib.httpcache import FilesystemCacheStorage as _FilesystemCacheStorage
-            return pickle.load(f)
+    def __init__(self, *args, **kwargs):
-
+from w3lib.http import headers_raw_to_dict, headers_dict_to_raw
-            return # not cached
+            return  # not cached
-    FilesystemCacheStorage, HttpCacheMiddleware
+from scrapy.contrib.httpcache import FilesystemCacheStorage, DbmCacheStorage
-    storage_class = FilesystemCacheStorage
+    storage_class = DbmCacheStorage
-            {'Content-Type': 'text/html'}, body='test body', status=202)
+                               headers={'User-Agent': 'test'})
-        i = BasePersonItem(name='John', age='22')
+        i = BasePersonItem(name='John')
-        self.assertTrue(i.is_valid())
+from django.core.exceptions import ValidationError
-        return model
+            self.instance.save()
-    
+
-                log.err(output, 'Error processing %(item)s', item=item, spider=spider)
+                log.err(output, 'Error processing %s' % item, spider=spider)
-        except: # FIXME: catching everything!
+        except:  # FIXME: catching everything!
-            }
+        'Cache-Control': 'max-age=172800',
-                headers=self.HEADERS, policy=self.POLICY)
+        return threads.deferToThread(k.set_contents_from_file, buf,
-            }
+        '': FSImagesStore,
-        if os.path.isabs(uri): # to support win32 paths like: C:\\some\dir
+        if os.path.isabs(uri):  # to support win32 paths like: C:\\some\dir
-            raise ImageException
+            raise ImageException('download-error')
-            raise ImageException
+            raise ImageException('empty-content')
-                    level=log.WARNING, spider=info.spider)
+        except ImageException as exc:
-            raise ImageException
+        except Exception as exc:
-                return # returning None force download
+                return  # returning None force download
-                return # returning None force download
+                return  # returning None force download
-                return # returning None force download
+                return  # returning None force download
-        dfd.addCallbacks(_onsuccess, lambda _:None)
+        dfd.addCallbacks(_onsuccess, lambda _: None)
-                    (width, height, self.MIN_WIDTH, self.MIN_HEIGHT, response.url))
+            raise ImageException("Image too small (%dx%d < %dx%d)" %
-
+        image.save(buf, 'JPEG')
-        shell.start(url=url)
+        shell.start(url=url, spider=spider)
-                log.msg(level=log.DEBUG, spider=spider, **logkws)
+                log.msg(spider=spider, **logkws)
-                logkws['level'] = log.DEBUG
+from scrapy import log
-class HttpCachePolicy(object):
+class DummyPolicy(object):
-        self.policy = settings.get('HTTPCACHE_POLICY')
+        
-            self.use_dummy_cache = False
+
-        if not self.use_dummy_cache and response.headers.has_key('cache-control'):
+        retval = super(RFC2616Policy, self).should_cache_response(response)
-        if not self.use_dummy_cache and request.headers.has_key('cache-control'):
+        retval = super(RFC2616Policy, self).should_cache_request(request)
-class HttpCacheMiddleware(HttpCachePolicy):
+class HttpCacheMiddleware(object):
-        o = cls.from_settings(crawler.settings, crawler.stats)
+        o = cls(crawler.settings, crawler.stats)
-        if not self.should_cache_request(request):
+        if not self.policy.should_cache_request(request):
-        if response and self.should_cache_response(response):
+        if response and self.policy.should_cache_response(response):
-            else:
+            if isinstance(self.policy, RFC2616Policy):
-            else:
+        if (self.policy.should_cache_request(request)
-HTTPCACHE_POLICY = 'dummy'
+HTTPCACHE_POLICY = 'scrapy.contrib.downloadermiddleware.httpcache.DummyPolicy'
-            'HTTPCACHE_POLICY': 'dummy'
+            'HTTPCACHE_POLICY': self.dummy_policy
-        if settings.get('HTTPCACHE_POLICY') == 'dummy':
+        if settings.get('HTTPCACHE_POLICY') == self.dummy_policy:
-    def test_real_http_cache_middleware_response304_not_cached(self):
+    def test_middleware_rfc2616policy_response304_not_cached(self):
-        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:
+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:
-    def test_real_http_cache_middleware_response_nostore_not_cached(self):
+    def test_middleware_rfc2616policy_response_nostore_not_cached(self):
-        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:
+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:
-    def test_real_http_cache_middleware_request_nostore_not_cached(self):
+    def test_middleware_rfc2616policy_request_nostore_not_cached(self):
-        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:
+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:
-    def test_real_http_cache_middleware_response_cached_and_fresh(self):
+    def test_middleware_rfc2616policy_response_cached_and_fresh(self):
-        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:
+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy) as mw:
-    def test_real_http_cache_middleware_response_cached_and_stale(self):
+    def test_middleware_rfc2616policy_response_cached_and_stale(self):
-        with self._middleware(HTTPCACHE_POLICY='rfc2616',
+        with self._middleware(HTTPCACHE_POLICY=self.rfc2616_policy,
-    def test_real_http_cache_storage_response_cached_and_fresh(self):
+    def test_storage_rfc2616policy_response_cached_and_fresh(self):
-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:
-    def test_real_http_cache_storage_response403_cached_and_further_requests_ignored(self):
+    def test_storage_rfc2616policy_response403_cached_and_further_requests_ignored(self):
-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:
-    def test_real_http_cache_storage_response_cached_and_stale(self):
+    def test_storage_rfc2616policy_response_cached_and_stale(self):
-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:
-    def test_real_http_cache_storage_response_cached_and_stale_with_cache_validators(self):
+    def test_storage_rfc2616policy_response_cached_and_stale_with_cache_validators(self):
-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:
-    def test_real_http_cache_storage_response_cached_and_transparent(self):
+    def test_storage_rfc2616policy_response_cached_and_transparent(self):
-        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
+        with self._storage(HTTPCACHE_POLICY=self.rfc2616_policy) as storage:
-class HttpCacheMiddleware(object):
+class HttpCachePolicy(object):
-        self.policy_response = settings.get('HTTPCACHE_POLICY_RESPONSE')
+        super(HttpCacheMiddleware, self).__init__(settings)
-        o = cls(crawler.settings, crawler.stats)
+        o = cls.from_settings(crawler.settings, crawler.stats)
-        if not self.is_cacheable(request):
+        if not self.should_cache_request(request):
-        if response and self.is_cacheable_response(response):
+        if response and self.should_cache_response(response):
-            and self.is_cacheable_response(response)):
+        if (self.should_cache_request(request)
-HTTPCACHE_POLICY_RESPONSE = lambda response : True
+HTTPCACHE_POLICY = 'dummy'
-            'HTTPCACHE_POLICY_RESPONSE': lambda response : True,
+            'HTTPCACHE_POLICY': 'dummy'
-        if settings.getbool('HTTPCACHE_USE_DUMMY'):
+        if settings.get('HTTPCACHE_POLICY') == 'dummy':
-        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:
+        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:
-        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:
+        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:
-        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:
+        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:
-        with self._middleware(HTTPCACHE_USE_DUMMY=False) as mw:
+        with self._middleware(HTTPCACHE_POLICY='rfc2616') as mw:
-        with self._middleware(HTTPCACHE_USE_DUMMY=False,
+        with self._middleware(HTTPCACHE_POLICY='rfc2616',
-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:
+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:
+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:
+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:
+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
-        with self._storage(HTTPCACHE_USE_DUMMY=False) as storage:
+        with self._storage(HTTPCACHE_POLICY='rfc2616') as storage:
-                    self.stats.inc_value('httpcache/hits', spider=spider)
+                    self.stats.inc_value('httpcache/hit', spider=spider)
-                log.msg(level=log.WARNING, spider=spider, **logkws)
+                if 'level' not in logkws:
-            log.msg(level=log.DEBUG, spider=spider, **logkws)
+            if 'level' not in logkws:
-
+        self.policy_request = settings.get('HTTPCACHE_POLICY_REQUEST')
-        storage = self.storage_class(settings)
+        if settings.getbool('HTTPCACHE_USE_DUMMY'):
-        return response.status not in self.ignore_http_codes
+        retval = response.status not in self.ignore_http_codes
-
+        retval = urlparse_cached(request).scheme not in self.ignore_schemes
-
+            if self.use_dummy_cache:
-            self.stats.inc_value('httpcache/store', spider=spider)
+            and self.is_cacheable_response(response)):
-            self.stats.inc_value('httpcache/hits', spider=spider)
+            self.stats.inc_value('httpcache/hit', spider=spider)
-        self.stats.inc_value('httpcache/misses', spider=spider)
+        self.stats.inc_value('httpcache/miss', spider=spider)
-            self.spider = create_spider_for_request(self.crawler.spiders, url)
+            self.spider = create_spider_for_request(self.crawler.spiders, Request(url))
-        key = request.meta.get('download_slotkey')
+        key = request.meta.get('download_slot')
-        self.inactive_slots = {}
+        key = self._get_slot_key(request, spider)
-        return key, self.slots[key]
+        return key
-        request.meta['download_slotkey'] = key
+        request.meta['download_slot'] = key
-        dfd = self.middleware.download(dlfunc, request, spider)
+        self.active.add(request)
-        deferred = defer.Deferred()
+    def _enqueue_request(self, request, spider):
-        downloading = bool(self.downloader.slots)
+        downloading = bool(self.downloader.active)
-from scrapy.resolver import dnscache
+
-            settings.getfloat("DOWNLOAD_DELAY"))
+        if not crawler.settings.getbool('AUTOTHROTTLE_ENABLED'):
-        return 1
+        self.debug = crawler.settings.getbool("AUTOTHROTTLE_DEBUG")
-        spider.max_concurrent_requests = 1
+    def _spider_opened(self, spider):
-            return
+    def _min_delay(self, spider):
-        self._check_concurrency(slot, latency)
+    def _max_delay(self, spider):
-                latency*1000, len(response.body)))
+    def _start_delay(self, spider):
-        return key, downloader.slots.get(key) or downloader.inactive_slots.get(key)
+    def _response_downloaded(self, response, request, spider):
-                slot.concurrency += 1
+    def _get_slot(self, request, spider):
-        new_delay = (slot.delay + latency) / 2.0 if latency < slot.delay else latency
+        # If latency is bigger than old delay, then use latency instead of mean.
-        # by reducing delay instead of increase.
+        # Dont adjust delay if response status != 200 and new delay is smaller
-    return avg, stdev
+        request.meta['download_slotkey'] = key
-        deferred = defer.Deferred().addCallback(_downloaded)
+        deferred = defer.Deferred()
-        # 2. After response arrives,  remove the request from transferring
+        # 2. Notify response_downloaded listeners about the recent download
-from scrapy import log
+
-            return random.uniform(0.5*self.delay, 1.5*self.delay)
+            return random.uniform(0.5 * self.delay, 1.5 * self.delay)
-            (type(spider).__name__, type(spider).__name__))
+                      (type(spider).__name__, type(spider).__name__))
-            "use CONCURRENT_REQUESTS_PER_DOMAIN instead", ScrapyDeprecationWarning)
+        warnings.warn("CONCURRENT_REQUESTS_PER_SPIDER setting is deprecated, "
-            if not slot.active: # remove empty slots
+            if not slot.active:  # remove empty slots
-	return receiver, receiver.func_code, 0
+def function(receiver):
-			
+    """Call receiver with arguments and an appropriate subset of named
-	if hasattr(receiver, '__call__'):
+	if inspect.isclass(receiver) and hasattr(receiver, '__call__'):
-			
+			
-            reactor.callLater(sleep, self._finish, request)
+        if 'latency' in request.args:
-    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0 --set CONCURRENT_REQUESTS=50 -a qps=10 -a sleep=0.3
+    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0 --set CONCURRENT_REQUESTS=50 -a qps=10 -a latency=0.3
-
+    download_delay = None
-    sleep = None 
+    latency = None
-            url += '?sleep={0}'.format(self.sleep)
+        if self.latency is not None:
-
+
-                reactor.callLater(penalty, d.callback, spider)
+            if penalty > 0:
-        slot.lastseen = now
+            slot.lastseen = now
-            log.err('image_downloaded hook failed',
+            log.err('image_downloaded hook failed: %s' % ex,
-import unittest, tempfile, shutil, time
+import time
-from scrapy.contrib.downloadermiddleware.httpcache import FilesystemCacheStorage, HttpCacheMiddleware
+from scrapy.contrib.downloadermiddleware.httpcache import \
-        self.response = Response('http://www.example.com', headers={'Content-Type': 'text/html'}, body='test body', status=202)
+        self.request = Request('http://www.example.com',
-        mw = HttpCacheMiddleware(self._get_settings(**new_settings), self.crawler.stats)
+    @contextmanager
-        return mw
+        try:
-        assert storage.retrieve_response(self.spider, request2) is None
+        with self._storage() as storage:
-        assert storage.retrieve_response(self.spider, self.request)
+        with self._storage(HTTPCACHE_EXPIRATION_SECS=0) as storage:
-        assert 'cached' in response.flags
+        with self._middleware() as mw:
-        assert 'cached' in cached.flags
+        with self._middleware() as mw:
-        assert 'cached' in response.flags
+        with self._middleware(HTTPCACHE_IGNORE_MISSING=True) as mw:
-        assert 'cached' in cached.flags
+        with self._middleware() as mw:
-        assert mw.process_request(req, self.spider) is None
+        with self._middleware() as mw:
-        assert 'cached' in cached.flags
+        with self._middleware() as mw:
-        assert mw.process_request(req, self.spider) is None
+        with self._middleware(HTTPCACHE_IGNORE_SCHEMES=['s3']) as mw:
-        assert mw.process_request(self.request, self.spider) is None
+        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[202]) as mw:
-        assert 'cached' in response.flags
+        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[203]) as mw:
-    skip = False
+else:
-        self.tailf.seek(self.info['tail'][2])
+        os.lseek(self.tailf.fileno(), self.info['tail'][2], os.SEEK_SET)
-    'long_description': 'Scrapy is a high level scraping and web crawling framework for writing spiders to crawl and parse web pages for all kinds of purposes, from information retrieval to monitoring or testing web sites.',
+    'long_description': open('README.rst').read(),
-            settings.getint("DOWNLOAD_DELAY"))
+        return max(settings.getfloat("AUTOTHROTTLE_MIN_DOWNLOAD_DELAY"),
-from twisted.python import failure
+    def __init__(self):
-                        link = Link(url=url)
+                        link = Link(url=url, nofollow=True if dict(attrs).get('rel') == 'nofollow' else False)
-                      clickdata=None, dont_click=False, **kwargs):
+                      clickdata=None, dont_click=False, formxpath=None, **kwargs):
-        form = _get_form(response, formname, formnumber)
+        form = _get_form(response, formname, formnumber, formxpath)
-def _get_form(response, formname, formnumber):
+def _get_form(response, formname, formnumber, formxpath):
-            with open(self.statefn) as f:
+            with open(self.statefn, 'rb') as f:
-from unittest.runner import TextTestRunner
+from unittest import TextTestRunner
-from unittest.case import TestCase
+from unittest import TestCase
-from unittest.runner import TextTestRunner
+from unittest import TextTestRunner
-        self.results = TextTestRunner()._makeResult()
+        self.results = TextTestRunner(verbosity=opts.verbose)._makeResult()
-        self.testcase = create_testcase(method)
+        self.testcase_pre = self.create_testcase(method, 'pre-hook')
-        request.callback = wrapper
+        if hasattr(self, 'pre_process'):
-        request.callback = wrapper
+        if hasattr(self, 'post_process'):
-        pass
+        spider = TestSpider()
-        contracts = self.conman.extract_contracts(TestSpider.returns_request)
+        contracts = self.conman.extract_contracts(spider.returns_request)
-                self.results)
+        request = self.conman.from_method(spider.returns_request, self.results)
-                self.results)
+        request = self.conman.from_method(spider.parse_no_url, self.results)
-                request = self.conman.from_method(bound_method)
+                request = self.conman.from_method(bound_method, self.results)
-from scrapy.exceptions import ContractFail
+
-    def from_method(self, method, fail=False):
+    def from_method(self, method, results):
-                    request = contract.add_pre_hook(request, fail)
+                    request = contract.add_pre_hook(request, results)
-                    request = contract.add_post_hook(request, fail)
+                    request = contract.add_post_hook(request, results)
-        self.method = method
+        self.testcase = create_testcase(method)
-    def add_pre_hook(self, request, fail=False):
+    def add_pre_hook(self, request, results):
-            return list(iterate_spider_output(cb(response)))
+            except AssertionError:
-    def add_post_hook(self, request, fail=False):
+    def add_post_hook(self, request, results):
-            output = list(iterate_spider_output(cb(response)))
+                output = list(iterate_spider_output(cb(response)))
-            return output
+            except AssertionError:
-            (method.im_class.name, method.__name__, self)
+    pass
-        conman = ContractsManager(self.contracts)
+    def setUp(self):
-        contracts = conman.extract_contracts(TestSpider.returns_request)
+        contracts = self.conman.extract_contracts(TestSpider.returns_request)
-        request = conman.from_method(TestSpider.returns_request)
+        request = self.conman.from_method(TestSpider.returns_request,
-        request = conman.from_method(TestSpider.parse_no_url)
+        request = self.conman.from_method(TestSpider.parse_no_url,
-        request = conman.from_method(spider.returns_item, fail=True)
+        request = self.conman.from_method(spider.returns_item, self.results)
-        request = conman.from_method(spider.returns_request, fail=True)
+        request = self.conman.from_method(spider.returns_request, self.results)
-        self.assertRaises(ContractFail, request.callback, response)
+        request = self.conman.from_method(spider.returns_fail, self.results)
-        request = conman.from_method(spider.scrapes_item_ok, fail=True)
+        request = self.conman.from_method(spider.scrapes_item_ok, self.results)
-        self.assertRaises(ContractFail, request.callback, response)
+        request = self.conman.from_method(spider.scrapes_item_fail,
-        interval = settings.getfloat('LOGSTATS_INTERVAL')
+        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')
-
+    # --- backwards compatibility for scrapy.conf.settings singleton ---
-settings = crawler.settings
+import sys
-__version__ = "0.15.1"
+version_info = (0, 17, 0)
-        return "Check contracts for given spider"
+        return "Check spider contracts"
-import sys
+from scrapy.project import crawler
-    settings = get_project_settings()
+import warnings
-            log.msg("Dumping spider stats:\n" + pprint.pformat(self._stats), \
+            log.msg("Dumping Scrapy stats:\n" + pprint.pformat(self._stats), \
-            if output:
+            # some pages are quite small so output is '' and f.extrabuf
-        if not log.started:
+        if not self.configured:
-        self._crawler.configure()
+            self._crawler.configure()
-        return sflo
+    loglevel = _get_log_level(loglevel)
-    if started or not settings.getbool('LOG_ENABLED'):
+    if not settings.getbool('LOG_ENABLED'):
-            log.start_from_settings(self.settings)
+            log.start_from_crawler(self._crawler)
-    def __init__(self, f, level=INFO, encoding='utf-8'):
+    def __init__(self, f, level=INFO, encoding='utf-8', crawler=None):
-    def emit(self, eventDict):
+    def _emit(self, eventDict):
-def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8'):
+def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8', crawler=None):
-        sflo = ScrapyFileLogObserver(file, loglevel, logencoding)
+        sflo = ScrapyFileLogObserver(file, loglevel, logencoding, crawler)
-def start_from_settings(settings):
+def start_from_crawler(crawler):
-        settings['LOG_ENCODING'])
+        settings['LOG_ENCODING'], crawler)
-        self._stats = {}
+        pass
-                depth = response.request.meta['depth'] + 1
+                depth = response.meta['depth'] + 1
-            response.request.meta['depth'] = 0
+        if self.stats and 'depth' not in response.meta:
-        dwld.addCallbacks(_on_success, _on_error)
+        dwld.addCallbacks(_on_success)
-        meta = response.request.meta
+        meta = response.meta
-    try:
+    settings_module_path = os.environ.get(ENVVAR)
-    except ImportError:
+    else:
-    def execute(self, args, check_code=True, settings='missing'):
+    def execute(self, args, check_code=True, settings=None):
-        env['SCRAPY_SETTINGS_MODULE'] = settings
+        if settings is not None:
-        self.assertIsNotNone(request)
+        self.assertNotEqual(request, None)
-        self.assertIsNone(request)
+        self.assertEqual(request, None)
-        p = Popen(['git', 'rev-parse', 'HEAD'], stdout=PIPE)
+        p = Popen(['git', 'describe', '--always'], stdout=PIPE)
-from scrapy.utils import display
+
-        # display.pprint(output)
+
-        parser.add_option("-l", "--list", dest="list", action="store_true", \
+        parser.add_option("-l", "--list", dest="list", action="store_true",
-                settings['SPIDER_CONTRACTS'])
+        contracts = build_component_list(
-from scrapy.utils.misc import get_spec
+from scrapy.utils.python import get_spec
-    def from_method(self, method):
+    def from_method(self, method, fail=False):
-                    request = contract.add_pre_hook(request)
+                    request = contract.add_pre_hook(request, fail)
-                    request = contract.add_post_hook(request)
+                    request = contract.add_post_hook(request, fail)
-    def add_pre_hook(self, request):
+    def add_pre_hook(self, request, fail=False):
-            try: self.pre_process(response)
+            try:
-                print e.format(self.method)
+                if fail:
-    def add_post_hook(self, request):
+    def add_post_hook(self, request, fail=False):
-            try: self.post_process(output)
+            try:
-                print e.format(self.method)
+                if fail:
-    return args, kwargs
+def get_spec(func):
-        self.MAX_CONCURRENCY = settings.getint("AUTOTHROTTLE_MAX_CONCURRENCY", 8)
+        self.MAX_CONCURRENCY = self._max_concurency(settings)
-        self.MIN_DOWNLOAD_DELAY = settings.getint("AUTOTHROTTLE_MIN_DOWNLOAD_DELAY")
+    def _min_download_delay(self, settings):
-            self.crawler.crawl(spider)
+        spname = args[0]
-    # backwards compatibility to support scrapy.conf.settings
+    # --- backwards compatibility for scrapy.conf.settings singleton ---
-def execute(argv=None):
+def execute(argv=None, settings=None):
-    settings = get_project_settings()
+    if settings is None:
-            log.start()
+            log.start_from_settings(self.settings)
-# This module is kept for backwards compatibility.
+# This module is kept for backwards compatibility, so users can import
-settings = get_project_settings()
+        self.logformatter = crawler.logformatter
-                logkws = log.formatter.crawled(request, response, spider)
+                logkws = self.logformatter.crawled(request, response, spider)
-                logkws = log.formatter.dropped(item, ex, response, spider)
+                logkws = self.logformatter.dropped(item, ex, response, spider)
-            logkws = log.formatter.scraped(output, response, spider)
+            logkws = self.logformatter.scraped(output, response, spider)
-    _DEFAULT_ENCODING = settings['DEFAULT_RESPONSE_ENCODING']
+    _DEFAULT_ENCODING = 'ascii'
-    elif isinstance(level_name_or_id, int):
+def _get_log_level(level_name_or_id):
-    if log.defaultObserver: # check twisted log not already started
+def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8'):
-        sflo = ScrapyFileLogObserver(file, loglevel, settings['LOG_ENCODING'])
+        sflo = ScrapyFileLogObserver(file, loglevel, logencoding)
-formatter = load_object(settings['LOG_FORMATTER'])()
+def start_from_settings(settings):
-project settings.
+To select the backend explicitly use the SCRAPY_SELECTORS_BACKEND environment
-from scrapy.conf import settings
+import os
-elif settings['SELECTORS_BACKEND'] == 'libxml2':
+backend = os.environ.get('SCRAPY_SELECTORS_BACKEND')
-    from scrapy.selector.dummysel import *
+elif backend == 'lxml':
-            from scrapy.selector.libxml2sel import *
+        import libxml2
-USER_AGENT = 'Scrapy/0.15 (+http://scrapy.org)'
+USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % __import__('scrapy').__version__
-            self.assertRaises(RuntimeError, sel.__nonzero__, 'a', 'b')
+from scrapy.settings.deprecated import check_deprecated_settings
-def execute(argv=None):
+def execute(argv=None, settings=None):
-    settings = get_project_settings()
+    if settings is None:
-            log.start()
+            log.start_from_settings(self.settings)
-# This module is kept for backwards compatibility.
+# This module is kept for backwards compatibility, so users can import
-    _DEFAULT_ENCODING = settings['DEFAULT_RESPONSE_ENCODING']
+    _DEFAULT_ENCODING = 'ascii'
-from scrapy.conf import settings
+formatter = None
-    elif isinstance(level_name_or_id, int):
+    if isinstance(level_name_or_id, int):
-
+def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8'):
-        sflo = ScrapyFileLogObserver(file, loglevel, settings['LOG_ENCODING'])
+        sflo = ScrapyFileLogObserver(file, loglevel, logencoding)
-formatter = load_object(settings['LOG_FORMATTER'])()
+def start_from_settings(settings):
-USER_AGENT = 'Scrapy/0.15 (+http://scrapy.org)'
+USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % __import__('scrapy').__version__
-            self.stats.inc_value('httpcache/hit', spider=spider)
+            self.stats.inc_value('httpcache/hits', spider=spider)
-        self.stats.inc_value('httpcache/miss', spider=spider)
+        self.stats.inc_value('httpcache/misses', spider=spider)
-        self.cachedir = data_path(settings['HTTPCACHE_DIR'])
+        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)
-        self.dbs = {}
+        self.db = None
-        self.dbs[spider] = self.dbmodule.open(dbpath, 'c')
+        self.db = self.dbmodule.open(dbpath, 'c')
-        self.dbs[spider].close()
+        self.db.close()
-        self.dbs[spider]['%s_time' % key] = str(time())
+        self.db['%s_data' % key] = pickle.dumps(data, protocol=2)
-        db = self.dbs[spider]
+        db = self.db
-HTTPCACHE_STORAGE = 'scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage'
+HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmCacheStorage'
-        return HttpCacheMiddleware(self._get_settings(**new_settings), self.crawler.stats)
+        mw = HttpCacheMiddleware(self._get_settings(**new_settings), self.crawler.stats)
-        mw = HttpCacheMiddleware(self._get_settings(), self.crawler.stats)
+        mw = self._get_middleware()
-        mw = HttpCacheMiddleware(self._get_settings(), self.crawler.stats)
+        mw = self._get_middleware()
-def data_path(path):
+def data_path(path, createdir=False):
-    return path if isabs(path) else join(project_data_dir(), path)
+    if not isabs(path):
-        liverefs = format_live_refs() if self.dumprefs else ""
+        liverefs = format_live_refs()
-    def __init__(self, stats, trackrefs=False):
+    def __init__(self, stats):
-        o = cls(crawler.stats, crawler.settings.getbool('TRACK_REFS'))
+        o = cls(crawler.stats)
-                self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict))
+        for cls, wdict in live_refs.iteritems():
-tracking by enabling the TRACK_REFS setting.
+subclass form object_ref (instead of object).
-from scrapy.utils.project import inside_project
+from scrapy.utils.project import inside_project, get_project_settings
-def _get_commands_dict(inproject):
+def _get_commands_dict(settings, inproject):
-def _print_header(inproject):
+def _print_header(settings, inproject):
-    _print_header(inproject)
+def _print_commands(settings, inproject):
-    cmds = _get_commands_dict(inproject)
+    cmds = _get_commands_dict(settings, inproject)
-    _print_header(inproject)
+def _print_unknown_command(settings, cmdname, inproject):
-    cmds = _get_commands_dict(inproject)
+    cmds = _get_commands_dict(settings, inproject)
-        _print_commands(inproject)
+        _print_commands(settings, inproject)
-        _print_unknown_command(cmdname, inproject)
+        _print_unknown_command(settings, cmdname, inproject)
-    return settings
+# This module is kept for backwards compatibility.
-from os import makedirs, environ
+import cPickle as pickle
-from scrapy.utils.python import is_writable
+from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env
-    scrapy_module = environ.get('SCRAPY_SETTINGS_MODULE')
+    scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')
-        makedirs(d)
+        os.makedirs(d)
-            spider=spider, reason=reason))
+            spider=spider, reason=reason, spider_stats=self.crawler.stats.get_stats()))
-        crawler.signals.connect(o.stats_spider_closing, signal=signals.stats_spider_closing)
+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)
-    def stats_spider_opened(self, spider):
+    def spider_opened(self, spider):
-    def stats_spider_closing(self, spider, reason):
+    def spider_closed(self, spider, reason):
-        self._signals = crawler.signals
+        self._stats = {}
-        return self._stats[spider].get(key, default)
+        return self._stats.get(key, default)
-        return self._stats[spider]
+        return self._stats
-        self._stats[spider][key] = value
+        self._stats[key] = value
-        self._stats[spider] = stats
+        self._stats = stats
-        d = self._stats[spider]
+        d = self._stats
-        d[key] = max(d.setdefault(key, value), value)
+        self._stats[key] = max(self._stats.setdefault(key, value), value)
-        d[key] = min(d.setdefault(key, value), value)
+        self._stats[key] = min(self._stats.setdefault(key, value), value)
-        return [x for x in self._stats.iteritems() if x[0]]
+        self._stats.clear()
-        self._signals.send_catch_log(stats_spider_opened, spider=spider)
+        self._stats = {}
-            log.msg("Dumping spider stats:\n" + pprint.pformat(stats), \
+            log.msg("Dumping spider stats:\n" + pprint.pformat(self._stats), \
-        self._persist_stats(stats, spider=None)
+        self._persist_stats(self._stats, spider)
-    def _persist_stats(self, stats, spider=None):
+    def _persist_stats(self, stats, spider):
-            self.spider_stats[spider.name] = stats
+    def _persist_stats(self, stats, spider):
-            self.stats.inc_value('scheduler/disk_enqueued', spider=self.spider)
+            self.stats.inc_value('scheduler/enqueued/disk', spider=self.spider)
-            self.stats.inc_value('scheduler/memory_enqueued', spider=self.spider)
+            self.stats.inc_value('scheduler/enqueued/memory', spider=self.spider)
-            self.stats.inc_value('scheduler/memory_dequeued', spider=self.spider)
+            self.stats.inc_value('scheduler/dequeued/memory', spider=self.spider)
-                self.stats.inc_value('scheduler/disk_dequeued', spider=self.spider)
+                self.stats.inc_value('scheduler/dequeued/disk', spider=self.spider)
-        if not self._dqpush(request):
+        dqok = self._dqpush(request)
-        return self.mqs.pop() or self._dqpop()
+        request = self.mqs.pop()
-            "end_time": datetime.isoformat(s.end_time)} for s in self.root.launcher.finished
+            "start_time": s.start_time.isoformat(' '),
-            'default + loaded + started')
+            'default + started')
-            'override + loaded + started')
+            'override + started')
-            'override + loaded + started')
+            'override + started')
-    def __init__(self):
+    def __init__(self, settings):
-from scrapy.conf import settings
+from scrapy.settings import default_settings
-        default_log_level = getattr(log, settings['LOG_LEVEL'])
+        default_log_level = getattr(log, default_settings.LOG_LEVEL)
-        self.mail = MailSender()
+        self.mail = MailSender.from_settings(crawler.settings)
-    def __init__(self, stats, recipients):
+    def __init__(self, stats, recipients, mail):
-        crawler.connect(o.stats_spider_closed, signal=signals.stats_spider_closed)
+        mail = MailSender.from_settings(crawler.settings)
-        mail = MailSender()
+    def spider_closed(self, spider):
-        mail.send(self.recipients, "Scrapy stats for: %s" % spider.name, body)
+        return self.mail.send(self.recipients, "Scrapy stats for: %s" % spider.name, body)
-        self.mailfrom = mailfrom or settings['MAIL_FROM']
+    def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',
-            raise NotConfigured("MAIL_HOST and MAIL_FROM settings are required")
+    @classmethod
-    def send(self, to, subject, body, cc=None, attachs=()):
+    def send(self, to, subject, body, cc=None, attachs=(), _callback=None):
-                       cc=cc, attach=attachs, msg=msg)
+        if _callback:
-
+from scrapy.mail import MailSender
-        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body')
+        mailsender = MailSender(debug=True)
-        mailsender = MailSender(debug=True, crawler=self.crawler)
+        mailsender = MailSender(debug=True)
-                       attachs=attachs)
+                       attachs=attachs, _callback=self._catch_mail_sent)
-        finished = [{"id": s.job, "spider": s.spider} for s in self.root.launcher.finished
+        finished = [{"id": s.job, "spider": s.spider,
-            self.pre_process(response)
+            try: self.pre_process(response)
-            self.post_process(output)
+            try: self.post_process(output)
-    pass
+    
-    check_call([sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=f)
+    retry_on_eintr(check_call, [sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=f)
-        func_args.pop(0) # self
+        func_args.pop(0)
-            spider_failure.printTraceback()
+        self.assertEqual(get_func_args(object), [])
-def get_func_args(func):
+def get_func_args(func, stripself=False):
-        func_args.pop(0) # self
+        return get_func_args(func.__init__, True)
-        func_args.pop(0) # self
+        return get_func_args(func.__func__, True)
-        func_args.pop(0) # self
+        if inspect.isroutine(func):
-        o = cls(crawler.settings)
+        if len(get_func_args(cls)) < 1:
-    WeakKeyCache, stringify_dict
+    WeakKeyCache, stringify_dict, get_func_args
-            func_args = []
+        func_args, _, _, _ = inspect.getargspec(func.__call__)
-        if not procfs_supported():
+        try:
-        return get_vmvalue_from_procfs('VmRSS')
+        return self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss * 1024
-        self.handlers = DownloadHandlers()
+        self.handlers = DownloadHandlers(crawler.settings)
-    def __init__(self):
+    def __init__(self, settings):
-                dh = cls()
+                dh = cls(settings)
-        self.httpclientfactory = httpclientfactory
+    def __init__(self, settings):
-        factory = self.httpclientfactory(request)
+        factory = self.HTTPClientFactory(request)
-                        ClientContextFactory())
+                        self.ClientContextFactory())
-    def __init__(self, aws_access_key_id=None, aws_secret_access_key=None, \
+    def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \
-        self._download_http = httpdownloadhandler().download_request
+        self._download_http = httpdownloadhandler(settings).download_request
-        self.download_request = FileDownloadHandler().download_request
+        self.download_request = FileDownloadHandler(Settings()).download_request
-        self.download_request = HttpDownloadHandler().download_request
+        self.download_request = HttpDownloadHandler(Settings()).download_request
-        self.download_request = HttpDownloadHandler().download_request
+        self.download_request = HttpDownloadHandler(Settings()).download_request
-        s3reqh = S3DownloadHandler(self.AWS_ACCESS_KEY_ID, \
+        s3reqh = S3DownloadHandler(Settings(), self.AWS_ACCESS_KEY_ID, \
-        log.msg(format=fmt, project=self.project, spider=self.spider,
+    def log(self, action):
-                "a callback to use for parsing" % self.spider.name, log.ERROR)
+            log.msg(format='No CrawlSpider rules found in spider %(spider)r, '
-                log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)
+                log.msg(format='Unable to find spider: %(spider)s',
-                log.msg('Unable to find spider for: %s' % request, log.ERROR)
+                log.msg(format='Unable to find spider for: %(url)s',
-                spider=self.spider)
+            log.msg(format='No response downloaded for: %(request)s',
-                            (cb, self.spider.name), level=log.ERROR)
+                    log.msg(format='Cannot find callback %(callback)r in spider: %(spider)s',
-                    spider=spider, level=log.DEBUG)
+            log.msg(format="Redirecting (%(reason)s) to %(redirected)s from %(request)s",
-                    spider=spider, level=log.DEBUG)
+            log.msg(format="Discarding %(request)s: max redirections reached",
-                    spider=spider, level=log.DEBUG)
+            log.msg(format="Retrying %(request)s (failed %(retries)d times): %(reason)s",
-
+            log.msg(format="Gave up retrying %(request)s (failed %(retries)d times): %(reason)s",
-            log.msg("Forbidden by robots.txt: %s" % request, log.DEBUG)
+            log.msg(format="Forbidden by robots.txt: %(request)s",
-            log.msg("Memory usage exceeded %dM. Shutting down Scrapy..." % mem, level=log.ERROR)
+            log.msg(format="Memory usage exceeded %(memusage)dM. Shutting down Scrapy...",
-            log.msg("Memory usage reached %dM" % mem, level=log.WARNING)
+            log.msg(format="Memory usage reached %(memusage)dM",
-                    % (response.status, request, referer), level=log.WARNING, spider=info.spider)
+            log.msg(format='Image (code: %(status)s): Error downloading image from %(request)s referred in <%(referer)s>',
-                    % (request, referer), level=log.WARNING, spider=info.spider)
+            log.msg(format='Image (empty-content): Empty image from %(request)s referred in <%(referer)s>: no-content',
-        log.msg(msg, level=log.DEBUG, spider=info.spider)
+        log.msg(format='Image (%(status)s): Downloaded image from %(request)s referred in <%(referer)s>',
-            log.msg(str(ex), level=log.WARNING, spider=info.spider)
+            log.err('image_downloaded hook failed',
-            log.msg(msg, level=log.WARNING, spider=info.spider)
+            log.msg(format='Image (unknown-error): Error downloading '
-                    (self.MEDIA_NAME, request.url, referer), level=log.DEBUG, spider=info.spider)
+            log.msg(format='Image (uptodate): Downloaded %(medianame)s from %(request)s referred in <%(referer)s>',
-                        level=log.DEBUG, spider=spider)
+                    log.msg(format="Ignoring link (depth > %(maxdepth)d): %(requrl)s ",
-                            level=log.DEBUG, spider=spider)
+                        log.msg(format="Filtered offsite request to %(domain)r: %(request)s",
-                    level=log.DEBUG, spider=spider)
+                log.msg(format="Ignoring link (url length > %(maxlength)d): %(url)s ",
-                log.msg("Ignoring invalid sitemap: %s" % response, log.WARNING)
+                log.msg(format="Ignoring invalid sitemap: %(response)s",
-                        fmt, log.DEBUG, spider=spider)
+                log.msg(format='Decompressed response with format: %(responsefmt)s',
-                    level=log.DEBUG, spider=spider)
+                logkws = log.formatter.crawled(request, response, spider)
-        log.msg("Closing spider (%s)" % reason, spider=spider)
+        log.msg(format="Closing spider (%(reason)s)", reason=reason, spider=spider)
-        dfd.addBoth(lambda _: log.msg("Spider closed (%s)" % reason, spider=spider))
+        dfd.addBoth(lambda _: log.msg(format="Spider closed (%(reason)s)", reason=reason, spider=spider))
-                    (request, str(e)), level=log.ERROR, spider=self.spider)
+                log.msg(format="Unable to serialize request: %(request)s - reason: %(reason)s",
-                spider=self.spider)
+            log.msg(format="Resuming crawl (%(queuesize)d requests scheduled)",
-                (type(output).__name__, request), log.ERROR, spider=spider)
+            typename = type(output).__name__
-                log.msg("Error downloading %s: %s" % (request, errmsg), log.ERROR, spider=spider)
+                log.msg(format='Error downloading %(request)s: %(errmsg)s',
-                    level=log.WARNING, spider=spider)
+                logkws = log.formatter.dropped(item, ex, response, spider)
-                log.err(output, 'Error processing %s' % item, spider=spider)
+                log.err(output, 'Error processing %(item)s', item=item, spider=spider)
-                log.DEBUG, spider=spider)
+            logkws = log.formatter.scraped(output, response, spider)
-            "unclean shutdown" % signame, level=log.INFO)
+        log.msg(format="Received %(signame)s, shutting down gracefully. Send again to force ",
-            level=log.INFO)
+        log.msg(format='Received %(signame)s twice, forcing unclean shutdown',
-    message = ev.get('message')
+
-    ev['message'] = message
+        ev['message'] = message
-    ev['why'] = why
+        ev['why'] = why
-def msg(message, level=INFO, **kw):
+def msg(message=None, _level=INFO, **kw):
-    log.msg(message, **kw)
+    if message is None:
-    kw.setdefault('system', 'scrapy')
+    kw.setdefault('system', 'scrapy')
-            request, referer, flags)
+        return {
-        return u"Scraped from %s%s%s" % (src, os.linesep, item)
+        return {
-        return u"Dropped: %s%s%s" % (exception, os.linesep, item)
+        return {
-                (to, cc, subject, len(attachs)), level=log.DEBUG)
+            log.msg(format='Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s Subject="%(mailsubject)s" Attachs=%(mailattachs)d',
-            (to, cc, subject, nattachs))
+        log.msg(format='Mail sent OK: To=%(mailto)s Cc=%(mailcc)s '
-            (to, cc, subject, nattachs, errstr), level=log.ERROR)
+        log.msg(format='Unable to send mail: To=%(mailto)s Cc=%(mailcc)s '
-                    log.msg("Disabled %s: %s" % (clsname, e.args[0]), log.WARNING)
+                    log.msg(format="Disabled %(clsname)s: %(eargs)s",
-            level=log.DEBUG)
+        log.msg(format="Enabled %(componentname)ss: %(enabledlist)s", level=log.DEBUG,
-        log.msg("Telnet console listening on %s:%d" % (h.host, h.port), log.DEBUG)
+        log.msg(format="Telnet console listening on %(host)s:%(port)d",
-import sys
+import sys
-            cwd=self.cwd, env=self.env, **kwargs)
+        p = subprocess.Popen(args, cwd=self.cwd, env=self.env,
-        self.assert_("[myspider] INFO: Spider closed (finished)" in log)
+        self.assert_("[myspider] DEBUG: It Works!" in log, log)
-        self.assertEqual(self.formatter.crawled(req, res, self.spider),
+        logkws = self.formatter.crawled(req, res, self.spider)
-        self.assertEqual(self.formatter.crawled(req, res, self.spider),
+        logkws = self.formatter.crawled(req, res, self.spider)
-        lines = self.formatter.dropped(item, exception, response, self.spider).splitlines()
+        logkws = self.formatter.dropped(item, exception, response, self.spider)
-        lines = self.formatter.scraped(item, response, self.spider).splitlines()
+        logkws = self.formatter.scraped(item, response, self.spider)
-            log.msg("ignoring row %d (length: %d, should be: %d)" % (csv_r.line_num, len(row), len(headers)), log.WARNING)
+            log.msg(format="ignoring row %(csvlnum)d (length: %(csvrow)d, should be: %(csvheader)d)",
-                    receiver, log.ERROR, spider=spider)
+                log.msg(format="Cannot return deferreds from signal handler: %(receiver)s",
-            (request, ", ".join(snames)), log.ERROR)
+        log.msg(format='More than one spider can handle: %(request)s - %(snames)s',
-        log.msg('Unable to find spider that handles: %s' % request, log.ERROR)
+        log.msg(format='Unable to find spider that handles: %(request)s',
-        log.msg("Web service listening on %s:%d" % (h.host, h.port), log.DEBUG)
+        log.msg(format='Web service listening on %(host)s:%(port)d',
-    log.msg("Scrapyd web console available at http://%s:%s/" % (bind_address, http_port))
+    log.msg(format="Scrapyd web console available at http://%(bind_address)s:%(http_port)s/",
-            self.max_proc, self.runner), system="Launcher")
+        log.msg(format='%(parent)s started: max_proc=%(max_proc)r, runner=%(runner)r',
-        log.msg(msg, system="Launcher")
+        fmt = 'project=%(project)r spider=%(spider)r job=%(job)r pid=%(pid)r log=%(log)r items=%(items)r'
-    def open_spider(self, spider, start_requests=None, close_if_idle=True):
+    def open_spider(self, spider, start_requests=(), close_if_idle=True):
-        slot = Slot(start_requests or (), close_if_idle, nextcall, scheduler)
+        start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)
-                log.msg('Cannot find callback %r in spider: %s' % (callback, self.spider.name))
+            if not callable(cb):
-from scrapy.http import Response, XmlResponse, Request
+from scrapy.http import Response, XmlResponse
-            self.assertEqual(new.body, self.uncompressed_body, fmt)
+            assert_samelines(self, new.body, self.uncompressed_body, fmt)
-        self.assertEqual(new.body, rsp.body)
+        assert_samelines(self, new.body, rsp.body)
-                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\nbar'},
+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},
-                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\nbar'},
+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},
-                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\nbar'},
+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},
-                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\nbar'},
+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},
-            self.max_proc = cpu_count() * config.getint('max_proc_per_cpu', 4)
+        self.max_proc = self._get_max_proc(config)
-        self.assertEqual(sorted(get_spider_list('mybot')), ['spider1', 'spider2'])
+        spiders = get_spider_list('mybot', pythonpath=get_pythonpath())
-def get_spider_list(project, runner=None):
+def get_spider_list(project, runner=None, pythonpath=None):
-            self.env['PYTHONPATH'] = os.environ['PYTHONPATH']
+        self.env['PYTHONPATH'] = get_pythonpath()
-
+from scrapy.utils.test import get_pythonpath
-            self.env['PYTHONPATH'] = os.environ['PYTHONPATH']
+        self.env['PYTHONPATH'] = get_pythonpath()
-import os
+import os, sys
-
+    from scrapy.crawler import Crawler
-        self.conman = ContractsManager()
+    def add_options(self, parser):
-            self.conman.register(concls)
+    def run(self, args, opts):
-        # schedule requests
+        # contract requests
-            self.crawler.crawl(spider, requests)
+
-        self.crawler.start()
+        if opts.list:
-    registered = {}
+    contracts = {}
-        self.registered[contract.name] = contract
+    def __init__(self, contracts):
-                args = re.split(r'\s*\,\s*', args)
+                args = re.split(r'\s+', args)
-                contracts.append(self.registered[name](method, *args))
+                contracts.append(self.contracts[name](method, *args))
-            args['callback'] = method
+            args, kwargs = get_spec(Request.__init__)
-                args = contract.adjust_request_args(args)
+                kwargs = contract.adjust_request_args(kwargs)
-                request = contract.prepare_request(request)
+            args.remove('self')
-            return request
+                return request
-    def prepare_request(self, request):
+    def add_pre_hook(self, request):
-
+
-        @returns requests, 1+
+
-        # validate input
+        assert len(self.args) in [1, 2, 3]
-            self.modifier = None
+        try:
-            assertion = (occurences == self.num)
+        assertion = (self.min_bound <= occurences <= self.max_bound)
-                (occurences, self.obj_name, self.raw_num))
+                (occurences, self.obj_name, expected))
-        @scrapes page_name, page_body
+        @scrapes page_name page_body
-                        raise ContractFail('%r field is missing' % arg)
+                        raise ContractFail("'%s' field is missing" % arg)
-    """Error in constructing contracts for a method"""
+class ContractFail(AssertionError):
-]
+SPIDER_CONTRACTS = {}
-    """Returns (args, kwargs) touple for a function
+    """Returns (args, kwargs) tuple for a function
-        out = p.stdout.read()
+        out = retry_on_eintr(p.stdout.read)
-        out = p.stdout.read()
+        out = retry_on_eintr(p.stdout.read)
-            'ProjectName': string_camelcase(settings.get('BOT_NAME')),
+            'project_name': self.settings.get('BOT_NAME'),
-        spiders_module = __import__(settings['NEWSPIDER_MODULE'], {}, {}, [''])
+        spiders_module = __import__(self.settings['NEWSPIDER_MODULE'], {}, {}, [''])
-from scrapy.item import Field, Item, ItemMeta
+import warnings
-        return model
+from scrapy.contrib.djangoitem import DjangoItem
-from scrapy.contrib_exp.djangoitem import DjangoItem, Field
+from scrapy.contrib.djangoitem import DjangoItem, Field
-        o = cls()
+        o = cls(crawler.settings)
-        dispatcher.connect(self.response_downloaded, signals.response_downloaded)
+        self.crawler.signals.connect(self.item_scraped, signals.item_scraped)
-from scrapy.contracts import Contract
+from scrapy.contracts import ContractsManager
-        for key, value in vars(type(spider)).iteritems():
+        for key, value in vars(type(spider)).items():
-                request.callback = wrapper
+                request = self.conman.from_method(bound_method)
-                requests.append(request)
+                if request:
-from .default import *
+import re
-        pass
+from scrapy.http import Request
-from .base import Contract
+from . import Contract
-        return request.replace(url=self.args[0])
+    def adjust_request_args(self, args):
-    name = 'returns_request'
+        if not assertion:
-                    assert arg in x, '%r field is missing' % arg
+                    if not arg in x:
-        i1 = TestItem(name=u'Joseph\xa3', age='22')
+        i1 = TestItem(name=u'Joseph', age='22')
-        self.encoder = json.JSONEncoder(**kwargs)
+        self.encoder = ScrapyJSONEncoder(**kwargs)
-        self.encoder = json.JSONEncoder(**kwargs)
+        self.encoder = ScrapyJSONEncoder(**kwargs)
-            dispatcher.connect(self.page_count, signal=signals.response_received)
+            crawler.signals.connect(self.page_count, signal=signals.response_received)
-            dispatcher.connect(self.spider_opened, signal=signals.spider_opened)
+            crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)
-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)
+            crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)
-        dispatcher.connect(self.item_dropped, signal=signals.item_dropped)
+    def __init__(self, stats):
-        stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)
+        self.stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)
-        stats.set_value('finish_reason', reason, spider=spider)
+        self.stats.set_value('finish_time', datetime.datetime.utcnow(), spider=spider)
-        stats.inc_value('item_scraped_count', spider=spider)
+        self.stats.inc_value('item_scraped_count', spider=spider)
-        stats.inc_value('item_dropped_reasons_count/%s' % reason, spider=spider)
+        self.stats.inc_value('item_dropped_count', spider=spider)
-    def __init__(self, settings):
+    def __init__(self, settings, stats):
-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)
+        self.stats = stats
-        return cls(crawler.settings)
+        o = cls(crawler.settings, crawler.stats)
-            stats.inc_value('httpcache/hit', spider=spider)
+            self.stats.inc_value('httpcache/hit', spider=spider)
-        stats.inc_value('httpcache/miss', spider=spider)
+        self.stats.inc_value('httpcache/miss', spider=spider)
-            stats.inc_value('httpcache/store', spider=spider)
+            self.stats.inc_value('httpcache/store', spider=spider)
-        dispatcher.connect(self.spider_closed, signals.spider_closed)
+        crawler.signals.connect(self.spider_opened, signals.spider_opened)
-from scrapy.stats import stats
+    def __init__(self, stats):
-        return cls()
+        return cls(crawler.stats)
-        stats.inc_value('downloader/request_method_count/%s' % request.method, spider=spider)
+        self.stats.inc_value('downloader/request_count', spider=spider)
-        stats.inc_value('downloader/request_bytes', reqlen, spider=spider)
+        self.stats.inc_value('downloader/request_bytes', reqlen, spider=spider)
-        stats.inc_value('downloader/response_status_count/%s' % response.status, spider=spider)
+        self.stats.inc_value('downloader/response_count', spider=spider)
-        stats.inc_value('downloader/response_bytes', reslen, spider=spider)
+        self.stats.inc_value('downloader/response_bytes', reslen, spider=spider)
-        stats.inc_value('downloader/exception_type_count/%s' % ex_class, spider=spider)
+        self.stats.inc_value('downloader/exception_count', spider=spider)
-        dispatcher.connect(self.item_scraped, signals.item_scraped)
+
-        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)
+
-    def __init__(self, trackrefs=False):
+    def __init__(self, stats, trackrefs=False):
-        return cls(crawler.settings.getbool('TRACK_REFS'))
+        o = cls(crawler.stats, crawler.settings.getbool('TRACK_REFS'))
-            stats.set_value('memdebug/libxml2_leaked_bytes', self.libxml2.debugMemory(1))
+            self.stats.set_value('memdebug/libxml2_leaked_bytes', self.libxml2.debugMemory(1))
-        stats.set_value('memdebug/gc_garbage_count', len(gc.garbage))
+        self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage))
-                stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict))
+                self.stats.set_value('memdebug/live_refs/%s' % cls.__name__, len(wdict))
-from scrapy import log
+from scrapy import signals, log
-        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)
+        crawler.signals.connect(self.engine_started, signal=signals.engine_started)
-        stats.set_value('memusage/startup', self.get_virtual_size())
+        self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())
-        stats.max_value('memusage/max', self.get_virtual_size())
+        self.crawler.stats.max_value('memusage/max', self.get_virtual_size())
-            stats.set_value('memusage/limit_reached', 1)
+            self.crawler.stats.set_value('memusage/limit_reached', 1)
-                stats.set_value('memusage/limit_notified', 1)
+                self.crawler.stats.set_value('memusage/limit_notified', 1)
-            stats.set_value('memusage/warning_reached', 1)
+            self.crawler.stats.set_value('memusage/warning_reached', 1)
-                stats.set_value('memusage/warning_notified', 1)
+                self.crawler.stats.set_value('memusage/warning_notified', 1)
-        stats.inc_value('image_status_count/%s' % status, spider=spider)
+        spider.crawler.stats.inc_value('image_count', spider=spider)
-    def from_settings(cls, settings):
+    def from_crawler(cls, crawler):
-        return cls(maxdepth, stats, verbose, prio)
+        return cls(maxdepth, crawler.stats, verbose, prio)
-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)
+
-        dispatcher.connect(obj.spider_opened, signal=signals.spider_opened)
+        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)
-    def __init__(self, recipients):
+    def __init__(self, stats, recipients):
-
+        if not recipients:
-        body += "\n".join("%-50s : %s" % i for i in stats.get_stats().items())
+        body += "\n".join("%-50s : %s" % i for i in self.stats.get_stats().items())
-        dispatcher.connect(self.response_received, signal=signals.response_received)
+        crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)
-        JsonRpcResource.__init__(self, crawler, stats)
+        JsonRpcResource.__init__(self, crawler, crawler.stats)
-from scrapy.utils.signal import send_catch_log
+        self.signals = crawler.signals
-            send_catch_log(signal=signals.response_downloaded, \
+            self.signals.send_catch_log(signal=signals.response_downloaded, \
-from scrapy.utils.signal import send_catch_log, send_catch_log_deferred
+        self.crawler = crawler
-        yield send_catch_log_deferred(signal=signals.engine_started)
+        yield self.signals.send_catch_log_deferred(signal=signals.engine_started)
-                send_catch_log(signal=signals.response_received, \
+                self.signals.send_catch_log(signal=signals.response_received, \
-        scheduler = self.scheduler_cls.from_settings(self.settings)
+        scheduler = self.scheduler_cls.from_crawler(self.crawler)
-        yield send_catch_log_deferred(signals.spider_opened, spider=spider)
+        self.crawler.stats.open_spider(spider)
-        res = send_catch_log(signal=signals.spider_idle, \
+        res = self.signals.send_catch_log(signal=signals.spider_idle, \
-        dfd.addBoth(lambda _: send_catch_log_deferred(signal=signals.spider_closed, \
+        dfd.addBoth(lambda _: self.signals.send_catch_log_deferred(signal=signals.spider_closed, \
-        dfd.addBoth(lambda _: stats.close_spider(spider, reason=reason))
+        dfd.addBoth(lambda _: self.crawler.stats.close_spider(spider, reason=reason))
-        yield stats.engine_stopped()
+        yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)
-    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False):
+    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False, stats=None):
-    def from_settings(cls, settings):
+    def from_crawler(cls, crawler):
-        return cls(dupefilter, job_dir(settings), dqclass, mqclass, logunser)
+        return cls(dupefilter, job_dir(settings), dqclass, mqclass, logunser, crawler.stats)
-            stats.inc_value('scheduler/disk_enqueued', spider=self.spider)
+            if self.stats:
-        stats.inc_value('scheduler/memory_enqueued', spider=self.spider)
+        if self.stats:
-from scrapy.stats import stats
+        self.signals = crawler.signals
-        send_catch_log(signal=signals.spider_error, failure=_failure, response=response, \
+        self.signals.send_catch_log(signal=signals.spider_error, failure=_failure, response=response, \
-        stats.inc_value("spider_exceptions/%s" % _failure.value.__class__.__name__, \
+        self.crawler.stats.inc_value("spider_exceptions/%s" % _failure.value.__class__.__name__, \
-            send_catch_log(signal=signals.request_received, request=output, \
+            self.signals.send_catch_log(signal=signals.request_received, request=output, \
-                return send_catch_log_deferred(signal=signals.item_dropped, \
+                return self.signals.send_catch_log_deferred(signal=signals.item_dropped, \
-            return send_catch_log_deferred(signal=signals.item_scraped, \
+            return self.signals.send_catch_log_deferred(signal=signals.item_scraped, \
-from scrapy.xlib.pydispatch import dispatcher
+from scrapy.signalmanager import SignalManager
-        dispatcher.connect(self.stop, signals.engine_stopped)
+        self.signals.connect(self.stop, signals.engine_stopped)
-            smtpport=None, debug=False):
+            smtpport=None, debug=False, crawler=None):
-        send_catch_log(signal=mail_sent, to=to, subject=subject, body=body,
+        if self.signals:
-        return cls.from_settings(crawler.settings)
+        sm = cls.from_settings(crawler.settings)
-from scrapy.utils.misc import load_object
+from scrapy.project import crawler
-    stats = DummyStatsCollector()
+import warnings
-        self._dump = settings.getbool('STATS_DUMP')
+    def __init__(self, crawler):
-        send_catch_log(stats_spider_opened, spider=spider)
+        self._signals.send_catch_log(stats_spider_opened, spider=spider)
-        send_catch_log(stats_spider_closing, spider=spider, reason=reason)
+        self._signals.send_catch_log(stats_spider_closing, spider=spider, reason=reason)
-        send_catch_log(stats_spider_closed, spider=spider, reason=reason, \
+        self._signals.send_catch_log(stats_spider_closed, spider=spider, reason=reason, \
-        super(MemoryStatsCollector, self).__init__()
+    def __init__(self, crawler):
-        dispatcher.connect(self.stop_listening, signals.engine_stopped)
+        self.crawler.signals.connect(self.start_listening, signals.engine_started)
-            'stats': stats,
+            'stats': self.crawler.stats,
-        send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)
+        self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)
-from scrapy.stats import stats
+from scrapy.utils.test import get_crawler
-        stats.open_spider(self.spider)
+        self.crawler.stats.open_spider(self.spider)
-        stats.close_spider(self.spider, '')
+        self.crawler.stats.close_spider(self.spider, '')
-        return HttpCacheMiddleware(self._get_settings(**new_settings))
+        return HttpCacheMiddleware(self._get_settings(**new_settings), self.crawler.stats)
-        mw = HttpCacheMiddleware(self._get_settings())
+        mw = HttpCacheMiddleware(self._get_settings(), self.crawler.stats)
-        mw = HttpCacheMiddleware(self._get_settings())
+        mw = HttpCacheMiddleware(self._get_settings(), self.crawler.stats)
-from scrapy.stats import stats
+from scrapy.utils.test import get_crawler
-        self.mw = DownloaderStats()
+        self.mw = DownloaderStats(self.crawler.stats)
-        stats.open_spider(self.spider)
+        self.crawler.stats.open_spider(self.spider)
-        self.assertEqual(stats.get_value('downloader/request_count', \
+        self.assertEqual(self.crawler.stats.get_value('downloader/request_count', \
-        self.assertEqual(stats.get_value('downloader/response_count', \
+        self.assertEqual(self.crawler.stats.get_value('downloader/response_count', \
-        self.assertEqual(stats.get_value('downloader/exception_count', \
+        self.assertEqual(self.crawler.stats.get_value('downloader/exception_count', \
-        stats.close_spider(self.spider, '')
+        self.crawler.stats.close_spider(self.spider, '')
-
+from scrapy.utils.test import get_crawler
-        dispatcher.connect(self._catch_mail_sent, signal=mail_sent)
+        self.crawler = get_crawler()
-        dispatcher.disconnect(self._catch_mail_sent, signal=mail_sent)
+        self.crawler.signals.disconnect(self._catch_mail_sent, signal=mail_sent)
-        mailsender = MailSender(debug=True)
+        mailsender = MailSender(debug=True, crawler=self.crawler)
-        mailsender = MailSender(debug=True)
+        mailsender = MailSender(debug=True, crawler=self.crawler)
-        self.stats = StatsCollector()
+        self.stats = StatsCollector(get_crawler())
-from scrapy.xlib.pydispatch import dispatcher
+from scrapy.utils.test import get_crawler
-        stats = StatsCollector()
+        stats = StatsCollector(self.crawler)
-        stats = DummyStatsCollector()
+        stats = DummyStatsCollector(self.crawler)
-        dispatcher.connect(spider_closed, signal=stats_spider_closed)
+        self.crawler.signals.connect(spider_opened, signal=stats_spider_opened)
-        stats = StatsCollector()
+        stats = StatsCollector(self.crawler)
-        dispatcher.disconnect(spider_closed, signal=stats_spider_closed)
+        self.crawler.signals.disconnect(spider_opened, signal=stats_spider_opened)
-        dispatcher.connect(self.stop_listening, signals.engine_stopped)
+        crawler.signals.connect(self.start_listening, signals.engine_started)
-        self.errorcount = settings.getint('CLOSESPIDER_ERRORCOUNT')
+        self.timeout = crawler.settings.getint('CLOSESPIDER_TIMEOUT')
-            raise NotConfigured
+    def __init__(self, debug=False):
-    def __init__(self, settings=conf.settings):
+    def __init__(self, settings):
-        return spider.settings.get('DEFAULT_REQUEST_HEADERS').items()
+        return self._settings.get('DEFAULT_REQUEST_HEADERS').items()
-    def __init__(self):
+    def __init__(self, timeout=180):
-        return spider.settings.getint('DOWNLOAD_TIMEOUT')
+        return self._timeout
-    def __init__(self, settings=conf.settings):
+    def __init__(self, settings):
-    def __init__(self, settings=conf.settings):
+    def __init__(self, settings):
-    def __init__(self):
+    def __init__(self, settings):
-    def __init__(self):
+    def __init__(self, settings):
-        if not settings.getbool('DOWNLOADER_STATS'):
+    @classmethod
-    def __init__(self):
+    def __init__(self, user_agent='Scrapy'):
-        return spider.settings['USER_AGENT']
+        return self.user_agent
-from scrapy.conf import settings
+        from scrapy.conf import settings
-    def __init__(self):
+    def __init__(self, settings):
-        conf.update(settings[setting_prefix])
+        conf = dict(self.settings['%s_BASE' % setting_prefix])
-    def __init__(self, settings=conf.settings):
+    def __init__(self, settings):
-            raise NotConfigured
+    def __init__(self, interval=60.0):
-    def __init__(self):
+    def __init__(self, trackrefs=False):
-
+        self.trackrefs = trackrefs
-        if settings.getbool('TRACK_REFS'):
+        if self.trackrefs:
-        if follow and settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):
+        if follow and self._follow_links:
-                
+
-        self.recipients = settings.getlist("STATSMAILER_RCPTS")
+    def __init__(self, recipients):
-        
+
-        return self._settings
+        return self.crawler.settings
-        return defaults, spider, DefaultHeadersMiddleware()
+        return defaults, spider, DefaultHeadersMiddleware.from_crawler(crawler)
-        return request, spider, DownloadTimeoutMiddleware()
+        return request, spider, DownloadTimeoutMiddleware.from_crawler(crawler)
-        spider.DOWNLOAD_TIMEOUT = 2
+        spider.download_timeout = 2
-        spider.DOWNLOAD_TIMEOUT = 2
+        spider.download_timeout = 2
-        self.mw = RedirectMiddleware()
+        self.mw = RedirectMiddleware.from_crawler(crawler)
-        self.mw = RetryMiddleware()
+        self.mw = RetryMiddleware.from_crawler(crawler)
-        return spider, UserAgentMiddleware()
+        return spider, UserAgentMiddleware.from_crawler(crawler)
-        spider.USER_AGENT = None
+        spider.user_agent = None
-        spider.USER_AGENT = 'spider_useragent'
+        spider.user_agent = 'spider_useragent'
-        spider.USER_AGENT = 'spider_useragent'
+        spider.user_agent = 'spider_useragent'
-        spider.USER_AGENT = None
+        spider.user_agent = None
-from scrapy.settings import Settings, SpiderSettings
+from scrapy.settings import Settings
-        self.assertEqual(settings.getint('DOWNLOAD_TIMEOUT'), 15)
+from functools import wraps
-        self.q.appendleft(obj)
+        self.push = self.q.append
-            return self.q.pop()
+        q = self.q
-        self.q.append(obj)
+    def pop(self):
-    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar', 'ppt',
+    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar',
-    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar',
+    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar', 'ppt',
-    name = models.CharField(max_length=255)
+    name = models.CharField(max_length=255, default='Robot')
-        modelargs = dict((f, self.get(f, None)) for f in self._model_fields)
+        modelargs = dict((k, self.get(k)) for k in self._values
-            warnings.warn("Cannot import scrapy settings module %s" % scrapy_module)
+        except ImportError as exc:
-        return cookies
+        if isinstance(request.cookies, dict):
-        self.putChild('listjobs.json', webservice.ListJobs(self))
+        services = config.items('services', ())
-                "http://doc.scrapy.org/topics/telnetconsole.html", # see #284
+                "http://doc.scrapy.org/en/latest/topics/telnetconsole.html",
-<p>For more information about the API, see the <a href="http://doc.scrapy.org/topics/scrapyd.html">Scrapyd documentation</a></p>
+<p>For more information about the API, see the <a href="http://doc.scrapy.org/en/latest/topics/scrapyd.html">Scrapyd documentation</a></p>
-                log.msg('Cannot find callback %r in spider: %s' % (callback, spider.name))
+                log.msg('Cannot find callback %r in spider: %s' % (callback, self.spider.name))
-    launcher = Launcher(config, app)
+    laupath = config.get('launcher', 'scrapyd.launcher.Launcher')
-from urlparse import urlparse, uses_netloc, uses_query
+from urlparse import urlparse
-# workaround for http://bugs.python.org/issue7904
+# workaround for http://bugs.python.org/issue7904 - Python < 2.7
-    requests = []
+    items = {}
-        #     help="print each depth level one by one")
+        parser.add_option("-v", "--verbose", dest="verbose", action="store_true", \
-
+            depth = response.meta['_depth']
-            self.items += items
+            self.add_items(depth, items)
-                self.requests += requests
+    spider = None
-
+    def run_callback(self, response, cb):
-            for rule in spider.rules:
+    def get_callback_from_rules(self, response):
-                "a callback to use for parsing" % spider.name, log.ERROR)
+                "a callback to use for parsing" % self.spider.name, log.ERROR)
-    def print_results(self, items, requests, cb_name, opts):
+    def print_results(self, opts):
-            display.pprint([dict(x) for x in items], colorize=not opts.nocolour)
+            print "# Scraped Items ", "-"*60
-            display.pprint(requests, colorize=not opts.nocolour)
+            print "# Requests ", "-"*65
-    def get_spider(self, request, opts):
+    def set_spider(self, url, opts):
-                return self.crawler.spiders.create(opts.spider)
+                self.spider = self.crawler.spiders.create(opts.spider)
-        self.crawler.crawl(spider, [request])
+            self.spider = create_spider_for_request(self.crawler.spiders, url)
-        if not responses:
+
-        return responses[0], spider
+                spider=self.spider)
-        self.print_results(items, requests, callback, opts)
+        else:
-        self.crawler.start()
+        shell = Shell(self.crawler, update_vars=self.update_vars, code=opts.code)
-    def __init__(self, crawler, update_vars=None, inthread=False, code=None):
+    def __init__(self, crawler, update_vars=None, code=None):
-        self.inthread = inthread
+        self.inthread = not threadable.isInIOThread()
-    def start(self, *a, **kw):
+    def start(self, url=None, request=None, response=None, spider=None):
-        self.itemsfile = env['SCRAPY_FEED_URI']
+        self.logfile = env.get('SCRAPY_LOG_FILE')
-        node = self.libxml2.htmlParseDoc(html, 'utf-8')
+        node = libxml2.htmlParseDoc(html, 'utf-8')
-import libxml2
+from scrapy import optional_features
-                     libxml2.XML_PARSE_NOWARNING
+if 'libxml2' in optional_features:
-                      libxml2.HTML_PARSE_NOWARNING
+    html_parser_options = libxml2.HTML_PARSE_RECOVER + \
-import libxml2
+from scrapy import optional_features
-        skip = str(e)
+    skip = 'libxml2' not in optional_features
-import unittest
+from twisted.trial import unittest
-                import lxml.etree2
+                import lxml.etree
-    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.1', 'lxml', 'pyOpenSSL']
+    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.2', 'lxml', 'pyOpenSSL']
-        env['SCRAPY_FEED_URI'] = self._get_file(message, self.items_dir, 'jl')
+        if self.logs_dir:
-                encoding=unicode)
+                encoding=unicode, with_tail=False)
-        # FIXME: can't get this to work with lxml backend
+    @libxml2debug
-    test_select_on_text_nodes.skip = True
+    test_nested_select_on_text_nodes.skip = True
-        item['images'] = [x for ok, x in results if ok]
+        if 'images' in item.fields:
-                                          "http://www.example.com")
+        self.assertEqual(canonicalize_url("http://www.example.com/"),
-                                          "http://www.example.com")
+        self.assertEqual(canonicalize_url("http://www.EXAMPLE.com/"),
-    path = safe_url_string(urllib.unquote(path))
+    path = safe_url_string(urllib.unquote(path)) or '/'
-            return JsonRpcResource(newtarget)
+            return JsonRpcResource(self.crawler, newtarget)
-copyright = u'2008-2011, Insophia'
+copyright = u'2008-2012, Scrapinghub'
-   ur'Insophia', 'manual'),
+   ur'Scrapinghub', 'manual'),
-import sys, os, glob
+import sys, os, glob, shutil
-        os.remove(f)
+    for f in glob.glob("debian/python-scrapy%s*" % suffix) + \
-    html_body_declared_encoding, http_content_type_encoding, to_unicode
+    html_body_declared_encoding, http_content_type_encoding
-            self._cached_ubody = to_unicode(self.body, self.encoding)
+            charset = 'charset=%s' % benc
-    
+
-        r = self.response_class("http://www.example.com", headers={"Content-type": ["text/html; charset=UKNOWN"]}, body="\xc2\xa3")
+        r = self.response_class("http://www.example.com",
-        r = self.response_class("http://www.example.com", body='\xff\xfeh\x00i\x00', encoding='utf-16')
+        r = self.response_class("http://www.example.com",
-        r6 = self.response_class("http://www.example.com", headers={"Content-type": ["text/html; charset=utf-8"]}, body="\xef\xbb\xbfWORD\xe3\xab")
+        r6 = self.response_class("http://www.example.com",
-        self.assertEqual(r6.body_as_unicode(), u'\ufeffWORD\ufffd\ufffd')
+        self.assertEqual(r6.body_as_unicode(), u'WORD\ufffd\ufffd')
-                        '|descendant::input[@type!="submit" '
+                        '|descendant::input[@type!="submit" and @type!="image" '
-from scrapy.http import Response, HtmlResponse
+from scrapy.http import Response, HtmlResponse, TextResponse
-    if not isinstance(response, HtmlResponse):
+    body = response.body
-    fd, fname = tempfile.mkstemp('.html')
+    fd, fname = tempfile.mkstemp(ext)
-import Image
+from PIL import Image
-    import Image
+    from PIL import Image
-    if v is None and ele.tag == 'select' and not ele.multiple:
+    if ele.tag == 'select':
-            return None, None
+        return (n, o[0]) if o else (None, None)
-        self.assertEqual(fs['six'].value, 'seven')
+        response = _buildresponse(
-        self.assertEqual(r1.headers['Accept-Encoding'], 'gzip,deflate')
+        response = _buildresponse(
-        r1 = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})
+        response = _buildresponse(
-        self.assertEqual(urlargs['six'], ['seven'])
+        fs = _qs(r1)
-        self.assertEqual(fs['two'].value, '2')
+        response = _buildresponse(
-        self.assertEqual(urlargs['two'], ['2'])
+        response = _buildresponse(
-        self.assertEqual(urlargs['two'], ['2'])
+        response = _buildresponse(
-        r1 = self.request_class.from_response(response, \
+        response = _buildresponse(
-        self.assertEqual(urlargs['two'], ['clicked2'])
+        fs = _qs(req)
-        r1 = self.request_class.from_response(response, \
+        response = _buildresponse(
-        self.assertEqual(urlargs['inputname'], ['inputvalue'])
+        fs = _qs(req)
-        req = self.request_class.from_response(res, formname='form2', \
+        response = _buildresponse(
-        self.assertFalse('field1' in urlargs, urlargs)
+        fs = _qs(req)
-        self.assertEqual(urlargs['clickme'], ['two'])
+        response = _buildresponse('''<form><input type="submit" name="clickme" value="one"> </form>''')
-        response = HtmlResponse("http://www.example.com/this/list.html", body=respbody)
+        response = _buildresponse(
-        self.assertFalse('clickable2' in urlargs, urlargs)
+        fs = _qs(r1)
-                          clickdata={'type': 'submit'})
+        response = _buildresponse(
-                                clickdata={'nonexistent': 'notme'})
+        response = _buildresponse(
-        response = HtmlResponse("http://www.example.com/lala.html", body=respbody)
+        response = _buildresponse("""<html></html>""")
-        response = HtmlResponse("http://www.example.com/lala.html", body=respbody)
+        response = _buildresponse(
-        r1 = self.request_class.from_response(response, formdata={'two':'3'}, callback=lambda x: x)
+        response = _buildresponse(
-
+        fs = _qs(r1)
-        r1 = self.request_class.from_response(response, formname="form2", callback=lambda x: x)
+        response = _buildresponse(
-        self.assertEqual(fs['four'].value, "4")
+        fs = _qs(r1)
-        r1 = self.request_class.from_response(response, formname="form3", callback=lambda x: x)
+        response = _buildresponse(
-        self.assertEqual(fs['one'].value, "1")
+        fs = _qs(r1)
-        self.assertRaises(IndexError, self.request_class.from_response, response, formname="form3", formnumber=2)
+        response = _buildresponse(
-              for k, v in ((e.name, _value(e)) for e in inputs) \
+              for k, v in (_value(e) for e in inputs) \
-    return v
+            return n, o[0]
-
+            <select name="i6"></select>
-            <input type="radio" name="i1" value="iv1">
+            <input type="radio" name="i1" value="i1v1">
-            <input type="checkbox" name="i1" value="iv1">
+            <input type="checkbox" name="i1" value="i1v1">
-            <input type="text" name="i1" value="iv1">
+            <input type="text" name="i1" value="i1v1">
-        self.assertEqual(fs, {'i1': ['iv1'], 'i2': ['']})
+        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': ['']})
-            <input type="hidden" name="i1" value="iv1">
+            <input type="hidden" name="i1" value="i1v1">
-        self.assertEqual(fs, {'i1': ['iv1'], 'i2': ['']})
+        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': ['']})
-            <input type="hidden" name="i1" value="iv1">
+            <input type="hidden" name="i1" value="i1v1">
-        self.assertEqual(fs, {'i1': ['iv1'], 'i2': ['']})
+        self.assertEqual(fs, {'i1': ['i1v1'], 'i2': ['']})
-    inputs = form.xpath('descendant::input[@type!="submit"]|descendant::textarea|descendant::select')
+    inputs = form.xpath('descendant::textarea'
-              if k not in formdata]
+              for k, v in ((e.name, _value(e)) for e in inputs) \
-        form = _get_form(root, formname, formnumber, response)
+        form = _get_form(response, formname, formnumber)
-    """
+def _get_form(response, formname, formnumber):
-    inputs = [(n, u'' if v is None else v) for n, v in form.fields.items() if n not in formdata]
+    inputs = form.xpath('descendant::input[@type!="submit"]|descendant::textarea|descendant::select')
-            inputs.append(clickable)
+            values.append(clickable)
-    return inputs
+    values.extend(formdata.iteritems())
-    inputs = [(n, v) for n, v in form.form_values() if n not in formdata]
+    inputs = [(n, u'' if v is None else v) for n, v in form.fields.items() if n not in formdata]
-        if clickable and clickable[0] not in formdata:
+        if clickable and clickable[0] not in formdata and not clickable[0] is None:
-# http://doc.scrapy.org/topics/spiders.html
+# Please refer to the documentation for information on how to create and manage
-lxml will be used.
+project settings.
-            text = _img_attr(img, "alt") or _img_attr(img, "title") or ""
+        for img in selector.select('descendant-or-self::img'):
-from scrapy.selector.libxml2sel import XPathSelectorList, HtmlXPathSelector
+from scrapy.selector import XPathSelectorList, HtmlXPathSelector
-                ret.append(Link(unicode_to_str(url[0], encoding), alt[0]))
+        """Extract the links of all the images found in the selector given."""
-            _add_link(selector)
+        selectors = [selector] if selector.select("local-name()").re("^img$") \
-        return ret
+
-              Link(url='http://example.com/sample3.html', text=u'sample 3') ])
+            [Link(url='http://example.com/sample2.jpg', text=u'sample 2')])
-    clickables = [el for el in form.inputs if el.type == 'submit']
+    clickables = [el for el in form.xpath('.//input[@type="submit"]')]
-    def test_from_response_multiple_forms_clickdata(self):
+    def test_from_response_with_select(self):
-          <input type="submit" name="clickable" value="clicked">
+          <input type="submit" name="clickable" value="clicked2">
-                                              clickdata={'name': 'clickable'})
+        req = self.request_class.from_response(res, formname='form2', \
-        self.assertEqual(urlargs['clickable'], ['clicked'])
+        self.assertEqual(urlargs['clickable'], ['clicked2'])
-    def test_from_response_multiple_forms_clickdata(self):
+    def test_from_response_override_clickable(self):
-            u''.join(u'[@%s="%s"]' % tuple(c) for c in clickdata.iteritems())
+            u''.join(u'[@%s="%s"]' % c for c in clickdata.iteritems())
-            clickable = _get_clickable(clickdata, clickables, form)
+        clickable = _get_clickable(clickdata, form)
-def _get_clickable(clickdata, clickables, form):
+def _get_clickable(clickdata, form):
-        el = clickables.pop(0)
+        el = clickables[0]
-        raise ValueError('No clickeable element matching clickdata: %r' % (clickdata,))
+        raise ValueError('No clickable element matching clickdata: %r' % (clickdata,))
-    def test_from_response_submit_first_clickeable(self):
+    def test_from_response_submit_first_clickable(self):
-<input type="submit" name="clickeable1" value="clicked1">
+<input type="submit" name="clickable1" value="clicked1">
-<input type="submit" name="clickeable2" value="clicked2">
+<input type="submit" name="clickable2" value="clicked2">
-        self.assertFalse('clickeable2' in urlargs, urlargs)
+        self.assertEqual(urlargs['clickable1'], ['clicked1'])
-    def test_from_response_submit_not_first_clickeable(self):
+    def test_from_response_submit_not_first_clickable(self):
-<input type="submit" name="clickeable1" value="clicked1">
+<input type="submit" name="clickable1" value="clicked1">
-<input type="submit" name="clickeable2" value="clicked2">
+<input type="submit" name="clickable2" value="clicked2">
-        r1 = self.request_class.from_response(response, formdata={'two': '2'}, clickdata={'name': 'clickeable2'})
+        r1 = self.request_class.from_response(response, formdata={'two': '2'}, clickdata={'name': 'clickable2'})
-        self.assertFalse('clickeable1' in urlargs, urlargs)
+        self.assertEqual(urlargs['clickable2'], ['clicked2'])
-<input type="submit" name="clickeable" value="clicked2">
+<input type="submit" name="clickable" value="clicked1">
-                clickdata={'name': 'clickeable', 'value': 'clicked2'})
+                clickdata={'name': 'clickable', 'value': 'clicked2'})
-        self.assertEqual(urlargs['clickeable'], ['clicked2'])
+        self.assertEqual(urlargs['clickable'], ['clicked2'])
-          <input type="submit" name="clickeable" value="clicked">
+          <input type="submit" name="clickable" value="clicked">
-          <input type="submit" name="clickeable" value="clicked">
+          <input type="submit" name="clickable" value="clicked">
-                                              clickdata={'name': 'clickeable'})
+                                              clickdata={'name': 'clickable'})
-        self.assertEqual(urlargs['clickeable'], ['clicked'])
+        self.assertEqual(urlargs['clickable'], ['clicked'])
-<input type="submit" name="clickeable1" value="clicked1">
+<input type="submit" name="clickable1" value="clicked1">
-<input type="submit" name="clickeable2" value="clicked2">
+<input type="submit" name="clickable2" value="clicked2">
-        self.assertFalse('clickeable2' in urlargs, urlargs)
+        self.assertFalse('clickable1' in urlargs, urlargs)
-<input type="submit" name="clickeable1" value="clicked1">
+<input type="submit" name="clickable1" value="clicked1">
-<input type="submit" name="clickeable2" value="clicked2">
+<input type="submit" name="clickable2" value="clicked2">
-          <input type="submit" name="clickeable" value="clicked">
+          <input type="submit" name="clickable" value="clicked">
-
+    try:
-
+import lxml.html
-        form = _get_form(hxs, formname, formnumber, response)
+        root = LxmlDocument(response, lxml.html.HTMLParser)
-def _get_form(hxs, formname, formnumber, response):
+def _get_form(root, formname, formnumber, response):
-    if not hxs.forms:
+    if not root.forms:
-        f = hxs.xpath('//form[@name="%s"]' % formname)
+        f = root.xpath('//form[@name="%s"]' % formname)
-            form = hxs.forms[formnumber]
+            form = root.forms[formnumber]
-    __slots__ = ['xmlDoc', 'xpathContext', '__weakref__']
+    __slots__ = ['__weakref__']
-    'XPathSelectorList']
+           'XPathSelectorList']
-        '__weakref__']
+    __slots__ = ['response', 'text', 'namespaces', '_expr', '_root', '__weakref__']
-            self.response = TextResponse(url='about:blank', \
+    def __init__(self, response=None, text=None, namespaces=None, _root=None, _expr=None):
-        self._xpathev = None
+        if response is not None:
-        return self._root
+        self.response = response
-            xpathev = self.root.xpath
+            xpathev = self._root.xpath
-        result = [self.__class__(root=x, expr=xpath, namespaces=self.namespaces)
+        result = [self.__class__(_root=x, _expr=xpath, namespaces=self.namespaces)
-            return etree.tostring(self.root, method=self._tostring_method, \
+            return etree.tostring(self._root, method=self._tostring_method, \
-            if self.root is True:
+            if self._root is True:
-            elif self.root is False:
+            elif self._root is False:
-                return unicode(self.root)
+                return unicode(self._root)
-        return "<%s xpath=%r data=%s>" % (type(self).__name__, self.expr, data)
+        return "<%s xpath=%r data=%s>" % (type(self).__name__, self._expr, data)
-from scrapy.utils.test import libxml2debug
+import unittest
-    #                     u'<root>la</root>')
+    xs_cls = XPathSelector
-    return lxdoc
+"""
-from .document import Libxml2Document
+from .libxml2document import Libxml2Document, xmlDoc_from_html, xmlDoc_from_xml
-from scrapy.selector.document import Libxml2Document
+from scrapy.selector.libxml2document import Libxml2Document
-                                    % (el, clickdata))
+        raise ValueError("Multiple elements found (%r) matching the criteria "
-        raise ValueError('No clickeable element matching clickdata')
+        raise ValueError('No clickeable element matching clickdata: %r' % (clickdata,))
-        self.assertRaises(MultipleElementsFound,
+        self.assertRaises(ValueError,
-    if len(el) > 1:
+    xpath = u'.//*' + \
-        return (el[0].name, el[0].value)
+        raise ValueError('No clickeable element matching clickdata')
-        inputs.append(clickable)
+    if not dont_click:
-                return (el.name, el.value)
+    # If we don't have clickdata, we just use the first clickable element
-        # as such
+    # If clickdata is given, we compare it to the clickable elements to find a
-    # clickable element
+            return (el.name, el.value)
-        return (el.name, el.value)
+        return (el[0].name, el[0].value)
-                    inputs.append((name, el.value))
+    inputs = [(n, v) for n, v in form.form_values() if n not in formdata]
-            return unicode(self.root)
+            if self.root is True:
-        assert false in [u'0', u'False'], false
+        self.assertEquals(xs.select("//input[@name='a']/@name='a'").extract(), [u'1'])
-                base_url=self.response.url)
+            url = self.response.url
-            result = self.xpathev(xpath)
+            xpathev = self.root.xpath
-            result = [self.__class__(root=result, expr=xpath, namespaces=self.namespaces)]
+
-        self.xxs_cls(r1) # shouldn't raise error
+        self.hxs_cls(r1).select('//text()').extract()
-
+    app.add_role('commit', commit_role)
-    url = 'https://github.com/scrapy/scrapy/blob/master/' + text
+    ref = 'https://github.com/scrapy/scrapy/blob/master/' + text
-        tag = html._nons(el.tag)
+        tag = _nons(el.tag)
-            if u' xmlns' in response.body_as_unicode()[:200]:
+            if xmlns_in_resp:
-    inputs.extend([(key, value) for key, value in formdata.iteritems()])
+    inputs.extend(formdata.iteritems())
-"""Helper functinos for working with signals"""
+"""Helper functions for working with signals"""
-            self.headers['Content-Type'] = 'application/x-www-form-urlencoded'
+            querystr = _urlencode(items, self.encoding)
-                else: formdata = {}
+                formdata = dict(formdata) if formdata else {}
-                              base_url=response.url)
+        kwargs.setdefault('encoding', response.encoding)
-        return unicode_to_str(string, encoding)
+        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)
-    required because simplejson fails to serialize dicts containing
+    required because json library fails to serialize dicts containing
-        import libxml2
+        import lxml
-            import lxml
+            import libxml2
-            from scrapy.selector.lxmlsel import *
+            from scrapy.selector.libxml2sel import *
-        from scrapy.selector.libxml2sel import *
+        from scrapy.selector.lxmlsel import *
-    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.1', 'pyOpenSSL']
+    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.1', 'lxml', 'pyOpenSSL']
-                (request, spider_failure.getErrorMessage()), log.ERROR, spider=spider)
+            errmsg = spider_failure.getErrorMessage()
-            return req
+from scrapy.http.request.form import MultipleElementsFound
-from scrapy.xlib.ClientForm import ParseFile
+from lxml import html
-        return unicode_to_str(string, encoding)
+
-    def from_response(cls, response, formname=None, formnumber=0, formdata=None, 
+    def from_response(cls, response, formname=None, formnumber=0, formdata=None,
-        if not form:
+        if not hasattr(formdata, "items"):
-                form = forms[formnumber]
+                if formdata:
-            url, body, headers = form._switch_click('request_data')
+                pass
-            url, body, headers = form.click_request_data(**(clickdata or {}))
+            xpath_pred = []
-        kwargs.setdefault('headers', {}).update(headers)
+            xpath_expr = '//*%s' % ''.join(xpath_pred)
-        return cls(url, method=form.method, body=body, **kwargs)
+    # If we don't have clickdata, we just use the first
-        return key, downloader.slots.get(key)
+        return key, downloader.slots.get(key) or downloader.inactive_slots.get(key)
-
+        self.inactive_slots = {}
-                del self.slots[key]
+                self.inactive_slots[key] = self.slots.pop(key)
-                concurrency = self.ip_concurrency
+            if key in self.inactive_slots:
-            self.slots[key] = Slot(concurrency, delay, self.settings)
+                if self.ip_concurrency:
-        regex = re.compile(regex)
+        regex = re.compile(regex, re.UNICODE)
-USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)
+USER_AGENT = 'Scrapy/0.15 (+http://scrapy.org)'
-from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, Response
+from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, HtmlResponse
-        response = Response("http://www.example.com/this/list.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/this/list.html", body=respbody)
-        response = Response("http://www.example.com/this/list.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/this/list.html", body=respbody)
-        response = Response("http://www.example.com/this/list.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/this/list.html", body=respbody)
-        response = Response("http://www.example.com/this/list.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/this/list.html", body=respbody)
-        response = Response("http://www.example.com/this/list.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/this/list.html", body=respbody)
-        response = Response("http://www.example.com/this/list.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/this/list.html", body=respbody)
-        response = Response("http://www.example.com/this/list.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/this/list.html", body=respbody)
-        response = Response("http://www.example.com/lala.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/lala.html", body=respbody)
-        response = Response("http://www.example.com/lala.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/lala.html", body=respbody)
-        response = Response("http://www.example.com/formname.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/formname.html", body=respbody)
-        response = Response("http://www.example.com/formname.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/formname.html", body=respbody)
-        response = Response("http://www.example.com/formname.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/formname.html", body=respbody)
-        response = Response("http://www.example.com/formname.html", body=respbody)
+        response = HtmlResponse("http://www.example.com/formname.html", body=respbody)
-from scrapy.xlib import twisted_250_monkeypatches, urlparse_monkeypatches
+from scrapy.xlib import urlparse_monkeypatches
-    setup_args['install_requires'] = ['Twisted>=2.5', 'w3lib>=1.1', 'pyOpenSSL']
+    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.1', 'pyOpenSSL']
-from scrapy.exceptions import UsageError, ScrapyDeprecationWarning
+from scrapy.exceptions import UsageError
-            self.pagecount = settings.getint('CLOSESPIDER_ITEMPASSED')
+CLOSESPIDER_ERRORCOUNT = 0
-import gzip, warnings
+import gzip, warnings, inspect
-        return [Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls]
+        return (Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls)
-        return reqs
+            yield self.make_requests_from_url(url)
-import sys, optparse, urllib
+import sys, optparse, urllib, json
-    print "Scrapy %s requires Python 2.5 or above" % __version__
+if sys.version_info < (2,6):
-
+import json
-from scrapy.utils.py26 import json
+from shutil import copytree, ignore_patterns
-
+import json
-
+import json
-from os.path import abspath, dirname, join
+from pkgutil import get_data
-import unittest, cPickle as pickle
+import unittest, json, cPickle as pickle
-import unittest
+import unittest, json
-from scrapy.utils.py26 import json
+import json
-from scrapy.utils.py26 import json
+import json
-Similar to scrapy.utils.py26, but for Python 2.7
+This module provides functions added in Python 2.7, which weren't yet available
-higher than 2.5 which is the lowest version supported by Scrapy.
+higher than 2.5 which used to be the lowest version supported by Scrapy.
-
+import json
-
+import json
-from twisted.web import resource
+import json
-from scrapy.utils.py26 import json
+from twisted.web import resource
-
+from multiprocessing import cpu_count
-
+import json
-
+from pkgutil import get_data
-    setup_args['install_requires'] = ['Twisted>=2.5', 'w3lib', 'pyOpenSSL']
+    setup_args['install_requires'] = ['Twisted>=2.5', 'w3lib>=1.1', 'pyOpenSSL']
-    log.msg("Scrapyd web console available at http://localhost:%s/" % http_port)
+    webservice = TCPServer(http_port, server.Site(Root(config, app)), interface=bind_address)
-    html_body_declared_encoding, http_content_type_encoding
+    html_body_declared_encoding, http_content_type_encoding, to_unicode
-        return resolve_encoding(enc)
+        return self._declared_encoding() or self._body_inferred_encoding()
-            self._cached_ubody = self.body.decode(self.encoding, 'scrapy_replace')
+            self._cached_ubody = to_unicode(self.body, self.encoding)
-            benc, _ = html_to_unicode(content_type, self.body, default_encoding=self._DEFAULT_ENCODING)
+            benc, ubody = html_to_unicode(content_type, self.body, \
-            #    self._cached_ubody = dammit.unicode
+            self._cached_ubody = ubody
-        self.assertEqual(r3._declared_encoding(), "iso-8859-1")
+        self.assertEqual(r3._headers_encoding(), "cp1252")
-        jar = self.jars[spider]
+        cookiejarkey = request.meta.get("cookiejar")
-        jar = self.jars[spider]
+        cookiejarkey = request.meta.get("cookiejar")
-        self.mw.spider_closed(self.spider)
+    def test_cookiejar_key(self):
-    host = parse_url(url).hostname
+    host = parse_url(url).netloc
-        first_buf = None
+        checksum = None
-        return md5sum(first_buf)
+        return checksum
-        return get_vmvalue_from_procfs('VmSize')
+        return get_vmvalue_from_procfs('VmRSS')
-            self.crawler.stop()
+            open_spiders = self.crawler.engine.open_spiders
-MEMUSAGE_ENABLED = 1
+MEMUSAGE_ENABLED = False
-
+    pass
-from scrapy.http.response.dammit import UnicodeDammit
+from w3lib.encoding import html_to_unicode, resolve_encoding, \
-from scrapy.utils.encoding import encoding_exists, resolve_encoding
+from scrapy.utils.encoding import encoding_exists
-                    return encoding
+        return http_content_type_encoding(content_type)
-            benc = dammit.originalEncoding
+            content_type = self.headers.get('Content-Type')
-                self._cached_ubody = dammit.unicode
+            #if self._cached_ubody is None and benc != 'utf-16':
-        return None
+        return html_body_declared_encoding(self.body)
-
+    pass
-from scrapy.utils.encoding import resolve_encoding
+from w3lib.encoding import resolve_encoding
-from scrapy.conf import settings
+from w3lib.encoding import resolve_encoding
-def encoding_exists(encoding, _aliases=_ENCODING_ALIASES):
+def encoding_exists(encoding):
-        codecs.lookup(resolve_encoding(encoding, _aliases))
+        codecs.lookup(resolve_encoding(encoding))
-        s += pformat(get_engine_status())
+        s += pformat(get_engine_status(self.crawler.engine))
-        slot = self._get_slot(response.request)
+        key, slot = self._get_slot(response.request)
-                (slot.concurrency, slot.delay*1000, \
+            spider.log("slot: %s | conc:%2d | delay:%5d ms | latency:%5d ms | size:%6d bytes" % \
-        return downloader.slots.get(key)
+        return key, downloader.slots.get(key)
-import os
+from scrapy.utils.trackref import format_live_refs
-
+        liverefs = format_live_refs() if self.dumprefs else ""
-            "\n{0}\n{1}".format(enginestatus, stackdumps)
+            "\n{0}\n{1}\n{2}".format(enginestatus, liverefs, stackdumps)
-        except IOError:
+        except (IOError, struct.error):
-            self._url = safe_url_string(unicode_url, self.encoding)
+            self._set_url(url.encode(self.encoding))
-                self.crawl(request, spider)
+            except Exception, exc:
-    from twisted.internet.ssl import ClientContextFactory
+ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])
-        self.DEBUG = settings.getint("AUTOTHROTTLE_DEBUG", False)
+        self.DEBUG = settings.getbool("AUTOTHROTTLE_DEBUG")
-        self.last_lat = self.START_DELAY, 0.0
+        self.MIN_DOWNLOAD_DELAY = settings.getint("AUTOTHROTTLE_MIN_DOWNLOAD_DELAY")
-        
+
-        if parent:
+        if parent is not None:
-from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy
+import time
-        self.jar = _CookieJar(policy or DefaultCookiePolicy())
+    def __init__(self, policy=None, check_expired_frequency=10000):
-
+        self.policy._now = self.jar._now = int(time.time())
-    datadir = os.path.join(project_data_dir(), '.scrapy', 'scrapyd')
+    datadir = os.path.join(project_data_dir(), 'scrapyd')
-        self.logs_to_keep = config.getint('logs_to_keep', 5)
+        self.items_dir = config.get('items_dir', 'items')
-        env['SCRAPY_LOG_FILE'] = self._get_log_file(message)
+        env['SCRAPY_LOG_FILE'] = self._get_file(message, self.logs_dir, 'log')
-        logsdir = os.path.join(self.logs_dir, message['_project'], \
+    def _get_file(self, message, dir, ext):
-        to_delete = sorted((os.path.join(logsdir, x) for x in os.listdir(logsdir)), key=os.path.getmtime)[:-self.logs_to_keep]
+        to_delete = sorted((os.path.join(logsdir, x) for x in \
-        return os.path.join(logsdir, "%s.log" % message['_job'])
+        return os.path.join(logsdir, "%s.%s" % (message['_job'], ext))
-            self.spider, self.job, self.pid, self.logfile)
+        msg += "project=%r spider=%r job=%r pid=%r log=%r items=%r" % (self.project, \
-    for k in ['eggs_dir', 'logs_dir', 'dbs_dir']: # create dirs
+    for k in ['eggs_dir', 'logs_dir', 'items_dir', 'dbs_dir']: # create dirs
-        s += "<tr><th colspan='6' style='background-color: #ddd'>Pending</th></tr>"
+        s += "<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th><th>Log</th><th>Items</th>"
-        s += "<tr><th colspan='6' style='background-color: #ddd'>Running</th></tr>"
+        s += "<tr><th colspan='7' style='background-color: #ddd'>Running</th></tr>"
-        s += "<tr><th colspan='6' style='background-color: #ddd'>Finished</th></tr>"
+        s += "<tr><th colspan='7' style='background-color: #ddd'>Finished</th></tr>"
-<li><a href="http://doc.scrapy.org/topics/scrapyd.html">Documentation</a></li>
+<li><a href="http://doc.scrapy.org/en/latest/topics/scrapyd.html">Documentation</a></li>
-        self.putChild('procmon', ProcessMonitor(self))
+        self.putChild('jobs', Jobs(self))
-<li><a href="/procmon">Process monitor</a></li>
+<li><a href="/jobs">Jobs</a></li>
-class ProcessMonitor(resource.Resource):
+class Jobs(resource.Resource):
-        s += "<h1>Process monitor</h1>"
+        s += "<h1>Jobs</h1>"
-        s += "</tr>"
+        s += "<tr><th colspan='6' style='background-color: #ddd'>Pending</th></tr>"
-
+    def remove(func):
-from scrapy.utils.gz import gunzip
+from scrapy.utils.gz import gunzip, is_gzipped
-                log.msg("Ignoring non-XML sitemap: %s" % response, log.WARNING)
+            body = self._get_sitemap_body(response)
-    return ctype in ('application/x-gzip', 'application/gzip')
+    def _get_sitemap_body(self, response):
-import warnings
+import gzip, warnings
-        shell=True, env=env)
+        shell=True)
-__version__ = "0.15.0"
+version_info = (0, 15, 1)
-    rev = Popen(["hg", "tip", "--template", "{rev}"], stdout=PIPE).communicate()[0]
+if os.environ.get('SCRAPY_VERSION_FROM_GIT'):
-        f.write("\n__version__ = '.'.join(map(str, version_info)) + '.%s'" % rev.strip())
+        f.write("\n__version__ = '%s'" % v.strip())
-        if self.is_cacheable(request) and self.is_cacheable_response(response):
+        if (self.is_cacheable(request)
-        elif self.ignore_missing:
+
-import sys, os
+import sys
-        self.processes.pop(slot)
+        process = self.processes.pop(slot)
-    def render_POST(self, txrequest):
+    def render_GET(self, txrequest):
-        return {"status":"ok", "jobs": jlist}
+        running = [{"id": s.job, "spider": s.spider} for s in spiders if s.project == project]
-from scrapy.http import Request
+from scrapy.http import Request, HtmlResponse
-Extensions for debugging Scrapy 
+Extensions for debugging Scrapy
-    def __init__(self):
+
-        msg += format_engine_status()
+        stackdumps = self._thread_stacks()
-        c = S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY)
+        c = S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)
-__version__ = "0.14.0"
+version_info = (0, 15, 0)
-__version__ = "0.13.0"
+version_info = (0, 14, 0)
-        return d
+        self.spider = spider
-            return self._spiders[spider_name](**spider_kwargs)
+            spcls = self._spiders[spider_name]
-from scrapy.utils.response import body_or_str, response_httprepr, open_in_browser
+from scrapy.utils.response import body_or_str, response_httprepr, open_in_browser, \
-from scrapy.xlib.BeautifulSoup import BeautifulSoup
+_noscript_re = re.compile(u'<noscript>.*?</noscript>', re.IGNORECASE | re.DOTALL)
-        to_delete = sorted(os.listdir(logsdir), reverse=True)[:-self.logs_to_keep]
+        to_delete = sorted((os.path.join(logsdir, x) for x in os.listdir(logsdir)), key=os.path.getmtime)[:-self.logs_to_keep]
-            os.remove(os.path.join(logsdir, x))
+            os.remove(x)
-        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY')
+        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY_MAIL')
-            self._url = safe_url_string(url)
+            self._url = escape_ajax(safe_url_string(url))
-                           ConnectionLost, PartialDownloadError, TCPTimedOutError,
+                           ConnectionLost, TCPTimedOutError,
-from twisted.web.client import PartialDownloadError, HTTPClientFactory
+from twisted.web.client import HTTPClientFactory
-from cPickle import Pickler
+import cPickle as pickle
-    def __init__(self, file, protocol=0, **kwargs):
+    def __init__(self, file, protocol=2, **kwargs):
-        self.pickler = Pickler(file, protocol)
+        self.file =file
-        self.pickler.dump(dict(self._get_serialized_fields(item)))
+        d = dict(self._get_serialized_fields(item))
-            help="store scraped items into FILE (using feed exports)")
+            help="dump scraped items into FILE (use - for stdout)")
-            help="format to use in feed exports (default: %default)")
+            help="format to use for dumping items with -o (default: %default)")
-            self.settings.overrides['FEED_URI'] = opts.output
+            if opts.output == '-':
-            slot.itemcount, nbytes, slot.uri)
+        logfmt = "%%s %s feed (%d items) in: %s" % (self.format, \
-    def store(file, spider):
+    def open(spider):
-        return threads.deferToThread(self._store_in_thread, file, spider)
+    def open(self, spider):
-    def _store_in_thread(self, file, spider):
+    def _store_in_thread(self, file):
-        copyfileobj(file, self._stdout)
+    def open(self, spider):
-class FileFeedStorage(BlockingFeedStorage):
+class FileFeedStorage(object):
-    def _store_in_thread(self, file, spider):
+    def open(self, spider):
-        f.close()
+        return open(self.path, 'ab')
-    def _store_in_thread(self, file, spider):
+    def _store_in_thread(self, file):
-    def _store_in_thread(self, file, spider):
+    def _store_in_thread(self, file):
-    def __init__(self, file, exp):
+    def __init__(self, file, exporter, storage, uri):
-        self.exporter = exp
+        self.exporter = exporter
-        self.slots[spider] = SpiderSlot(file, exp)
+        uri = self.urifmt % self._get_uri_params(spider)
-        d = defer.maybeDeferred(storage.store, slot.file, spider)
+            slot.itemcount, nbytes, slot.uri)
-class FileFeedStorageTest(FeedStorageTest):
+class FileFeedStorageTest(unittest.TestCase):
-class FTPFeedStorageTest(FeedStorageTest):
+class FTPFeedStorageTest(unittest.TestCase):
-        yield storage.store(StringIO("content"), BaseSpider("default"))
+        file = storage.open(BaseSpider("default"))
-class StdoutFeedStorageTest(FeedStorageTest):
+class StdoutFeedStorageTest(unittest.TestCase):
-        yield storage.store(StringIO("content"), BaseSpider("default"))
+        file = storage.open(BaseSpider("default"))
-        self.assertLess(os.path.getsize(self.path), size)
+        assert os.path.getsize(self.path), size
-    url = 'http://dev.scrapy.org/browser/' + text
+    url = 'https://github.com/scrapy/scrapy/blob/master/' + text
-            prio = settings.getint('DEPTH_PRIORITY')
+        prio = settings.getint('DEPTH_PRIORITY')
-    def __init__(self, dupefilter, jobdir=None, dqclass=None, logunser=False):
+    def __init__(self, dupefilter, jobdir=None, dqclass=None, mqclass=None, logunser=False):
-        self.dqdir = join(jobdir, 'requests.queue') if jobdir else None
+        self.dqdir = self._dqdir(jobdir)
-        return cls(dupefilter, job_dir(settings), dqclass, logunser)
+        return cls(dupefilter, job_dir(settings), dqclass, mqclass, logunser)
-        return MemoryQueue()
+        return self.mqclass()
-DEPTH_PRIORITY = 1
+DEPTH_PRIORITY = 0
-SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleDiskQueue'
+SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'
-Scheduler disk-based queues
+Scheduler queues
-from scrapy.utils.queue import DiskQueue
+from scrapy.utils import queue
-class PickleDiskQueue(DiskQueue):
+    class SerializableQueue(queue_class):
-        super(PickleDiskQueue, self).push(s)
+        def push(self, obj):
-            return pickle.loads(s)
+        def pop(self):
-class MarshalDiskQueue(DiskQueue):
+def _pickle_serialize(obj):
-            return marshal.loads(s)
+PickleFifoDiskQueue = _serializable_queue(queue.FifoDiskQueue, \
-from scrapy.squeue import MarshalDiskQueue, PickleDiskQueue
+from scrapy.squeue import MarshalFifoDiskQueue, MarshalLifoDiskQueue, PickleFifoDiskQueue, PickleLifoDiskQueue
-class MarshalDiskQueueTest(t.DiskQueueTest):
+class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):
-        return MarshalDiskQueue(self.qdir, chunksize=self.chunksize)
+        return MarshalFifoDiskQueue(self.qdir, chunksize=self.chunksize)
-class ChunkSize1MarshalDiskQueueTest(MarshalDiskQueueTest):
+class ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):
-class ChunkSize2MarshalDiskQueueTest(MarshalDiskQueueTest):
+class ChunkSize2MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):
-class ChunkSize3MarshalDiskQueueTest(MarshalDiskQueueTest):
+class ChunkSize3MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):
-class ChunkSize4MarshalDiskQueueTest(MarshalDiskQueueTest):
+class ChunkSize4MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):
-class PickleDiskQueueTest(MarshalDiskQueueTest):
+class PickleFifoDiskQueueTest(MarshalFifoDiskQueueTest):
-        return PickleDiskQueue(self.qdir, chunksize=self.chunksize)
+        return PickleFifoDiskQueue(self.qdir, chunksize=self.chunksize)
-class ChunkSize1PickleDiskQueueTest(PickleDiskQueueTest):
+class ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
-class ChunkSize2PickleDiskQueueTest(PickleDiskQueueTest):
+class ChunkSize2PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
-class ChunkSize3PickleDiskQueueTest(PickleDiskQueueTest):
+class ChunkSize3PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
-class ChunkSize4PickleDiskQueueTest(PickleDiskQueueTest):
+class ChunkSize4PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):
-from scrapy.utils.queue import MemoryQueue, DiskQueue
+from scrapy.utils.queue import FifoMemoryQueue, LifoMemoryQueue, FifoDiskQueue, LifoDiskQueue
-class TestMemoryQueue(MemoryQueue):
+def track_closed(cls):
-        self.closed = False
+    class TrackingClosed(cls):
-        self.closed = True
+        def __init__(self, *a, **kw):
-class TestDiskQueue(DiskQueue):
+    return TrackingClosed
-class MemoryPriorityQueueTest(unittest.TestCase):
+class FifoMemoryPriorityQueueTest(unittest.TestCase):
-        self.q = PriorityQueue(qfactory)
+        self.q = PriorityQueue(self.qfactory)
-class DiskPriorityQueueTest(MemoryPriorityQueueTest):
+class LifoMemoryPriorityQueueTest(FifoMemoryPriorityQueueTest):
-        self.q = PriorityQueue(qfactory)
+        self.q = PriorityQueue(self.qfactory)
-from scrapy.utils.queue import MemoryQueue, DiskQueue
+from scrapy.utils.queue import FifoMemoryQueue, LifoMemoryQueue, FifoDiskQueue, LifoDiskQueue
-class MemoryQueueTest(unittest.TestCase):
+class FifoMemoryQueueTest(unittest.TestCase):
-        return MemoryQueue()
+        return FifoMemoryQueue()
-class DiskQueueTest(MemoryQueueTest):
+class LifoMemoryQueueTest(unittest.TestCase):
-        return DiskQueue(self.qdir, chunksize=self.chunksize)
+        return FifoDiskQueue(self.qdir, chunksize=self.chunksize)
-class ChunkSize1DiskQueueTest(DiskQueueTest):
+class ChunkSize1FifoDiskQueueTest(FifoDiskQueueTest):
-class ChunkSize2DiskQueueTest(DiskQueueTest):
+class ChunkSize2FifoDiskQueueTest(FifoDiskQueueTest):
-class ChunkSize3DiskQueueTest(DiskQueueTest):
+class ChunkSize3FifoDiskQueueTest(FifoDiskQueueTest):
-class ChunkSize4DiskQueueTest(DiskQueueTest):
+class ChunkSize4FifoDiskQueueTest(FifoDiskQueueTest):
-class MemoryQueue(object):
+class FifoMemoryQueue(object):
-class DiskQueue(object):
+class LifoMemoryQueue(FifoMemoryQueue):
-        self.statefn = os.path.join(jobdir, 'spider.state')
+    def __init__(self, jobdir=None):
-        obj = cls(jobdir)
+        obj = cls(crawler.settings.get('JOBDIR'))
-            pickle.dump(spider.state, f, protocol=2)
+        if self.jobdir:
-        if os.path.exists(self.statefn):
+        if self.jobdir and os.path.exists(self.statefn):
-from scrapy.conf import settings
+from scrapy.utils.httpobj import urlparse_cached
-    def __init__(self):
+    def __init__(self, crawler):
-        self.last_lat = {}
+        self.START_DELAY = settings.getfloat("AUTOTHROTTLE_START_DELAY", 5.0)
-
+        self.last_latencies = [self.START_DELAY]
-        if not latency:
+        
-        latencies = self.last_latencies[spider]
+        self._adjust_delay(slot, latency, response)
-            self.last_lat[spider] = curavg, curdev
+            preavg, predev = self.last_lat
-                spider.max_concurrent_requests += 1
+                if slot.concurrency > 1:
-    cmd.set_crawler(crawler)
+    cmd.settings = settings
-from scrapy.conf import settings
+    def __init__(self):
-            help="log level (default: %s)" % settings['LOGLEVEL'])
+            help="log level (default: %s)" % self.settings['LOG_LEVEL'])
-            settings.overrides.update(arglist_to_dict(opts.set))
+            self.settings.overrides.update(arglist_to_dict(opts.set))
-            settings.overrides['LOG_FILE'] = opts.logfile
+            self.settings.overrides['LOG_ENABLED'] = True
-            settings.overrides['LOG_LEVEL'] = opts.loglevel
+            self.settings.overrides['LOG_ENABLED'] = True
-            settings.overrides['LOG_ENABLED'] = False
+            self.settings.overrides['LOG_ENABLED'] = False
-        base_url = urljoin(response_url, self.base_url) if self.base_url else response_url
+    def _extract_links(self, response_text, response_url, response_encoding, base_url=None):
-    check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)-r$(hg tip --template "{rev}")+$(date +%s) "Automatic build"', shell=True)
+    env={'SCRAPY_VERSION_FROM_GIT': '1'}
-    env={'SCRAPY_VERSION_FROM_HG': '1'}
+    env={'SCRAPY_VERSION_FROM_GIT': '1'}
-                if self.should_follow(x, spider):
+                if x.dont_filter or self.should_follow(x, spider):
-        offsite_reqs = [Request('http://scrapy2.org')]
+                       Request('http://sub.scrapy.org/1'),
-        return {"status": "ok", "jobs": jobids}
+        return {"status": "ok", "jobid": jobid}
-    """Stores and loads spider state"""
+    """Store and load spider state during a scraping job"""
-        return cls(jobdir)
+        obj = cls(jobdir)
-from w3lib.url import urljoin_rfc
+from urlparse import urljoin
-                redirected_url = urljoin_rfc(request.url, response.headers['location'])
+                redirected_url = urljoin(request.url, response.headers['location'])
-            redirected_url = urljoin_rfc(request.url, response.headers['location'])
+            redirected_url = urljoin(request.url, response.headers['location'])
-            redirected_url = urljoin_rfc(request.url, response.headers['location'])
+            redirected_url = urljoin(request.url, response.headers['location'])
-from w3lib.url import safe_url_string, urljoin_rfc
+from w3lib.url import safe_url_string
-        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url
+        base_url = urljoin(response_url, self.base_url) if self.base_url else response_url
-            link.url = urljoin_rfc(base_url, link.url, response_encoding)
+            if isinstance(link.url, unicode):
-from w3lib.url import urljoin_rfc
+from urlparse import urljoin
-        base_url = urljoin_rfc(response.url, base_url[0]) if base_url else response.url
+        base_url = urljoin(response.url, base_url[0].encode(response.encoding)) if base_url else response.url
-            link.url = urljoin_rfc(base_url, link.url, response.encoding)
+            link.url = urljoin(base_url, link.url)
-        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url
+        base_url = urljoin(response_url, self.base_url) if self.base_url else response_url
-        clean_url = lambda u: urljoin_rfc(base_url, remove_entities(clean_link(u.decode(response_encoding))))
+        clean_url = lambda u: urljoin(base_url, remove_entities(clean_link(u.decode(response_encoding))))
-from urlparse import urlparse
+from urlparse import urlparse, urljoin
-from w3lib.url import safe_url_string, urljoin_rfc
+from w3lib.url import safe_url_string
-            base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url
+            base_url = urljoin(response_url, self.base_url) if self.base_url else response_url
-            link.url = urljoin_rfc(base_url, link.url, response_encoding)
+            if isinstance(link.url, unicode):
-from scrapy.exceptions import DontCloseSpider
+from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning
-        self._concurrent_spiders = self.settings.getint('CONCURRENT_SPIDERS')
+        self._concurrent_spiders = self.settings.getint('CONCURRENT_SPIDERS', 1)
-        self.assertEqual(env['SCRAPY_CONCURRENT_SPIDERS'], '1')
+from __future__ import with_statement
-import unittest
+from twisted.trial import unittest
-from scrapy.utils.queue import MemoryQueue
+from scrapy.utils.queue import MemoryQueue, DiskQueue
-        super(TestMemoryQueue, self).__init__()
+    def __init__(self, *a, **kw):
-class PriorityQueueTest(unittest.TestCase):
+
-                self.queues[p] = q
+            self.queues[p] = self.qfactory(p)
-        q.push(obj)
+        if priority not in self.queues:
-            q = self.queues.pop(self.curprio)
+        if len(q) == 0:
-            prios = self.queues.keys()
+            prios = [p for p, q in self.queues.items() if len(q) > 0]
-        for q in self.queues.values():
+        active = []
-        return self.queues.keys()
+        return active
-    def __init__(self, dupefilter, jobdir=None, dqclass=None):
+    def __init__(self, dupefilter, jobdir=None, dqclass=None, logunser=False):
-        return cls(dupefilter, job_dir(settings), dqclass)
+        logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS')
-        except ValueError: # non serializable request
+        except ValueError, e: # non serializable request
-        s = pickle.dumps(obj, protocol=2)
+        try:
-        super(PickleDiskQueue, self).push(pickle.dumps(obj))
+        s = pickle.dumps(obj, protocol=2)
-class PickleDiskQueueTest(t.DiskQueueTest):
+class PickleDiskQueueTest(MarshalDiskQueueTest):
-            raise UsageError("Invalid --set value, use --set NAME=VALUE", print_help=False)
+            raise UsageError("Invalid -s value, use -s NAME=VALUE", print_help=False)
-        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--set', 'TEST1=override'), \
+        self.assertEqual(self._execute('settings', '--get', 'TEST1', '-s', 'TEST1=override'), \
-        self.assertEqual(cargs, ['lala', '-a', 'arg1=val1', '--set', 'ONE=two'])
+        self.assertEqual(cargs, ['lala', '-a', 'arg1=val1', '-s', 'ONE=two'])
-        args += ['--set']
+        args += ['-s']
-        group.add_option("--logfile", dest="logfile", metavar="FILE", \
+        group.add_option("--logfile", metavar="FILE", \
-        group.add_option("-L", "--loglevel", dest="loglevel", metavar="LEVEL", \
+        group.add_option("-L", "--loglevel", metavar="LEVEL", \
-        group.add_option("--nolog", action="store_true", dest="nolog", \
+        group.add_option("--nolog", action="store_true", \
-        group.add_option("--profile", dest="profile", metavar="FILE", default=None, \
+        group.add_option("--profile", metavar="FILE", default=None, \
-        group.add_option("--lsprof", dest="lsprof", metavar="FILE", default=None, \
+        group.add_option("--lsprof", metavar="FILE", default=None, \
-        group.add_option("--pidfile", dest="pidfile", metavar="FILE", \
+        group.add_option("--pidfile", metavar="FILE", \
-        group.add_option("-s", "--set", dest="set", action="append", default=[], metavar="NAME=VALUE", \
+        group.add_option("-s", "--set", action="append", default=[], metavar="NAME=VALUE", \
-        group.add_option("--set", dest="set", action="append", default=[], metavar="NAME=VALUE", \
+        group.add_option("-s", "--set", dest="set", action="append", default=[], metavar="NAME=VALUE", \
-SCHEDULER_DISK_QUEUE = 'scrapy.squeue.MarshalDiskQueue'
+SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleDiskQueue'
-import marshal
+import marshal, cPickle as pickle
-from scrapy.squeue import MarshalDiskQueue
+from scrapy.squeue import MarshalDiskQueue, PickleDiskQueue
-from scrapy.utils.project import sqlite_db
+from scrapyd.sqlite import JsonSqlitePriorityQueue
-from scrapy.utils.sqlite import SqlitePriorityQueue, JsonSqlitePriorityQueue, \
+from scrapyd.sqlite import SqlitePriorityQueue, JsonSqlitePriorityQueue, \
-        verifyObject(ISpiderContextStorage, SqliteSpiderContextStorage())
+    def test_get_crawl_args_with_settings(self):
-        filter.close()
+        filter.close('finished')
-        return self.df.close()
+        return self.df.close(reason)
-    def close(self): # can return a deferred
+    def close(self, reason): # can return a deferred
-    def close(self):
+    def close(self, reason):
-                    request.priority += depth * self.prio
+                    request.priority -= depth * self.prio
-            self.dqs.push(reqd, request.priority)
+            self.dqs.push(reqd, -request.priority)
-        self.mqs.push(request, request.priority)
+        self.mqs.push(request, -request.priority)
-        for s in spiders.list():
+        for s in self.crawler.spiders.list():
-        self.spiders = spman_cls.from_settings(self.settings)
+        self.spiders = spman_cls.from_crawler(self)
-from scrapy import log, signals
+from scrapy import signals
-                shell = IPython.embed(user_ns=namespace)
+                IPython.embed(user_ns=namespace)
-            shell()
+                shell()
-                shell = IPython.embed(user_ns=namespace)
+                IPython.embed(user_ns=namespace)
-            shell()
+                shell()
-    def __init__(self, download_func=None, crawler=None):
+    def __init__(self, download_func=None):
-        return cls(crawler=crawler)
+        try:
-            self.conn = connect_s3(aws_access_key_id, aws_secret_access_key)
+            self.conn = _S3Connection(aws_access_key_id, aws_secret_access_key)
-                '%s/%s' % (bucket, path))
+        signed_headers = self.conn.make_request(
-        return len(self.dqs) or len(self.mqs)
+        return len(self) > 0
-        return bool(self.queues)
+        slots = self.crawler.engine.slots
-            'est': lambda x: print_engine_status(self.crawler),
+            'est': lambda: print_engine_status(self.crawler.engine),
-        return self.dqs or self.mqs
+        return len(self.dqs) or len(self.mqs)
-        url = '%s://%s.s3.amazonaws.com%s' % (scheme, p.hostname, p.path)
+        bucket = p.hostname
-                '%s/%s' % (p.hostname, p.path))
+                '%s/%s' % (bucket, path))
-        self.assertEqual(urlparse('s3://bucket/key').netloc, 'bucket')
+    def test_s3_url(self):
-from urlparse import urlparse, uses_netloc
+from urlparse import urlparse, uses_netloc, uses_query
-    def __init__(self):
+    def __init__(self, crawler):
-                    crawler.engine.close_spider(spider, 'closespider_errorcount')
+                    self.crawler.engine.close_spider(spider, 'closespider_errorcount')
-            crawler.engine.close_spider(spider, 'closespider_pagecount')
+            self.crawler.engine.close_spider(spider, 'closespider_pagecount')
-            crawler.engine.close_spider, spider=spider, \
+            self.crawler.engine.close_spider, spider=spider, \
-            crawler.engine.close_spider(spider, 'closespider_itemcount')
+            self.crawler.engine.close_spider(spider, 'closespider_itemcount')
-        if not settings.getbool('ROBOTSTXT_OBEY'):
+    def __init__(self, crawler):
-            dfd = crawler.engine.download(robotsreq, spider)
+            dfd = self.crawler.engine.download(robotsreq, spider)
-        if not settings.getbool('MEMUSAGE_ENABLED'):
+    def __init__(self, crawler):
-        self.report = settings.getbool('MEMUSAGE_REPORT')
+        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY')
-                        (settings['BOT_NAME'], mem, socket.gethostname())
+                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())
-            crawler.stop()
+            self.crawler.stop()
-                        (settings['BOT_NAME'], mem, socket.gethostname())
+                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())
-    def __init__(self, download_func=None):
+    def __init__(self, download_func=None, crawler=None):
-            self.crawler = crawler
+        self.crawler = crawler
-        self._target = _crawler
+    def __init__(self, crawler):
-        JsonResource.__init__(self)
+    def __init__(self, crawler, spider_name=None):
-        status = get_engine_status(self._crawler.engine)
+        status = get_engine_status(self.crawler.engine)
-        return EngineStatusResource(name, self._crawler)
+        return EngineStatusResource(name, self.crawler)
-        self._target = _stats
+    def __init__(self, crawler):
-        self.settings = settings
+    def __init__(self, crawler):
-        self.middleware = DownloaderMiddlewareManager.from_settings(settings)
+        self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')
-        self.settings = settings
+    def __init__(self, crawler, spider_closed_callback):
-        self._concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')
+        self.scheduler_cls = load_object(self.settings['SCHEDULER'])
-    def __init__(self, engine, settings):
+    def __init__(self, crawler):
-        self.engine = engine
+        self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)
-            self.engine.close_spider(spider, exc.reason or 'cancelled')
+            self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')
-            self.engine.crawl(request=output, spider=spider)
+            self.crawler.engine.crawl(request=output, spider=spider)
-        self.extensions = ExtensionManager.from_settings(self.settings)
+        self.extensions = ExtensionManager.from_crawler(self)
-        self.engine = ExecutionEngine(self.settings, self._spider_closed)
+        self.engine = ExecutionEngine(self, self._spider_closed)
-    def from_settings(cls, settings):
+    def from_settings(cls, settings, crawler=None):
-                if hasattr(mwcls, 'from_settings'):
+                if crawler and hasattr(mwcls, 'from_crawler'):
-``install`` method, like this::
+--------- WARNING: THIS MODULE IS DEPRECATED -----------
-    crawler.install()
+This module is deprecated. If you want to get the Scrapy crawler from your
-After that, you can import the (singleton) crawler like this::
+For example:
-    from scrapy.project import crawler
+    @classmethod
-        if not settings.getbool('TELNETCONSOLE_ENABLED'):
+    def __init__(self, crawler):
-        self.host = settings['TELNETCONSOLE_HOST']
+        self.portrange = map(int, crawler.settings.getlist('TELNETCONSOLE_PORT'))
-            'extensions': crawler.extensions,
+            'engine': self.crawler.engine,
-            'est': print_engine_status,
+            'spiders': self.crawler.spiders,
-def get_engine_status(engine=None):
+def get_engine_status(engine):
-def print_engine_status(engine=None):
+def print_engine_status(engine):
-            from scrapy.project import crawler
+    def __init__(self, crawler):
-        self.spref = kw.pop('spref', None) or SpiderReferencer()
+        crawler = kw.pop('crawler', None)
-    json_encoder = ScrapyJSONEncoder()
+    def __init__(self, crawler, target=None):
-        JsonResource.__init__(self)
+    def __init__(self, crawler, target=None):
-        if not settings.getbool('WEBSERVICE_ENABLED'):
+    def __init__(self, crawler):
-            settings['WEBSERVICE_RESOURCES'])
+        self.crawler = crawler
-            res = res_cls()
+            res = res_cls(crawler)
-from scrapy.exceptions import UsageError
+from scrapy.exceptions import UsageError, ScrapyDeprecationWarning
-        DeprecationWarning, stacklevel=3)
+        ScrapyDeprecationWarning, stacklevel=3)
-            warnings.warn("CLOSESPIDER_ITEMPASSED setting is deprecated, use CLOSESPIDER_ITEMCOUNT instead", DeprecationWarning)
+            warnings.warn("CLOSESPIDER_ITEMPASSED setting is deprecated, use CLOSESPIDER_ITEMCOUNT instead", ScrapyDeprecationWarning)
-    DeprecationWarning, stacklevel=2)
+    ScrapyDeprecationWarning, stacklevel=2)
-                "use DEPTH_PRIORITY instead", DeprecationWarning)
+                "use DEPTH_PRIORITY instead", ScrapyDeprecationWarning)
-            "use CONCURRENT_REQUESTS_PER_DOMAIN instead", DeprecationWarning)
+            "use CONCURRENT_REQUESTS_PER_DOMAIN instead", ScrapyDeprecationWarning)
-            (c, attrname, c), DeprecationWarning, stacklevel=2)
+            (c, attrname, c), ScrapyDeprecationWarning, stacklevel=2)
-            DeprecationWarning, stacklevel=2)
+            ScrapyDeprecationWarning, stacklevel=2)
-            warnings.warn(message, category=DeprecationWarning, stacklevel=2)
+            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)
-        (cname, oldattr, version, cname, newattr), DeprecationWarning, stacklevel=3)
+        (cname, oldattr, version, cname, newattr), ScrapyDeprecationWarning, stacklevel=3)
-        _, self.type = self._root.tag.split('}', 1)
+        rt = self._root.tag
-                _, name = el.tag.split('}', 1)
+                tag = el.tag
-from scrapy.core.downloader.responsetypes import responsetypes
+from scrapy.responsetypes import responsetypes
-from scrapy.core.downloader.responsetypes import responsetypes
+from scrapy.responsetypes import responsetypes
-from scrapy.core.downloader.responsetypes import responsetypes
+from scrapy.responsetypes import responsetypes
-from scrapy.core.downloader.responsetypes import responsetypes
+from scrapy.responsetypes import responsetypes
-from scrapy.core.downloader.responsetypes import responsetypes
+from scrapy.responsetypes import responsetypes
-from scrapy.core.downloader.responsetypes import responsetypes
+from scrapy.responsetypes import responsetypes
-        mimedata = get_data(__package__, 'mime.types')
+        mimedata = get_data('scrapy', 'mime.types')
-from scrapy.core.downloader.responsetypes import responsetypes
+from scrapy.responsetypes import responsetypes
-        reactor.installResolver(CachingThreadedResolver(reactor))
+        if self.settings.getbool('DNSCACHE_ENABLED'):
-        return marshal.loads(super(MarshalDiskQueue, self).pop())
+        s = super(MarshalDiskQueue, self).pop()
-from scrapy.resolver import gethostbyname
+from scrapy.resolver import dnscache
-        key = urlparse_cached(request).hostname
+        key = urlparse_cached(request).hostname or ''
-        "len(engine.slots[spider].scheduler.mq)",
+        "len(engine.slots[spider].scheduler.dqs or [])",
-                r = Request(url=link.url, callback='_response_downloaded')
+                r = Request(url=link.url, callback=self._response_downloaded)
-    'JsonItemExporter']
+    'JsonItemExporter', 'MarshalItemExporter']
-            shell = IPython.Shell.IPShellEmbed(argv=[], user_ns=namespace)
+            try:
-        return len(self.dqs) + len(self.mqs)
+        return len(self.dqs) + len(self.mqs) if self.dqs else len(self.mqs)
-
+        def _on_error(failure):
-        dwld.addCallback(_on_success)
+        dwld.addCallbacks(_on_success, _on_error)
-        return False
+import warnings
-    def __init__(self, maxdepth, stats=None, verbose_stats=False):
+    def __init__(self, maxdepth, stats=None, verbose_stats=False, prio=1):
-        return cls(maxdepth, stats, verbose)
+        return cls(maxdepth, stats, verbose, prio)
-        return self._response_downloaded(response, self.parse_start_url, cb_kwargs={}, follow=True)
+        return self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)
-        for rule in self._rules:
+        for n, rule in enumerate(self._rules):
-                r.meta['link_text'] = link.text
+                r = Request(url=link.url, callback='_response_downloaded')
-    def _response_downloaded(self, response, callback, cb_kwargs, follow):
+    def _response_downloaded(self, response):
-from scrapy.utils.datatypes import PriorityQueue, PriorityStack
+from __future__ import with_statement
-        self.pending_requests = Queue()
+    def __init__(self, dupefilter, jobdir=None, dqclass=None):
-        return cls(dupefilter, dfo=dfo)
+        dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])
-        return bool(self.pending_requests)
+        return self.dqs or self.mqs
-            self.pending_requests.push(request, -request.priority)
+        if not request.dont_filter and self.df.request_seen(request):
-            return self.pending_requests.pop()[0]
+        return self.mqs.pop() or self._dqpop()
-        pass
+    def _dqpush(self, request):
-        pass
+    def _mqpush(self, request):
-                 cookies=None, meta=None, encoding='utf-8', priority=0.0,
+                 cookies=None, meta=None, encoding='utf-8', priority=0,
-DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'
+DUPEFILTER_CLASS = 'scrapy.dupefilter.RFPDupeFilter'
-SCHEDULER_ORDER = 'DFO'
+SCHEDULER_DISK_QUEUE = 'scrapy.squeue.MarshalDiskQueue'
-from scrapy.contrib.dupefilter import RequestFingerprintDupeFilter, NullDupeFilter
+from scrapy.dupefilter import RFPDupeFilter
-class RequestFingerprintDupeFilterTest(unittest.TestCase):
+class RFPDupeFilterTest(unittest.TestCase):
-        filter.open_spider(spider)
+        filter = RFPDupeFilter()
-        assert filter.request_seen(spider, r1)
+        assert not filter.request_seen(r1)
-        assert filter.request_seen(spider, r3)
+        assert not filter.request_seen(r2)
-        filter.close_spider(spider)
+        filter.close()
-from scrapy.utils.datatypes import PriorityQueue, PriorityStack, CaselessDict
+from scrapy.utils.datatypes import CaselessDict
-
+import unittest
-        "len(engine.scheduler.pending_requests[spider])",
+        "len(engine.slots[spider].scheduler.dq)",
-They must implement the following methods:
+They must implement the following method:
-* request_seen(spider, request, dont_record=False)
+* request_seen(request, dont_record=False)
-        pass
+class BaseDupeFilter(object):
-        pass
+    @classmethod
-    def request_seen(self, spider, request, dont_record=False):
+    def request_seen(self, request, dont_record=False):
-class RequestFingerprintDupeFilter(object):
+class RequestFingerprintDupeFilter(BaseDupeFilter):
-        del self.fingerprints[spider]
+        super(RequestFingerprintDupeFilter, self).__init__()
-    def request_seen(self, spider, request, dont_record=False):
+    def request_seen(self, request, dont_record=False):
-        if fp in self.fingerprints[spider]:
+        if fp in self.fingerprints:
-            self.fingerprints[spider].add(fp)
+            self.fingerprints.add(fp)
-    def __init__(self, start_requests, close_if_idle, nextcall):
+    def __init__(self, start_requests, close_if_idle, nextcall, scheduler):
-        self.scheduler = load_object(settings['SCHEDULER'])()
+        self.scheduler_cls = load_object(settings['SCHEDULER'])
-        request = self.scheduler.next_request(spider)
+        slot = self.slots[spider]
-        pending = self.scheduler.spider_has_pending_requests(spider)
+        pending = self.slots[spider].scheduler.has_pending_requests()
-        return self.scheduler.enqueue_request(spider, request)
+        return self.slots[spider].scheduler.enqueue_request(request)
-        slot = Slot(start_requests or (), close_if_idle, nextcall)
+        scheduler = self.scheduler_cls.from_settings(self.settings)
-        yield self.scheduler.open_spider(spider)
+        yield scheduler.open(spider)
-        dfd.addBoth(lambda _: self.scheduler.close_spider(spider))
+        dfd.addBoth(lambda _: slot.scheduler.close(reason))
-        self.dupefilter = load_object(settings['DUPEFILTER_CLASS'])()
+    def __init__(self, dupefilter, dfo=False):
-            return bool(self.pending_requests[spider])
+    @classmethod
-            raise RuntimeError('Scheduler spider already opened: %s' % spider)
+    def has_pending_requests(self):
-        return self.dupefilter.open_spider(spider)
+    def enqueue_request(self, request):
-        return self.dupefilter.close_spider(spider)
+    def next_request(self):
-        # the spider is being closed.
+    def open(self, spider):
-        return not self.pending_requests
+    def close(self, reason):
-    def __init__(self, concurrency, settings):
+    def __init__(self, concurrency, delay, settings):
-        self.delay = settings.getfloat('DOWNLOAD_DELAY')
+        self.delay = delay
-        key, slot = self._get_slot(request)
+        key, slot = self._get_slot(request, spider)
-    def _get_slot(self, request):
+    def _get_slot(self, request, spider):
-            self.slots[key] = Slot(concurrency, self.settings)
+            if self.ip_concurrency:
-                reactor.callLater(penalty, d.callback, spider, slot)
+                d.addCallback(self._process_queue, slot)
-from scrapy.conf import settings
+from scrapy.utils.reactor import CallLaterOnce
-        self.spider = spider
+    """Downloader slot"""
-        return len(self.active) > 2 * self.spider.max_concurrent_requests
+        return self.concurrency - len(self.transferring)
-        self.next_request_calls.clear()
+        if self.randomize_delay:
-    def __init__(self):
+    def __init__(self, settings):
-        slot = self.slots[spider]
+        key, slot = self._get_slot(request)
-        dfd = self.middleware.download(self._enqueue_request, request, spider)
+        dlfunc = partial(self._enqueue_request, slot=slot)
-
+    def needs_backout(self):
-                d.addBoth(lambda x: slot.next_request_calls.remove(call))
+                reactor.callLater(penalty, d.callback, spider, slot)
-        self.downloader = Downloader()
+        self.downloader = Downloader(self.settings)
-            or self.downloader.slots[spider].needs_backout() \
+            or self.downloader.needs_backout() \
-        return spider not in self.downloader.slots
+        downloading = bool(self.downloader.slots)
-        dwld = mustbe_deferred(self.downloader.fetch, request, spider)
+        dwld = self.downloader.fetch(request, spider)
-CONCURRENT_REQUESTS_PER_SPIDER = 8
+
-    'scrapy.contrib.throttle.AutoThrottle': 0,
+        "len(engine.downloader.active)",
-            self.methods['process_item'].append(func)
+        if hasattr(pipe, 'process_item'):
-    """
+    """Link objects represent an extracted link by the LinkExtractor."""
-    __slots__ = ['url', 'text', 'nofollow']
+    __slots__ = ['url', 'text', 'fragment', 'nofollow']
-    def __init__(self, url, text='', nofollow=False):
+    def __init__(self, url, text='', fragment='', nofollow=False):
-        return self.url == other.url and self.text == other.text and self.nofollow == other.nofollow
+        return self.url == other.url and self.text == other.text and \
-        return hash(self.url) ^ hash(self.text) ^ hash(self.nofollow)
+        return hash(self.url) ^ hash(self.text) ^ hash(self.fragment) ^ hash(self.nofollow)
-        return 'Link(url=%r, text=%r, nofollow=%r)' % (self.url, self.text, self.nofollow)
+        return 'Link(url=%r, text=%r, fragment=%r, nofollow=%r)' % \
-        self.assertEqual(hash(l1), hash(l3))
+        self._assert_same_links(l1, l1)
-        self.assertEqual(hash(l4), hash(l6))
+        self._assert_same_links(l4, l4)
-dnscache = LocalCache(10000) # XXX: make size a setting?
+dnscache = LocalCache(10000)
-        return result
+from scrapy.resolver import CachingThreadedResolver
-        dfd = self.middleware.download(self.enqueue, request, spider)
+        dfd = self.middleware.download(self._enqueue_request, request, spider)
-    def enqueue(self, request, spider):
+    def _enqueue_request(self, request, spider):
-        self._process_queue(spider)
+        self._process_queue(spider, slot)
-
+    def _process_queue(self, spider, slot):
-                call = reactor.callLater(penalty, d.callback, spider)
+                call = reactor.callLater(penalty, d.callback, spider, slot)
-            self._process_queue(spider)
+            self._process_queue(spider, slot)
-        return self._spiders[spider_name](**spider_kwargs)
+        try:
-        self.concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')
+        self._concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')
-        return len(self.downloader.slots) < self.downloader.concurrent_spiders
+        return len(self.slots) < self._concurrent_spiders
-                                   ConnectionLost
+                                   ConnectionLost, TCPTimedOutError
-                           ConnectionLost, PartialDownloadError, IOError)
+                           ConnectionLost, PartialDownloadError, TCPTimedOutError,
-        self.active.add(response)
+        self.active.add(request)
-        self.active.remove(response)
+    def finish_response(self, response, request):
-            slot.finish_response(response)
+            slot.finish_response(response, request)
-            return slot.nextcall.schedule(5)
+            slot.nextcall.schedule(5)
-            return self.slots[spider].nextcall.schedule(5)
+            self.slots[spider].nextcall.schedule(5)
-from twisted.internet import reactor, defer
+from twisted.internet import defer
-    def __init__(self, start_requests, close_if_idle):
+    def __init__(self, start_requests, close_if_idle, nextcall):
-        else:
+    def _next_request(self, spider):
-            return reactor.callLater(5, self.next_request, spider)
+            return slot.nextcall.schedule(5)
-            if not self._next_request(spider):
+            if not self._next_request_from_scheduler(spider):
-    def _next_request(self, spider):
+    def _next_request_from_scheduler(self, spider):
-        d.addBoth(lambda _: self.next_request(spider))
+        d.addBoth(lambda _: slot.nextcall.schedule())
-        d.addBoth(self.scraper.enqueue_scrape, request, spider)
+        d = self.scraper.enqueue_scrape(response, request, spider)
-        return self.downloader.slots.keys()
+        return self.slots.keys()
-        self.next_request(spider)
+        self.slots[spider].nextcall.schedule()
-            self.next_request(spider)
+            slot.nextcall.schedule()
-        self.slots[spider] = Slot(start_requests or (), close_if_idle)
+        nextcall = CallLaterOnce(self._next_request, spider)
-        self.next_request(spider)
+        slot.nextcall.schedule()
-            return
+            return self.slots[spider].nextcall.schedule(5)
-            return
+        slot = self.slots[spider]
-    for spider in set(engine.downloader.slots.keys() + engine.scraper.slots.keys()):
+    for spider in engine.slots.keys():
-DATABASE_NAME = ':memory:'
+DATABASES = {
-    """Simple class to keep information and state for each open spider"""
+class Slot(object):
-        self.sites = {}
+        self.slots = {}
-        site = self.sites[spider]
+        slot = self.slots[spider]
-        site.active.add(request)
+        slot.active.add(request)
-            site.active.remove(request)
+            slot.active.remove(request)
-        site = self.sites[spider]
+        slot = self.slots[spider]
-        site.queue.append((request, deferred))
+        slot.queue.append((request, deferred))
-        if not site:
+        """Effective download requests from slot queue"""
-        delay = site.download_delay()
+        delay = slot.download_delay()
-            if penalty > 0 and site.free_transfer_slots():
+            penalty = delay - now + slot.lastseen
-                d.addBoth(lambda x: site.next_request_calls.remove(call))
+                slot.next_request_calls.add(call)
-        site.lastseen = now
+        slot.lastseen = now
-            dfd = self._download(site, request, spider)
+        # Process enqueued requests if there are free slots to transfer for this slot
-    def _download(self, site, request, spider):
+    def _download(self, slot, request, spider):
-        site.transferring.add(request)
+        slot.transferring.add(request)
-            site.transferring.remove(request)
+            slot.transferring.remove(request)
-        self.sites[spider] = SpiderInfo(spider)
+        assert spider not in self.slots, "Spider already opened: %s" % spider
-        site.cancel_request_calls()
+        assert spider in self.slots, "Spider not opened: %s" % spider
-        return not self.sites
+        return not self.slots
-            or self.downloader.sites[spider].needs_backout() \
+            or self.downloader.slots[spider].needs_backout() \
-            and self.downloader.sites[spider].active
+        downloading = spider in self.downloader.slots \
-        return spider not in self.downloader.sites
+        return spider not in self.downloader.slots
-        return self.downloader.sites.keys()
+        return self.downloader.slots.keys()
-        return len(self.downloader.sites) < self.downloader.concurrent_spiders
+        return len(self.downloader.slots) < self.downloader.concurrent_spiders
-        "len(engine.downloader.sites)",
+        "len(engine.downloader.slots)",
-        "engine.downloader.sites[spider].lastseen",
+        "len(engine.downloader.slots[spider].queue)",
-    for spider in set(engine.downloader.sites.keys() + engine.scraper.slots.keys()):
+    for spider in set(engine.downloader.slots.keys() + engine.scraper.slots.keys()):
-            or self.scraper.sites[spider].needs_backout()
+            or self.scraper.slots[spider].needs_backout()
-            and self.scraper.sites[spider].is_idle()
+        scraper_idle = spider in self.scraper.slots \
-    """Object for holding data of the responses being scraped"""
+class Slot(object):
-        self.sites = {}
+        self.slots = {}
-        self.sites[spider] = SpiderInfo()
+        assert spider not in self.slots, "Spider already opened: %s" % spider
-        return site.closing
+        assert spider in self.slots, "Spider not opened: %s" % spider
-        return not self.sites
+        return not self.slots
-            site.closing.callback(spider)
+    def _check_if_closing(self, spider, slot):
-        if site is None:
+        slot = self.slots.get(spider, None)
-        dfd = site.add_response_request(response, request)
+        dfd = slot.add_response_request(response, request)
-            self._scrape_next(spider, site)
+            slot.finish_response(response)
-        self._scrape_next(spider, site)
+        self._scrape_next(spider, slot)
-            response, request, deferred = site.next_response_request_deferred()
+    def _scrape_next(self, spider, slot):
-            self.sites[spider].itemproc_size += 1
+            self.slots[spider].itemproc_size += 1
-        self.sites[spider].itemproc_size -= 1
+        self.slots[spider].itemproc_size -= 1
-        "len(engine.scraper.sites)",
+        "len(engine.scraper.slots)",
-        "engine.scraper.sites[spider].needs_backout()",
+        "len(engine.scraper.slots[spider].queue)",
-    for spider in set(engine.downloader.sites.keys() + engine.scraper.sites.keys()):
+    for spider in set(engine.downloader.sites.keys() + engine.scraper.slots.keys()):
-from scrapy.xlib.BeautifulSoup import UnicodeDammit
+from scrapy.http.response.dammit import UnicodeDammit
-    response_httprepr, get_cached_beautifulsoup, open_in_browser
+from scrapy.utils.response import body_or_str, response_httprepr, open_in_browser
-"""Beautiful Soup
+"""
-from scrapy.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest, NotConfigured
-        self.requests = iter(start_requests)
+        self.start_requests = iter(start_requests)
-        if slot.requests and not self._needs_backout(spider):
+        if slot.start_requests and not self._needs_backout(spider):
-                request = slot.requests.next()
+                request = slot.start_requests.next()
-                slot.requests = None
+                slot.start_requests = None
-            slot = self.slots[spider]
+        slot = self.slots[spider]
-                    self._spider_idle(spider)
+                slot.requests = None
-
+class ISpiderQueue(Interface):
-from scrapy.interfaces import ISpiderQueue
+from scrapyd.interfaces import ISpiderQueue
-from scrapy.spiderqueue import SqliteSpiderQueue
+from scrapyd.interfaces import ISpiderQueue
-from scrapy.spiderqueue import SqliteSpiderQueue
+from scrapyd.spiderqueue import SqliteSpiderQueue
-    # for the operations to complete
+        if not store_uri:
-        return "[options] <spider|url> ..."
+        return "[options] <spider>"
-                    q.append_url(url, spider)
+        if len(args) < 1:
-from scrapy import log
+from scrapy.utils.spider import create_spider_for_request
-            default_spider=BaseSpider('default'))
+            spider = self.crawler.spiders.create(opts.spider)
-        self.crawler.queue.append_request(request, spider)
+        self.crawler.crawl(spider, [request])
-        self.crawler.queue.append_spider(spider)
+
-    default_settings = {'KEEP_ALIVE': True}
+    default_settings = {'KEEP_ALIVE': True, 'LOGSTATS_INTERVAL': 0}
-    def __init__(self):
+    def __init__(self, start_requests, close_if_idle):
-            self._spider_idle(spider)
+            slot = self.slots[spider]
-    def open_spider(self, spider):
+    def open_spider(self, spider, start_requests=None, close_if_idle=True):
-        self.slots[spider] = Slot()
+        self.slots[spider] = Slot(start_requests or (), close_if_idle)
-        """Don't call this method. Use self.queue to start new spiders"""
+    def crawl(self, spider, requests=None):
-            self.engine.crawl(request, spider)
+        if requests is None:
-            self._start_next_spider()
+            self.stop()
-        self.crawler.engine.open_spider(spider)
+        self.crawler.engine.open_spider(spider, close_if_idle=False)
-        self.crawler.queue.append_spider(self.spider)
+        self.crawler.crawl(self.spider)
-        d.addBoth(self._remove_request, slot, request)
+        d.addBoth(self._downloaded, slot, request, spider)
-    def _remove_request(self, _, slot, request):
+    def _downloaded(self, response, slot, request, spider):
-        return _
+        return self.download(response, spider) \
-        slot = self.slots[request]
+        slot = self.slots[spider]
-        self.env['PYTHONPATH'] = os.path.dirname(scrapy.__path__[0])
+        if 'PYTHONPATH' in os.environ:
-from os.path import exists, join, dirname, abspath
+from os.path import exists, join, abspath
-        self.env['PYTHONPATH'] = dirname(scrapy.__path__[0])
+        if 'PYTHONPATH' in os.environ:
-from scrapy.exceptions import IgnoreRequest, DontCloseSpider
+from scrapy.exceptions import DontCloseSpider
-        d.addErrback(log.err, "Unhandled error on engine.crawl()", spider=spider)
+        d.addErrback(log.err, spider=spider)
-        dwld.addCallbacks(_on_success, _on_error)
+        dwld.addCallback(_on_success)
-from scrapy.exceptions import CloseSpider, IgnoreRequest, DropItem
+from scrapy.exceptions import CloseSpider, DropItem
-            return dfd.addErrback(self._check_propagated_failure, \
+            return dfd.addErrback(self._log_download_errors, \
-    def handle_spider_error(self, _failure, request, response, spider, propagated_failure=None):
+    def handle_spider_error(self, _failure, request, response, spider):
-        log.err(_failure, msg, spider=spider)
+        log.err(_failure, "Spider error processing %s" % request, spider=spider)
-        generated requests
+    def _log_download_errors(self, spider_failure, download_failure, request, spider):
-        if isinstance(spider_failure.value, IgnoreRequest):
+        if spider_failure is download_failure:
-            return spider_failure # exceptions raised in the spider code
+        return spider_failure
-        return u"Scraped from %s%s%s" % (response, os.linesep, item)
+        src = response.getErrorMessage() if isinstance(response, Failure) else response
-        setup_args['install_requires'] += ['simplejson']
+else:
-        self.retry_http_codes = map(int, settings.getlist('RETRY_HTTP_CODES'))
+        self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))
-            log.msg("Discarding %s (failed %d times): %s" % (request, retries, reason),
+            log.msg("Gave up retrying %s (failed %d times): %s" % (request, retries, reason),
-# contrib.middleware.retry.RetryMiddleware default settings
+RETRY_ENABLED = True
-RETRY_HTTP_CODES = ['500', '503', '504', '400', '408']
+RETRY_HTTP_CODES = [500, 503, 504, 400, 408]
-from scrapy.exceptions import IgnoreRequest, DropItem
+from scrapy.exceptions import CloseSpider, IgnoreRequest, DropItem
-    _encoding_re  = _template % ('encoding', r'(?P<charset>[\w-]+)')
+    _content2_re   = _template % ('charset', r'(?P<charset>[\w-]+)')
-    METATAG_RE2 = re.compile(r'<meta\s+%s\s+%s' % (_content_re, _httpequiv_re), re.I)
+    METATAG2_RE  = re.compile(r'<meta\s+%s' % _content2_re, re.I)
-        match = self.METATAG_RE.search(chunk) or self.METATAG_RE2.search(chunk)
+        match = self.METATAG_RE.search(chunk) or self.METATAG2_RE.search(chunk)
-        return "Crawled (%d) %s (referer: %s)%s" % (response.status, \
+        return u"Crawled (%d) %s (referer: %s)%s" % (response.status, \
-        return "Scraped from %s" % response + os.linesep + str(item)
+        return u"Scraped from %s%s%s" % (response, os.linesep, item)
-        return "Dropped: %s" % unicode(exception) + os.linesep + str(item)
+        return u"Dropped: %s%s%s" % (exception, os.linesep, item)
-
+        dfd.addBoth(lambda _: self.scheduler.close_spider(spider))
-    status = {'global': {}, 'spiders': {}}
+    status = {'global': [], 'spiders': {}}
-            status['global'][test] = eval(test)
+            status['global'] += [(test, eval(test))]
-            status['global'][test] = "%s (exception)" % type(e).__name__
+            status['global'] += [(test, "%s (exception)" % type(e).__name__)]
-        x = {}
+        x = []
-                x[test] = eval(test)
+                x += [(test, eval(test))]
-                x[test] = "%s (exception)" % type(e).__name__
+                x += [(test, "%s (exception)" % type(e).__name__)]
-    for test, result in status['global'].items():
+    for test, result in status['global']:
-        for test, result in tests.items():
+        for test, result in tests:
-                dfd = self._download(site, request, spider)
+            dfd = self._download(site, request, spider)
-        site.closing = defer.Deferred()
+        site = self.sites.pop(spider)
-        return site.closing
+class Slot(object):
-        self.closing_dfds = {} # dict (spider -> deferred) of spiders being closed
+        self.slots = {}
-        #schd.addErrback(log.err, "Error on engine.crawl()")
+        slot = self.slots[request]
-            return defer.succeed(None)
+
-        self.closing[spider] = reason
+
-        self.closing_dfds[spider] = dfd
+
-            spider=spider)
+        dfd.addErrback(log.err, spider=spider)
-        return dfd
+        dfd.addErrback(log.err, spider=spider)
-        return dlist
+        dfd.addBoth(lambda _: self._cancel_next_call(spider))
-            spider=spider)
+        dfd.addErrback(log.err, spider=spider)
-        dfd.addBoth(lambda _: self.closing_dfds.pop(spider).callback(spider))
+
-        "engine.scheduler.spider_has_pending_requests(spider)",
+        "engine.slots[spider].closing",
-        "engine.downloader.sites[spider].closing",
+  <url>
-            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'}])
+            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},
-                d[name] = el.text.strip()
+                d[name] = el.text.strip() if el.text else ''
-CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z]')
+CAMELCASE_INVALID_CHARS = re.compile('[^a-zA-Z\d]')
-    return ctype in ('application/x-gzip', 'applicatoin/gzip')
+    return ctype in ('application/x-gzip', 'application/gzip')
-            if not isinstance(response, XmlResponse):
+            if isinstance(response, XmlResponse):
-            s = Sitemap(response.body)
+
-        self.assertEqual(request.headers.get('Accept-Encoding'), 'gzip,deflate')
+        self.assertEqual(request.headers.get('Accept-Encoding'), 'x-gzip,gzip,deflate')
-        request.headers.setdefault('Accept-Encoding', 'gzip,deflate')
+        request.headers.setdefault('Accept-Encoding', 'x-gzip,gzip,deflate')
-        if encoding == 'gzip':
+        if encoding == 'gzip' or encoding == 'x-gzip':
-                    f.write(response.body)
+            request.meta['handle_httpstatus_all'] = True
-from scrapy.http import Request
+from scrapy.http import Request, XmlResponse
-            self._cbs.append((r, c))
+            self._cbs.append((regex(r), c))
-                    yield Request(sitemap['loc'], callback=self._parse_sitemap)
+                for loc in iterloc(s):
-                    loc = url['loc']
+                for loc in iterloc(s):
-        return proc(self._values[field_name])
+        try:
-        url = metadata.get('response_url') or metadata['url']
+        url = metadata.get('response_url')
-class RobotsTest(unittest.TestCase):
+    def test_sitemap_strip(self):
-                d[name] = el.text
+                d[name] = el.text.strip()
-from scrapy.contrib.spiders.init import InitSpider
+from scrapy.spider import BaseSpider
-class CrawlSpider(InitSpider):
+class CrawlSpider(BaseSpider):
-from scrapy.contrib.spiders.init import InitSpider
+from scrapy.spider import BaseSpider
-class XMLFeedSpider(InitSpider):
+class XMLFeedSpider(BaseSpider):
-        raise NotImplemented
+        raise NotImplementedError
-class CSVFeedSpider(InitSpider):
+class CSVFeedSpider(BaseSpider):
-    def parse_row(self, row):
+    def parse_row(self, response, row):
-        raise NotImplemented
+        raise NotImplementedError
-    for spider in engine.downloader.sites + engine.scraper.sites:
+    for spider in set(engine.downloader.sites.keys() + engine.scraper.sites.keys()):
-    check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)-r$(hg tip --template "{rev}")+$(date +%s) "Automatic build"', shell=True)
+    env={'SCRAPY_VERSION_FROM_HG': '1'}
-from scrapy.utils.request import request_fingerprint, request_deferred
+from scrapy.utils.request import request_fingerprint
-            dfd = request_deferred(request)
+            dfd = self.crawler.engine.download(request, info.spider)
-        d = self.download(request, spider)
+        d = self._download(request, spider)
-    for spider in engine.downloader.sites:
+    for spider in engine.downloader.sites + engine.scraper.sites:
-        self.slots = defaultdict(Slot)
+        self.slots = {}
-from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider
+from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider, SitemapSpider
-from scrapy.contrib.spiders.feed import XMLFeedSpider, CSVFeedSpider
+from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider
-    os.environ['SCRAPY_SETTINGS_MODULE'] = settings_module
+    os.environ.setdefault('SCRAPY_SETTINGS_MODULE', settings_module)
-            print self._cbs
+from scrapy.contrib.spiders.sitemap import SitemapSpider
-            settings.getint('CLOSESPIDER_ITEMPASSED') # XXX: legacy support
+        self.itemcount = settings.getint('CLOSESPIDER_ITEMCOUNT')
-        self.itempassed = settings.getint('CLOSESPIDER_ITEMPASSED')
+        self.itemcount = settings.getint('CLOSESPIDER_ITEMCOUNT') or \
-        if self.itempassed:
+        if self.itemcount:
-            crawler.engine.close_spider(spider, 'closespider_itempassed')
+        if self.counts[spider] == self.itemcount:
-CLOSESPIDER_ITEMPASSED = 0
+CLOSESPIDER_ITEMCOUNT = 0
-from scrapy.utils.engine import print_engine_status
+from scrapy.utils.engine import format_engine_status
-        print_engine_status()
+        msg = "Dumping stack trace and engine status" + os.linesep
-from os import listdir
+        parser.add_option("-e", "--edit", dest="edit", action="store_true",
-        for filename in sorted(listdir(self.templates_dir)):
+        for filename in sorted(os.listdir(self.templates_dir)):
-        self.assert_(env['SCRAPY_LOG_FILE'].endswith(os.path.join('mybot', 'myspider', 'ID2.log')))
+        self.assert_(env['SCRAPY_LOG_FILE'].endswith(os.path.join('mybot', 'myspider', 'ID.log')))
-        print os.linesep.join(spiders.list())
+        for s in spiders.list():
-        self.assert_(env['SCRAPY_LOG_FILE'].endswith('/mybot/myspider/ID.log'))
+        self.assert_(env['SCRAPY_LOG_FILE'].endswith(os.path.join('mybot', 'myspider', 'ID2.log')))
-    from .models import Person
+    from .models import Person, IdentifiedPerson
-                if model_field != cls._model_meta.pk:
+                if model_field.auto_created == False:
-        self.req = Request('scrapytest.org')
+        self.req = Request('http://scrapytest.org')
-        r1 = CustomRequest('example.com', 'http://www.example.com')
+        r1 = CustomRequest('http://www.example.com')
-        request = Request('url')
+        request = Request('http://url')
-        response = Response('url', body='')
+        request = Request('http://url')
-        request = Request('url')
+        request = Request('http://url')
-        req = Request('url1', meta=dict(response=rsp), callback=cb, errback=eb)
+        rsp = Response('http://url1')
-        req = Request('url1', meta=dict(response=fail), callback=cb, errback=eb)
+        req = Request('http://url1', meta=dict(response=fail), callback=cb, errback=eb)
-        req1 = Request('url1', meta=dict(response=rsp1))
+        rsp1 = Response('http://url1')
-        req2 = Request('url2', meta=dict(response=fail))
+        req2 = Request('http://url2', meta=dict(response=fail))
-        req = Request('url')
+        req = Request('http://url')
-        req2 = Request('url2')
+        req1 = Request('http://url1')
-        req1 = Request('url1', meta=dict(response=rsp1))
+        rsp1 = Response('http://url1')
-        req1 = Request('url1', meta=dict(response=rsp1))
+        rsp1 = Response('http://url1')
-        rsp1 = Response('url')
+        rsp1 = Response('http://url')
-        req1 = Request('url', meta=dict(response=rsp1_func))
+        req1 = Request('http://url', meta=dict(response=rsp1_func))
-        req = Request('url', meta=dict(result='ITSME', response=self.fail))
+        req = Request('http://url', meta=dict(result='ITSME', response=self.fail))
-        return "%s(%s)" % (self.__class__.__name__, args)
+    __repr__ = __str__
-
+    __repr__ = __str__
-        eggf = StringIO(d['egg'][0])
+        project = txrequest.args['project'][0]
-from cStringIO import StringIO
+from scrapy.utils.gz import gunzip
-            body = GzipFile(fileobj=StringIO(body)).read()
+            body = gunzip(body)
-        self.assertEqual(headers['Content-Type'], ['text/plain'])
+        assert 'Server: TwistedWeb' in out
-
+    def _print_headers(self, headers, prefix):
-            pprint.pprint(response.headers)
+            self._print_headers(response.request.headers, '>')
-from scrapy.utils.request import request_fingerprint
+from scrapy.utils.request import request_fingerprint, request_deferred
-        return crawler.engine.download
+        self.download_func = download_func
-        return mustbe_deferred(self.download, request, info).addCallbacks(
+        if self.download_func:
-            allowed_statuses = response.request.meta['handle_httpstatus_list']
+        meta = response.request.meta
-            return dwld
+        request = self.scheduler.next_request(spider)
-        schd = mustbe_deferred(self.schedule, request, spider)
+        self.schedule(request, spider)
-                return mustbe_deferred(self.schedule, response, spider)
+            return response
-from scrapy.core.schedulermw import SchedulerMiddlewareManager
+from scrapy.utils.misc import load_object
-        self.middleware = SchedulerMiddlewareManager.from_settings(settings)
+        self.dupefilter = load_object(settings['DUPEFILTER_CLASS'])()
-        return self.middleware.open_spider(spider)
+        return self.dupefilter.open_spider(spider)
-        return self.middleware.close_spider(spider)
+        return self.dupefilter.close_spider(spider)
-        return dfd
+        if request.dont_filter or not self.dupefilter.request_seen(spider, request):
-        self.pending_requests[spider].clear()
+        # TODO: flush queue here or discard enqueued requests, depending on how
-            return (None, None)
+        q = self.pending_requests[spider]
-
+from scrapy.utils.request import request_deferred
-        d = self.crawler.engine.schedule(request, spider)
+        d = request_deferred(request)
-                ['get_media_requests', 'media_to_download', 'download',
+                ['get_media_requests', 'media_to_download',
-                ['get_media_requests', 'media_to_download', 'download',
+                ['get_media_requests', 'media_to_download',
-from base64 import urlsafe_b64encode
+from twisted.internet.defer import Deferred
-from scrapy import log
+from scrapy.stats import stats
-            s += "%-30s : %d %s\n" % f
+            stats.set_value('memdebug/libxml2_leaked_bytes', self.libxml2.debugMemory(1))
-        log.msg(report)
+            for cls, wdict in live_refs.iteritems():
-    def _engine_stopped(self):
+    def engine_stopped(self):
-        self.assertEquals(self.stats.get_value('envinfo/request_depth_limit'), 1)
+import sys, os
-Scrapy extension for collecting scraping stats
+Extension for collecting core stats like items scraped and start/finish times
-        stats.inc_value('spiders_closed_count/%s' % reason)
+        stats.set_value('finish_reason', reason, spider=spider)
-
+from pprint import pformat
-        return "%s(%s)" % (self.__class__.__name__, values)
+        return pformat(dict(self))
-        return "Scraped %s in <%s>" % (item, response.url)
+        return "Scraped from %s" % response + os.linesep + str(item)
-        return "Dropped %s - %s" % (item, unicode(exception))
+        return "Dropped: %s" % unicode(exception) + os.linesep + str(item)
-        assert 'Item' in out
+        assert '{}' in out
-                         "TestItem(name=u'John Doe', number=123)")
+                         "{'name': u'John Doe', 'number': 123}")
-            u"Dropped {} - \u2018")
+        lines = self.formatter.dropped(item, exception, response, self.spider).splitlines()
-            dispatcher.connect(self.item_passed, signal=signals.item_passed)
+            dispatcher.connect(self.item_scraped, signal=signals.item_scraped)
-    def item_passed(self, item, spider):
+    def item_scraped(self, item, spider):
-        dispatcher.connect(self.item_passed, signals.item_passed)
+        dispatcher.connect(self.item_scraped, signals.item_scraped)
-    def item_passed(self, item, spider):
+    def item_scraped(self, item, spider):
-            dfd.addBoth(self._itemproc_finished, output, spider)
+            dfd = self.itemproc.process_item(output, spider)
-    def _itemproc_finished(self, output, item, spider):
+    def _itemproc_finished(self, output, item, response, spider):
-                log.msg(log.formatter.dropped(item, ex, spider), \
+                log.msg(log.formatter.dropped(item, ex, response, spider), \
-                item=output, spider=spider, output=output, original_item=item)
+            log.msg(log.formatter.scraped(output, response, spider), \
-        return "Scraped %s in <%s>" % (item, request.url)
+    def scraped(self, item, response, spider):
-    def dropped(self, item, exception, spider):
+    def dropped(self, item, exception, response, spider):
-item_passed = object()
+
-        self.assertEqual(self.formatter.dropped(item, exception, self.spider),
+        response = Response("http://www.example.com")
-        dfd.addErrback(self.handle_spider_error, request, spider)
+        dfd.addErrback(self.handle_spider_error, request, response, spider)
-    def handle_spider_error(self, _failure, request, spider, propagated_failure=None):
+    def handle_spider_error(self, _failure, request, response, spider, propagated_failure=None):
-        it = iter_errback(result, self.handle_spider_error, request, spider)
+        it = iter_errback(result, self.handle_spider_error, request, response, spider)
-            return
+from scrapy.exceptions import NotConfigured
-        if not settings.getbool('AUTOTHROTTLE_ENABLED', True):
+        if not settings.getbool('AUTOTHROTTLE_ENABLED'):
-        pass
+        self.pending_requests[spider].clear()
-        q.clear()
+        pass
-from scrapy.exceptions import IgnoreRequest
+from scrapy.utils.engine import print_engine_status
-        print "Got signal. Dumping stack trace..."
+        print_engine_status()
-            dfd.errback(Failure(IgnoreRequest()))
+        q.clear()
-            raise
+        return self._values[key]
-    age = Field(default=1)
+    age = Field()
-
+from docutils.parsers.rst.roles import set_classes
-        stats.inc_value('spider_count/%s' % reason, spider=spider)
+        stats.inc_value('spiders_closed_count/%s' % reason)
-from scrapy.utils.url import canonicalize_url, url_is_from_any_domain
+from scrapy.utils.url import canonicalize_url, url_is_from_any_domain, url_has_any_extension
-                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None):
+                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None,
-        links = [link for link in links if _is_valid_url(link.url)]
+        links = [x for x in links if self._link_allowed(x)]
-            links = [link for link in links if _matches(link.url, self.allow_res)]
+            allowed &= _matches(link.url, self.allow_res)
-            links = [link for link in links if not _matches(link.url, self.deny_res)]
+            allowed &= not _matches(link.url, self.deny_res)
-            links = [link for link in links if url_is_from_any_domain(link.url, self.allow_domains)]
+            allowed &= url_is_from_any_domain(parsed_url, self.allow_domains)
-        return links
+            allowed &= not url_is_from_any_domain(parsed_url, self.deny_domains)
-
+    def test_deny_extensions(self):
-    host = urlparse.urlparse(url).hostname
+    host = parse_url(url).hostname
-    scheme, netloc, path, params, query, fragment = urlparse.urlparse(url)
+    scheme, netloc, path, params, query, fragment = parse_url(url)
-    def __init__(self, maxdepth, stats=None):
+    def __init__(self, maxdepth, stats=None, verbose_stats=False):
-        return cls(maxdepth, stats)
+        return cls(maxdepth, stats, verbose)
-                        self.stats.set_value('request_depth_max', depth, spider=spider)
+                    if self.verbose_stats:
-            self.stats.inc_value('request_depth_count/0', spider=spider)
+            if self.verbose_stats:
-        self.mw = DepthMiddleware(1, self.stats)
+        self.mw = DepthMiddleware(1, self.stats, True)
-STATS_DUMP = False
+STATS_DUMP = True
-        spider = spclasses.pop()()
+        spider = spclasses.pop()(**opts.spargs)
-            latencies[:] = []
+            del latencies[:]
-    load. It has the following design goals:
+    load.
-    It adjusts download delays and concurrency based on the following rules:
+    This adjusts download delays and concurrency based on the following rules:
-    the HTTP headers.
+    average of previous download delay and the latency of the response.
-        self.deferred = defer.Deferred().addCallback(self._build_response)
+        self.start_time = time()
-    def _build_response(self, body):
+    def _build_response(self, body, request):
-                response=response, request=request, spider=spider)
+                send_catch_log(signal=signals.response_received, \
-        self.queue = []
+        self.queue = deque()
-        response, request, deferred = self.queue.pop(0)
+        response, request, deferred = self.queue.popleft()
-        self.queue = []
+        self.queue = deque()
-            request, deferred = site.queue.pop(0)
+            request, deferred = site.queue.popleft()
-
+        setattr_default(spider, 'download_delay', spider.settings.getfloat('DOWNLOAD_DELAY'))
-        return self.max_concurrent_requests - len(self.transferring)
+        return self.spider.max_concurrent_requests - len(self.transferring)
-        return len(self.active) > 2 * self.max_concurrent_requests
+        return len(self.active) > 2 * self.spider.max_concurrent_requests
-            return self._download_delay
+        delay = self.spider.download_delay
-            if penalty > 0:
+            if penalty > 0 and site.free_transfer_slots():
-        self.method = method.upper()
+        self.method = str(method).upper()
-        return [value]
+        if not hasattr(value, '__iter__'):
-import weakref
+    def test_encode_utf8(self):
-    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml', 'w3lib']
+    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml', 'w3lib', 'pyOpenSSL']
-import sys
+from w3lib.url import is_url
-from scrapy.utils.url import is_url
+from w3lib.form import encode_multipart
-from scrapy.utils.multipart import encode_multipart
+from w3lib.url import is_url
-from scrapy.utils.url import is_url
+from w3lib.url import is_url
-from scrapy.utils.http import basic_auth_header
+from w3lib.http import basic_auth_header
-from scrapy.utils.http import headers_dict_to_raw, headers_raw_to_dict
+from w3lib.url import urljoin_rfc
-
+from w3lib.url import file_uri_to_path
-from scrapy.utils.url import file_uri_to_path
+from w3lib.url import safe_url_string, urljoin_rfc
-
+from w3lib.url import urljoin_rfc
-from scrapy.utils.url import canonicalize_url, urljoin_rfc
+from scrapy.utils.url import canonicalize_url
-from scrapy.utils.markup import remove_tags, remove_entities, replace_escape_chars
+from w3lib.url import urljoin_rfc
-from scrapy.utils.url import safe_url_string, urljoin_rfc, canonicalize_url, url_is_from_any_domain
+from scrapy.utils.url import canonicalize_url, url_is_from_any_domain
-from scrapy.utils.url import safe_url_string, urljoin_rfc
+from w3lib.url import file_uri_to_path
-from scrapy.utils.url import file_uri_to_path
+from w3lib.http import headers_dict_to_raw
-from scrapy.utils.http import headers_dict_to_raw
+from w3lib.url import safe_url_string
-from scrapy.utils.url import safe_url_string
+from w3lib.url import any_to_uri
-from scrapy.utils.url import any_to_uri
+from cStringIO import StringIO
-from cStringIO import StringIO
+from w3lib.url import path_to_file_uri
-from scrapy.utils.url import path_to_file_uri
+from w3lib.url import path_to_file_uri
-from scrapy.utils.url import path_to_file_uri
+from w3lib.html import remove_entities
-from scrapy.utils.markup import remove_entities
+from w3lib.http import basic_auth_header
-    urljoin_rfc, url_is_from_spider, file_uri_to_path, path_to_file_uri, any_to_uri
+from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url
-from base64 import urlsafe_b64encode
+"""
-    to the dictionary.
+For new code, always import from w3lib.http instead of this module
-    return 'Basic ' + urlsafe_b64encode("%s:%s" % (username, password))
+from w3lib.http import *
-    'text' can be a unicode string or a regular string encoded as 'utf-8'
+Transitional module for moving to the w3lib library.
-        yield txt[offset:]
+For new code, always import from w3lib.html instead of this module
-    return ret_text
+from w3lib.html import *
-from cStringIO import StringIO
+"""
-    (filename, content) for file uploads.
+For new code, always import from w3lib.form instead of this module
-    return body.getvalue(), boundary
+from w3lib.form import *
-import re
+from w3lib import html
-    """ Return the base url of the given response used to resolve relative links. """
+    """Return the base url of the given response, joined with the response url"""
-        _baseurl_cache[response] = urljoin_rfc(response.url, match.group(1)) if match else response.url
+        text = response.body_as_unicode()[0:4096]
-    """
+    """Parse the http-equiv refrsh parameter from the given response"""
-        #_metaref_cache[response] = match.groups() if match else (None, None)
+        text = response.body_as_unicode()[0:4096]
-    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml']
+    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml', 'w3lib']
-                           ConnectionLost, PartialDownloadError)
+                           ConnectionLost, PartialDownloadError, IOError)
-        self._debug_cookie(request)
+        self._debug_cookie(request, spider)
-        self._debug_set_cookie(response)
+        self._debug_set_cookie(response, spider)
-        """log Cookie header for request"""
+    def _debug_cookie(self, request, spider):
-            log.msg('Cookie: %s for %s' % (c, request.url), level=log.DEBUG)
+            cl = request.headers.getlist('Cookie')
-        """log Set-Cookies headers but exclude cookie values"""
+    def _debug_set_cookie(self, response, spider):
-
+            if cl:
-    def __init__(self, file, include_headers_line=True, **kwargs):
+    def __init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs):
-
+from __future__ import with_statement
-        textf(region)
+        yield textf(region)
-&pounds;85.00
+&pound;85.00
-&pounds;100.00
+&pound;100.00
-&pounds;85.00
+&pound;85.00
-&pounds;100.00
+&pound;100.00
-            'price_before_discount': [u'&pounds;100.00'],
+            'description': ['\nSL342\n \nNice product for ladies\n \n&pound;85.00\n'],
-from scrapy.contrib.ibl.extractors import text
+from scrapy.contrib.ibl.extractors import text, html
-    __slots__ = ('name', 'description', 'extractor', 'required', 'allow_markup')
+    __slots__ = ('name', 'description', 'extractor', 'required')
-            allow_markup=False):
+    def __init__(self, name, description, extractor=text, required=False):
-            field.get('allow_markup', False))
+            field.get('ibl_extractor', text), field.get('required', False))
-from scrapy.contrib.ibl.htmlpage import HtmlTagType
+from scrapy.contrib.ibl.htmlpage import HtmlTagType, HtmlPageRegion
-class TokenType(object):
+class TokenType(HtmlTagType):
-    NON_PAIRED_TAG = HtmlTagType.UNPAIRED_TAG
+class PageRegion(object):
-        self.ignored_regions = ignored_regions or []
+        self.ignored_regions = [i if isinstance(i, PageRegion) else PageRegion(*i) \
-        return self.text[text_start:text_end]
+    __slots__ = ('htmlpage', 'token_page_indexes')
-        page_tokens list
+    def __init__(self, htmlpage, token_dict, page_tokens, token_page_indexes):
-        This assumes start_token_index <= end_token_index
+        Arguments:
-        return self.text[text_start:text_end]
+        Page.__init__(self, token_dict, page_tokens)
-        character)
+    def htmlpage_region(self, start_token_index, end_token_index):
-            for i in xrange(start_token_index, end_token_index)])
+        start = self.token_page_indexes[start_token_index]
-        corresponding token index.
+    def htmlpage_region_inside(self, start_token_index, end_token_index):
-        If the tag or attribute is not present, None is returned
+        This excludes the tokens at the specified indexes
-        return self.tag_attributes.get(token_index, {}).get(attribute) 
+        start = self.token_page_indexes[start_token_index] + 1
-                    self.text[start:follow])
+        for token, tindex in zip(self.page_tokens, self.token_page_indexes):
-                        self.tag_attributes)
+                % ('\n'.join(summary), self.htmlpage.body)
-class AnnotationTag(object):
+
-        self.end_index = end_index
+        PageRegion.__init__(self, start_index, end_index)
-        TemplatePage, ExtractionPage, AnnotationText, TokenDict)
+from scrapy.contrib.ibl.extraction.pageobjects import (AnnotationTag,
-        for data in html_page.parsed_body:
+        for index, data in enumerate(html_page.parsed_body):
-                self.handle_tag(data)
+                self.handle_tag(data, index)
-                self.handle_data(data)
+                self.handle_data(data, index)
-    def handle_data(self, html_data_fragment):
+    def handle_data(self, html_data_fragment, index):
-    def handle_tag(self, html_tag):
+    def handle_tag(self, html_tag, index):
-    def handle_tag(self, html_tag):
+    def handle_tag(self, html_tag, index):
-    def handle_data(self, html_data_fragment):
+    def handle_data(self, html_data_fragment, index):
-        self.tag_attrs = {}
+        self._page_token_indexes = []
-            self.tag_attrs[len(self.token_list) - 1] = html_tag.attributes
+    def handle_tag(self, html_tag, index):
-                self.token_start_index, self.token_follow_index, self.tag_attrs)
+        return ExtractionPage(self.html_page, self.token_dict, array(self.token_list), 
-from itertools import groupby
+from itertools import groupby, izip, starmap
-from scrapy.contrib.ibl.extraction.pageobjects import AnnotationTag, LabelledRegion
+from scrapy.contrib.ibl.extraction.pageobjects import (AnnotationTag,
-_ID = lambda x: x
+_EXTRACT_HTML = lambda x: x
-    return obj
+    return getattr(obj, 'annotation', obj)
-    >>> ex.extract(page, 0, 3, [LabelledRegion(1, 2)])
+    >>> ex.extract(page, 0, 3, [PageRegion(1, 2)])
-                self.allow_markup = False
+                self.content_validate = _EXTRACT_HTML
-                extractf = descriptor.extractor if descriptor else _ID
+                extractf = descriptor.extractor if descriptor else _EXTRACT_HTML
-        return [(self.annotation.surrounds_attribute, complete_data)] if complete_data else []
+        # extract content between annotation indexes
-            tag_value = extraction_page.tag_attribute(start_index, ta)
+            tag_value = extraction_page.htmlpage_tag(start_index).attributes.get(ta)
-                extracted = f(tag_value)
+                region = HtmlPageRegion(extraction_page.htmlpage, tag_value)
-            if self.content_validate != _ID:
+            messages.append(self.annotation.surrounds_attribute)
-                if f != _ID:
+                if f != _EXTRACT_HTML:
-        ignored_regions = [i if isinstance(i, LabelledRegion) else LabelledRegion(*i) for i in (ignored_regions or [])]
+        if ignored_regions is None:
-                        similar_ignored_regions.append(LabelledRegion(p, e))
+                        similar_ignored_regions.append(PageRegion(p, e))
-        text_end = page.token_start_indexes[end or -1]
+        text_start = page.htmlpage.parsed_body[page.token_page_indexes[start]].start
-                page.text[text_end:text_end+50].replace('\n', ' '))
+                page.htmlpage.body[text_start-50:text_start].replace('\n', ' '), 
-    body of text. It extracts based on the longest unique prefix and suffix.
+    """Data Extractor for extracting text fragments from an annotation page
-    >>> extractor.extract("by Marc Newson.")
+    >>> extractor.extract_text("by Marc Newson.")
-    >>> extractor.extract("by Marc Newson.")
+    >>> extractor.extract_text("by Marc Newson.")
-    >>> extractor.extract("by Marc Newson.")
+    >>> extractor.extract_text("by Marc Newson.")
-    >>> extractor.extract("y Marc Newson.") is None
+    >>> extractor.extract_text("y Marc Newson.") is None
-        """attempt to extract a substring from the text"""
+    def extract(self, region):
-from scrapy.utils.markup import remove_entities
+from scrapy.utils.markup import remove_entities, remove_comments
-        return stripped
+# tags to keep (only for attributes with markup)
-encapsulates page related information and prevents parsing multiple times.
+Container objects for representing html pages and their parts in the IBL
-    def __init__(self, url=None, headers=None, body=None, page_id=None):
+    """HtmlPage
-    body = property(lambda x: x._body, _set_body)
+    body = property(lambda x: x._body, _set_body, doc="raw html for the page")
-        image_url)
+        image_url, html, notags)
-    A('description', "The full description of the product", allow_markup=True),
+    A('description', "The full description of the product", html),
-    ('similar page extraction', [ANNOTATED_PAGE1], EXTRACT_PAGE1, None,
+    ('similar page extraction', [ANNOTATED_PAGE1], EXTRACT_PAGE1, DEFAULT_DESCRIPTOR,
-    ('multiple attributes and annotation', [ANNOTATED_PAGE2], EXTRACT_PAGE2, None,
+    ('multiple attributes and annotation', [ANNOTATED_PAGE2], EXTRACT_PAGE2, DEFAULT_DESCRIPTOR,
-    ('ambiguous description', [ANNOTATED_PAGE3], EXTRACT_PAGE3, None,
+    ('ambiguous description', [ANNOTATED_PAGE3], EXTRACT_PAGE3, DEFAULT_DESCRIPTOR,
-    ('repeated elements', [ANNOTATED_PAGE4], EXTRACT_PAGE4, None,
+    ('repeated elements', [ANNOTATED_PAGE4], EXTRACT_PAGE4, DEFAULT_DESCRIPTOR,
-    ('repeated identical variants', [ANNOTATED_PAGE5], EXTRACT_PAGE5, None,
+    ('repeated identical variants', [ANNOTATED_PAGE5], EXTRACT_PAGE5, DEFAULT_DESCRIPTOR,
-    ('irregular variants', [ANNOTATED_PAGE6], EXTRACT_PAGE6, None,
+    ('irregular variants', [ANNOTATED_PAGE6], EXTRACT_PAGE6, DEFAULT_DESCRIPTOR,
-#    ('variants in table columns', [ANNOTATED_PAGE7], EXTRACT_PAGE7, None,
+#    ('variants in table columns', [ANNOTATED_PAGE7], EXTRACT_PAGE7, DEFAULT_DESCRIPTOR,
-    'ignored_regions', [ANNOTATED_PAGE8], EXTRACT_PAGE8, None,
+    'ignored_regions', [ANNOTATED_PAGE8], EXTRACT_PAGE8, DEFAULT_DESCRIPTOR,
-             'description': [u'\n A very nice product for all intelligent people \n\n'],
+             'description': [u'\n A very nice product for all intelligent people \n \n'],
-    'shifted_ignored_regions', [ANNOTATED_PAGE9], EXTRACT_PAGE9, None,
+    'shifted_ignored_regions', [ANNOTATED_PAGE9], EXTRACT_PAGE9, DEFAULT_DESCRIPTOR,
-             'description': [u'\n A very nice product for all intelligent people \n\n'],
+             'description': [u'\n A very nice product for all intelligent people \n \n'],
-    'special_partial_annotation', [ANNOTATED_PAGE11], EXTRACT_PAGE11, None,
+    'special_partial_annotation', [ANNOTATED_PAGE11], EXTRACT_PAGE11, DEFAULT_DESCRIPTOR,
-            'description': [u'\nSL342\n \nNice product for ladies\n \n&pounds;85.00\n'],
+            'description': ['\nSL342\n \nNice product for ladies\n \n&pounds;85.00\n'],
-    'ignore-beneath', [ANNOTATED_PAGE12], EXTRACT_PAGE12a, None,
+    'ignore-beneath', [ANNOTATED_PAGE12], EXTRACT_PAGE12a, DEFAULT_DESCRIPTOR,
-    'ignore-beneath with extra tags', [ANNOTATED_PAGE12], EXTRACT_PAGE12b, None,
+    'ignore-beneath with extra tags', [ANNOTATED_PAGE12], EXTRACT_PAGE12b, DEFAULT_DESCRIPTOR,
-    ('nested annotation with replica outside', [ANNOTATED_PAGE13a], EXTRACT_PAGE13a, None,
+    ('nested annotation with replica outside', [ANNOTATED_PAGE13a], EXTRACT_PAGE13a, DEFAULT_DESCRIPTOR,
-    ('outside annotation with nested replica', [ANNOTATED_PAGE13b], EXTRACT_PAGE13b, None,
+    ('outside annotation with nested replica', [ANNOTATED_PAGE13b], EXTRACT_PAGE13b, DEFAULT_DESCRIPTOR,
-    ('consistency check', [ANNOTATED_PAGE14], EXTRACT_PAGE14, None,
+    ('consistency check', [ANNOTATED_PAGE14], EXTRACT_PAGE14, DEFAULT_DESCRIPTOR,
-          {'description': [u'Description\n\n'],
+    ('consecutive nesting', [ANNOTATED_PAGE15], EXTRACT_PAGE15, DEFAULT_DESCRIPTOR,
-    ('nested inside not found', [ANNOTATED_PAGE16], EXTRACT_PAGE16, None,
+    ('nested inside not found', [ANNOTATED_PAGE16], EXTRACT_PAGE16, DEFAULT_DESCRIPTOR,
-    ('ignored region helps to find attributes', [ANNOTATED_PAGE17], EXTRACT_PAGE17, None,
+    ('ignored region helps to find attributes', [ANNOTATED_PAGE17], EXTRACT_PAGE17, DEFAULT_DESCRIPTOR,
-    ('ignored region in partial annotation', [ANNOTATED_PAGE18], EXTRACT_PAGE18, None,
+    ('ignored region in partial annotation', [ANNOTATED_PAGE18], EXTRACT_PAGE18, DEFAULT_DESCRIPTOR,
-    ('repeated partial annotations with variants', [ANNOTATED_PAGE20], EXTRACT_PAGE20, None,
+    ('repeated partial annotations with variants', [ANNOTATED_PAGE20], EXTRACT_PAGE20, DEFAULT_DESCRIPTOR,
-    ('variants with swatches', [ANNOTATED_PAGE21], EXTRACT_PAGE21, None,
+    ('variants with swatches', [ANNOTATED_PAGE21], EXTRACT_PAGE21, DEFAULT_DESCRIPTOR,
-    ('variants with swatches complete', [ANNOTATED_PAGE22], EXTRACT_PAGE22, None,
+    ('variants with swatches complete', [ANNOTATED_PAGE22], EXTRACT_PAGE22, DEFAULT_DESCRIPTOR,
-    ('repeated (variants) with ignore annotations', [ANNOTATED_PAGE23], EXTRACT_PAGE23, None,
+    ('repeated (variants) with ignore annotations', [ANNOTATED_PAGE23], EXTRACT_PAGE23, DEFAULT_DESCRIPTOR,
-        assert ep.token_html(1) == '<p some-attr="foo">'
+        assert ep.htmlpage.fragment_data(ep.htmlpage_tag(0)) == '<html>'
-        assert ep.html_between_tokens(1, 3) == 'this is a test</p> '
+        assert ep.htmlpage_region_inside(1, 2) == 'this is a test'
-        self.conn = sqlite3.connect(self.database)
+        # about check_same_thread: http://twistedmatrix.com/trac/ticket/4040
-        self.conn = sqlite3.connect(self.database)
+        # about check_same_thread: http://twistedmatrix.com/trac/ticket/4040
-    __slots__ = ['url', 'text']
+    __slots__ = ['url', 'text', 'nofollow']
-    def __init__(self, url, text=''):
+    def __init__(self, url, text='', nofollow=False):
-    
+        return self.url == other.url and self.text == other.text and self.nofollow == other.nofollow
-        return hash(self.url) ^ hash(self.text)
+        return hash(self.url) ^ hash(self.text) ^ hash(self.nofollow)
-        return '<Link url=%r text=%r >' % (self.url, self.text)
+        return 'Link(url=%r, text=%r, nofollow=%r)' % (self.url, self.text, self.nofollow)
-    >>> ex.extract(page, 0, 3, [LabelledRegion(*(1,2))])
+    >>> ex.extract(page, 0, 3, [LabelledRegion(1, 2)])
-                        similar_ignored_regions.append(LabelledRegion(*(p, e)))
+                        similar_ignored_regions.append(LabelledRegion(p, e))
-        version = _get_version(target, opts)
+
-            _log("Building egg of %s-%s" % (project, version))
+
-        _upload_egg(target, egg, project, version)
+            _log("Writing egg to %s" % opts.build_egg)
-            "Spider %r not opened when crawling: %s" % (spider.name, request)
+        assert spider in self.open_spiders, \
-    return [safe_url_string(remove_entities(url(imgurl)))] if imgurl else None
+    return imgurl
-        yield xs.select('//' + nodename)[0]
+        yield xs.select(selxpath)[0]
-        self.assertEqual(str_to_unicode('a\xedb', 'utf-8', errors='replace'), u'a\ufffdb')
+        assert u'\ufffd' in str_to_unicode('a\xedb', 'utf-8', errors='replace')
-        (str_to_unicode('a\xedb', 'latin-', errors='replace'), u'a?b')
+        assert '?' in unicode_to_str(u'a\ufffdb', 'latin-1', errors='replace')
-            link.text = str_to_unicode(link.text, response_encoding)
+            link.text = str_to_unicode(link.text, response_encoding, errors='replace')
-            link.text = str_to_unicode(link.text, response_encoding)
+            link.text = str_to_unicode(link.text, response_encoding, errors='replace')
-def str_to_unicode(text, encoding=None):
+def str_to_unicode(text, encoding=None, errors='strict'):
-        return text.decode(encoding)
+        return text.decode(encoding, errors)
-def unicode_to_str(text, encoding=None):
+def unicode_to_str(text, encoding=None, errors='strict'):
-        return text.encode(encoding)
+        return text.encode(encoding, errors)
-def xmliter_lxml(obj, nodename):
+def xmliter_lxml(obj, nodename, namespace=None):
-    iterable = etree.iterparse(reader, tag=nodename, encoding=reader.encoding)
+    tag = '{%s}%s' % (namespace, nodename) if namespace else nodename
-        yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]
+        xs = XmlXPathSelector(text=nodetext)
-        self.assertEqual(attrs, 
+        self.assertEqual(attrs,
-        
+
-        
+
-_TAG = "<(\/?)(\w+(?::\w+)?)((?:\s+" + _ATTR + ")+\s*|\s*)(\/?)>?"
+_ATTR = "((?:[^=/<>\s]|/(?!>))+)(?:\s*=(?:\s*\"(.*?)\"|\s*'(.*?)'|([^>\s]+))?)?"
-            self.populate_vars(request.url, response, request, spider)
+            self.populate_vars(response, request, spider)
-        self.populate_vars(url, response, request, spider)
+        self.populate_vars(response, request, spider)
-    def populate_vars(self, url=None, response=None, request=None, spider=None):
+    def populate_vars(self, response=None, request=None, spider=None):
-        self.vars['item'] = item
+        self.vars['item'] = self.item_class()
-            self.vars['spider'] = spider
+        self.vars['spider'] = spider
-    inproject = bool(settings.settings_module)
+    inproject = inside_project()
-        print "\n".join(self.crawler.spiders.list())
+        spman_cls = load_object(settings['SPIDER_MANAGER_CLASS'])
-                    level=level, spider=spider)
+                    level=log.ERROR, spider=spider)
-                    log.msg(e)
+                    clsname = clspath.split('.')[-1]
-    d = abspath(join(dirname(scrapy_cfg), '.scrapy'))
+    if not inside_project():
-            warnings.warn("%r is not writable - using in-memory SQLite instead" % db)
+        try:
-        path = self.mktemp()
+        path = os.path.abspath(self.mktemp())
-
+from __future__ import with_statement
-_TAG = "<(\/?)(\w+(?::\w+)?)((?:\s+" + _ATTR + ")+\s*|\s*)(\/?)>"
+_TAG = "<(\/?)(\w+(?::\w+)?)((?:\s+" + _ATTR + ")+\s*|\s*)(\/?)>?"
-<img data-scrapy-annotate="{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}"src="image.jpg" />
+<img data-scrapy-annotate="{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;src&quot;: &quot;image_urls&quot;}}" src="image.jpg" />
-__version__ = "0.12.0"
+version_info = (0, 13, 0)
-__version__ = "0.11.0"
+version_info = (0, 12, 0)
-    return int(v[1]) * _vmvalue_scale[v[2]]
+    if sys.platform == "sunos5":
-from os import makedirs
+from os import makedirs, environ
-def get_spider_list(project, runner='scrapyd.runner'):
+def get_spider_list(project, runner=None):
-        return out.splitlines()
+import os, pkg_resources
-    def get_environment(self, message, slot, eggpath):
+    def get_environment(self, message, slot):
-        elif project in self.settings:
+        if project in self.settings:
-    def get_environment(message, slot, eggpath):
+    def get_environment(message, slot):
-from .interfaces import IPoller, IEggStorage, IEnvironment
+from .interfaces import IPoller, IEnvironment
-        self.egg_runner = config.get('egg_runner', 'scrapyd.eggrunner')
+        self.runner = config.get('runner', 'scrapyd.runner')
-            self.max_proc, self.egg_runner), system="Launcher")
+        log.msg("%s started: max_proc=%r, runner=%r" % (self.parent.name, \
-        args = [sys.executable, '-m', self.egg_runner, 'crawl']
+        args = [sys.executable, '-m', self.runner, 'crawl']
-        env = e.get_environment(msg, slot, eggpath)
+        env = e.get_environment(msg, slot)
-        pp = ScrapyProcessProtocol(eggpath, slot, project, msg['_spider'], \
+        pp = ScrapyProcessProtocol(slot, project, msg['_spider'], \
-        pp.deferred.addBoth(self._process_finished, eggpath, slot)
+        pp.deferred.addBoth(self._process_finished, slot)
-            os.remove(eggpath)
+    def _process_finished(self, _, slot):
-        self.eggfile = eggfile
+    def __init__(self, slot, project, spider, job, env):
-            self.spider, self.job, self.pid, self.eggfile, self.logfile)
+        msg += "project=%r spider=%r job=%r pid=%r log=%r" % (self.project, \
-class EggStorageTest(unittest.TestCase):
+class EnvironmentTest(unittest.TestCase):
-        env = self.environ.get_environment(msg, slot, '/path/to/file.egg')
+        env = self.environ.get_environment(msg, slot)
-import unittest
+from __future__ import with_statement
-from scrapyd.utils import get_crawl_args
+import os
-from .eggutils import get_spider_list_from_eggfile
+from .utils import get_spider_list
-        spiders = get_spider_list_from_eggfile(eggf, project)
+        spiders = get_spider_list(project)
-            eggrunner=self.root.egg_runner)
+        spiders = get_spider_list(project, runner=self.root.runner)
-        self.eggrunner = config.get('egg_runner')
+        self.runner = config.get('runner')
-from .similarity import common_prefix
+from .pageobjects import TokenDict
-            'match_common_prefix', 'metadata')
+            'metadata')
-            match_common_prefix=False):
+            annotation_text=None, tag_attributes=None, variant_id=None):
-                    text_region.follow_text).extract
+            region_extract = TextRegionDataExtractor(text_region.start_text, 
-    
+   
-<p data-scrapy-annotate="{&quot;variant&quot;: 0, &quot;common_prefix&quot;: true, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}"/>
+<p data-scrapy-annotate="{&quot;variant&quot;: 0, &quot;annotations&quot;: {&quot;content&quot;: &quot;price&quot;}}"/>
-copyright = u'2008-2010, Insophia'
+copyright = u'2008-2011, Insophia'
-    version = scrapy.__version__
+    version = '.'.join(map(str, scrapy.version_info[:2]))
-release = version
+    release = ''
-        self.mimetypes = MimeTypes([mimefile])
+        self.mimetypes = MimeTypes()
-from scrapy.http import XmlResponse, TextResponse
+from scrapy.http import XmlResponse, TextResponse, Response
-from scrapy.http import Response
+from scrapy.http import TextResponse
-    encoding = obj.encoding if isinstance(obj, Response) else encoding or 'utf-8'
+    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or 'utf-8'
-            egg = open(opts.egg, 'rb')
+            egg = opts.egg
-def _upload_egg(target, eggfile, project, version):
+def _upload_egg(target, eggpath, project, version):
-        'egg': ('project.egg', eggfile.read()),
+        'egg': ('project.egg', eggdata),
-    check_call([sys.executable, 'setup.py', 'bdist_egg', '-d', d], stdout=f)
+    check_call([sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=f)
-    return open(egg, 'rb'), d
+    return egg, d
-                item=item, spider=spider, output=output)
+                item=output, spider=spider, output=output, original_item=item)
-version_info = (0, 11, 0, 'dev')
+version_info = (0, 11, 0)
-    version = "%s.r%s" % (version, rev)
+    with open('scrapy/__init__.py', 'a') as f:
-        print "Scrapy %s" % scrapy.__version__
+        if opts.verbose:
-from twisted.internet.defer import DeferredQueue
+from twisted.internet.defer import DeferredQueue, inlineCallbacks, maybeDeferred, returnValue
-                return self.dq.put(self._message(msg, p))
+            c = yield maybeDeferred(q.count)
-    path = urllib.quote(urllib.unquote(path))
+    path = safe_url_string(urllib.unquote(path))
-        return "Control the spider queue"
+        return "Deprecated command. See Scrapyd documentation."
-        return "Start Scrapy in server mode"
+        return "Deprecated command. Use 'server' command instead"
-        self.putChild('', Home())
+        self.putChild('', Home(self))
-"""
+""" % vars
-        sources += closest_scrapy_cfg()
+        scrapy_cfg = closest_scrapy_cfg()
-    def __init__(self, values=None):
+    def __init__(self, values=None, extra_sources=()):
-    config = Config()
+def get_application(config=None):
-    return appfunc()
+    return appfunc(config)
-def application():
+def application(config):
-            self.spider, self.job, self.pid, self.eggfile)
+        msg += "project=%r spider=%r job=%r pid=%r egg=%r log=%r" % (self.project, \
-        project = message['project']
+        project = message['_project']
-            message['spider'])
+        logsdir = os.path.join(self.logs_dir, message['_project'], \
-        return os.path.join(logsdir, "%s.log" % message['_id'])
+        return os.path.join(logsdir, "%s.log" % message['_job'])
-        IEnvironment.get_environment().
+        The message is a dict containing (at least):
-           _id is a unique identifier for this run)
+        `message` is the message received from the IPoller.next() method
-        project = msg['project']
+        project = msg['_project']
-        pp = ScrapyProcessProtocol(eggpath, slot, project, msg['spider'], msg['_id'])
+        env = stringify_dict(env, keys_only=False)
-    def __init__(self, eggfile, slot, project, spider, job):
+    def __init__(self, eggfile, slot, project, spider, job, env):
-        d['spider'] = d.pop('name')
+        d['_project'] = project
-        msg = {'project': 'mybot', 'spider': 'myspider', '_id': 'ID'}
+        msg = {'_project': 'mybot', '_spider': 'myspider', '_job': 'ID'}
-        msg = {'project': 'newbot', 'spider': 'myspider', '_id': 'ID'}
+        msg = {'_project': 'newbot', '_spider': 'myspider', '_job': 'ID'}
-        self.failUnlessEqual(d2.result, {'project': 'mybot2', 'spider': 'spider2'})
+        self.failUnlessEqual(d1.result, {'_project': 'mybot1', '_spider': 'spider1'})
-        msg = {'project': 'lolo', 'spider': 'lala'}
+        msg = {'_project': 'lolo', '_spider': 'lala'}
-        msg = {'project': 'lolo', 'spider': 'lala', 'arg1': u'val1'}
+        msg = {'_project': 'lolo', '_spider': 'lala', 'arg1': u'val1'}
-    del msg['project'], msg['spider']
+    args = [unicode_to_str(msg['_spider'])]
-        args['_id'] = jobid
+        args['_job'] = jobid
-<li><a href="logs/">Logs</li>
+<li><a href="/procmon">Process monitor</a></li>
-        s += "<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th>"
+        s += "<th>Project</th><th>Spider</th><th>Job</th><th>PID</th><th>Runtime</th><th>Log</th>"
-from .webservice import Root
+from .website import Root
-        env['SCRAPY_LOG_FILE'] = logpath
+        env['SCRAPY_LOG_FILE'] = self._get_log_file(message)
-        `message` is the message received from the IPoller.next()
+        `message` is the message received from the IPoller.next() augmented to
-from scrapy.utils.python import unicode_to_str
+from scrapy.utils.python import stringify_dict
-            self.max_proc = cpu_count() * config.getint('max_proc_per_cpu')
+            self.max_proc = cpu_count() * config.getint('max_proc_per_cpu', 4)
-        spider = unicode_to_str(message['spider'])
+        msg = stringify_dict(message, keys_only=False)
-        args += get_crawl_args(message)
+        args += get_crawl_args(msg)
-        pp = ScrapyProcessProtocol(eggpath, slot, project, spider)
+        env = e.get_environment(msg, slot, eggpath)
-    def __init__(self, eggfile, slot, project, spider):
+    def __init__(self, eggfile, slot, project, spider, job):
-            self.spider, self.slot, self.pid, self.eggfile)
+        msg += "project=%r spider=%r job=%r pid=%r egg=%r" % (self.project, \
-        msg = {'project': 'mybot'}
+        msg = {'project': 'mybot', 'spider': 'myspider', '_id': 'ID'}
-        self.assert_(env['SCRAPY_LOG_FILE'].endswith('slot3.log'))
+        self.assert_(env['SCRAPY_LOG_FILE'].endswith('/mybot/myspider/ID.log'))
-        msg = {'project': 'newbot'}
+        msg = {'project': 'newbot', 'spider': 'myspider', '_id': 'ID'}
-        self.assert_(env['SCRAPY_LOG_FILE'].endswith('slot3.log'))
+        self.assert_(env['SCRAPY_LOG_FILE'].endswith('/newbot/myspider/ID.log'))
-        msg = {'project': 'lolo', 'spider': 'lala', 'arg1': 'val1'}
+        msg = {'project': 'lolo', 'spider': 'lala', 'arg1': u'val1'}
-    for k, v in stringify_dict(msg).items():
+    for k, v in stringify_dict(msg, keys_only=False).items():
-        return {"status": "ok"}
+        jobid = uuid.uuid1().hex
-        eggstorage.put(eggf, project, version)
+        self.root.eggstorage.put(eggf, project, version)
-        projects = self.root.app.getComponent(ISpiderScheduler).list_projects()
+        projects = self.root.scheduler.list_projects()
-        versions = eggstorage.list(project)
+        versions = self.root.eggstorage.list(project)
-        _, eggf = eggstorage.get(project)
+        _, eggf = self.root.eggstorage.get(project)
-        eggstorage.delete(project, version)
+        self.root.eggstorage.delete(project, version)
-        self.app.getComponent(ISpiderScheduler).update_projects()
+from datetime import datetime
-        return {"status": "ok", "spiders": len(spiders)}
+        return {"status": "ok", "project": project, "version": version, \
-        self.max_proc = config.getint('max_proc', 0) or cpu_count()
+        self.max_proc = config.getint('max_proc', 0)
-        project = message['project']
+        project = unicode_to_str(message['project'])
-        pp = ScrapyProcessProtocol(eggpath, slot)
+        pp = ScrapyProcessProtocol(eggpath, slot, project, spider)
-    def __init__(self, eggfile, slot):
+    def __init__(self, eggfile, slot, project, spider):
-        msg += "slot=%r pid=%r egg=%r" % (self.slot, self.pid, self.eggfile)
+        msg += "project=%r spider=%r slot=%r pid=%r egg=%r" % (self.project, \
-                return self.dq.put(self._message(p))
+                msg = q.pop()
-        return {'project': str(project)}
+    def _message(self, queue_msg, project):
-        self.failUnlessEqual(d2.result, {'project': 'mybot2'})
+        self.failUnlessEqual(d1.result, {'project': 'mybot1', 'spider': 'spider1'})
-        return {"status": "ok", "spiders": spiders}
+        return {"status": "ok", "spiders": len(spiders)}
-        reactor.spawnProcess(pp, cmd[0], cmd, env=env)
+        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)
-        reqstr = txrequest.content.read()
+        reqstr = txrequest.content.getvalue()
-    def _extract_links(self, response_text, response_url, response_encoding):
+    def _extract_links(self, response_text, response_url, response_encoding, base_url=None):
-        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url
+        if base_url is None:
-        links = self._extract_links(html, response.url, response.encoding)
+        links = self._extract_links(html, response.url, response.encoding, base_url)
-    __slots__ = ['_url', 'headers', 'status', '_body', 'request', '_meta', \
+    __slots__ = ['_url', 'headers', 'status', '_body', 'request', \
-    def __init__(self, url, status=200, headers=None, body='', meta=None, flags=None):
+    def __init__(self, url, status=200, headers=None, body='', flags=None, request=None):
-        self.request = None
+        self.request = request
-        return self._meta
+        try:
-        attrs = ['url', 'status', 'body', 'headers', 'meta', 'flags']
+        attrs = ['url', 'status', 'body', 'headers', 'request', 'flags']
-        for x in ['url', 'status', 'headers', 'body', 'meta', 'flags']:
+        for x in ['url', 'status', 'headers', 'body', 'request', 'flags']:
-        self._encoding = encoding
+    def __init__(self, *args, **kwargs):
-        super(TextResponse, self).__init__(url, status, headers, body, meta, flags)
+        super(TextResponse, self).__init__(*args, **kwargs)
-from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers
+from scrapy.http import Request, Response, TextResponse, HtmlResponse, XmlResponse, Headers
-        r = self.response_class("http://www.example.com", meta=meta, headers=headers, body=body)
+        r = self.response_class("http://www.example.com", headers=headers, body=body)
-
+    def test_copy_meta(self):
-        r4 = r3.replace(body='', meta={}, flags=[])
+        r3 = self.response_class("http://www.example.com", flags=['cached'])
-        self.assertEqual(r4.meta, {})
+import sys, os, glob
-__version__ = "0.11"
+__version__ = "0.11.0"
-        import scrapy.xlib.simplejson as json
+    import simplejson as json
-    main()
+    if sys.version_info < (2, 6):
-        msg = self._queue.pop()
+        msg = yield self._queue.pop()
-        except (netrc.NetrcParseError, TypeError):
+        except (netrc.NetrcParseError, IOError, TypeError):
-    return urlparse.urlunparse((scheme, netloc, path, params, query, fragment))
+    return urlparse.urlunparse((scheme, netloc.lower(), path, params, query, fragment))
-        ignored_regions = [LabelledRegion(*i) for i in (ignored_regions or [])]
+        ignored_regions = [i if isinstance(i, LabelledRegion) else LabelledRegion(*i) for i in (ignored_regions or [])]
-        self.vars = {}
+        self.vars = {}
-from scrapy.http import Request, Response, TextResponse
+from scrapy.http import Request, Response, HtmlResponse, XmlResponse
-            if isinstance(response, TextResponse):
+            if isinstance(response, XmlResponse):
-        return "[options] [ <target:project> | -l <target> | -L ]"
+        return "[options] [ [target] | -l | -L <target> ]"
-        return "Deploy project in Scrapyd server"
+        return "Deploy project in Scrapyd target"
-            "(aka target) and project."
+            "(known as target)"
-        parser.add_option("-L", "--list-targets", action="store_true", \
+        parser.add_option("-l", "--list-targets", action="store_true", \
-        parser.add_option("-l", "--list-projects", metavar="TARGET", \
+        parser.add_option("-L", "--list-projects", metavar="TARGET", \
-        version = _get_version(opts)
+        target_name = _get_target_name(args)
-            _log("Bulding egg of %s-%s" % (project, version))
+            _log("Building egg of %s-%s" % (project, version))
-    sys.stderr.write("%s\n" % message)
+    sys.stderr.write(message + os.linesep)
-        target_name, project = args[0].split(':', 1)
+def _get_target_name(args):
-    return target, project
+        return 'default'
-    targets = _DEFAULT_TARGETS.copy()
+    baset = dict(cfg.items('deploy')) if cfg.has_section('deploy') else {}
-            targets[x[7:]] = dict(cfg.items(x))
+        if x.startswith('deploy:'):
-    if opts.version == 'HG':
+def _get_version(target, opts):
-        return opts.version
+    elif version:
-        return "Control execution queue"
+        return "Control the spider queue"
-from tempfile import NamedTemporaryFile, mkdtemp
+from tempfile import NamedTemporaryFile
-        shutil.rmtree(tmpdir)
+    with NamedTemporaryFile(suffix='.egg') as f:
-    tmpdir = mkdtemp()
+    tmpdir = mkdtemp(prefix='eggs-%s-' % project)
-    d = pkg_resources.find_distributions(eggpath).next()
+    try:
-        spiders = get_spider_list_from_eggfile(eggf, project)
+        spiders = get_spider_list_from_eggfile(eggf, project, \
-def get_spider_list_from_eggfile(eggfile, project):
+def get_spider_list_from_eggfile(eggfile, project, eggrunner='scrapyd.eggrunner'):
-            pargs = [sys.executable, '-m', 'scrapyd.eggrunner', 'list']
+            pargs = [sys.executable, '-m', eggrunner, 'list']
-                raise RuntimeError(msg)
+                msg = err or out or 'unknown error'
-            out = proc.communicate()[0]
+            proc = Popen(pargs, stdout=PIPE, stderr=PIPE, cwd=tmpdir, env=env)
-            egg = _build_egg()
+            egg, tmpdir = _build_egg()
-        shutil.rmtree(d)
+    f = tempfile.TemporaryFile(dir=d)
-            cls = self.from_url(url)
+            cls = self.from_filename(url)
-        self.cachedir = cachedir
+        self.cachedir = data_path(settings['HTTPCACHE_DIR'])
-        if 0 <= self.expiration_secs < time() - mtime:
+        if 0 < self.expiration_secs < time() - mtime:
-HTTPCACHE_DIR = ''
+HTTPCACHE_ENABLED = False
-        storage = self._get_storage(HTTPCACHE_EXPIRATION_SECS=-1)
+        storage = self._get_storage(HTTPCACHE_EXPIRATION_SECS=0)
-def expand_data_path(path):
+def data_path(path):
-    return join(project_data_dir(), path)
+    return path if isabs(path) else join(project_data_dir(), path)
-        db = expand_data_path(path)
+        db = data_path(path)
-            log.DEBUG)
+        self.port = listen_tcp(self.portrange, self.host, self)
-def listen_tcp(portrange, factory):
+def listen_tcp(portrange, host, factory):
-        return reactor.listenTCP(portrange, factory)
+        return reactor.listenTCP(portrange, factory, interface=host)
-        return reactor.listenTCP(0, factory)
+        return reactor.listenTCP(0, factory, interface=host)
-        return reactor.listenTCP(portrange[0], factory)
+        return reactor.listenTCP(portrange[0], factory, interface=host)
-            return reactor.listenTCP(x, factory)
+            return reactor.listenTCP(x, factory, interface=host)
-            log.DEBUG)
+        self.port = listen_tcp(self.portrange, self.host, self)
-from scrapy import log, signals
+from scrapy.utils.project import sqlite_db
-                "instead. Error was: %r" % (database, str(e)), log.WARNING)
+        self.d = self.sqlite_dict_class(database, table)
-        return cls(settings['SQLITE_DB'])
+        return cls(sqlite_db(settings['SQLITE_DB']))
-from scrapy import log
+from scrapy.utils.project import sqlite_db
-                "instead. Error was: %r" % (database, str(e)), log.WARNING)
+        self.q = JsonSqlitePriorityQueue(database, table)
-        return cls(settings['SQLITE_DB'])
+        return cls(sqlite_db(settings['SQLITE_DB']))
-
+        parser.add_option("--egg", metavar="FILE",
-        egg = _build_egg()
+        if opts.egg:
-            log.msg(log.formatter.passed(item, spider), log.INFO, spider=spider)
+            log.msg(log.formatter.passed(output, spider), log.INFO, spider=spider)
-        _log("Server response (%s):" % f.getcode())
+        _log("Server response (%s):" % f.code)
-        _log("Deploy failed (%s):" % e.getcode())
+        _log("Deploy failed (%s):" % e.code)
-                encoding=unicode).strip()
+                encoding=unicode)
-            return unicode(self.root).strip()
+            return unicode(self.root)
-        self.assertEqual(divtwo.select("//li").extract(),
+        self.assertEqual(map(unicode.strip, divtwo.select("//li").extract()),
-        self.assertEqual(divtwo.select("./ul/li").extract(),
+        self.assertEqual(map(unicode.strip, divtwo.select("./ul/li").extract()),
-        self.assertEqual(divtwo.select(".//li").extract(),
+        self.assertEqual(map(unicode.strip, divtwo.select(".//li").extract()),
-            raise NotConfigured
+            self.libxml2 = None
-        self.libxml2.debugMemory(1)
+        if self.libxml2:
-        figures.append(("libxml2 memory leak", self.libxml2.debugMemory(1), "bytes"))
+        if self.libxml2:
-import weakref
+import weakref, os
-def print_live_refs(ignore=NoneType):
+def format_live_refs(ignore=NoneType):
-    print
+        return "The trackref module is disabled. Use TRACK_REFS setting to enable it."
-            now-oldest)
+        s += "%-30s %6d   oldest: %ds ago" % (cls.__name__, len(wdict), \
-        self._download_func = download_func or self._default_download_func()
+        self._cached_download_func = download_func
-This is a middleware to respect robots.txt policies. To active it you must
+This is a middleware to respect robots.txt policies. To activate it you must
-from scrapy import signals
+from scrapy import signals, log
-            raise IgnoreRequest("URL forbidden by robots.txt: %s" % request.url)
+            log.msg("Forbidden by robots.txt: %s" % request, log.DEBUG)
-from scrapy.tests.test_selector import XPathSelectorTestCase
+from scrapy.tests import test_selector
-class XPathSelectorTestCase(XPathSelectorTestCase):
+class Libxml2XPathSelectorTestCase(test_selector.XPathSelectorTestCase):
-from scrapy.tests.test_selector import XPathSelectorTestCase
+from scrapy.tests import test_selector
-class XPathSelectorTestCase(XPathSelectorTestCase):
+class LxmlXPathSelectorTestCase(test_selector.XPathSelectorTestCase):
-from scrapy.selector import XPathSelectorList, HtmlXPathSelector
+from scrapy.selector.libxml2sel import XPathSelectorList, HtmlXPathSelector
-import re
+"""
-from scrapy.selector import XmlXPathSelector, HtmlXPathSelector, \
+from scrapy.selector.libxml2sel import XmlXPathSelector, HtmlXPathSelector, \
-                         [u'<root>lala</root>'])
+class XPathSelectorTestCase(XPathSelectorTestCase):
-# TODO: we should merge these tests with test_selector_libxml2.py
+"""
-nolxml = False
+from scrapy.http import TextResponse, XmlResponse
-
+    has_lxml = False
-class XPathSelectorTestCase(unittest.TestCase):
+class XPathSelectorTestCase(XPathSelectorTestCase):
-    if nolxml:
+    if has_lxml:
-        from simplejson._speedups import make_encoder
+        from scrapy.xlib.simplejson._speedups import make_encoder
-from simplejson.scanner import make_scanner
+from scrapy.xlib.simplejson.scanner import make_scanner
-        from simplejson._speedups import scanstring
+        from scrapy.xlib.simplejson._speedups import scanstring
-        from simplejson import _speedups
+        from scrapy.xlib.simplejson import _speedups
-from simplejson.decoder import PosInf
+from scrapy.xlib.simplejson.decoder import PosInf
-        from simplejson._speedups import make_scanner
+        from scrapy.xlib.simplejson._speedups import make_scanner
-        elif result:
+        else:
-    def __init__(self, config):
+    def __init__(self, config, initenv=os.environ):
-        env = os.environ.copy()
+        env = self.initenv.copy()
-        self.environ = Environment(config)
+        self.environ = Environment(config, initenv={})
-    __getslice__ = select = re = extract = extract_unquoted = _raise
+    select = re = extract = register_namespace = __nonzero__ = _raise
-from scrapy.utils.python import flatten, unicode_to_str
+from scrapy.utils.python import unicode_to_str
-        """Return a unicode string of the content referenced by the XPathSelector"""
+        # TODO: this function should be deprecated. but what would be use instead?
-            'name', type(self.xmlNode).__name__), self.expr)
+        data = repr(self.extract()[:40])
-    """XPathSelector for HTML content"""
+from scrapy.utils.python import flatten
-    """XPathSelector for HTML content"""
+import unittest
-        return extract_regex(regex, self.extract(), 'utf-8')
+        return extract_regex(regex, self.extract())
-        return extract_regex(regex, self.extract(), 'utf-8')
+        return extract_regex(regex, self.extract())
-    __slots__ = ['response', 'text', 'expr', 'namespaces', '_root', '__weakref__']
+    __slots__ = ['response', 'text', 'expr', 'namespaces', '_root', '_xpathev', \
-            result = xpatheval(xpath)
+            result = self.xpathev(xpath)
-    200 OK
+    '200 OK'
-    404 Not Found
+    '404 Not Found'
-import pkgutil
+from scrapy.utils.py26 import get_data
-            default_config = pkgutil.get_data(__package__, 'default_scrapyd.conf')
+            default_config = get_data(__package__, 'default_scrapyd.conf')
-import pkgutil, unittest
+import unittest
-        eggfile = StringIO(pkgutil.get_data(__package__, 'mybot.egg'))
+        eggfile = StringIO(get_data(__package__, 'mybot.egg'))
-                         u'<root>la</root>')
+    # XXX: this test was disabled because lxml behaves inconsistently when
-    from .lxmlsel import *
+    from scrapy.selector.lxmlsel import *
-    from .libxml2sel import *
+    from scrapy.selector.libxml2sel import *
-    from .dummysel import *
+    from scrapy.selector.dummysel import *
-            from .dummysel import *
+            from scrapy.selector.dummysel import *
-            from .lxmlsel import *
+            from scrapy.selector.lxmlsel import *
-        from .libxml2sel import *
+        from scrapy.selector.libxml2sel import *
-            'surrounds_variant','match_common_prefix', 'metadata')
+            'match_common_prefix', 'metadata')
-            surrounds_variant=None, match_common_prefix=False):
+            match_common_prefix=False):
-        
+ 
-        if self.variant_stack:
+        if self.variant_stack and annotation.variant_id is None:
-        if annotation.surrounds_attribute or annotation.surrounds_variant:
+        if annotation.surrounds_attribute:
-            if annotation.surrounds_variant and self.variant_stack:
+            if annotation.variant_id and self.variant_stack:
-                if prev != annotation.surrounds_variant:
+                if prev != annotation.variant_id:
-    for cls in (RepeatedDataExtractor, AdjacentVariantExtractor, RepeatedDataExtractor,
+    for cls in (AdjacentVariantExtractor, RepeatedDataExtractor, AdjacentVariantExtractor, RepeatedDataExtractor,
-            elif len(list(egroup)) > 1:
+            else:
-        
+ANNOTATED_PAGE21 = u"""
-        
+    
-            policy='public-read')
+        return threads.deferToThread(k.set_contents_from_file, buf, \
-    setup_args['install_requires'] = ['Twisted>=2.5']
+    setup_args['install_requires'] = ['Twisted>=2.5', 'lxml']
-import unittest
+from twisted.trial import unittest
-    XPathSelector
+nolxml = False
-import libxml2
+        try:
-        libxml2.debugMemory(1)
+        self.libxml2.debugMemory(1)
-        libxml2.cleanupParser()
+        self.libxml2.cleanupParser()
-        figures.append(("libxml2 memory leak", libxml2.debugMemory(1), "bytes"))
+        figures.append(("libxml2 memory leak", self.libxml2.debugMemory(1), "bytes"))
-XPath selectors 
+XPath selectors
-        return extract_regex(regex, self.extract(), 'utf-8')
+Two backends are currently available: libxml2 and lxml
-        return text
+To select the backend explicitly use the SELECTORS_BACKEND variable in your
-            return unicode(self.xmlNode.getContent(), 'utf-8', errors='ignore')
+from scrapy.conf import settings
-    _get_libxml2_doc = staticmethod(xmlDoc_from_html)
+            from .lxmlsel import *
-
+# TODO: we should merge these tests with test_selector_libxml2.py
-import libxml2
+    try:
-            assert fp not in self.info.downloaded, self.info.downloaded
+            self.assertTrue(fp in self.info.downloading)
-            assert False, 'This can not be called'
+            self.fail('it must cache rsp1 result and must not try to redownload')
-    def __init__(self, store_uri):
+    def __init__(self, store_uri, download_func=None):
-        super(ImagesPipeline, self).__init__()
+        super(ImagesPipeline, self).__init__(download_func=download_func)
-from scrapy.conf import settings
+def _mocked_download_func(request, info):
-        self.pipeline = ImagesPipeline(self.tempdir)
+        self.pipeline = ImagesPipeline(self.tempdir, download_func=_mocked_download_func)
-        self.crawler.uninstall()
+from scrapy.utils.signal import disconnect_all
-            self.downloading = {}
+            self.downloading = set()
-            self.waiting = {}
+            self.waiting = defaultdict(list)
-    def __init__(self):
+    def __init__(self, download_func=None):
-        self.crawler = crawler
+        return crawler.engine.download
-        dlist = [self._enqueue(r, info) for r in requests]
+        dlist = [self._process_request(r, info) for r in requests]
-    def _enqueue(self, request, info):
+    def _process_request(self, request, info):
-        # if already downloaded, return cached result.
+        # Return cached result if request was already seen
-        # defer pre-download request processing
+        info.waiting[fp].append(wad)
-        dfd.addCallback(_post_media_to_download)
+        dfd.addCallback(self._check_media_to_download, request, info)
-        return self.crawler.engine.download(request, info.spider)
+        return self._download_func(request, info.spider)
-                    log.err(result, '%s found errors proessing %s' % (self.__class__.__name__, item))
+            msg = '%s found errors proessing %s' % (self.__class__.__name__, item)
-from twisted.internet import defer, reactor
+from twisted.python.failure import Failure
-from scrapy.crawler import Crawler
+from scrapy.utils.test import get_crawler
-class _MockedMediaPipeline(MediaPipeline):
+def _mocked_download_func(request, info):
-        return item.get('requests')
+class BaseMediaPipelineTestCase(unittest.TestCase):
-    pipeline_class = _MockedMediaPipeline
+    pipeline_class = MediaPipeline
-        self.pipe = self.pipeline_class()
+        self.pipe = self.pipeline_class(download_func=_mocked_download_func)
-        item = dict(name='sofa')
+    def test_default_media_to_download(self):
-    @defer.inlineCallbacks
+    def test_default_download_func(self):
-        req = Request('http://media.com/2.gif')
+        req = Request('url')
-        assert request_fingerprint(req) in info.downloaded
+        assert request_fingerprint(req) in self.info.downloaded
-        req2 = Request('http://media.com/1.jpg')
+        req1 = Request('url1')
-        assert collected == [fail]
+        assert request_fingerprint(req1) in self.info.downloaded
-                _, _, following_data = self._doextract(page, following_regions, sindex or start_index, end_region)
+                _, _, following_data = self._doextract(page, following_regions, sindex or start_index, end_index)
-            end_index, _, following_data = self._doextract(page, following_regions, start_index, end_region)
+            end_index, _, following_data = self._doextract(page, following_regions, start_index, end_index)
-            _, _, nested_data = self._doextract(page, nested_regions, start_index, end_region)
+            _, _, nested_data = self._doextract(page, nested_regions, start_index, end_index)
-        self.connect_sdb().create_domain(self._sdbdomain)
+        self.connect_sdb(aws_access_key_id=self._access_key, aws_secret_access_key=self._secret_key).create_domain(self._sdbdomain)
-        self.connect_sdb().put_attributes(self._sdbdomain, sdb_item_id, sdb_item)
+        self.connect_sdb(aws_access_key_id=self._access_key, aws_secret_access_key=self._secret_key).put_attributes(self._sdbdomain, sdb_item_id, sdb_item)
-    sources = [os.path.expanduser('~/.scrapy.cfg'), '/etc/scrapy.cfg']
+    sources = ['/etc/scrapy.cfg', r'c:\scrapy\scrapy.cfg', \
-        sources.insert(0, closest_scrapy_cfg())
+        sources.append(closest_scrapy_cfg())
-        if isinstance(exception, self.EXCEPTIONS_TO_RETRY):
+        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \
-    open("%s_%d.html" % (SAMPLES_FILE_PREFIX, count), "w").write(unicode_to_str(source))
+    open("%s_%d.html" % (SAMPLES_FILE_PREFIX, count), "wb").write(unicode_to_str(source))
-    open("%s_%d.json" % (SAMPLES_FILE_PREFIX, count), "w")\
+    open("%s_%d.json" % (SAMPLES_FILE_PREFIX, count), "wb")\
-            parsed = json.loads(str_to_unicode(open(fname, "r").read()),\
+            source = str_to_unicode(open("%s_%d.html" % (SAMPLES_FILE_PREFIX, count), "rb").read())
-            annotations = json.loads(str_to_unicode(open(fname, "r").read()))
+            source = str_to_unicode(open("%s_%d.html" % (SAMPLES_FILE_PREFIX, count), "rb").read())
-_HTML_REGEXP = re.compile(_TAG, re.I | re.DOTALL)
+_HTML_REGEXP = re.compile("%s|%s|%s" % (_COMMENT, _SCRIPT, _TAG), re.I | re.DOTALL)
-_SCRIPT_RE = re.compile("(<script.*?>).*?(</script.*?>)", re.DOTALL | re.I)
+_COMMENT_REGEXP = re.compile(_COMMENT, re.DOTALL)
-        yield _parse_tag(match)
+
-    closing, tag, attr_text = data[:3]
+    closing, tag, attr_text = data[4:7]
- {'end': 124, 'start': 101},
+ {'end': 104, 'start': 101},
- {'end': 91, 'start': 61},
+ {'end': 76, 'start': 61},
- {'end': 40, 'start': 20},
+ {'end': 23, 'start': 20},
-from gzip import GzipFile
+from scrapy.utils.python import unicode_to_str, str_to_unicode
-SAMPLES_FILE = "samples_htmlpage.json.gz"
+SAMPLES_FILE_PREFIX = os.path.join(path, "samples/samples_htmlpage")
-            samples.append(json.loads(line))
+    count = 0
-    samples_file.close()
+    open("%s_%d.html" % (SAMPLES_FILE_PREFIX, count), "w").write(unicode_to_str(source))
-        expected_parsed = sample["parsed"]
+    def _test_sample(self, source, expected_parsed, samplecount=None):
-                assert False, "[%s,%s] %s != [%s,%s] %s" % (element.start, \
+                errstring = "[%s,%s] %s != [%s,%s] %s" % (element.start, \
-                assert False, "(%s) %s != (%s) %s for text\n%s" % (count_element, \
+                errstring = "(%s) %s != (%s) %s for text\n%s" % (count_element, \
-        self._test_sample(sample)
+        self._test_sample(PAGE, parsed)
-    
+        count = 0
-        self._test_sample(sample)
+        self._test_sample(PAGE2, parsed)
-        self._test_sample(sample)
+        self._test_sample(PAGE3, parsed)
-        self._test_sample(sample)
+        self._test_sample(PAGE4, parsed)
-        self._test_sample(sample)
+        self._test_sample(PAGE5, parsed)
-        self._test_sample(sample)
+        self._test_sample(PAGE6, parsed)
- {'attributes': {u'style': u'&#34;margin:', u'0pt&#34;': None, u'class': u'&#34;MsoNormal&#34;', u'0cm': None}, 'end': 80, 'start': 15, 'tag': u'p', 'tag_type': 2},
+ {'attributes': {u'style': u'&#34;margin:', u'0pt&#34;': None, u'class': u'&#34;MsoNormal&#34;', u'0cm': None}, 'end': 80, 'start': 15, 'tag': u'p', 'tag_type': 1},
-            template = HtmlPage(body=str_to_unicode(source))
+        SAMPLES_FILE_PREFIX = os.path.join(path, "samples/samples_pageparsing")
-ZipPath.setContent = lambda x, y: None
+try:
-from scrapy import signals
+from scrapy import signals, log
-__version__ = "0.10.3"
+version_info = (0, 10, 4, 'dev')
-        
+
-    assert len(portrange) in [1, 2], "invalid portrange: %s" % portrange
+    assert len(portrange) <= 2, "invalid portrange: %s" % portrange
-    'version': __import__('scrapy').__version__,
+    'version': version,
-        return self.request.hostname
+        return urlparse_cached(self.request).hostname
-import re, os, time
+from urlparse import urlparse
-        self.assertEquals(req.headers.get("Cookie2"), '$Version="1"')
+from scrapy.http import Request, Response
-        # CookieJar.clear_session_cookies method
+class WrappedRequestTest(TestCase):
-        res = Response('http://www.perlmeister.com/scripts', headers=headers)
+    def setUp(self):
-            counter[key] = counter[key] + 1
+    def test_get_full_url(self):
-            counter["session_before"] == 0))
+    def test_get_host(self):
-        req = Request('http://www.perlmeister.com/foo')
+    def test_get_type(self):
-                'PART_NUMBER=ROCKET_LAUNCHER_0001; NO_A_BOT; GOOD_CUSTOMER')
+    def test_is_unverifiable(self):
-class CookielibWrappersTest(TestCase):
+    def test_get_origin_req_host(self):
-        return WrappedRequest(Request('%s%s' % (url, tail), headers=headers))
+    def test_has_header(self):
-        self.assertEquals(request_path(req), "/")
+    def test_get_header(self):
-        self.assertEquals(request_port(req), DEFAULT_HTTP_PORT)
+    def test_header_items(self):
-        self.assertEquals(request_host(req), "www.acme.com")
+    def test_add_unredirected_header(self):
-            import dummy_threading as _threading
+class WrappedResponseTest(TestCase):
-        assert not isinstance(c.jar._cookies_lock, _threading._RLock)
+    def setUp(self):
-        self.assertEqual(file_uri_to_path(x), os.path.abspath(fn))
+        self.assertEqual(file_uri_to_path(x).lower(), os.path.abspath(fn).lower())
-                             "file:///C|/windows/clock.avi")
+                             "file:///C:/windows/clock.avi")
-            self.assertEqual(file_uri_to_path("file:///C|/windows/clock.avi"),
+            self.assertEqual(file_uri_to_path("file:///C:/windows/clock.avi"),
-            uri = "file:///C|/windows/clock.avi"
+            uri = "file:///C:/windows/clock.avi"
-                             "file:///C|/windows/clock.avi")
+                             "file:///C:/windows/clock.avi")
-    WeakKeyCache
+    WeakKeyCache, stringify_dict
-
+    def test_stringify_dict(self):
-        self.item_class = load_object(settings['DEFAULT_ITEM_CLASS'])
+        self.item_class = load_object(crawler.settings['DEFAULT_ITEM_CLASS'])
-        self.vars['settings'] = settings
+        self.vars['settings'] = self.crawler.settings
-        return self.crawler.engine.schedule(request, spider)
+        d = self.crawler.engine.schedule(request, spider)
-        response = threads.blockingCallFromThread(reactor, \
+        response, spider = threads.blockingCallFromThread(reactor, \
-        shell.start(url=url).addBoth(lambda _: self.crawler.stop())
+        shell = Shell(self.crawler, update_vars=self.update_vars, inthread=True, \
-    def __init__(self, crawler, update_vars=None, inthread=False):
+    def __init__(self, crawler, update_vars=None, inthread=False, code=None):
-        start_python_console(self.vars)
+        if self.code:
-            log.err(Failure(), "Error fetching: %s" % request_or_url, spider=spider)
+        if isinstance(request_or_url, Request):
-        self.print_help()
+        if not self.code:
-            return "<CrawlerSettings module=None>"
+        return "<CrawlerSettings module=%r>" % self.settings_module
-        response = None
+        # we enclose all this code in a try/except block to see errors when
-        self.populate_vars(url, response, request, spider)
+            log.err(Failure(), "Error fetching: %s" % request_or_url, spider=spider)
-        return "<CrawlerSettings module=%r>" % self.settings_module.__name__
+        if self.settings_module:
-                f.write("default = %s" % settings.settings_module_path + os.linesep)
+                f.write("default = %s" % settings.settings_module.__name__ + os.linesep)
-from scrapy.settings import Settings
+from scrapy.settings import Settings, SpiderSettings
-        return headers.items()
+        return spider.settings.get('DEFAULT_REQUEST_HEADERS').items()
-        return getattr(spider, "download_timeout", None)
+        if hasattr(spider, 'download_timeout'):
-            or settings['USER_AGENT']
+        self._useragents[spider] = spider.settings['USER_AGENT']
-from scrapy.conf import settings
+from scrapy.utils import deprecate
-    def __init__(self, settings=settings):
+    def __init__(self):
-        return getattr(spider, 'user_agent', None) or self.default_useragent
+        if hasattr(spider, 'user_agent'):
-            self._download_delay = settings.getfloat('DOWNLOAD_DELAY')
+    def __init__(self, spider):
-            self._download_delay = float(download_delay)
+            self._download_delay = spider.settings.getfloat('DOWNLOAD_DELAY')
-        if self._download_delay and settings.getbool('RANDOMIZE_DOWNLOAD_DELAY'):
+            if hasattr(spider, 'max_concurrent_requests'):
-        )
+        self.sites[spider] = SpiderInfo(spider)
-    def __init__(self, request, timeout=DOWNLOAD_TIMEOUT):
+    def __init__(self, request, timeout=180):
-            settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])
+    def get_defaults_spider_mw(self):
-        self.assertEquals(req.headers, self.default_request_headers)
+        mw.process_request(req, spider)
-            k = set(self.default_request_headers).pop()
+        if defaults:
-        self.spider.default_request_headers = spider_headers
+        spider.DEFAULT_REQUEST_HEADERS = spider_headers
-        self.assertEquals(req.headers, dict(self.default_request_headers, **spider_headers))
+        mw.process_request(req, spider)
-
+        mw.process_request(req, spider)
-        self.req = Request('http://scrapytest.org/')
+    def get_request_spider_mw(self):
-        assert 'download_timeout' not in self.req.meta
+    def test_default_download_timeout(self):
-        self.assertEquals(self.req.meta.get('download_timeout'), 2)
+        req, spider, mw = self.get_request_spider_mw()
-        self.assertEquals(self.req.meta.get('download_timeout'), 1)
+        req, spider, mw = self.get_request_spider_mw()
-        del self.mw
+    def get_spider_and_mw(self, default_useragent):
-        self.mw.default_useragent = 'default_useragent'
+        spider, mw = self.get_spider_and_mw('default_useragent')
-        assert self.mw.process_request(req, self.spider) is None
+        assert mw.process_request(req, spider) is None
-        self.spider.user_agent = None
+    def test_remove_agent(self):
-        self.assertEquals(req.headers['User-Agent'], 'default_useragent')
+        assert mw.process_request(req, spider) is None
-        self.spider.user_agent = 'spider_useragent'
+        spider, mw = self.get_spider_and_mw('default_useragent')
-        assert self.mw.process_request(req, self.spider) is None
+        assert mw.process_request(req, spider) is None
-        self.spider.user_agent = 'spider_useragent'
+        spider, mw = self.get_spider_and_mw('default_useragent')
-        assert self.mw.process_request(req, self.spider) is None
+        assert mw.process_request(req, spider) is None
-        self.spider.user_agent = None
+        spider, mw = self.get_spider_and_mw(None)
-        assert self.mw.process_request(req, self.spider) is None
+        assert mw.process_request(req, spider) is None
-from scrapy.crawler import Crawler
+from scrapy.utils.test import get_crawler
-        self.crawler = Crawler(settings)
+        self.crawler = get_crawler()
-    settings = get_project_settings()
+settings = get_project_settings()
-#
+"""
-
+from . import default_settings
-from scrapy.conf import settings, Settings
+from scrapy.conf import settings
-from scrapy.conf import Settings
+from scrapy.settings import Settings
-from scrapy.conf import Settings
+from scrapy.settings import Settings
-from scrapy.conf import Settings
+from scrapy.settings import Settings
-from scrapy.conf import Settings
+from scrapy.settings import Settings
-from scrapy.conf import Settings
+from scrapy.settings import Settings
-        return []
+from scrapy.utils.misc import walk_modules
-            raise RuntimeError("Module %r does not define a Command class" % modname)
+    for cmd in _iter_command_classes(module):
-extensions = ['scrapydocs', 'sphinx.ext.autodoc']
+extensions = ['scrapydocs']
-        self.dbs_dir = config.get('dbs_dir', 'dbs')
+        self.config = config
-        self.queues = get_spider_queues(self.eggs_dir, self.dbs_dir)
+        self.queues = get_spider_queues(self.config)
-        self.dbs_dir = config.get('dbs_dir', 'dbs')
+        self.config = config
-        self.queues = get_spider_queues(self.eggs_dir, self.dbs_dir)
+        self.queues = get_spider_queues(self.config)
-        self.queues = get_spider_queues(eggs_dir, dbs_dir)
+        self.queues = get_spider_queues(config)
-        self.queues = get_spider_queues(eggs_dir, dbs_dir)
+        self.queues = get_spider_queues(config)
-import pkg_resources
+from ConfigParser import NoSectionError
-def get_spider_queues(eggsdir, dbsdir):
+def get_spider_queues(config):
-    for project in os.listdir(eggsdir):
+    for project in get_project_list(config):
-        import warnings
+            env.pop('SCRAPY_SETTINGS_DISABLED', None)
-    def from_response(cls, response, formnumber=0, formdata=None, 
+    def from_response(cls, response, formname=None, formnumber=0, formdata=None, 
-            raise IndexError("Form number %d not found in %s" % (formnumber, response))
+        
-            self.current_link.text = data.strip()
+        if self.current_link:
-To view the testing web server in a brwoser you can start it by running this
+To view the testing web server in a browser you can start it by running this
-        if image.mode != 'RGB':
+        if image.format == 'PNG' and image.mode == 'RGBA':
-            path)
+        self.assertEqual(self.pipeline.store._get_filesystem_path(key), path)
-__version__ = "0.10.2"
+version_info = (0, 10, 3, 'dev')
-__version__ = "0.10.2"
+version_info = (0, 11, 0, 'dev')
-__version__ = "0.10.1"
+version_info = (0, 10, 2, '')
-from scrapy.http import Response
+from scrapy.http import Response, TextResponse
-                response = response.replace(cls=respcls, body=decoded_body)
+                kwargs = dict(cls=respcls, body=decoded_body)
-                response = response.replace(body=decoded_body)
+                respcls = responsetypes.from_args(headers=response.headers, \
-    def from_content_type(self, content_type):
+    def from_content_type(self, content_type, content_encoding=None):
-            cls = self.from_content_type(headers['Content-type'])
+            cls = self.from_content_type(headers['Content-type'], \
-from scrapy.http import Response, Request
+from scrapy.http import Response, Request, HtmlResponse
-from twisted.internet.defer import maybeDeferred
+from twisted.internet import defer, threads
-        return maybeDeferred(func, *a, **kw)
+        return defer.maybeDeferred(func, *a, **kw)
-import sys, os
+import os
-import sys
+import sys, os
-    """Run scrapy for the settings module name passed"""
+eggpath = os.environ.get('SCRAPY_EGGFILE')
-    main(sys.argv[1], sys.argv[2:])
+from scrapy.cmdline import execute
-            version = self.list(project)[-1]
+            try:
-            pargs = [sys.executable, '-m', 'scrapyd.eggrunner', f.name, 'list']
+            pargs = [sys.executable, '-m', 'scrapyd.eggrunner', 'list']
-    def get_environment(self, message, slot):
+    def get_environment(self, message, slot, eggpath):
-        returned."""
+        returned. If no egg is found for the given project/version (None, None)
-    def get_environment(message, slot):
+    def get_environment(message, slot, eggpath):
-        args = [sys.executable, '-m', self.egg_runner, eggpath, 'crawl']
+        args = [sys.executable, '-m', self.egg_runner, 'crawl']
-        env = e.get_environment(message, slot)
+        env = e.get_environment(message, slot, eggpath)
-        os.remove(eggpath)
+        if eggpath:
-    def test_get_environment(self):
+    def test_get_environment_with_eggfile(self):
-        env = self.environ.get_environment(msg, slot)
+        env = self.environ.get_environment(msg, slot, '/path/to/file.egg')
-        self.cp.read(sources)
+    def __init__(self, values=None):
-class FilesystemEggStorage(Service):
+class FilesystemEggStorage(object):
-    def list(self, project):
+    def list(project):
-    def delete(self, project, version=None):
+    def delete(project, version=None):
-__version__ = "0.10"
+version_info = (0, 10, 1, 'dev')
-__version__ = "0.10"
+version_info = (0, 11, 0, 'dev')
-        if request.meta.get('dont_merge_cookies', False):
+        if 'dont_merge_cookies' in request.meta:
-        if request.meta.get('dont_merge_cookies', False):
+        if 'dont_merge_cookies' in request.meta:
-from scrapy.utils.memory import get_vmvalue_from_procfs
+from scrapy.utils.memory import get_vmvalue_from_procfs, procfs_supported
-        if not os.path.exists('/proc'):
+        if not procfs_supported():
-from scrapy.utils.memory import get_vmvalue_from_procfs
+from scrapy.utils.memory import get_vmvalue_from_procfs, procfs_supported
-        if not os.path.exists('/proc'):
+        if not procfs_supported():
-__version__ = "0.10-rc1"
+version_info = (0, 10, 0, '')
-    'version': version,
+    'version': __import__('scrapy').__version__,
-from scrapy.utils.request import request_fingerprint
+        if 'dont_retry' in request.meta:
-        kwargs.setdefault('encoding', getattr(self, '_encoding', None))
+        kwargs.setdefault('encoding', self.encoding)
-        # make sure replace() rediscovers the encoding (if not given explicitly) when changing the body
+    def test_replace_encoding(self):
-        r5 = self.response_class("http://www.example.com", body=body)
+        r5 = self.response_class("http://www.example.com", body=body)
-        self._assert_response_values(r6, 'utf-8', body2)
+        self._assert_response_values(r6, 'iso-8859-1', body2)
-                self.crawler.engine.schedule, request, spider)
+                self._schedule, request, spider)
-__version__ = "0.10-dev"
+version_info = (0, 10, 0, 'rc1')
-        return "Dropped %s - %s" % (item, str(exception))
+        return "Dropped %s - %s" % (item, unicode(exception))
-        return self.decode(value)
+        value = self.conn.execute(q, (key,)).fetchone()
-        q = "insert into %s (key, value) values (?,?)" % self.table
+        q = "insert or replace into %s (key, value) values (?,?)" % self.table
-            req.url = url
+        self.requests = [x.replace(url=safe_url_string(urljoin_rfc(base_url, \
-from itertools import ifilter, imap
+from itertools import ifilter
-        return imap(self._replace_url, requests)
+        return (x.replace(url=canonicalize_url(x.url)) for x in requests)
-    url = property(_get_url, _set_url)
+    url = property(_get_url, deprecated_setter(_set_url, 'url'))
-    body = property(_get_body, _set_body)
+    body = property(_get_body, deprecated_setter(_set_body, 'body'))
-            self.body = urllib.urlencode(query, doseq=1)
+            self._set_body(urllib.urlencode(query, doseq=1))
-    url = property(_get_url, _set_url)
+    url = property(_get_url, deprecated_setter(_set_url, 'url'))
-    body = property(_get_body, _set_body)
+    body = property(_get_body, deprecated_setter(_set_body, 'body'))
-        r2.url = "http://www.example.com/other"
+        r2 = r1.replace(url = "http://www.example.com/other")
-                return dfd
+                return mustbe_deferred(self.schedule, response, spider)
-                'encoding', 'priority', 'dont_filter']:
+                'encoding', 'priority', 'dont_filter', 'callback', 'errback']:
-        assert r2.errback is None
+        assert r2.callback is r1.callback
-        self.scheduler.open_spider(spider)
+        yield self.scheduler.open_spider(spider)
-        self.middleware = SchedulerMiddlewareManager()
+        self.middleware = SchedulerMiddlewareManager.from_settings(settings)
-        self.middleware.open_spider(spider)
+        return self.middleware.open_spider(spider)
-        self.middleware.close_spider(spider)
+        return self.middleware.close_spider(spider)
-            settings['DOWNLOADER_MIDDLEWARES'])
+        return build_component_list(settings['SCHEDULER_MIDDLEWARES_BASE'], \
-docs/topics/scheduler-middleware.rst
+Scheduler Middleware manager
-from collections import defaultdict
+
-from scrapy.utils.misc import load_object
+from scrapy.middleware import MiddlewareManager
-class SchedulerMiddlewareManager(object):
+class SchedulerMiddlewareManager(MiddlewareManager):
-        self.load()
+    component_name = 'scheduler middleware'
-        self.loaded = True
+    @classmethod
-                self.mw_cbs[name].append(mwfunc)
+        if hasattr(mw, 'enqueue_request'):
-            for mwfunc in self.mw_cbs['enqueue_request']:
+            for mwfunc in self.methods['enqueue_request']:
-from scrapy.core.downloader.manager import Downloader
+"""
-
+    'scrapy.contrib.downloadermiddleware.downloadtimeout.DownloadTimeoutMiddleware': 350,
-            download_timeout=DOWNLOAD_TIMEOUT):
+    def __init__(self, httpclientfactory=HTTPClientFactory):
-        factory = self._create_factory(request, spider)
+        factory = self.httpclientfactory(request)
-
+from scrapy.conf import settings
-    def __init__(self, request, timeout=0):
+    def __init__(self, request, timeout=DOWNLOAD_TIMEOUT):
-        self.timeout = timeout
+        self.timeout = request.meta.get('download_timeout') or timeout
-        d = self.download_request(request, spider)
+        request = Request(self.getURL('wait'), meta=dict(download_timeout=0.000001))
-        self.middleware = DownloaderMiddlewareManager()
+        self.middleware = DownloaderMiddlewareManager.from_settings(settings)
-docs/topics/downloader-middleware.rst
+Downloader Middleware manager
-from scrapy.utils.misc import load_object
+from scrapy.middleware import MiddlewareManager
-class DownloaderMiddlewareManager(object):
+class DownloaderMiddlewareManager(MiddlewareManager):
-        self.load()
+    @classmethod
-            self.request_middleware.append(mw.process_request)
+            self.methods['process_request'].append(mw.process_request)
-            self.response_middleware.insert(0, mw.process_response)
+            self.methods['process_response'].insert(0, mw.process_response)
-        self.loaded = True
+            self.methods['process_exception'].insert(0, mw.process_exception)
-            for method in self.request_middleware:
+            for method in self.methods['process_request']:
-            for method in self.response_middleware:
+            for method in self.methods['process_response']:
-            for method in self.exception_middleware:
+            for method in self.methods['process_exception']:
-            self.methods['process_spider_output'].append(mw.process_spider_output)
+            self.methods['process_spider_output'].insert(0, mw.process_spider_output)
-            self.methods['process_spider_exception'].append(mw.process_spider_exception)
+            self.methods['process_spider_exception'].insert(0, mw.process_spider_exception)
-from twisted.application.service import Application, Service
+from twisted.application.service import Application
-    app = Application("Scrapy")
+def application():
-        default_config = pkgutil.get_data(__package__, 'default_scrapyd.cfg')
+        sources = self._getsources()
-            (self.max_proc, self.egg_runner), system="Launcher")
+        log.msg("%s started: max_proc=%r, egg_runner=%r" % (self.parent.name, \
-        return factory
+        return self.httpclientfactory(request, timeout)
-        def _deactivate(_):
+        def _deactivate(response):
-            return _
+            return response
-        deferred = defer.Deferred()
+
-                response=response, spider=spider)
+
-    AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']
+    AWS_ACCESS_KEY_ID = None
-    THUMBS = settings.get('IMAGES_THUMBS', {})
+    MIN_WIDTH = 0
-    def __init__(self):
+    def __init__(self, store_uri):
-        super(ImagesPipeline, self).__init__()
+        return cls(store_uri)
-        self.stats = settings.getbool('DEPTH_STATS')
+    def __init__(self, maxdepth, stats=None):
-            stats.set_value('envinfo/request_depth_limit', self.maxdepth)
+            stats.set_value('envinfo/request_depth_limit', maxdepth)
-                        stats.set_value('request_depth_max', depth, spider=spider)
+                    self.stats.inc_value('request_depth_count/%s' % depth, spider=spider)
-            stats.inc_value('request_depth_count/0', spider=spider)
+            self.stats.inc_value('request_depth_count/0', spider=spider)
-        if not self.maxlength:
+
-
+
-        self.pipeline = ImagesPipeline()
+        self.pipeline = ImagesPipeline(self.tempdir)
-        settings.disabled = self.settings_disabled_before
+        rmtree(self.tempdir)
-from scrapy.stats import stats
+from scrapy.statscol import StatsCollector
-        stats.open_spider(self.spider)
+        self.stats = StatsCollector()
-        self.assertEquals(stats.get_value('envinfo/request_depth_limit'), 1)
+        self.mw = DepthMiddleware(1, self.stats)
-        rdc = stats.get_value('request_depth_count/1', spider=self.spider)
+        rdc = self.stats.get_value('request_depth_count/1', spider=self.spider)
-        rdm = stats.get_value('request_depth_max', spider=self.spider)
+        rdm = self.stats.get_value('request_depth_max', spider=self.spider)
-        stats.close_spider(self.spider, '')
+        self.stats.close_spider(self.spider, '')
-        out = list(self.mw.process_spider_output(res, reqs, self.spider))
+        mw = UrlLengthMiddleware(maxlength=25)
-        self.spidermw = SpiderMiddlewareManager()
+        self.spidermw = SpiderMiddlewareManager.from_settings(settings)
-docs/topics/spider-middleware.rst
+Spider Middleware manager
-from scrapy.utils.conf import build_component_list
+from scrapy.middleware import MiddlewareManager
-from scrapy.conf import settings
+from scrapy.utils.conf import build_component_list
-        self.load()
+class SpiderMiddlewareManager(MiddlewareManager):
-            self.spider_middleware.append(mw.process_spider_input)
+            self.methods['process_spider_input'].append(mw.process_spider_input)
-            self.result_middleware.insert(0, mw.process_spider_output)
+            self.methods['process_spider_output'].append(mw.process_spider_output)
-        self.loaded = True
+            self.methods['process_spider_exception'].append(mw.process_spider_exception)
-            for method in self.spider_middleware:
+            for method in self.methods['process_spider_input']:
-            for method in self.exception_middleware:
+            for method in self.methods['process_spider_exception']:
-            for method in self.result_middleware:
+            for method in self.methods['process_spider_output']:
-        pass
+    implements(IFeedStorage)
-        copyfileobj(file, sys.stdout)
+        copyfileobj(file, self._stdout)
-from scrapy.contrib.feedexport import FileFeedStorage, FTPFeedStorage, S3FeedStorage
+from scrapy.contrib.feedexport import IFeedStorage, FileFeedStorage, FTPFeedStorage, S3FeedStorage, StdoutFeedStorage
-
+    def test_interface(self):
-        return self._assert_stores(FTPFeedStorage(uri), path)
+        st = FTPFeedStorage(uri)
-            raise unittest.SkipTest("Missing library: boto")
+            raise unittest.SkipTest("No S3 URI available for testing")
-
+from scrapy.utils.test import assert_aws_environ
-        skip = "AWS keys not found"
+    def setUp(self):
-import unittest
+from twisted.internet.defer import inlineCallbacks, maybeDeferred
-        verifyObject(ISpiderQueue, SqliteSpiderQueue())
+        verifyObject(ISpiderQueue, self.q)
-from scrapy.selector.document import Libxml2Document
+from twisted.trial.unittest import SkipTest
-from scrapy import signals
+from scrapy import conf
-    def __init__(self):
+    def __init__(self, settings=conf.settings):
-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)
+        self._headers = WeakKeyCache(self._default_headers)
-        for k, v in self._default_headers[spider].iteritems():
+    def _default_headers(self, spider):
-                **getattr(spider, 'default_request_headers', {}))
+                headers[k] = v
-        self._default_headers.pop(spider)
+    def process_request(self, request, spider):
-from scrapy.utils.request import request_authenticate
+from scrapy.utils.http import basic_auth_header
-    """This middleware allows spiders to use HTTP auth in a cleaner way
+    """Set Basic HTTP Authorization header
-            request_authenticate(request, http_user, http_pass)
+        auth = self._cache[spider]
-    default_useragent = settings.get('USER_AGENT')
+    def __init__(self, settings=settings):
-        ua = getattr(spider, 'user_agent', None) or self.default_useragent
+        ua = self.cache[spider]
-        self.mw.spider_closed(self.spider)
+        self.spider = TestSpider('foo')
-        assert self.mw.process_request(req, spider) is None
+        assert self.mw.process_request(req, self.spider) is None
-    memoizemethod_noargs, isbinarytext, equal_attributes
+    memoizemethod_noargs, isbinarytext, equal_attributes, \
-
+def basic_auth_header(username, password):
-    request.headers['Authorization'] = 'Basic ' + b64userpass
+    request.headers['Authorization'] = basic_auth_header(username, password)
-        elif isinstance(output, Request):
+        if isinstance(output, Request):
-class RequestHandlers(object):
+class DownloadHandlers(object):
-        handlers.update(settings.get('REQUEST_HANDLERS', {}))
+        handlers = settings.get('DOWNLOAD_HANDLERS_BASE')
-class FileRequestHandler(object):
+class FileDownloadHandler(object):
-class HttpRequestHandler(object):
+class HttpDownloadHandler(object):
-from .http import HttpRequestHandler
+from .http import HttpDownloadHandler
-class S3RequestHandler(object):
+class S3DownloadHandler(object):
-            httprequesthandler=HttpRequestHandler):
+            httpdownloadhandler=HttpDownloadHandler):
-        self._download_http = httprequesthandler().download_request
+        self._download_http = httpdownloadhandler().download_request
-from .handlers import RequestHandlers
+from .handlers import DownloadHandlers
-        self.handlers = RequestHandlers()
+        self.handlers = DownloadHandlers()
-from scrapy.core.downloader.handlers.s3 import S3RequestHandler
+from scrapy.core.downloader.handlers.file import FileDownloadHandler
-        self.download_request = FileRequestHandler().download_request
+        self.download_request = FileDownloadHandler().download_request
-        self.download_request = HttpRequestHandler().download_request
+        self.download_request = HttpDownloadHandler().download_request
-        self.download_request = HttpRequestHandler().download_request
+        self.download_request = HttpDownloadHandler().download_request
-class HttpRequestHandlerMock(object):
+class HttpDownloadHandlerMock(object):
-        s3reqh = S3RequestHandler(self.AWS_ACCESS_KEY_ID, \
+        s3reqh = S3DownloadHandler(self.AWS_ACCESS_KEY_ID, \
-                httprequesthandler=HttpRequestHandlerMock)
+                httpdownloadhandler=HttpDownloadHandlerMock)
-from .handlers import download_any
+from .handlers import RequestHandlers
-        dfd = mustbe_deferred(download_any, request, spider)
+        dfd = mustbe_deferred(self.handlers.download_request, request, spider)
-TELNETCONSOLE_PORT = 6023  # if None, uses a dynamic port
+TELNETCONSOLE_PORT = [6023, 6073]
-WEBSERVICE_PORT = 6080
+WEBSERVICE_PORT = [6080, 7030]
-from twisted.internet import reactor, protocol
+from twisted.internet import protocol
-from scrapy import signals
+from scrapy import log, signals
-        self.portnum = settings.getint('TELNETCONSOLE_PORT')
+        self.portrange = map(int, settings.getlist('TELNETCONSOLE_PORT'))
-        self.port = reactor.listenTCP(self.portnum, self)
+        self.port = listen_tcp(self.portrange, self)
-See docs/topics/ws.rst
+See docs/topics/webservice.rst
-from scrapy import signals
+from scrapy import log, signals
-        self.portnum = settings.getint('WEBSERVICE_PORT')
+        self.portrange = map(int, settings.getlist('WEBSERVICE_PORT'))
-        self.port = reactor.listenTCP(self.portnum, self)
+        self.port = listen_tcp(self.portrange, self)
-        request = Request('file://%s' % self.tmpname + '^')
+        request = Request(path_to_file_uri(self.tmpname + '^'))
-
+from scrapy.utils.url import file_uri_to_path
-    """file download"""
+    @defers
-            body = f.read()
+        filepath = file_uri_to_path(request.url)
-
+QUEUE_POLL_INTERVAL = 5
-        self.queue = ExecutionQueue(self.spiders, spq, keepalive)
+        pollint = self.settings.getfloat('QUEUE_POLL_INTERVAL')
-            self._nextcall = reactor.callLater(self.queue.polling_delay, \
+            self._nextcall = reactor.callLater(self.queue.poll_interval, \
-    def __init__(self, spiders, queue, keepalive=False):
+    def __init__(self, spiders, queue, poll_interval=5, keep_alive=False):
-        self._keepalive = keepalive
+        self._keepalive = keep_alive
-        self.queue = ExecutionQueue(TestSpiderManager(), None, self.keep_alive)
+        self.queue = ExecutionQueue(TestSpiderManager(), None, keep_alive=self.keep_alive)
-        self.q = JsonSqlitePriorityQueue(database, table)
+        try:
-        self.assertEqual(spider.allowed_domains, [])
+HTTPCACHE_IGNORE_SCHEMES = ['file']
-        return urlparse_cached(request).scheme in ['http', 'https']
+        return urlparse_cached(request).scheme not in self.ignore_schemes
-        # We failback to metadata['url'] to support old generated caches ' should be removed for Scrapy 0.11
+        # We failback to metadata['url'] to support old generated caches. TODO: remove for Scrapy 0.11
-        url = metadata['url']
+        # We failback to metadata['url'] to support old generated caches ' should be removed for Scrapy 0.11
-    def get_host_regex(self, domains):
+    def get_host_regex(self, spider):
-        domains = [d.replace('.', r'\.') for d in domains]
+        allowed_domains = getattr(spider, 'allowed_domains', None)
-        self.host_regexes[spider] = self.get_host_regex(spider.allowed_domains)
+        self.host_regexes[spider] = self.get_host_regex(spider)
-
+        self.spider = self._get_spider()
-from scrapy.xlib import twisted_250_monkeypatches
+from scrapy.xlib import twisted_250_monkeypatches, urlparse_monkeypatches
-    'https': 'scrapy.core.downloader.handlers.http.download_http',
+    'file': 'scrapy.core.downloader.handlers.file.FileRequestHandler',
-from scrapy.exceptions import NotSupported
+from scrapy.exceptions import NotSupported, NotConfigured
-            self._handlers[scheme] = load_object(cls)
+        for scheme, clspath in handlers.iteritems():
-            raise NotSupported("Unsupported URL scheme '%s' in: <%s>" % (scheme, request.url))
+            msg = self._notconfigured.get(scheme, \
-    return defer.maybeDeferred(_all_in_one_read_download_file, request, spider)
+class FileRequestHandler(object):
-    return respcls(url=request.url, body=body)
+    def download_request(self, request, spider):
-
+DOWNLOAD_TIMEOUT = settings.getint('DOWNLOAD_TIMEOUT')
-from scrapy.core.downloader.handlers.http import download_http
+from scrapy.core.downloader.handlers.file import FileRequestHandler
-        return download_file(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
-        d = download_file(request, BaseSpider('foo'))
+        d = self.download_request(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider('foo'))
+        d = self.download_request(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider('foo'))
+        d = self.download_request(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider('foo'))
+        d = self.download_request(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider('foo'))
+        d = self.download_request(request, BaseSpider('foo'))
-        d = download_http(request, spider)
+        d = self.download_request(request, spider)
-        return download_http(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
-        return download_http(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
-        d = download_http(request, BaseSpider('foo'))
+        d = self.download_request(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider('foo'))
+        d = self.download_request(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider('foo'))
+        d = self.download_request(request, BaseSpider('foo'))
-        return download_http(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
-        return download_http(request, BaseSpider('foo')).addCallback(_test)
+        return self.download_request(request, BaseSpider('foo')).addCallback(_test)
-
+# prevents noisy (and innocent) dropin.cache errors when loading spiders from
-        sources.insert(0, closest_scrapy_cfg())
+    sources = get_sources(use_closest)
-from twisted.web import server, resource, error
+from twisted.web import server, error
-class JsonResource(resource.Resource):
+class JsonResource(JsonResource_):
-
+import sys, os
-        data_files.append([dirpath, [os.path.join(dirpath, f) for f in filenames]])
+for scrapy_dir in ['scrapy', 'scrapyd']:
-from scrapy.utils.spider import iterate_spider_output
+from scrapy.utils.spider import iterate_spider_output, create_spider_for_request
-            spider = self.crawler.spiders.create_for_request(request)
+            spider = create_spider_for_request(self.crawler.spiders, request)
-            spider = self._spiders.create_for_request(request, **kwargs)
+            spider = create_spider_for_request(self._spiders, request, **kwargs)
-            spider = self._spiders.create_for_request(Request(url), **kwargs)
+            spider = create_spider_for_request(self._spiders, Request(url), \
-            spider = self.crawler.spiders.create_for_request(request, \
+            spider = create_spider_for_request(self.crawler.spiders, request, \
-        return default_spider
+            if cls.handles_request(request)]
-        return TestSpider('create_for_request', **kwargs)
+    def find_by_request(self, request):
-from scrapy.conf import settings
+from scrapy.utils.conf import arglist_to_dict
-    default_settings = {'LOG_ENABLED': False}
+    default_settings = {'LOG_LEVEL': 'WARNING'}
-        return "[options] <list|clear|add spider1 ..>"
+        return "[options] <list|clear|count|add spider1 ..>"
-            help="spider arguments to use for adding spiders")
+            help="set spider argument (may be repeated)")
-        queue = self.crawler.queue.queue
+        q = self.crawler.queue._queue
-                print "Added (priority=%s): %s" % (opts.priority, msg)
+                self._call(q.add, x, **opts.spargs)
-                print "(priority=%s) %s" % (y, x)
+            x = self._call(q.list)
-            print "Cleared %s queue" % botname
+            self._call(q.clear)
-    default_settings = {'QUEUE_CLASS': 'scrapy.queue.KeepAliveExecutionQueue'}
+    default_settings = {'KEEP_ALIVE': True}
-    default_settings = {'LOG_ENABLED': False}
+KEEP_ALIVE = False
-
+SPIDER_QUEUE_CLASS = 'scrapy.spiderqueue.SqliteSpiderQueue'
-        return False
+from zope.interface import implements
-        self.queue = queue_cls(self.spiders)
+        spq_cls = load_object(self.settings['SPIDER_QUEUE_CLASS'])
-    def __init__(self, _spiders):
+    def __init__(self, spiders, queue, keepalive=False):
-        self._spiders = _spiders
+        self._spiders = spiders
-        pass
+        msg = self._queue.pop()
-        return not bool(self.spider_requests)
+        return not self._keepalive and not bool(self.spider_requests)
-        return False
+from zope.interface import implements
-from scrapy.queue import ExecutionQueue, KeepAliveExecutionQueue
+from scrapy.queue import ExecutionQueue
-    queue_class = ExecutionQueue
+    keep_alive = False
-        self.queue = self.queue_class(_spiders=TestSpiderManager())
+        self.queue = ExecutionQueue(TestSpiderManager(), None, self.keep_alive)
-    queue_class = KeepAliveExecutionQueue
+    keep_alive = True
-        self.database = database
+    def __init__(self, database=None, table="dict"):
-        self.conn = sqlite3.connect(database)
+        self.conn = sqlite3.connect(self.database)
-        self.database = database
+    def __init__(self, database=None, table="queue"):
-        self.conn = sqlite3.connect(database)
+        self.conn = sqlite3.connect(self.database)
-                f.write("settings = %s" % settings.settings_module_path + os.linesep)
+                f.write("[settings]" + os.linesep)
-from scrapy.utils.conf import set_scrapy_settings_envvar
+from scrapy.utils.conf import init_env
-            set_scrapy_settings_envvar(project)
+            init_env(project)
-from ConfigParser import RawConfigParser
+from ConfigParser import SafeConfigParser
-    dict"""
+    dict
-        projdir = os.path.dirname(scrapy_cfg)
+def init_env(project='default', set_syspath=True):
-from itertools import groupby
+from scrapy.http import Request
-from scrapy.command import ScrapyCommand
+from scrapy.exceptions import UsageError
-def _get_commands_from_module(module):
+def _get_commands_from_module(module, inproject):
-            d[cmdname] = command()
+            if inproject or not command.requires_project:
-            print 'WARNING: Module %r does not define a Command class' % modname
+            raise RuntimeError("Module %r does not define a Command class" % modname)
-    cmds = _get_commands_from_module('scrapy.commands')
+def _get_commands_dict(inproject):
-        cmds.update(_get_commands_from_module(cmds_module))
+        cmds.update(_get_commands_from_module(cmds_module, inproject))
-def _get_command_name(argv):
+def _pop_command_name(argv):
-    if inside_project:
+def _print_header(inproject):
-    cmds = _get_commands_dict()
+    cmds = _get_commands_dict(inproject)
-            print "  %-13s %s" % (cmdname, cmdclass.short_desc())
+        print "  %-13s %s" % (cmdname, cmdclass.short_desc())
-    print 'Use "scrapy <command> -h" for more info about a command'
+    print 'Use "scrapy <command> -h" to see more info about a command'
-def check_deprecated_scrapy_ctl(argv):
+def _check_deprecated_scrapy_ctl(argv, inproject):
-    if settings.settings_module:
+    if inproject:
-    cmdname = _get_command_name(argv)
+    inproject = bool(settings.settings_module)
-        print 'Use "scrapy -h" for help' 
+        conflict_handler='resolve')
-        parser.print_help()
+    cmd.add_options(parser)
-        return _run_command_profiled(cmd, args, opts)
+        _run_command_profiled(cmd, args, opts)
-        return cmd.run(args, opts)
+        cmd.run(args, opts)
-        log.msg("writing cProfile stats to %r" % opts.profile)
+        sys.stderr.write("scrapy: writing cProfile stats to %r\n" % opts.profile)
-        log.msg("writing lsprof stats to %r" % opts.lsprof)
+        sys.stderr.write("scrapy: writing lsprof stats to %r\n" % opts.lsprof)
-    p.runctx('ret = cmd.run(args, opts)', globals(), loc)
+    p.runctx('cmd.run(args, opts)', globals(), loc)
-    return loc['ret']
+from scrapy import log
-            sys.exit()
+            raise UsageError("Invalid --set value, use --set NAME=VALUE", print_help=False)
-
+from scrapy.exceptions import UsageError
-            sys.exit(2)
+            raise UsageError("Invalid -a value, use -a NAME=VALUE", print_help=False)
-            return False
+            raise UsageError()
-            return False
+            raise UsageError()
-            return False
+            raise UsageError()
-from scrapy.utils.misc import load_object
+from scrapy.exceptions import UsageError
-            return False
+            raise UsageError()
-                return False
+                raise UsageError()
-            return False
+            raise UsageError()
-from scrapy import log
+from scrapy.exceptions import UsageError
-            return False
+            raise UsageError()
-            return
+            raise UsageError("File not found: %s\n" % filename)
-            return
+            raise UsageError("Unable to load %r: %s\n" % (filename, e))
-            return
+            raise UsageError("No spider found in file: %s\n" % filename)
-            return False
+            raise UsageError()
-        self.assertEqual(0, self.call('genspider', 'test_name'))
+        self.assertEqual(2, self.call('genspider', 'test_name'))
-        self.assert_("ERROR: No spider found in file" in log)
+        self.assert_("No spider found in file" in log)
-        self.assert_("ERROR: File not found: some_non_existent_file" in log)
+        self.assert_("File not found: some_non_existent_file" in log)
-        self.assert_("ERROR: Unable to load" in log)
+        self.assert_("Unable to load" in log)
-            sys.stderr.write("Invalid --arg value, use --arg NAME=VALUE\n")
+            sys.stderr.write("Invalid -a value, use -a NAME=VALUE\n")
-        group.add_option("--set", dest="set", action="append", default=[], \
+        group.add_option("--set", dest="set", action="append", default=[], metavar="NAME=VALUE", \
-                sys.exit(2)
+        try:
-            q.append_spider_name(name)
+            q.append_spider_name(name, **opts.spargs)
-                spider = self.crawler.spiders.create(opts.spider)
+                spider = self.crawler.spiders.create(opts.spider, **opts.spargs)
-                spider = self.crawler.spiders.create(name)
+                spider = self.crawler.spiders.create(name, **opts.spargs)
-from scrapy.utils.conf import build_component_list
+from scrapy.utils.conf import build_component_list, arglist_to_dict
-                f.write(str(os.getpid()))
+                f.write(str(os.getpid()) + os.linesep)
-        pass
+        closed = getattr(spider, 'closed', None)
-        spider arguments. If the sipder name is not found, it raises a
+        spider arguments. If the spider name is not found, it raises a
-from scrapy import log
+from scrapy import log, signals
-    def append_url(self, url, spider=None, **kwargs):
+    def append_url(self, url=None, spider=None, **kwargs):
-    def append_spider_name(self, spider_name, **spider_kwargs):
+    def append_spider_name(self, name=None, **spider_kwargs):
-            spider = self._spiders.create(spider_name, **spider_kwargs)
+            spider = self._spiders.create(name, **spider_kwargs)
-            log.msg('Unable to find spider: %s' % spider_name, log.ERROR)
+            log.msg('Unable to find spider: %s' % name, log.ERROR)
-            print scrapy.__version__
+            print "Scrapy %s" % scrapy.__version__
-scripts = ['bin/scrapy', 'bin/scrapy-ws.py']
+scripts = ['bin/scrapy']
-        msg("Started project: %s" % settings['BOT_NAME'])
+        msg("Scrapy %s started (bot: %s)" % (scrapy.__version__, \
-write a web service client. Feel free to improve or write you own.
+Example script to control a Scrapy server using its JSON-RPC web service.
-    jsonrpc_call(opts, 'manager/queue', 'append_spider_name', args[0])
+    jsonrpc_call(opts, 'crawler/queue', 'append_spider_name', args[0])
-    jsonrpc_call(opts, 'manager/engine', 'close_spider', args[0])
+    jsonrpc_call(opts, 'crawler/engine', 'close_spider', args[0])
-    for x in json_get(opts, 'manager/engine/open_spiders'):
+    for x in json_get(opts, 'crawler/engine/open_spiders'):
-    for x in jsonrpc_call(opts, 'spiders', 'list'):
+    for x in jsonrpc_call(opts, 'crawler/spiders', 'list'):
-
+from scrapy.spider import BaseSpider
-from scrapy.contrib.logformatter import crawled_logline
+from scrapy.logformatter import LogFormatter
-    def test_crawled_logline(self):
+    def setUp(self):
-        self.assertEqual(crawled_logline(req, res),
+        self.assertEqual(self.formatter.crawled(req, res, self.spider),
-        self.assertEqual(crawled_logline(req, res),
+        self.assertEqual(self.formatter.crawled(req, res, self.spider),
-LOG_FORMATTER_CRAWLED = 'scrapy.contrib.logformatter.crawled_logline'
+LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'
-                log.msg(self._crawled_logline(request, response), \
+                log.msg(log.formatter.crawled(request, response, spider), \
-                spider=spider)
+            log.msg(log.formatter.scraped(output, request, response, spider), \
-                log.msg("Dropped %s - %s" % (item, str(ex)), level=log.WARNING, spider=spider)
+                log.msg(log.formatter.dropped(item, ex, spider), \
-            log.msg("Passed %s" % item, log.INFO, spider=spider)
+            log.msg(log.formatter.passed(item, spider), log.INFO, spider=spider)
-from scrapy import log
+from scrapy import log, signals
-        self.configured = False
+    def __init__(self, settings, spider_closed_callback):
-        self.configured = True
+        self.scraper = Scraper(self, self.settings)
-        self.engine.configure(self._spider_closed)
+        self.engine = ExecutionEngine(self.settings, self._spider_closed)
-    default_settings = {'QUEUE_CLASS': 'scrapy.core.queue.KeepAliveExecutionQueue'}
+    default_settings = {'QUEUE_CLASS': 'scrapy.queue.KeepAliveExecutionQueue'}
-QUEUE_CLASS = 'scrapy.core.queue.ExecutionQueue'
+QUEUE_CLASS = 'scrapy.queue.ExecutionQueue'
-from scrapy.core.queue import ExecutionQueue
+from scrapy.queue import ExecutionQueue
-from scrapy.core.queue import ExecutionQueue
+from scrapy.queue import ExecutionQueue
-from twisted.internet import defer
+from scrapy.queue import *
-        return False
+import warnings
-from scrapy.core.queue import ExecutionQueue, KeepAliveExecutionQueue
+from scrapy.queue import ExecutionQueue, KeepAliveExecutionQueue
-    if twisted.__version__ < '8.0.0':
+    from twisted.python.versions import Version
-this module with the ``runserver`` argument::
+To view the testing web server in a brwoser you can start it by running this
-        return crawler.engine.download(request, info.spider)
+        return self.crawler.engine.download(request, info.spider)
-            'scrapy_settings')
+        if self.ENVVAR not in os.environ:
-    main()
+import sys
-    def __init__(self, spider_name=None, _manager=crawler):
+    def __init__(self, spider_name=None, _crawler=crawler):
-        self._manager = _manager
+        self._crawler = _crawler
-        status = get_engine_status(self._manager.engine)
+        status = get_engine_status(self._crawler.engine)
-        return EngineStatusResource(name, self._manager)
+        return EngineStatusResource(name, self._crawler)
-from scrapy.tests.test_utils_serialize import ExecutionMangerStub
+from scrapy.tests.test_utils_serialize import CrawlerMock
-class urllib_stub(object):
+class urllib_mock(object):
-        crawler = ExecutionMangerStub([])
+        crawler = CrawlerMock([])
-        ul = urllib_stub(1)
+        ul = urllib_mock(1)
-        ul = urllib_stub()
+        ul = urllib_mock()
-        ul = urllib_stub(result={'one': 1})
+        ul = urllib_mock(result={'one': 1})
-        ul = urllib_stub(error={'code': 123, 'message': 'hello', 'data': 'some data'})
+        ul = urllib_mock(error={'code': 123, 'message': 'hello', 'data': 'some data'})
-class ExecutionEngineStub(object):
+class _EngineMock(object):
-class ExecutionMangerStub(object):
+class CrawlerMock(object):
-        self.engine = ExecutionEngineStub(open_spiders)
+        self.engine = _EngineMock(open_spiders)
-        self.spref = SpiderReferencer(manager)
+        crawler = CrawlerMock(open_spiders)
-        self.manager = crawler
+        self.crawler = crawler
-            for spider in self.manager.engine.open_spiders:
+            for spider in self.crawler.engine.open_spiders:
-    crawler.configure()
+    cmd.set_crawler(crawler)
-        q = ExecutionQueue()
+        q = self.crawler.queue
-                spider = crawler.spiders.create(opts.spider)
+                spider = self.crawler.spiders.create(opts.spider)
-                spider = crawler.spiders.create(name)
+                spider = self.crawler.spiders.create(name)
-        crawler.start()
+        self.crawler.start()
-            spider_names = crawler.spiders.find_by_request(Request(url))
+            spider_names = self.crawler.spiders.find_by_request(Request(url))
-                spider = crawler.spiders.create(opts.spider)
+                spider = self.crawler.spiders.create(opts.spider)
-        crawler.queue.append_request(request, spider, \
+        self.crawler.queue.append_request(request, spider, \
-        crawler.start()
+        self.crawler.start()
-            spider = crawler.spiders.create(name)
+            spider = self.crawler.spiders.create(name)
-        print "\n".join(crawler.spiders.list())
+        print "\n".join(self.crawler.spiders.list())
-                return crawler.spiders.create(opts.spider)
+                return self.crawler.spiders.create(opts.spider)
-            spider = crawler.spiders.create_for_request(request)
+            spider = self.crawler.spiders.create_for_request(request)
-        crawler.start()
+        self.crawler.queue.append_request(request, spider)
-class Command(ScrapyCommand):
+class Command(runserver.Command):
-        queue = load_object(settings['SERVICE_QUEUE'])().queue
+        queue = self.crawler.queue.queue
-from scrapy.utils.misc import load_object
+    def process_options(self, args, opts):
-        crawler.start()
+        self.crawler.start()
-        crawler.start()
+        self.crawler.queue.append_spider(spider)
-    def _print_setting(self, opts):
+        settings = self.crawler.settings
-            print settings_.get(opts.get)
+            print settings.get(opts.get)
-            print settings_.getbool(opts.getbool)
+            print settings.getbool(opts.getbool)
-            print settings_.getint(opts.getint)
+            print settings.getint(opts.getint)
-            print settings_.getfloat(opts.getfloat)
+            print settings.getfloat(opts.getfloat)
-            print settings_.getlist(opts.getlist)
+            print settings.getlist(opts.getlist)
-from scrapy.core.queue import KeepAliveExecutionQueue
+    default_settings = {'QUEUE_CLASS': 'scrapy.core.queue.KeepAliveExecutionQueue'}
-        crawler.start()
+        shell = Shell(self.crawler, update_vars=self.update_vars, inthread=True)
-SERVICE_QUEUE_FILE = 'scrapy.db'
+SERVER_QUEUE_CLASS = 'scrapy.contrib.queue.SqliteExecutionQueue'
-from scrapy.project import crawler
+        from scrapy.project import crawler
-        self.queue = JsonSqlitePriorityQueue(settings['SERVICE_QUEUE_FILE'])
+        self.queue = JsonSqlitePriorityQueue(settings['SQLITE_DB'])
-        self.scraper = Scraper(self)
+        self.scraper = Scraper(self, self.crawler.settings)
-            _spiders = crawler.spiders
+    def __init__(self, _spiders):
-    def __init__(self, engine):
+    def __init__(self, engine, settings):
-            self.spiders.load()
+    def install(self):
-        self.engine.configure(self._spider_closed)
+    def uninstall(self):
-from scrapy.conf import settings
+"""
-crawler = CrawlerProcess(settings)
+    crawler.install()
-        self.loaded = False
+    def __init__(self, spider_modules):
-
+from scrapy.xlib.pydispatch import dispatcher
-        reactor.callWhenRunning(reactor.listenTCP, port, self)
+        self.portnum = settings.getint('TELNETCONSOLE_PORT')
-"""
+import sys, os, re, urlparse
-from twisted.web import server, resource, static, util
+from twisted.internet import reactor, defer
-from scrapy.project import crawler
+from scrapy.conf import Settings
-def start_test_site():
+def start_test_site(debug=False):
-#    r.putChild("test", TestResource())
+    if debug:
-class CrawlingSession(object):
+class CrawlerRun(object):
-            disconnect_all(signals.stats_spider_closed)
+        start_urls = [self.geturl("/"), self.geturl("/redirect")]
-        # expected urls that should be visited
+    @defer.inlineCallbacks
-        urls_expected = set([session.geturl(p) for p in must_be_visited])
+        urls_visited = set([rp[0].url for rp in self.run.respplug])
-        """
+    def _assert_received_requests(self):
-        self.assertEqual(3, len(session.reqplug))
+        self.assertEqual(3, len(self.run.reqplug))
-        urls_expected = set([session.geturl(p) for p in paths_expected])
+        urls_requested = set([rq[0].url for rq in self.run.reqplug])
-        """
+    def _assert_downloaded_responses(self):
-        self.assertEqual(6, len(session.respplug))
+        self.assertEqual(6, len(self.run.respplug))
-            if session.getpath(response.url) == '/item999.html':
+        for response, _ in self.run.respplug:
-            if session.getpath(response.url) == '/redirect':
+            if self.run.getpath(response.url) == '/redirect':
-        for item, response in session.itemresp:
+    def _assert_scraped_items(self):
-                         session.signals_catched[signals.spider_closed])
+    def _assert_signals_catched(self):
-        print "Test server running at http://localhost:%d/ - hit Ctrl-C to finish." % port.getHost().port
+        start_test_site(debug=True)
-        unittest.main()
+from scrapy.crawler import Crawler
-        assert self.spiderman.loaded
+        self.spiderman = SpiderManager(['test_spiders_xxx'])
-        self.spiderman.load(['scrapy.tests.test_spidermanager.test_spiders.spider1'])
+        self.spiderman = SpiderManager(['scrapy.tests.test_spidermanager.test_spiders.spider1'])
-        self.spiderman.load(['scrapy.tests.test_spidermanager.test_spiders.spider0'])
+        self.spiderman = SpiderManager(['scrapy.tests.test_spidermanager.test_spiders.spider0'])
-        r = jsonrpc_server_call(t, 'invalid json data')
+        r = jsonrpc_server_call(t, 'invalid json data', self.json_decoder)
-        r = jsonrpc_server_call(t, '{"test": "test"}')
+        r = jsonrpc_server_call(t, '{"test": "test"}', self.json_decoder)
-        r = jsonrpc_server_call(t, '{"method": "notfound", "id": 1}')
+        r = jsonrpc_server_call(t, '{"method": "notfound", "id": 1}', self.json_decoder)
-        r = jsonrpc_server_call(t, '{"method": "exception", "id": 1}')
+        r = jsonrpc_server_call(t, '{"method": "exception", "id": 1}', self.json_decoder)
-        r = jsonrpc_server_call(t, '{"method": "call", "id": 2}')
+        r = jsonrpc_server_call(t, '{"method": "call", "id": 2}', self.json_decoder)
-        r = jsonrpc_server_call(t, '{"method": "call", "params": [456, 123], "id": 3}')
+        r = jsonrpc_server_call(t, '{"method": "call", "params": [456, 123], "id": 3}', \
-        r = jsonrpc_server_call(t, '{"method": "call", "params": {"data": 789}, "id": 3}')
+        r = jsonrpc_server_call(t, '{"method": "call", "params": {"data": 789}, "id": 3}', \
-        self.manager = manager or crawler
+    def __init__(self, crawler=None):
-        self.spref = kw.pop('spref', None) or SpiderReferencer()
+        crawler = kw.pop('crawler', None)
-        port = settings.getint('WEBSERVICE_PORT')
+        self.portnum = settings.getint('WEBSERVICE_PORT')
-        reactor.callWhenRunning(reactor.listenTCP, port, self)
+        dispatcher.connect(self.start_listening, signals.engine_started)
-
+from scrapy.utils.misc import load_object
-    def __init__(self, settings, spiders):
+    def __init__(self, settings):
-        self.spiders = spiders
+        self.spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()
-crawler = CrawlerProcess(settings, _spiders)
+crawler = CrawlerProcess(settings)
-    crawler.configure(control_reactor=True)
+    log.start()
-from scrapy import log
+from scrapy import log, signals
-            log.start()
+    def configure(self, queue=None):
-            reactor.run(installSignalHandlers=False)
+
-        install_shutdown_handlers(self._signal_kill)
+        install_shutdown_handlers(signal.SIG_IGN)
-        install_shutdown_handlers(signal.SIG_IGN)
+        reactor.callFromThread(self._stop_reactor)
-from scrapy.crawler import Crawler
+from scrapy.crawler import CrawlerProcess
-crawler = Crawler(settings, _spiders)
+crawler = CrawlerProcess(settings, _spiders)
-scripts = ['bin/scrapy', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py']
+scripts = ['bin/scrapy', 'bin/scrapy-ws.py']
-        self.spiderman.load(['scrapy.tests.test_contrib_spidermanager.test_spiders.spider1'])
+        self.spiderman.load(['scrapy.tests.test_spidermanager.test_spiders.spider1'])
-        self.spiderman.load(['scrapy.tests.test_contrib_spidermanager.test_spiders.spider0'])
+        self.spiderman.load(['scrapy.tests.test_spidermanager.test_spiders.spider0'])
-            smtpport=None):
+            smtpport=None, debug=False):
-        if settings.getbool('MAIL_DEBUG'):
+        if self.debug:
-
+    def tearDown(self):
-        mailsender = MailSender()
+        mailsender = MailSender(debug=True)
-        mailsender = MailSender()
+        mailsender = MailSender(debug=True)
-        if not value:
+        if value is None:
-        if not value:
+        if value is None:
-SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.SpiderManager'
+SPIDER_MANAGER_CLASS = 'scrapy.spidermanager.SpiderManager'
-from scrapy.contrib.spidermanager import SpiderManager
+from scrapy.spidermanager import SpiderManager
-        self.path = u.path
+        self.path = file_uri_to_path(uri)
-        uri = "file://%s" % path
+        uri = path_to_file_uri(path)
-        uri = "file://%s" % path
+        uri = path_to_file_uri(path)
-        samples_file = open(os.path.join(path, "samples_pageparsing.json.gz"), "r")
+        samples_file = open(os.path.join(path, "samples_pageparsing.json.gz"), "rb")
-        skip = True
+        skip = "lxml not available"
-    'scrapy.contrib.webservice.manager.ManagerResource': 1,
+    'scrapy.contrib.webservice.crawler.CrawlerResource': 1,
-        self._target = _spiders
+from scrapy.extension import ExtensionManager
-    def __init__(self, spiders, extensions):
+    def __init__(self, settings, spiders):
-        self.extensions = extensions
+        self.settings = settings
-            self.extensions.load()
+        self.extensions = ExtensionManager.from_settings(self.settings)
-ExtensionManager (extensions) to be used as singleton.
+The Extension Manager
-from scrapy.utils.misc import load_object
+from scrapy.middleware import MiddlewareManager
-        self.disabled = {}
+class ExtensionManager(MiddlewareManager):
-        """
+    component_name = 'extension'
-        extlist = build_component_list(settings['EXTENSIONS_BASE'], \
+    @classmethod
-        enabled = [type(x).__name__ for x in middlewares]
+        enabled = [x.__class__.__name__ for x in middlewares]
-crawler = Crawler(_spiders, _extensions)
+crawler = Crawler(settings, _spiders)
-            help="always use this spider")
+            help="use this spider without looking for one")
-            help="don't show extracted links")
+            help="don't show links to follow (extracted requests)")
-        self.callbacks = opts.callbacks.split(',') if opts.callbacks else []
+            help="use CrawlSpider rules to discover the callback")
-        return (), ()
+    def run_callback(self, spider, response, callback, opts):
-        display.nocolour = opts.nocolour
+    def print_results(self, items, requests, cb_name, opts):
-
+            display.pprint([dict(x) for x in items], colorize=not opts.nocolour)
-        request = Request(args[0], callback=responses.append)
+            print "# Requests - callback: %s" % cb_name, "-"*68
-                spider = crawler.spiders.create(opts.spider)
+                return crawler.spiders.create(opts.spider)
-                return
+            if spider:
-        crawler.configure()
+    def get_response_and_spider(self, url, opts):
-        response = responses[0]
+            log.msg('No response downloaded for: %s' % request, log.ERROR, \
-                self.print_results(items, links, callback, opts)
+    def run(self, args, opts):
-            self.print_results(items, links, 'parse', opts)
+            callback = self.get_callback_from_rules(spider, response)
-Helper functions for formatting and pretty printing some objects
+pprint and pformat wrappers with colorization support
-nocolour = False
+import sys
-    if nocolour or not sys.stdout.isatty():
+def _colorize(text, colorize=True):
-        return colorize(pypprint.pformat(repr(obj)))
+    return _colorize(pformat_(obj), kwargs.pop('colorize', True))
-from scrapy.extension import extensions
+from scrapy.project import crawler
-    def __init__(self, _extensions=extensions):
+    def __init__(self, _extensions=None):
-    def __init__(self, spiders):
+    def __init__(self, spiders, extensions):
-            extensions.load()
+        if not self.extensions.loaded:
-        log.msg("Enabled extensions: %s" % ", ".join(extensions.enabled.iterkeys()),
+        log.msg("Enabled extensions: %s" % ", ".join(self.extensions.enabled.iterkeys()),
-from scrapy.utils.misc import load_object
+from scrapy.extension import ExtensionManager
-crawler = Crawler(_spiders)
+_extensions = ExtensionManager()
-            'extensions': extensions,
+            'extensions': crawler.extensions,
-from scrapy.spider import spiders
+from scrapy.project import crawler
-                spider = spiders.create(opts.spider)
+                spider = crawler.spiders.create(opts.spider)
-                spider = spiders.create(name)
+                spider = crawler.spiders.create(name)
-            spider_names = spiders.find_by_request(Request(url))
+            spider_names = crawler.spiders.find_by_request(Request(url))
-from scrapy.spider import BaseSpider, spiders
+from scrapy.spider import BaseSpider
-                spider = spiders.create(opts.spider)
+                spider = crawler.spiders.create(opts.spider)
-from scrapy.spider import spiders
+from scrapy.project import crawler
-            spider = spiders.create(name)
+            spider = crawler.spiders.create(name)
-from scrapy.spider import spiders
+from scrapy.project import crawler
-        print "\n".join(spiders.list())
+        print "\n".join(crawler.spiders.list())
-from scrapy.spider import spiders
+from scrapy.project import crawler
-                spider = spiders.create(opts.spider)
+                spider = crawler.spiders.create(opts.spider)
-            spider = spiders.create_for_request(request)
+            spider = crawler.spiders.create_for_request(request)
-from scrapy.spider import spiders
+from scrapy.project import crawler
-    def __init__(self, _spiders=spiders):
+    def __init__(self, _spiders=None):
-        dfd.addBoth(lambda _: spiders.close_spider(spider))
+        dfd.addBoth(lambda _: self.crawler.spiders.close_spider(spider))
-    def __init__(self, _spiders=spiders):
+    def __init__(self, _spiders=None):
-    def __init__(self):
+    def __init__(self, spiders):
-            spiders.load()
+        if not self.spiders.loaded:
-crawler = Crawler()
+_spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()
-from scrapy.spider import BaseSpider, spiders
+from scrapy.spider import BaseSpider
-                log_multiple=True)
+            spider = self.crawler.spiders.create_for_request(request, \
-            'spiders': spiders,
+            'spiders': crawler.spiders,
-    scrapymanager.configure(control_reactor=True)
+    from scrapy.project import crawler
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        scrapymanager.start()
+        crawler.queue = q
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        scrapymanager.queue.append_request(request, spider, \
+        crawler.configure()
-        scrapymanager.start()
+        crawler.start()
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        scrapymanager.start()
+        crawler.configure()
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        scrapymanager.start()
+        crawler.queue = queue_class()
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        scrapymanager.start()
+        crawler.queue.append_spider(spider)
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        scrapymanager.start()
+        shell = Shell(crawler, update_vars=self.update_vars, inthread=True)
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-            scrapymanager.engine.close_spider, spider=spider, \
+            crawler.engine.close_spider, spider=spider, \
-            scrapymanager.engine.close_spider(spider, 'closespider_itempassed')
+            crawler.engine.close_spider(spider, 'closespider_itempassed')
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-            dfd = scrapymanager.engine.download(robotsreq, spider)
+            dfd = crawler.engine.download(robotsreq, spider)
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-            scrapymanager.stop()
+            crawler.stop()
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        return scrapymanager.engine.download(request, info.spider)
+        return crawler.engine.download(request, info.spider)
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-            lastseen = scrapymanager.engine.downloader.sites[spider].lastseen
+            lastseen = crawler.engine.downloader.sites[spider].lastseen
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        pending = scrapymanager.engine.scheduler.pending_requests.get(spider, [])
+        pending = crawler.engine.scheduler.pending_requests.get(spider, [])
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-    def __init__(self, spider_name=None, _manager=scrapymanager):
+    def __init__(self, spider_name=None, _manager=crawler):
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-    def __init__(self, _manager=scrapymanager):
+    def __init__(self, _manager=crawler):
-    def __init__(self):
+    def __init__(self, crawler):
-import signal
+import warnings
-scrapymanager = ExecutionManager()
+from scrapy.project import crawler
-    Shell(scrapymanager).start(response=response, spider=spider)
+    from scrapy.project import crawler
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-            'manager': scrapymanager,
+            'engine': crawler.engine,
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-            scrapymanager.start()
+            crawler.configure()
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        engine = scrapymanager.engine
+        engine = crawler.engine
-from scrapy.core.manager import scrapymanager
+from scrapy.project import crawler
-        self.manager = manager or scrapymanager
+        self.manager = manager or crawler
-STATS_CLASS = 'scrapy.stats.collector.MemoryStatsCollector'
+STATS_CLASS = 'scrapy.statscol.MemoryStatsCollector'
-        dispatcher.connect(self.stats_spider_closing, signal=stats_spider_closing)
+        dispatcher.connect(self.stats_spider_opened, signal=signals.stats_spider_opened)
-from scrapy.stats.collector import StatsCollector
+from scrapy.statscol import StatsCollector
-        connect_sdb().create_domain(self._sdbdomain)
+        import boto
-        connect_sdb().put_attributes(self._sdbdomain, sdb_item_id, sdb_item)
+        self.connect_sdb().put_attributes(self._sdbdomain, sdb_item_id, sdb_item)
-from scrapy.stats import stats, signals
+from scrapy.stats import stats
-from scrapy.stats.collector import DummyStatsCollector
+from scrapy.statscol import DummyStatsCollector
-from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \
+from scrapy.signals import stats_spider_opened, stats_spider_closing, \
-            disconnect_all(stats_signals.stats_spider_closed)
+            disconnect_all(signals.stats_spider_opened)
-from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \
+from scrapy.statscol import StatsCollector, DummyStatsCollector
-from scrapy.conf import settings
+from scrapy.conf import settings, Settings
-    relevant_classes = (BaseSpider, Request, Response, BaseItem, XPathSelector)
+    relevant_classes = (BaseSpider, Request, Response, BaseItem, \
-        return "Generate new spider based on template passed with -t or --template"
+        return "Generate new spider using pre-defined templates"
-        return "Parse the given URL (using the spider) and print the results"
+        return "Parse URL (using its spider) and print the results"
-        return "Run a spider"
+        return "Run a self-contained spider (without creating a project)"
-        return "Query Scrapy settings"
+        return "Get settings values"
-        return "Create new project with an initial project template"
+        return "Create new project"
-        return "Open a URL in browser, as seen by Scrapy"
+        return "Open URL in browser, as seen by Scrapy"
-from os.path import join, dirname, abspath, exists
+from os.path import join, dirname, abspath, exists, splitext
-
+_templates_base_dir = settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], \
-        parser.add_option("--dump", dest="dump", action="store_true")
+        parser.add_option("-l", "--list", dest="list", action="store_true",
-            template_file = self._find_template(opts.template)
+            template_file = self._find_template(opts.dump)
-                print template.read() 
+                print open(template_file, 'r').read()
-
+        name, domain = args[0:2]
-        # if spider already exists and not force option then halt
+            # if spider already exists and not --force then halt
-                print "Spider '%s' already exists in module:" % name
+                print "Spider %r already exists in module:" % name
-
+                return
-        return template_file
+        template_file = join(self.templates_dir, '%s.tmpl' % template)
-        for filename in sorted(files):
+        print "Available templates:"
-                print filename
+                print "  %s" % splitext(filename)[0]
-        out = os.tmpfile()
+    def proc(self, *new_args, **kwargs):
-        self.assertEqual(1, self.call('genspider', 'test_spider', 'test.com'))
+    def test_template(self, tplname='crawl'):
-        self.test_template_default('--template=basic')
+        self.test_template('basic')
-        self.test_template_default('--template=csvfeed')
+        self.test_template('csvfeed')
-        self.test_template_default('--template=crawl')
+        self.test_template('xmlfeed')
-        self.assertEqual(0, self.call('genspider', '--dump', '--template=basic'))
+        self.assertEqual(0, self.call('genspider', '--dump=basic'))
-        p = self.call_get_proc('runspider', fname)
+        p = self.proc('runspider', fname)
-        p = self.call_get_proc('runspider', fname)
+        p = self.proc('runspider', fname)
-        p = self.call_get_proc('runspider', 'some_non_existent_file')
+        p = self.proc('runspider', 'some_non_existent_file')
-        p = self.call_get_proc('runspider', fname)
+        p = self.proc('runspider', fname)
-    print "To run a command:"
+    print "Usage:"
-    print "==================\n"
+    print "Available commands:"
-            print "  %s" % cmdclass.short_desc()
+            print "  %-13s %s" % (cmdname, cmdclass.short_desc())
-        args = [sys.executable, '-m', 'scrapy.cmdline', 'start']
+        args = [sys.executable, '-m', 'scrapy.cmdline', 'runserver']
-        return "Start the Scrapy manager but don't run any spider (idle mode)"
+        return "Start Scrapy in server mode"
-        shell.start(url)
+        shell = Shell(scrapymanager, update_vars=self.update_vars, inthread=True)
-import urlparse
+from scrapy.item import BaseItem
-from scrapy.selector import XmlXPathSelector, HtmlXPathSelector
+from scrapy.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector
-
+from scrapy.http import Request, Response, TextResponse
-    def __init__(self, update_vars=None, nofetch=False):
+    relevant_classes = (BaseSpider, Request, Response, BaseItem, XPathSelector)
-        self.update_vars = update_vars
+        self.update_vars = update_vars or (lambda x: None)
-        self.nofetch = nofetch
+        self.inthread = inthread
-    def fetch(self, request_or_url, print_help=False):
+    def _start(self, url=None, request=None, response=None, spider=None):
-            url = parse_url(request_or_url)
+            url = any_to_uri(request_or_url)
-        scrapymanager.engine.open_spider(spider)
+        if spider is None:
-                scrapymanager.engine.schedule, request, spider)
+                self.crawler.engine.schedule, request, spider)
-        if not self.nofetch:
+        if self.inthread:
-            self.update_vars(self.vars)
+        self.update_vars(self.vars)
-        signal.signal(signal.SIGINT, signal.SIG_IGN)
+        self.p("Available Scrapy objects:")
-        start_python_console(self.vars)
+    def p(self, line=''):
-def inspect_response(response):
+def inspect_response(response, spider=None):
-    Shell(nofetch=True).inspect_response(response)
+    from scrapy.core.manager import scrapymanager
-    urljoin_rfc, url_is_from_spider
+    urljoin_rfc, url_is_from_spider, file_uri_to_path, path_to_file_uri, any_to_uri
-        'Out', 'help'] and not varname.startswith('_')
+        'Out', 'help', 'namespace'] and not varname.startswith('_')
-            pass
+        start_python_console(self.vars)
-        self._run_console()
+        start_python_console(self.vars)
-                log.msg("Crawling <%s>: %s" % (request.url, errmsg), \
+                log.msg("Error downloading <%s>: %s" % (request.url, errmsg), \
-    default_settings = {'LOG_LEVEL': 'WARNING'}
+from twisted.python.failure import Failure
-            request = Request(url)
+            request = Request(url, dont_filter=True)
-                print "Done - use shelp() to see available objects"
+        response = None
-        print "================="
+        print "Available objects:"
-                print "  %-10s: %s" % (k, v)
+                print "  %-10s %s" % (k, v)
-        print "  shelp()           : Prints this help."
+        print "Convenient shortcuts:"
-        print "  view(response)    : View response in a browser"
+            print "  fetch(req_or_url) Fetch a new request or URL and update shell objects"
-        print "Inspecting: %s" % response
+        print "Scrapy Shell - inspecting response: %s" % response
-
+    check_deprecated_scrapy_ctl(argv) # TODO: remove for Scrapy 0.11
-
+from scrapy.command import ScrapyCommand
-SERVICE_QUEUE = 'scrapy.core.queue.KeepAliveExecutionQueue'
+SERVICE_QUEUE = 'scrapy.contrib.queue.SqliteExecutionQueue'
-from os.path import exists, join, dirname
+from os.path import exists, join, dirname, abspath
-import unittest
+
-from scrapy.contrib.exporter import XmlItemExporter
+from scrapy import log
-        raise ValueError("Only Python files supported: %s" % abspath)
+        raise ValueError("Not a Python source file: %s" % abspath)
-            help="store scraped items to FILE in XML format")
+        return "Run the spider defined in the given file"
-
+        filename = args[0]
-        scrapymanager.queue.append_spider(module.SPIDER)
+        scrapymanager.queue.append_spider(spider)
-from scrapy.spider import BaseSpider
+from scrapy.utils.spider import iter_spider_classes
-                    self._spiders[name] = obj
+        for spcls in iter_spider_classes(module):
-from scrapy.utils.spider import iterate_spider_output
+from scrapy.utils.spider import iterate_spider_output, iter_spider_classes
-
+    app.add_crossref_type(
-#   scrapy-ctl.py genspider myspider myspider-domain.com
+#   scrapy genspider myspider myspider-domain.com
-#   scrapy-ctl.py genspider myspider myspider-domain.com
+#   scrapy genspider myspider myspider-domain.com
-    print "  scrapy-ctl.py <command> [options] [args]\n"
+    print "  scrapy <command> [options] [args]\n"
-    print "  scrapy-ctl.py <command> -h\n"
+    print "  scrapy <command> -h\n"
-            print "Error running: scrapy-ctl.py %s\n" % cmdname
+            print "Error running: scrapy %s\n" % cmdname
-        print 'Use "scrapy-ctl.py -h" for help' 
+        print 'Use "scrapy -h" for help' 
-#   scrapy-ctl.py genspider myspider myspider-domain.com
+#   scrapy genspider myspider myspider-domain.com
-        assert exists(join(self.proj_path, 'scrapy-ctl.py'))
+        assert exists(join(self.proj_path, 'scrapy.cfg'))
-    'scripts': ['bin/scrapy', 'bin/scrapy-ctl.py', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py'],
+    'scripts': scripts,
-        if hasattr(pipe, 'process_item'):
+        func = getattr(pipe, 'process_item', None)
-            if get_func_args(pipe.process_item.im_func)[1] == 'spider':
+            fargs = get_func_args(func.im_func)
-            self.methods['process_item'].append(pipe.process_item)
+                func = self._wrap_old_process_item(func)
-    _update_default_settings(settings['COMMANDS_SETTINGS_MODULE'], cmdname)
+    settings.defaults.update(cmd.default_settings)
-FEED_URI_ARGS = None # a function to extend uri arguments
+FEED_URI_PARAMS = None # a function to extend uri arguments
-"""
+import warnings
-    'scripts': ['bin/scrapy-ctl.py', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py'],
+    'scripts': ['bin/scrapy', 'bin/scrapy-ctl.py', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py'],
-    def test_url_is_from_any_domain(self):
+    def test_url_is_from_spider(self):
-    return url_is_from_any_domain(url, [spider.name] + spider.allowed_domains)
+    return url_is_from_any_domain(url, [spider.name] + \
-        if not self.start_urls:
+        elif not getattr(self, 'name', None):
-        if not self.allowed_domains:
+        if not hasattr(self, 'allowed_domains'):
-from scrapy.utils.signal import send_catch_log
+from scrapy.utils.signal import send_catch_log, send_catch_log_deferred
-        send_catch_log(signal=signals.engine_started)
+        yield send_catch_log_deferred(signal=signals.engine_started)
-        send_catch_log(signals.spider_opened, spider=spider)
+        yield send_catch_log_deferred(signals.spider_opened, spider=spider)
-        dfd = defer.maybeDeferred(stats.close_spider, spider, reason=reason)
+        dfd = send_catch_log_deferred(signal=signals.spider_closed, \
-        send_catch_log(signal=signals.engine_stopped)
+        yield send_catch_log_deferred(signal=signals.engine_stopped)
-from scrapy.utils.signal import send_catch_log
+from scrapy.utils.signal import send_catch_log, send_catch_log_deferred
-            dfd = self.itemproc.process_item(output, spider)
+            dfd = send_catch_log_deferred(signal=signals.item_scraped, \
-                send_catch_log(signal=signals.item_dropped, \
+                return send_catch_log_deferred(signal=signals.item_dropped, \
-            send_catch_log(signal=signals.item_passed, \
+            return send_catch_log_deferred(signal=signals.item_passed, \
-from scrapy.utils.signal import send_catch_log
+from scrapy.utils.signal import send_catch_log, send_catch_log_deferred
-class SignalUtilsTest(unittest.TestCase):
+class SendCatchLogTest(unittest.TestCase):
-            assert "test_handler_error" in event['message'][0]
+            assert "error_handler" in event['message'][0]
-        result = send_catch_log(test_signal, arg='test')
+        dispatcher.connect(self.error_handler, signal=test_signal)
-        assert test_handler_check in handlers_called
+        assert self.error_handler in handlers_called
-        self.assertEqual(result[0][0], test_handler_error)
+        self.assertEqual(result[0][0], self.error_handler)
-        self.assertEqual(result[1], (test_handler_check, "OK"))
+        self.assertEqual(result[1], (self.ok_handler, "OK"))
-        dispatcher.disconnect(test_handler_check, signal=test_signal)
+        dispatcher.disconnect(self.error_handler, signal=test_signal)
-from scrapy.utils.defer import defer_result, defer_succeed, parallel
+from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback
-        msg = "Spider exception caught while processing <%s> (referer: <%s>)" % \
+        msg = "Spider error processing <%s> (referer: <%s>)" % \
-        dfd = parallel(iter(result), self.concurrent_items,
+        it = iter_errback(result, self.handle_spider_error, request, spider)
-    process_chain_both, process_parallel
+    process_chain_both, process_parallel, iter_errback
-    def process_item(self, spider, item):
+    def process_item(self, item, spider):
-    def process_item(self, spider, item):
+    def process_item(self, item, spider):
-    def process_item(self, spider, item):
+    def process_item(self, item, spider):
-from scrapy.conf import settings
+from scrapy.middleware import MiddlewareManager
-class ItemPipelineManager(object):
+class ItemPipelineManager(MiddlewareManager):
-        self.load()
+    component_name = 'item pipeline'
-        self.loaded = True
+    @classmethod
-        pass
+    # FIXME: remove in Scrapy 0.11
-        pass
+    def _add_middleware(self, pipe):
-            return d
+            self.methods['process_item'].append(pipe.process_item)
-        return deferred
+    def process_item(self, item, spider):
-    def process_item(self, spider, item):
+    def process_item(self, item, spider):
-    def spider_opened(self, spider):
+    def open_spider(self, spider):
-    def spider_closed(self, spider):
+    def close_spider(self, spider):
-    def process_item(self, spider, item):
+    def process_item(self, item, spider):
-        self.scraper.open_spider(spider)
+        yield self.scraper.open_spider(spider)
-        self.itemproc = load_object(settings['ITEM_PROCESSOR'])()
+        itemproc_cls = load_object(settings['ITEM_PROCESSOR'])
-        self.itemproc.open_spider(spider)
+        yield self.itemproc.open_spider(spider)
-        self.itemproc.close_spider(spider)
+        site.closing.addCallback(self.itemproc.close_spider)
-            site.closing.callback(None)
+            site.closing.callback(spider)
-        self.pipe.spider_opened(self.spider)
+        self.pipe.open_spider(self.spider)
-        self.pipe.spider_closed(self.spider)
+        self.pipe.close_spider(self.spider)
-        new_item = yield self.pipe.process_item(self.spider, item)
+        new_item = yield self.pipe.process_item(item, self.spider)
-        new_item = yield self.pipe.process_item(self.spider, item)
+        new_item = yield self.pipe.process_item(item, self.spider)
-        new_item = yield self.pipe.process_item(self.spider, item)
+        new_item = yield self.pipe.process_item(item, self.spider)
-        yield self.pipe.process_item(self.spider, item)
+        yield self.pipe.process_item(item, self.spider)
-        yield self.pipe.process_item(self.spider, item)
+        yield self.pipe.process_item(item, self.spider)
-def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8'):
+def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=True):
-        message[0] = "%s: %s" % (lvlname, message[0])
+        if prepend_level:
-        why = "%s: %s" % (lvlname, unicode_to_str(why, encoding))
+        why = unicode_to_str(why, encoding)
-        return []
+        raise NotImplementedError
-        log.msg("Enabled %ss: %s" % (cls.component_name, ",".join(enabled)), \
+        log.msg("Enabled %ss: %s" % (cls.component_name, ", ".join(enabled)), \
-        log.msg(msg, log.ERROR, spider=spider)
+        msg = "Spider exception caught while processing <%s> (referer: <%s>)" % \
-                    log.ERROR, spider=spider)
+                log.err(output, 'Error processing %s' % item, spider=spider)
-            return requests + items[0:]
+from collections import defaultdict
-from itertools import imap
+from twisted.internet import reactor, defer
-from scrapy.utils.defer import mustbe_deferred
+from scrapy.utils.defer import mustbe_deferred, process_chain, \
-            dfd = Deferred()
+            dfd = defer.Deferred()
-warnings.warn("scrapy.core.exceptions is deprecated, use scrapy.exceptions instead", \
+warnings.warn("scrapy.core.exceptions is deprecated and will be removed in Scrapy 0.11, use scrapy.exceptions instead", \
-warnings.warn("scrapy.core.signals is deprecated, use scrapy.signals instead", \
+warnings.warn("scrapy.core.signals is deprecated and will be removed in Scrapy 0.11, use scrapy.signals instead", \
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-Scrapy core signals
+from scrapy.signals import *
-item_dropped = object()
+import warnings
-from scrapy.core import signals
+from scrapy import signals
-from scrapy.core import signals
+from scrapy import signals
-        from scrapy.core import signals
+        from scrapy import signals
-from scrapy.core.exceptions import DropItem
+from scrapy.exceptions import DropItem
-from scrapy.core.exceptions import DropItem
+from scrapy.exceptions import DropItem
-from scrapy.core.exceptions import NotConfigured, IgnoreRequest
+from scrapy.exceptions import NotConfigured, IgnoreRequest
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest
-from scrapy.core.exceptions import NotConfigured, IgnoreRequest
+from scrapy.exceptions import NotConfigured, IgnoreRequest
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import DropItem, NotConfigured, IgnoreRequest
+from scrapy.exceptions import DropItem, NotConfigured, IgnoreRequest
-from scrapy.core.exceptions import IgnoreRequest, NotConfigured
+from scrapy.exceptions import IgnoreRequest, NotConfigured
-from scrapy.core.exceptions import NotConfigured, DontCloseSpider
+from scrapy.exceptions import NotConfigured, DontCloseSpider
-from scrapy.core.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured, NotSupported
+from scrapy.exceptions import NotConfigured, NotSupported
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotSupported
+from scrapy.exceptions import NotSupported
-from scrapy.core.exceptions import NotSupported
+from scrapy.exceptions import NotSupported
-from scrapy.core.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import IgnoreRequest, DontCloseSpider
+from scrapy.exceptions import IgnoreRequest, DontCloseSpider
-Scrapy core exceptions
+from scrapy.exceptions import *
-    pass
+import warnings
-from scrapy.core.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import IgnoreRequest, DropItem
+from scrapy.exceptions import IgnoreRequest, DropItem
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-from scrapy.core.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest
-from scrapy.core.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest
-from scrapy.core.exceptions import IgnoreRequest
+from scrapy.exceptions import IgnoreRequest
-from scrapy.core.exceptions import NotConfigured
+from scrapy.exceptions import NotConfigured
-                sender='download_http', response=response, spider=spider)
+        send_catch_log(signal=signals.request_uploaded, request=request, \
-                        sender=self.__class__, response=response, spider=spider)
+                        response=response, spider=spider)
-            send_catch_log(signal=signals.response_received, sender=self.__class__, \
+            send_catch_log(signal=signals.response_received, \
-        send_catch_log(signal=signals.engine_started, sender=self.__class__)
+        send_catch_log(signal=signals.engine_started)
-        send_catch_log(signals.spider_opened, sender=self.__class__, spider=spider)
+        send_catch_log(signals.spider_opened, spider=spider)
-        res = send_catch_log(signal=signals.spider_idle, sender=self.__class__, \
+        res = send_catch_log(signal=signals.spider_idle, \
-            spider=spider, reason=reason)
+        send_catch_log(signal=signals.spider_closed, spider=spider, reason=reason)
-        send_catch_log(signal=signals.engine_stopped, sender=self.__class__)
+        send_catch_log(signal=signals.engine_stopped)
-            send_catch_log(signal=signals.item_scraped, sender=self.__class__, \
+            send_catch_log(signal=signals.item_scraped, \
-                send_catch_log(signal=signals.item_dropped, sender=self.__class__, \
+                send_catch_log(signal=signals.item_dropped, \
-            send_catch_log(signal=signals.item_passed, sender=self.__class__, \
+            send_catch_log(signal=signals.item_passed, \
-            log.err(result, "Signal handler error", spider=spider)
+            log.err(result, "Error caught on signal handler: %s" % receiver, \
-from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, getAllReceivers
+from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \
-
+from twisted.trial import unittest
-        self.assert_(isinstance(result[0][1], Exception))
+        self.assert_(isinstance(result[0][1], Failure))
-        except DontCloseSpider:
+        res = send_catch_log(signal=signals.spider_idle, sender=self.__class__, \
-                level=log.ERROR)
+
-from scrapy.xlib.pydispatch.robust import sendRobust
+from twisted.python.failure import Failure
-    the signal handlers
+def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):
-    return results
+    dont_log = named.pop('dont_log', None)
-        log.FileLogObserver.emit(self, ev)
+        ev = _adapt_eventdict(eventDict, self.level, self.encoding)
-        site = self.sites[spider]
+        site = self.sites.get(spider, None)
-    cmdline.execute()
+from scrapy.utils.py26 import json
-    'CsvItemExporter', 'XmlItemExporter']
+    'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter', \
-from scrapy.utils.py26 import json
+from scrapy.contrib.exporter import JsonLinesItemExporter
-        self.file.write(self.encoder.encode(itemdict) + '\n')
+import warnings
-            exp = jsonlines.JsonLinesItemExporter(file, **exp_kwargs)
+            exp = exporter.JsonLinesItemExporter(file, **exp_kwargs)
-    PickleItemExporter, CsvItemExporter, XmlItemExporter
+    PickleItemExporter, CsvItemExporter, XmlItemExporter, JsonLinesItemExporter, \
-                if spider.handles_request(request)]
+        return [name for name, cls in self._spiders.iteritems()
-                log.msg('Could not find spider for url: %s' % url,
+                log.msg('Could not find spider that handles url: %s' % url,
-                        log.ERROR)
+                log.msg('More than one spider can handle url: %s - %s' % \
-                if url_is_from_spider(request.url, spider)]
+                if spider.handles_request(request)]
-            log.msg('More than one spider found for: %s' % request, log.ERROR)
+            log.msg('More than one spider can handle: %s - %s' % \
-            log.msg('Unable to find spider for: %s' % request, log.ERROR)
+            log.msg('Unable to find spider that handles: %s' % request, log.ERROR)
-            set(['spider1', 'spider2']))
+            set(['spider1', 'spider2', 'spider3']))
-        if eventDict.get('system') != 'scrapy':
+        ev = eventDict.copy()
-        level = eventDict.get('logLevel')
+        level = ev.get('logLevel')
-        message = eventDict.get('message')
+        spider = ev.get('spider')
-        why = eventDict.get('why')
+        ev['message'] = message
-        log.FileLogObserver.emit(self, eventDict)
+        ev['why'] = why
-        self.assertEqual(self.logged(), "[-] INFO: Hello")
+        self.assertEqual(self.logged(), "[scrapy] INFO: Hello")
-        self.assertEqual(self.logged(), "[-] WARNING: Hello")
+        self.assertEqual(self.logged(), "[scrapy] WARNING: Hello")
-        self.assertEqual(self.logged(), "[-] WARNING: Hello")
+        self.assertEqual(self.logged(), "[scrapy] WARNING: Hello")
-        self.assertEqual(self.logged(), "[-] NOLEVEL: Hello")
+        self.assertEqual(self.logged(), "[scrapy] NOLEVEL: Hello")
-        self.assertEqual(self.logged(), "[-] INFO: Price: \xc2\xa3100")
+        self.assertEqual(self.logged(), "[scrapy] INFO: Price: \xc2\xa3100")
-        self.assertEqual(self.logged(), "[-] INFO: World")
+        self.assertEqual(self.logged(), "[scrapy] INFO: World")
-        self.assertEqual(self.first_log_line(), "[-] ERROR: \xc2\xa3")
+# this test fails in twisted trial observer, not in scrapy observer
-        self.assertEqual(self.logged(), "[-] INFO: Price: \xa3100")
+        self.assertEqual(self.logged(), "[scrapy] INFO: Price: \xa3100")
-        self.assertEqual(self.first_log_line(), "[-] ERROR: \xa3")
+# this test fails in twisted trial observer, not in scrapy observer
-                self._start_next_spider)
+                self._spider_closed)
-        self._nextcall = reactor.callLater(0, self._spider_closed)
+        self._nextcall = reactor.callLater(0, self._start_next_spider)
-
+    urljoin_rfc, url_is_from_spider
-    return url_is_from_any_domain(url, spider.allowed_domains)
+    return url_is_from_any_domain(url, [spider.name] + spider.allowed_domains)
-from traceback import format_exc
+import logging
-SILENT, CRITICAL, ERROR, WARNING, INFO, DEBUG = range(6)
+DEBUG = logging.DEBUG
-    5: "DEBUG",
+    logging.DEBUG: "DEBUG",
-BOT_NAME = settings['BOT_NAME']
+started = False
-logmessage_received = object()
+class ScrapyFileLogObserver(log.FileLogObserver):
-log_encoding = 'utf-8'
+    def __init__(self, f, level=INFO, encoding='utf-8'):
-started = False
+    def emit(self, eventDict):
-        lvlname = settings['LOG_LEVEL'] or settings['LOGLEVEL']
+        lvlname = settings['LOG_LEVEL']
-    elif isinstance(level_name_or_id, int) and 0 <= level_name_or_id <= 5:
+    elif isinstance(level_name_or_id, int):
-
+    global started
-        logfile = logfile or settings['LOG_FILE'] or settings['LOGFILE']
+        loglevel = _get_log_level(loglevel)
-    if domain is not None:
+def msg(message, level=INFO, **kw):
-    msg(message, level, component, domain, spider)
+        warnings.warn("Argument `component` of scrapy.log.msg() is deprecated", \
-    log.err(_stuff, _why, **kwargs)
+def err(_stuff=None, _why=None, **kw):
-import unittest
+from cStringIO import StringIO
-class ItemTest(unittest.TestCase):
+class LogTest(unittest.TestCase):
-        self.assertRaises(ValueError, log._get_log_level, 99999)
+class ScrapyFileLogObserverTest(unittest.TestCase):
-        def log_received(message, level):
+        def log_received(event):
-            assert level == log.ERROR
+            assert "test_handler_error" in event['message'][0]
-        dispatcher.connect(log_received, signal=log.logmessage_received)
+        txlog.addObserver(log_received)
-        dispatcher.disconnect(log_received, signal=log.logmessage_received)
+        txlog.removeObserver(log_received)
-SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.TwistedPluginSpiderManager'
+SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.SpiderManager'
-import sys
+import inspect
-from scrapy.spider.models import ISpider
+from scrapy.utils.misc import walk_modules
-    """Spider manager based in Twisted Plugin System"""
+class SpiderManager(object):
-        return spider
+        return self._spiders[spider_name](**spider_kwargs)
-        """Load spiders from module directory."""
+        """Load spiders from spider_modules or SPIDER_MODULES setting."""
-                self._spiders[spider.name] = spider
+        self._spiders = {}
-            sys.exit(2)
+    def _load_spiders(self, module):
-            self._spiders[name] = new_module.SPIDER
+        pass
-from scrapy.contrib.spidermanager import TwistedPluginSpiderManager
+import scrapy.spider
-class TwistedPluginSpiderManagerTest(unittest.TestCase):
+class SpiderManagerTest(unittest.TestCase):
-        self.spiderman = TwistedPluginSpiderManager()
+        self.spiderman = SpiderManager()
-        self.assertEqual(set(self.spiderman.list()), 
+        self.assertEqual(set(self.spiderman.list()),
-        assert not wref()
+    def test_load_spider_module(self):
-        assert spider1 is not spider2
+    def test_load_base_spider(self):
-SPIDER = Spider2()
+import sys
-        self.assertRaises(ImportError, list, walk_modules('nomodule999'))
+        self.assertRaises(ImportError, walk_modules, 'nomodule999')
-from pkgutil import walk_packages
+from pkgutil import iter_modules
-    returns them.
+def walk_modules(path, load=False):
-    path ie: 'scrapy.contrib.downloadermiddelware.redirect'
+    For example: walk_modules('scrapy.utils')
-    yield mod
+        for _, subpath, ispkg in iter_modules(mod.__path__):
-    unittest.main()
+import unittest
-                    
+
-        elif self._read_bool_template_attribute(html_tag, "ignore-beneath") and html_tag.tag == "img":
+        elif self._read_bool_template_attribute(html_tag, "ignore-beneath"):
-        pass
+version = ".".join(map(str, __import__('scrapy').version_info[:2]))
-    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None):
+
-    """
+
-                yield r
+                yield rule.process_request(r)
-
+            rule.process_request = get_method(rule.process_request)
-                site.closing.callback(None)
+            self._check_if_closing(spider, site)
-                **getattr(spider, 'default_headers', {}))
+                **getattr(spider, 'default_request_headers', {}))
-        self.default_headers = dict([(k, [v]) for k, v in \
+        self.default_request_headers = dict([(k, [v]) for k, v in \
-        self.assertEquals(req.headers, self.default_headers)
+        self.assertEquals(req.headers, self.default_request_headers)
-    def test_spider_default_headers(self):
+    def test_spider_default_request_headers(self):
-            k = set(self.default_headers).pop()
+        if self.default_request_headers:
-        self.spider.default_headers = spider_headers
+        self.spider.default_request_headers = spider_headers
-        self.assertEquals(req.headers, dict(self.default_headers, **spider_headers))
+        self.assertEquals(req.headers, dict(self.default_request_headers, **spider_headers))
-        self.assertEquals(req.headers, self.default_headers)
+        self.default_request_headers.update(headers)
-
+from scrapy.xlib.pydispatch import dispatcher
-        self.default_headers = settings.get('DEFAULT_REQUEST_HEADERS')
+        self.global_default_headers = settings.get('DEFAULT_REQUEST_HEADERS')
-        for k, v in self.default_headers.iteritems():
+        for k, v in self._default_headers[spider].iteritems():
-from scrapy.http import Response, Request
+from scrapy.http import Request
-        self.assertEqual(request_httprepr(r1), 'GET http://www.example.com HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
+        self.assertEqual(request_httprepr(r1), 'GET / HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
-        self.assertEqual(request_httprepr(r1), 'POST http://www.example.com HTTP/1.1\r\nHost: www.example.com\r\nContent-Type: text/html\r\n\r\nSome body')
+        self.assertEqual(request_httprepr(r1), 'POST / HTTP/1.1\r\nHost: www.example.com\r\nContent-Type: text/html\r\n\r\nSome body')
-    s += "Host: %s\r\n" % hostname
+    parsed = urlparse_cached(request)
-        s += get_engine_status()
+        s += pformat(get_engine_status())
-    uc = unichr(int(name, base))
+    try:
-        if response:
+        if response and self.is_cacheable_response(response):
-        if self.is_cacheable(request):
+        if self.is_cacheable(request) and self.is_cacheable_response(response):
-        
+
-        return cls(url, method=form.method, body=body, headers=headers, **kwargs)
+        kwargs.setdefault('headers', {}).update(headers)
-META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\s*=\s*(?P<quote>["\'])(?P<int>\d+)\s*;\s*url=(?P<url>.*?)(?P=quote)', \
+META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\s*=\s*(?P<quote>["\'])(?P<int>(\d*\.)?\d+)\s*;\s*url=(?P<url>.*?)(?P=quote)', \
-            interval = int(match.group('int'))
+            interval = float(match.group('int'))
-__version__ = "0.9-rc1"
+version_info = (0, 9, 0, '')
-        # non-ascii chars in the url (default encoding - utf8)
+        # non-ascii chars in the url (utf8)
-        response = TextResponse(url='http://example.com', body=body)
+        response = TextResponse(url='http://example.com', body=body, encoding='utf-8')
-        # non-ascii chars in the url (custom encoding - latin1)
+        # non-ascii chars in the url (latin1)
-    s  = "HTTP/1.1 %d %s\r\n" % (response.status, RESPONSES[response.status])
+    s = "HTTP/1.1 %d %s\r\n" % (response.status, RESPONSES.get(response.status, ''))
-from scrapy.utils.markup import remove_entities
+from scrapy.utils.markup import remove_entities, remove_comments
-        body_chunk = remove_entities(response.body_as_unicode()[0:4096])
+        body_chunk = remove_comments(remove_entities(response.body_as_unicode()[0:4096]))
-            raise AttributeError("Use [%r] to access item field value" % name)
+            raise AttributeError("Use item[%r] to get field value" % name)
-
+This code requires the numpy library.
-_tokenize = nltk.tokenize.WordPunctTokenizer().tokenize
+# Based on nltk's WordPunctTokenizer
-if nltk and numpy:
+if numpy:
-if nltk and numpy:
+if numpy:
-__version__ = "0.9-rc1"
+version_info = (0, 10, 0, 'dev')
-__version__ = "0.9-dev"
+version_info = (0, 9, 0, 'rc1')
-        return DeferredList(dlist, consumeErrors=1).addCallback(self.item_completed, item, info)
+        dlist = [self._enqueue(r, info) for r in requests]
-        wad = Deferred().addCallbacks(request.callback, request.errback)
+        cb = request.callback or (lambda _: _)
-            return defer_result(info.downloaded[fp]).chainDeferred(wad)
+            return defer_result(info.downloaded[fp]).addCallbacks(cb, eb)
-        """
+        """Defines how to download the media request"""
-        """
+        """Check request before starting download"""
-        """
+        """Handler for success downloads"""
-        """
+        """Handler for failed downloads"""
-            bbody = open(urlparse.urlparse(burl).path).read()
+            path = urlparse.urlparse(burl).path
-#        'mybot.settings': 0,
+#    'mybot.settings': 0,
-copyright = u'2008-2009, Insophia'
+copyright = u'2008-2010, Insophia'
-    'scripts': ['bin/scrapy-ctl.py', 'bin/scrapy-ws.py'],
+    'scripts': ['bin/scrapy-ctl.py', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py'],
-    def process_item(self, domain, item):
+    def process_item(self, spider, item):
-    allow_domains = ['directory.google.com']
+    allowed_domains = ['directory.google.com']
-from twisted.mail.smtp import SMTPSenderFactory
+from twisted.mail.smtp import ESMTPSenderFactory
-        self.mailfrom = mailfrom if mailfrom else settings['MAIL_FROM']
+    def __init__(self, smtphost=None, mailfrom=None, smtpuser=None, smtppass=None, \
-        dfd = self._sendmail(self.smtphost, self.mailfrom, rcpts, msg.as_string())
+        dfd = self._sendmail(rcpts, msg.as_string())
-        instantiates a quiet (noisy=False) SMTPSenderFactory """
+    def _sendmail(self, to_addrs, msg):
-        factory = SMTPSenderFactory(from_addr, to_addrs, msg, d)
+        factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, \
-        reactor.connectTCP(smtphost, port, factory)
+        reactor.connectTCP(self.smtphost, self.smtpport, factory)
-
+from twisted.internet import defer
-    spider_ref_re = re.compile('^spider:([0-9a-f]+)(:.*)?$')
+    spider_ref_re = re.compile('^spider:([0-9a-f]+)?:?(.+)?$')
-            spid = int(m.group(1), 16)
+            spid, spname = m.groups()
-                if id(spider) == spid:
+                if "%x" % id(spider) == spid or spider.name == spname:
-from scrapy.core.queue import KeepAliveExecutionQueue
+from scrapy.utils.misc import load_object
-        scrapymanager.queue = q
+        queue_class = load_object(settings['SERVICE_QUEUE'])
-        """Called when there are no more itemsl left in self.spider_requests.
+        """Called when there are no more items left in self.spider_requests.
-                return correctly_extracted, extraction_tree.template.id
+                return correctly_extracted, extraction_tree.template
-            'surrounds_variant','match_common_prefix')
+            'surrounds_variant','match_common_prefix', 'metadata')
-            attribute_annotations = jannotation.get('annotations', {}).items()
+            attribute_annotations = jannotation.pop('annotations', {}).items()
-            if jannotation.get('common_prefix', False):
+            if jannotation.pop('common_prefix', False):
-            self.extra_required_attrs.extend(jannotation.get('required', []))
+            self.extra_required_attrs.extend(jannotation.pop('required', []))
-        if jannotation.get('generated', False):
+        if jannotation.pop('generated', False):
-        if jannotation.get('common_prefix', False):
+        if jannotation.pop('common_prefix', False):
-        self.extra_required_attrs.extend(jannotation.get('required', []))
+                
-        variant_id = jannotation.get('variant', 0)
+        variant_id = jannotation.pop('variant', 0)
-        attribute_annotations = jannotation.get('annotations', {}).items()
+        attribute_annotations = jannotation.pop('annotations', {}).items()
-    'scripts': ['bin/scrapy-ctl.py'],
+    'scripts': ['bin/scrapy-ctl.py', 'bin/scrapy-ws.py'],
-                self.outdata)
+            tolog += [">>> stdout (last %d lines) <<<" % self.TAIL_LINES]
-        log.msg(msg)
+            tolog += [">>> stderr (last %d lines) <<<" % self.TAIL_LINES]
-        return os.linesep.join(data.splitlines()[-lines:])
+        return os.linesep.join(data.split(os.linesep)[-lines:])
-                (self.botname, self.pid, self.settings_module, self.logfile))
+            msg = "Crawler %r finished: pid=%r settings=%r log=%r" % \
-            log.msg("Crawler %r died: exitstatus=%r pid=%r settings=%r log=%r" % \
+            msg = "Crawler %r died: exitstatus=%r pid=%r settings=%r log=%r" % \
-                self.logfile))
+                self.logfile)
-        pp = ScrapyProcessProtocol(self, botname, settings_module, logfile)
+        pp = ScrapyProcessProtocol(botname, settings_module, logfile)
-        self.service = service
+    def __init__(self, botname, settings_module, logfile):
-from twisted.internet import reactor, protocol, error
+from twisted.internet import reactor, defer, protocol, error
-        reactor.callWhenRunning(self.start_processes)
+        reactor.callWhenRunning(self.spawn_processes)
-            self.start_process(i+1)
+    def spawn_processes(self):
-        args = [sys.executable, '-m', 'scrapy.service']
+    def spawn_process(self, settings_module, position):
-        pp = ScrapyProcessProtocol(self, id, env.get('SCRAPY_LOG_FILE'))
+        botname = self.get_bot_name(settings_module)
-            env['SCRAPY_LOG_FILE'] = "%s-%s%s" % (file, suffix, ext)
+    def respawn_process(self, pp, args, env):
-    def __init__(self, service, id, logfile):
+    def __init__(self, service, botname, settings_module, logfile):
-        self.id = id
+        self.botname = botname
-            self.logfile))
+        log.msg("Crawler %r started: pid=%r settings=%r log=%r" % \
-                self.pid, self.logfile))
+            log.msg("Crawler %r finished: pid=%r settings=%r log=%r" % \
-    scrapymanager.start(keep_alive=True)
+            log.msg("Crawler %r died: exitstatus=%r pid=%r settings=%r log=%r" % \
-    'scrapy.management.web.WebConsole': 0,
+    'scrapy.webservice.WebService': 0,
-    'scrapy.contrib.webconsole.stats.StatsDump': 0,
+WEBSERVICE_ENABLED = True
-"""
+import warnings
-        return "<%s %r>" % (type(self).__name__, self.name)
+        return "<%s %r at 0x%0x>" % (type(self).__name__, self.name, id(self))
-
+    status = {'global': {}, 'spiders': {}}
-            s += "%-47s : %s\n" % (test, eval(test))
+            status['global'][test] = eval(test)
-    s += "\n"
+            status['global'][test] = "%s (exception)" % type(e).__name__
-        s += "Spider: %s\n" % spider
+        x = {}
-                s += "  %-50s : %s\n" % (test, eval(test))
+                x[test] = eval(test)
-                s += "  %-50s : %s (exception)\n" % (test, type(e).__name__)
+                x[test] = "%s (exception)" % type(e).__name__
-    print get_engine_status(engine)
+    print format_engine_status(engine)
-        r1 = self.request_class("http://www.example.com", callback=somecallback)
+        r1 = self.request_class("http://www.example.com", callback=somecallback, errback=somecallback)
-        assert r1.deferred is not r2.deferred
+        # make sure copy does not propagate callbacks
-        wad = request.deferred or Deferred()
+        wad = Deferred().addCallbacks(request.callback, request.errback)
-                r = Request(url=link.url)
+                callback = partial(self._response_downloaded, callback=rule.callback, \
-        request.deferred.callbacks[0] = new_cbs
+        request.callback = self._profiled_callback(request.callback, spider)
-                return newrequest.deferred
+                dfd = mustbe_deferred(self.schedule, newrequest, spider)
-        return request.deferred.addCallback(iterate_spider_output)
+        dfd = defer_result(result)
-        'dont_filter', 'headers', 'cookies', 'deferred', 'priority', \
+        'dont_filter', 'headers', 'cookies', 'callback', 'errback', 'priority', \
-        self.deferred = callback or defer.Deferred()
+        assert callback or not errback, "Cannot use errback without a callback"
-        return Request(url, callback=self.parse, dont_filter=True)
+        return Request(url, dont_filter=True)
-        pass
+        raise NotImplementedError
-from scrapy.spider.middleware import SpiderMiddlewareManager
+from scrapy.core.spidermw import SpiderMiddlewareManager
-    'scrapy.management.telnet.TelnetConsole': 0,
+    'scrapy.telnet.TelnetConsole': 0,
-        insults.ServerProtocol, manhole.Manhole, telnet_namespace)
+# signal to update telnet variables
-        self.protocol = makeProtocol
+
-
+        assert spider not in self.sites, "Spider already opened: %s" % spider
-        site.closing = True
+        site.closing = defer.Deferred()
-        return defer.succeed(None)
+        if spider in self.closing:
-        dfd.addErrback(log.err, "Unhandled error on SpiderManager.close_spider()",
+        dfd = defer.maybeDeferred(stats.close_spider, spider, reason=reason)
-            raise RuntimeError('Scraper spider already opened: %s' % spider)
+        assert spider not in self.sites, "Spider already opened: %s" % spider
-        self.sites.pop(spider)
+        assert spider in self.sites, "Spider not opened: %s" % spider
-        reactor.callLater(0, self._spider_closed_callback)
+        dfd.addBoth(lambda _: self._spider_closed_callback(spider))
-    def _spider_closed(self):
+    def _spider_closed(self, spider=None):
-        rsp = Response(url, headers={'Location': url2}, status=301)
+        def _test(method):
-        self.assertEqual(req2.url, url2)
+            req2 = self.mw.process_response(req, rsp, self.spider)
-        assert self.mw.process_response(req, rsp, self.spider) is rsp
+            # response without Location header but with status code is 3XX should be ignored
-from unittest import TestCase
+from twisted.trial.unittest import TestCase, SkipTest
-        
+
-    def _run(self, name, templates, page, extractors, expected_output):
+    def setUp(self):
-                self._run(*data)
+                self._run_extraction(*data)
-from unittest import TestCase
+from twisted.trial.unittest import TestCase, SkipTest
-from scrapy.contrib.ibl.extraction.pageobjects import TokenDict, TokenType
+try:
-import unittest
+import shutil
-# just a hack to avoid cyclic imports of scrapy.spider when running this test
+from twisted.trial import unittest
-        self.spiderman.load(['scrapy.tests.test_contrib_spidermanager'])
+        self.spiderman.load(['test_spiders_xxx'])
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-            spider=spider, reason='closespider_timeout')
+        self.tasks[spider] = reactor.callLater(self.timeout, \
-            scrapyengine.close_spider(spider, 'closespider_itempassed')
+            scrapymanager.engine.close_spider(spider, 'closespider_itempassed')
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-            dfd = scrapyengine.download(robotsreq, spider)
+            dfd = scrapymanager.engine.download(robotsreq, spider)
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-                scrapyengine.close_spider(spider)
+                scrapymanager.engine.close_spider(spider)
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-        return scrapyengine.download(request, info.spider)
+        return scrapymanager.engine.download(request, info.spider)
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-            lastseen = scrapyengine.downloader.sites[spider].lastseen
+            lastseen = scrapymanager.engine.downloader.sites[spider].lastseen
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-        pending = scrapyengine.scheduler.pending_requests.get(spider, [])
+        pending = scrapymanager.engine.scheduler.pending_requests.get(spider, [])
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-        dwl = scrapyengine.downloader
+        sch = scrapymanager.engine.scheduler
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-        for domain, request_queue in scrapyengine.scheduler.pending_requests.iteritems():
+        for domain, request_queue in scrapymanager.engine.scheduler.pending_requests.iteritems():
-                    scrapyengine.close_spider(self.running[name])
+                    scrapymanager.engine.close_spider(self.running[name])
-                if name not in scrapyengine.scheduler.pending_requests:
+                if name not in scrapymanager.engine.scheduler.pending_requests:
-                if name not in scrapyengine.scheduler.pending_requests:
+                if name not in scrapymanager.engine.scheduler.pending_requests:
-from scrapy.core.engine import scrapyengine
+from scrapy.core.engine import ExecutionEngine
-        self.engine = scrapyengine
+        self.engine = ExecutionEngine()
-    'engine': scrapyengine,
+    'engine': scrapymanager.engine,
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-    uptime = time() - scrapyengine.start_time
+    uptime = time() - scrapymanager.engine.start_time
-        response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, \
+        scrapymanager.engine.open_spider(spider)
-from scrapy.core.engine import scrapyengine
+from scrapy.core.manager import scrapymanager
-        engine = scrapyengine
+        engine = scrapymanager.engine
-from scrapy.command.models import ScrapyCommand
+from scrapy.command import ScrapyCommand
-from scrapy.command.models import ScrapyCommand
+"""
-from scrapy.command.cmdline import execute
+from scrapy.cmdline import execute
-from scrapy.command.cmdline import execute
+from scrapy.cmdline import execute
-from scrapy.command.cmdline import execute
+from scrapy.cmdline import execute
-from scrapy.command.cmdline import execute
+from scrapy.cmdline import execute
-from __future__ import with_statement
+# TODO: remove this module for Scrapy 0.10
-import cProfile
+from scrapy import cmdline
-    execute()
+def execute():
-from scrapy.command.cmdline import execute
+from scrapy.cmdline import execute
-        args = (sys.executable, '-m', 'scrapy.command.cmdline') + new_args
+        args = (sys.executable, '-m', 'scrapy.cmdline') + new_args
-        args = (sys.executable, '-m', 'scrapy.command.cmdline') + new_args
+        args = (sys.executable, '-m', 'scrapy.cmdline') + new_args
-    cmds = _get_commands_from_module('scrapy.command.commands')
+    cmds = _get_commands_from_module('scrapy.commands')
-            scrapymanager.crawl_spider_name(name)
+            q.append_spider_name(name)
-                    scrapymanager.crawl_url(url, spider)
+                    q.append_url(url, spider)
-                log.msg('Could not find spider: %s' % opts.spider, log.ERROR)
+                log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)
-                    scrapymanager.crawl_url(url, spider)
+                    q.append_url(url, spider)
-        request = Request(args[0], callback=responses.append, dont_filter=True)
+        cb = lambda x: self._print_response(x, opts)
-        scrapymanager.crawl_request(request, spider)
+        scrapymanager.configure()
-        request = Request(args[0])
+        responses = [] # to collect downloaded responses
-                log.msg('Could not find spider: %s' % opts.spider, log.ERROR)
+                log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)
-        request = request.replace(callback=responses.append)
+            spider = spiders.create_for_request(request)
-        scrapymanager.crawl_request(request, spider)
+        scrapymanager.configure()
-        scrapymanager.crawl_spider(module.SPIDER)
+        scrapymanager.queue.append_spider(module.SPIDER)
-        scrapymanager.start(keep_alive=True)
+        q = KeepAliveExecutionQueue()
-from scrapy.management.web import banner
+from scrapy.management.web import banner, webconsole_discover_module
-        self.scheduled = [s.name for s in scrapyengine.spider_scheduler._pending_spiders]
+        self.scheduled = [s[0].name for s in scrapymanager.queue.spider_requests]
-                    removed.append(name)
+                q = scrapymanager.queue
-                    scrapymanager.crawl_spider_name(name)
+                    scrapymanager.queue.append_spider_name(name)
-                    scrapymanager.crawl_spider_name(name)
+                    scrapymanager.queue.append_spider_name(name)
-from twisted.internet import reactor, task, defer
+from twisted.internet import reactor, defer
-    def configure(self):
+    def configure(self, spider_closed_callback):
-        self.spider_scheduler = load_object(settings['SPIDER_SCHEDULER'])()
+        self._spider_closed_callback = spider_closed_callback
-            return
+        assert not self.running, "Engine already running"
-            return
+        assert self.running, "Engine not running"
-            pass
+        dfd = self._close_all_spiders()
-            return
+        assert not self.running, "Call engine.stop() before engine.kill()"
-
+    def has_capacity(self):
-
+        assert self.has_capacity(), "No free spider slots when opening %r" % \
-
+        self.scheduler.open_spider(spider)
-
+        self.next_request(spider)
-        reactor.callLater(0, self._mainloop)
+        reactor.callLater(0, self._spider_closed_callback)
-from twisted.internet import reactor
+from twisted.internet import reactor, defer
-        self.interrupted = False
+        self.engine = scrapyengine
-    def configure(self, control_reactor=True):
+    def configure(self, control_reactor=True, queue=None):
-        scrapyengine.configure()
+        self.queue = queue or ExecutionQueue()
-        scrapyengine.start()
+    @defer.inlineCallbacks
-        return default
+        if self._nextcall.active():
-        reactor.callFromThread(scrapyengine.kill)
+        reactor.callFromThread(self.engine.kill)
-            BaseSpider('default'), log_multiple=True)
+        spider = spiders.create_for_request(request, BaseSpider('default'), \
-        scrapymanager.start(keep_alive=True)
+        scrapymanager.queue = KeepAliveExecutionQueue()
-            scrapymanager.crawl_spider(self.spider)
+            scrapymanager.queue.append_spider(self.spider)
-from twisted.internet import defer
+from twisted.internet import defer, threads
-    request_priority = 1000
+    AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']
-                return {'checksum': checksum, 'last_modified': modified_stamp}
+        def _onsuccess(boto_key):
-        return self._download_request(req, info).addCallback(_onsuccess)
+    def _get_boto_bucket(self):
-
+        headers = {'Cache-Control': 'max-age=172800'} # 2 days of cache
-        return scrapyengine.download(request, info.spider)
+        return threads.deferToThread(k.set_contents_from_file, buf, headers, \
-class HttpErrorException(IgnoreRequest):
+class HttpError(IgnoreRequest):
-        super(HttpErrorException, self).__init__(*args, **kwargs)
+        super(HttpError, self).__init__(*args, **kwargs)
-        raise HttpErrorException(response, 'Ignoring non-200 response')
+        raise HttpError(response, 'Ignoring non-200 response')
-from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpErrorException
+from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpError
-        self.assertRaises(HttpErrorException,
+        self.assertRaises(HttpError,
-
+    def _process_links(self, links):
-        return self._extract_links(response.body, response.url, response.encoding)
+        links = self._extract_links(response.body, response.url, response.encoding)
-            html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.select(xpath_expr).extract()) \
+            html = ''.join(''.join(html_fragm for html_fragm in hxs.select(xpath_expr).extract()) \
-            links = BaseSgmlLinkExtractor.extract_links(self, response)
+            html = response.body
-    elif hasattr(arg, '__iter__'):
+    elif not isinstance(arg, dict) and hasattr(arg, '__iter__'):
-        self.add_value(field_name, self._get_values(field_name, xpath, re))
+    def add_xpath(self, field_name, xpath, *processors, **kw):
-        self.replace_value(field_name, self._get_values(field_name, xpath, re))
+    def replace_xpath(self, field_name, xpath, *processors, **kw):
-        return x.re(re) if re else x.extract()
+    def get_xpath(self, xpath, *processors, **kw):
-from scrapy.utils.misc import arg_to_iter
+from scrapy.utils.misc import arg_to_iter, extract_regex
-        value = self.get_value(value, *processors)
+    def add_value(self, field_name, value, *processors, **kw):
-        value = self.get_value(value, *processors)
+    def replace_value(self, field_name, value, *processors, **kw):
-    def get_value(self, value, *processors):
+    def get_value(self, value, *processors, **kw):
-    def add_value(self, field_name, value):
+    def add_value(self, field_name, value, *processors):
-        self._values[field_name] += arg_to_iter(processed_value)
+        if processed_value:
-    def replace_value(self, field_name, value):
+    def _replace_value(self, field_name, value):
-        self.add_value(field_name, value)
+        self._add_value(field_name, value)
-            proc = self.item.fields[field_name].get('input_processor', \
+            proc = self._get_item_field_attr(field_name, 'input_processor', \
-            proc = self.item.fields[field_name].get('output_processor', \
+            proc = self._get_item_field_attr(field_name, 'output_processor', \
-        for link in links:
+        for link in self.links:
-__cvsid__ = "$Id$"
+__cvsid__ = "$Id: dispatcher.py,v 1.1.1.1 2006/07/07 15:59:38 mcfletch Exp $"
-				else:
+	try:
-					except Exception, err:
+						receivers = connections[senderkey][signal]
-			
+					else:
-	if callable(onDelete):
+	if onDelete is not None:
-		self.selfName = str(target.im_self)
+		self.selfName = target.im_self.__class__.__name__
-Copyright (c) 2004-2008, Leonard Richardson
+Copyright (c) 2004-2010, Leonard Richardson
-__copyright__ = "Copyright (c) 2004-2008 Leonard Richardson"
+__version__ = "3.0.8.1"
-class PageElement:
+class PageElement(object):
-        if hasattr(replaceWith, 'parent') and replaceWith.parent == self.parent:
+        myIndex = self.parent.index(self)
-            index = self.parent.contents.index(replaceWith)
+            index = replaceWith.parent.index(replaceWith)
-                self.parent.contents.remove(self)
+                del self.parent.contents[self.parent.index(self)]
-            or isinstance(newChild, unicode)) \
+        if isinstance(newChild, basestring) \
-        if hasattr(newChild, 'parent') and newChild.parent != None:
+        if hasattr(newChild, 'parent') and newChild.parent is not None:
-                if index and index < position:
+            if newChild.parent is self:
-        while i:
+        while i is not None:
-        while i:
+        while i is not None:
-        while i:
+        while i is not None:
-        while i:
+        while i is not None:
-        while i:
+        while i is not None:
-        if attrs == None:
+        if attrs is None:
-                if isString(val):
+                if isinstance(val, basestring):
-                i.extract()
+        if len(self.contents) == 0:
-        raise StopIteration
+        # Just use the iterator from the contents
-        raise StopIteration
+        if not len(self.contents):
-            kwargs['class'] = attrs
+        if isinstance(attrs, basestring):
-        if isList(markup) and not isinstance(markup, Tag):
+        if hasattr(markup, "__iter__") \
-                 isString(markup):
+                 isinstance(markup, basestring):
-            result = markup != None
+        if matchAgainst is True:
-            if markup and not isString(markup):
+            if markup and not isinstance(markup, basestring):
-            elif isList(matchAgainst):
+            elif hasattr(matchAgainst, '__iter__'): # list-like
-            elif matchAgainst and isString(markup):
+            elif matchAgainst and isinstance(markup, basestring):
-        elif isList(portion):
+        elif hasattr(portion, '__iter__'): # is a list
-                if not isList(self.markupMassage):
+                if not hasattr(self.markupMassage, "__iter__"):
-               or methodName.find('do_') == 0:
+        if methodName.startswith('start_') or methodName.startswith('end_') \
-        elif methodName.find('__') != 0:
+        elif not methodName.startswith('__'):
-            if (nestingResetTriggers != None
+            if (nestingResetTriggers is not None
-                or (nestingResetTriggers == None and isResetNesting
+                or (nestingResetTriggers is None and isResetNesting
-            attrs = ''.join(map(lambda(x, y): ' %s="%s"' % (x, y), attrs))
+            attrs = ''.join([' %s="%s"' % (x, y) for x, y in attrs])
-                                    'spacer', 'link', 'frame', 'base'])
+                                    ('br' , 'hr', 'input', 'img', 'meta',
-                            'center']
+    NESTABLE_INLINE_TAGS = ('span', 'font', 'q', 'object', 'bdo', 'sub', 'sup',
-    NESTABLE_BLOCK_TAGS = ['blockquote', 'div', 'fieldset', 'ins', 'del']
+    NESTABLE_BLOCK_TAGS = ('blockquote', 'div', 'fieldset', 'ins', 'del')
-    NON_NESTABLE_BLOCK_TAGS = ['address', 'form', 'p', 'pre']
+    NON_NESTABLE_BLOCK_TAGS = ('address', 'form', 'p', 'pre')
-     ['em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',
+     ('em', 'big', 'i', 'small', 'tt', 'abbr', 'acronym', 'strong',
-      'big']
+      'big')
-    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ['noscript']
+    I_CANT_BELIEVE_THEYRE_NESTABLE_BLOCK_TAGS = ('noscript',)
-        if type(sub) == types.TupleType:
+        if isinstance(sub, tuple):
-    return urlparse.urljoin(base, '?'+sep.join(querylist))
+    return '?'.join([base, sep.join(querylist)]) if querylist else base
-A hierarchical approach to wrapper induction
+    A hierarchical approach to wrapper induction
-Extracting web data using instance based learning
+    Extracting web data using instance based learning
-def extract_regex(regex, text, encoding):
+def extract_regex(regex, text, encoding='utf-8'):
-import sys, os
+import sys, os, warnings
-
+import os
-from tempfile import NamedTemporaryFile
+import tempfile
-        webbrowser.open("file://%s" % f.name)
+    fd, fname = tempfile.mkstemp('.html')
-                    % (request, referer), level=log.WARNING, spider=info.spider)
+            log.msg('Image (code: %s): Error downloading image from %s referred in <%s>' \
-def remove_tags(text, which_ones=(), encoding=None):
+def remove_tags(text, which_ones=(), keep=(), encoding=None):
-                      if is empty remove all tags.
+        which_ones and keep are both tuples, there are four cases:
-        regex = '<.*?>'
+
-    return retags.sub(u'', str_to_unicode(text, encoding))
+    return retags.sub(remove_tag, str_to_unicode(text, encoding))
-        tags = ['<%s>|<%s .*?>|</%s>' % (tag, tag, tag) for tag in which_ones]
+        tags = ['<%s/?>|<%s .*?>|</%s>' % (tag, tag, tag) for tag in which_ones]
-        tags = '|'.join(['<%s.*?</%s>' % (tag, tag) for tag in which_ones])
+        tags = '|'.join([r'<%s.*?</%s>|<%s\s*/>' % (tag, tag, tag) for tag in which_ones])
-        return []
+        raise HttpErrorException(response, 'Ignoring non-200 response')
-                    return result
+                try:
-from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware
+from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpErrorException
-                          [])
+        self.assertEquals(None,
-                          None)
+        self.assertEquals(None,
-                          None)
+        self.assertEquals(None,
-    """Clean url arguments leaving only those passed in the parameterlist.
+    """Clean url arguments leaving only those passed in the parameterlist keeping order
-    for k, s, v in parameters:
+    for ksv in query.split(sep):
-            querylist.append([k, s, v])
+            querylist.append(ksv)
-    return urlparse.urljoin(base, query)
+    return urlparse.urljoin(base, '?'+sep.join(querylist))
-            from scrapy.contrib.exporter import jsonlines
+        self.assertEqual('product.html',
-    return base + '?' + sep.join(''.join(ksv) for ksv in querylist)
+    query = '?' + sep.join(''.join(ksv) for ksv in querylist)
-                         'product.html?id=200&foo=bar')
+        self.assertEqual('product.html?id=200',
-        parameters = []
+def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='=', remove=False, unique=True):
-    unique = {}
+    seen = set()
-    return '?'.join([base, query])
+    for k, s, v in parameters:
-    import simplejson as json
+from scrapy.utils.py26 import json
-import cPickle as pickle
+import unittest, cPickle as pickle
-
+from scrapy.utils.py26 import json
-        from scrapy.contrib.exporter.jsonlines import json
+
-from scrapy.utils.python import ignore_patterns, copytree
+from scrapy.utils.py26 import ignore_patterns, copytree
-from shutil import copytree, ignore_patterns, copy2, copystat
+from shutil import copy2, copystat
-if sys.version_info < (2, 6):
+if sys.version_info >= (2, 6):
-import sys, os
+import sys
-
+import sys, os
-from scrapy.utils.httpobj import urlparse_cached
+from time import strftime, gmtime
-                time.gmtime())
+        if request.meta.get('sign_s3_request'):
-                priority=self.request_priority)
+                meta={'sign_s3_request': True}, priority=self.request_priority)
-
+import codecs
-
+
-            self._cached_ubody = self.body.decode(self.encoding, 'replace')
+            self._cached_ubody = self.body.decode(self.encoding, 'scrapy_replace')
-        self.assertEqual(r6.body_as_unicode(), u'\ufeffWORD\ufffd')
+        self.assertEqual(r6.body_as_unicode(), u'\ufeffWORD\ufffd\ufffd')
-    'x-sjis': 'shift-jis',
+    # gb2312 is superseded by gb18030
-    'latin_1': 'cp1252',
+    # others
-                BaseSpider())
+                BaseSpider('default'))
-    def __init__(self):
+    def __init__(self, *a, **kw):
-        super(CrawlSpider, self).__init__()
+        super(CrawlSpider, self).__init__(*a, **kw)
-            BaseSpider(), log_multiple=True)
+            BaseSpider('default'), log_multiple=True)
-            self.name = 'default'
+            raise ValueError("%s must have a name" % type(self).__name__)
-        spider = BaseSpider()
+        spider = BaseSpider('foo')
-        spider = BaseSpider()
+        spider = BaseSpider('foo')
-        spider = BaseSpider()
+        spider = BaseSpider('foo')
-        spider = BaseSpider()
+        spider = BaseSpider('foo')
-        spider = BaseSpider()
+        spider = BaseSpider('foo')
-        spider = BaseSpider()
+        spider = BaseSpider('foo')
-        return Spider()
+        return Spider('foo')
-        return download_file(request, BaseSpider()).addCallback(_test)
+        return download_file(request, BaseSpider('foo')).addCallback(_test)
-        d = download_file(request, BaseSpider())
+        d = download_file(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider())
+        d = download_http(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider())
+        d = download_http(request, BaseSpider('foo'))
-        spider = BaseSpider()
+        spider = BaseSpider('foo')
-        return download_http(request, BaseSpider()).addCallback(_test)
+        return download_http(request, BaseSpider('foo')).addCallback(_test)
-        return download_http(request, BaseSpider()).addCallback(_test)
+        return download_http(request, BaseSpider('foo')).addCallback(_test)
-        d = download_http(request, BaseSpider())
+        d = download_http(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider())
+        d = download_http(request, BaseSpider('foo'))
-        d = download_http(request, BaseSpider())
+        d = download_http(request, BaseSpider('foo'))
-        return download_http(request, BaseSpider()).addCallback(_test)
+        return download_http(request, BaseSpider('foo')).addCallback(_test)
-        return download_http(request, BaseSpider()).addCallback(_test)
+        return download_http(request, BaseSpider('foo')).addCallback(_test)
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        spider = TestSpider()
+        spider = TestSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-spider = BaseSpider()
+spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        spider = BaseSpider()
+        spider = BaseSpider('foo')
-        spider = BaseSpider()
+        spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.assertEqual(spider.foo, 'bar')
+        self.assertRaises(ValueError, self.spider_class)
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-        self.spider = BaseSpider()
+        self.spider = BaseSpider('foo')
-            warnings.warn("Spider.domain_name attribute is deprecated, use Spider.name instead", \
+            warnings.warn("Spider.domain_name attribute is deprecated, use Spider.name instead and Spider.allowed_domains", \
-            self.allowed_domains = [self.name] + list(self.extra_domain_names)
+            self.allowed_domains = [self.name]
-            self.extra_domain_names = self.allowed_domains
+        # XXX: SEP-12 forward compatibility (remove for 0.10)
-                BaseSpider('default'))
+                BaseSpider())
-            BaseSpider('default'), log_multiple=True)
+            BaseSpider(), log_multiple=True)
-            raise ValueError("%s must have a name" % type(self).__name__)
+            self.name = 'default'
-        self.assertRaises(ValueError, self.spider_class, somearg='foo')
+        spider = self.spider_class()
-    def create(self, spider_id):
+    def create(self, spider_name, **spider_kwargs):
-        return self._spiders[spider_id]
+        spider = self._spiders[spider_name]
-        return [domain for domain, spider in self._spiders.iteritems()
+        """Returns list of spiders names that match the given Request"""
-    def __init__(self):
+    def __init__(self, *a, **kw):
-        super(CrawlSpider, self).__init__()
+        super(CrawlSpider, self).__init__(*a, **kw)
-        super(InitSpider, self).__init__()
+    def __init__(self, *a, **kw):
-    def __init__(self, name=None):
+    def __init__(self, name=None, **kwargs):
-class OldSpider(BaseSpider):
+class BaseSpiderTest(unittest.TestCase):
-    extra_domain_names = ('example.org', 'example.net')
+    spider_class = BaseSpider
-class NewSpider(BaseSpider):
+    class OldSpider(spider_class):
-    allowed_domains = ('example.org', 'example.net')
+        domain_name = 'example.com'
-            spider = OldSpider()
+            spider = self.OldSpider()
-        spider = OldSpider()
+        spider = self.OldSpider()
-        spider = NewSpider()
+        spider = self.NewSpider()
-        spider = BaseSpider("example.com")
+        spider = self.spider_class("example.com")
-    domain_name = 'directory.google.com'
+    name = 'google_directory'
-    domain_name = 'imdb.com'
+    name = 'imdb.com'
-    domain_name = 'directory.google.com'
+    name = 'directory.google.com'
-        return "[options] <domain|url> ..."
+        return "[options] <spider|url> ..."
-        return "Start crawling a domain or URL"
+        return "Start crawling from a spider or URL"
-            scrapymanager.crawl_domain(dom)
+        urls, names = self._split_urls_and_names(args)
-    def _split_urls_and_domains(self, args):
+    def _split_urls_and_names(self, args):
-        domains = []
+        names = []
-        return urls, domains
+                names.append(arg)
-    prefixing it with a letter if it doesn't start with one
+    """Sanitize the given module name, by replacing dashes and points
-    module_name = module_name.replace('-', '_')
+    module_name = module_name.replace('-', '_').replace('.', '_')
-        return "[options] <spider_module_name> <spider_domain_name>"
+        return "[options] <name> <domain>"
-        if len(args) < 2:
+        if len(args) != 2:
-        module = sanitize_module_name(args[0])
+        name = args[0]
-            spider = spiders.create(domain)
+            spider = spiders.create(name)
-                print "Spider '%s' already exists in module:" % domain
+                print "Spider '%s' already exists in module:" % name
-            self._genspider(module, domain, opts.template, template_file)
+            self._genspider(module, name, domain, opts.template, template_file)
-    def _genspider(self, module, domain, template_name, template_file):
+    def _genspider(self, module, name, domain, template_name, template_file):
-            'site': domain,
+            'name': name,
-        print "Created spider %r using template %r in module:" % (domain, \
+        print "Created spider %r using template %r in module:" % (name, \
-                log.msg('Cannot find callback %s in %s spider' % (callback, spider.domain_name))
+                log.msg('Cannot find callback %s in %s spider' % (callback, spider.name))
-                        % spider.domain_name, log.ERROR)
+                        % spider.name, log.ERROR)
-        if spider.domain_name == 's3.amazonaws.com' \
+        if spider.name == 's3.amazonaws.com' \
-        return join(self.cachedir, spider.domain_name, key[0:2], key)
+        return join(self.cachedir, spider.name, key[0:2], key)
-domain.
+spider.
-It uses the scrapy stats service to keep track of which domains are already
+It uses the scrapy stats service to keep track of which spiders are already
-        self.empty_domains = set()
+        self.empty_spiders = set()
-            log.msg("No products sampled for: %s" % " ".join(self.empty_domains), \
+        if self.empty_spiders:
-            self.empty_domains.add(spider.domain_name)
+            self.empty_spiders.add(spider.name)
-            len(self.empty_domains)), level=log.INFO)
+        log.msg("Sampled %d spiders so far (%d empty)" % (self.spiders_count, \
-    completed) to accelerate the processing of remaining domains"""
+    """This middleware drops items and requests (when spider sampling has been
-        self.created_directories.pop(spider.domain_name, None)
+        self.created_directories.pop(spider.name, None)
-    domain_name = "s3.amazonaws.com"
+    name = "s3.amazonaws.com"
-                self._spiders[spider.domain_name] = spider
+                self._spiders[spider.name] = spider
-        if domain not in self._spiders:
+        name = spider.name
-        spider = self._spiders[domain]
+        spider = self._spiders[name]
-            self._spiders[domain] = new_module.SPIDER
+            self._spiders[name] = new_module.SPIDER
-        self.host_regexes[spider] = self.get_host_regex(domains)
+        self.host_regexes[spider] = self.get_host_regex(spider.allowed_domains)
-        body += "\n\n%s stats\n\n" % spider.domain_name
+        body += "\n\n%s stats\n\n" % spider.name
-        mail.send(self.recipients, "Scrapy stats for: %s" % spider.domain_name, body)
+        mail.send(self.recipients, "Scrapy stats for: %s" % spider.name, body)
-                 (spider.domain_name, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(runtime))
+                 (spider.name, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(runtime))
-        self.running[spider.domain_name] = spider
+        self.running[spider.name] = spider
-        self.finished.add(spider.domain_name)
+        del self.running[spider.name]
-        self.idle = [d for d in self.enabled_domains if d not in self.scheduled
+        self.scheduled = [s.name for s in scrapyengine.spider_scheduler._pending_spiders]
-            s += "<option>%s</option>\n" % domain
+        s += '<select name="add_pending_spiders" multiple="multiple">\n'
-            s += "<option>%s</option>\n" % domain
+        s += '<select name="remove_pending_spiders" multiple="multiple">\n'
-            s += "<option>%s</option>\n" % domain
+        s += '<select name="stop_running_spiders" multiple="multiple">\n'
-            s += "<option>%s</option>\n" % domain
+        s += '<select name="rerun_finished_spiders" multiple="multiple">\n'
-        if "stop_running_domains" in args:
+        if "stop_running_spiders" in args:
-            s += "Stopped spiders: <ul><li>%s</li></ul>" % "</li><li>".join(stopped_domains)
+            stopped_spiders = []
-        if "remove_pending_domains" in args:
+        if "remove_pending_spiders" in args:
-                    removed.append(domain)
+            for name in args["remove_pending_spiders"]:
-                s += "Removed scheduled spiders: <ul><li>%s</li></ul>" % "</li><li>".join(args["remove_pending_domains"])
+                s += "Removed scheduled spiders: <ul><li>%s</li></ul>" % "</li><li>".join(args["remove_pending_spiders"])
-                    scrapymanager.crawl_domain(domain)
+        if "add_pending_spiders" in args:
-            s += "Scheduled spiders: <ul><li>%s</li></ul>" % "</li><li>".join(args["add_pending_domains"])
+            s += "Scheduled spiders: <ul><li>%s</li></ul>" % "</li><li>".join(args["add_pending_spiders"])
-                self.finished.remove(domain)
+        if "rerun_finished_spiders" in args:
-            s += "Re-scheduled finished spiders: <ul><li>%s</li></ul>" % "</li><li>".join(args["rerun_finished_domains"])
+            s += "Re-scheduled finished spiders: <ul><li>%s</li></ul>" % "</li><li>".join(args["rerun_finished_spiders"])
-        self.enabled_domains = spiders.list()
+        self.enabled_spiders = spiders.list()
-            s += "<h3>%s</h3>\n" % spider.domain_name
+            s += "<h3>%s</h3>\n" % spider.name
-        """Schedule given domain for crawling."""
+    def crawl_spider_name(self, name):
-            spider = spiders.create(domain)
+            spider = spiders.create(name)
-            log.msg('Could not find spider for domain: %s' % domain, log.ERROR)
+            log.msg('Could not find spider: %s' % name, log.ERROR)
-    system = domain or (spider.domain_name if spider else component)
+    system = domain or (spider.name if spider else component)
-    kwargs['system'] = domain or (spider.domain_name if spider else component)
+    kwargs['system'] = domain or (spider.name if spider else component)
-    invariant(_valid_domain_name)
+    """Interface used by TwistedPluginSpiderManager to discover spiders"""
-    domain_name = None
+    name = None
-            self.domain_name = domain_name
+    allowed_domains = []
-            self.extra_domain_names = []
+        if not self.allowed_domains:
-        return "<%s %r>" % (type(self).__name__, self.domain_name)
+        return "<%s %r>" % (type(self).__name__, self.name)
-        self.domain_stats = {}
+        self.spider_stats = {}
-            self.domain_stats[spider.domain_name] = stats
+            self.spider_stats[spider.name] = stats
-        sdb_item_id = "%s_%s" % (spider.domain_name, ts)
+        sdb_item_id = "%s_%s" % (spider.name, ts)
-        sdb_item['domain'] = spider.domain_name
+        sdb_item['spider'] = spider.name
-        self.assertEqual(1, self.call('genspider', 'otherspider', 'test.com'))
+        self.assertEqual(0, self.call('genspider', 'test_spider', 'test.com', *args))
-    extra_domain_names = ["localhost"]
+    name = "scrapytest.org"
-        self.domain = 'scrapytest.org'
+        self.name = 'scrapytest.org'
-        self.assertEqual(session.spider.domain_name, session.domain)
+        self.assertEqual(session.spider.name, session.name)
-        self.spider.extra_domain_names = ['scrapy.org']
+        self.spider.name = 'scrapytest.org'
-    return url_is_from_any_domain(url, domains)
+    return url_is_from_any_domain(url, spider.allowed_domains)
-        # schedule first domains
+        urls, domains = self._split_urls_and_domains(args)
-                scrapymanager.crawl_url(url, spider)
+        if opts.spider:
-                # instance spider for each url-list
+            for name, urls in self._group_urls_by_spider(urls):
-        # crawl just scheduled arguments without keeping idle
+
-from scrapy.utils.fetch import fetch
+from scrapy.core.manager import scrapymanager
-        responses = fetch(args)
+        if len(args) != 1 or not is_url(args[0]):
-from scrapy.utils.fetch import fetch
+from scrapy.core.manager import scrapymanager
-            result = callback_fcn(response)
+            result = iterate_spider_output(callback_fcn(response))
-            print "An URL is required"
+        if not len(args) == 1 or not is_url(args[0]):
-                    continue
+        responses = [] # to collect downloaded responses
-                self.print_results(items, links, 'parse', opts)
+                log.msg('No rules found for spider "%s", ' \
-
+from scrapy import log
-                scrapymanager.crawl_url(arg)
+                urls.append(arg)
-                scrapymanager.crawl_domain(arg)
+                domains.append(arg)
-            sys.exit(1)
+
-            log.msg('Cannot find spider for url: %s' % response.url, level=log.ERROR)
+        spider_names = spiders.find_by_request(response.request)
-        return self._spiders.get(domain)
+    def create(self, spider_id):
-                        return p
+    def find_by_request(self, request):
-from scrapy.spider import BaseSpider, spiders
+from scrapy.spider import spiders
-        spider = spider or spiders.fromurl(url)
+        if spider is None:
-        spider = spider or spiders.fromurl(request.url)
+        if spider is None:
-        else:
+        try:
-        spider = spiders.fromurl(url) or BaseSpider('default')
+
-            self.populate_vars(url, response, request)
+            self.populate_vars(url, response, request, spider)
-    def populate_vars(self, url=None, response=None, request=None):
+    def populate_vars(self, url=None, response=None, request=None, spider=None):
-            self.vars['spider'] = spiders.fromurl(url)
+            self.vars['spider'] = spider
-        scrapymanager.runonce(*args)
+        for arg in args:
-        scrapymanager.runonce(module.SPIDER)
+
-        scrapymanager.start(*args)
+        scrapymanager.start(keep_alive=True)
-                    scrapymanager.crawl(domain)
+                    scrapymanager.crawl_domain(domain)
-                    scrapymanager.crawl(domain)
+                    scrapymanager.crawl_domain(domain)
-        """Schedule the given args for crawling. args is a list of urls or domains"""
+    def crawl_url(self, url, spider=None):
-            reactor.run(installSignalHandlers=False)
+        spider = spider or spiders.fromurl(request.url)
-    def start(self):
+    def start(self, keep_alive=False):
-        scrapyengine.keep_alive = True
+        scrapyengine.keep_alive = keep_alive
-        scrapymanager.start()
+        scrapymanager.start(keep_alive=True)
-            scrapymanager.runonce(self.spider)
+            scrapymanager.crawl_spider(self.spider)
-    scrapymanager.runonce(*requests)
+    for url in urls:
-        # XXX: Policy for replacing invalid chars may change without prior notice
+        # XXX: Policy for replacing invalid chars may suffer minor variations
-    __slots__ = ['_encoding', '_cached_benc']
+    __slots__ = ['_encoding', '_cached_benc', '_cached_ubody']
-            enc = self._body_inferred_encoding() or self._DEFAULT_ENCODING
+        if enc and not encoding_exists(enc):
-        return self.body.decode(benc) if benc == 'utf-16' else dammit.unicode
+        if self._cached_ubody is None:
-            self.body_as_unicode()
+            enc = self._get_encoding()
-        self._assert_response_encoding(resp, settings['DEFAULT_RESPONSE_ENCODING'])
+        self._assert_response_encoding(resp, self.response_class._DEFAULT_ENCODING)
-        self._assert_response_values(r1, settings['DEFAULT_RESPONSE_ENCODING'], body)
+        self._assert_response_values(r1, self.response_class._DEFAULT_ENCODING, body)
-                dont_filter=None, errback=None):
+    def replace(self, *args, **kwargs):
-                              errback=errback)
+        for x in ['url', 'method', 'headers', 'body', 'cookies', 'meta', \
-            flags=None, cls=None, **kwargs):
+    def replace(self, *args, **kwargs):
-        return new
+        for x in ['url', 'status', 'headers', 'body', 'meta', 'flags']:
-        response = HtmlResponse(url='http://example.org', body="""\
+        response = HtmlResponse(url='https://example.org', body="""\
-        _baseurl_cache[response] = match.group(1) if match else response.url
+        _baseurl_cache[response] = urljoin_rfc(response.url, match.group(1)) if match else response.url
-
+ENCODING_ALIASES = {}
-            return self._redirect(redirected, request, spider, 'meta refresh')
+        if isinstance(response, HtmlResponse):
-import codecs
+from scrapy.utils.encoding import encoding_exists, resolve_encoding
-    __slots__ = ['_encoding', '_body_inferred_encoding']
+    __slots__ = ['_encoding', '_cached_benc']
-        self._body_inferred_encoding = None
+        self._cached_benc = None
-        return self._encoding or self.headers_encoding() or self.body_encoding()
+        enc = self._declared_encoding()
-                    pass
+    def _declared_encoding(self):
-        if self._body_inferred_encoding is None:
+        denc = self._declared_encoding()
-        return self._body_inferred_encoding
+        return self._cached_benc
-from scrapy.http import Request, Response, Headers
+from scrapy.http import Request, Response, HtmlResponse, Headers
-        rsp = Response(url='http://example.org', body=body)
+        rsp = HtmlResponse(url='http://example.org', body=body)
-        rsp = Response(url='http://example.org', body=body)
+        rsp = HtmlResponse(url='http://example.org', body=body)
-        rsp = Response(url='http://example.org', body=body)
+        rsp = HtmlResponse(url='http://example.org', body=body)
-    unittest.main()
+from scrapy.utils.encoding import resolve_encoding
-        self.assertEqual(response.encoding, encoding)
+        self._assert_response_encoding(response, encoding)
-        self.assertEqual(r2.encoding, "cp852")
+        self._assert_response_encoding(r2, "cp852")
-        self.assertEqual(r3.encoding, "latin1")
+        self.assertEqual(r3._declared_encoding(), "latin1")
-        self.assertEqual(resp.encoding, settings['DEFAULT_RESPONSE_ENCODING'])
+        self._assert_response_encoding(resp, settings['DEFAULT_RESPONSE_ENCODING'])
-        assert r4.body_encoding() is not None and r4.body_encoding() != 'ascii'
+        r5 = self.response_class("http://www.example.com", headers={"Content-type": ["text/html; charset=None"]}, body="\xc2\xa3")
-        response = Response(url='http://example.org', body="""\
+        response = HtmlResponse(url='http://example.org', body="""\
-        response = Response(url='http://example.org', body=body)
+        response = TextResponse(url='http://example.org', body=body)
-        response = Response(url='http://example.org', body=body)
+        response = TextResponse(url='http://example.org', body=body)
-        response = Response(url='http://example.org', body=body)
+        response = TextResponse(url='http://example.org', body=body)
-        response = Response(url='http://example.org', body=body)
+        response = TextResponse(url='http://example.org', body=body)
-        response = Response(url='http://example.com', body=body)
+        response = TextResponse(url='http://example.com', body=body)
-        response = Response(url='http://example.com/page/this.html', body=body)
+        response = TextResponse(url='http://example.com/page/this.html', body=body)
-        response = Response(url='http://example.com', body=body)
+        response = TextResponse(url='http://example.com', body=body)
-        response = Response(url='http://example.org')
+        response = TextResponse(url='http://example.org')
-def add_encoding_alias(encoding, alias, overwrite=False):
+from scrapy.conf import settings
-        alias_exists = True
+        codecs.lookup(resolve_encoding(encoding, _aliases))
-        codecs.register(lambda x: codec if x == alias else None)
+        return False
-    assert isinstance(obj, (Response, basestring)), "obj must be Response or basestring, not %s" % type(obj).__name__
+    assert isinstance(obj, (Response, basestring)), \
-BASEURL_RE = re.compile(r'<base\s+href\s*=\s*[\"\']\s*([^\"\'\s]+)\s*[\"\']', re.I)
+BASEURL_RE = re.compile(ur'<base\s+href\s*=\s*[\"\']\s*([^\"\'\s]+)\s*[\"\']', re.I)
-        match = BASEURL_RE.search(response.body[0:4096])
+        match = BASEURL_RE.search(response.body_as_unicode()[0:4096])
-META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\s*=\s*(?P<quote>["\'])(?P<int>\d+)\s*;\s*url=(?P<url>.*?)(?P=quote)', re.DOTALL | re.IGNORECASE)
+META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\s*=\s*(?P<quote>["\'])(?P<int>\d+)\s*;\s*url=(?P<url>.*?)(?P=quote)', \
-            errors='ignore'))
+        body_chunk = remove_entities(response.body_as_unicode()[0:4096])
-        base_url = self.base_url if self.base_url else response_url
+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url
-            else unicode_to_str(response.url, response.encoding)
+        base_url = urljoin_rfc(response.url, base_url[0]) if base_url else response.url
-        base_url = self.base_url if self.base_url else response_url
+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url
-        base_url = self.base_url if self.base_url else response_url
+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url
-        base_url = self.base_url if self.base_url else response_url
+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url
-        base_url = self.base_url if self.base_url else response_url
+        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url
-        reqx = BaseSgmlRequestExtractor()
+        response = HtmlResponse("https://example.org/p/index.html", body=html)
-            )
+        # base url is an absolute path and relative to host
-add_encoding_alias('cp1252', 'iso8859-1', overwrite=True)
+add_encoding_alias('cp1252', 'iso8859-1', overwrite=True)
-    def extract_from_selector(self, selector, parent=None):
+    def extract_from_selector(self, selector, encoding, parent=None):
-                ret.append(Link(unicode_to_str(url[0]), alt[0]))
+                ret.append(Link(unicode_to_str(url[0], encoding), alt[0]))
-                        ret.extend(self.extract_from_selector(child, parent=selector))
+                        ret.extend(self.extract_from_selector(child, encoding, parent=selector))
-        base_url = unicode_to_str(base_url[0]) if base_url else unicode_to_str(response.url)
+        base_url = unicode_to_str(base_url[0], response.encoding) if base_url \
-                links.extend(self.extract_from_selector(selector))
+                links.extend(self.extract_from_selector(selector, response.encoding))
-                encoding='utf-8')
+            response = TextResponse(url='about:blank', \
-    return bool(_ent_re.search(str_to_unicode(text)))
+def has_entities(text, encoding=None):
-def replace_tags(text, token=''):
+def replace_tags(text, token='', encoding=None):
-    return _tag_re.sub(token, str_to_unicode(text))
+    return _tag_re.sub(token, str_to_unicode(text, encoding))
-def remove_comments(text):
+def remove_comments(text, encoding=None):
-    return re.sub('<!--.*?-->', u'', str_to_unicode(text), re.DOTALL)
+    return re.sub('<!--.*?-->', u'', str_to_unicode(text, encoding), re.DOTALL)
-def remove_tags(text, which_ones=()):
+def remove_tags(text, which_ones=(), encoding=None):
-        tags = ['<%s>|<%s .*?>|</%s>' % (tag,tag,tag) for tag in which_ones]
+        tags = ['<%s>|<%s .*?>|</%s>' % (tag, tag, tag) for tag in which_ones]
-    return retags.sub(u'', str_to_unicode(text))
+    return retags.sub(u'', str_to_unicode(text, encoding))
-def remove_tags_with_content(text, which_ones=()):
+def remove_tags_with_content(text, which_ones=(), encoding=None):
-    text = str_to_unicode(text)
+    text = str_to_unicode(text, encoding)
-        tags = '|'.join(['<%s.*?</%s>' % (tag,tag) for tag in which_ones])
+        tags = '|'.join(['<%s.*?</%s>' % (tag, tag) for tag in which_ones])
-def replace_escape_chars(text, which_ones=('\n','\t','\r'), replace_by=u''):
+def replace_escape_chars(text, which_ones=('\n', '\t', '\r'), replace_by=u'', \
-    return str_to_unicode(text)
+        text = text.replace(ec, str_to_unicode(replace_by, encoding))
-def unquote_markup(text, keep=(), remove_illegal=True):
+def unquote_markup(text, keep=(), remove_illegal=True, encoding=None):
-    text = str_to_unicode(text)
+    text = str_to_unicode(text, encoding)
-def str_to_unicode(text, encoding='utf-8'):
+def str_to_unicode(text, encoding=None):
-def unicode_to_str(text, encoding='utf-8'):
+def unicode_to_str(text, encoding=None):
-def canonicalize_url(url, keep_blank_values=True, keep_fragments=False):
+def canonicalize_url(url, keep_blank_values=True, keep_fragments=False, \
-    url = unicode_to_str(url)
+    url = unicode_to_str(url, encoding)
-# default logging level
+# default values
-    global log_level, started
+    global log_level, log_encoding, started
-    msg_txt = unicode_to_str("%s: %s" % (level_names[level], message))
+    msg_txt = unicode_to_str("%s: %s" % (level_names[level], message), log_encoding)
-        _why = unicode_to_str("ERROR: %s" % _why)
+        _why = unicode_to_str("ERROR: %s" % _why, log_encoding)
-                part.add_header('Content-Disposition', 'attachment; filename="%s"' % attach_name)
+                part.add_header('Content-Disposition', 'attachment; filename="%s"' \
-        # ---------------------------------------------------------------------------
+        dfd = self._sendmail(self.smtphost, self.mailfrom, rcpts, msg.as_string())
-        log.msg('Mail sent OK: To=%s Cc=%s Subject="%s" Attachs=%d' % (to, cc, subject, nattachs))
+        log.msg('Mail sent OK: To=%s Cc=%s Subject="%s" Attachs=%d' % \
-        log.msg('Unable to send mail: To=%s Cc=%s Subject="%s" Attachs=%d - %s' % (to, cc, subject, nattachs, errstr), level=log.ERROR)
+        log.msg('Unable to send mail: To=%s Cc=%s Subject="%s" Attachs=%d - %s' % \
-                self.stop()
+import urlparse
-
+        assert open_in_browser(response, _openfunc=browser_open), \
-def open_in_browser(response, debug=False):
+def open_in_browser(response, _openfunc=webbrowser.open):
-    webbrowser.open("file://%s" % fname)
+    return _openfunc("file://%s" % fname)
-from scrapy.http import Response, TextResponse
+from scrapy.http import Response, TextResponse, HtmlResponse
-    response_httprepr, get_cached_beautifulsoup
+    response_httprepr, get_cached_beautifulsoup, open_in_browser
-def open_in_browser(response):
+def open_in_browser(response, debug=False):
-        ret = loc['ret']
+        return _run_command_profiled(cmd, args, opts)
-    return ret
+        return cmd.run(args, opts)
-from scrapy.spider import spiders
+from scrapy.utils.signal import send_catch_log
-    command_executed['opts'] = opts.__dict__.copy()
+# Signal that carries information about the command which was executed
-    _save_command_executed(cmdname, cmd, args, opts)
+    send_catch_log(signal=command_executed, cmdname=cmdname, cmdobj=cmd, \
-SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO
+SCHEDULER_ORDER = 'DFO'
-                cookies=None, meta=None, encoding=None, dont_filter=None):
+    def replace(self, url=None, callback=None, method=None, headers=None, body=None, \
-                              dont_filter=self.dont_filter if dont_filter is None else dont_filter)
+                              priority=self.priority if priority is None else priority,
-                return encoding.group(1)
+                enc = encoding.group(1)
-            self.download_delay = settings.getfloat('DOWNLOAD_DELAY')
+            self._download_delay = settings.getfloat('DOWNLOAD_DELAY')
-        if self.download_delay:
+            self._download_delay = float(download_delay)
-            penalty = site.download_delay - now + site.lastseen
+        delay = site.download_delay()
-    memoizemethod_noargs, isbinarytext
+    memoizemethod_noargs, isbinarytext, equal_attributes
-        if download_delay:
+        if self.download_delay:
-            self.download_delay = settings.getint('DOWNLOAD_DELAY')
+            self.download_delay = settings.getfloat('DOWNLOAD_DELAY')
-            dwld.addErrback(log.err, "Unhandled error on engine._next_request")
+            dwld.addErrback(log.err, "Unhandled error on engine._next_request()",
-        schd.addErrback(log.err, "Unhandled error on engine.crawl()")
+        schd.addErrback(log.err, "Unhandled error on engine.crawl()", spider=spider)
-            log.err(spider_failure, 'Unhandled error propagated to spider and wasn\'t handled')
+            log.err(spider_failure, 'Unhandled error propagated to spider', \
-    kwargs['system'] = domain or spider.domain_name if spider else component
+    if domain is not None:
-__version__ = "0.8"
+version_info = (0, 9, 0, 'dev')
-__version__ = "0.8-rc1"
+version_info = (0, 8, 0, '', 0)
-    'name': 'scrapy',
+    'name': 'Scrapy',
-__version__ = "0.8-dev"
+version_info = (0, 8, 0, 'rc1', 0)
-        fd = open(self.tmpname + '<', 'w')
+        fd = open(self.tmpname + '^', 'w')
-        assert request.url.upper().endswith('%3C')
+        request = Request('file://%s' % self.tmpname + '^')
-    filepath = request.url.split("file://")[1]
+    filepath = url2pathname(request.url.split("file://")[1])
-        fd = open(self.tmpname, 'w')
+        fd = open(self.tmpname + '<', 'w')
-        request = Request('file://%s' % self.tmpname)
+        request = Request('file://%s' % self.tmpname + '<')
-        raise ValueError("download_delay must be positive")
+        raise ValueError("Spider 'domain_name' attribute is required")
-         """Optional User-Agent to use for this domain""")
+    domain_name = Attribute("The domain name of the site to be scraped.")
-    """
+class SpiderInfo(object):
-        self.concurrent_domains = settings.getint('CONCURRENT_SPIDERS')
+        self.concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')
-            raise RuntimeError('Downloader spider already opened: %s' % domain)
+            raise RuntimeError('Downloader spider already opened: %s' % spider)
-        self.sites[spider] = SiteInfo(
+        self.sites[spider] = SpiderInfo(
-                spider.domain_name)
+            raise RuntimeError('Downloader spider already closed: %s' % spider)
-        return len(self.sites) < self.concurrent_domains
+        return len(self.sites) < self.concurrent_spiders
-            return []
+            return default or []
-        self.defaults = {}
+    def __init__(self, values=None):
-                return self.defaults[opt_name]
+        if opt_name in self.values:
-
+
-settings = Settings()
+
-                self.assertFalse(cur < req, "module %s >= %s required" % ('OpenSSL', required_version))
+            installed_version = map(int, module.__version__.split('.')[:2])
-from unittest import TestCase, main
+from twisted.trial import unittest
-    def test_openssl(self):
+class ScrapyUtilsTest(unittest.TestCase):
-            return # no openssl installed
+            raise unittest.SkipTest("OpenSSL is not available")
-    main()
+    unittest.main()
-def print_live_refs():
+def print_live_refs(ignore=NoneType):
-        dfd.addBoth(log.msg, "Spider closed (%s)" % reason, spider=spider)
+        dfd.addErrback(log.err, "Unhandled error on SpiderManager.close_spider()",
-                reactor.callLater(penalty, self._process_queue, spider=spider)
+                d = defer.Deferred()
-            raise RuntimeError('Downloader spider already closed: %s' % domain)
+            raise RuntimeError('Downloader spider already closed: %s' % \
-        self._next_request_pending = set()
+        self._next_request_calls = {}
-            return reactor.callLater(0, self.next_request, spider, now=True)
+            self._next_request_calls.pop(spider, None)
-        if tsk and not tsk.called:
+        if tsk and tsk.active():
-        self.domains[spider].finished = datetime.now().replace(microsecond=0)
+        del self.domains[spider]
-        s += "<tr><th>Spider</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\n"
+        s += "<tr><th>Spider</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Run time</th></tr>\n"
-            runtime = stats.finished - stats.started if stats.finished else datetime.now() - stats.started
+            runtime = datetime.now() - stats.started
-                 (spider.domain_name, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(stats.finished), str(runtime))
+            s += '<tr><td>%s</td><td align="right">%d</td><td align="right">%d</td><td align="right">%d</td><td align="right">%d</td><td align="right">%d</td><td align="right">%d</td><td>%s</td><td>%s</td></tr>\n' % \
-        for netloc in self._spider_netlocs[domain]:
+        for netloc in self._spider_netlocs[spider]:
-        del self._spider_netlocs[domain]
+        del self._spider_netlocs[spider]
-class BaseSpider(object):
+class BaseSpider(object_ref):
-                        'Middleware %s.enqueue_request must return None, Response or Deferred, got %s' % \
+                assert result is None or isinstance(result, Deferred), \
-configurable amount of idle time is reached
+SpiderCloseDelay is an extension that keeps a idle spiders open until a
-class DelayedCloseDomain(object):
+class SpiderCloseDelay(object):
-        self.delay = settings.getint('DOMAIN_CLOSE_DELAY')
+        self.delay = settings.getint('SPIDER_CLOSE_DELAY')
-
+SPIDER_SCHEDULER = 'scrapy.contrib.spiderscheduler.FifoSpiderScheduler'
-from scrapy.core.exceptions import NotConfigured, DontCloseDomain
+from scrapy.core.exceptions import NotConfigured, DontCloseSpider
-            raise DontCloseDomain
+            raise DontCloseSpider
-from scrapy.core.exceptions import IgnoreRequest, DontCloseDomain
+from scrapy.core.exceptions import IgnoreRequest, DontCloseSpider
-        multiple times. If some extension raises a DontCloseDomain exception
+        multiple times. If some extension raises a DontCloseSpider exception
-        except DontCloseDomain:
+        except DontCloseSpider:
-            log.err("Exception catched on spider_idle signal dispatch")
+        except Exception, e:
-    """Request the domain not to be closed yet"""
+class DontCloseSpider(Exception):
-from distutils.core import setup
+# It doesn't depend on setuptools, but if setuptools is available it'll use
-    classifiers = [
+setup_args = {
-)
+}
-apply_patches()
+from scrapy.xlib import twisted_250_monkeypatches
-        add_missing_blockingCallFromThread()
+NOTE: This module must not fail if twisted module is not available.
-            self.next_request(spider)
+            reactor.callLater(5, self.next_request, spider)
-from scrapy.stats.signals import stats_domain_opened, stats_domain_closing
+from scrapy.stats.signals import stats_spider_opened, stats_spider_closing
-        dispatcher.connect(self.stats_domain_closing, signal=stats_domain_closing)
+        dispatcher.connect(self.stats_spider_opened, signal=stats_spider_opened)
-        stats.inc_value('domain_count/opened')
+    def stats_spider_opened(self, spider):
-        stats.inc_value('domain_count/%s' % reason, domain=domain)
+    def stats_spider_closing(self, spider, reason):
-        stats.inc_value('item_scraped_count', domain=spider.domain_name)
+        stats.inc_value('item_scraped_count', spider=spider)
-        stats.inc_value('item_passed_count', domain=spider.domain_name)
+        stats.inc_value('item_passed_count', spider=spider)
-        stats.inc_value('item_dropped_reasons_count/%s' % reason, domain=spider.domain_name)
+        stats.inc_value('item_dropped_count', spider=spider)
-        stats.inc_value('downloader/request_method_count/%s' % request.method, domain=domain)
+        stats.inc_value('downloader/request_count', spider=spider)
-        stats.inc_value('downloader/request_bytes', reqlen, domain=domain)
+        stats.inc_value('downloader/request_bytes', reqlen, spider=spider)
-        stats.inc_value('downloader/response_status_count/%s' % response.status, domain=domain)
+        stats.inc_value('downloader/response_count', spider=spider)
-        stats.inc_value('downloader/response_bytes', reslen, domain=domain)
+        stats.inc_value('downloader/response_bytes', reslen, spider=spider)
-        stats.inc_value('downloader/exception_type_count/%s' % ex_class, domain=spider.domain_name)
+        stats.inc_value('downloader/exception_count', spider=spider)
-        sampled = stats.get_value("items_sampled", 0, domain=spider.domain_name)
+        sampled = stats.get_value("items_sampled", 0, spider=spider)
-            stats.set_value("items_sampled", sampled, domain=spider.domain_name)
+            stats.set_value("items_sampled", sampled, spider=spider)
-        if reason == 'finished' and not stats.get_value("items_sampled", domain=spider.domain_name):
+        if reason == 'finished' and not stats.get_value("items_sampled", spider=spider):
-        if stats.get_value("items_sampled", domain=spider.domain_name) >= items_per_spider:
+        if stats.get_value("items_sampled", spider=spider) >= items_per_spider:
-        if stats.get_value("items_sampled", domain=spider.domain_name) >= items_per_spider:
+        if stats.get_value("items_sampled", spider=spider) >= items_per_spider:
-        self.inc_stats(info.spider.domain_name, status)
+        self.inc_stats(info.spider, status)
-            self.inc_stats(info.spider.domain_name, 'uptodate')
+            self.inc_stats(info.spider, 'uptodate')
-        stats.inc_value('image_status_count/%s' % status, domain=domain)
+    def inc_stats(self, spider, status):
-                        stats.set_value('request_depth_max', depth, domain=domain)
+                    stats.inc_value('request_depth_count/%s' % depth, spider=spider)
-            stats.inc_value('request_depth_count/0', domain=domain)
+            stats.inc_value('request_depth_count/0', spider=spider)
-StatsMailer extension sends an email when a domain finishes scraping.
+StatsMailer extension sends an email when a spider finishes scraping.
-        dispatcher.connect(self.stats_domain_closed, signal=signals.stats_domain_closed)
+        dispatcher.connect(self.stats_spider_closed, signal=signals.stats_spider_closed)
-    def stats_domain_closed(self, domain, domain_stats):
+    def stats_spider_closed(self, spider, spider_stats):
-        mail.send(self.recipients, "Scrapy stats for: %s" % domain, body)
+        body += "\n\n%s stats\n\n" % spider.domain_name
-            s += stats_html_table(stats.get_stats(domain))
+        for spider, spider_stats in stats.iter_spider_stats():
-                tcc+ct, domain=domain)
+            tcc = stats.get_value('profiling/total_callback_time', 0, spider=spider)
-                stats.set_value('profiling/slowest_callback_time', ct, domain=domain)
+                stats.set_value('profiling/slowest_callback_time', ct, spider=spider)
-                    domain=domain)
+                    spider=spider)
-                    domain=domain)
+                    spider=spider)
-                    count=mafter-mbefore, domain=domain)
+                    count=mafter-mbefore, spider=spider)
-        stats.open_domain(spider.domain_name)
+        stats.open_spider(spider)
-        stats.close_domain(spider.domain_name, reason=reason)
+        stats.close_spider(spider, reason=reason)
-        # FIXME: this can't be called here because the stats domain may be
+        # FIXME: this can't be called here because the stats spider may be
-        #    domain=spider.domain_name)
+        #    spider=spider)
-            domain=spider.domain_name)
+            spider=spider)
-            domain=spider.domain_name)
+            spider=spider)
-            # FIXME: this can't be called here because the stats domain may be
+            # FIXME: this can't be called here because the stats spider may be
-            #        self.sites[domain].itemproc_size, domain=domain)
+            #        self.sites[spider].itemproc_size, spider=spider)
-    stats_domain_closed
+from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \
-        return self._stats[domain].get(key, default)
+    def get_value(self, key, default=None, spider=None):
-        return self._stats[domain]
+    def get_stats(self, spider=None):
-        self._stats[domain][key] = value
+    def set_value(self, key, value, spider=None):
-        self._stats[domain] = stats
+    def set_stats(self, stats, spider=None):
-        d = self._stats[domain]
+    def inc_value(self, key, count=1, start=0, spider=None):
-        d = self._stats[domain]
+    def max_value(self, key, value, spider=None):
-        d = self._stats[domain]
+    def min_value(self, key, value, spider=None):
-        self._stats[domain].clear()
+    def clear_stats(self, spider=None):
-        return [d for d in self._stats.keys() if d is not None]
+    def iter_spider_stats(self):
-        send_catch_log(stats_domain_opened, domain=domain)
+    def open_spider(self, spider):
-            domain_stats=stats)
+    def close_spider(self, spider, reason):
-        self._persist_stats(stats, domain)
+            log.msg("Dumping spider stats:\n" + pprint.pformat(stats), \
-        self._persist_stats(stats, domain=None)
+        self._persist_stats(stats, spider=None)
-    def _persist_stats(self, stats, domain=None):
+    def _persist_stats(self, stats, spider=None):
-        self.domain_stats[domain] = stats
+
-    def get_value(self, key, default=None, domain=None):
+    def get_value(self, key, default=None, spider=None):
-    def set_value(self, key, value, domain=None):
+    def set_value(self, key, value, spider=None):
-    def set_stats(self, stats, domain=None):
+    def set_stats(self, stats, spider=None):
-    def inc_value(self, key, count=1, start=0, domain=None):
+    def inc_value(self, key, count=1, start=0, spider=None):
-    def max_value(self, key, value, domain=None):
+    def max_value(self, key, value, spider=None):
-    def min_value(self, key, value, domain=None):
+    def min_value(self, key, value, spider=None):
-        if domain is None: # only store domain-specific stats
+    def _persist_stats(self, stats, spider=None):
-            (domain, stored, datas))
+            (spider.domain_name, stored, datas))
-        if domain is None: # only store domain-specific stats
+    def _persist_stats(self, stats, spider=None):
-            dfd = threads.deferToThread(self._persist_to_sdb, domain, stats.copy())
+            dfd = threads.deferToThread(self._persist_to_sdb, spider, stats.copy())
-                domain=domain)
+                spider=spider)
-            self._persist_to_sdb(domain, stats)
+            self._persist_to_sdb(spider, stats)
-        sdb_item_id = "%s_%s" % (domain, ts)
+    def _persist_to_sdb(self, spider, stats):
-        sdb_item['domain'] = domain
+        sdb_item['domain'] = spider.domain_name
-    def _get_timestamp(self, domain):
+    def _get_timestamp(self, spider):
-stats_domain_closed = object()
+stats_spider_opened = object()
-        self.spider.domain_name = 'scrapytest.org'
+        self.spider = BaseSpider('scrapytest.org')
-        stats.open_domain(self.spider.domain_name)
+        stats.open_spider(self.spider)
-            domain=self.spider.domain_name), 1)
+            spider=self.spider), 1)
-            domain=self.spider.domain_name), 1)
+            spider=self.spider), 1)
-            domain=self.spider.domain_name), 1)
+            spider=self.spider), 1)
-        stats.close_domain(self.spider.domain_name, '')
+        stats.close_spider(self.spider, '')
-        self.spider.domain_name = 'scrapytest.org'
+        self.spider = BaseSpider('scrapytest.org')
-        stats.open_domain(self.spider.domain_name)
+        stats.open_spider(self.spider)
-                              domain=self.spider.domain_name)
+        rdc = stats.get_value('request_depth_count/1', spider=self.spider)
-                              domain=self.spider.domain_name)
+        rdm = stats.get_value('request_depth_max', spider=self.spider)
-        stats.close_domain(self.spider.domain_name, '')
+        stats.close_spider(self.spider, '')
-    stats_domain_closed
+from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \
-        stats.set_value('test', 'value', domain='a')
+        stats.open_spider('a')
-            signals_catched.add(stats_domain_opened)
+        def spider_opened(spider):
-            assert domain == 'example.com'
+        def spider_closing(spider, reason):
-            signals_catched.add(stats_domain_closing)
+            signals_catched.add(stats_spider_closing)
-            assert domain == 'example.com'
+        def spider_closed(spider, reason, spider_stats):
-            signals_catched.add(stats_domain_closed)
+            assert spider_stats == {'test': 1}
-        dispatcher.connect(domain_closed, signal=stats_domain_closed)
+        dispatcher.connect(spider_opened, signal=stats_spider_opened)
-        assert stats_domain_closed in signals_catched
+        stats.open_spider(self.spider)
-        dispatcher.disconnect(domain_closed, signal=stats_domain_closed)
+        dispatcher.disconnect(spider_opened, signal=stats_spider_opened)
-from scrapy.http import Request
+from scrapy.http import Request, TextResponse
-            self.vars['hxs'] = HtmlXPathSelector(response)
+            if isinstance(response, TextResponse):
-        self.mw.spider_closed('scrapytest.org')
+        self.mw.spider_closed(self.spider)
-
+CONCURRENT_REQUESTS_PER_SPIDER = 8
-            self.max_concurrent_requests = settings.getint('REQUESTS_PER_SPIDER')
+            self.max_concurrent_requests = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')
-    def __init__(self):
+    def __init__(self, overrides=None):
-HTTPCACHE_SECTORIZE = True
+HTTPCACHE_STORAGE = 'scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage'
-import datetime
+from os.path import join, exists
-from scrapy.xlib.pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from scrapy.conf import settings
+from scrapy.utils.misc import load_object
-        self.cache = Cache(settings['HTTPCACHE_DIR'], sectorize=settings.getbool('HTTPCACHE_SECTORIZE'))
+
-        dispatcher.connect(self.open_domain, signal=signals.spider_opened)
+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)
-        self.cache.open_domain(spider.domain_name)
+    def spider_closed(self, spider):
-        if not is_cacheable(request):
+        if not self.is_cacheable(request):
-
+        response = self.storage.retrieve_response(spider, request)
-
+        if self.is_cacheable(request):
-    return urlparse_cached(request).scheme in ['http', 'https']
+class FilesystemCacheStorage(object):
-    def __init__(self, cachedir, sectorize=False):
+    def __init__(self, settings=conf.settings):
-            return None # not cached
+        self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')
-            responseheaders = f.read()
+    def open_spider(self, spider):
-
+        headers = Headers(headers_raw_to_dict(rawheaders))
-        response.flags.append('cached')
+        response = respcls(url=url, headers=headers, status=status, body=body)
-
+    def store_response(self, spider, request, response):
-        with open(os.path.join(requestpath, 'meta_data'), 'w') as f:
+            'url': request.url,
-        with open(os.path.join(requestpath, 'response_headers'), 'w') as f:
+        with open(join(rpath, 'pickled_meta'), 'wb') as f:
-        with open(os.path.join(requestpath, 'response_body'), 'w') as f:
+        with open(join(rpath, 'response_body'), 'wb') as f:
-        with open(os.path.join(requestpath, 'request_headers'), 'w') as f:
+        with open(join(rpath, 'request_headers'), 'wb') as f:
-                f.write(request.body)
+        with open(join(rpath, 'request_body'), 'wb') as f:
-            self.should_follow(x, spider))
+        for x in result:
-            os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings'))
+            'scrapy_settings')
-DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'
+DEFAULT_ITEM_CLASS = 'scrapy.item.Item'
-from scrapy.item import Item, Field, ScrapedItem
+from scrapy.item import Item, Field
-                log.msg('dropping old cached response from %s' % metadata['timestamp'], level=log.DEBUG, domain=domain)
+                log.msg('dropping old cached response from %s' % metadata['timestamp'], \
-                    domain=spider.domain_name, level=log.DEBUG)
+                    spider=spider, level=log.DEBUG)
-                    domain=spider.domain_name, level=log.DEBUG)
+                    spider=spider, level=log.DEBUG)
-                    domain=spider.domain_name, level=log.DEBUG)
+                    spider=spider, level=log.DEBUG)
-                    domain=spider.domain_name, level=log.DEBUG)
+                    spider=spider, level=log.DEBUG)
-            log.msg("Reloading module %s" % module_name, domain=domain, \
+            log.msg("Reloading module %s" % module_name, spider=spider, \
-                        level=log.DEBUG, domain=domain)
+                        level=log.DEBUG, spider=spider)
-                    level=log.DEBUG, domain=spider.domain_name)
+                    level=log.DEBUG, spider=spider)
-                        fmt, log.DEBUG, domain=spider.domain_name)
+                        fmt, log.DEBUG, spider=spider)
-        log.msg(msg, log.ERROR, domain=spider.domain_name)
+        log.msg(msg, log.ERROR, spider=spider)
-                domain=domain)
+                spider=spider)
-                (type(output).__name__, request), log.ERROR, domain=domain)
+                (type(output).__name__, request), log.ERROR, spider=spider)
-                log.msg("Dropped %s - %s" % (item, str(ex)), level=log.WARNING, domain=domain)
+                log.msg("Dropped %s - %s" % (item, str(ex)), level=log.WARNING, spider=spider)
-                    log.ERROR, domain=domain)
+                    log.ERROR, spider=spider)
-            log.msg("Passed %s" % item, log.INFO, domain=domain)
+            log.msg("Passed %s" % item, log.INFO, spider=spider)
-        log.msg(message, domain=self.domain_name, level=level)
+        log.msg(message, spider=self, level=level)
-# args: message, level, domain
+# args: message, level, spider
-        domain=domain, spider=spider)
+        spider=spider)
-CLOSEDOMAIN_ITEMPASSED = 0
+CLOSESPIDER_TIMEOUT = 0
-    'scrapy.contrib.closedomain.CloseDomain': 0,
+    'scrapy.contrib.closespider.CloseSpider': 0,
-"""CloseDomain is an extension that forces spiders to be closed after certain
+"""CloseSpider is an extension that forces spiders to be closed after certain
-class CloseDomain(object):
+class CloseSpider(object):
-        self.itempassed = settings.getint('CLOSEDOMAIN_ITEMPASSED')
+        self.timeout = settings.getint('CLOSESPIDER_TIMEOUT')
-            spider=spider, reason='closedomain_timeout')
+            spider=spider, reason='closespider_timeout')
-            scrapyengine.close_spider(spider, 'closedomain_itempassed')
+            scrapyengine.close_spider(spider, 'closespider_itempassed')
-        s += "<tr><th>Domain</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\n"
+        s += "<tr><th>Spider</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\n"
-
+CONCURRENT_SPIDERS = 8
-                 settings['CONCURRENT_DOMAINS'],
+                 settings['CONCURRENT_SPIDERS'],
-        self.concurrent_domains = settings.getint('CONCURRENT_DOMAINS')
+        self.concurrent_domains = settings.getint('CONCURRENT_SPIDERS')
-REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain
+REQUESTS_PER_SPIDER = 8     # max simultaneous requests per domain
-            self.max_concurrent_requests = settings.getint('REQUESTS_PER_DOMAIN')
+            self.max_concurrent_requests = settings.getint('REQUESTS_PER_SPIDER')
-                    % (request, referer), level=log.WARNING, domain=info.domain)
+                    % (request, referer), level=log.WARNING, spider=info.spider)
-                    % (request, referer), level=log.WARNING, domain=info.domain)
+                    % (request, referer), level=log.WARNING, spider=info.spider)
-        self.inc_stats(info.domain, status)
+        log.msg(msg, level=log.DEBUG, spider=info.spider)
-            log.msg(str(ex), level=log.WARNING, domain=info.domain)
+            log.msg(str(ex), level=log.WARNING, spider=info.spider)
-            log.err(domain=info.domain)
+            log.err(spider=info.spider)
-            log.msg(msg, level=log.WARNING, domain=info.domain)
+            log.msg(msg, level=log.WARNING, spider=info.spider)
-            self.inc_stats(info.domain, 'uptodate')
+                    (self.MEDIA_NAME, request.url, referer), level=log.DEBUG, spider=info.spider)
-  wether to close the domain after enough products have been sampled
+ITEMSAMPLER_CLOSE_SPIDER
-close_domain = settings.getbool('ITEMSAMPLER_CLOSE_DOMAIN', False)
+items_per_spider = settings.getint('ITEMSAMPLER_COUNT', 1)
-        self.domains_count = 0
+        self.spiders_count = 0
-        if sampled < items_per_domain:
+    def process_item(self, spider, item):
-            if close_domain and sampled == items_per_domain:
+            stats.set_value("items_sampled", sampled, domain=spider.domain_name)
-            log.msg("No products sampled for: %s" % " ".join(self.empty_domains), level=log.WARNING)
+            log.msg("No products sampled for: %s" % " ".join(self.empty_domains), \
-        log.msg("Sampled %d domains so far (%d empty)" % (self.domains_count, len(self.empty_domains)), level=log.INFO)
+        if reason == 'finished' and not stats.get_value("items_sampled", domain=spider.domain_name):
-        if stats.get_value("items_sampled", domain=spider.domain_name) >= items_per_domain:
+        if stats.get_value("items_sampled", domain=spider.domain_name) >= items_per_spider:
-        if stats.get_value("items_sampled", domain=spider.domain_name) >= items_per_domain:
+        if stats.get_value("items_sampled", domain=spider.domain_name) >= items_per_spider:
-            d = mustbe_deferred(current_stage.process_item, spider.domain_name, item)
+            d = mustbe_deferred(current_stage.process_item, spider, item)
-    def process_item(self, domain, item):
+    def process_item(self, spider, item):
-    class DomainInfo(object):
+    class SpiderInfo(object):
-        self.domaininfo = {}
+        self.spiderinfo = {}
-        self.domaininfo[spider.domain_name] = self.DomainInfo(spider)
+        self.spiderinfo[spider] = self.SpiderInfo(spider)
-        del self.domaininfo[spider.domain_name]
+        del self.spiderinfo[spider]
-        info = self.domaininfo[domain]
+    def process_item(self, spider, item):
-            dwld.addErrback(log.err, domain=info.domain)
+            dwld.addErrback(log.err, spider=info.spider)
-    def process_item(self, domain, item):
+    def process_item(self, spider, item):
-            if self.stores[domain][guid] == item:
+        if guid in self.stores[spider]:
-        self.log(domain, item, status)
+            self.stores[spider][guid] = item
-        self.stores[domain] = Shove(uri, **self.opts)
+        uri = Template(self.uritpl).substitute(domain=spider.domain_name)
-        self.stores[spider.domain_name].sync()
+        self.stores[spider].sync()
-        log.msg("Shove (%s): Item guid=%s" % (status, item.guid), level=log.DEBUG, domain=domain)
+    def log(self, spider, item, status):
-    system = domain or spider.domain_name if spider else component
+    system = domain or (spider.domain_name if spider else component)
-                    level=log.DEBUG)
+                    level=log.DEBUG, spider=spider)
-from scrapy.utils.python import unique as unique_list
+from scrapy.utils.python import unique as unique_list, str_to_unicode
-            link.text = link.text.decode(response_encoding)
+            link.text = str_to_unicode(link.text, response_encoding)
-            dispatcher.connect(self.domain_opened, signal=signals.domain_opened)
+            dispatcher.connect(self.spider_opened, signal=signals.spider_opened)
-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)
+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)
-    def domain_opened(self, spider):
+    def spider_opened(self, spider):
-    def domain_closed(self, spider):
+    def spider_closed(self, spider):
-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)
+        dispatcher.connect(self.spider_idle, signal=signals.spider_idle)
-    def domain_idle(self, domain):
+    def spider_idle(self, spider):
-            lastseen = scrapyengine.downloader.sites[domain].lastseen
+            lastseen = scrapyengine.downloader.sites[spider].lastseen
-            lastseen = self.opened_at[domain]
+            lastseen = self.opened_at[spider]
-        self.opened_at.pop(domain, None)
+    def spider_closed(self, spider):
-        dispatcher.connect(self.domain_closed, signals.domain_closed)
+        dispatcher.connect(self.spider_closed, signals.spider_closed)
-        jar = self.jars[spider.domain_name]
+        jar = self.jars[spider]
-        jar = self.jars[spider.domain_name]
+        jar = self.jars[spider]
-        self.jars.pop(domain, None)
+    def spider_closed(self, spider):
-        dispatcher.connect(self.open_domain, signal=signals.domain_opened)
+        dispatcher.connect(self.open_domain, signal=signals.spider_opened)
-        self.cache.open_domain(domain)
+    def open_domain(self, spider):
-        dispatcher.connect(self.domain_closed, signals.domain_closed)
+        dispatcher.connect(self.spider_opened, signals.spider_opened)
-    def domain_opened(self, spider):
+    def spider_opened(self, spider):
-    def domain_closed(self, domain, spider):
+    def spider_closed(self, spider):
-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)
+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)
-    def domain_closed(self, domain, spider, reason):
+    def spider_closed(self, spider, reason):
-        dispatcher.connect(self.domain_closed, signals.domain_closed)
+        dispatcher.connect(self.spider_closed, signals.spider_closed)
-        self.created_directories.pop(domain, None)
+    def spider_closed(self, spider):
-        dispatcher.connect(self.domain_closed, signals.domain_closed)
+        dispatcher.connect(self.spider_opened, signals.spider_opened)
-    def domain_opened(self, spider):
+    def spider_opened(self, spider):
-        del self.domaininfo[domain]
+    def spider_closed(self, spider):
-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)
+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)
-        self.spider_hostnames[spider.domain_name].add(url_hostname)
+        self.spider_hostnames[spider].add(url_hostname)
-        for hostname in self.spider_hostnames:
+    def spider_closed(self, spider):
-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)
+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)
-    def domain_opened(self, spider):
+    def spider_opened(self, spider):
-    def domain_closed(self, spider):
+    def spider_closed(self, spider):
-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)
+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)
-        self.dropped_count[domain] = 0
+    def spider_opened(self, spider):
-        dropped_count = self.dropped_count[domain]
+    def spider_closed(self, spider):
-            max_pending = self.max_pending[domain]
+            max_pending = self.max_pending[spider]
-        del self.max_pending[domain]
+                    (dropped_count, max_pending), level=log.DEBUG, spider=spider)
-        max_pending = self.max_pending.get(domain, 0)
+        max_pending = self.max_pending.get(spider, 0)
-            return imap(lambda v: self._limit_requests(v, domain, max_pending), result)
+            return imap(lambda v: self._limit_requests(v, spider, max_pending), result)
-    def _limit_requests(self, request_or_other, domain, max_pending):
+    def _limit_requests(self, request_or_other, spider, max_pending):
-            free_slots = max_pending - self._pending_count(domain)
+            free_slots = max_pending - self._pending_count(spider)
-                self.dropped_count[domain] += 1
+                self.dropped_count[spider] += 1
-        pending = scrapyengine.scheduler.pending_requests.get(domain, [])
+    def _pending_count(self, spider):
-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)
+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)
-    def domain_opened(self, domain, spider):
+    def spider_opened(self, spider):
-        self.domains[spider.domain_name] = pstats
+        self.domains[spider] = pstats
-        self.domains[spider.domain_name].finished = datetime.now().replace(microsecond=0)
+    def spider_closed(self, spider):
-        self.domains[spider.domain_name].scraped += 1
+        self.domains[spider].scraped += 1
-            self.domains[spider.domain_name].crawled += 1
+        if self.domains.get(spider):
-        totdomains = totscraped = totcrawled = totscheduled = totactive = totpending = totdqueued = tottransf = 0
+        totdomains = totscraped = totcrawled = totscheduled = totactive = totdqueued = tottransf = 0
-            stats = self.domains[d]
+        for spider in sorted(self.domains.keys()):
-                 (d, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(stats.finished), str(runtime))
+                 (spider.domain_name, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(stats.finished), str(runtime))
-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)
+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)
-    def domain_opened(self, spider):
+    def spider_opened(self, spider):
-    def domain_closed(self, spider):
+    def spider_closed(self, spider):
-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)
+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)
-    def domain_opened(self, domain):
+    def spider_opened(self, spider):
-        self.stores[domain].sync()
+    def spider_closed(self, spider):
-        """Scrape the next request for the domain passed.
+        """Scrape the next request for the spider passed.
-                level=log.ERROR, domain=spider.domain_name)
+                level=log.ERROR, spider=spider)
-        """Add more domains to be scraped if the downloader has the capacity.
+        """Add more spiders to be scraped if the downloader has the capacity.
-                    level=log.DEBUG, domain=spider.domain_name)
+                    level=log.DEBUG, spider=spider)
-                    level=level, domain=domain)
+                    level=level, spider=spider)
-        log.msg("Domain opened", domain=domain)
+        log.msg("Spider opened", spider=spider)
-            domain=domain, spider=spider)
+        stats.open_domain(spider.domain_name)
-            domain=domain, spider=spider)
+        send_catch_log(signals.spider_opened, sender=self.__class__, spider=spider)
-        """Called when a domain gets idle. This function is called when there
+        """Called when a spider gets idle. This function is called when there
-        (in the domain_idle signal handler) the domain is not closed until the
+        (in the spider_idle signal handler) the spider is not closed until the
-        again for this domain.
+        again for this spider.
-                domain=domain, spider=spider)
+            dispatcher.send(signal=signals.spider_idle, sender=self.__class__, \
-            log.err("Exception catched on domain_idle signal dispatch")
+            log.err("Exception catched on spider_idle signal dispatch")
-            log.msg("Closing domain (%s)" % reason, domain=domain)
+            log.msg("Closing spider (%s)" % reason, spider=spider)
-        """Call _finish_closing_spider if domain is idle"""
+        """Call _finish_closing_spider if spider is idle"""
-        stats.close_domain(domain, reason=reason)
+        send_catch_log(signal=signals.spider_closed, sender=self.__class__, \
-        dfd.addBoth(log.msg, "Domain closed (%s)" % reason, domain=domain)
+        dfd.addBoth(log.msg, "Spider closed (%s)" % reason, spider=spider)
-domain_closed = object()
+spider_opened = object()
-def msg(message, level=INFO, component=BOT_NAME, domain=None):
+def msg(message, level=INFO, component=BOT_NAME, domain=None, spider=None):
-    system = domain if domain else component
+        domain=domain, spider=spider)
-def exc(message, level=ERROR, component=BOT_NAME, domain=None):
+def exc(message, level=ERROR, component=BOT_NAME, domain=None, spider=None):
-    msg(message, level, component, domain)
+    msg(message, level, component, domain, spider)
-    kwargs['system'] = domain if domain else component
+    kwargs['system'] = domain or spider.domain_name if spider else component
-        self.mw.domain_closed('scrapytest.org')
+        self.mw.spider_closed('scrapytest.org')
-            dispatcher.connect(self.record_signal, signals.domain_closed)
+            dispatcher.connect(self.record_signal, signals.spider_opened)
-                         session.signals_catched[signals.domain_closed])
+        assert signals.spider_opened in session.signals_catched
-        self.mw.domain_opened(self.spider)
+        self.mw.spider_opened(self.spider)
-        self.mw.domain_closed(self.spider)
+        self.mw.spider_closed(self.spider)
-        dispatcher.disconnect(domain_open, signal=stats_domain_opened)
+        dispatcher.disconnect(domain_opened, signal=stats_domain_opened)
-        def domain_open(domain):
+        def domain_opened(domain):
-        dispatcher.connect(domain_open, signal=stats_domain_opened)
+        dispatcher.connect(domain_opened, signal=stats_domain_opened)
-    log_level = globals()[loglevel] if loglevel else DEBUG
+    log_level = _get_log_level(loglevel)
-        else:
+        if dont_click:
-        return requests
+                yield r
-        return res
+            for requests_or_item in iterate_spider_output(cb_res):
-    def from_response(cls, response, formnumber=0, formdata=None, clickdata=None, **kwargs):
+    def from_response(cls, response, formnumber=0, formdata=None, 
-        url, body, headers = form.click_request_data(**(clickdata or {}))
+    def test_from_response_dont_click(self):
-    utf8body = body_as_utf8(response)
+    utf8body = body_as_utf8(response) or ' '
-    utf8body = body_as_utf8(response)
+    utf8body = body_as_utf8(response) or ' '
-        # TypeError, so the try/except block silences them
+        # so the try/except block silences them
-        finally:
+        except:
-        self.assertRaises(TypeError, self.response_class, u"http://www.example.com/")
+        # instantiate with unicode url without encoding (should set default encoding)
-        self._assert_response_values(r1, 'ascii', body)
+        self._assert_response_values(r1, settings['DEFAULT_RESPONSE_ENCODING'], body)
-        encoding = getattr(response, 'encoding', 'utf-8')
+        encoding = getattr(response, 'encoding', None) or 'utf-8'
-            redirected.headers.pop('Content-Length', None)
+            redirected = self._redirect_request_using_get(request, redirected_url)
-            redirected.headers.pop('Content-Length', None)
+            redirected = self._redirect_request_using_get(request, url)
-            redirected = request.replace(url=url)
+            redirected = request.replace(url=url, method='GET', body='')
-            redirected = request.replace(url=urljoin_rfc(request.url, url))
+        if url and interval < self.max_metarefresh_delay:
-        self.assertEqual(get_meta_refresh(response), ('5', 'http://example.org/newpage'))
+        self.assertEqual(get_meta_refresh(response), (5, 'http://example.org/newpage'))
-        self.assertEqual(get_meta_refresh(response), ('5', 'http://example.org/newpage'))
+        self.assertEqual(get_meta_refresh(response), (5, 'http://example.org/newpage'))
-        self.assertEqual(get_meta_refresh(response), ('1', 'http://example.org/newpage'))
+        self.assertEqual(get_meta_refresh(response), (1, 'http://example.org/newpage'))
-META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\d+);\s*url=([^"\']+)', re.DOTALL | re.IGNORECASE)
+META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\s*=\s*(?P<quote>["\'])(?P<int>\d+)\s*;\s*url=(?P<url>.*?)(?P=quote)', re.DOTALL | re.IGNORECASE)
-    (None, None) is returned [instead of (interval, None)]
+    """Parse the http-equiv parameter of the HTML meta element from the given
-        _metaref_cache[response] = match.groups() if match else (None, None)
+        encoding = getattr(response, 'encoding', 'utf-8')
-    def from_response(cls, response, formnumber=0, formdata=None, **kwargs):
+    def from_response(cls, response, formnumber=0, formdata=None, clickdata=None, **kwargs):
-            form.controls = [c for c in form.controls if c.name not in formdata.keys()]
+            form.controls = [c for c in form.controls if c.name not in formdata]
-        return request
+
-    lines = body_or_str(obj, unicode=False).splitlines(True)
+    lines = StringIO(body_or_str(obj, unicode=False))
-        self.headers = request.headers
+        self.headers = Headers(request.headers)
-    def test_host_header(self):
+    def test_host_header_not_in_request_headers(self):
-        d.addCallback(self.assertEquals, '127.0.0.1:%d' % self.portno)
+        d.addCallback(self.assertEquals, 'example.com')
-from scrapy.utils.iterators import csviter, xmliter, _xmliter_lxml, _xmliter_regex
+from scrapy.utils.iterators import csviter, xmliter
-    xmliter = staticmethod(_xmliter_lxml)
+    xmliter = staticmethod(xmliter_lxml)
-def _xmliter_regex(obj, nodename):
+def xmliter(obj, nodename):
-
+import bz2
-import bz2
+
-            'bz2': self.is_bzip2
+        self._formats = {
-    def is_tar(self, response):
+    def _is_tar(self, response):
-            tar_file = tarfile.open(name=mktemp(), fileobj=self.archive)
+            tar_file = tarfile.open(name=mktemp(), fileobj=archive)
-            raise self.ArchiveIsEmpty
+            return
-    def is_zip(self, response):
+    def _is_zip(self, response):
-            zip_file = zipfile.ZipFile(self.archive)
+            zip_file = zipfile.ZipFile(archive)
-            return False
+            return
-            raise self.ArchiveIsEmpty
+        body = zip_file.read(namelist[0])
-    def is_gzip(self, response):
+    def _is_gzip(self, response):
-            decompressed_body = gzip_file.read()
+            body = gzip.GzipFile(fileobj=archive).read()
-        return response.replace(body=decompressed_body, cls=respcls)
+            return
-    def is_bzip2(self, response):
+    def _is_bzip2(self, response):
-            decompressed_body = bz2.decompress(self.body)
+            body = bz2.decompress(response.body)
-        return response.replace(body=decompressed_body, cls=respcls)
+            return
-        of the used decompressor """
+        respcls = responsetypes.from_args(body=body)
-        self.archive.write(self.body)
+    def process_response(self, request, response, spider):
-            new_response = self.decompressors[decompressor](response)
+        for fmt, func in self._formats.iteritems():
-                log.msg('Decompressed response with format: %s' % format, log.DEBUG, domain=spider.domain_name)
+                log.msg('Decompressed response with format: %s' % \
-from scrapy.http import Response, XmlResponse
+from scrapy.http import Response, XmlResponse, Request
-    formats = ['tar', 'xml.bz2', 'xml.gz', 'zip']
+
-        self.assertEqual(response.body, self.uncompressed_body)
+
-from scrapy.utils.iterators import csviter, xmliter, xmliter_lxml, xmliter_regex
+from scrapy.utils.iterators import csviter, xmliter, _xmliter_lxml, _xmliter_regex
-    xmliter = staticmethod(xmliter_regex)
+    xmliter = staticmethod(_xmliter_regex)
-    xmliter = staticmethod(xmliter_lxml)
+    xmliter = staticmethod(_xmliter_lxml)
-def xmliter_regex(obj, nodename):
+def _xmliter_regex(obj, nodename):
-def xmliter_lxml(obj, nodename):
+def _xmliter_lxml(obj, nodename):
-    xmliter = xmliter_lxml
+    xmliter = _xmliter_lxml
-    xmliter = xmliter_regex
+    xmliter = _xmliter_regex
-import unittest
+from twisted.trial import unittest
-from scrapy.utils.iterators import csviter, xmliter
+from scrapy.utils.iterators import csviter, xmliter, xmliter_lxml, xmliter_regex
-    # to reproduce the missing functionality
+
-        for x in xmliter(response, 'product'):
+        for x in self.xmliter(response, 'product'):
-        self.assertEqual([x.select("text()").extract() for x in xmliter(body, 'product')],
+        self.assertEqual([x.select("text()").extract() for x in self.xmliter(body, 'product')],
-        my_iter = xmliter(response, 'item')
+        my_iter = self.xmliter(response, 'item')
-        iter = xmliter(body, 'product')
+        iter = self.xmliter(body, 'product')
-            xmliter(response, 'item').next().extract(),
+            self.xmliter(response, 'item').next().extract(),
-def _xmliter_regex(obj, nodename):
+def xmliter_regex(obj, nodename):
-def _xmliter_lxml(obj, nodename, encoding='utf-8'):
+def xmliter_lxml(obj, nodename):
-    xmliter = _xmliter_lxml
+    xmliter = xmliter_lxml
-    xmliter = _xmliter_regex
+    xmliter = xmliter_regex
-def xmliter(obj, nodename):
+
-class Libxml2Document(object):
+class Libxml2Document(object_ref):
-            obj = object.__new__(cls)
+            obj = object_ref.__new__(cls)
-        try:
+        finally:
-from unittest import TestCase
+import sys
-        scheme = urlparse.urlparse(uri).scheme
+        if os.path.isabs(uri): # to support win32 paths like: C:\\some\dir
-        return os.path.join(self.basedir, key)
+        path_comps = key.split('/')
-SETTINGS_DISABLED = settings.disabled
+        self.settings_disabled_before = settings.disabled
-        settings.disabled = SETTINGS_DISABLED
+        settings.disabled = self.settings_disabled_before
-                    level=log.DEBUG, domain=domain)
+                log.msg(self._crawled_logline(request, response), \
-                    referer, errmsg), level=level, domain=domain)
+                log.msg("Crawling <%s>: %s" % (request.url, errmsg), \
-            return "<%s %s>" % (self.method, self.url)
+        return "<%s %s>" % (self.method, self.url)
-        return "%s<%s%s>" % (flags, status, self.url)
+        return "<%d %s>" % (self.status, self.url)
-    return defer_succeed(response)
+"""Download handlers for different schemes"""
-from urlparse import urlparse, urlunparse
+from urlparse import urlparse, urlunparse, urldefrag
-from twisted.web.client import HTTPClientFactory, PartialDownloadError
+from twisted.web.client import PartialDownloadError, HTTPClientFactory
-        self.factory.noPage(defer.TimeoutError("Getting %s took longer than %s seconds." % (self.factory.url, self.factory.timeout)))
+        self.factory.noPage(\
-
+    def __init__(self, request, timeout=0):
-        self.deferred = defer.Deferred()
+        self.deferred = defer.Deferred().addCallback(self._build_response)
-            )
+    def _build_response(self, body):
-        *args, **kwargs).deferred
+import os
-from scrapy.http import Headers
+from scrapy.http import Request, Headers
-        f = client.ScrapyHTTPClientFactory(url)
+        f = client.ScrapyHTTPClientFactory(Request(url))
-        factory = client.ScrapyHTTPClientFactory(
+        factory = client.ScrapyHTTPClientFactory(Request(
-                'Useful': 'value'})
+                'Useful': 'value'}))
-        factory = client.ScrapyHTTPClientFactory('http://foo/bar')
+        factory = client.ScrapyHTTPClientFactory(Request('http://foo/bar'))
-        factory = client.ScrapyHTTPClientFactory(
+        factory = client.ScrapyHTTPClientFactory(Request(
-            headers={'Content-Type': 'application/x-www-form-urlencoded'})
+            headers={'Content-Type': 'application/x-www-form-urlencoded'}))
-        factory = client.ScrapyHTTPClientFactory(
+        factory = client.ScrapyHTTPClientFactory(Request(
-                })
+                }))
-        factory = client.ScrapyHTTPClientFactory(
+        factory = client.ScrapyHTTPClientFactory(Request(
-                }))
+                })))
-        return client.getPage(self.getURL("payload"), body=s).addCallback(self.assertEquals, s)
+        return getPage(self.getURL("payload"), body=s).addCallback(self.assertEquals, s)
-        d = client.getPage(self.getURL("broken"))
+        d = getPage(self.getURL("broken"))
-            client.getPage(self.getURL("host"), headers={"Host": "www.example.com"}).addCallback(self.assertEquals, "www.example.com")])
+            getPage(self.getURL("host")).addCallback(self.assertEquals, "127.0.0.1:%d" % self.portno),
-        d = client.getPage(self.getURL("file"))
+        d = getPage(self.getURL("file"))
-            return client.getPage(self.getURL("file"), method=method)
+        def _getPage(method):
-            getPage("HEAD").addCallback(self.assertEqual, "")])
+            _getPage("head").addCallback(self.assertEqual, ""),
-        d = client.getPage(self.getURL("host"), timeout=100)
+        d = getPage(self.getURL("host"), timeout=100)
-            client.getPage(self.getURL("wait"), timeout=0.000001),
+            getPage(self.getURL("wait"), timeout=0.000001),
-        return client.getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)
+        return getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)
-        factory = client.ScrapyHTTPClientFactory(url)
+        factory = client.ScrapyHTTPClientFactory(Request(url))
-        return client.getPage(self.getURL("redirect")).addCallback(self._cbRedirect)
+        return getPage(self.getURL("redirect")).addCallback(self._cbRedirect)
-            return self._validate_output(request, result, spider)
+            return result
-def pformat_dictobj(obj):
+def _pformat_dictobj(obj):
-        return pformat_dictobj(obj)
+    if isinstance(obj, BaseItem):
-        return colorize(pypprint.pformat(obj))
+        return colorize(pypprint.pformat(repr(obj)))
-
+from lxml import html, etree
-        l = TestXPathItemLoader(response=response)
+        l = TestXPathItemLoader(response=self.response)
-        l = TestXPathItemLoader(response=response)
+        l = TestXPathItemLoader(response=self.response)
-        self._values[field_name] = arg_to_iter(processed_value)
+        self._values.pop(field_name, None)
-__version__ = "0.7.0-rc1"
+version_info = (0, 7, 0, 'final', 0)
-from scrapy.core.exceptions import DropItem, NotConfigured
+from scrapy.core.exceptions import DropItem, NotConfigured, IgnoreRequest
-            raise ImageException(msg)
+            log.msg('Image (http-error): Error downloading image from %s referred in <%s>' \
-            raise ImageException(msg)
+            log.msg('Image (empty-content): Empty image from %s referred in <%s>: no-content' \
-            raise ex
+            raise
-        raise ImageException(msg)
+        if not isinstance(failure.value, IgnoreRequest):
-        return DeferredList(dlist).addCallback(self.item_completed, item, info)
+        return DeferredList(dlist, consumeErrors=1).addCallback(self.item_completed, item, info)
-        'text': 'scrapy.http.TextResponse',
+        'text/*': 'scrapy.http.TextResponse',
-        else:
+        if mimetype is None:
-            res.extend(cb_res)
+            res.extend(iterate_spider_output(cb_res))
-from scrapy.utils.misc import load_object, arg_to_iter
+from scrapy.utils.spider import iterate_spider_output
-        return [result] if isinstance(result, BaseItem) else arg_to_iter(result)
+        return request.deferred.addCallback(iterate_spider_output)
-__version__ = "0.7.0-rc1"
+version_info = (0, 8, 0, '', 0)
-
+from scrapy.utils.python import get_func_args
-            kwargs['body'] = xmlrpclib.dumps(params, methodname)
+        encoding = kwargs.get('encoding', None)
-import unittest
+import unittest
-        self.assertEqual(r.method, "GET")
+        self.assertEqual(r.method, self.default_method)
-        self.assertEqual(r.meta, {})
+        self.assertEqual(r.headers, self.default_headers)
-        r1 = self.request_class("http://www.example.com")
+        r1 = self.request_class("http://www.example.com", method='GET')
-        self.assertEqual((r1.headers, r2.headers), ({}, hdrs))
+        self.assertEqual((r1.headers, r2.headers), (self.default_headers, hdrs))
-        r = XmlRpcRequest('http://scrapytest.org/rpc2', methodname='login', params=('username', 'password'))
+    request_class = XmlRpcRequest
-        self.assertEqual(r.body, "<?xml version='1.0'?>\n<methodCall>\n<methodName>login</methodName>\n<params>\n<param>\n<value><string>username</string></value>\n</param>\n<param>\n<value><string>password</string></value>\n</param>\n</params>\n</methodCall>\n")
+        self.assertEqual(r.body, xmlrpclib.dumps(**kwargs))
-# XXX: XmlRpcRequest doesn't respect original Request API. is this expected?
+class XmlRpcRequestTest(RequestTest):
-    def test_basic(self):
+    def test_xmlrpc_basic(self):
-
+    classifiers = [
-version = '0.7.0'
+try:
-__version__ = "0.7.0"
+version_info = (0, 7, 0, 'candidate', 0)
-from os.path import exists, join
+from os.path import exists, join, dirname
-    name = 'Scrapy',
+    name = 'scrapy',
-    author_email = '',
+    maintainer = 'Pablo Hoffman',
-    THUMBS = settings['IMAGES_THUMBS']
+    THUMBS = settings.get('IMAGES_THUMBS', {})
-        for thumb_id, size in self.THUMBS.iteritems() or []:
+        for thumb_id, size in self.THUMBS.iteritems():
-                self.close_spider, spider, reason='shutdown')
+        def before_shutdown():
-            self._finish_closing_spider(spider)
+            return self._finish_closing_spider(spider)
-            send_catch_log(signal=signals.engine_stopped, sender=self.__class__)
+        dfd = defer.maybeDeferred(spiders.close_spider, spider)
-def defer_failed(_failure):
+def defer_fail(_failure):
-        return defer_failed(result)
+        return defer_fail(result)
-        return defer_failed(failure.Failure())
+        return defer_fail(failure.Failure())
-        self.xg.characters(serialized_value)
+        if hasattr(serialized_value, '__iter__'):
-        self.running = set()
+        self.running = {}
-        self.running.add(domain)
+    def domain_opened(self, spider):
-        self.finished.add(domain)
+    def domain_closed(self, spider):
-        self.idle = [d for d in enabled_domains if d not in self.scheduled
+        self.scheduled = [s.domain_name for s in scrapyengine.spider_scheduler._pending_spiders]
-                (len(self.running),
+        s += "<tr><th>Idle (%d)</th><th>Scheduled (%d)</th><th>Running (%d/%d)</th><th>Finished (%d)</th></tr>\n" % \
-                 len(self.idle))
+                 len(self.finished))
-        # running
+        # idle
-        for domain in sorted(self.running):
+        s += '<select name="add_pending_domains" multiple="multiple">\n'
-        s += '<input type="submit" value="Stop selected">\n'
+        s += '<input type="submit" value="Schedule selected">\n'
-        # finished
+        # running
-        for domain in sorted(self.finished):
+        s += '<select name="stop_running_domains" multiple="multiple">\n'
-        s += '<input type="submit" value="Re-schedule selected">\n'
+        s += '<input type="submit" value="Stop selected">\n'
-        # idle
+        # finished
-        for domain in sorted(self.idle):
+        s += '<select name="rerun_finished_domains" multiple="multiple">\n'
-        s += "<span style='font-size: small'>(enter one domain per line)</span>\n"
+        s += '<input type="submit" value="Re-schedule selected">\n'
-            s += "Stopped spiders: <ul><li>%s</li></ul>" % "</li><li>".join(args["stop_running_domains"])
+            stopped_domains = []
-                scrapyengine.close_domain(domain)
+                if domain in self.running:
-                if scrapyengine.domain_scheduler.remove_pending_domain(domain):
+                if scrapyengine.spider_scheduler.remove_pending_domain(domain):
-                s += "</p>"
+        self.enabled_domains = spiders.list()
-from datetime import datetime
+from time import time
-    s += "<p>Bot: <b>%s</b> | Host: <b>%s</b> | Uptime: <b>%s</b> | Time: <b>%s</b> </p>\n" % (settings['BOT_NAME'], socket.gethostname(), str(uptime), str(now.replace(microsecond=0)))
+    uptime = time() - scrapyengine.start_time
-        s += "<h2><a href='/%s/'>%s</a></h2>\n" % (module.webconsole_id, module.webconsole_name)
+        s += "<h2><a href='/%s/'>%s</a></h2>\n" % (module.webconsole_id, \
-            for _, obj in dispatcher.send(signal=webconsole_discover_module, sender=self.__class__):
+            for _, obj in dispatcher.send(signal=webconsole_discover_module, \
-from datetime import datetime
+from time import time
-        self.start_time = datetime.utcnow()
+        self.start_time = time()
-        "datetime.utcnow()-engine.start_time",
+        "time()-engine.start_time",
-        self.pending_spiders = []
+        self._pending_spiders = []
-            return self.pending_spiders.pop(0)
+        if self._pending_spiders:
-        self.pending_spiders.append(spider)
+        self._pending_spiders.append(spider)
-        self.pending_spiders = [d for d in self.pending_spiders if d != spider]
+        self._pending_spiders = [d for d in self._pending_spiders if d != spider]
-        return spider in self.pending_spiders
+        return spider in self._pending_spiders
-        schd.addErrback(log.err, "Unhandled error on engine.crawl()")
+        # FIXME: we can't log errors because we would be preventing them from
-                log.msg("Crawled %s (referer: <%s>)" % (response, referer), \
+                log.msg("Crawled %s (referer: <%s>)" % (request, referer), \
-    def close_domain(self, domain):
+    def close_spider(self, spider):
-        spiders.close_domain(domain)
+        spiders.close_spider(spider)
-  open a domain for tracking duplicates (typically used to reserve resources)
+* open_spider(spider)
-  close a domain (typically used for freeing resources)
+* close_spider(spider)
-* request_seen(domain, request, dont_record=False)
+* request_seen(spider, request, dont_record=False)
-    def open_domain(self, domain):
+    def open_spider(self, spider):
-    def close_domain(self, domain):
+    def close_spider(self, spider):
-    def request_seen(self, domain, request, dont_record=False):
+    def request_seen(self, spider, request, dont_record=False):
-        self.fingerprints[domain] = set()
+    def open_spider(self, spider):
-        del self.fingerprints[domain]
+    def close_spider(self, spider):
-    def request_seen(self, domain, request, dont_record=False):
+    def request_seen(self, spider, request, dont_record=False):
-        if fp in self.fingerprints[domain]:
+        if fp in self.fingerprints[spider]:
-            self.fingerprints[domain].add(fp)
+            self.fingerprints[spider].add(fp)
-        seen = self.dupefilter.request_seen(domain, request)
+    def enqueue_request(self, spider, request):
-        self.dupefilter.open_domain(domain)
+    def open_spider(self, spider):
-        self.dupefilter.close_domain(domain)
+    def close_spider(self, spider):
-        for name in ['enqueue_request', 'open_domain', 'close_domain']:
+        for name in ['enqueue_request', 'open_spider', 'close_spider']:
-                result = mwfunc(domain=spider.domain_name, request=request)
+                result = mwfunc(spider=spider, request=request)
-            mwfunc(spider.domain_name)
+        for mwfunc in self.mw_cbs['open_spider']:
-            mwfunc(spider.domain_name)
+        for mwfunc in self.mw_cbs['close_spider']:
-        domain = 'scrapytest.org'
+        spider = BaseSpider()
-        filter.open_domain(domain)
+        filter.open_spider(spider)
-        assert filter.request_seen(domain, r1)
+        assert not filter.request_seen(spider, r1)
-        assert filter.request_seen(domain, r3)
+        assert not filter.request_seen(spider, r2)
-        filter.close_domain(domain)
+        filter.close_spider(spider)
-        domain = 'scrapytest.org'
+        spider = BaseSpider()
-        filter.open_domain(domain)
+        filter.open_spider(spider)
-        filter.close_domain(domain)
+        assert not filter.request_seen(spider, r1)
-        self.mw.open_domain(DOMAIN)
+        self.spider = BaseSpider()
-        self.mw.close_domain(DOMAIN)
+        self.mw.close_spider(self.spider)
-        self.assertRaises(IgnoreRequest, self.mw.enqueue_request, DOMAIN, r4)
+        assert not self.mw.enqueue_request(self.spider, r1)
-    """Spider locator and manager"""
+    """Spider manager based in Twisted Plugin System"""
-                self.add_spider(spider)
+                ISpider.validateInvariants(spider)
-    Visited http://scrapy.org 
+    Visited: http://scrapy.org 
-from scrapy.fetcher import fetch
+from scrapy.utils.fetch import fetch
-            help="print HTTP headers instead of body")
+            help="print response HTTP headers instead of body")
-            print "A URL is required"
+        if len(args) != 1:
-from scrapy.fetcher import fetch
+from scrapy.utils.fetch import fetch
-DOMAIN_SCHEDULER = 'scrapy.contrib.domainsch.FifoDomainScheduler'
+SPIDER_SCHEDULER = 'scrapy.contrib.spiderscheduler.FifoSpiderScheduler'
-            domain=domain, reason='closedomain_timeout')
+    def domain_opened(self, spider):
-            scrapyengine.close_domain(spider.domain_name, 'closedomain_itempassed')
+        self.counts[spider] += 1
-        tsk = self.tasks.pop(domain, None)
+    def domain_closed(self, spider):
-    def process_item(self, domain, item):
+    def process_item(self, item, spider):
-                scrapyengine.close_domain(domain)
+                scrapyengine.close_spider(spider)
-    def open_domain(self, domain):
+    def open_spider(self, spider):
-    def close_domain(self, domain):
+    def close_spider(self, spider):
-            self.s3_spider = spiders.fromdomain(domain)
+        use_custom_spider = bool(settings['IMAGES_S3STORE_SPIDER'])
-            self.spider = spiders.fromdomain(domain)
+        def __init__(self, spider):
-        self.domaininfo[domain] = self.DomainInfo(domain)
+    def domain_opened(self, spider):
-    It supports a limited number of connections per domain and many domains in
+    It supports a limited number of connections per spider and many spiders in
-        site = self.sites[spider.domain_name]
+        site = self.sites[spider]
-            raise IgnoreRequest('Cannot fetch on a closing domain')
+            raise IgnoreRequest('Cannot fetch on a closing spider')
-            self._close_if_idle(spider.domain_name)
+            self._close_if_idle(spider)
-        site = self.sites[spider.domain_name]
+        site = self.sites[spider]
-        site = self.sites.get(domain)
+        site = self.sites.get(spider)
-        self._close_if_idle(domain)
+        self._close_if_idle(spider)
-        site = self.sites.get(domain)
+    def _close_if_idle(self, spider):
-            del self.sites[domain]
+            del self.sites[spider]
-                log.msg("Crawled while closing domain: %s" % request, \
+                log.msg("Crawled while closing spider: %s" % request, \
-            raise RuntimeError('Downloader domain already opened: %s' % domain)
+    def open_spider(self, spider):
-        self.sites[domain] = SiteInfo(
+        self.sites[spider] = SiteInfo(
-        site = self.sites.get(domain)
+    def close_spider(self, spider):
-            raise RuntimeError('Downloader domain already closed: %s' % domain)
+            raise RuntimeError('Downloader spider already closed: %s' % domain)
-        """Does the downloader have capacity to handle more domains"""
+        """Does the downloader have capacity to handle more spiders"""
-        self.closing = {} # dict (domain -> reason) of spiders being closed
+        self.closing = {} # dict (spider -> reason) of spiders being closed
-        self.domain_scheduler = load_object(settings['DOMAIN_SCHEDULER'])()
+        self.spider_scheduler = load_object(settings['SPIDER_SCHEDULER'])()
-        for domain in self.open_domains:
+        for spider in self.open_spiders:
-                self.close_domain, domain, reason='shutdown')
+                self.close_spider, spider, reason='shutdown')
-        return domain
+    def next_spider(self):
-    def next_request(self, domain, now=False):
+    def next_request(self, spider, now=False):
-        The domain is closed if there are no more pages to scrape.
+        The spider is closed if there are no more pages to scrape.
-            return reactor.callLater(0, self.next_request, domain, now=True)
+            self._next_request_pending.discard(spider)
-            return reactor.callLater(5, self.next_request, domain)
+            return reactor.callLater(5, self.next_request, spider)
-            if not self._next_request(domain):
+        while not self._needs_backout(spider):
-            self._domain_idle(domain)
+        if self.spider_is_idle(spider):
-    def _needs_backout(self, domain):
+    def _needs_backout(self, spider):
-            or self.scraper.sites[domain].needs_backout()
+            or self.spider_is_closed(spider) \
-    def _next_request(self, domain):
+    def _next_request(self, spider):
-        request, deferred = self.scheduler.next_request(domain)
+        request, deferred = self.scheduler.next_request(spider)
-            and self.downloader.sites[domain].active
+    def spider_is_idle(self, spider):
-        """Return True if the domain is fully closed (ie. not even in the
+    def spider_is_closed(self, spider):
-        return domain not in self.downloader.sites
+        return spider not in self.downloader.sites
-        """Return True if the domain is fully opened (ie. not in closing
+    def spider_is_open(self, spider):
-        return domain in self.downloader.sites and domain not in self.closing
+        return spider in self.downloader.sites and spider not in self.closing
-    def open_domains(self):
+    def open_spiders(self):
-        schd.addBoth(lambda _: self.next_request(spider.domain_name))
+        schd.addBoth(lambda _: self.next_request(spider))
-        if domain in self.closing:
+        if spider in self.closing:
-        return self.scheduler.enqueue_request(domain, request)
+        if not self.scheduler.spider_is_open(spider):
-            if not self.next_domain():
+            if not self.next_spider():
-            self.next_request(domain)
+            self.next_request(spider)
-    def open_domain(self, domain):
+    def open_spider(self, spider):
-        self.next_request(domain)
+        self.next_request(spider)
-        self.scraper.open_domain(domain)
+        self.downloader.open_spider(spider)
-    def _domain_idle(self, domain):
+    def _spider_idle(self, spider):
-        spider = spiders.fromdomain(domain)
+        domain = spider.domain_name
-            self.next_request(domain)
+            self.next_request(spider)
-            self.close_domain(domain, reason='finished')
+        if self.spider_is_idle(spider):
-        if domain not in self.closing:
+    def close_spider(self, spider, reason='cancelled'):
-            return self._finish_closing_domain_if_idle(domain)
+            self.closing[spider] = reason
-            self._finish_closing_domain(domain)
+    def _finish_closing_spider_if_idle(self, spider):
-            dfd.addCallback(self._finish_closing_domain_if_idle)
+            dfd.addCallback(self._finish_closing_spider_if_idle)
-            reactor.callLater(delay, dfd.callback, domain)
+            reactor.callLater(delay, dfd.callback, spider)
-        reason = self.closing.pop(domain, 'finished')
+    def _finish_closing_spider(self, spider):
-        log.msg("Domain closed (%s)" % reason, domain=domain) 
+        log.msg("Domain closed (%s)" % reason, domain=domain) 
-        if not self.open_domains:
+        if not self.open_spiders:
-from scrapy.spider import spiders
+from scrapy.spider import BaseSpider, spiders
-                perdomain.setdefault(spider.domain_name, []).append(req)
+def _get_spider_requests(*args):
-    return perdomain
+            raise TypeError("Unsupported argument: %r" % arg)
-            for request in requests[domain]:
+        assert self.configured, "Scrapy Manager not yet configured"
-        for name in ('enqueue_request', 'open_domain', 'close_domain'):
+        for name in ['enqueue_request', 'open_domain', 'close_domain']:
-    def enqueue_request(self, wrappedfunc, domain, request):
+    def enqueue_request(self, wrappedfunc, spider, request):
-                result = mwfunc(domain=domain, request=request)
+                result = mwfunc(domain=spider.domain_name, request=request)
-            return wrappedfunc(domain=domain, request=request)
+            return wrappedfunc(spider=spider, request=request)
-    def open_domain(self, domain):
+    def open_spider(self, spider):
-            mwfunc(domain)
+            mwfunc(spider.domain_name)
-    def close_domain(self, domain):
+    def close_spider(self, spider):
-            mwfunc(domain)
+            mwfunc(spider.domain_name)
-        return domain in self.pending_requests
+    def spider_is_open(self, spider):
-            return bool(self.pending_requests[domain])
+    def spider_has_pending_requests(self, spider):
-            raise RuntimeError('Scheduler domain already opened: %s' % domain)
+    def open_spider(self, spider):
-        self.middleware.open_domain(domain)
+        self.pending_requests[spider] = Priority()
-    def close_domain(self, domain):
+    def close_spider(self, spider):
-        associated with the domain.
+        associated with the spider.
-        self.pending_requests.pop(domain, None)
+        if spider not in self.pending_requests:
-        return self.middleware.enqueue_request(self._enqueue_request, domain, request)
+    def enqueue_request(self, spider, request):
-    def _enqueue_request(self, domain, request):
+    def _enqueue_request(self, spider, request):
-        self.pending_requests[domain].push((request, dfd), -request.priority)
+        self.pending_requests[spider].push((request, dfd), -request.priority)
-        q = self.pending_requests[domain]
+    def clear_pending_requests(self, spider):
-        """Return the next available request to be downloaded for a domain.
+    def next_request(self, spider):
-        the given domain.
+        the given spider.
-            return self.pending_requests[domain].pop()[0] # [1] is priority
+            return self.pending_requests[spider].pop()[0] # [1] is priority
-class SiteInfo(object):
+class SpiderInfo(object):
-        self.itemproc.open_domain(domain)
+    def open_spider(self, spider):
-        self.itemproc.open_domain(domain)
+    def close_spider(self, spider):
-        site = self.sites[spider.domain_name]
+        site = self.sites[spider]
-        if domain in self.engine.closing:
+        if spider in self.engine.closing:
-            self.sites[domain].itemproc_size += 1
+            self.sites[spider].itemproc_size += 1
-        self.sites[domain].itemproc_size -= 1
+        self.sites[spider].itemproc_size -= 1
-        return "%s(%s)" % (self.__class__.__name__, repr(d))
+        attrs = ['url', 'method', 'body', 'headers', 'cookies', 'meta']
-        return "%s(%s)" % (self.__class__.__name__, repr(d))
+        attrs = ['url', 'status', 'body', 'headers', 'meta', 'flags']
-from scrapy.spider import spiders
+from scrapy.spider import BaseSpider, spiders
-        spider = get_or_create_spider(url)
+        spider = spiders.fromurl(url) or BaseSpider('default')
-    start_urls = []
+    # XXX: class attributes kept for backwards compatibility
-import unittest
+import sys, os, re, urlparse, unittest
-        self.spider = spiders.fromdomain(self.domain)
+        self.spider = TestSpider()
-            scrapymanager.runonce(self.domain)
+            scrapymanager.runonce(self.spider)
-        "engine.scraper.sites[domain].needs_backout()",
+    spider_tests = [
-        for test in domain_tests:
+    for spider in engine.downloader.sites:
-    THUMBS = settings.getlist('IMAGES_THUMBS')
+    THUMBS = settings['IMAGES_THUMBS']
-        for thumb_id, size in self.THUMBS or []:
+        for thumb_id, size in self.THUMBS.iteritems() or []:
-        return {'scraped_url': request.url, 'path': key, 'checksum': checksum}
+        return {'url': request.url, 'path': key, 'checksum': checksum}
-            return {'scraped_url': request.url, 'path': key, 'checksum': checksum}
+            return {'url': request.url, 'path': key, 'checksum': checksum}
-import urlparse
+from urlparse import urlparse
-    framework.
+def fetch(urls):
-    downloaded.
+    Suitable for for calling from a script, shouldn't be called from spiders.
-
+    map(get_or_create_spider, urls)
-        domain = urlparse.urlparse(url).hostname
+        domain = urlparse(url).hostname
-        print "\n".join(spiders.asdict().keys())
+        print "\n".join(spiders.list())
-        self.spider_modules = None
+        self._invaliddict = {}
-        return self.asdict().get(domain)
+        return self._spiders.get(domain)
-            return self.asdict().get(self.force_domain)
+            return self._spiders.get(self.force_domain)
-                return self.asdict()[domain]
+            if domain in self._spiders:         # try first locating by domain
-                plist = self.asdict().values()
+                plist = self._spiders.values()
-        return self._spiders
+    def list(self):
-        log.msg("Reloaded %d/%d scrapy spiders" % (reloaded, len(pdict)), level=log.DEBUG)
+        ISpider.validateInvariants(spider)
-        enabled_domains = spiders.asdict().keys()
+        enabled_domains = spiders.list()
-        enabled_domains = spiders.asdict().keys()
+        enabled_domains = spiders.list()
-        domain = str(urlparse.urlparse(url).hostname or spiders.default_domain)
+        domain = urlparse.urlparse(url).hostname
-from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory
+HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])
-    factory = ScrapyHTTPClientFactory.from_request(request, timeout)
+    factory = HTTPClientFactory.from_request(request, timeout)
-    'scrapy.contrib.debug.StackTraceDump': 0,
+
-            self._url = safe_url_string(decoded_url, self.encoding)
+        if isinstance(url, str):
-                    "must specify the encoding" % self.__class__.__name__)
+            if self.encoding is None:
-                import readline
+        try:
-            code.interact(local=self.vars)
+                import code
-        stats.close_domain(self.spider.domain_name)
+    def tearDown(self):
-    __slots__ = ['url', 'headers', 'status', '_body', 'request', '_meta', \
+    __slots__ = ['_url', 'headers', 'status', '_body', 'request', '_meta', \
-        self.url = url
+        self._set_url(url)
-            response = TextResponse(url=None, body=unicode_to_str(text), \
+            response = TextResponse(url='about:blank', body=unicode_to_str(text), \
-        self.assertRaises(Exception, Request)
+        self.assertRaises(Exception, self.request_class)
-        r = Request('http://www.example.com')
+        self.assertRaises(TypeError, self.request_class, 123)
-        r = Request("http://www.example.com")
+        r = self.request_class("http://www.example.com")
-        r = Request("http://www.example.com", meta=meta, headers=headers, body="a body")
+        r = self.request_class("http://www.example.com", meta=meta, headers=headers, body="a body")
-        p = Request(url=url, headers=r.headers)
+        r = self.request_class(url=url, headers=headers)
-        r2 = Request(url=url)
+        r1 = self.request_class(url=url)
-        r = Request(url="http://www.scrapy.org/path")
+        r = self.request_class(url="http://www.scrapy.org/path")
-        r = Request(url="http://www.scrapy.org/blank%20space")
+        r = self.request_class(url="http://www.scrapy.org/blank%20space")
-        r = Request(url="http://www.scrapy.org/blank space")
+        r = self.request_class(url="http://www.scrapy.org/blank space")
-        r2 = Request(url=u"http://www.scrapy.org/price/\xa3", encoding="latin1")
+        r1 = self.request_class(url=u"http://www.scrapy.org/price/\xa3", encoding="utf-8")
-        r1 = Request(url="http://www.example.com/")
+        r1 = self.request_class(url="http://www.example.com/")
-        r2 = Request(url="http://www.example.com/", body="")
+        r2 = self.request_class(url="http://www.example.com/", body="")
-        r3 = Request(url="http://www.example.com/", body=u"Price: \xa3100", encoding='utf-8')
+        r3 = self.request_class(url="http://www.example.com/", body=u"Price: \xa3100", encoding='utf-8')
-        r4 = Request(url="http://www.example.com/", body=u"Price: \xa3100", encoding='latin1')
+        r4 = self.request_class(url="http://www.example.com/", body=u"Price: \xa3100", encoding='latin1')
-        r1 = Request("http://www.example.com", callback=somecallback)
+        r1 = self.request_class("http://www.example.com", callback=somecallback)
-        class CustomRequest(Request):
+        class CustomRequest(self.request_class):
-        r1 = Request("http://www.example.com")
+        r1 = self.request_class("http://www.example.com")
-        r3 = Request("http://www.example.com", meta={'a': 1}, dont_filter=True)
+        r3 = self.request_class("http://www.example.com", meta={'a': 1}, dont_filter=True)
-                x.__class__.__name__
+        x = self.request_class('http://www.example.com')
-class FormRequestTest(unittest.TestCase):
+    request_class = FormRequest
-        r1 = FormRequest("http://www.example.com", formdata={})
+        r1 = self.request_class("http://www.example.com", formdata={})
-        r2 = FormRequest("http://www.example.com", formdata=data)
+        r2 = self.request_class("http://www.example.com", formdata=data)
-        r3 = FormRequest("http://www.example.com", formdata=data, encoding='latin1')
+        r3 = self.request_class("http://www.example.com", formdata=data, encoding='latin1')
-        r3 = FormRequest("http://www.example.com", formdata=data)
+        r3 = self.request_class("http://www.example.com", formdata=data)
-        r1 = FormRequest.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'}, callback=lambda x: x)
+        r1 = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'}, callback=lambda x: x)
-        r1 = FormRequest.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})
+        r1 = self.request_class.from_response(response, formdata={'one': ['two', 'three'], 'six': 'seven'})
-        r1 = FormRequest.from_response(response, formdata={'two': '2'})
+        r1 = self.request_class.from_response(response, formdata={'two': '2'})
-        self.assertRaises(ValueError, FormRequest.from_response, response)
+        self.assertRaises(ValueError, self.request_class.from_response, response)
-        self.assertRaises(IndexError, FormRequest.from_response, response, formnumber=1)
+        self.assertRaises(IndexError, self.request_class.from_response, response, formnumber=1)
-class ResponseTest(unittest.TestCase):
+
-        self.assertTrue(isinstance(Response('http://example.com/', body='body'), Response))
+        self.assertRaises(Exception, self.response_class)
-        self.assertTrue(isinstance(Response('http://example.com/', headers={}, status=200, body=''), Response))
+        self.assertTrue(isinstance(self.response_class('http://example.com/', headers={}, status=200, body=''), self.response_class))
-        r = Response("http://www.example.com")
+        r = self.response_class("http://www.example.com")
-        r = Response("http://www.example.com", meta=meta, headers=headers, body="a body")
+        r = self.response_class("http://www.example.com", meta=meta, headers=headers, body=body)
-        r = Response("http://www.example.com", status=301)
+        r = self.response_class("http://www.example.com", status=301)
-        r = Response("http://www.example.com", status='301')
+        r = self.response_class("http://www.example.com", status='301')
-        self.assertRaises(ValueError, Response, "http://example.com", status='lala200')
+        self.assertRaises(ValueError, self.response_class, "http://example.com", status='lala200')
-        r1 = Response("http://www.example.com", body="Some body")
+        r1 = self.response_class("http://www.example.com", body="Some body")
-        class CustomResponse(Response):
+        class CustomResponse(self.response_class):
-        r1 = Response("http://www.example.com")
+        r1 = self.response_class("http://www.example.com")
-        r3 = Response("http://www.example.com", meta={'a': 1}, flags=['cached'])
+        r3 = self.response_class("http://www.example.com", meta={'a': 1}, flags=['cached'])
-        self.assertEqual(r1.body_as_unicode(), unicode_string)
+    def test_weakref_slots(self):
-        r4 = TextResponse("http://www.example.com", body="\xa2\xa3")
+class ResponseText(BaseResponseTest):
-        self.assertRaises(TypeError, TextResponse, "http://www.example.com", body=u"\xa3")
+        self.assertRaises(TypeError, self.response_class, "http://www.example.com", body=u"\xa3")
-        r1 = HtmlResponse("http://www.example.com", body=body)
+        r1 = self.response_class("http://www.example.com", body=body)
-        r2 = HtmlResponse("http://www.example.com", body=body)
+        r2 = self.response_class("http://www.example.com", body=body)
-        r3 = HtmlResponse("http://www.example.com", headers={"Content-type": ["text/html; charset=iso-8859-1"]}, body=body)
+        r3 = self.response_class("http://www.example.com", headers={"Content-type": ["text/html; charset=iso-8859-1"]}, body=body)
-        r1 = XmlResponse("http://www.example.com", body=body)
+        r1 = self.response_class("http://www.example.com", body=body)
-        r2 = XmlResponse("http://www.example.com", body=body)
+        r2 = self.response_class("http://www.example.com", body=body)
-        r3 = XmlResponse("http://www.example.com", body=body, encoding='utf-8')
+        r3 = self.response_class("http://www.example.com", body=body, encoding='utf-8')
-        r5 = XmlResponse("http://www.example.com", body=body)
+        r5 = self.response_class("http://www.example.com", body=body)
-
+"""
-from urlparse import urlsplit
+from scrapy.utils.aws import sign_request
-        self.secret_key = settings['AWS_SECRET_ACCESS_KEY'] or os.environ.get('AWS_SECRET_ACCESS_KEY')
+        self.access_key = settings['AWS_ACCESS_KEY_ID'] or \
-            request.headers['Date'] = time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime())
+            request.headers['Date'] = time.strftime("%a, %d %b %Y %H:%M:%S GMT", \
-from scrapy.contrib import aws
+
-# keys are provided by amazon developer guide at
+# just some random keys. keys are provided by amazon developer guide at
-
+"""Helper function for working with Amazon Web Services"""
-from scrapy.utils.db import mysql_connect
+from scrapy.utils.mysql import mysql_connect
-    'scrapy.contrib.downloadermiddleware.cache.HttpCacheMiddleware': 900,
+    'scrapy.contrib.downloadermiddleware.httpcache.HttpCacheMiddleware': 900,
-
+from unittest import TestCase
-        rp = self.robot_parser(request.url, spider)
+        rp = self.robot_parser(request, spider)
-    def robot_parser(self, url, spider):
+    def robot_parser(self, request, spider):
-        return len(self.queue) > 2 * self.max_concurrent_requests
+        return len(self.active) > 2 * self.max_concurrent_requests
-        self._headers_written = False
+        self._headers_not_written = True
-        self._write_headers_and_set_fields_to_export(item)
+        if self._headers_not_written:
-        self.csv_writer.writerow(self.fields_to_export)
+        if self.include_headers_line:
-        self._header_written = False
+        self._headers_written = False
-
+        self._write_headers_and_set_fields_to_export(item)
-            self.csv_writer.writerow(self.fields_to_export)
+    def _write_headers_and_set_fields_to_export(self, item):
-    def __init__(self, file, include_headers_line=False, **kwargs):
+    def __init__(self, file, include_headers_line=True, **kwargs):
-            self.csv_writer.writerow(self.fields_to_export)
+        self._header_written = False
-        self.assertEqual(self.output.getvalue(), '22,John\xc2\xa3\r\n')
+        self.assertEqual(self.output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n')
-            fields_to_export=self.i.fields.keys())
+        ie = CsvItemExporter(output, fields_to_export=self.i.fields.keys())
-        if reactor.running:
+        try:
-        elif not self.open_domains:
+        self._mainloop()
-    scrapymanager.configure()
+    scrapymanager.configure(control_reactor=True)
-        return self.asdict().get(domain_name)
+    def fromdomain(self, domain):
-    def _domain_closed(self, domain, spider):
+    def close_domain(self, domain):
-        self.queue = []
+        self.queue = []
-        """ Main method to use to request a download
+        """Main method to use to request a download
-            raise IgnoreRequest('Can\'t fetch on a closing domain')
+            raise IgnoreRequest('Cannot fetch on a closing domain')
-        return self.middleware.download(self.enqueue, request, spider).addBoth(_deactivate)
+        dfd = self.middleware.download(self.enqueue, request, spider)
-        deferred = defer.Deferred()
+        if site.closing:
-        self.process_queue(spider)
+        self._process_queue(spider)
-        """
+        """Effective download requests from site queue"""
-                reactor.callLater(penalty, self.process_queue, spider=spider)
+                reactor.callLater(penalty, self._process_queue, spider=spider)
-        # Process requests in queue if there are free slots to transfer for this site
+        # Process enqueued requests if there are free slots to transfer for this site
-            self._download(site, request, spider).chainDeferred(deferred)
+            if site.closing:
-            self.process_queue(spider)
+            self._process_queue(spider)
-        self.process_queue(spider)
+        self._process_queue(spider)
-from twisted.internet import reactor, task
+from twisted.internet import reactor, task, defer
-    def start(self, control_reactor=True):
+    def start(self):
-        """Stop the execution engine"""
+        """Stop the execution engine gracefully"""
-            stats.close_domain(domain, reason='shutdown')
+            reactor.addSystemEventTrigger('before', 'shutdown', \
-        send_catch_log(signal=signals.engine_stopped, sender=self.__class__)
+
-        return self.scheduler.is_idle() and self.downloader.is_idle() and self.scraper.is_idle()
+        return self.scheduler.is_idle() and self.downloader.is_idle() and \
-            return dwld.addErrback(log.err)
+            dwld.addErrback(log.err, "Unhandled error on engine._next_request")
-        scraper_idle = domain in self.scraper.sites and self.scraper.sites[domain].is_idle()
+        scraper_idle = domain in self.scraper.sites \
-        downloading = domain in self.downloader.sites and self.downloader.sites[domain].active
+        downloading = domain in self.downloader.sites \
-        schd.addErrback(log.err)
+        schd.addErrback(log.err, "Unhandled error on engine.crawl()")
-                    domain=domain)
+                log.msg("Crawled %s (referer: <%s>)" % (response, referer), \
-            return Failure(IgnoreRequest(str(ex)))
+            exc = _failure.value
-            log.err(_why="Exception catched on domain_idle signal dispatch")
+            log.err("Exception catched on domain_idle signal dispatch")
-            self._finish_closing_domain_if_idle(domain)
+            return self._finish_closing_domain_if_idle(domain)
-        if self.domain_is_idle(domain):
+        if self.domain_is_idle(domain) or self.killed:
-            reactor.callLater(5, self._finish_closing_domain_if_idle, domain)
+            dfd = defer.Deferred()
-        self._mainloop()
+        spiders.close_domain(domain)
-    pass
+
-from scrapy.conf import settings
+from scrapy.utils.ossignal import install_shutdown_handlers, signal_names
-        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)
+            install_shutdown_handlers(self._signal_shutdown)
-        requests = self._parse_args(args)
+        requests = _parse_args(args)
-            self.configure()
+        assert self.configured, "Scrapy Manger not yet configured"
-    def start(self, control_reactor=True):
+    def start(self):
-        self.configure(control_reactor)
+        assert self.configured, "Scrapy Manger not yet configured"
-            self.stop()
+        scrapyengine.start()
-        return perdomain
+        if self.control_reactor and reactor.running:
-        log.msg(msg_txt, system=system)
+    msg_txt = unicode_to_str("%s: %s" % (level_names[level], message))
-def err(*args, **kwargs):
+def err(_stuff=None, _why=None, **kwargs):
-    log.err(*args, **kwargs)
+    if _why:
-            self.vars['shelp'] = self.print_help
+        if not self.nofetch:
-        self._spiders[domain] = new_module.SPIDER
+        module = sys.modules[module_name]
-        shell.inspect_response(response)
+    Shell(nofetch=True).inspect_response(response)
-        return self.download(request, info)
+from contextlib import contextmanager
-
+
-    log._switch_descriptors()
+    with log._std_descriptors():
-        self.pipeline = BaseImagesPipeline()
+        from scrapy.contrib.pipeline.images import ImagesPipeline
-                    % (mtype, request, referer)
+            msg = 'Image (http-error): Error downloading image from %s referred in <%s>' \
-                    % (mtype, request, referer)
+            msg = 'Image (empty-content): Empty image from %s referred in <%s>: no-content' \
-                (status, mtype, request, referer)
+        msg = 'Image (%s): Downloaded image from %s referred in <%s>' % \
-from scrapy.conf import settings
+from scrapy.http import Request
-class BaseImagesPipeline(MediaPipeline):
+class FSImagesStore(object):
-     )
+    EXPIRES = settings.getint('IMAGES_EXPIRES', 90)
-            if age_days > self.IMAGES_EXPIRES:
+            if age_days > self.EXPIRES:
-        dfd = defer.maybeDeferred(self.stat_key, key, info)
+        dfd = defer.maybeDeferred(self.store.stat_image, key, info)
-        dfd.addErrback(log.err, self.__class__.__name__ + '.stat_key')
+        dfd.addErrback(log.err, self.__class__.__name__ + '.store.stat_image')
-            self.store_image(key, image, buf, info)
+            self.store.persist_image(key, image, buf, info)
-        return req
+        return Request(url, method=method, body=body, headers=headers)
-        return '%s#%s' % (key, checksum) if checksum else key
+        return {'scraped_url': request.url, 'path': key, 'checksum': checksum}
-            return '%s#%s' % (key, checksum) if checksum else key
+            return {'scraped_url': request.url, 'path': key, 'checksum': checksum}
-from twisted.internet import defer
+from twisted.internet.defer import Deferred, DeferredList
-        for request in requests or ():
+        requests = arg_to_iter(self.get_media_requests(item, info))
-            lst.append(dfd)
+                    errbackArgs=(item, request, info),)
-        return dlst
+        return DeferredList(dlist).addCallback(self.item_completed, item, info)
-        wad = request.deferred or defer.Deferred()
+        wad = request.deferred or Deferred()
-            return wad # break
+            return defer_result(info.downloaded[fp]).chainDeferred(wad)
-            for wad in waiting: # call each waiting client with result
+            info.downloading.pop(fp)
-        def _evaluated(result):
+        def _post_media_to_download(result):
-            dwld.addErrback(_bugtrap) # catch media_downloaded and media_failed unhandled errors
+            dwld.addErrback(log.err, domain=info.domain)
-        dfd.addCallback(_evaluated)
+        dfd.addCallback(_post_media_to_download)
-        return value of this method isn't important and is recommended to return None.
+        return value of this method is used for results parameter of item_completed hook
-        return value of this method isn't important and is recommended to return None.
+        return value of this method is used for results parameter of item_completed hook
-        new_module = rebuild(sys.modules[module_name])
+        log.msg("Reloading module %s" % module_name, domain=domain, \
-    'scrapy.stats.corestats.CoreStats': 0,
+    'scrapy.contrib.corestats.CoreStats': 0,
-        reload(sys.modules[module])
+"""
-        """
+        pass
-        """
+"""
-SPIDERPROFILER_ENABLED = False
+SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.TwistedPluginSpiderManager'
-from scrapy.spider.manager import SpiderManager
+from scrapy.utils.misc import load_object
-spiders = SpiderManager()
+spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()
-                    self.sites[domain].itemproc_size, domain=domain)
+            # FIXME: this can't be called here because the stats domain may be
-def save_command_executed(cmdname, cmd, args, opts):
+def _save_command_executed(cmdname, cmd, args, opts):
-def find_commands(dir):
+def _find_commands(dir):
-def get_commands_from_module(module):
+def _get_commands_from_module(module):
-    for cmdname in find_commands(mod.__path__[0]):
+    for cmdname in _find_commands(mod.__path__[0]):
-    cmds = get_commands_from_module('scrapy.command.commands')
+def _get_commands_dict():
-        cmds.update(get_commands_from_module(cmds_module))
+        cmds.update(_get_commands_from_module(cmds_module))
-def get_command_name(argv):
+def _get_command_name(argv):
-def print_usage(inside_project):
+def _print_usage(inside_project):
-    cmds = get_commands_dict()
+    cmds = _get_commands_dict()
-def update_default_settings(module, cmdname):
+def _update_default_settings(module, cmdname):
-    cmds = get_commands_dict()
+    cmds = _get_commands_dict()
-    update_default_settings(settings['COMMANDS_SETTINGS_MODULE'], cmdname)
+    cmdname = _get_command_name(argv)
-        print_usage(settings.settings_module)
+        _print_usage(settings.settings_module)
-    ret = run_command(cmd, args, opts)
+    _save_command_executed(cmdname, cmd, args, opts)
-def run_command(cmd, args, opts):
+def _run_command(cmd, args, opts):
-            metavar="SETTING=VALUE", default=[], \
+        group.add_option("--set", dest="set", action="append", default=[], \
-
+        self.configured = True
-        self.configure()
+        if not self.configured:
-        return "Fetch a URL using the Scrapy downloader and print its content to stdout. You may want to use --nolog to disable logging"
+        return "Fetch a URL using the Scrapy downloader and print its content " \
-        parser.add_option("--headers", dest="headers", action="store_true", help="print HTTP headers instead of body")
+        parser.add_option("--headers", dest="headers", action="store_true", \
-        parser.add_option("-c", "--callbacks", dest="callbacks", action="store", help="use the provided callback(s) for parsing the url (separated with commas)")
+        parser.add_option("--nolinks", dest="nolinks", action="store_true", \
-                    log.msg('No rules found for spider "%s", please specify a callback for parsing' % spider.domain_name)
+                    log.msg('No rules found for spider "%s", please specify a callback for parsing' \
-            return
+            raise NotConfigured
-from scrapy.xlib.pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-
+            raise NotConfigured
-        s += scrapyengine.getstatus()
+        s += get_engine_status()
-from scrapy.core.engine import scrapyengine
+from scrapy.utils.engine import get_engine_status
-        s += scrapyengine.getstatus()
+        s += get_engine_status()
-
+from scrapy.utils.engine import print_engine_status
-                                  manhole.Manhole, telnet_namespace)
+        insults.ServerProtocol, manhole.Manhole, telnet_namespace)
-        self.warned = False
+        self.warned = False
-    def virtual(self):
+    def get_virtual_size(self):
-        stats.set_value('memusage/startup', self.virtual)
+        stats.set_value('memusage/startup', self.get_virtual_size())
-        stats.max_value('memusage/max', self.virtual)
+        stats.max_value('memusage/max', self.get_virtual_size())
-        if self.virtual > self.limit:
+        if self.get_virtual_size() > self.limit:
-        if self.virtual > self.warning:
+        if self.get_virtual_size() > self.warning:
-        s += "Current memory usage           : %dM\r\n" % (self.virtual/1024/1024)
+        s += "Current memory usage           : %dM\r\n" % (self.get_virtual_size()/1024/1024)
-from scrapy.extension import extensions
+from scrapy.utils.memory import get_vmvalue_from_procfs
-        new_cbs = ((self._profiled_callback(old_cbs[0][0], spider), old_cbs[0][1], old_cbs[0][2]), old_cbs[1])
+        new_cbs = ((self._profiled_callback(old_cbs[0][0], spider), old_cbs[0][1], \
-            stats.set_value('profiling/total_callback_time' % spider.domain_name, tcc+ct, domain=domain)
+            stats.set_value('profiling/total_callback_time' % spider.domain_name, \
-                stats.set_value('profiling/total_mem_allocated_in_callbacks', tma+mafter-mbefore, domain=domain)
+                stats.set_value('profiling/slowest_callback_name', function.__name__, \
-        return self.memusage.virtual if self.memusage else 0.0
+        return get_vmvalue_from_procfs('VmSize') if self._mem_tracking else 0.0
-from scrapy.xlib.pydispatch import dispatcher
+from twisted.internet import task
-
+        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)
-        self.tasks = []
+        self._mainloop_task = task.LoopingCall(self._mainloop)
-                reactor.run() # blocking call
+        if self.running:
-            send_catch_log(signal=signals.engine_stopped, sender=self.__class__)
+        if not self.running:
-
+from datetime import datetime
-from twisted.internet import protocol
+from twisted.internet import reactor, protocol
-        scrapyengine.listenTCP(port, self)
+        reactor.callWhenRunning(reactor.listenTCP, port, self)
-        scrapyengine.listenTCP(port, self)
+        reactor.callWhenRunning(reactor.listenTCP, port, self)
-        self.assertEqual(0, self.call('crawl'))
+class GenspiderCommandTest(CommandTest):
-        self.assertEqual(0, self.call('genspider', '--dump', '--template=basic'))
+    def test_template_default(self, *args):
-        self.assertEqual(0, self.call('list'))
+    def test_template_basic(self):
-    template = 'basic'
+    def test_template_xmlfeed(self):
-        self.assertEqual(1, self.call('genspider', 'otherspider', 'test.com'))
+    def test_template_crawl(self):
-    template = 'crawl'
+    def test_dump(self):
-    template = 'csvfeed'
+class MiscCommandsTest(CommandTest):
-    template = 'xmlfeed'
+    def test_list(self):
-        self.global_defaults = import_(self.default_settings_module)
+        self.global_defaults = default_settings
-
+from scrapy.utils.simpledb import to_sdb_value
-                (type(self).__name__, type(obj).__name__, ref))
+    def _to_sdb_value(self, obj, key=None):
-        args.extend(new_args)
+        args = (sys.executable, '-m', 'scrapy.command.cmdline') + new_args
-    def call(self, new_args, **kwargs):
+    def call(self, *new_args, **kwargs):
-    
+
-        self.assertEqual(ret, 0)
+        self.assertEqual(0, self.call('startproject', self.project_name))
-        self.assertEqual(ret, 1)
+        self.assertEqual(1, self.call('startproject', self.project_name))
-        self.call(['startproject', self.project_name])
+        self.call('startproject', self.project_name)
-        self.assertEqual(ret, 0)
+        self.assertEqual(0, self.call('crawl'))
-        self.assertEqual(ret, 0)
+        self.assertEqual(0, self.call('genspider', '--list'))
-        self.assertEqual(ret, 0)
+        self.assertEqual(0, self.call('list'))
-        self.assertEqual(ret, 0)
+        self.assertEqual(0, self.call('genspider', 'testspider', 'test.com', \
-import boto
+from boto import connect_sdb
-        
+        connect_sdb().create_domain(self._sdbdomain)
-        ts = datetime.utcnow().isoformat()
+        ts = self._get_timestamp(domain).isoformat()
-        sdb_item = dict([(k, self._to_sdb_value(v)) for k, v in stats.iteritems()])
+        sdb_item = dict((k, self._to_sdb_value(v, k)) for k, v in stats.iteritems())
-        sdb.batch_put_attributes(self._sdbdomain, {sdb_item_id: sdb_item})
+        connect_sdb().put_attributes(self._sdbdomain, sdb_item_id, sdb_item)
-        if isinstance(obj, int):
+    def _to_sdb_value(self, obj, ref=None):
-                type(obj).__name__)
+            raise TypeError("%s unsupported type '%s' referenced as '%s'" % \
-    from models import Person
+    from .models import Person
-    def engine_stopped(self):
+    def _engine_stopped(self):
-            log.msg("Dumping global stats:\n" + pprint.pformat(self.get_stats()))
+            log.msg("Dumping global stats:\n" + pprint.pformat(stats))
-        super(MemoryStatsCollector, self).close_domain(domain, reason)
+    def _persist_stats(self, stats, domain=None):
-            table = 'domain_data_history'
+    def _persist_stats(self, stats, domain=None):
-        super(MysqlStatsCollector, self).close_domain(domain, reason)
+        c = self._mysql_conn.cursor()
-        super(SimpledbStatsCollector, self).close_domain(domain, reason)
+    def _persist_stats(self, stats, domain=None):
-        logstdout = logstdout or settings.getbool('LOG_STDOUT')
+        if logstdout is None:
-            env=env, **kwargs)
+            env=self.env, **kwargs)
-    django = True
+    import django
-    django = False
+    django = None
-    def _get_exporter(self, **kwargs):
+    def setUp(self):
-        exported = simplejson.loads(self.output.getvalue().strip())
+        from scrapy.contrib.exporter.jsonlines import json
-BOT_NAME = PROJECT_NAME
+BOT_NAME = 'googledir'
-            'ProjectName': string_camelcase(settings.get('PROJECT_NAME')),
+            'project_name': settings.get('BOT_NAME'),
-import os
+import shutil
-PROJECT_TEMPLATES_PATH = os.path.join(scrapy.__path__[0], 'templates/project')
+TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'project')
-TEMPLATES = (
+TEMPLATES_TO_RENDER = (
-            print "Invalid project name: %s\n\n%s" % (project_name, message)
+        if not re.search(r'^[_a-zA-Z]\w*$', project_name):
-                    ProjectName=string_camelcase(project_name))
+        moduletpl = join(TEMPLATES_PATH, 'module')
-# Place here all your scrapy spiders
+# This package will contain the spiders of your Scrapy project
-    s += "===================\n"
+def print_usage(inside_project):
-    return s
+        if inside_project or not cmdclass.requires_project:
-
+        opts, args = parser.parse_args(args=argv[1:])
-        print 'Type "%s -h" for help' % argv[0]
+        print 'Use "scrapy-ctl.py -h" for help' 
-        sys.exit(1)
+    del args[0]  # remove command name from args
-        return "[url]"
+        return "[url|file]"
-            "local files you can use a URL like file://path/to/file.html"
+        return "Interactive console for scraping the given url"
-
+import scrapy
-        assert exists(join(proj_mod_path, 'spiders', '__init__.py'))
+        assert exists(join(self.proj_path, 'scrapy-ctl.py'))
-            'testspider.py'))
+        assert exists(join(self.proj_mod_path, 'spiders', 'testspider.py'))
-        self.assertEqual(exists(join(proj_mod_path, 'spiders', '__init__.py')), True)
+        assert exists(join(proj_path, 'scrapy-ctl.py'))
-            'spiders', 'testspider.py')), True)
+        assert os.path.exists(join(self.cwd, self.project_name, 'spiders', \
-                               env=env, **kwargs)
+            env=env, **kwargs)
-class ScrapyCtlStartprojectTest(ProjectTest):
+class StartprojectTest(ProjectTest):
-class ScrapyCtlCommandsTest(CommandTest):
+class MiscCommandsTest(CommandTest):
-        parser.add_option("-c", "--callback", dest="callback", action="store", help="use the provided callback for starting to crawl the given url")
+        parser.add_option("--nopipeline", dest="nopipeline", action="store_true", \
-                    urls = spider.start_urls if hasattr(spider.start_urls, '__iter__') else [spider.start_urls]
+                    urls = spider.start_urls if hasattr(spider.start_urls, \
-                        requests.extend(Request(url=url, callback=getattr(spider, opts.callback)) for url in urls)
+                        requests.extend(Request(url=url, callback=getattr(spider, \
-                        log.msg('Callback %s doesnt exist in spider %s' % (opts.callback, spider.domain_name), log.ERROR)
+                        log.msg('Callback %s doesnt exist in spider %s' % \
-                log.msg('Cannot create any requests with the provided arguments', log.ERROR)
+                log.msg('Cannot create any requests with the provided arguments', \
-import sys
+import sys
-import sys
+import sys
-from os.path import exists, join
+from os.path import exists, join
-from scrapy.conf import settings
+import unittest
-class ProjectTest(TestCase):
+class ProjectTest(unittest.TestCase):
-        env = self.env if hasattr(self, 'env') else os.environ
+        env = getattr(self, 'env', os.environ)
-            return
+            sys.exit(1)
-os.environ.setdefault('SCRAPYSETTINGS_MODULE', 'googledir.settings')
+os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'googledir.settings')
-            'scrapy_settings')
+        settings_module_path = os.environ.get('SCRAPY_SETTINGS_MODULE', \
-os.environ.setdefault('SCRAPYSETTINGS_MODULE', '${project_name}.settings')
+os.environ.setdefault('SCRAPY_SETTINGS_MODULE', '${project_name}.settings')
-    run. Those can be accessed through the ``domain_stats`` attribute"""
+from __future__ import with_statement
-    def __init__(self, update_vars):
+    def __init__(self, update_vars=None, nofetch=False):
-            self.vars['fetch'] = self.fetch
+            if not self.nofetch:
-        self.update_vars(self.vars)
+        if self.update_vars:
-        print "  fetch(req_or_url) : Fetch a new request or URL and update objects"
+        if not self.nofetch:
-            self.print_help()
+    def inspect_response(self, response):
-from scrapy.fetcher import get_or_create_spider
+from scrapy.shell import Shell
-        return "Interactive console for scraping the given url. For scraping local files you can use a URL like file://path/to/file.html"
+        return "Interactive console for scraping the given url. For scraping " \
-        """ You can use this function to update the Scrapy objects that will be available in the shell"""
+    def update_vars(self, vars):
-        scrapymanager.start()
+        url = args[0] if args else None
-import re, weakref
+import re
-from scrapy.http.response import Response
+from scrapy.http import Response, HtmlResponse
-        self.set_settings_module()
+        settings_module_path = os.environ.get('SCRAPYSETTINGS_MODULE', \
-                'scrapy_settings')
+    def set_settings_module(self, settings_module_path):
-SETTINGS_DISABLED = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)
+import_ = lambda x: __import__(x, {}, {}, [''])
-    global_defaults = None
+    default_settings_module = 'scrapy.conf.default_settings'
-        if not SETTINGS_DISABLED:
+        if not self.disabled:
-            self.settings_module = __import__(settings_module_path, {}, {}, [''])
+            self.settings_module = import_(settings_module_path)
-from scrapy.utils.misc import load_object
+from scrapy.utils.misc import load_object, arg_to_iter
-        return [result] if isinstance(result, (BaseItem, Request)) else result
+        return [result] if isinstance(result, BaseItem) else arg_to_iter(result)
-        commands.update(cmdline.custom_commands_dict())
+        commands = cmdline.get_commands_dict()
-            print "%s: %s" % (cmdname, cmd.short_desc())
+            title = "%s command" % cmdname
-            print help()
+            print "\n".join(textwrap.wrap(help()))
-                    cls.fields[model_field.name] = Field()
+                if model_field != cls._model_meta.pk:
-from scrapy.item import DictItem, Field
+from scrapy.item import Field, Item, ItemMeta
-class DjangoItemMeta(type):
+class DjangoItemMeta(ItemMeta):
-                if model_field != cls._model_meta.pk:
+                if model_field != cls._model_meta.pk and \
-class DjangoItem(DictItem):
+class DjangoItem(Item):
-class _ItemMeta(type):
+class ItemMeta(type):
-    __metaclass__ = _ItemMeta
+    __metaclass__ = ItemMeta
-        print '%r %r' % (name, serialized_value), self.encoding
+        print '%r %r' % (name, serialized_value), self.encoding
-import sys
+import sys
-                module_globals=func.func_globals)
+            warnings.warn(message, category=DeprecationWarning, stacklevel=2)
-class DjangoItemTest(TestCase):
+class DjangoItemTest(unittest.TestCase):
-    def setUp(
+    def setUp(self):
-    __metaclass__ = _ItemMeta
+class DictItem(DictMixin, BaseItem):
-    if module_name[0] not in string.letters:
+    if module_name[0] not in string.ascii_letters:
-        template_file = join(settings['TEMPLATES_DIR'], '%s.tmpl' % template)
+        template_file = join(settings['TEMPLATES_DIR'], 'spiders', '%s.tmpl' % template)
-        files = set()
+        files = set(listdir(SPIDER_TEMPLATES_PATH))
-        files.update(listdir(SPIDER_TEMPLATES_PATH))
+            files.update(listdir(join(settings['TEMPLATES_DIR'], 'spiders')))
-    main()
+#!/usr/bin/env python
-from scrapy.conf import settings, SETTINGS_MODULE
+from scrapy.conf import settings
-def usage(prog):
+def usage():
-    s += "%s <command> [options] [args]\n" % prog
+    s += "scrapy-ctl.py <command> [options] [args]\n"
-    s += "%s <command> -h\n" % prog
+    s += "scrapy-ctl.py <command> -h\n"
-    execute_with_args(sys.argv)
+def execute(argv=None):
-        print usage(argv[0])
+        print usage()
-        group.add_option("--set", dest="settings", action="append", \
+        group.add_option("--set", dest="set", action="append", \
-        for setting in opts.settings:
+        for setting in opts.set:
-SETTINGS_MODULE = os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings')
+        self.set_settings_module()
-            pass
+        self.global_defaults = __import__('scrapy.conf.default_settings', {}, {}, [''])
-        self.spider_modules = settings.getlist('SPIDER_MODULES')
+        self.spider_modules = None
-    def load(self):
+    def load(self, spider_modules=None):
-    def reload(self, skip_domains=None):
+    def reload(self, spider_modules=None, skip_domains=None):
-        self.load()  # second call to update spider instances
+        self.load(spider_modules=spider_modules)  # second call to update spider instances
-from scrapy.spider import spiders
+from scrapy.spider import BaseSpider
-        self.spider = spiders.fromdomain('scrapytest.org')
+        self.spider = BaseSpider()
-from scrapy.spider import spiders
+from scrapy.spider import BaseSpider
-        self.spider = spiders.fromdomain('scrapytest.org')
+        self.spider = BaseSpider()
-from scrapy.spider import spiders
+from scrapy.spider import BaseSpider
-        self.spider = spiders.fromdomain('scrapytest.org')
+        self.spider = BaseSpider()
-from scrapy.spider import spiders
+from scrapy.spider import BaseSpider
-        self.spider = spiders.fromdomain('scrapytest.org')
+        self.spider = BaseSpider()
-from scrapy.spider import spiders
+from scrapy.spider import BaseSpider
-        self.spider = spiders.fromdomain('scrapytest.org')
+        self.spider = BaseSpider()
-        spiders.reload()
+        spiders.load(['scrapy.tests.test_spiders'])
-    scripts = ['bin/scrapy-admin.py'],
+    scripts = ['bin/scrapy-ctl.py'],
-    def parse_item(self, response, selector):
+    def parse_node(self, response, selector):
-            ret = self.parse_item(response, selector)
+            ret = self.parse_node(response, selector)
-            raise NotConfigured('You must define parse_item method in order to scrape this XML feed')
+        if not hasattr(self, 'parse_node'):
-    __slots__ = ['_encoding', 'method', '_url', '_body', '_meta', '_cache', \
+    __slots__ = ['_encoding', 'method', '_url', '_body', '_meta', \
-        'flags', '_cache', '__weakref__']
+        'flags', '__weakref__']
-from scrapy.utils.python import memoizemethod
+from scrapy.utils.python import memoizemethod_noargs
-    @memoizemethod('cache')
+    @memoizemethod_noargs
-from scrapy.utils.python import memoizemethod
+from scrapy.utils.python import memoizemethod_noargs
-        content_type = headers.get('Content-Type')
+    @memoizemethod_noargs
-    @memoizemethod('cache')
+    @memoizemethod_noargs
-from scrapy.utils.python import memoizemethod
+from scrapy.utils.python import memoizemethod_noargs
-    @memoizemethod('cache')
+    @memoizemethod_noargs
-from scrapy.utils.python import str_to_unicode, unicode_to_str, memoizemethod, isbinarytext
+from scrapy.utils.python import str_to_unicode, unicode_to_str, \
-    def test_memoizemethod(self):
+    def test_memoizemethod_noargs(self):
-                return [arg1, arg2]
+            @memoizemethod_noargs
-        four = a.heavyfunc('two')
+        one = a.cached()
-        assert three is four
+import weakref
-
+def memoizemethod_noargs(method):
-    return MemoizeMethod
+    cache = weakref.WeakKeyDictionary()
-            }
+        }
-            repr(self.status), repr(self.body))
+        d = {
-        Request.__init__(self, *args, **kwargs)
+        super(FormRequest, self).__init__(*args, **kwargs)
-        Request.__init__(self, *args, **kwargs)
+        super(XmlRpcRequest, self).__init__(*args, **kwargs)
-            raise TypeError("Cannot assign a unicode body to a raw Response. Use TextResponse, HtmlResponse, etc")
+            raise TypeError("Cannot assign a unicode body to a raw Response. " \
-            raise TypeError("Response body must either str or unicode. Got: '%s'" % type(body).__name__)
+            raise TypeError("Response body must either str or unicode. Got: '%s'" \
-                (type(self).__name__, repr(self.url), repr(self.headers), repr(self.status), repr(self.body))
+            (type(self).__name__, repr(self.url), repr(self.headers), \
-    def replace(self, url=None, status=None, headers=None, body=None, meta=None, flags=None, cls=None, **kwargs):
+    def replace(self, url=None, status=None, headers=None, body=None, meta=None, \
-        return self._body_declared_encoding() or self._body_inferred_encoding()
+        return self._body_declared_encoding() or super(HtmlResponse, self).body_encoding()
-    __slots__ = ['_encoding']
+    __slots__ = ['_encoding', '_body_inferred_encoding']
-    def __init__(self, url, status=200, headers=None, body=None, meta=None, flags=None, encoding=None):
+    def __init__(self, url, status=200, headers=None, body=None, meta=None, \
-        Response.__init__(self, url, status, headers, body, meta, flags)
+        self._body_inferred_encoding = None
-        elif isinstance(body, unicode):
+    def _set_body(self, body):
-    body = property(lambda x: x._body, set_body)
+            super(TextResponse, self)._set_body(body)
-        possible_encodings = (self._encoding, self.headers_encoding(), self._body_declared_encoding())
+        possible_encodings = (self._encoding, self.headers_encoding(), \
-        # XXX: sometimes dammit.unicode fails, even when it recognizes the encoding correctly
+        self._body_inferred_encoding = dammit.originalEncoding
-        if 'body_inferred_encoding' not in self.cache:
+        if self._body_inferred_encoding is None:
-        return self.cache['body_inferred_encoding']
+        return self._body_inferred_encoding
-        return self._body_declared_encoding() or self._body_inferred_encoding()
+        return self._body_declared_encoding() or super(XmlResponse, self).body_encoding()
-    if 'base_url' not in response.cache:
+    if response not in _baseurl_cache:
-    return response.cache['base_url']
+        _baseurl_cache[response] = match.group(1) if match else response.url
-    if 'meta_refresh_url' not in response.cache:
+    if response not in _metaref_cache:
-    return response.cache['meta_refresh_url']
+        _metaref_cache[response] = match.groups() if match else (None, None)
-from scrapy.utils.request import request_fingerprint, request_authenticate, request_httprepr
+from scrapy.utils.request import request_fingerprint, _fingerprint_cache, \
-        self.assertEqual(request_fingerprint(r1), r1.cache['fingerprint'])
+        self.assertEqual(request_fingerprint(r1), _fingerprint_cache[r1][None])
-def request_fingerprint(request, include_headers=()):
+
-    except KeyError:
+        include_headers = tuple([h.lower() for h in sorted(include_headers)])
-        return fphash
+        if include_headers:
-class CookiesMiddlewareTest(TestCase):
+class UserAgentMiddlewareTest(TestCase):
-from scrapy.utils.misc import arg_to_iter, load_object
+from scrapy.utils.misc import load_object
-            return arg_to_iter(result)
+        return [result] if isinstance(result, (BaseItem, Request)) else result
-            log.msg("Spider must return Request, BaseItem or None, got '%s' in %s" % \
+            log.msg("Spider must return Request, BaseItem or None, got %r in %s" % \
-        return request.deferred.addCallback(arg_to_iter)
+        return request.deferred.addCallback(self._iterable_spider_output)
-        return [item]
+        return item
-    return scheme, host, port, path
+    return scheme, netloc, host, port, path
-            self.scheme, self.host, self.port, self.path = _parsed_url_args(parsedurl)
+            self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsedurl)
-            self.scheme, self.host, self.port, self.path = _parse(url)
+            self.scheme, self.netloc, self.host, self.port, self.path = _parse(url)
-        self.headers.setdefault('Host', self.host)
+        self.headers.setdefault('Host', self.netloc)
-        return (f.scheme, f.host, f.port, f.path)
+        return (f.scheme, f.netloc, f.host, f.port, f.path)
-                )
+    ("http://127.0.0.1?c=v&c2=v2#fragment",     ('http', lip, lip, 80, '/?c=v&c2=v2')),
-            self.assertEquals(self._parse(url), test, url)
+            self.assertEquals(client._parse(url), test, url)
-        scheme, host, port, path = self._parse(goodInput)
+        scheme, netloc, host, port, path = self._parse(goodInput)
-            client.getPage(self.getURL("host")).addCallback(self.assertEquals, "127.0.0.1"),
+            client.getPage(self.getURL("host")).addCallback(self.assertEquals, "127.0.0.1:%d" % self.portno),
-        d.addCallback(self.assertEquals, "127.0.0.1")
+        d.addCallback(self.assertEquals, "127.0.0.1:%d" % self.portno)
-        scheme, host, port, path = client._parse(url)
+        scheme, netloc, host, port, path = client._parse(url)
-    export_empty_fields = False
+    def __init__(self, **kwargs):
-        serializer = field.get('serializer', identity)
+        serializer = field.get('serializer', self._to_str_if_unicode)
-        self.xg = XMLGenerator(file)
+    def __init__(self, file, **kwargs):
-        self.csv_writer = csv.writer(*args, **kwargs)
+    def __init__(self, file, include_headers_line=False, **kwargs):
-                                   " use include_headers_line")
+                raise RuntimeError("You must set fields_to_export in order" + \
-        self.pickler = Pickler(*args, **kwargs)
+    def __init__(self, file, protocol=0, **kwargs):
-        super(PprintItemExporter, self).__init__()
+    def __init__(self, file, **kwargs):
-        super(JsonLinesItemExporter, self).__init__()
+    def __init__(self, file, **kwargs):
-        self.encoder = json.JSONEncoder(*args, **kwargs)
+        self.encoder = json.JSONEncoder(**kwargs)
-        self.i = TestItem(name=u'John', age='22')
+        self.i = TestItem(name=u'John\xa3', age='22')
-        return BaseItemExporter()
+    def _get_exporter(self, **kwargs):
-            self.i.fields['name'], 'name', self.i['name']), 'John')
+            self.i.fields['name'], 'name', self.i['name']), 'John\xc2\xa3')
-        ie = CustomItemExporter()
+    def test_fields_to_export(self):
-            ie.serialize_field(self.i.fields['age'], 'age', self.i['age']), '23')
+        ie = self._get_exporter(fields_to_export=['name'], encoding='latin-1')
-        i = CustomFieldItem(name=u'John', age='22')
+        i = CustomFieldItem(name=u'John\xa3', age='22')
-        ie.fields_to_export = ['name']
+        ie = self._get_exporter()
-        return PprintItemExporter(self.output)
+    def _get_exporter(self, **kwargs):
-        self.assertEqual(dict(self.i), eval(self.output.getvalue()))
+        self._assert_expected_item(eval(self.output.getvalue()))
-        return PickleItemExporter(self.output)
+
-        self.assertEqual(dict(self.i), pickle.loads(self.output.getvalue()))
+        self._assert_expected_item(pickle.loads(self.output.getvalue()))
-        return CsvItemExporter(self.output)
+    def _get_exporter(self, **kwargs):
-        self.assertEqual(self.output.getvalue(), '22,John\r\n')
+        self.assertEqual(self.output.getvalue(), '22,John\xc2\xa3\r\n')
-
+        ie = CsvItemExporter(output, include_headers_line=True)
-        ie.fields_to_export = self.i.fields.keys()
+        ie = CsvItemExporter(output, include_headers_line=True, \
-        self.assertEqual(output.getvalue(), 'age,name\r\n22,John\r\n')
+        self.assertEqual(output.getvalue(), 'age,name\r\n22,John\xc2\xa3\r\n')
-        return XmlItemExporter(self.output)
+    def _get_exporter(self, **kwargs):
-        expected_value = '<?xml version="1.0" encoding="iso-8859-1"?>\n<items><item><age>22</age><name>John</name></item></items>'
+        expected_value = '<?xml version="1.0" encoding="utf-8"?>\n<items><item><age>22</age><name>John\xc2\xa3</name></item></items>'
-    def _get_exporter(self):
+    def _get_exporter(self, **kwargs):
-                raise unittest.SkipTest("simplejson module not available") 
+                raise unittest.SkipTest("simplejson module not available")
-        return JsonLinesItemExporter(self.output)
+        return JsonLinesItemExporter(self.output, **kwargs)
-        self.assertEqual(self.output.getvalue(), '{"age": "22", "name": "John"}\n')
+        import simplejson
-        send_catch_log(test_signal, arg='test')
+        result = send_catch_log(test_signal, arg='test')
-        if isinstance(result, Exception):
+    results = sendRobust(*args, **kwargs)
-                " exception=%r" % (receiver, result), level=log.ERROR)
+                " exception=%r" % (receiver, response), level=log.ERROR)
-    handlers
+    """Same as dispatcher.robust.sendRobust but logs any exceptions raised by
-        log.exc("Exception catched on signal dispatch")
+    for receiver, result in sendRobust(*args, **kwargs):
-                domain=domain)
+        send_catch_log(stats_domain_closing, domain=domain, reason=reason)
-        self.close_domain(domain, reason)
+from scrapy.xlib.pydispatch import dispatcher
-master_doc = 'contents'
+master_doc = 'index'
-}
+#html_additional_pages = {}
-            ret = self.parse_item(response, xSel)
+        for selector in nodes:
-        return "Generate new spider based on template passed with --template"
+        return "Generate new spider based on template passed with -t or --template"
-        parser.add_option("--template", dest="template", default="crawl",
+        parser.add_option("--list", dest="list", action="store_true")
-            return False
+        if opts.list:
-            print "Use genspider --list to see all available templates."
+        if opts.dump:
-        self._genspider(module, domain, opts.template, template_file)
+
-            settings.overrides['LOGFILE'] = opts.logfile
+            settings.overrides['LOG_FILE'] = opts.logfile
-            settings.overrides['LOGLEVEL'] = opts.loglevel
+            settings.overrides['LOG_LEVEL'] = opts.loglevel
-LOGLEVEL='WARNING'
+LOG_LEVEL='WARNING'
-LOGFILE = None
+LOG_LEVEL = 'DEBUG'
-def start(logfile=None, loglevel=None, log_stdout=None):
+def start(logfile=None, loglevel=None, logstdout=None):
-    loglevel = loglevel or settings['LOGLEVEL']
+    loglevel = loglevel or settings['LOG_LEVEL'] or settings['LOGLEVEL']
-        log_stdout = log_stdout or settings.getbool('LOG_STDOUT')
+        logfile = logfile or settings['LOG_FILE'] or settings['LOGFILE']
-        log.startLogging(file, setStdout=log_stdout)
+        log.startLogging(file, setStdout=logstdout)
-        stats.set_value('envinfo/logfile', settings['LOGFILE'])
+        stats.set_value('envinfo/logfile', settings['LOG_FILE'])
-                    % (mtype, request, referer, errmsg)
+            msg = 'Image (http-error): Error downloading %s from %s referred in <%s>' \
-from scrapy.contrib.pipeline.images import BaseImagesPipeline, md5sum
+from scrapy.contrib.pipeline.images import BaseImagesPipeline
-    """Calculate the md5 checksum of a file
+def md5sum(file):
-        d = buffer.read(8096)
+        d = file.read(8096)
-SILENT, CRITICAL, ERROR, WARNING, INFO, DEBUG, TRACE = range(7)
+SILENT, CRITICAL, ERROR, WARNING, INFO, DEBUG = range(6)
-    6: "TRACE",
+from collections import defaultdict
-    reactor.connectTCP(ip, port or 80, factory)
+    reactor.connectTCP(url.hostname, port or 80, factory)
-    reactor.connectSSL(ip, port or 443, factory, contextFactory)
+    reactor.connectSSL(url.hostname, port or 443, factory, contextFactory)
-from scrapy.core.downloader.handlers import download_any
+from .middleware import DownloaderMiddlewareManager
-        signals.send_catch_log(signal=signals.response_downloaded, sender='download_http', response=r, spider=spider)
+        send_catch_log(signal=signals.request_uploaded, sender='download_http', \
-                    signals.send_catch_log(signal=signals.response_received, sender=self.__class__, response=response, spider=spider)
+                    send_catch_log(signal=signals.response_received, \
-            signals.send_catch_log(signal=signals.response_received, sender=self.__class__, response=response, spider=spider)
+            send_catch_log(signal=signals.response_received, sender=self.__class__, \
-from scrapy.core.scheduler import Scheduler
+from scrapy.utils.signal import send_catch_log
-            signals.send_catch_log(signal=signals.engine_started, sender=self.__class__)
+            send_catch_log(signal=signals.engine_started, sender=self.__class__)
-                signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, \
+                send_catch_log(signal=signals.domain_closed, sender=self.__class__, \
-            signals.send_catch_log(signal=signals.engine_stopped, sender=self.__class__)
+            send_catch_log(signal=signals.engine_stopped, sender=self.__class__)
-        signals.send_catch_log(signals.domain_open, sender=self.__class__, domain=domain, spider=spider)
+        send_catch_log(signals.domain_open, sender=self.__class__, \
-        signals.send_catch_log(signals.domain_opened, sender=self.__class__, domain=domain, spider=spider)
+        send_catch_log(signals.domain_opened, sender=self.__class__, \
-        for this domain.
+        """Called when a domain gets idle. This function is called when there
-            dispatcher.send(signal=signals.domain_idle, sender=self.__class__, domain=domain, spider=spider)
+            dispatcher.send(signal=signals.domain_idle, sender=self.__class__, \
-            log.exc("Exception catched on domain_idle signal dispatch")
+            log.err(_why="Exception catched on domain_idle signal dispatch")
-        signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, \
+        send_catch_log(signal=signals.domain_closed, sender=self.__class__, \
-            signals.send_catch_log(signal=signals.request_received, request=output, \
+            send_catch_log(signal=signals.request_received, request=output, \
-            signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, \
+            send_catch_log(signal=signals.item_scraped, sender=self.__class__, \
-                signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, \
+                send_catch_log(signal=signals.item_dropped, sender=self.__class__, \
-            signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, \
+            send_catch_log(signal=signals.item_passed, sender=self.__class__, \
-        log.exc("Exception catched on signal dispatch")
+from scrapy.utils.signal import send_catch_log
-        signals.send_catch_log(stats_domain_opened, domain=domain)
+        send_catch_log(stats_domain_opened, domain=domain)
-        signals.send_catch_log(stats_domain_closed, domain=domain, reason=reason, \
+        send_catch_log(stats_domain_closed, domain=domain, reason=reason, \
-        signals.send_catch_log(stats_domain_closing, domain=domain, reason=reason)
+        send_catch_log(stats_domain_closing, domain=domain, reason=reason)
-        return defer_fail(result)
+        return defer_failed(result)
-        return defer_fail(failure.Failure())
+        return defer_failed(failure.Failure())
-"""This directory contains third party modules used by Scrapy"""
+"""This package contains some third party modules that are distributed along
-    """same as twsited.internet.defer.fail, but delay calling errback """
+def defer_failed(_failure):
-    """same as twsited.internet.defer.succed, but delay calling callback"""
+    """Same as twsited.internet.defer.succed, but delay calling callback until
-    """same as twisted.internet.defer.maybeDeferred, but delay calling callback/errback"""
+    """Same as twisted.internet.defer.maybeDeferred, but delay calling
-    def serialize(self, field, name, value):
+    def serialize_field(self, field, name, value):
-        """Return the fields to export as a list of tuples (name, value)"""
+    def _get_serialized_fields(self, item, default_value=None, include_empty=None):
-        return [(k, item.get(k, default_value)) for k in field_iter]
+        for field_name in field_iter:
-            self._export_xml_field(item.fields[field], field, value)
+        for name, value in self._get_serialized_fields(item, default_value=''):
-    def _export_xml_field(self, field, name, value):
+    def _export_xml_field(self, name, serialized_value):
-            self.xg.characters(self.serialize(field, name, value))
+        self.xg.characters(serialized_value)
-        fields = self._get_fields_to_export(item, default_value='', \
+        fields = self._get_serialized_fields(item, default_value='', \
-        self.pickler.dump(dict(self._get_fields_to_export(item)))
+        self.pickler.dump(dict(self._get_serialized_fields(item)))
-        itemdict = dict(self._get_fields_to_export(item))
+        itemdict = dict(self._get_serialized_fields(item))
-            itemdict[field] = self.serialize(item.fields[field], field, value)
+        itemdict = dict(self._get_serialized_fields(item))
-from cPickle import Pickler
+import cPickle as pickle
-class BaseTest(unittest.TestCase):
+class BaseItemExporterTest(unittest.TestCase):
-
+        self.ie = self._get_exporter()
-        self.assertRaises(NotImplementedError, self.ie.export_item, self.i)
+    def _get_exporter(self):
-        self.assertEqual(self.ie.serialize( \
+    def _check_output(self):
-            self.ie.serialize(self.i.fields['age'], 'age', self.i['age']), '22')
+            self.ie.serialize_field(self.i.fields['age'], 'age', self.i['age']), '22')
-            def serialize(self, field, name, value):
+            def serialize_field(self, field, name, value):
-                    return super(CustomItemExporter, self).serialize(field, \
+                    return super(CustomItemExporter, self).serialize_field(field, \
-            ie.serialize(self.i.fields['name'], 'name', self.i['name']), 'John')
+            ie.serialize_field(self.i.fields['name'], 'name', self.i['name']), 'John')
-            ie.serialize(self.i.fields['age'], 'age', self.i['age']), '23')
+            ie.serialize_field(self.i.fields['age'], 'age', self.i['age']), '23')
-            self.ie.serialize(i.fields['name'], 'name', i['name']), 'John')
+            self.ie.serialize_field(i.fields['name'], 'name', i['name']), 'John')
-            self.ie.serialize(i.fields['age'], 'age', i['age']), '24')
+            self.ie.serialize_field(i.fields['age'], 'age', i['age']), '24')
-class PprintItemExporterTest(BaseTest):
+        self.assertEqual(list(ie._get_serialized_fields(self.i)), [('name', 'John')])
-        self.assertEqual(self.output.getvalue(), "{'age': '22', 'name': u'John'}\n")
+class PprintItemExporterTest(BaseItemExporterTest):
-        ie = PickleItemExporter(output)
+    def _check_output(self):
-        p.dump(dict(self.i))
+class PickleItemExporterTest(BaseItemExporterTest):
-        self.assertEqual(output.getvalue(), poutput.getvalue())
+    def _get_exporter(self):
-class CsvItemExporterTest(BaseTest):
+class CsvItemExporterTest(BaseItemExporterTest):
-        ie.finish_exporting()
+    def _get_exporter(self):
-        ie = CsvItemExporter(self.output)
+        output = StringIO()
-        self.assertEqual(self.output.getvalue(), 'age,name\r\n22,John\r\n')
+        self.assertEqual(output.getvalue(), 'age,name\r\n22,John\r\n')
-class XmlItemExporterTest(BaseTest):
+class XmlItemExporterTest(BaseItemExporterTest):
-        ie.finish_exporting()
+    def _get_exporter(self):
-class JsonLinesItemExporterTest(BaseTest):
+class JsonLinesItemExporterTest(BaseItemExporterTest):
-    def setUp(self):
+    def _get_exporter(self):
-    def test_export(self):
+        return JsonLinesItemExporter(self.output)
-
+    def _check_output(self):
-        itemdict = dict(self._get_fields_to_export(item))
+    def export_item(self, item):
-class JSONItemExporterTest(BaseTest):
+class JsonLinesItemExporterTest(BaseTest):
-        super(JSONItemExporterTest, self).__init__()
+        super(JsonLinesItemExporterTest, self).setUp()
-        ie = JsonLinesItemExporter(output)
+        ie = JsonLinesItemExporter(self.output)
-        self.assertEqual(output.getvalue(), '{"age": "22", "name": "John"}\n')
+        self.assertEqual(self.output.getvalue(), '{"age": "22", "name": "John"}\n')
-        dispatcher.connect(self.open_domain, signal=signals.domain_open)
+        dispatcher.connect(self.open_domain, signal=signals.domain_opened)
-        dispatcher.connect(self.domain_open, signals.domain_open)
+        dispatcher.connect(self.domain_opened, signals.domain_opened)
-    def domain_open(self, spider):
+    def domain_opened(self, spider):
-        dispatcher.connect(self.domain_open, signals.domain_open)
+        dispatcher.connect(self.domain_opened, signals.domain_opened)
-    def domain_open(self, domain):
+    def domain_opened(self, domain):
-        dispatcher.connect(self.domain_open, signal=signals.domain_open)
+        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)
-    def domain_open(self, domain, spider):
+    def domain_opened(self, domain, spider):
-        dispatcher.connect(self.domain_open, signal=signals.domain_open)
+        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)
-    def domain_open(self, domain, spider):
+    def domain_opened(self, domain, spider):
-        dispatcher.connect(self.open_domain, signal=signals.domain_open)
+        dispatcher.connect(self.open_domain, signal=signals.domain_opened)
-        dispatcher.connect(self.domain_open, signal=signals.domain_open)
+        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)
-    def domain_open(self, domain):
+    def domain_opened(self, domain):
-        signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason=reason)
+        signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, \
-domain_open = object()
+# XXX: deprecated signals (will be removed in Scrapy 0.8)
-from scrapy.xpath import HtmlXPathSelector
+from scrapy.selector import HtmlXPathSelector
-documentation in docs/ref/settings.rst
+documentation in docs/topics/settings.rst
-  (docs/ref/settings.rst)
+  (docs/topics/settings.rst)
-See documentation in docs/ref/extensions.rst
+See documentation in docs/topics/extensions.rst
-See documentation in docs/ref/extensions.rst
+See documentation in docs/topics/extensions.rst
-See documentation in docs/ref/downloader-middleware.rst
+See documentation in docs/topics/downloader-middleware.rst
-See documentation in docs/ref/downloader-middleware.rst
+See documentation in docs/topics/downloader-middleware.rst
-For more info see docs/ref/link-extractors.rst
+For more info see docs/topics/link-extractors.rst
-See documentation in docs/ref/extensions.rst
+See documentation in docs/topics/extensions.rst
-See documentation in docs/ref/extensions.rst
+See documentation in docs/topics/extensions.rst
-See documentation in docs/ref/scheduler-middleware.rst
+See documentation in docs/topics/scheduler-middleware.rst
-See documentation in docs/ref/spider-middleware.rst
+See documentation in docs/topics/spider-middleware.rst
-See documentation in docs/ref/spider-middleware.rst
+See documentation in docs/topics/spider-middleware.rst
-See documentation in docs/ref/spider-middleware.rst
+See documentation in docs/topics/spider-middleware.rst
-See documentation in docs/ref/spider-middleware.rst
+See documentation in docs/topics/spider-middleware.rst
-See documentation in docs/ref/spider-middleware.rst
+See documentation in docs/topics/spider-middleware.rst
-See documentation in docs/ref/spider-middleware.rst
+See documentation in docs/topics/spider-middleware.rst
-See documentation in docs/ref/spiders.rst
+See documentation in docs/topics/spiders.rst
-See documentation in docs/ref/spiders.rst
+See documentation in docs/topics/spiders.rst
-See documentation in docs/ref/extensions.rst
+See documentation in docs/topics/extensions.rst
-These exceptions are documented in docs/ref/exceptions.rst. Please don't add
+These exceptions are documented in docs/topics/exceptions.rst. Please don't add
-These signals are documented in docs/ref/signals.rst. Please don't add new
+These signals are documented in docs/topics/signals.rst. Please don't add new
-See documentation in docs/ref/request-response.rst
+See documentation in docs/topics/request-response.rst
-See documentation in docs/ref/request-response.rst
+See documentation in docs/topics/request-response.rst
-See documentation in docs/ref/request-response.rst
+See documentation in docs/topics/request-response.rst
-See documentation in docs/ref/request-response.rst
+See documentation in docs/topics/request-response.rst
-See documentation in docs/ref/request-response.rst
+See documentation in docs/topics/request-response.rst
-See documentation in docs/ref/request-response.rst
+See documentation in docs/topics/request-response.rst
-See documentation in docs/ref/request-response.rst
+See documentation in docs/topics/request-response.rst
-its documentation in: docs/ref/link-extractors.rst
+its documentation in: docs/topics/link-extractors.rst
-See documentation in docs/ref/logging.rst
+See documentation in docs/topics/logging.rst
-See documentation in docs/ref/email.rst
+See documentation in docs/topics/email.rst
-See documentation in docs/ref/selectors.rst
+See documentation in docs/topics/selectors.rst
-See documentation in docs/ref/spiders.rst
+See documentation in docs/topics/spiders.rst
-from scrapy.xpath import XmlXPathSelector, HtmlXPathSelector
+from scrapy.selector import XmlXPathSelector, HtmlXPathSelector
-from scrapy.xpath.selector import XPathSelectorList, HtmlXPathSelector
+from scrapy.selector import XPathSelectorList, HtmlXPathSelector
-from scrapy.xpath import HtmlXPathSelector
+from scrapy.selector import HtmlXPathSelector
-from scrapy.xpath import HtmlXPathSelector
+from scrapy.selector import HtmlXPathSelector
-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector
+from scrapy.selector import XmlXPathSelector, HtmlXPathSelector
-from scrapy.xpath.factories import xmlDoc_from_html
+from .factories import xmlDoc_from_html
-from scrapy.xpath import HtmlXPathSelector
+from scrapy.selector import HtmlXPathSelector
-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector, \
+from scrapy.selector import XmlXPathSelector, HtmlXPathSelector, \
-from scrapy.xpath.document import Libxml2Document
+from scrapy.selector.document import Libxml2Document
-from scrapy.xpath import XmlXPathSelector
+from scrapy.selector import XmlXPathSelector
-from scrapy.xpath.document import Libxml2Document
+from scrapy.selector.document import Libxml2Document
-documents using XPath. It requires libxml2 and its python bindings.
+from scrapy.selector import *
-from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector
+import warnings
-XPath selectors 
+from scrapy.selector import *
-    _get_libxml2_doc = staticmethod(xmlDoc_from_html)
+import warnings
-
+from scrapy.contrib.loader import XPathItemLoader
-        hxs = HtmlXPathSelector(response)
+        # The main selector we're using to extract data from the page
-        links = hxs.select('//td[descendant::a[contains(@href, "#pagerank")]]/following-sibling::td/font')
+        # The XPath to website links in the directory page
-        for link in links:
+        # Get a list of (sub) selectors to each website node pointed by the XPath
-            yield item
+            l = XPathItemLoader(item=item, selector=selector)
-            raise RuntimeError("%s must be instantiated with a selector" \
+            raise RuntimeError("%s must be instantiated with a selector " \
-# Define here the models for your scraped items
+from scrapy.item import Item, Field
-from scrapy.item import ScrapedItem
+class GoogledirItem(Item):
-class GoogledirItem(ScrapedItem):
+    name = Field()
-        return "Google Category: name=%s url=%s" % (self.name, self.url)
+        return "Google Category: name=%s url=%s" % (self['name'], self['url'])
-            if word in unicode(item.description).lower():
+            if word in unicode(item['description']).lower():
-            item.description = link.select('font[2]/text()').extract()
+            item['name'] = link.select('a/text()').extract()
-DEFAULT_ITEM_CLASS = 'scrapy.item.ScrapedItem'
+DEFAULT_ITEM_CLASS = 'scrapy.item.Item'
-from scrapy.newitem import Item
+from scrapy.item import Item
-from scrapy.newitem import Item, Field
+from scrapy.item import Item, Field
-from scrapy.newitem import Item, Field
+from scrapy.item import Item, Field
-                self.assertEqual('200', item.price)
+            self.assertEqual(item['url'], response.url)
-from scrapy.item import ScrapedItem
+from scrapy.item import Item, Field, ScrapedItem
-class ItemTestCase(unittest.TestCase):
+
-from scrapy.item import ScrapedItem
+from scrapy.item import Item, Field
-        item = ScrapedItem()
+        item = TestItem()
-        item.url = response.url
+            item['name'] = m.group(1)
-            item.price = m.group(1)
+            item['price'] = m.group(1)
-    def export(self, item):
+    def export_item(self, item):
-    def close(self):
+    def start_exporting(self):
-    def export(self, item):
+    def export_item(self, item):
-    def close(self):
+    def finish_exporting(self):
-                    "define fields_to_export attribute")
+                raise RuntimeError("You must set fields_to_export in order to" + \
-    def export(self, item):
+    def export_item(self, item):
-    def export(self, item):
+    def export_item(self, item):
-    def export(self, item):
+    def export_item(self, item):
-        self.file.write(self.encoder(itemdict) + '\n')
+        self.file.write(self.encoder.encode(itemdict) + '\n')
-        self.assertRaises(NotImplementedError, ie.export, i)
+class BaseTest(unittest.TestCase):
-        i = TestItem(name=u'John', age=22)
+        self.output = StringIO()
-        ie = BaseItemExporter()
+class BaseItemExporterTest(BaseTest):
-        self.assertEqual( ie._serialize_field(i.fields['age'], 'age', i['age']), '22')
+    def test_serialize(self):
-        i = TestItem(name=u'John', age=22)
+            def serialize(self, field, name, value):
-        self.assertEqual(ie._serialize_field(i.fields['age'], 'age', i['age']), '23')
+        self.assertEqual( \
-                return str(value + 2)
+        def custom_serializer(value):
-            age = CustomField()
+            age = Field(serializer=custom_serializer)
-        i = CustomFieldItem(name=u'John', age=22)
+        i = CustomFieldItem(name=u'John', age='22')
-        self.assertEqual(ie._serialize_field(i.fields['age'], 'age', i['age']), '24')
+        self.assertEqual(ie._get_fields_to_export(self.i), [('name', 'John')])
-class PprintItemExporterTest(unittest.TestCase):
+class PprintItemExporterTest(BaseTest):
-        ie.export(i)
+        ie = PprintItemExporter(self.output)
-        self.assertEqual(output.getvalue(), "{'age': 22, 'name': u'John'}\n")
+        self.assertEqual(self.output.getvalue(), "{'age': '22', 'name': u'John'}\n")
-class PickleItemExporterTest(unittest.TestCase):
+class PickleItemExporterTest(BaseTest):
-        ie.export(i)
+
-        p.dump(dict(i))
+        p.dump(dict(self.i))
-class CsvItemExporterTest(unittest.TestCase):
+class CsvItemExporterTest(BaseTest):
-        i = TestItem(name=u'John', age=22)
+        ie = CsvItemExporter(self.output)
-        ie.export(i)
+        self.assertEqual(self.output.getvalue(), '22,John\r\n')
-        self.assertEqual(output.getvalue(), 'age,name\r\n22,John\r\n')
+        ie.fields_to_export = self.i.fields.keys()
-class XmlItemExporterTest(unittest.TestCase):
+
-        i = TestItem(name=u'John', age=22)
+        ie = XmlItemExporter(self.output)
-        ie.export(i)
+        expected_value = '<?xml version="1.0" encoding="iso-8859-1"?>\n<items><item><age>22</age><name>John</name></item></items>'
-            '<?xml version="1.0" encoding="iso-8859-1"?>\n<items><item><age>22</age><name>John</name></item>')
+        self.assertEqual(self.output.getvalue(), expected_value)
-class JSONItemExporterTest(unittest.TestCase):
+class JSONItemExporterTest(BaseTest):
-        ie.export(i)
+        ie = JsonLinesItemExporter(output)
-        self.assertEqual(output.getvalue(), '{"age": 22, "name": "John"}\n')
+        self.assertEqual(output.getvalue(), '{"age": "22", "name": "John"}\n')
-"""
+        value = arg_to_iter(value)
-    def test_apply_concat_filter(self):
+    def test_iter_on_input_processor_input(self):
-    def test_map_concat_filter_multil(self):
+    def test_map_compose_filter_multil(self):
-    def test_empty_map_concat(self):
+    def test_empty_map_compose(self):
-        regex = r'^(|.*\.)(%s)$' % '|'.join(domains)
+        regex = r'^(.*\.)?(%s)$' % '|'.join(domains)
-            serializer = field.serializer
+    def serialize(self, field, name, value):
-            serializer = self._default_serializer
+            if include_empty:
-        return serializer(field, name, value)
+    def close(self):
-        return str(value)
+class XmlItemExporter(BaseItemExporter):
-class PprintItemExporter(BaseItemExporter):
+    item_element = 'item'
-        self.file = file
+        super(XmlItemExporter, self).__init__()
-class PickleItemExporter(BaseItemExporter):
+        self.xg.startElement(self.item_element, {})
-        self.pickler = Pickler(*args, **kwargs)
+    def close(self):
-        self.pickler.dump(dict(item))
+    def _export_xml_field(self, field, name, value):
-    fields_to_export = ()
+    include_headers_line = False
-                values.append('')
+        fields = self._get_fields_to_export(item, default_value='', \
-    fields_to_export = ()
+class PickleItemExporter(BaseItemExporter):
-        self.xg.startElement(self.root_element, {})
+    def __init__(self, *args, **kwargs):
-        self.xg.endElement(self.item_element)
+        self.pickler.dump(dict(self._get_fields_to_export(item)))
-        self.xg.endElement(name)
+class PprintItemExporter(BaseItemExporter):
-
+from scrapy.contrib.exporter import BaseItemExporter
-        self.assertEqual(output.getvalue(), '<?xml version="1.0" encoding="iso-8859-1"?>\n<items><item><age>22</age><name>John</name></item>')
+        self.assertEqual(output.getvalue(), \
-from scrapy.utils.ref import object_ref
+from scrapy.utils.trackref import object_ref
-from scrapy.utils.ref import object_ref
+from scrapy.utils.trackref import object_ref
-from scrapy.utils.ref import object_ref
+from scrapy.utils.trackref import object_ref
-        reprdict = dict(items for items in self.__dict__.iteritems() if not items[0].startswith('_'))
+        reprdict = dict(items for items in self.__dict__.iteritems() \
-from scrapy.utils.ref import print_live_refs
+from scrapy.utils.trackref import print_live_refs
-references to object instances, for certain classes"""
+"""This module provides some functions and classes to record and report
-from scrapy.utils.ref import object_ref
+from scrapy.utils.trackref import object_ref
-    __slots__ = 'url', 'text'
+    __slots__ = ['url', 'text']
-
+import weakref
-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector
+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector, \
-
+    __slots__ = ()
-    __slots__ = ['xmlDoc', 'xpathContext']
+    __slots__ = ['xmlDoc', 'xpathContext', '__weakref__']
-    __slots__ = ['doc', 'xmlNode', 'response', 'expr']
+    __slots__ = ['doc', 'xmlNode', 'expr', '__weakref__']
-                    response=self.response) for node in xpath_result])
+                return XPathSelectorList([self.__class__(node=node, parent=self, \
-                    expr=xpath, response=self.response)])
+                return XPathSelectorList([self.__class__(node=xpath_result, \
-
+    @deprecated(use_instead='XPathSelector.select')
-
+    @deprecated(use_instead='XPathSelectorList.select')
-class Request(object):
+class Request(object_ref):
-class Response(object):
+class Response(object_ref):
-class BaseItem(object):
+from scrapy.utils.ref import object_ref
-class XPathSelector(object):
+class XPathSelector(object_ref):
-from scrapy.utils.url import url_is_from_spider
+from scrapy.utils.httpobj import urlparse_cached
-            self.is_url_from_spider(x.url, spider))
+            self.should_follow(x, spider))
-        return url_is_from_spider(url, spider)
+    def domain_closed(self, spider):
-]
+EXTENSIONS = {}
-from scrapy.utils.middleware import build_middleware_list
+from scrapy.utils.conf import build_component_list
-                                       settings['DOWNLOADER_MIDDLEWARES'])
+        mwlist = build_component_list(settings['DOWNLOADER_MIDDLEWARES_BASE'], \
-from scrapy.utils.middleware import build_middleware_list
+from scrapy.utils.conf import build_component_list
-                                       settings['SCHEDULER_MIDDLEWARES'])
+        mwlist = build_component_list(settings['SCHEDULER_MIDDLEWARES_BASE'], \
-        for extension_path in settings.getlist('EXTENSIONS'):
+        extlist = build_component_list(settings['EXTENSIONS_BASE'], \
-from scrapy.utils.middleware import build_middleware_list
+from scrapy.utils.conf import build_component_list
-                                       settings['SPIDER_MIDDLEWARES'])
+        mwlist = build_component_list(settings['SPIDER_MIDDLEWARES_BASE'], \
-from scrapy.utils.middleware import build_middleware_list
+from scrapy.utils.conf import build_component_list
-class UtilsMiddlewareTestCase(unittest.TestCase):
+class UtilsConfTestCase(unittest.TestCase):
-    def test_build_middleware_list(self):
+    def test_build_component_list(self):
-        self.assertEqual(build_middleware_list(base, custom),
+        self.assertEqual(build_component_list(base, custom),
-        self.assertEqual(build_middleware_list(base, custom), custom)
+        self.assertEqual(build_component_list(base, custom), custom)
-from scrapy.utils.misc import render_templatefile, string_camelcase
+from scrapy.utils.template import render_templatefile, string_camelcase
-        assert vmsize > vmrss
+        self.assert_(isinstance(vmsize, int))
-    return float(v[1]) * _vmvalue_scale[v[2]]
+    return int(v[1]) * _vmvalue_scale[v[2]]
-                or (request.url.hostname and request.url.hostname.endswith('s3.amazonaws.com')):
+                or (hostname and hostname.endswith('s3.amazonaws.com')):
-from scrapy.http import Response, Headers
+from scrapy.http import Headers
-    return request.url.scheme in ['http', 'https']
+    return urlparse_cached(request).scheme in ['http', 'https']
-        self._parsers[response.url.netloc] = rp
+        self._parsers[urlparse_cached(response).netloc] = rp
-
+from scrapy.utils.httpobj import urlparse_cached
-from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory as HTTPClientFactory
+from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory
-    scheme = request.url.scheme
+    scheme = urlparse_cached(request).scheme
-    elif request.url.scheme == 'file':
+    elif scheme == 'file':
-        raise NotSupported("Unsupported URL scheme '%s' in: <%s>" % (request.url.scheme, request.url))
+        raise NotSupported("Unsupported URL scheme '%s' in: <%s>" % (scheme, request.url))
-                                timeout=timeout)
+    factory = ScrapyHTTPClientFactory.from_request(request, timeout)
-    port = request.url.port
+    url = urlparse_cached(request)
-    port = request.url.port
+    url = urlparse_cached(request)
-from urlparse import urlunparse
+from urlparse import urlparse, urlunparse
-        parsed = Url(url.strip()).parsedurl
+from scrapy.http import Headers
-
+def _parse(url):
-    def __init__(self, url, method='GET', body=None, headers=None, timeout=0):
+    def __init__(self, url, method='GET', body=None, headers=None, timeout=0, parsedurl=None):
-        self.scheme, self.host, self.port, self.path = _parse(url)
+        if parsedurl:
-Request, Response and Url outside this module.
+Request and Response outside this module.
-from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, Cookie
+from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy
-        return self.request.url.netloc
+        return urlparse_cached(self.request).netloc
-        return self.request.url.scheme
+        return urlparse_cached(self.request).scheme
-            self._url = url
+            self._url = safe_url_string(decoded_url, self.encoding)
-        self.url = Url(url)
+        self.url = url
-from cStringIO import StringIO
+from cStringIO import StringIO
-from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, Url, Response
+from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, Response
-        # url argument must be basestring or Url
+        # url argument must be basestring
-        r = Request(Url('http://www.example.com'))
+        r = Request('http://www.example.com')
-        assert isinstance(r.url, Url)
+        assert isinstance(r.url, str)
-        assert isinstance(r.url, Url)
+        assert isinstance(r.url, str)
-        urlargs = cgi.parse_qs(r1.url.query)
+        self.assertEqual(urlparse(r1.url).hostname, "www.example.com")
-from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers, Url
+from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers
-        assert isinstance(r.url, Url)
+        assert isinstance(r.url, str)
-
+import unittest
-from scrapy.http import Url, Headers
+from scrapy.http import Headers
-        f = client.ScrapyHTTPClientFactory(Url(url))
+        f = client.ScrapyHTTPClientFactory(url)
-
+    hostname = urlparse_cached(request).hostname
-    s += "Host: %s\r\n" % request.url.hostname
+    s += "Host: %s\r\n" % hostname
-from scrapy.spider import spiders
+from scrapy.core.exceptions import NotConfigured, IgnoreRequest
-        self._spiderdomains = {}
+        self._spider_netlocs = {}
-        if rp and not rp.can_fetch(agent, request.url):
+        useragent = self._useragents[spider]
-            robotsurl = "%s://%s/robots.txt" % parsedurl[0:2]
+    def robot_parser(self, url, spider):
-        del self._spiderdomains[domain]
+            dfd = scrapyengine.download(robotsreq, spider)
-from contextlib import closing
+"""Helper functions which doesn't fit anywhere else"""
-from scrapy.utils.python import flatten, unicode_to_str
+from scrapy.utils.python import flatten
-from scrapy.utils.misc import items_to_csv, load_object, arg_to_iter
+from scrapy.utils.misc import load_object, arg_to_iter
-from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_iter
+from scrapy.utils.misc import items_to_csv, load_object, arg_to_iter
-
+from functools import wraps
-    def wraps(func):
+    def wrapped(func):
-                message,
+            warnings.warn_explicit(message,
-            )
+                lineno=func.func_code.co_firstlineno + 1,
-
+    return wrapped
-from scrapy.utils.misc import render_templatefile, string_camelcase
+from scrapy.utils.template import render_templatefile, string_camelcase
-        return "[options] <spider_name> <spider_domain_name>"
+        return "[options] <spider_module_name> <spider_domain_name>"
-        template_file = join(settings['TEMPLATES_DIR'], 'spider_%s.tmpl' % opts.template)
+        template_file = join(settings['TEMPLATES_DIR'], 'spider_%s.tmpl' % \
-            print "Template '%s.tmpl' not found" % opts.template
+            print "Unable to create spider: template %r not found." % opts.template
-        name = self.normalize_name(args[0])
+        module = sanitize_module_name(args[0])
-        self._genspider(name, domain, template_file)
+        spider = spiders.fromdomain(domain)
-    def _genspider(self, name, domain, template_file):
+    def _genspider(self, module, domain, template_name, template_file):
-            'name': name,
+            'module': module,
-            'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('_')])
+            'classname': '%sSpider' % ''.join([s.capitalize() \
-        spider_file = '%s/%s.py' % (spiders_dir, name)
+        spider_file = "%s.py" % join(spiders_dir, module)
-
+        print "Created spider %r using template %r in module:" % (domain, \
-        if not d: break
+        if not d:
-        links = hxs.x('//td[descendant::a[contains(@href, "#pagerank")]]/following-sibling::td/font')
+        links = hxs.select('//td[descendant::a[contains(@href, "#pagerank")]]/following-sibling::td/font')
-            item.description = link.x('font[2]/text()').extract()
+            item.name = link.select('a/text()').extract()
-                _add_link(selector.x('@src'), selector.x('@alt') or selector.x('@title'))
+                _add_link(selector.select('@src'), selector.select('@alt') or \
-                children = selector.x('child::*')
+                children = selector.select('child::*')
-                    _add_link(selector.x('@href'), selector.x('@title'))
+                    _add_link(selector.select('@href'), selector.select('@title'))
-        base_url = xs.x('//base/@href').extract()
+        base_url = xs.select('//base/@href').extract()
-                selectors = xs.x(location)
+                selectors = xs.select(location)
-            html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.x(xpath_expr).extract()) for xpath_expr in self.restrict_xpaths)
+            html_slice = ''.join(''.join(html_fragm for html_fragm in hxs.select(xpath_expr).extract()) \
-        x = self.selector.x(xpath)
+        x = self.selector.select(xpath)
-            nodes = selector.x('//%s' % self.itertag)
+            nodes = selector.select('//%s' % self.itertag)
-            nodes = selector.x('//%s' % self.itertag)
+            nodes = selector.select('//%s' % self.itertag)
-            attrs.append((x.x("@id").extract(), x.x("name/text()").extract(), x.x("./type/text()").extract()))
+            attrs.append((x.select("@id").extract(), x.select("name/text()").extract(), x.select("./type/text()").extract()))
-        self.assertEqual([x.x("text()").extract() for x in xmliter(body, 'product')],
+        self.assertEqual([x.select("text()").extract() for x in xmliter(body, 'product')],
-        self.assertEqual(node.x('price/text()').extract(), [])
+        self.assertEqual(node.select('title/text()').extract(), ['Item 1'])
-        xl = xpath.x('//input')
+        xl = xpath.select('//input')
-                         [x.extract() for x in xpath.x('//input')])
+        self.assertEqual(xpath.select('//input').extract(),
-        self.assertEqual([x.extract() for x in xpath.x("//input[@name='a']/@name")],
+        self.assertEqual([x.extract() for x in xpath.select("//input[@name='a']/@name")],
-        self.assertEqual([x.extract() for x in xpath.x("number(concat(//input[@name='a']/@value, //input[@name='b']/@value))")],
+        self.assertEqual([x.extract() for x in xpath.select("number(concat(//input[@name='a']/@value, //input[@name='b']/@value))")],
-        self.assertEqual(xpath.x("concat('xpath', 'rules')").extract(),
+        self.assertEqual(xpath.select("concat('xpath', 'rules')").extract(),
-        self.assertEqual([x.extract() for x in xpath.x("concat(//input[@name='a']/@value, //input[@name='b']/@value)")],
+        self.assertEqual([x.extract() for x in xpath.select("concat(//input[@name='a']/@value, //input[@name='b']/@value)")],
-        assert isinstance(XmlXPathSelector(text=text).x("//p")[0],
+        assert isinstance(XmlXPathSelector(text=text).select("//p")[0],
-        assert isinstance(HtmlXPathSelector(text=text).x("//p")[0], 
+        assert isinstance(HtmlXPathSelector(text=text).select("//p")[0], 
-        self.assertEqual(XmlXPathSelector(text=text).x("//div").extract(),
+        self.assertEqual(XmlXPathSelector(text=text).select("//div").extract(),
-        self.assertEqual(HtmlXPathSelector(text=text).x("//div").extract(),
+        self.assertEqual(HtmlXPathSelector(text=text).select("//div").extract(),
-        self.assertEqual(divtwo.x("//li").extract(),
+        divtwo = x.select('//div[@class="two"]')
-        self.assertEqual(divtwo.x("./ul/li").extract(),
+        self.assertEqual(divtwo.select("./ul/li").extract(),
-        self.assertEqual(divtwo.x(".//li").extract(),
+        self.assertEqual(divtwo.select(".//li").extract(),
-        self.assertEqual(divtwo.x("./li").extract(),
+        self.assertEqual(divtwo.select("./li").extract(),
-        self.assertEqual(x.x("//ul/li").re(name_re),
+        self.assertEqual(x.select("//ul/li").re(name_re),
-        self.assertEqual(x.x("//ul/li").re("Age: (\d+)"),
+        self.assertEqual(x.select("//ul/li").re("Age: (\d+)"),
-        self.assertEqual(xxs.x('.').extract(),
+        self.assertEqual(xxs.select('.').extract(),
-        self.assertEqual(x.x("//somens:a").extract(), 
+        self.assertEqual(x.select("//somens:a").extract(), 
-        self.assertEqual(x.x("//p:SecondTestTag/xmlns:material").extract()[0], '<material/>')
+        self.assertEqual(len(x.select("//xmlns:TestTag")), 1)
-            x.x(xpath)
+            x.select(xpath)
-        self.assertEquals(x.x("//span[@id='blank']/text()").extract(),
+        self.assertEquals(x.select("//span[@id='blank']/text()").extract(),
-        self.assertEqual(xxs.x('/root/text()').extract_unquoted(), [
+        self.assertEqual(xxs.select('/root').extract_unquoted(), [u''])
-        self.assertEqual(xxs.x('//text()').extract_unquoted(), [
+        self.assertEqual(xxs.select('//*').extract_unquoted(), [u'', u'', u''])
-        yield XmlXPathSelector(text=nodetext).x('//' + nodename)[0]
+        yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]
-    def x(self, xpath):
+    def select(self, xpath):
-    __call__ = x
+
-        if self.x('self::text()'):
+        if self.select('self::text()'):
-    def x(self, xpath):
+    def select(self, xpath):
-        return XPathSelectorList(flatten([x.x(xpath) for x in self]))
+        return XPathSelectorList(flatten([x.select(xpath) for x in self]))
-                get_vmvalue_from_procfs()/1024/1024, "Mb"))
+    __slots__ = ['encoding']
-        self.set_body(body)
+        self._set_url(url)
-    def set_url(self, url):
+        self._meta = dict(meta) if meta else None
-    def set_body(self, body):
+    url = property(_get_url, _set_url)
-    body = property(lambda x: x._body, set_body)
+
-        self.cached = False
+        self._set_body(body)
-        self.cache = {}
+        self._meta = dict(meta) if meta else None
-    def set_body(self, body):
+    def _get_body(self):
-    body = property(lambda x: x._body, set_body)
+
-from scrapy.extension import extensions
+from scrapy.stats import stats
-            figures.append(("Memory usage at shutdown", int(memusage.virtual/1024/1024), "Mb"))
+        if stats.get_value('memusage/startup'):
-            s += "%-30s : %s %s\n" % f
+            s += "%-30s : %d %s\n" % f
-            self.mail.send(self.rcpts, "Scrapy Memory Debugger results at %s" % socket.gethostname(), report)
+            self.mail.send(self.rcpts, "Scrapy Memory Debugger results at %s" % \
-import sys
+from scrapy.utils.memory import get_vmvalue_from_procfs
-            raise NotConfigured("MemoryUsage extension is only available on Linux")
+        if not os.path.exists('/proc'):
-        return self._vmvalue('VmStk:')
+        return get_vmvalue_from_procfs('VmSize')
-        stats.set_value('memusage/startup', int(self.virtual))
+        stats.set_value('memusage/startup', self.virtual)
-        return float(v[1]) * self._scale[v[2]]
+        stats.max_value('memusage/max', self.virtual)
-        s += "Maximum memory usage           : %dM\r\n" % (self.data['max']/1024/1024)
+        s = "Memory usage at engine startup : %dM\r\n" % (stats.get_value('memusage/startup')/1024/1024)
-_unreserved_marks = "-_.!~*'()" # RFC 2396 sec 2.3
+_reserved = ';/?:@&=+$|,#' # RFC 3986 (Generic Syntax)
-    according to RFC-2396.
+    according to RFC-3986.
-        return (x for x in result if not isinstance(x, Request) or url_is_from_spider(x.url, spider))
+        return (x for x in result if not isinstance(x, Request) or \
-    """Return True if the url belongs to the given domain"""
+    """Return True if the url belongs to any of the given domains"""
-    return urlparse.urljoin(unicode_to_str(base, encoding), unicode_to_str(ref, encoding))
+    return urlparse.urljoin(unicode_to_str(base, encoding), \
-    queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], keep_blank_values=keep_blank_values)
+    queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], \
-    query = sep.join([kvsep.join(pair) for pair in querylist if pair[0] in parameterlist])
+    query = sep.join([kvsep.join(pair) for pair in querylist if pair[0] in \
-_unreserved_marks = "-_.!~*'()" #RFC 2396 sec 2.3
+_unreserved_marks = "-_.!~*'()" # RFC 2396 sec 2.3
-    encoded string into a legal URL.
+def safe_url_string(url, encoding='utf8'):
-    Illegal characters are escaped (RFC-3986)
+    If a unicode url is given, it is first converted to str using the given
-    It is safe to call this function multiple times.
+    Calling this function on an already "safe" url will return the url
-    encoding of that page.
+    Always returns a str.
-    s = unicode_to_str(url, use_encoding)
+    s = unicode_to_str(url, encoding)
-            return []
+        if 200 <= response.status < 300: # common case
-        self._next_request(domain)
+        while not self._needs_backout(domain):
-            self._domain_idle(domain)
+            return dwld.addErrback(log.err)
-Copyright (c) 2004-2007, Leonard Richardson
+Copyright (c) 2004-2008, Leonard Richardson
-__version__ = "3.0.6"
+__version__ = "3.0.7a"
-#This hack makes Beautiful Soup able to parse XML with namespaces
+#These hacks make Beautiful Soup able to parse XML with namespaces
-                 convertEntities=None, selfClosingTags=None):
+                 convertEntities=None, selfClosingTags=None, isHTML=False):
-            self._feed()
+            self._feed(isHTML=isHTML)
-    def _feed(self, inDocumentEncoding=None):
+    def _feed(self, inDocumentEncoding=None, isHTML=False):
-                      smartQuotesTo=self.smartQuotesTo)
+                      smartQuotesTo=self.smartQuotesTo, isHTML=isHTML)
-            if not currentData.translate(self.STRIP_ASCII_SPACES):
+            currentData = u''.join(self.currentData)
-    CHARSET_RE = re.compile("((^|;)\s*charset=)([^;]*)")
+    CHARSET_RE = re.compile("((^|;)\s*charset=)([^;]*)", re.M)
-                               "%SOUP-ENCODING%", contentType)
+                if (self.declaredHTMLEncoding is not None or
-                    # Go through it again with the new information.
+                    # Go through it again with the encoding information.
-                 smartQuotesTo='xml'):
+                 smartQuotesTo='xml', isHTML=False):
-                     self._detectEncoding(markup)
+                     self._detectEncoding(markup, isHTML)
-    def _detectEncoding(self, xml_data):
+    def _detectEncoding(self, xml_data, isHTML=False):
-        if xml_encoding_match:
+        xml_encoding_match = re.compile(
-    soup = BeautifulSoup(sys.stdin.read())
+    soup = BeautifulSoup(sys.stdin)
-                site.active_size, domain=spider.domain_name),
+        # FIXME: this can't be called here because the stats domain may be
-"""
+"""Functions for dealing with databases"""
-    pass
+mysql_uri_re = r"mysql:\/\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/:]+)(:(?P<port>\d+))?/(?P<db>.*)$"
-    m = re.search(r"mysql:\/\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/:]+)(:(?P<port>\d+))?/(?P<db>.*)$", db_uri)
+def parse_uri(mysql_uri):
-
+    """Connects to a MySQL DB given a mysql URI"""
-        return conn
+    if not d:
-from scrapy.contrib.loader.processor import Join, Identity, Compose, MapCompose
+from scrapy.contrib.loader.processor import Join, Identity, TakeFirst, \
-        self._values[field_name] += arg_to_iter(parsed_value)
+        processed_value = self._process_input_value(field_name, value)
-        self._values[field_name] = arg_to_iter(parsed_value)
+        processed_value = self._process_input_value(field_name, value)
-    def _parse_input_value(self, field_name, value):
+    def _process_input_value(self, field_name, value):
-from scrapy.newitem.exporters import BaseItemExporter
+from scrapy.contrib.exporter import BaseItemExporter
-        super(BaseItemExporter, self).__init__()
+        super(JSONItemExporter, self).__init__()
-import pprint
+
-from scrapy.newitem.exporters import *
+from scrapy.contrib.exporter import BaseItemExporter, PprintItemExporter, \
-            raise unittest.SkipTest("Json library not available") 
+            import json
-        i = TestItem(name=u'John', age=22)
+        from scrapy.contrib.exporter.jsonexporter import JSONItemExporter
-        self.ie.export(i)
+        output = StringIO()
-        self.assertEqual(self.output.getvalue(), '{"age": 22, "name": "John"}\n')
+        self.assertEqual(output.getvalue(), '{"age": 22, "name": "John"}\n')
-        item = ip.load_item()
+        il = ItemLoader(item=i)
-        item = ip.load_item()
+        il = TestItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'Marta', u'Pepe'])
+        il = TestItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'Pepe'])
+        il = TestItemLoader()
-    def test_map_concat_filter_multiple_functions(self):
+    def test_map_concat_filter_multil(self):
-        item = ip.load_item()
+        il = TestItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'mart'])
+        il = DefaultedItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'mart'])
+        il = InheritDefaultedItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'Marta'])
+        il = ChildItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'Marta'])
+        il = ChildChildItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'marta'])
+        il = IdentityDefaultedItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'marta'])
+        il = IdentityDefaultedItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'mARTA'])
+        il = ChildItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'MART'])
+        il = ChildDefaultedItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])
+        il = TestItemLoader()
-        self.assertEqual(ip.get_output_value('name'), u'Mar Ta')
+        il = TakeFirstItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])
+        il = TestItemLoader()
-        self.assertEqual(ip.get_output_value('name'), u'Mar Ta')
+        il = TakeFirstItemLoader()
-        self.assertEqual(ip.get_output_value('name'), u'Mar<br>Ta')
+        il = TakeFirstItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])
+        il = TestItemLoader()
-        self.assertEqual(ip.get_output_value('name'), [u'Mar', u'Ta'])
+        il = LalaItemLoader()
-        self.assertEqual(ip.get_output_value('url'), ['val'])
+        il = ChildItemLoader()
-        self.assertEqual(ip.get_output_value('url'), ['val'])
+        il = ChildItemLoader(key=u'val')
-        self.assertEqual(ip.get_output_value('url'), ['val'])
+        il = ChildItemLoader()
-        self.assertEqual(ip.get_output_value('url'), ['marta'])
+        il = ChildItemLoader(item=it)
-        self.assertRaises(KeyError, ip.add_value, 'wrong_field', [u'lala', u'lolo'])
+        il = TestItemLoader()
-from scrapy.item.models import BaseItem
+from scrapy.item import BaseItem
-from scrapy.item.models import BaseItem
+from scrapy.item import BaseItem
-from scrapy.item.models import BaseItem
+from scrapy.item import BaseItem
-from scrapy.item.models import BaseItem
+from scrapy.item import BaseItem
-from scrapy.item.models import BaseItem
+from scrapy.item import BaseItem
-from scrapy.item.models import BaseItem
+from scrapy.item import BaseItem
-class ApplyConcat(object):
+class MapCompose(object):
-        return list(values)
+        return values
-from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity, Compose
+from scrapy.contrib.loader.processor import Join, Identity, Compose, MapCompose
-    name_in = ApplyConcat(lambda v: v.title())
+    name_in = MapCompose(lambda v: v.title())
-    default_input_processor = ApplyConcat(lambda v: v[:-1])
+    default_input_processor = MapCompose(lambda v: v[:-1])
-        proc = ApplyConcat(filter_world, str.upper)
+        proc = MapCompose(filter_world, str.upper)
-            name_in = ApplyConcat(lambda v: v.title(), lambda v: v[:-1])
+            name_in = MapCompose(lambda v: v.title(), lambda v: v[:-1])
-            url_in = ApplyConcat(lambda v: v.lower())
+            url_in = MapCompose(lambda v: v.lower())
-            summary_in = ApplyConcat(lambda v: v)
+            url_in = MapCompose(lambda v: v.upper())
-            name_in = ApplyConcat()
+            name_in = MapCompose()
-            name_in = ApplyConcat(TestItemLoader.name_in, unicode.swapcase)
+            name_in = MapCompose(TestItemLoader.name_in, unicode.swapcase)
-            name_in = ApplyConcat(DefaultedItemLoader.default_input_processor, unicode.swapcase)
+            name_in = MapCompose(DefaultedItemLoader.default_input_processor, unicode.swapcase)
-            url_in = ApplyConcat(processor_with_args, key=u'val')
+            url_in = MapCompose(processor_with_args, key=u'val')
-            url_in = ApplyConcat(processor_with_args)
+            url_in = MapCompose(processor_with_args)
-            url_in = ApplyConcat(processor_with_args)
+            url_in = MapCompose(processor_with_args)
-            url_in = ApplyConcat(processor)
+            url_in = MapCompose(processor)
-    name_in = ApplyConcat(lambda v: v.title())
+    name_in = MapCompose(lambda v: v.title())
-class Pipe(object):
+class Compose(object):
-    def __init__(self, *functions, **default_parser_context):
+    def __init__(self, *functions, **default_loader_context):
-        self.default_parser_context = default_parser_context
+        self.default_loader_context = default_loader_context
-            context = MergeDict(parser_context, self.default_parser_context)
+    def __call__(self, value, loader_context=None):
-            context = self.default_parser_context
+            context = self.default_loader_context
-from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity, Pipe
+from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity, Compose
-    def test_pipe_pprocwssor(self):
+    def test_compose_processor(self):
-            name_out = Pipe(lambda v: v[0], lambda v: v.title(), lambda v: v[:-1])
+            name_out = Compose(lambda v: v[0], lambda v: v.title(), lambda v: v[:-1])
-    def populate_item(self):
+    def load_item(self):
-    def test_populate_item_using_default_loader(self):
+    def test_load_item_using_default_loader(self):
-        item = ip.populate_item()
+        item = ip.load_item()
-    def test_populate_item_using_custom_loader(self):
+    def test_load_item_using_custom_loader(self):
-        item = ip.populate_item()
+        item = ip.load_item()
-        item = ip.populate_item()
+        item = ip.load_item()
-        item = il.populate_item()
+        item = il.load_item()
-from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity
+from scrapy.contrib.loader.processor import ApplyConcat, Join, Identity, Pipe
-META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\d+);\s*url=([^"\']+)', re.IGNORECASE)
+META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\d+);\s*url=([^"\']+)', re.DOTALL | re.IGNORECASE)
-Item Parser
+Item Loader
-See documentation in docs/topics/itemparser.rst
+See documentation in docs/topics/loaders.rst
-from .parsers import Identity
+from .common import wrap_loader_context
-class ItemParser(object):
+class ItemLoader(object):
-    default_output_parser = Identity()
+    default_input_processor = Identity()
-        return parser(self._values[field_name])
+        proc = self.get_output_processor(field_name)
-        return parser
+    def get_input_processor(self, field_name):
-        return parser
+    def get_output_processor(self, field_name):
-        return parser(value)
+        proc = self.get_input_processor(field_name)
-class XPathItemParser(ItemParser):
+class XPathItemLoader(ItemLoader):
-        super(XPathItemParser, self).__init__(item, **context)
+        super(XPathItemLoader, self).__init__(item, **context)
-This module provides some commonly used parser functions for Item Parsers.
+This module provides some commonly used processors for Item Loaders.
-See documentation in docs/topics/itemparser.rst
+See documentation in docs/topics/loaders.rst
-from .common import wrap_parser_context
+from .common import wrap_loader_context
-    def __init__(self, *functions, **default_parser_context):
+    def __init__(self, *functions, **default_loader_context):
-        self.default_parser_context = default_parser_context
+        self.default_loader_context = default_loader_context
-    def __call__(self, value, parser_context=None):
+    def __call__(self, value, loader_context=None):
-            context = MergeDict(parser_context, self.default_parser_context)
+        if loader_context:
-        wrapped_funcs = [wrap_parser_context(f, context) for f in self.functions]
+            context = self.default_loader_context
-from scrapy.contrib.itemparser.parsers import ApplyConcat, Join, Identity
+from scrapy.contrib.loader import ItemLoader, XPathItemLoader
-# test item parsers
+# test item loaders
-class NameItemParser(ItemParser):
+class NameItemLoader(ItemLoader):
-class TestItemParser(NameItemParser):
+class TestItemLoader(NameItemLoader):
-    default_input_parser = ApplyConcat(lambda v: v[:-1])
+class DefaultedItemLoader(NameItemLoader):
-# test parsers
+# test processors
-        return parser_context['key']
+def processor_with_args(value, other=None, loader_context=None):
-class ItemParserTest(unittest.TestCase):
+class ItemLoaderTest(unittest.TestCase):
-        ip = ItemParser(item=i)
+        ip = ItemLoader(item=i)
-        ip = TestItemParser()
+        ip = TestItemLoader()
-        ip = TestItemParser()
+        ip = TestItemLoader()
-        ip = TestItemParser()
+        ip = TestItemLoader()
-    def test_map_concat_filter(self):
+    def test_apply_concat_filter(self):
-        self.assertEqual(parser(['hello', 'world', 'this', 'is', 'scrapy']),
+        proc = ApplyConcat(filter_world, str.upper)
-        class TestItemParser(NameItemParser):
+        class TestItemLoader(NameItemLoader):
-        ip = TestItemParser()
+        ip = TestItemLoader()
-        ip = DefaultedItemParser()
+    def test_default_input_processor(self):
-        class InheritDefaultedItemParser(DefaultedItemParser):
+    def test_inherited_default_input_processor(self):
-        ip = InheritDefaultedItemParser()
+        ip = InheritDefaultedItemLoader()
-        class ChildItemParser(TestItemParser):
+    def test_input_processor_inheritance(self):
-        ip = ChildItemParser()
+        ip = ChildItemLoader()
-        class ChildChildItemParser(ChildItemParser):
+        class ChildChildItemLoader(ChildItemLoader):
-        ip = ChildChildItemParser()
+        ip = ChildChildItemLoader()
-        class IdentityDefaultedItemParser(DefaultedItemParser):
+        class IdentityDefaultedItemLoader(DefaultedItemLoader):
-        ip = IdentityDefaultedItemParser()
+        ip = IdentityDefaultedItemLoader()
-        class IdentityDefaultedItemParser(DefaultedItemParser):
+    def test_identity_input_processor(self):
-        ip = IdentityDefaultedItemParser()
+        ip = IdentityDefaultedItemLoader()
-            name_in = ApplyConcat(TestItemParser.name_in, unicode.swapcase)
+    def test_extend_custom_input_processors(self):
-        ip = ChildItemParser()
+        ip = ChildItemLoader()
-            name_in = ApplyConcat(DefaultedItemParser.default_input_parser, unicode.swapcase)
+    def test_extend_default_input_processors(self):
-        ip = ChildDefaultedItemParser()
+        ip = ChildDefaultedItemLoader()
-        ip = TestItemParser()
+    def test_output_processor_using_function(self):
-        class TakeFirstItemParser(TestItemParser):
+        class TakeFirstItemLoader(TestItemLoader):
-        ip = TakeFirstItemParser()
+        ip = TakeFirstItemLoader()
-        ip = TestItemParser()
+    def test_output_processor_using_classes(self):
-        class TakeFirstItemParser(TestItemParser):
+        class TakeFirstItemLoader(TestItemLoader):
-        ip = TakeFirstItemParser()
+        ip = TakeFirstItemLoader()
-        class TakeFirstItemParser(TestItemParser):
+        class TakeFirstItemLoader(TestItemLoader):
-        ip = TakeFirstItemParser()
+        ip = TakeFirstItemLoader()
-        ip = TestItemParser()
+    def test_default_output_processor(self):
-            default_output_parser = Identity()
+        class LalaItemLoader(TestItemLoader):
-        ip = LalaItemParser()
+        ip = LalaItemLoader()
-            url_in = ApplyConcat(parser_with_args, key=u'val')
+    def test_loader_context_on_declaration(self):
-        ip = ChildItemParser()
+        ip = ChildItemLoader()
-            url_in = ApplyConcat(parser_with_args)
+    def test_loader_context_on_instantiation(self):
-        ip = ChildItemParser(key=u'val')
+        ip = ChildItemLoader(key=u'val')
-            url_in = ApplyConcat(parser_with_args)
+    def test_loader_context_on_assign(self):
-        ip = ChildItemParser()
+        ip = ChildItemLoader()
-            return parser_context['item']['name']
+    def test_item_passed_to_input_processor_functions(self):
-            url_in = ApplyConcat(parser)
+        class ChildItemLoader(TestItemLoader):
-        ip = ChildItemParser(item=it)
+        ip = ChildItemLoader(item=it)
-        ip = TestItemParser()
+        ip = TestItemLoader()
-class TestXPathItemParser(XPathItemParser):
+class TestXPathItemLoader(XPathItemLoader):
-class XPathItemParserTest(unittest.TestCase):
+class XPathItemLoaderTest(unittest.TestCase):
-        self.assertRaises(RuntimeError, XPathItemParser)
+        self.assertRaises(RuntimeError, XPathItemLoader)
-        l = TestXPathItemParser(selector=sel)
+        l = TestXPathItemLoader(selector=sel)
-        l = TestXPathItemParser(response=response)
+        l = TestXPathItemLoader(response=response)
-        l = TestXPathItemParser(response=response)
+        l = TestXPathItemLoader(response=response)
-
+import unittest
-            serializer = getattr('serialize_%s' % name)
+            serializer = getattr(self, 'serialize_%s' % name)
-            serializer = _default_serializer(field, name, value)
+            serializer = self._default_serializer
-    def _default_serializer(field, name, value):
+    def _default_serializer(self, field, name, value):
-                    item[field])
+                values.append(self._serialize_field(item.fields[field], field,
-                self._export_xml_field(field._field, 'value', v)
+from cPickle import Pickler
-from scrapy.conf import settings
+import unittest 
-        self.assertEqual(stats, {})
+class StatsCollectorTest(unittest.TestCase):
-        self.assertEqual(stats.getpath('one/other/three'), 25)
+        self.assertEqual(stats.get_stats(), {})
-        """
+    def test_dummy_collector(self):
-    main()
+    unittest.main()
-         }
+class BaseItemExporter(object):
-        return str(value)
+        if hasattr(self, 'serialize_%s' % name):
-        return str(value)
+        return serializer(field, name, value)
-    def _serialize_float_field(self, field, name, value):
+    def _default_serializer(field, name, value):
-
+
-                site.active_size, 0, domain=spider.domain_name),
+                site.active_size, domain=spider.domain_name),
-                    self.sites[domain].itemproc_size, 0, domain=domain)
+                    self.sites[domain].itemproc_size, domain=domain)
-    def max_value(self, key, value, default, domain=None):
+    def max_value(self, key, value, domain=None):
-        d[key] = max(d.setdefault(key, default), value)
+        d[key] = max(d.setdefault(key, value), value)
-    def min_value(self, key, value, default, domain=None):
+    def min_value(self, key, value, domain=None):
-        d[key] = min(d.setdefault(key, default), value)
+        d[key] = min(d.setdefault(key, value), value)
-            raise unittest.SkipTest(e)
+            import json
-import simplejson
+try:
-class ScrapyJSONEncoder(simplejson.JSONEncoder):
+
-    'json': lambda obj: simplejson.dumps(obj, cls=ScrapyJSONEncoder),
+    'json': lambda obj: json.dumps(obj, cls=ScrapyJSONEncoder),
-    'json': lambda text: simplejson.loads(text),
+    'json': lambda text: json.loads(text),
-        self.assertEqual(req, None)
+        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)
-        self.assertEqual(req, None)
+        self.assertRaises(IgnoreRequest, self.mw.process_response, req, rsp, self.spider)
-
+"""
-from scrapy.item.adaptors import AdaptorPipe
+from scrapy.contrib.item.adaptors import AdaptorPipe
-from scrapy.item.adaptors import adaptize
+from scrapy.contrib.item.adaptors import adaptize
-from scrapy.item.adaptors import AdaptorPipe
+from scrapy.contrib.item.adaptors import AdaptorPipe
-from scrapy.item.adaptors import AdaptorPipe
+from scrapy.contrib.item.adaptors import AdaptorPipe
-        assert soup1 is not soup3
+
-from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh, response_httprepr
+from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh, \
-import re
+import re, weakref
-    unittest.main()
+from scrapy.xpath.document import Libxml2Document
-class XPathTestCase(unittest.TestCase):
+class XPathSelectorTestCase(unittest.TestCase):
-
+from scrapy.xpath.document import Libxml2Document
-(xmlDoc) for proper garbage collection.
+This module contains a simple class (Libxml2Document) which provides cache and
-        self.xpathContext = self.xmlDoc.xpathNewContext()
+    cache = weakref.WeakKeyDictionary()
-from scrapy.xpath.extension import Libxml2Document
+from scrapy.xpath.document import Libxml2Document
-                self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)
+            self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)
-        self._selector = selector
+        self.selector = selector
-        self.add_value(field_name, self._selector.x(xpath).extract(), \
+    def add_xpath(self, field_name, xpath, re=None, **new_loader_args):
-        self.replace_value(field_name, self._selector.x(xpath).extract(), \
+    def replace_xpath(self, field_name, xpath, re=None, **new_loader_args):
-The scrapy.xpath module provides useful classes for parsing HTML and XML
+The scrapy.xpath module provides useful classes for selecting HTML and XML
-    def __call__(self, value, loader_args):
+    def __call__(self, value, loader_args=None):
-        referer = request.headers.get('Referer', None)
+        referer = request.headers.get('Referer')
-                log.msg("Crawled %s from <%s>" % (response, referer), level=log.DEBUG, \
+                log.msg("Crawled %s (referer: <%s>)" % (response, referer), level=log.DEBUG, \
-            log.msg("Downloading <%s> from <%s>: %s" % (request.url, referer, errmsg), log.ERROR, domain=domain)
+            errmsg = str(_failure) if not isinstance(ex, IgnoreRequest) \
-                log.msg("Crawled %s from <%s>" % (response, referer), level=log.DEBUG, domain=domain)
+                log.msg("Crawled %s from <%s>" % (response, referer), level=log.DEBUG, \
-        msg = "SPIDER BUG processing <%s> from <%s>: %s" % (request.url, referer, _failure)
+        msg = "Spider exception caught while processing <%s> (referer: <%s>): %s" % \
-    if response.encoding in ('utf-8', 'utf8'):
+    if response.encoding in utf8_encodings:
-        lxdoc = libxml2.htmlReadDoc(utf8body, response.url, 'utf-8', html_parser_options)
+        lxdoc = libxml2.htmlReadDoc(utf8body, response.url, 'utf-8', \
-        lxdoc = libxml2.htmlReadDoc(utf8body.replace("\x00", ""), response.url, 'utf-8', html_parser_options)
+        lxdoc = libxml2.htmlReadDoc(utf8body.replace("\x00", ""), response.url, \
-        lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', xml_parser_options)
+        lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', \
-        lxdoc = libxml2.readDoc(utf8body.replace("\x00", ""), response.url, 'utf-8', xml_parser_options)
+        lxdoc = libxml2.readDoc(utf8body.replace("\x00", ""), response.url, \
-                self.doc = response.getlibxml2doc(factory=self.xmlDoc_factory)
+                self.doc = response.getlibxml2doc(factory=self._get_libxml2_doc)
-                self.doc = Libxml2Document(response, factory=self.xmlDoc_factory)
+                self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)
-            self.doc = Libxml2Document(response, factory=self.xmlDoc_factory)
+            self.doc = Libxml2Document(response, factory=self._get_libxml2_doc)
-        return "<%s (%s) xpath=%s>" % (type(self).__name__, getattr(self.xmlNode, 'name', type(self.xmlNode).__name__), self.expr)
+        return "<%s (%s) xpath=%s>" % (type(self).__name__, getattr(self.xmlNode, \
-    xmlDoc_factory = staticmethod(xmlDoc_from_xml)
+    _get_libxml2_doc = staticmethod(xmlDoc_from_xml)
-    xmlDoc_factory = staticmethod(xmlDoc_from_html)
+    _get_libxml2_doc = staticmethod(xmlDoc_from_html)
-        if not item:
+        if item is None:
-from scrapy.newitem.loader import Loader
+from scrapy.newitem.loader import Loader, XPathLoader
-from scrapy.xpath.constructors import xmlDoc_from_html
+from scrapy.xpath.factories import xmlDoc_from_html
-        self.xmlDoc = constructor(response)
+    def __init__(self, response, factory=xmlDoc_from_html):
-from scrapy.xpath.constructors import xmlDoc_from_html
+from scrapy.xpath.factories import xmlDoc_from_html
-    cachekey = 'lx2doc_%s' % constructor.__name__
+def getlibxml2doc(response, factory=xmlDoc_from_html):
-        lx2doc = Libxml2Document(response, constructor=constructor)
+        lx2doc = Libxml2Document(response, factory=factory)
-from scrapy.xpath.constructors import xmlDoc_from_html, xmlDoc_from_xml
+from scrapy.xpath.factories import xmlDoc_from_html, xmlDoc_from_xml
-    def __init__(self, response=None, text=None, node=None, parent=None, expr=None, constructor=xmlDoc_from_html):
+    xmlDoc_factory = staticmethod(xmlDoc_from_html)
-                self.doc = response.getlibxml2doc(constructor=constructor)  # try with cached version first
+                # try with cached version first
-                self.doc = Libxml2Document(response, constructor=constructor)
+                self.doc = Libxml2Document(response, factory=self.xmlDoc_factory)
-            self.doc = Libxml2Document(response, constructor=constructor)
+            response = TextResponse(url=None, body=unicode_to_str(text), \
-                                          for node in xpath_result])
+                return XPathSelectorList([cls(node=node, parent=self, expr=xpath, \
-                return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath, response=self.response)])
+                return XPathSelectorList([cls(node=xpath_result, parent=self, \
-        XPathSelector.__init__(self, *args, **kwargs)
+    xmlDoc_factory = staticmethod(xmlDoc_from_xml)
-        XPathSelector.__init__(self, *args, **kwargs)
+    xmlDoc_factory = staticmethod(xmlDoc_from_html)
-ITEM_PROCESSOR = 'scrapy.item.pipeline.ItemPipelineManager'
+ITEM_PROCESSOR = 'scrapy.contrib.pipeline.ItemPipelineManager'
-        return deferred
+"""
-            cls = load_object(stage)
+        self.enabled.clear()
-        log.msg("Enabled item pipelines: %s" % ", ".join([type(p).__name__ for p in self.pipeline]),
+                    pipe = cls()
-LOG_ENABLED = False
+# This dict holds information about the executed command for later use
-        return [f[:-3] for f in os.listdir(dir) if not f.startswith('_') and f.endswith('.py')]
+        return [f[:-3] for f in os.listdir(dir) if not f.startswith('_') and \
-def builtin_commands_dict():
+def get_commands_from_module(module):
-        modname = 'scrapy.command.commands.%s' % cmdname
+    mod = __import__(module, {}, {}, [''])
-            print 'WARNING: Builtin command module %s exists but Command class not found' % modname
+            print 'WARNING: Module %r does not define a Command class' % modname
-    return d
+def get_commands_dict():
-def getcmdname(argv):
+def get_command_name(argv):
-
+    cmds = get_commands_dict()
-def command_settings(cmdname):
+def update_default_settings(module, cmdname):
-        update_defaults(settings.defaults, module)
+        mod = __import__('%s.%s' % (module, cmdname), {}, {}, [''])
-command_executed = {}
+        return
-        print "Error: Cannot find %r module in python path." % SETTINGS_MODULE
+        print "Error: Cannot find %r module in python path" % SETTINGS_MODULE
-    cmds.update(custom_commands_dict())
+    cmds = get_commands_dict()
-    command_settings(cmdname)
+    cmdname = get_command_name(argv)
-        parser.print_help()
+    return ret
-    s += "Available commands:\n\n"
+def usage(prog):
-        print usage(argv)
+        print usage(argv[0])
-    parser = optparse.OptionParser(conflict_handler='resolve')
+    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \
-        log.msg("Profiling enabled. Analyze later with: python -m pstats %s" % opts.profile)
+    if opts.profile or opts.lsprof:
-            with open(fn, 'w') as f:
+        if opts.profile:
-            pass
+from optparse import OptionGroup
-        A long description of the command. Return short description when not
+        """A long description of the command. Return short description when not
-        An extensive help for the command. It will be shown when using the
+        """An extensive help for the command. It will be shown when using the
-        parser.add_option("--logfile", dest="logfile", metavar="FILE", \
+        group = OptionGroup(parser, "Global Options")
-        parser.add_option("-L", "--loglevel", dest="loglevel", metavar="LEVEL", \
+        group.add_option("-L", "--loglevel", dest="loglevel", metavar="LEVEL", \
-        parser.add_option("--default-spider", dest="default_spider", default=None, \
+            help="log level (default: %s)" % settings['LOGLEVEL'])
-        parser.add_option("--spider", dest="spider", default=None, \
+        group.add_option("--spider", dest="spider", default=None, \
-        parser.add_option("--profile", dest="profile", metavar="FILE", default=None, \
+        group.add_option("--profile", dest="profile", metavar="FILE", default=None, \
-        parser.add_option("--pidfile", dest="pidfile", metavar="FILE", \
+        group.add_option("--lsprof", dest="lsprof", metavar="FILE", default=None, \
-        parser.add_option("--set", dest="settings", action="append", \
+        group.add_option("--set", dest="settings", action="append", \
-    parser = optparse.OptionParser()
+    parser = optparse.OptionParser(conflict_handler='resolve')
-                            help="Override Scrapy setting SETTING with VALUE. May be repeated.")
+        parser.add_option("--logfile", dest="logfile", metavar="FILE", \
-            open(opts.pidfile, "w").write(str(pid))
+            with open(opts.pidfile, "w") as f:
-import pprint
+from scrapy.stats import stats
-                subj = "%s terminated: memory usage exceeded %dM at %s" % (settings['BOT_NAME'], mem, socket.gethostname())
+                subj = "%s terminated: memory usage exceeded %dM at %s" % \
-                subj = "%s warning: memory usage reached %dM at %s" % (settings['BOT_NAME'], mem, socket.gethostname())
+                subj = "%s warning: memory usage reached %dM at %s" % \
-class Loader(unittest.TestCase):
+class LoaderTest(unittest.TestCase):
-        class ChildLoadeLoader(TestLoader):
+        class ChildLoader(TestLoader):
-class ItemLoader(object):
+class Loader(object):
-from scrapy.newitem.loader import ItemLoader
+from scrapy.newitem.loader import Loader
-class NameItemLoader(ItemLoader):
+class NameLoader(Loader):
-class TestItemLoader(NameItemLoader):
+class TestLoader(NameLoader):
-class DefaultedItemLoader(NameItemLoader):
+class DefaultedLoader(NameLoader):
-class ItemLoaderTest(unittest.TestCase):
+class Loader(unittest.TestCase):
-        il = ItemLoader(item=i)
+        il = Loader(item=i)
-        il = TestItemLoader()
+        il = TestLoader()
-        il = TestItemLoader()
+        il = TestLoader()
-        il = TestItemLoader()
+        il = TestLoader()
-        class TestItemLoader(NameItemLoader):
+        class TestLoader(NameLoader):
-        il = TestItemLoader()
+        il = TestLoader()
-        il = DefaultedItemLoader()
+        il = DefaultedLoader()
-        class InheritDefaultedItemLoader(DefaultedItemLoader):
+        class InheritDefaultedLoader(DefaultedLoader):
-        il = InheritDefaultedItemLoader()
+        il = InheritDefaultedLoader()
-        class ChildItemLoader(TestItemLoader):
+        class ChildLoader(TestLoader):
-        il = ChildItemLoader()
+        il = ChildLoader()
-        class ChildChildItemLoader(ChildItemLoader):
+        class ChildChildLoader(ChildLoader):
-        il = ChildChildItemLoader()
+        il = ChildChildLoader()
-        class IdentityDefaultedItemLoader(DefaultedItemLoader):
+        class IdentityDefaultedLoader(DefaultedLoader):
-        il = IdentityDefaultedItemLoader()
+        il = IdentityDefaultedLoader()
-        class IdentityDefaultedItemLoader(DefaultedItemLoader):
+        class IdentityDefaultedLoader(DefaultedLoader):
-        il = IdentityDefaultedItemLoader()
+        il = IdentityDefaultedLoader()
-            name_exp = TreeExpander(TestItemLoader.name_exp, unicode.swapcase)
+        class ChildLoader(TestLoader):
-        il = ChildItemLoader()
+        il = ChildLoader()
-            name_exp = TreeExpander(DefaultedItemLoader.default_expander, unicode.swapcase)
+        class ChildDefaultedLoader(DefaultedLoader):
-        il = ChildDefaultedItemLoader()
+        il = ChildDefaultedLoader()
-        il = TestItemLoader()
+        il = TestLoader()
-        class TakeFirstItemLoader(TestItemLoader):
+        class TakeFirstLoader(TestLoader):
-        il = TakeFirstItemLoader()
+        il = TakeFirstLoader()
-        il = TestItemLoader()
+        il = TestLoader()
-        class TakeFirstItemLoader(TestItemLoader):
+        class TakeFirstLoader(TestLoader):
-        il = TakeFirstItemLoader()
+        il = TakeFirstLoader()
-        class TakeFirstItemLoader(TestItemLoader):
+        class TakeFirstLoader(TestLoader):
-        il = TakeFirstItemLoader()
+        il = TakeFirstLoader()
-        il = TestItemLoader()
+        il = TestLoader()
-        class LalaItemLoader(TestItemLoader):
+        class LalaLoader(TestLoader):
-        il = LalaItemLoader()
+        il = LalaLoader()
-        class ChildItemLoader(TestItemLoader):
+        class ChildLoader(TestLoader):
-        il = ChildItemLoader()
+        il = ChildLoader()
-        class ChildItemLoader(TestItemLoader):
+        class ChildLoader(TestLoader):
-        il = ChildItemLoader(key=u'val')
+        il = ChildLoader(key=u'val')
-        class ChildItemLoader(TestItemLoader):
+        class ChildLoadeLoader(TestLoader):
-        il = ChildItemLoader()
+        il = ChildLoader()
-        class ChildItemLoader(TestItemLoader):
+        class ChildLoader(TestLoader):
-        il = ChildItemLoader(item=it)
+        il = ChildLoader(item=it)
-        il = TestItemLoader()
+        il = TestLoader()
-        self._item = loader_args['item']
+    def __init__(self, item=None, **loader_args):
-This module provides some commonly used Expanders
+This module provides some commonly used Expanders.
-This module provides some commonly used Reducers
+This module provides some commonly used Reducers.
-    """
+class Join(object):
-from scrapy.newitem.loader.reducers import JoinStrings, Identity
+from scrapy.newitem.loader.reducers import Join, Identity
-            name_red = JoinStrings()
+            name_red = Join()
-            name_red = JoinStrings("<br>")
+            name_red = Join("<br>")
-        if args or kwargs: # avoid instantiating dict for most common case
+        if args or kwargs: # avoid creating dict for most common case
-                raise KeyError(key)
+            if 'default' in field:
-            name = Field(default_factory=lambda: u'John')
+            name = Field(default=u'John')
-from htmlentitydefs import name2codepoint as entity_defs
+from htmlentitydefs import name2codepoint
-def remove_entities(text, keep=(), remove_illegal=True):
+def remove_entities(text, keep=(), remove_illegal=True, encoding='utf-8'):
-    'text' can be a unicode string or a regular string encoded as 'utf-8'
+    'text' can be a unicode string or a regular string encoded in the given
-
+                number = name2codepoint.get(entity_body)
-            return m.group(0)
+        return u'' if remove_illegal else m.group(0)
-    return _ent_re.sub(convert_entity, str_to_unicode(text))
+    return _ent_re.sub(convert_entity, str_to_unicode(text, encoding))
-            self.start_time = datetime.now()
+            self.start_time = datetime.utcnow()
-            "datetime.now()-self.start_time", 
+            "datetime.utcnow()-self.start_time", 
-            stored = datetime.now()
+            stored = datetime.utcnow()
-        ts = datetime.now().isoformat()
+        ts = datetime.utcnow().isoformat()
-        stats.set_value('start_time', datetime.datetime.now(), domain=domain)
+        stats.set_value('start_time', datetime.datetime.utcnow(), domain=domain)
-        stats.set_value('finish_time', datetime.datetime.now(), domain=domain)
+        stats.set_value('finish_time', datetime.datetime.utcnow(), domain=domain)
-when needed.
+for debugging.
-import datetime
+from time import time
-            tbefore = datetime.datetime.now()
+            tbefore = time()
-            ct = tafter-tbefore
+            ct = time() - tbefore
-            sct = stats.get_value('profiling/slowest_callback_time', datetime.timedelta(0), domain=domain)
+            tcc = stats.get_value('profiling/total_callback_time', 0, domain=domain)
-        self._item = loader_args.setdefault('item', self.default_item_class())
+        if 'item' not in loader_args:
-    def test_extend_expanders(self):
+    def test_extend_custom_expanders(self):
-    def test_staticdefaults(self):
+    def test_extend_default_expanders(self):
-    pass
+    """Container of field metadata"""
-        if args or kwargs: # don't instantiate dict for simple (most common) case
+        if args or kwargs: # avoid instantiating dict for most common case
-
+from scrapy.utils.datatypes import MergeDict
-from scrapy.newitem.loader.reducers import take_first
+from scrapy.newitem.loader.reducers import TakeFirst
-    item_class = Item
+    default_item_class = Item
-        self._item = loader_args.get('item') or self.item_class()
+        self._item = loader_args.setdefault('item', self.default_item_class())
-        return getattr(self, 'expand_%s' % field_name, self.expand)
+        expander = getattr(self, '%s_exp' % field_name, None)
-            return self._item.fields[field_name].get('reducer', self.reduce)
+        reducer = getattr(self, '%s_red' % field_name, None)
-            loader_args = self._loader_args
+            loader_args = MergeDict(new_loader_args, self._loader_args)
-ItemLoader expanders
+This module provides some commonly used Expanders
-    algorithm".
+class TreeExpander(object):
-            func = func.im_func
+    def __init__(self, *functions, **default_loader_args):
-                wfunc = func
+                wfunc = self.wrap_with_args(func)
-        wrapped_funcs.append(wfunc)
+                wfunc = self.wrap_no_args(func)
-    def _expander(loader, value, loader_args):
+    def __call__(self, value, loader_args):
-        largs = default_loader_args
+        largs = self.default_loader_args
-        for func in wrapped_funcs:
+            largs = MergeDict(loader_args, self.default_loader_args)
-                next_values += arg_to_iter(func(loader, v, largs))
+                next_values += arg_to_iter(func(v, largs))
-    return _expander
+class IdentityExpander(object):
-ItemLoader reducers
+This module provides some commonly used Reducers
-            return value
+class TakeFirst(object):
-    return values
+    def __call__(self, values):
-    return u' '.join(values)
+
-from scrapy.newitem.loader.expanders import tree_expander
+from scrapy.newitem.loader.expanders import TreeExpander, IdentityExpander
-class BaseItem(Item):
+class NameItem(Item):
-class TestItem(BaseItem):
+class TestItem(NameItem):
-
+class NameItemLoader(ItemLoader):
-    expand = tree_expander(lambda v: v[:-1])
+class TestItemLoader(NameItemLoader):
-    pass
+# test expanders
-        il = TestItemLoader()
+    def test_get_item_using_default_loader(self):
-            expand_name = tree_expander(lambda v: v.title(), lambda v: v[:-1])
+    def test_tree_expander_multiple_functions(self):
-        self.assertEqual(dil.get_reduced_value('name'), u'mart')
+    def test_default_expander(self):
-        dil = InheritDefaultedItemLoader()
+    def test_inherited_default_expander(self):
-        self.assertEqual(dil.get_reduced_value('name'), u'mart')
+        il = InheritDefaultedItemLoader()
-    def test_inheritance(self):
+    def test_expander_inheritance(self):
-            expand_url = tree_expander(lambda v: v.lower())
+            url_exp = TreeExpander(lambda v: v.lower())
-            expand_summary = tree_expander(lambda v: v)
+            url_exp = TreeExpander(lambda v: v.upper())
-    def test_identity(self):
+    def test_empty_tree_expander(self):
-            expand_name = tree_expander()
+            name_exp = TreeExpander()
-    def test_staticmethods(self):
+    def test_extend_expanders(self):
-            expand_name = tree_expander(TestItemLoader.expand_name, unicode.swapcase)
+            name_exp = TreeExpander(TestItemLoader.name_exp, unicode.swapcase)
-            expand_name = tree_expander(DefaultedItemLoader.expand, unicode.swapcase)
+            name_exp = TreeExpander(DefaultedItemLoader.default_expander, unicode.swapcase)
-    def test_reducer(self):
+    def test_reducer_using_function(self):
-            reduce_name = staticmethod(u" ".join)
+            name_red = u" ".join
-            return value
+        class TakeFirstItemLoader(TestItemLoader):
-            expand_url = tree_expander(expander_func_with_args)
+            url_exp = TreeExpander(expander_func_with_args)
-        il = ChildItemLoader(val=u'val')
+        il = ChildItemLoader(key=u'val')
-        il.add_value('url', u'text', val=u'val')
+        il.add_value('url', u'text', key=u'val')
-    def test_add_value_unknown_field(self):
+    def test_add_value_on_unknown_field(self):
-        il.add_value('wrong_field', [u'lala', u'lolo'])
+        self.assertRaises(KeyError, il.add_value, 'wrong_field', [u'lala', u'lolo'])
-        self.assertRaises(KeyError, il.get_item)
+if __name__ == "__main__":
-
+from scrapy.newitem.loader.reducers import take_first
-        self._values[field_name].extend(evalue)
+        self._values[field_name] += self._expand_value(field_name, value, \
-        self._values[field_name] = evalue
+        self._values[field_name] = self._expand_value(field_name, value, \
-            item[field_name] = self.get_value(field_name)
+            item[field_name] = self.get_reduced_value(field_name)
-        values = self._values[field_name]
+    def get_expanded_value(self, field_name):
-        return reducer(values)
+        return reducer(self._values[field_name])
-    def reduce(self, values):
+    def reduce(self, values): # default reducer
-"""Some common reducers"""
+"""
-from scrapy.newitem.loader import ItemLoader, tree_expander
+from scrapy.newitem.loader import ItemLoader
-        ib = TestItemLoader()
+    def test_get_item(self):
-        item = ib.get_item()
+        il.add_value('name', u'marta')
-        ib = TestItemLoader()
+        il = TestItemLoader()
-        self.assertEqual(ib.get_value('name'), u'Mart')
+        il.add_value('name', u'marta')
-        item = ib.get_item()
+        item = il.get_item()
-        dib = DefaultedItemLoader()
+        dil = DefaultedItemLoader()
-        self.assertEqual(dib.get_value('name'), u'mart')
+        dil.add_value('name', u'marta')
-        dib = InheritDefaultedItemLoader()
+        dil = InheritDefaultedItemLoader()
-        self.assertEqual(dib.get_value('name'), u'mart')
+        dil.add_value('name', u'marta')
-        ib = ChildItemLoader()
+        il = ChildItemLoader()
-        self.assertEqual(ib.get_value('url'), u'http://scrapy.org')
+        il.add_value('url', u'HTTP://scrapy.ORG')
-        self.assertEqual(ib.get_value('name'), u'Marta')
+        il.add_value('name', u'marta')
-        ib = ChildChildItemLoader()
+        il = ChildChildItemLoader()
-        self.assertEqual(ib.get_value('url'), u'HTTP://SCRAPY.ORG')
+        il.add_value('url', u'http://scrapy.org')
-        self.assertEqual(ib.get_value('name'), u'Marta')
+        il.add_value('name', u'marta')
-        ib = IdentityDefaultedItemLoader()
+        il = IdentityDefaultedItemLoader()
-        self.assertEqual(ib.get_value('name'), u'marta')
+        il.add_value('name', u'marta')
-        ib = ChildItemLoader()
+        il = ChildItemLoader()
-        self.assertEqual(ib.get_value('name'), u'mARTA')
+        il.add_value('name', u'marta')
-        ib = ChildDefaultedItemLoader()
+        il = ChildDefaultedItemLoader()
-        self.assertEqual(ib.get_value('name'), u'MART')
+        il.add_value('name', u'marta')
-        ib = TestItemLoader()
+        il = TestItemLoader()
-        self.assertEqual(ib.get_value('name'), u'Mar')
+        il.add_value('name', [u'mar', u'ta'])
-        ib = TakeFirstItemLoader()
+        il = TakeFirstItemLoader()
-        self.assertEqual(ib.get_value('name'), u'Mar Ta')
+        il.add_value('name', [u'mar', u'ta'])
-        self.assertEqual(ib.get_value('url'), 'val')
+        il = ChildItemLoader(val=u'val')
-        self.assertEqual(ib.get_value('url'), 'val')
+        il = ChildItemLoader()
-        ib.add_value('wrong_field', [u'lala', u'lolo'])
+        il = TestItemLoader()
-        self.assertRaises(KeyError, ib.get_item)
+        self.assertRaises(KeyError, il.get_item)
-from scrapy.newitem.models import Item
+from UserDict import DictMixin
-from scrapy.newitem.models import Item
+from scrapy.newitem import Item
-            return field.from_unicode_list(values)
+        return reducer(values)
-        return getattr(self, 'reduce_%s' % field_name, None)
+        try:
-
+"""Some common reducers"""
-        if value is not u'':
+        if value:
-def return_values(values):
+def identity(values):
-from scrapy.newitem import Item, fields
+from scrapy.newitem import Item, Field
-    name = fields.TextField()
+    name = Field()
-    summary = fields.TextField()
+    url = Field()
-            expand_name = tree_expander(TestItemLoader.expand_name, string.swapcase)
+            expand_name = tree_expander(TestItemLoader.expand_name, unicode.swapcase)
-            expand_name = tree_expander(DefaultedItemLoader.expand, string.swapcase)
+            expand_name = tree_expander(DefaultedItemLoader.expand, unicode.swapcase)
-        self.assertEqual(ib.get_value('name'), u'Mar Ta')
+        self.assertEqual(ib.get_value('name'), u'Mar')
-            reduce_name = staticmethod(reducers.take_first)
+            reduce_name = staticmethod(u" ".join)
-        self.assertEqual(ib.get_value('name'), u'Mar')
+        self.assertEqual(ib.get_value('name'), u'Mar Ta')
-from scrapy.newitem.fields import BaseField
+from scrapy.newitem import Item, Field
-            name = fields.TextField()
+            name = Field()
-            name = fields.TextField()
+            name = Field()
-            name = fields.TextField(default=u'John')
+            name = Field(default_factory=lambda: u'John')
-            number = fields.IntegerField()
+            name = Field()
-        i['number'] = '123'
+        i['number'] = 123
-            name = fields.TextField()
+            name = Field()
-            name = fields.TextField()
+            name = Field()
-            values = fields.TextField() 
+            name = Field()
-            values = fields.TextField() 
+            name = Field()
-            keys = fields.IntegerField()
+            keys = Field()
-            name = fields.TextField()
+            name = Field()
-        for func2 in wrapped_funcs:
+        for func in wrapped_funcs:
-                next_values.extend(arg_to_iter(val))
+            for v in values:
-        self._field = field_type()
+    def __init__(self, field, default=None):
-            return [self._field.to_python(v) for v in value]
+            return [self.field.to_python(v) for v in value]
-    names = fields.ListField(fields.TextField)
+    names = fields.ListField(fields.TextField())
-    def test_multi(self):
+    def test_list(self):
-            names = fields.ListField(fields.TextField)
+            names = fields.ListField(fields.TextField())
-        field = fields.ListField(fields.TextField)
+        field = fields.ListField(fields.TextField())
-class MultiValuedField(BaseField):
+class ListField(BaseField):
-        super(MultiValuedField, self).__init__(default)
+        super(ListField, self).__init__(default)
-    names = fields.MultiValuedField(fields.TextField)
+class ListFieldTestItem(Item):
-    item_class = MultiValuedTestItem
+class ListFieldItemAdaptor(ItemAdaptor):
-        ma = MultiValuedItemAdaptor()
+        ma = ListFieldItemAdaptor()
-        class TestMultiItem(Item):
+        class TestListItem(Item):
-            names = fields.MultiValuedField(fields.TextField)
+            names = fields.ListField(fields.TextField)
-        i = TestMultiItem()
+        i = TestListItem()
-        field = fields.MultiValuedField(fields.TextField)
+        field = fields.ListField(fields.TextField)
-    def __new__(meta, class_name, bases, attrs):
+    def __new__(mcs, class_name, bases, attrs):
-        cls = type.__new__(meta, class_name, bases, new_attrs)
+        cls = type.__new__(mcs, class_name, bases, new_attrs)
-        dispatcher.connect(self.send_stats, signal=signals.stats_domain_closing)
+        dispatcher.connect(self.stats_domain_closed, signal=signals.stats_domain_closed)
-    def send_stats(self, domain):
+    def stats_domain_closed(self, domain, domain_stats):
-        body += "\n".join("%-50s : %s" % i for i in stats.get_stats(domain).items())
+        body += "\n".join("%-50s : %s" % i for i in domain_stats.items())
-        signals.send_catch_log(stats_domain_closed, domain=domain, reason=reason)
+        stats = self._stats.pop(domain)
-        else:
+        if hasattr(value, '__iter__'):
-        elif isinstance(value, unicode):
+        if isinstance(value, unicode):
-        return u' '.join((self.to_python(x) for x in value))
+            raise TypeError("%s values cannot be created from '%s' objects" % \
-        self.assertEqual(dia.name, u'MART')
+# FIXME: deprecated tests - will be replaced by ItemBuilder tests
-        self.assertRaises(TypeError, TestItem, name=3)
+        self.assertRaises(TypeError, TestItem, name=set())
-        self.assertEqual(i['name'], u'John Doe')
+        self.assertRaises(TypeError, fields.TextField, default=set())
-        self.assertRaises(TypeError, i.__setitem__, 'field', 'string')
+        i['field'] = 3
-        self.assertRaises(TypeError, set_invalid_value)
+    def test_from_unicode_list(self):
-        self.assert_(isinstance(i['field'], unicode))
+        field = fields.TextField()
-        self.assertRaises(TypeError, i.__setitem__, 'field', [u'hello', 'world']) 
+        field = fields.MultiValuedField(fields.TextField)
-        self.assertEqual(i['field'], '')
+        field = fields.IntegerField()
-
+    MEMORYSTORE = 'scrapy.contrib_exp.history.memorystore.MemoryStore'
-        historycls = load_object(settings['MEMORYSTORE'])
+        historycls = load_object(self.MEMORYSTORE)
-            self.running.remove(domain)
+        self.running.remove(domain)
-        self.not_scheduled = [d for d in enabled_domains if d not in self.scheduled
+        self.scheduled = scrapyengine.domain_scheduler.pending_domains
-        s += "<tr><th>Running (%d/%d)</th><th>Scheduled (%d)</th><th>Finished (%d)</th><th>Not scheduled (%d)</th></tr>\n" % \
+        s += "<tr><th>Running (%d/%d)</th><th>Scheduled (%d)</th><th>Finished (%d)</th><th>Idle (%d)</th></tr>\n" % \
-                 len(self.not_scheduled))
+                 len(self.idle))
-        for domain in sorted(self.scheduled):
+        for domain in self.scheduled:
-        # not scheduled
+        # idle
-        for domain in sorted(self.not_scheduled):
+        for domain in sorted(self.idle):
-        s += '<input type="submit" value="Bulk remove domains">\n'
+        s += '<input type="submit" value="Bulk remove domains"><br />\n'
-        s += '<input type="submit" value="Bulk schedule domains">\n'
+        s += '<input type="submit" value="Bulk schedule domains"><br/>\n'
-                if domain not in scrapyengine.scheduler.pending_domains_count:
+                if domain not in scrapyengine.scheduler.pending_requests:
-                if domain in enabled_domains and domain not in scrapyengine.scheduler.pending_domains_count:
+                if domain in enabled_domains and domain not in scrapyengine.scheduler.pending_requests:
-                if domain not in scrapyengine.scheduler.pending_domains_count:
+                if domain not in scrapyengine.scheduler.pending_requests:
-        totdomains = totscraped = totcrawled = totscheduled = totactive = totpending = 0
+        totdomains = totscraped = totcrawled = totscheduled = totactive = totpending = totdqueued = tottransf = 0
-        s += "<tr><th>Domain</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Pending</th><th>Downloader<br/>Active</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\n"
+        s += "<tr><th>Domain</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\n"
-            pending = len(dwl.sites[d].queue) if d in dwl.queue else 0
+            dqueued = len(dwl.sites[d].queue) if d in dwl.sites else 0
-                 (d, stats.scraped, stats.crawled, scheduled, pending, active, str(stats.started), str(stats.finished), str(runtime))
+            s += '<tr><td>%s</td><td align="right">%d</td><td align="right">%d</td><td align="right">%d</td><td align="right">%d</td><td align="right">%d</td><td align="right">%d</td><td>%s</td><td>%s</td><td>%s</td></tr>\n' % \
-             (totdomains, totscraped, totcrawled, totscheduled, totactive, totpending)
+            totdqueued += dqueued
-
+def stats_html_table(statsdict):
-        s += "</pre></code>\n"
+        s += "<h3>Global stats</h3>\n"
-from scrapy.patches import apply_patches
+from scrapy.xlib.patches import apply_patches
-WEBCONSOLE_PORT = None
+WEBCONSOLE_PORT = 6080
-monkeypatches.apply_patches()
+from scrapy.patches import apply_patches
-from scrapy.conf import settings
+from scrapy.conf import settings, SETTINGS_MODULE
-        sys.exit()
+        sys.exit(2)
-    settings = None
+    settings_module = None
-            self.settings = self._import(SETTINGS_MODULE)
+        self.settings_module = self._import(SETTINGS_MODULE)
-        return __import__(modulepath, {}, {}, [''])
+        try:
-
+            if hasattr(self.settings_module, opt_name):
-            return getattr(self.global_defaults, opt_name)
+        return getattr(self.global_defaults, opt_name, None)
-                self._export_xml_field(field, 'value', v)
+                self._export_xml_field(field._field, 'value', v)
-        self.file.write(pprint.pprint(dict(item)) + '\n')
+        self.file.write(pprint.pformat(dict(item)) + '\n')
-        super(PprintItemExporter, self).__init__(self)
+        super(PprintItemExporter, self).__init__()
-        super(PickleItemExporter, self).__init__(self)
+        super(PickleItemExporter, self).__init__()
-        super(CsvItemExporter, self).__init__(self)
+        super(CsvItemExporter, self).__init__()
-        if not tsk.called:
+        tsk = self.tasks.pop(domain, None)
-CLOSEDOMAIN_NOTIFY = []
+CLOSEDOMAIN_ITEMPASSED = 0
-time has expired.
+"""CloseDomain is an extension that forces spiders to be closed after certain
-import pprint
+from collections import defaultdict
-from scrapy.stats import stats
+
-            raise NotConfigured
+        self.itempassed = settings.getint('CLOSEDOMAIN_ITEMPASSED')
-        dispatcher.connect(self.domain_opened, signal=signals.domain_opened)
+        if self.timeout:
-        self.tasks[domain] = scrapyengine.addtask(self.close_domain, self.timeout, args=[domain])
+        self.tasks[domain] = reactor.callLater(self.timeout, scrapyengine.close_domain, \
-            self.mail.send(self.notify, subj, body)
+    def item_passed(self, item, spider):
-            scrapyengine.removetask(self.tasks[domain])
+        self.counts.pop(domain, None)
-        reqlen = len(request.httprepr())
+        reqlen = len(request_httprepr(request))
-        reslen = len(response.httprepr())
+        reslen = len(response_httprepr(response))
-        elif max_response_size and max_response_size > len(response.httprepr()):  
+        elif max_response_size and max_response_size > len(response_httprepr(response)):  
-from scrapy.utils.request import request_fingerprint, request_authenticate
+from scrapy.utils.request import request_fingerprint, request_authenticate, request_httprepr
-
+    def test_request_httprepr(self):
-from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh
+from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh, response_httprepr
-        body="""
+        body = """
-        body="""<meta http-equiv="refresh" content="5" />"""
+        body = """<meta http-equiv="refresh" content="5" />"""
-        body="""<meta http-equiv="refresh" content="5;
+        body = """<meta http-equiv="refresh" content="5;
-from datetime import datetime
+from time import time
-        self.opened_at = defaultdict(datetime.now)
+        self.opened_at = defaultdict(time)
-        if delta.seconds < self.delay:
+        if time() < lastseen + self.delay:
-        dispatcher.connect(self.close_domain, signal=signals.domain_closed)
+        dispatcher.connect(self._start_closing_domain, signal=signals.domain_closed)
-        signals.send_catch_log(stats_domain_closing, domain=domain, reason=reason)
+    def _start_closing_domain(self, domain, reason):
-    def inc_value(self, key, count=1, domain=None):
+    def inc_value(self, key, count=1, start=0, domain=None):
-            log.msg("Scraped %s in <%s>" % (output, request.url), domain=domain)
+            log.msg("Scraped %s in <%s>" % (output, request.url), level=log.DEBUG, \
-                log.msg("Dropped %s - %s" % (item, str(ex)), log.DEBUG, domain=domain)
+                log.msg("Dropped %s - %s" % (item, str(ex)), level=log.WARNING, domain=domain)
-from scrapy.contrib_exp.newitem.models import Item
+from scrapy.newitem.models import Item
-from scrapy.contrib_exp.newitem.fields import BaseField
+from scrapy.newitem.fields import BaseField
-from scrapy.contrib_exp.newitem import Item, fields
+from scrapy.newitem import Item, fields
-from scrapy.contrib_exp.newitem.fields import BaseField
+from scrapy.newitem import Item, fields
-        # load init values
+
-        return self._values[key]
+        try:
-depth to scrape or things like that
+Depth Spider Middleware
-                    log.msg("Ignoring link (depth > %d): %s " % (self.maxdepth, request.url), level=log.DEBUG, domain=spider.domain_name)
+                    log.msg("Ignoring link (depth > %d): %s " % (self.maxdepth, request.url), \
-                        stats.set_value('request_depth_max', depth, domain=spider.domain_name)
+                    stats.inc_value('request_depth_count/%s' % depth, domain=domain)
-        if self.stats and 'depth' not in response.request.meta: # otherwise we loose stats for depth=0 
+        # base case (depth=0)
-            stats.inc_value('request_depth_count/0', domain=spider.domain_name)
+            stats.inc_value('request_depth_count/0', domain=domain)
-            return [] # skip response
+            return []
-the spider.
+Offsite Spider Middleware
-(returned by the spider) will be dropped.
+Request Limit Spider middleware
-applied. 
+See documentation in docs/ref/spider-middleware.rst
-RestrictMiddleware: restricts crawling to fixed set of particular URLs
+Restrict Spider Middleware
-        return (r for r in result or () if _filter(r))
+        return ifilter(lambda r: isinstance(r, Request) \
-UrlFilterMiddleware: canonicalizes URLs to filter out duplicated ones
+Url Filter Middleware
-UrlLengthMiddleware: Filters out requests with URLs longer than URLLENGTH_LIMIT
+Url Length Spider Middleware
-                log.msg("Ignoring link (url length > %d): %s " % (self.maxlength, request.url), level=log.DEBUG, domain=spider.domain_name)
+                log.msg("Ignoring link (url length > %d): %s " % (self.maxlength, request.url), \
-    """Filter out already seen requests to avoid visiting pages more than once."""
+
-from scrapy.item import ScrapedItem
+from scrapy.item.models import BaseItem
-            items = [self.pipeline_process(i, spider, opts) for i in result if isinstance(i, ScrapedItem)]
+            items = [self.pipeline_process(i, spider, opts) for i in result if \
-from scrapy.item import ScrapedItem, ItemDelta
+from scrapy.item import ScrapedItem
-class RobustItemDelta(ItemDelta):
+class RobustItemDelta(object):
-        return a list of ScrapedItems and/or Requests"""
+        return a list of BaseItem and/or Requests"""
-from scrapy.item import ScrapedItem
+from scrapy.item.models import BaseItem
-        This method must return either a ScrapedItem, a Request, or a list
+        This method must return either a BaseItem, a Request, or a list
-            if isinstance(ret, (ScrapedItem, Request)):
+            if isinstance(ret, (BaseItem, Request)):
-            if isinstance(ret, (ScrapedItem, Request)):
+            if isinstance(ret, (BaseItem, Request)):
-from scrapy.item import ScrapedItem
+from scrapy.item.models import BaseItem
-        guids = [i.guid for i in arg_to_iter(items) if isinstance(i, ScrapedItem)]
+        guids = [i.guid for i in arg_to_iter(items) if isinstance(i, BaseItem)]
-from scrapy.item import ScrapedItem
+from scrapy.item.models import BaseItem
-class Item(DictMixin, ScrapedItem):
+class Item(DictMixin, BaseItem):
-from scrapy.item import ScrapedItem
+from scrapy.item.models import BaseItem
-        elif isinstance(output, ScrapedItem):
+        elif isinstance(output, BaseItem):
-            log.msg("Spider must return Request, ScrapedItem or None, got '%s' in %s" % \
+            log.msg("Spider must return Request, BaseItem or None, got '%s' in %s" % \
-from scrapy.item.models import ScrapedItem, ItemDelta
+from scrapy.item.models import ScrapedItem
-import copy
+class BaseItem(object):
-    """
+class ScrapedItem(BaseItem):
-from scrapy.item import ScrapedItem
+from scrapy.item.models import BaseItem
-                'Item pipelines must return a ScrapedItem, got %s' % type(item).__name__
+            assert isinstance(item, BaseItem), \
-from scrapy.item import ScrapedItem
+from scrapy.item.models import BaseItem
-    elif isinstance(obj, (ScrapedItem, Request, Response)):
+    elif isinstance(obj, (BaseItem, Request, Response)):
-            for item_field in cls.item_class.get_fields():
+            for item_field in cls.item_class.fields:
-        if name in cls.item_class.get_fields():
+        if name in cls.item_class.fields:
-        setattr(self.item_instance, name, ovalue)
+        self.item_instance[name] = ovalue
-        return getattr(self.item_instance, name)
+        return self.item_instance[name]
-        cls._fields = cls._fields.copy()
+        fields = {}
-                delattr(cls, n)
+                fields[n] = v
-    """ This is the base class for all scraped items. """
+class Item(DictMixin, ScrapedItem):
-    _fields = {}
+    fields = {}
-    def __init__(self, values=None):
+    def __init__(self, *args, **kwargs):
-                raise AttributeError(name)
+        
-        return "%s(%s)" % (self.__class__.__name__, repr(values))
+        values = ', '.join('%s=%r' % field for field in self.iteritems())
-        return cls._fields
+    def get_id(self):
-        self.assertEqual(ia.item_instance.name, u'Marta')
+        self.assertEqual(ia.item_instance['name'], u'Marta')
-        self.assertEqual(i.name, u'name')
+        i['name'] = u'name'
-        
+
-        self.assert_(i.name is None)
+        self.assertRaises(KeyError, i.__getitem__, 'name')
-        self.assertEqual(i2.name, u'john doe')
+        i4 = TestItem(i3)
-        self.assertRaises(TypeError, TestItem, name=u'john doe')
+        self.assertRaises(KeyError, TestItem, {'name': u'john doe',
-                                                     'other': u'foo'})
+        self.assertRaises(TypeError, TestItem, name=3)
-        self.assertEqual(i.names, [u'name1', u'name2'])
+        i['name'] = u'name'
-        self.assertRaises(AttributeError, getattr, i, 'field')
+        self.assertRaises(KeyError, i.__setitem__, 'field', 'text')
-        self.assertEqual(i.name, u'John')
+        self.assertEqual(i['name'], u'John')
-        self.assertEqual(i.name, u'John Doe')
+        i['name'] = (u'John', u'Doe')
-        i.number = '123'
+        i['name'] = u'John Doe'
-                         "TestItem({'name': u'John Doe', 'number': 123})")
+                         "TestItem(name=u'John Doe', number=123)")
-        self.assertEqual(i2.number, 123)
+        self.assertEqual(i2['name'], 'John Doe')
-                return self.name
+                return self['name']
-                self.name = name
+                self['name'] = name
-        i.name = u'lala'
+        self.assertRaises(KeyError, i.get_name)
-        self.assert_(i.field is True)
+        i['field'] = True
-        self.assert_(i.field is True)
+        i['field'] = 1
-        self.assert_(i.field is False)
+        i['field'] = False
-        self.assert_(i.field is False)
+        i['field'] = 0
-        self.assert_(i.field is False)
+        i['field'] = None
-        self.assertEqual(i.field, d_today)
+        i['field'] = d_today
-        self.assertEqual(i.field, dt_today.date())
+        i['field'] = dt_today
-        self.assertEqual(i.field, datetime.date(2009, 5, 21))
+        i['field'] = '2009-05-21'
-        self.assertRaises(ValueError, setattr, i, 'field', '21-05-2009')
+        self.assertRaises(ValueError, i.__setitem__, 'field', '21-05-2009')
-        self.assertRaises(ValueError, setattr, i, 'field', '2009-05-51')
+        self.assertRaises(ValueError, i.__setitem__, 'field', '2009-05-51')
-        self.assertRaises(TypeError, setattr, i, 'field', None)
+        self.assertRaises(TypeError, i.__setitem__, 'field', None)
-        self.assertEqual(i.field, dt_today)
+        i['field'] = dt_today
-        self.assertEqual(i.field, datetime.datetime(d_today.year,
+        i['field'] = d_today
-        self.assertEqual(i.field, datetime.datetime(2009, 5, 21, 11, 8, 10,
+        i['field'] = '2009-05-21 11:08:10.100'
-        self.assertEqual(i.field, datetime.datetime(2009, 5, 21, 11, 8, 10))
+        i['field'] = '2009-05-21 11:08:10'
-        self.assertEqual(i.field, datetime.datetime(2009, 5, 21, 11, 8))
+        i['field'] = '2009-05-21 11:08'
-        self.assertEqual(i.field, datetime.datetime(2009, 5, 21))
+        i['field'] = '2009-05-21'
-        self.assertRaises(ValueError, setattr, i, 'field', '2009-05-21 11:08:10.usecs')
+        self.assertRaises(ValueError, i.__setitem__, 'field', '2009-05-21 11:08:10.usecs')
-        self.assertRaises(ValueError, setattr, i, 'field', '21-05-2009')
+        self.assertRaises(ValueError, i.__setitem__, 'field', '21-05-2009')
-        self.assertRaises(ValueError, setattr, i, 'field', '2009-05-51')
+        self.assertRaises(ValueError, i.__setitem__, 'field', '2009-05-51')
-        self.assertRaises(TypeError, setattr, i, 'field', None)
+        self.assertRaises(TypeError, i.__setitem__, 'field', None)
-        self.assertEqual(i.field, decimal.Decimal('3.14'))
+        i['field'] = decimal.Decimal('3.14')
-        self.assertEqual(i.field, decimal.Decimal('3.14'))
+        i['field'] = '3.14'
-        self.assertRaises(decimal.InvalidOperation, setattr, i, 'field', 'text')
+        self.assertRaises(decimal.InvalidOperation, i.__setitem__, 'field', 'text')
-        self.assertRaises(TypeError, setattr, i, 'field', None)
+        self.assertRaises(TypeError, i.__setitem__, 'field', None)
-        self.assertEqual(i.field, 3.14)
+        i['field'] = 3.14
-        self.assertEqual(i.field, 3.14)
+        i['field'] = '3.14'
-        self.assertRaises(ValueError, setattr, i, 'field', 'text')
+        self.assertRaises(ValueError, i.__setitem__, 'field', 'text')
-        self.assertRaises(TypeError, setattr, i, 'field', None)
+        self.assertRaises(TypeError, i.__setitem__, 'field', None)
-        self.assertEqual(i.field, 3)
+        i['field'] = 3
-        self.assertEqual(i.field, 3)
+        i['field'] = '3'
-        self.assertRaises(ValueError, setattr, i, 'field', 'text')
+        self.assertRaises(ValueError, i.__setitem__, 'field', 'text')
-        self.assertRaises(TypeError, setattr, i, 'field', None)
+        self.assertRaises(TypeError, i.__setitem__, 'field', None)
-        self.assert_(isinstance(i.field, unicode))
+        i['field'] = u'hello'
-        self.assertRaises(TypeError, setattr, i, 'field', 'string')
+        self.assertRaises(TypeError, i.__setitem__, 'field', 'string')
-        self.assertRaises(TypeError, setattr, i, 'field', None)
+        self.assertRaises(TypeError, i.__setitem__, 'field', None)
-            i.field = 3 
+            i['field'] = 3 
-        self.assert_(isinstance(i.field, unicode))
+        i['field'] = [u'hello', u'world']
-        self.assertRaises(TypeError, setattr, i, 'field', [u'hello', 'world']) 
+        self.assertRaises(TypeError, i.__setitem__, 'field', [u'hello', 3, u'world']) 
-        self.assertEqual(i.field, '')
+        i['field'] = []
-        self.assertEqual(i.field, dt_t)
+        i['field'] = dt_t
-        self.assertRaises(TypeError, setattr, i, 'field', None)
+        self.assertRaises(TypeError, i.__setitem__, 'field', None)
-        self.assertEqual(i.field, dt_dt.time)
+        i['field'] = dt_dt
-        self.assertEqual(i.field, datetime.time(11, 8, 10, 100))
+        i['field'] = '11:08:10.100'
-        self.assertEqual(i.field, datetime.time(11, 8, 10))
+        i['field'] = '11:08:10'
-        self.assertEqual(i.field, datetime.time(11, 8))
+        i['field'] = '11:08'
-        self.assertRaises(ValueError, setattr, i, 'field', '11:08:10.usecs')
+        self.assertRaises(ValueError, i.__setitem__, 'field', '11:08:10.usecs')
-        self.assertRaises(ValueError, setattr, i, 'field', '25:08:10')
+        self.assertRaises(ValueError, i.__setitem__, 'field', '25:08:10')
-        self.assertRaises(ValueError, setattr, i, 'field', 'string')
+        self.assertRaises(ValueError, i.__setitem__, 'field', 'string')
-import string
+import string
-        parser.add_option("--force", dest="force", help="If the spider already exists, overwrite it with the template", action="store_true")
+        parser.add_option("--template", dest="template", default="crawl",
-            print "Template named %s.tmpl does not exist" % opts.template
+        template_file = join(settings['TEMPLATES_DIR'], 'spider_%s.tmpl' % opts.template)
-        site = args[1]
+        domain = args[1]
-        if site in spiders_dict.keys():
+        if domain in spiders_dict.keys():
-                print "Spider '%s' already exists. Overwriting it..." % name
+                print "Spider '%s' already exists. Overwriting it..." % domain
-                print "Spider '%s' already exists" % name
+                print "Spider '%s' already exists" % domain
-        self._genspider(name, site, template_file)
+        self._genspider(name, domain, template_file)
-        if name[0] not in string.letters: # name must start with a letter, for valid python modules
+        # - are replaced by _, for valid python modules
-        """ Generate spider """
+    def _genspider(self, name, domain, template_file):
-            'site': site,
+            'site': domain,
-        spiders_dir = os.path.abspath(os.path.dirname(spiders_module.__file__))
+        spiders_dir = abspath(dirname(spiders_module.__file__))
-    m = re.search(r"mysql:\/\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/]+)/(?P<db>.*)$", db_uri)
+    m = re.search(r"mysql:\/\/(?P<user>[^:]+)(:(?P<passwd>[^@]+))?@(?P<host>[^/:]+)(:(?P<port>\d+))?/(?P<db>.*)$", db_uri)
-        kwargs.setdefault('encoding', self.encoding)
+        kwargs.setdefault('encoding', getattr(self, '_encoding', None))
-        self._assert_response_values(r3, 'iso-8859-1', body)
+        # make sure replace() preserves the explicit encoding passed in the constructor
-            for item_field in cls.item_class.fields.keys():
+            for item_field in cls.item_class.get_fields():
-        if name in cls.item_class.fields:
+        if name in cls.item_class.get_fields():
-        cls.fields = cls.fields.copy()
+        cls._fields = cls._fields.copy()
-                cls.fields[n] = v
+                cls._fields[n] = v
-    fields = {}
+    _fields = {}
-            self._values[name] = self.fields[name].to_python(value)
+        if name in self._fields:
-                return self.fields[name].get_default()
+                return self._fields[name].get_default()
-        values = dict((field, getattr(self, field)) for field in self.fields)
+        values = dict((field, getattr(self, field)) for field in self._fields)
-    pass
+
-# Define yours item pipelines here
+from scrapy.core.exceptions import DropItem
-        return item
+        for word in self.words_to_filter:
-# - Scrapy settings for googledir                                    -
+# - Scrapy settings for googledir project
-#COMMANDS_SETTINGS_MODULE = 'googledir.conf.commands'
+ITEM_PIPELINES = ['googledir.pipelines.FilterWordsPipeline']
-from scrapy.link.extractors import RegexLinkExtractor
+from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
-    start_urls = ['http://www.google.com/dirhp']
+
-        Rule(RegexLinkExtractor(allow='google.com/[A-Z][a-zA-Z_/]+$'),
+        Rule(SgmlLinkExtractor(allow='directory.google.com/[A-Z][a-zA-Z_/]+$'),
-            item.description = link.x('font[2]/text()')
+            item.description = link.x('font[2]/text()').extract()
-            log.msg("Dumping stats:\n" + pprint.pformat(self._stats[domain]), domain=domain)
+            log.msg("Dumping stats:\n" + pprint.pformat(self.get_stats(domain)), \
-from scrapy.stats import stats
+from scrapy.stats import stats, signals
-        dispatcher.connect(self.send_stats, signal=signals.domain_closed)
+        dispatcher.connect(self.send_stats, signal=signals.stats_domain_closing)
-        body = pprint.pformat(stats[domain])
+        body = "Global stats\n\n"
-
+from scrapy import log
-        self.print_or_send_report(report)
+        self.log_or_send_report(report)
-    def print_or_send_report(self, report):
+    def log_or_send_report(self, report):
-        print report
+        log.msg(report)
-    s += "Built-in subcommands:\n"
+    s  = "usage: %s <command> [options] [args]\n" % argv[0]
-        s += "    %s\n" % cmdclass.short_desc()
+    cmds = builtin_commands_dict()
-            s += "    %s\n" % cmdclass.short_desc()
+    for cmdname, cmdclass in sorted(cmds.iteritems()):
-        return "Parse the given URL and print the results"
+        return "Parse the given URL (using the spider) and print the results"
-        return "[options] [domain|url] ..."
+        return "[options] <domain|url> ..."
-        return "Run the web scraping engine from the command line"
+        return "Start crawling a domain or URL"
-        return "<spider_name> <spider_domain_name> [--template=template_name]"
+        return "[options] <spider_name> <spider_domain_name>"
-        return "Generate new spider based on a predefined template"
+        return "Generate new spider based on template passed with --template"
-        ScrapyCommand.add_options(self, parser)
+        return "Start the Scrapy manager but don't run any spider (idle mode)"
-import sys, pprint
+import pprint
-        return "Download a URL using the Scrapy downloader"
+        return "Fetch a URL using the Scrapy downloader"
-        parser.add_option("--headers", dest="headers", action="store_true", help="output HTTP headers only")
+        parser.add_option("--headers", dest="headers", action="store_true", help="print HTTP headers instead of body")
-                sys.stdout.write(str(responses[0].body))
+                print responses[0].body
-        return "List available spiders (both enabled and disabled)"
+        return "List available spiders"
-        print "Total spiders: %d" % len(spiders_dict)
+        print "\n".join(spiders.asdict().keys())
-STATS_DEBUG = False
+STATS_DUMP = False
-        self.debug = settings.getbool('STATS_DEBUG')
+        self._dump = settings.getbool('STATS_DUMP')
-            log.msg(pprint.pformat(self[domain]), domain=domain, level=log.DEBUG)
+        if self._dump:
-            project_root_path = project_name
+            if not re.search(r'^[_a-zA-Z]\w*$', project_name): # If it's not a valid directory name.
-            copytree(roottpl, project_name, ignore=IGNORE)
+                roottpl = os.path.join(PROJECT_TEMPLATES_PATH, 'root')
-                     ignore=IGNORE)
+                moduletpl = os.path.join(PROJECT_TEMPLATES_PATH, 'module')
-                        ProjectName=string_camelcase(project_name))
+                for path in TEMPLATES:
-
+STATS_CLASS = 'scrapy.stats.collector.MemoryStatsCollector'
-STATS_CLEANUP = False
+STATS_SDB_DOMAIN = 'scrapy_stats'
-        stats.incpath('%s/downloader/request_method_count/%s' % (spider.domain_name, request.method))
+        domain = spider.domain_name
-        stats.incpath('_global/downloader/request_bytes', reqlen)
+        stats.inc_value('downloader/request_bytes', reqlen, domain=domain)
-        self._inc_response_count(response, spider.domain_name)
+        domain = spider.domain_name
-        stats.incpath('_global/downloader/response_bytes', reslen)
+        stats.inc_value('downloader/exception_count')
-        sampled = stats.getpath("%s/items_sampled" % domain, 0)
+        sampled = stats.get_value("items_sampled", 0, domain=domain)
-            stats.setpath("%s/items_sampled" % domain, sampled)
+            stats.set_value("items_sampled", sampled, domain=domain)
-        if reason == 'finished' and not stats.getpath("%s/items_sampled" % domain):
+        if reason == 'finished' and not stats.get_value("items_sampled", domain=domain):
-        if stats.getpath("%s/items_sampled" % spider.domain_name) >= items_per_domain:
+        if stats.get_value("items_sampled", domain=spider.domain_name) >= items_per_domain:
-        if stats.getpath("%s/items_sampled" % spider.domain_name) >= items_per_domain:
+        if stats.get_value("items_sampled", domain=spider.domain_name) >= items_per_domain:
-        stats.incpath('%s/image_status_count/%s' % (domain, status))
+        stats.inc_value('image_count', domain=domain)
-            stats.setpath('%s/profiling/total_callback_time' % spider.domain_name, tcc+ct)
+            domain = spider.domain_name
-                stats.setpath('%s/profiling/slowest_callback_url' % spider.domain_name, args[0].url)
+                stats.set_value('profiling/slowest_callback_time', ct, domain=domain)
-                stats.setpath('%s/profiling/total_mem_allocated_in_callbacks' % spider.domain_name, tma+mafter-mbefore)
+                tma = stats.get_value('profiling/total_mem_allocated_in_callbacks', 0, domain=domain)
-            stats.setpath('_envinfo/request_depth_limit', self.maxdepth)
+            stats.set_value('envinfo/request_depth_limit', self.maxdepth)
-                        stats.setpath('%s/request_depth_max' % spider.domain_name, depth)
+                    stats.inc_value('request_depth_count/%s' % depth, domain=spider.domain_name)
-            stats.incpath('%s/request_depth_count/0' % spider.domain_name)
+            stats.inc_value('request_depth_count/0', domain=spider.domain_name)
-            _failure.value.__class__.__name__))
+        stats.inc_value("spider_exceptions/%s" % _failure.value.__class__.__name__, \
-from scrapy.stats.statscollector import StatsCollector
+from scrapy.stats.collector import DummyStatsCollector
-stats = StatsCollector()
+# if stats are disabled use a DummyStatsCollector to improve performance
-        stats.setpath('_envinfo/pid', os.getpid())
+        stats.set_value('envinfo/user', getpass.getuser())
-        dispatcher.connect(self.stats_domain_closing, signal=stats.domain_closing)
+        dispatcher.connect(self.stats_domain_opened, signal=stats_domain_opened)
-        stats.incpath('_global/domain_count/opened')
+    def stats_domain_opened(self, domain):
-        stats.incpath('_global/domain_count/%s' % reason)
+    def stats_domain_closing(self, domain, reason):
-        stats.incpath('_global/item_scraped_count')
+        stats.inc_value('item_scraped_count', domain=spider.domain_name)
-        stats.incpath('_global/item_passed_count')
+        stats.inc_value('item_passed_count', domain=spider.domain_name)
-        stats.incpath('_global/item_dropped_count')
+        stats.inc_value('item_dropped_count', domain=spider.domain_name)
-from scrapy.stats.statscollector import StatsCollector
+from scrapy.stats.collector import StatsCollector, DummyStatsCollector
-        stats = StatsCollector(enabled=False)
+        # TODO: restore stats tests for new stats collector
-        self.assertEqual(stats.getpath('anything'), None)
+        self.assertEqual(stats.get_value('anything'), None)
-        stats = StatsCollector(enabled=True)
+        stats = StatsCollector()
-        if hasattr(self, 'xmlDoc'):
+        # we must call both cleanup functions, so we try/except all exceptions
-        if hasattr(self, 'xpathContext'):
+        except:
-    name = fields.StringField()
+    name = fields.TextField()
-    summary = fields.StringField()
+    url = fields.TextField()
-    names = fields.MultiValuedField(fields.StringField)
+    names = fields.MultiValuedField(fields.TextField)
-        self.assertEqual(ia.name, 'Marta')
+        ia.name = u'marta'
-        self.assertEqual(dta.name, 'mart')
+        dta.name = u'marta'
-        assert ida.name == 'mart'
+        ida.name = u'marta'
-        self.assertEqual(ia.url, 'http://scrapy.org')
+        ia.url = u'HTTP://scrapy.ORG'
-        self.assertEqual(ia.name, 'Marta')
+        ia.name = u'marta'
-        self.assertEqual(ia.url, 'HTTP://SCRAPY.ORG')
+        ia.url = u'HTTP://scrapy.ORG'
-        self.assertEqual(ia.name, 'Marta')
+        ia.name = u'marta'
-        self.assertEqual(ia.name, 'mARTA')
+        ia.name = u'Marta'
-        self.assertEqual(dia.name, 'MART')
+        dia.name = u'marta'
-        assert ma.names == ['Name1', 'Name2']
+        ma.names = [u'name1', u'name2']
-        raise AttributeError
+        raise AttributeError(name)
-            or name == 'default_adaptor'):
+                or name == 'default_adaptor'):
-            fa = self.default_adaptor
+        fa = self._field_adaptors.get(name, self.default_adaptor)
-            or name == 'default_adaptor'):
+                or name == 'default_adaptor'):
-def adaptor(*funcs, **adaptor_args):
+def adaptor(*funcs, **default_adaptor_args):
-        _funcs.append((func, takes_args))
+        accepts_args = 'adaptor_args' in get_func_args(func)
-        aargs = dict(t for d in [pipe_adaptor_args, adaptor_args or {}] for t in d.items())
+        values = arg_to_iter(value)
-        for func, takes_args in _funcs:
+        for func, accepts_args in _funcs:
-
+            kwargs = pipe_kwargs if accepts_args else {}
-                if isinstance(val, (list, tuple)):
+                if hasattr(val, '__iter__'):
-        self.assertRaises(AttributeError, set_invalid_field)
+        self.assertRaises(AttributeError, setattr, i, 'field', 'text')
-        self.assertRaises(AttributeError, get_invalid_field)
+        self.assertRaises(AttributeError, getattr, i, 'field')
-        self.assertRaises(ValueError, set_invalid_format)
+        self.assertRaises(ValueError, setattr, i, 'field', '21-05-2009')
-        self.assertRaises(ValueError, set_invalid_date)
+        self.assertRaises(ValueError, setattr, i, 'field', '2009-05-51')
-        self.assertRaises(ValueError, set_invalid_usecs)
+        self.assertRaises(ValueError, setattr, i, 'field', '2009-05-21 11:08:10.usecs')
-            i.field = '21-05-2009'
+        self.assertRaises(ValueError, setattr, i, 'field', '21-05-2009')
-        self.assertRaises(ValueError, set_invalid_date)
+        self.assertRaises(ValueError, setattr, i, 'field', '2009-05-51')
-        self.assertRaises(decimal.InvalidOperation, set_invalid_value)
+        self.assertRaises(decimal.InvalidOperation, setattr, i, 'field', 'text')
-        self.assertRaises(ValueError, set_invalid_value)
+        self.assertRaises(ValueError, setattr, i, 'field', 'text')
-        self.assertRaises(ValueError, set_invalid_value)
+        self.assertRaises(ValueError, setattr, i, 'field', 'text')
-        self.assertRaises(TypeError, set_str)
+        self.assertRaises(TypeError, setattr, i, 'field', 'string')
-        self.assertRaises(ValueError, set_invalid_format)
+        self.assertRaises(ValueError, setattr, i, 'field', '11:08:10.usecs')
-            i.field = '25:08:10'
+        self.assertRaises(ValueError, setattr, i, 'field', '25:08:10')
-        self.assertRaises(ValueError, set_invalid_time)
+        self.assertRaises(ValueError, setattr, i, 'field', 'string')
-        assert i.name == u'name'
+        self.assertEqual(i.name, u'name')
-        assert i.name is None
+        self.assert_(i.name is None)
-        assert i2.name == u'john doe'
+        self.assertEqual(i2.name, u'john doe')
-        assert i.names == [u'name1', u'name2']
+        self.assertEqual(i.names, [u'name1', u'name2'])
-        assert i.name == u'John'
+        self.assertEqual(i.name, u'John')
-        assert i.name == u'John Doe'
+        self.assertEqual(i.name, u'John Doe')
-        assert itemrepr == "TestItem({'name': u'John Doe', 'number': 123})"
+        self.assertEqual(itemrepr,
-        assert i2.number == 123
+        self.assertEqual(i2.name, 'John Doe')
-        assert f.get_default() is None
+        self.assert_(f.get_default() is None)
-        assert i.field is True
+        self.assert_(i.field is True)
-        assert i.field is True
+        self.assert_(i.field is True)
-        assert i.field is False
+        self.assert_(i.field is False)
-        assert i.field is False
+        self.assert_(i.field is False)
-        assert i.field is False
+        self.assert_(i.field is False)
-        assert i.field == d_today
+        self.assertEqual(i.field, d_today)
-        assert i.field == dt_today.date()
+        self.assertEqual(i.field, dt_today.date())
-        assert i.field == datetime.date(2009, 5, 21)
+        self.assertEqual(i.field, datetime.date(2009, 5, 21))
-        assert i.field == dt_today
+        self.assertEqual(i.field, dt_today)
-                                            d_today.day)
+        self.assertEqual(i.field, datetime.datetime(d_today.year,
-        assert i.field == datetime.datetime(2009, 5, 21, 11, 8, 10, 100)
+        self.assertEqual(i.field, datetime.datetime(2009, 5, 21, 11, 8, 10,
-        assert i.field == datetime.datetime(2009, 5, 21, 11, 8, 10)
+        self.assertEqual(i.field, datetime.datetime(2009, 5, 21, 11, 8, 10))
-        assert i.field == datetime.datetime(2009, 5, 21, 11, 8)
+        self.assertEqual(i.field, datetime.datetime(2009, 5, 21, 11, 8))
-        assert i.field == datetime.datetime(2009, 5, 21)
+        self.assertEqual(i.field, datetime.datetime(2009, 5, 21))
-        assert i.field == decimal.Decimal('3.14')
+        self.assertEqual(i.field, decimal.Decimal('3.14'))
-        assert i.field == decimal.Decimal('3.14')
+        self.assertEqual(i.field, decimal.Decimal('3.14'))
-        assert i.field == 3.14
+        self.assertEqual(i.field, 3.14)
-        assert i.field == 3.14
+        self.assertEqual(i.field, 3.14)
-        assert i.field == 3
+        self.assertEqual(i.field, 3)
-        assert i.field == 3
+        self.assertEqual(i.field, 3)
-        assert isinstance(i.field, unicode) 
+        self.assertEqual(i.field, u'hello')
-        assert isinstance(i.field, unicode)
+        self.assert_(isinstance(i.field, unicode))
-        assert isinstance(i.field, unicode)
+        self.assert_(isinstance(i.field, unicode))
-        assert i.field == dt_t
+        self.assertEqual(i.field, dt_t)
-        assert i.field == dt_dt.time
+        self.assertEqual(i.field, dt_dt.time)
-        assert i.field == datetime.time(11, 8, 10, 100)
+        self.assertEqual(i.field, datetime.time(11, 8, 10, 100))
-        assert i.field == datetime.time(11, 8, 10)
+        self.assertEqual(i.field, datetime.time(11, 8, 10))
-        assert i.field == datetime.time(11, 8)
+        self.assertEqual(i.field, datetime.time(11, 8))
-        return ' '.join((self.to_python(x) for x in value))
+        return u' '.join((self.to_python(x) for x in value))
-        return value
+        raise NotImplementedError()
-        assert f.to_python(1) == 1
+        self.assertRaises(NotImplementedError, f.to_python, 1)
-        if isinstance(value, datetime.date):
+        elif isinstance(value, datetime.date):
-            raise ValueError("Invalid date: %s" % str(e))
+            year, month, day = map(int, value.split('-'))
-        if isinstance(value, datetime.date):
+        elif isinstance(value, datetime.date):
-                raise ValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')
+                try: # Try without seconds.
-                                     **kwargs)
+            raise TypeError("Cannot instatiante %s with %s" \
-        return decimal.Decimal(value) if value is not None else None
+        return decimal.Decimal(value)
-        return float(value) if value is not None else None
+        return float(value)
-        return int(value) if value is not None else None
+        return int(value)
-                             % type(value).__name__)
+            raise TypeError("%s requires a unicode (or iterable of unicodes), got %s" \
-        return ' '.join(value)
+        """Converts the input iterable into a single value"""
-            return None
+        elif isinstance(value, basestring):
-                usecs = int(usecs)
+            try: # Seconds are optional, so try converting seconds first.
-                raise ValueError('Enter a valid time in HH:MM[:ss[.uuuuuu]] format.')
+                try: # Try without seconds.
-
+            raise TypeError("Cannot instatiante %s with %s" \
-        self.assertRaises(ValueError, set_wrong_default)
+        self.assertRaises(TypeError, set_wrong_default)
-        assert i.field == True
+        assert i.field is True
-        assert i.field == True
+        assert i.field is True
-        assert i.field == False
+        assert i.field is False
-        assert i.field == False
+        assert i.field is False
-        self.assertRaises(ValueError, set_str)
+        # must be unicode!
-        self.assertRaises(ValueError, set_invalid_value)
+        self.assertRaises(TypeError, set_invalid_value)
-        self.default = default or self.to_python(None)
+        self._default = self.to_python(default) if default is not None else None
-                return self.fields[name].default
+                return self.fields[name].get_default()
-        assert f.default == None
+        assert f.get_default() is None
-        elif isinstance(value, basestring):
+        elif isinstance(value, unicode):
-                % type(value).__name__)
+            raise ValueError("TextField expects a unicode, got %s" \
-        assert i.name == 'name'
+        i.name = u'name'
-        assert i2.name == 'john doe'
+        i2 = TestItem({'name': u'john doe'})
-        self.assertRaises(TypeError, TestItem, name='john doe')
+        self.assertRaises(TypeError, TestItem, name=u'john doe')
-        self.assertRaises(AttributeError, TestItem, {'name': 'john doe', 'other': 'foo'})
+        self.assertRaises(AttributeError, TestItem, {'name': u'john doe',
-        assert i.names == ['name1', 'name2']
+        i.name = u'name'
-            name = fields.TextField(default='John')
+            name = fields.TextField(default=u'John')
-        assert i.name == 'John'
+        assert i.name == u'John'
-        assert i.name == 'John Doe'
+        i.name = (u'John', u'Doe')
-        i.name = 'John Doe'
+        i.name = u'John Doe'
-        assert itemrepr == "TestItem({'name': 'John Doe', 'number': 123})"
+        assert itemrepr == "TestItem({'name': u'John Doe', 'number': 123})"
-        i.change_name('other')
+        i.name = u'lala'
-        assert i.field == 'hello'
+        i.field = u'hello'
-class StringField(BaseField):
+class TextField(BaseField):
-            raise ValueError("StringField expects a basestring, got %s" \
+            raise ValueError("TextField expects a basestring, got %s" \
-            name = fields.StringField()
+            name = fields.TextField()
-            name = fields.StringField()
+            name = fields.TextField()
-            names = fields.MultiValuedField(fields.StringField)
+            name = fields.TextField()
-            name = fields.StringField(default='John')
+            name = fields.TextField(default='John')
-            name = fields.StringField()
+            name = fields.TextField()
-            name = fields.StringField()
+            name = fields.TextField()
-            name = fields.StringField()
+            name = fields.TextField()
-            name = fields.StringField()
+            name = fields.TextField()
-    def test_string_field(self):
+    def test_text_field(self):
-            field = fields.StringField()
+            field = fields.TextField()
-        if name in self.fields.keys():
+    def __getattr__(self, name):
-            raise AttributeError(name)
+            except KeyError:
-    # dirty hack to allow downloading pages from unknown domains
+    # XXX: hack to allow downloading pages from unknown domains
-        spiders._alldict[domain] = spider
+        spiders.add_spider(spider)
-class BooleanField(Field):
+class BooleanField(BaseField):
-ansi_date_re = re.compile(r'^\d{4}-\d{1,2}-\d{1,2}$')
+class DateField(BaseField):
-        if not ansi_date_re.search(value):
+        if not self.ansi_date_re.search(value):
-class DateTimeField(Field):
+class DateTimeField(BaseField):
-class DecimalField(Field):
+class DecimalField(BaseField):
-            raise ValueError("This value must be a decimal number.")
+        return decimal.Decimal(value) if value is not None else None
-class FloatField(Field):
+class FloatField(BaseField):
-            raise ValueError("This value must be a float.")
+        return float(value) if value is not None else None
-class IntegerField(Field):
+class IntegerField(BaseField):
-            raise ValueError("This value must be an integer.")
+        return int(value) if value is not None else None
-class StringField(Field):
+class StringField(BaseField):
-        if isinstance(value, basestring):
+        if hasattr(value, '__iter__'):
-        if value is None:
+        elif value is None:
-        raise ValueError("This field must be a string.")
+        else:
-class ItemMeta(type):
+class _ItemMeta(type):
-        for n, v in attrs.items():
+        for n, v in attrs.iteritems():
-    __metaclass__ = ItemMeta
+    __metaclass__ = _ItemMeta
-    def __init__(self):
+    def __init__(self, values=None):
-            return object.__setattr__(self, name, value)
+            return ScrapedItem.__setattr__(self, name, value)
-            self._values[name] = self.fields[name].assign(value)
+            self._values[name] = self.fields[name].to_python(value)
-            return object.__getattribute__(self, name)
+            return ScrapedItem.__getattribute__(self, name)
-        return "%s(%s)" % (self.__class__.__name__, repr(reprdict))
+        values = dict((field, getattr(self, field)) for field in self.fields)
-from scrapy.contrib_exp.newitem import * 
+from scrapy.contrib_exp.newitem import Item, fields
-    name = StringField()
+    name = fields.StringField()
-    summary = StringField()
+    url = fields.StringField()
-    names = MultiValuedField(StringField)
+    names = fields.MultiValuedField(fields.StringField)
-from scrapy.contrib_exp.newitem import *
+from scrapy.contrib_exp.newitem import Item, fields
-            name = StringField()
+            name = fields.StringField()
-            names = MultiValuedField(StringField)
+            name = fields.StringField()
-            name = StringField(default='John')
+            name = fields.StringField(default='John')
-    def test_topython_iter(self):
+    def test_to_python_iter(self):
-            name = StringField()
+            name = fields.StringField()
-            name = StringField()
+            name = fields.StringField()
-        assert i.__repr__() == "TestItem({'name': 'John Doe'})"
+        i.number = '123'
-        f = BaseField()
+        f = fields.BaseField()
-            field = BooleanField()
+            field = fields.BooleanField()
-            field = DateField()
+            field = fields.DateField()
-            field = DateTimeField()
+            field = fields.DateTimeField()
-            field = DecimalField()
+            field = fields.DecimalField()
-        self.assertRaises(ValueError, set_invalid_value)
+        self.assertRaises(decimal.InvalidOperation, set_invalid_value)
-            field = FloatField()
+            field = fields.FloatField()
-            field = IntegerField()
+            field = fields.IntegerField()
-            field = StringField()
+            field = fields.StringField()
-            xpath_result = self.doc.xpathContext.xpathEval(xpath)
+            try:
-        enabled_domains = set(spiders.asdict(include_disabled=False).keys())
+        enabled_domains = set(spiders.asdict().keys())
-        enabled_domains = spiders.asdict(include_disabled=False).keys()
+        enabled_domains = spiders.asdict().keys()
-        enabled_domains = spiders.asdict(include_disabled=False).keys()
+        enabled_domains = spiders.asdict().keys()
-
+        """Parse crawl arguments and return a dict of domains -> list of 
-        return self._enableddict.values()
+    def fromdomain(self, domain_name):
-    def fromurl(self, url, include_disabled=True):
+    def fromurl(self, url):
-            return self._alldict.get(self.force_domain)
+            return self.asdict().get(self.force_domain)
-                return self._alldict[domain]
+            if domain in self.asdict():         # try first locating by domain
-                plist = self.all if include_disabled else self.enabled
+                plist = self.asdict().values()
-        spider = self._alldict.get(self.default_domain)
+        spider = self.asdict().get(self.default_domain)
-    def asdict(self, include_disabled=True):
+    def asdict(self):
-        return self._alldict if include_disabled else self._enableddict
+        return self._spiders
-        self._enabled_spiders_set = self._enabled_spiders()
+        self._spiders = {}
-                self._enableddict[spider.domain_name] = spider
+            self._spiders[spider.domain_name] = spider
-        excluded to avoid inconsistent behaviours.
+        """Reload spiders by trying to discover any spiders added under the
-        pdict = self.asdict(include_disabled=False)
+        pdict = self.asdict()
-        This is an override of twisted.plugin.getPlugin, because we're
+        """This is an override of twisted.plugin.getPlugin, because we're
-
+# Generate dropin.cache for spiders under tests
-from scrapy.utils.url import urljoin_rfc as urljoin
+from scrapy.utils.url import urljoin_rfc
-            redirected_url = urljoin(request.url, response.headers['location'])
+            redirected_url = urljoin_rfc(request.url, response.headers['location'])
-            redirected_url = urljoin(request.url, response.headers['location'])
+            redirected_url = urljoin_rfc(request.url, response.headers['location'])
-            redirected = request.replace(url=urljoin(request.url, url))
+            redirected = request.replace(url=urljoin_rfc(request.url, url))
-from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin
+from scrapy.utils.url import safe_url_string, urljoin_rfc
-            link.url = urljoin(base_url, link.url)
+            link.url = urljoin_rfc(base_url, link.url, response_encoding)
-from scrapy.utils.url import canonicalize_url
+from scrapy.utils.url import canonicalize_url, urljoin_rfc
-            link.url = urlparse.urljoin(base_url, link.url)
+            link.url = urljoin_rfc(base_url, link.url, response.encoding)
-from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin
+from scrapy.utils.url import safe_url_string, urljoin_rfc
-            link.url = urljoin(base_url, link.url)
+            link.url = urljoin_rfc(base_url, link.url, response_encoding)
-from scrapy.utils.url import urljoin_rfc as urljoin
+from scrapy.utils.url import urljoin_rfc
-        clean_url = lambda u: urljoin(base_url, remove_entities(clean_link(u.decode(response_encoding))))
+        clean_url = lambda u: urljoin_rfc(base_url, remove_entities(clean_link(u.decode(response_encoding))))
-from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin, canonicalize_url, url_is_from_any_domain
+from scrapy.utils.url import safe_url_string, urljoin_rfc, canonicalize_url, url_is_from_any_domain
-            link.url = urljoin(base_url, link.url)
+            link.url = urljoin_rfc(base_url, link.url, response_encoding)
-from scrapy.utils.url import is_url
+from scrapy.utils.url import urljoin_rfc
-from scrapy.utils.python import flatten, unicode_to_str
+from scrapy.utils.python import unicode_to_str
-            raw_links = [urlparse.urljoin(base_url, unicode_to_str(rel_url)) for rel_url in raw_links]
+            raw_links = [urljoin_rfc(base_url, unicode_to_str(rel_url), self.response.encoding) for rel_url in raw_links]
-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url
+from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, \
-    return urlparse.urljoin(base, str(ref))
+def urljoin_rfc(base, ref, encoding='utf-8'):
-            dfd.addBoth(self._itemproc_finished, output, response, spider)
+            dfd.addBoth(self._itemproc_finished, output, spider)
-    def _itemproc_finished(self, output, item, response, spider):
+    def _itemproc_finished(self, output, item, spider):
-                    item=item, spider=spider, response=response, exception=output.value)
+                    item=item, spider=spider, exception=output.value)
-                item=item, spider=spider, response=response, output=output)
+                item=item, spider=spider, output=output)
-            import lsprofcalltree
+            from scrapy.xlib import lsprofcalltree
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from BeautifulSoup import BeautifulSoup
+from scrapy.xlib.BeautifulSoup import BeautifulSoup
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from lrucache import LRUCache
+from scrapy.xlib.lrucache import LRUCache
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from ClientForm import ParseFile
+from scrapy.xlib.ClientForm import ParseFile
-from BeautifulSoup import UnicodeDammit
+from scrapy.xlib.BeautifulSoup import UnicodeDammit
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from pydispatch import dispatcher
+from scrapy.xlib.pydispatch import dispatcher
-from BeautifulSoup import BeautifulSoup
+from scrapy.xlib.BeautifulSoup import BeautifulSoup
-            from pydispatch import dispatcher
+            from scrapy.xlib.pydispatch import dispatcher
-            from spidermonkey import Runtime
+            from scrapy.xlib.spidermonkey import Runtime
-    import BeautifulSoup
+    from scrapy.xlib import BeautifulSoup
-from pydispatch import saferef, robustapply, errors
+from scrapy.xlib.pydispatch import saferef, robustapply, errors
-from pydispatch.robustapply import robustApply
+from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, getAllReceivers
-        assert i.field == today
+        d_today = datetime.date.today()
-    def test_sting_field(self):
+        def set_invalid_value():
-                    sender=self.__class__, item=item, spider=spider, exception=output.value)
+                signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, \
-                sender=self.__class__, item=item, spider=spider, output=output)
+            signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, \
-        self.required = required
+    def __init__(self, default=None):
-    def __init__(self, field_type, required=False, default=None):
+    def __init__(self, field_type, default=None):
-        super(MultiValuedField, self).__init__(required, default)
+        super(MultiValuedField, self).__init__(default)
-    FAILURE_SIZE = 1024 # make failures equivalent to 1K responses in size
+    MIN_RESPONSE_SIZE = 1024
-            self.active_size += len(response.body)
+            self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)
-            self.active_size += self.FAILURE_SIZE
+            self.active_size += self.MIN_RESPONSE_SIZE
-            self.active_size -= len(response.body)
+            self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)
-            self.active_size -= self.FAILURE_SIZE
+            self.active_size -= self.MIN_RESPONSE_SIZE
-CONCURRENT_DOMAINS = 8    # number of domains to scrape in parallel
+CONCURRENT_DOMAINS = 8
-    def configure(self, scheduler=None, downloader=None):
+    def configure(self):
-        self.scheduler = scheduler or Scheduler()
+        self.scheduler = load_object(settings['SCHEDULER'])()
-        self.downloader = downloader or Downloader()
+        self.downloader = Downloader()
-                self.downloader.sites[domain].needs_backout() or \
+        if not self.running or self.domain_is_closed(domain) or \
-            "self.scraper.sites[domain].backlog_size",
+            "len(self.scraper.sites[domain].active)",
-from scrapy.utils.misc import load_object, arg_to_iter
+from scrapy.utils.misc import arg_to_iter
-        scrapyengine.configure(scheduler=scheduler)
+        scrapyengine.configure()
-from scrapy.core.exceptions import IgnoreRequest
+from scrapy.utils.defer import defer_result, defer_succeed, parallel
-    def __init__(self, max_backlog_size=5000000):
+    def __init__(self, max_active_size=5000000):
-        self.max_backlog_size = max_backlog_size
+        self.active = set()
-            self.backlog_size += len(response.body)
+            self.active_size += len(response.body)
-            self.backlog_size += self.FAILURE_SIZE
+            self.active_size += self.FAILURE_SIZE
-        self.processing.add(response)
+        self.active.add(response)
-        self.processing.remove(response)
+        self.active.remove(response)
-            self.backlog_size -= len(response.body)
+            self.active_size -= len(response.body)
-            self.backlog_size -= self.FAILURE_SIZE
+            self.active_size -= self.FAILURE_SIZE
-        return not (self.queue or self.processing)
+        return not (self.queue or self.active)
-        return self.backlog_size > self.max_backlog_size
+        return self.active_size > self.max_active_size
-        self.middleware = SpiderMiddlewareManager()
+        self.spidermw = SpiderMiddlewareManager()
-        del self.sites[domain]
+        self.sites.pop(domain)
-        self.scrape_next(spider)
+        self._scrape_next(spider, site)
-        # Process responses in queue
+    def _scrape_next(self, spider, site):
-        dfd.addCallback(self.handle_spider_output, request, spider)
+        dfd.addCallback(self.handle_spider_output, request, response, spider)
-            return self.middleware.scrape_response(self.call_spider, \
+            return self.spidermw.scrape_response(self.call_spider, \
-        return task.coiterate(imap(func, result or []))
+    def handle_spider_output(self, result, request, response, spider):
-    def process_spider_output(self, output, request, spider):
+    def _process_spidermw_output(self, output, request, response, spider):
-        if spider.domain_name in self.engine.closing:
+        domain = spider.domain_name
-            signals.send_catch_log(signal=signals.request_received, request=output, spider=spider)
+            signals.send_catch_log(signal=signals.request_received, request=output, \
-            pass # may be next time.
+            pass
-                    % (type(output).__name__, request), log.WARNING, domain=spider.domain_name)
+            log.msg("Spider must return Request, ScrapedItem or None, got '%s' in %s" % \
-from scrapy.core.exceptions import DropItem, NotConfigured
+from scrapy.core.exceptions import NotConfigured
-        self.domaininfo[domain] = set()
+        pass
-        del self.domaininfo[domain]
+        pass
-        if not self.pipeline or domain not in self.domaininfo:
+    def process_item(self, item, spider):
-        def _next_stage(item):
+        def next_stage(item, stages_left):
-            if not pipeline:
+                'Item pipelines must return a ScrapedItem, got %s' % type(item).__name__
-            d.addCallback(_next_stage)
+            current_stage = stages_left.pop(0)
-        deferred.addBoth(_pipeline_finished)
+        deferred = mustbe_deferred(next_stage, item, self.pipeline[:])
-    def item_passed(self, item, spider, pipe_output):
+    def item_passed(self, item, spider):
-from twisted.internet import defer, reactor
+from twisted.internet import defer, reactor, task
-            signal.signal(signal.SIGUSR1, self.dump_stacktrace)
+            signal.signal(signal.SIGUSR2, self.dump_stacktrace)
-        self.response = None
+        response = None
-            request = Request(url, callback=self._get_response)
+            request = Request(url)
-            request = self.user_ns['request'].replace(callback=self._get_response)
+            request = self.user_ns['request']
-            self.generate_vars(url, self.response, request)
+        response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, request, spider)
-        self.assertEqual(add_or_replace_parameter(url, 'BCat', 'newvalue', is_quoted=True),
+        self.assertEqual(add_or_replace_parameter(url, 'BCat', 'newvalue', url_is_quoted=True),
-    return result
+    return queryparams.get(parameter, [default])[0]
-                  
+
-        nested encoded affiliate URL with direct URL as parameters. For example: 
+        nested encoded affiliate URL with direct URL as parameters. For example:
-        and the URL extraction will fail, current workaround was made in the spider, 
+        and the URL extraction will fail, current workaround was made in the spider,
-        
+
-                        
+
-    return queryparams.get(parameter, [default])[0] 
+    result = queryparams.get(parameter, [default])[0]
-    
+
-    for pair in parameters: 
+    for pair in parameters:
-    
+
-def add_or_replace_parameter(url, name, new_value, sep='&'):
+
-            return scrapyengine.download(request, self.AmazonS3Spider)
+            return scrapyengine.schedule(request, self.AmazonS3Spider)
-    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware': 200,
+    'scrapy.contrib.spidermiddleware.requestlimit.RequestLimitMiddleware': 200,
-        return requests + items
+"""
-            dfd = scrapyengine.schedule(robotsreq, spiders.fromdomain(spiderdomain))
+            dfd = scrapyengine.download(robotsreq, spiders.fromdomain(spiderdomain))
-        return scrapyengine.schedule(request, info.spider)
+        return scrapyengine.download(request, info.spider)
-            return scrapyengine.schedule(request, self.AmazonS3Spider)
+            return scrapyengine.download(request, self.AmazonS3Spider)
-    schd = scrapyengine.schedule(request, spider)
+    schd = scrapyengine.download(request, spider)
-                signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason='shutdown')
+                signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, \
-                self.downloader.sites[domain].needs_backout():
+                self.downloader.sites[domain].needs_backout() or \
-        scraping = domain in self.scraper.sites and bool(self.scraper.sites[domain])
+        scraper_idle = domain in self.scraper.sites and self.scraper.sites[domain].is_idle()
-        return not (pending or downloading or scraping)
+        return scraper_idle and not (pending or downloading)
-        schd.addBoth(self.scraper.scrape, request, spider)
+        schd.addBoth(self.scraper.enqueue_scrape, request, spider)
-            "len(self.scraper.sites[domain])",
+            "len(self.scraper.sites[domain].queue)",
-"""Extract information from pages"""
+"""This module implements the Scraper component which parses responses and
-        self.sites[domain] = set()
+        self.sites[domain] = SiteInfo()
-    def scrape(self, response, request, spider):
+    def enqueue_scrape(self, response, request, spider):
-        dfd = self._scrape(response, request, spider) # returns spiders processed output
+        dfd = self._scrape2(response, request, spider) # returns spiders processed output
-        """Handle the diferent cases of request's result been a Response or a Failure"""
+    def _scrape2(self, request_result, request, spider):
-        stats.incpath("%s/spider_exceptions/%s" % (spider.domain_name, _failure.value.__class__.__name__))
+        stats.incpath("%s/spider_exceptions/%s" % (spider.domain_name, \
-from scrapy.stats import stats
+from scrapy.core.scraper import Scraper
-        self._scraping = {}
+        self.scraper = Scraper(self)
-        return self.scheduler.is_idle() and self.downloader.is_idle() and not self._scraping
+        return self.scheduler.is_idle() and self.downloader.is_idle() and self.scraper.is_idle()
-        scraping = self._scraping.get(domain)
+        scraping = domain in self.scraper.sites and bool(self.scraper.sites[domain])
-        return schd.addErrback(log.err)
+        schd.addBoth(self.scraper.scrape, request, spider)
-        self._scraping[domain] = set()
+        self.scraper.open_domain(domain)
-        del self._scraping[domain]
+        self.scraper.close_domain(domain)
-            "len(self._scraping)",
+            "self.scraper.is_idle()",
-            "len(self._scraping[domain])",
+            "len(self.scraper.sites[domain])",
-from scrapy.utils.defer import mustbe_deferred, defer_result
+from scrapy.utils.misc import load_object
-    def scrape(self, request, response, spider):
+    def scrape_response(self, scrape_func, response, request, spider):
-            return self.call(request, response, spider)
+            return scrape_func(response, request, spider)
-
+# max limit of items to process in parallel
-        return self.scheduler.is_idle() and self.pipeline.is_idle() and self.downloader.is_idle() and not self._scraping
+        return self.scheduler.is_idle() and self.downloader.is_idle() and not self._scraping
-        return not (pending or downloading or haspipe or scraping)
+        return not (pending or downloading or scraping)
-            assert isinstance(response, (Response, Exception)), "Expecting Response or Exception, got %s" % type(response).__name__
+            assert isinstance(response, (Response, Exception)), \
-            assert isinstance(item, ScrapedItem), 'Pipeline stages must return a ScrapedItem or raise DropItem'
+            assert isinstance(item, ScrapedItem), \
-        mws = []
+        """Load middleware defined in settings module"""
-        log.msg("Enabled downloader middlewares: %s" % ", ".join([type(m).__name__ for m in mws]),
+            try:
-        self.mws = []
+        self.enabled.clear()
-        log.msg("Enabled scheduler middlewares: %s" % ", ".join([type(m).__name__ for m in self.mws]),
+            try:
-        mws = []
+        self.enabled.clear()
-        log.msg("Enabled spider middlewares: %s" % ", ".join([type(m).__name__ for m in mws]),
+            try:
-            self._download(site, request, spider, deferred)
+            self._download(site, request, spider).chainDeferred(deferred)
-        # change!
+    def _download(self, site, request, spider):
-        dfd.addBoth(next_request)
+            return _
-import sys, os
+import sys
-templates_path = ['../templates/docs']
+templates_path = ['_templates']
-master_doc = 'index'
+master_doc = 'contents'
-#html_additional_pages = {}
+html_additional_pages = {
-    component = "%s/%s" % (component, domain) if domain else component
+    system = domain if domain else component
-        log.msg(msg_txt, system=component)
+        log.msg(msg_txt, system=system)
-    kwargs['system'] = "%s/%s" % (component, domain) if domain else component
+    kwargs['system'] = domain if domain else component
-import datetime
+from time import time
-        self.lastseen = None
+        self.lastseen = 0
-            penalty = site.download_delay - delta.seconds
+        now = time()
-    DOWNLOAD_PRIORITY = -1
+    DOWNLOAD_PRIORITY = 1000
-    DOWNLOAD_PRIORITY = -1
+    DOWNLOAD_PRIORITY = 1000
-        def _transferred(_):
+        def finish_transferring(_):
-            self.process_queue(spider)
+            return _
-        dfd = mustbe_deferred(download_any, request, spider)
+        # 3. Wait until deferred returned to engine completes
-        dfd.addBoth(_transferred) # request next download after finish processing current response
+
-            self._download(site, request, spider).chainDeferred(deferred)
+            self._download(site, request, spider, deferred)
-    def _download(self, site, request, spider):
+    def _download(self, site, request, spider, deferred):
-        return mustbe_deferred(download_any, request, spider).addBoth(_transferred)
+
-            mustbe_deferred(self.download, request, spider).chainDeferred(deferred)
+            dwld = mustbe_deferred(self.download, request, spider)
-        self._next_request_called = set()
+        self._next_request_pending = set()
-            self._next_request_called.add(domain)
+            self._next_request_pending.discard(domain)
-    """
+            return reactor.callLater(0, self.next_request, domain, now=True)
-
+        self._next_request_called = set()
-    def next_request(self, spider, now=False):
+    def next_request(self, domain, now=False):
-            return reactor.callLater(5, self.next_request, spider)
+            return reactor.callLater(5, self.next_request, domain)
-        domain = spider.domain_name
+            return reactor.callLater(0, self.next_request, domain, now=True)
-                        self.next_request(spider)
+                        self.next_request(domain)
-                self.next_request(spider)
+                self.next_request(domain)
-        self.next_request(spider)
+        self.next_request(domain)
-            self.next_request(spider)
+            self.next_request(domain)
-        self.next_request(spider)
+        self.next_request(domain)
-            self.next_request(spider)
+            self.next_request(domain)
-from twisted.internet import defer, reactor, task
+from twisted.internet import reactor, task
-        reason = self.closing.get(domain, 'finished')
+        reason = self.closing.pop(domain, 'finished')
-            return scd.addBoth(_remove)
+                return _
-        return schd
+        return schd.addErrback(log.err)
-Telnet Console extension 
+Scrapy Telnet Console extension
-See documentation in docs/ref/telnetconsole.rst
+See documentation in docs/topics/telnetconsole.rst
-#TELNETCONSOLE_PORT = 2020  # if not set uses a dynamic port
+TELNETCONSOLE_PORT = 6023  # if None, uses a dynamic port
-from twisted.internet import reactor
+from scrapy.spider import spiders
-                    'p': pprint.pprint}  # useful shortcut for debugging
+# if you add entries here also update topics/telnetconsole.rst
-        #protocol.ServerFactory.__init__(self)
+from itertools import imap
-            return scd
+            def _remove(_):
-                return task.coiterate(cb_spider_output, spmw_result)
+                return task.coiterate(imap(cb_spider_output, spmw_result))
-                        log.msg("Spider must return Request, ScrapedItem or None, got '%s' while processing %s" % (type(output).__name__, request), log.WARNING, domain=domain)
+                        log.msg("Spider must return Request, ScrapedItem or None, got '%s' while processing %s" \
-                return deferred_imap(cb_spider_output, spmw_result)
+                return task.coiterate(cb_spider_output, spmw_result)
-            scd = self.spidermiddleware.scrape(request, response, spider)
+            scd = mustbe_deferred(self.spidermiddleware.scrape, request, response, spider)
-        schd = self.schedule(request, spider)
+        schd = mustbe_deferred(self.schedule, request, spider)
-                return schd
+                schd = mustbe_deferred(self.schedule, newrequest, spider)
-        dwld = self.downloader.fetch(request, spider)
+        dwld = mustbe_deferred(self.downloader.fetch, request, spider)
-        return deferred
+        return dwld
-        self.debug_mode = settings.getbool('ENGINE_DEBUG')
+        self.closing = {} # dict (domain -> reason) of spiders being closed
-            self.open_domain(domain, spider)
+            self.open_domain(domain)
-                response.request = request # tie request to obtained response
+                response.request = request # tie request to response received
-                newrequest = response # proper alias
+                newrequest = response
-    def open_domain(self, domain, spider=None):
+    def open_domain(self, domain):
-        spider = spider or spiders.fromdomain(domain)
+        spider = spiders.fromdomain(domain)
-        self.engine = engine
+    def __init__(self):
-        self.downloader = downloader or Downloader(self)
+        self.downloader = downloader or Downloader()
-
+            self.scheduler.clear_pending_requests(domain)
-        """ 
+    def _finish_closing_domain_if_idle(self, domain):
-        reason = self.closing[domain]
+        reason = self.closing.get(domain, 'finished')
-    def resume(self):
+    def unpause(self):
-                self._domain_idle(domain)
+            mustbe_deferred(self.download, request, spider).chainDeferred(deferred)
-            s += "%-47s : %s\n" % (test, eval(test))
+            try:
-from scrapy.utils.defer import chain_deferred, mustbe_deferred
+from scrapy.utils.defer import mustbe_deferred
-        self.closed = False
+        self.closing = False
-    def capacity(self):
+    def free_transfer_slots(self):
-        return self.outstanding() > (2 * self.max_concurrent_requests)
+        return len(self.queue) > 2 * self.max_concurrent_requests
-            raise IgnoreRequest('Can\'t fetch on a closed domain')
+        if site.closing:
-        while site.queue and site.capacity() > 0:
+        while site.queue and site.free_transfer_slots() > 0:
-        if site.closed and site.is_idle():
+        self._close_if_idle(domain)
-        if not site or site.closed:
+        if not site or site.closing:
-        site.closed = True
+        site.closing = True
-    def next_request(self, spider, breakloop=True):
+    def next_request(self, spider, now=False):
-            return reactor.callLater(0, self.next_request, spider, breakloop=False)
+        # call next_request in next reactor loop by default
-        if domain in self.closing or self.downloader.needs_backout(domain):
+        if not self.running or \
-        downloading = domain in self.downloader.sites and self.downloader.sites[domain].outstanding() > 0
+        downloading = domain in self.downloader.sites and self.downloader.sites[domain].active
-        return domain in self.downloader.sites
+    def domain_is_closed(self, domain):
-            if not self.downloader.domain_is_open(domain):
+            if self.domain_is_closed(domain): # scheduler auto-open
-            "self.downloader.sites[domain].closed",
+            "self.downloader.sites[domain].closing",
-            self._close_domain(domain)
+            self.close_domain(domain, reason='finished')
-        self.downloader.close_domain(domain)
+            self.downloader.close_domain(domain)
-        reason = self.closing.get(domain, 'finished')
+        reason = self.closing[domain]
-        site = self.sites[domain]
+        site = self.sites[spider.domain_name]
-            raise IgnoreRequest('Can\'t fetch on a closed domain: %s' + request)
+            raise IgnoreRequest('Can\'t fetch on a closed domain')
-        # download delay handling
+        # Delay queue processing if a download_delay is configured
-        while site.queue and site.capacity()>0:
+        # Process requests in queue if there are free slots to transfer for this site
-            self._download(site, request, spider, deferred)
+            self._download(site, request, spider).chainDeferred(deferred)
-            self.engine.closed_domain(domain)
+            self.engine.closed_domain(domain) # notify engine.
-    def _download(self, site, request, spider, deferred):
+    def _download(self, site, request, spider):
-        def _finish(_):
+        def _transferred(_):
-        return dwld.addBoth(_finish)
+            return _
-        if status == 'finished' and not stats.getpath("%s/items_sampled" % domain):
+    def domain_closed(self, domain, spider, reason):
-    def domain_closed(self, domain, spider, status):
+    def domain_closed(self, domain, spider, reason):
-        self.cancelled = set() # domains in cancelation state
+        self.closing = {} # domains being closed
-                signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, status='cancelled')
+                signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason='shutdown')
-        if domain in self.cancelled or self.downloader.needs_backout(domain):
+        if domain in self.closing or self.downloader.needs_backout(domain):
-                    if domain in self.cancelled:
+                    if domain in self.closing:
-        self.cancelled.discard(domain)
+        self.closing.pop(domain, None)
-    def close_domain(self, domain):
+    def close_domain(self, domain, reason='cancelled'):
-            self.cancelled.add(domain)
+        if domain not in self.closing:
-        self.cancelled.discard(domain)
+        reason = self.closing.get(domain, 'finished')
-            "domain in self.cancelled",
+            "self.closing.get(domain)",
-# status is a string and its possible values are: "finished" or "cancelled"
+# args: domain, spider, reason
-    def stats_domain_closing(self, domain, spider, status):
+    def stats_domain_closing(self, domain, spider, reason):
-        stats.incpath('_global/domain_count/%s' % status)
+        stats.setpath('%s/finish_status' % domain, 'OK' if reason == 'finished' else reason)
-        dispatcher.send(signal=self.domain_closing, sender=self.__class__, domain=domain, spider=spider, status=status)
+    def _domain_closed(self, domain, spider, reason):
-        dispatcher.send(signal=self.domain_closed, sender=self.__class__, domain=domain, spider=spider, status=status)
+        dispatcher.send(signal=self.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason=reason)
-        self.assertEqual({'domain': session.domain, 'spider': session.spider, 'status': 'finished'},
+        self.assertEqual({'domain': session.domain, 'spider': session.spider, 'reason': 'finished'},
-class SiteDetails(object):
+class SiteInfo(object):
-        self.max_concurrent_requests = max_concurrent_requests if not download_delay else 1
+    def __init__(self, download_delay=None, max_concurrent_requests=None):
-        self.sites[domain] = site
+        self.sites[domain] = SiteInfo(
-        lastseen = scrapyengine.downloader.lastseen(domain)
+        try:
-            active, pending = len(dwl.active_requests(d)), len(dwl.request_queue(d))
+            active = len(dwl.sites[d].active) if d in dwl.sites else 0
-        downloading = not self.downloader.domain_is_idle(domain)
+        downloading = domain in self.downloader.sites and self.downloader.sites[domain].outstanding() > 0
-            "len(self.downloader.active_requests(domain))",
+            "len(self.downloader.sites[domain].queue)",
-        log.msg("Enabled downloader middlewares: %s" % ", ".join([type(m).__name__ for m in mws]))
+        log.msg("Enabled downloader middlewares: %s" % ", ".join([type(m).__name__ for m in mws]),
-        log.msg("Enabled extensions: %s" % ", ".join(extensions.enabled.iterkeys()))
+        log.msg("Enabled extensions: %s" % ", ".join(extensions.enabled.iterkeys()),
-        log.msg("Enabled scheduler middlewares: %s" % ", ".join([type(m).__name__ for m in self.mws]))
+        log.msg("Enabled scheduler middlewares: %s" % ", ".join([type(m).__name__ for m in self.mws]),
-        log.msg("Enabled item pipelines: %s" % ", ".join([type(p).__name__ for p in self.pipeline]))
+        log.msg("Enabled item pipelines: %s" % ", ".join([type(p).__name__ for p in self.pipeline]),
-        log.msg("Enabled spider middlewares: %s" % ", ".join([type(m).__name__ for m in mws]))
+        log.msg("Enabled spider middlewares: %s" % ", ".join([type(m).__name__ for m in mws]),
-        self.downloading = set()
+        self.transferring = set()
-        return not (self.active or self.downloading)
+        return not (self.active or self.transferring)
-        return self.max_concurrent_requests - len(self.downloading)
+        return self.max_concurrent_requests - len(self.transferring)
-        site.downloading.add(request)
+        site.transferring.add(request)
-            site.downloading.remove(request)
+            site.transferring.remove(request)
-        if domain not in self.sites:
+        site = self.sites.get(domain)
-        site = self.sites[domain]
+from scrapy.core.downloader.handlers import download_any
-        return self.middleware.download(request, spider).addBoth(_deactivate)
+        return self.middleware.download(self.enqueue, request, spider).addBoth(_deactivate)
-        dwld = mustbe_deferred(request, spider)
+        dwld = mustbe_deferred(download_any, request, spider)
-    def download(self, request, spider):
+    def download(self, download_func, request, spider):
-            return download_any(request=request, spider=spider)
+            return download_func(request=request, spider=spider)
-                raise IgnoreRequest
+        if site.closed:
-            raise IgnoreRequest('Trying to enqueue %s from closed site %s' % (request, domain))
+        return self.middleware.download(request, spider).addBoth(_deactivate)
-            self._download(request, spider, deferred)
+            self._download(site, request, spider, deferred)
-        site = self.sites.get(domain)
+    def _download(self, site, request, spider, deferred):
-                log.msg('Deactivating %s' % request_info(request), log.DEBUG)
+        def _finish(_):
-        dwld.addBoth(_remove)
+        dwld = mustbe_deferred(request, spider)
-        dwld.addBoth(_finish)
+        return dwld.addBoth(_finish)
-            return
+        if domain in self.sites:
-                log.msg('Domain %s already closed' % domain, log.DEBUG, domain=domain)
+        if domain not in self.sites:
-            return self.download_function(request=request, spider=spider)
+            return download_any(request=request, spider=spider)
-    return d1
+    return d1.chainDeferred(d2).addBoth(lambda _:d2)
-            "len(self.scheduler.pending_domains)",
+            "len(self.scheduler.pending_requests)",
-DUPEFILTER_FILTERCLASS = 'scrapy.dupefilter.SimplePerDomainFilter'
+DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'
-        clspath = settings.get('DUPEFILTER_FILTERCLASS')
+        clspath = settings.get('DUPEFILTER_CLASS')
-            raise IgnoreRequest('Skipped (already seen request)')
+        seen = self.dupefilter.request_seen(domain, request)
-        self.dupefilter.open(domain)
+        self.dupefilter.open_domain(domain)
-        self.dupefilter.close(domain)
+        self.dupefilter.close_domain(domain)
-from scrapy.dupefilter import SimplePerDomainFilter, NullFilter
+from scrapy.contrib.dupefilter import RequestFingerprintDupeFilter, NullDupeFilter
-class SimplePerDomainFilterTest(unittest.TestCase):
+class RequestFingerprintDupeFilterTest(unittest.TestCase):
-        assert domain in filter
+        filter = RequestFingerprintDupeFilter()
-        assert filter.has(domain, r1)
+        assert not filter.request_seen(domain, r1)
-        assert not filter.add(domain, r3)
+        assert not filter.request_seen(domain, r2)
-        assert domain not in filter
+        filter.close_domain(domain)
-class NullFilterTest(unittest.TestCase):
+class NullDupeFilterTest(unittest.TestCase):
-        filter.open(domain)
+        filter = NullDupeFilter()
-        filter.close(domain)
+        assert not filter.request_seen(domain, r1)
-REDIRECT_PRIORITY_ADJUST = -2
+REDIRECT_PRIORITY_ADJUST = +2
-RETRY_PRIORITY_ADJUST = +1
+RETRY_PRIORITY_ADJUST = -1
-        self.pending_requests[domain].push((request, dfd), request.priority)
+        self.pending_requests[domain].push((request, dfd), -request.priority)
-        assert req2.priority < req.priority
+        assert req2.priority > req.priority
-        assert req2.priority > req.priority
+        assert req2.priority < req.priority
-                chain_deferred(schd, redirected.deferred)
+                newrequest = response # proper alias
-
+from scrapy.core.exceptions import IgnoreRequest, NotConfigured
-        added = dupefilter.add(domain, request)
+        added = self.dupefilter.add(domain, request)
-from scrapy.core.scheduler import Scheduler, SchedulerMiddlewareManager
+from scrapy.core.scheduler import Scheduler
-        schd = self.schedulermiddleware.enqueue_request(domain, request)
+            self.scheduler.open_domain(domain)
-        return schd
+        return self.scheduler.enqueue_request(domain, request)
-
+        self.next_request(spider)
-        signals.send_catch_log(signals.domain_open, sender=self.__class__, domain=domain, spider=spider)
+        signals.send_catch_log(signals.domain_open, sender=self.__class__, domain=domain, spider=spider)
-        self.starters.pop(domain, None)
+from collections import defaultdict
-        self.mw_enqueue_request = []
+    def __init__(self):
-        mws = []
+        self.mws = []
-        self.loaded = True
+                else:
-    def enqueue_request(self, domain, request):
+    def _add_middleware(self, mw):
-                result = method(domain=domain, request=request)
+            for mwfunc in self.mw_cbs['enqueue_request']:
-                        (method.im_self.__class__.__name__, result.__class__.__name__)
+                        (mwfunc.im_self.__class__.__name__, result.__class__.__name__)
-            return self.scheduler.enqueue_request(domain=domain, request=request)
+            return wrappedfunc(domain=domain, request=request)
-        being scraped.
+    def close_domain(self, domain):
-from scrapy.dupefilter import dupefilter
+DOMAIN = 'scrapytest.org'
-        dupefilter.open('scrapytest.org')
+        self.mw = DuplicatesFilterMiddleware()
-        dupefilter.close('scrapytest.org')
+        self.mw.close_domain(DOMAIN)
-        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r4)
+        assert not self.mw.enqueue_request(DOMAIN, r1)
-following methods:
+The Domain Scheduler keeps track of next domains to scrape. They must implement
-  add domain to pending domains to scrape
+  add domain to pending queue
-  remove domain from pendings, do nothing if not pending
+  remove (all occurrences) of domain from pending queue, do nothing if not
-  Return ``True`` if the domain is pending, ``False`` otherwise
+  Return ``True`` if the domain is pending to scrape, ``False`` otherwise
-        self.pending_domains.remove(domain)
+        self.pending_domains = [d for d in self.pending_domains if d != domain]
-
+"""
-                if scrapyengine.scheduler.remove_pending_domain(domain):
+                if scrapyengine.domain_scheduler.remove_pending_domain(domain):
-        domain = self.scheduler.next_domain()
+        domain = self.domain_scheduler.next_domain()
-    def crawl(self, request, spider, domain_priority=0):
+    def crawl(self, request, spider):
-        schd = self.schedule(request, spider, domain_priority)
+        schd = self.schedule(request, spider)
-    def schedule(self, request, spider, domain_priority=0):
+    def schedule(self, request, spider):
-            return self._add_starter(request, spider, domain_priority)
+            return self._add_starter(request, spider)
-    def _add_starter(self, request, spider, domain_priority):
+    def _add_starter(self, request, spider):
-            self.scheduler.add_domain(domain, priority=domain_priority)
+        if not self.domain_scheduler.has_pending_domain(domain):
-                scrapyengine.crawl(request, spider, domain_priority=priority)
+                scrapyengine.crawl(request, spider)
-        """Enqueue a request to be downloaded for a domain that is currently being scraped."""
+        """Enqueue a request to be downloaded for a domain that is currently
-        for the domain.
+        ``(None, None)`` is returned if there aren't any request pending for
-from scrapy.spider import BaseSpider
+from scrapy.contrib.spiders.init import InitSpider
-class CrawlSpider(BaseSpider):
+class CrawlSpider(InitSpider):
-from scrapy.spider import BaseSpider
+from scrapy.contrib.spiders.init import InitSpider
-class XMLFeedSpider(BaseSpider):
+class XMLFeedSpider(InitSpider):
-class CSVFeedSpider(BaseSpider):
+class CSVFeedSpider(InitSpider):
-from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_imap
+from scrapy.utils.defer import chain_deferred, deferred_imap
-        return not (pending or downloading or haspipe or oninit or scraping)
+        return not (pending or downloading or haspipe or scraping)
-        dfd.addCallback(_state)
+        signals.send_catch_log(signals.domain_opened, sender=self.__class__, domain=domain, spider=spider)
-from scrapy.utils.misc import load_object
+from scrapy.utils.misc import load_object, arg_to_iter
-                perdomain.setdefault(spider.domain_name, []).append(req)
+                for req in arg_to_iter(spider.make_requests_from_url(url)):
-        return [self.make_request_from_url(url) for url in self.start_urls]
+        reqs = []
-    def make_request_from_url(self, url):
+    def make_requests_from_url(self, url):
-# not a singleton
+from twisted.internet.error import CannotListenError
-                reactor.listenTCP(*args, **kwargs)
+                try:
-        starters = self.starters.get(domain, [])
+        starters = self.starters.pop(domain, [])
-        del self.starters[domain]
+from tempfile import mktemp
-            tar_file = tarfile.open(fileobj=self.archive)
+            tar_file = tarfile.open(name=mktemp(), fileobj=self.archive)
-            tar_file = tarfile.open(name='tar.tmp', fileobj=self.archive)
+            tar_file = tarfile.open(fileobj=self.archive)
-    scripts = ['scrapy/bin/scrapy-admin.py'],
+    scripts = ['bin/scrapy-admin.py'],
-        parser.add_option("--set", dest="settings", action="append", metavar="SETTING:VALUE", default=[], 
+        parser.add_option("--set", dest="settings", action="append", metavar="SETTING=VALUE", default=[],
-                name, val = setting.split(':', 1)
+            if '=' in setting:
-                sys.stderr.write("%s: invalid argument --set=%s - proper format is --set=SETTING:VALUE'\n" % (sys.argv[0], setting))
+                sys.stderr.write("%s: invalid argument --set %s - proper format is --set SETTING=VALUE'\n" % (sys.argv[0], setting))
-    def start(self):
+    def start(self, control_reactor=True):
-            reactor.callWhenRunning(self._mainloop)
+            self.control_reactor = control_reactor
-            reactor.run() # blocking call
+            if control_reactor:
-            if reactor.running:
+            if self.control_reactor and reactor.running:
-        if self.downloader.needs_backout(domain):
+        if domain in self.cancelled or self.downloader.needs_backout(domain):
-        pending = self.scheduler.domain_has_pending(domain)
+        pending = self.scheduler.domain_has_pending_requests(domain)
-            def _onerror(_failure):
+        def _process_response(response):
-            def _bugtrap(_failure):
+            def eb_framework(_failure):
-            scd.addErrback(_bugtrap)
+            scd = self.spidermiddleware.scrape(request, response, spider)
-            request.deferred.errback(_failure) #TODO: merge into spider middleware.
+            request.deferred.errback(_failure) # TODO: merge into spider middleware.
-        schd.addCallbacks(_process, _cleanfailure)
+        schd.addCallbacks(_process_response, _cleanfailure)
-        if not self.scheduler.is_pending(domain):
+        if not self.scheduler.domain_is_pending(domain):
-
+        """Called when a domain gets idle. This function is called when there are no
-
+    def close_domain(self, domain):
-            "len(self.scheduler.pending_domains_count)",
+            "len(self.scheduler.pending_domains)",
-            "self.scheduler.domain_has_pending(domain)",
+            "self.scheduler.domain_has_pending_requests(domain)",
-
+                try:
-            reactor.addSystemEventTrigger('before', 'shutdown', self.stop)
+        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)
-class Scheduler(object) :
+class Scheduler(object):
-    ``pending_domains_count`` contains the names of all domains that are to be scheduled.
+    crawling order. The scheduler schedules websites and requests to be
-        self.pending_domains_count = {}
+        self.pending_domains = set()
-        self.dfo = settings.get('SCHEDULER_ORDER', '').upper() == 'DFO'
+        self.dfo = settings['SCHEDULER_ORDER'].upper() == 'DFO'
-    def is_pending(self, domain):
+    def domain_is_pending(self, domain):
-        return domain in self.pending_domains_count
+        return domain in self.pending_domains
-    def domain_has_pending(self, domain):
+    def domain_has_pending_requests(self, domain):
-        if self.pending_domains_count:
+        if self.pending_domains:
-                self.pending_domains_count[domain] -= 1
+            self.pending_domains.remove(domain)
-
+        """Add a new domain to be scraped, with the given priority. If the
-            self.pending_domains_count[domain] += 1
+        if domain not in self.pending_domains:
-        """Allocates resources for maintaining a schedule for domain."""
+        """Allocates scheduling resources for the given domain"""
-
+    def close_domain(self, domain):
-            log.msg(msg, level=log.WARNING)
+        self.pending_requests.pop(domain, None)
-            return self.pending_domains_count.pop(domain, 0)
+        if domain in self.pending_domains and not self.domain_is_open(domain):
-        scrapymanager.runonce(*args, **opts.__dict__)
+        scrapymanager.runonce(*args)
-        scrapymanager.start(*args, **opts.__dict__)
+        scrapymanager.start(*args)
-        extensions.load()
+        self.control_reactor = True
-        # schedule initial requets to be scraped at engine start
+        # schedule initial requests to be scraped at engine start
-    def runonce(self, *args, **opts):
+    def runonce(self, *args):
-        self.configure(*args, **opts)
+        self.configure()
-    def start(self, **opts):
+    def start(self, control_reactor=True):
-        self.configure(**opts)
+        self.configure(control_reactor)
-        self.stop()
+        scrapyengine.start(control_reactor=control_reactor)
-            signal.signal(signal.SIGBREAK, signal.SIG_IGN)
+        if self.control_reactor:
-        if not self.pipeline:
+        domain = spider.domain_name
-            raise IgnoreRequest('Unable to fetch (domain already closed): %s' % request)
+            if self.debug_mode:
-    log.start() # start logging
+    log.start()
-        import cProfile
+    # set loglevel
-    log_stdout = log_stdout or settings.getbool('LOG_STDOUT')
+    # set log observer
-    started = True
+        file = open(logfile, 'a') if logfile else sys.stderr
-                log.msg('dropping old cached response from %s' % metadata['timestamp'], level=log.DEBUG)
+                log.msg('dropping old cached response from %s' % metadata['timestamp'], level=log.DEBUG, domain=domain)
-            dfd = scrapyengine.schedule(robotsreq, spiders.fromdomain(spiderdomain), priority=self.DOWNLOAD_PRIORITY)
+            robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)
-        return scrapyengine.schedule(request, info.spider, priority=self.DOWNLOAD_PRIORITY)
+        request.priority = self.DOWNLOAD_PRIORITY
-    def enqueue_request(self, domain, request, priority):
+    def enqueue_request(self, domain, request):
-    def enqueue_request(self, domain, request, priority=1):
+    def enqueue_request(self, domain, request):
-            self.pending_requests[domain].put(record, priority)
+            self.pending_requests[domain].push(record, request.priority)
-    def crawl(self, request, spider, priority=0, domain_priority=0):
+    def crawl(self, request, spider, domain_priority=0):
-                    self.crawl(request=item, spider=spider, priority=priority)
+                    self.crawl(request=item, spider=spider)
-        schd = self.schedule(request, spider, priority, domain_priority)
+        schd = self.schedule(request, spider, domain_priority)
-    def schedule(self, request, spider, priority=0, domain_priority=0):
+    def schedule(self, request, spider, domain_priority=0):
-        schd = self.schedulermiddleware.enqueue_request(domain, request, priority)
+        schd = self.schedulermiddleware.enqueue_request(domain, request)
-                schd = self.schedule(redirected, spider, priority=self.REDIRECTION_PRIORITY)
+                redirected.priority = self.REDIRECTION_PRIORITY
-    def enqueue_request(self, domain, request, priority):
+    def enqueue_request(self, domain, request):
-                result = method(domain=domain, request=request, priority=priority)
+                result = method(domain=domain, request=request)
-            return self.scheduler.enqueue_request(domain=domain, request=request, priority=priority)
+            return self.scheduler.enqueue_request(domain=domain, request=request)
-            domain, priority = self.domains_queue.pop()
+            domain = self.domains_queue.pop()[0]
-    def enqueue_request(self, domain, request, priority=0):
+    def enqueue_request(self, domain, request):
-        self.pending_requests[domain].push((request, dfd), priority)
+        self.pending_requests[domain].push((request, dfd), request.priority)
-        except (KeyError, IndexError), ex:
+            return self.pending_requests[domain].pop()[0] # [1] is priority
-                 errback=None):
+                 cookies=None, meta=None, encoding='utf-8', priority=0.0,
-from scrapy.http import Request, Response
+from scrapy.http import Request
-        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r4, 1)
+        assert not mw.enqueue_request(domain, r1)
-This module implements the XmlRpcRequest class which is a more covenient class
+This module implements the XmlRpcRequest class which is a more convenient class
-    def __init__(self, elements):
+    def __init__(self):
-        self.priorities = self.prioritizer_class(requests.keys())
+        self.domainprio = load_object(settings['PRIORITIZER'])()
-            priority = self.priorities.get_priority(domain) 
+            priority = self.domainprio.get_priority(domain)
-        Reload all enabled spiders except for the ones that are currently
+        """Reload all enabled spiders except for the ones that are currently
-                perdomain.setdefault(spider.domain_name, []).extend(reqs)
+                req = spider.make_request_from_url(url)
-        self.elements.sort()
+        return random.randrange(0, 1000)
-        otherwise None if the entire domain was requested for scraping.
+    def start_requests(self):
-        return [Request(url, callback=self.parse, dont_filter=True) for url in urls]
+    def make_request_from_url(self, url):
-from scrapy.item import ScrapedItem
+from scrapy.http import Request
-        return ifilter(filter, result or ())
+        return (x for x in result if not isinstance(x, Request) or url_is_from_spider(x.url, spider))
-from scrapy.http import Request
+from itertools import ifilter
-        return (r for r in result or () if _filter(r))
+    def process_spider_output(self, response, result, spider):
-            raise ValidationError('A guid is required')
+        raise NotImplemented
-            return self._validate_output(request, result)
+            return self._validate_output(request, result, spider)
-    def _validate_output(self, request, result):
+    def _validate_output(self, request, result, spider):
-                        % (r, request), level=log.WARNING)
+                        % (r, request), level=log.WARNING, domain=spider.domain_name)
-            return result
+
-        return None
+    return hasattr(possible_iterator, '__iter__')
-            return result
+            return self.call(request, response, spider)
-        defer_result(response).chainDeferred(request.deferred)
+    def call(self, request, result, spider):
-        if isinstance(response, Response):
+        if isinstance(response, Response) and response.body:
-    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware': 200,
+    'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,
-from scrapy.core.exceptions import HttpException
+from scrapy.core.exceptions import IgnoreRequest
-            raise HttpException(status, None, response)
+            raise IgnoreRequest(response_status_message(status))
-from scrapy.core.exceptions import HttpException, IgnoreRequest
+            if not result:
-from scrapy.core.exceptions import IgnoreRequest, HttpException, DontCloseDomain
+from scrapy.core.exceptions import IgnoreRequest, DontCloseDomain
-                log.msg("Crawled %s from <%s>" % (response, request.headers.get('referer')), level=log.DEBUG, domain=domain)
+                log.msg("Crawled %s from <%s>" % (response, referer), level=log.DEBUG, domain=domain)
-            errmsg = str(ex) if isinstance(ex, HttpException) else str(_failure)
+            errmsg = str(_failure) if not isinstance(ex, IgnoreRequest) else _failure.getErrorMessage()
-            msg = 'Image (empty): Empty %s (no content) in %s referred in <%s>: no-content' \
+        if response.status != 200:
-        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % (status, mtype, request, referer)
+        status = 'cached' if 'cached' in response.flags else 'downloaded'
-                % (self.MEDIA_NAME, request, referer, errmsg)
+        msg = 'Image (unknow-error): Error downloading %s from %s referred in <%s>: %s' \
-            return {'checksum': checksum, 'last_modified': modified_stamp}
+            if response.status == 200:
-REDIRECTMIDDLEWARE_MAX_TIMES = 20 # uses Firefox default setting
+REDIRECT_MAX_METAREFRESH_DELAY = 100
-RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']
+RETRY_HTTP_CODES = ['500', '503', '504', '400', '408']
-
+from scrapy.utils.response import response_status_message
-                return exception.response
+    """This middleware filters out responses with status code others than 2XX
-from scrapy.core.exceptions import NotConfigured, HttpException, IgnoreRequest
+from scrapy.core.exceptions import NotConfigured, IgnoreRequest
-        if isinstance(response, Response) and not response.meta.get('cached'):
+        if is_cacheable(request):
-            self.cache.store(domain, key, request, response)
+            self.cache.store(spider.domain_name, key, request, response)
-        self.max_redirect_times = settings.getint('REDIRECTMIDDLEWARE_MAX_TIMES')
+    """Handle redirection of requests based on response status and meta-refresh html tag"""
-            return
+    def __init__(self):
-        if status in [302, 303] and 'Location' in response.headers:
+        if response.status in [302, 303] and 'Location' in response.headers:
-            return self._redirect(redirected, request, spider, status)
+            return self._redirect(redirected, request, spider, response.status)
-        if status in [301, 307] and 'Location' in response.headers:
+        if response.status in [301, 307] and 'Location' in response.headers:
-            return self._redirect(redirected, request, spider, status)
+            return self._redirect(redirected, request, spider, response.status)
-        if url and int(interval) < META_REFRESH_MAXSEC:
+        if url and int(interval) < self.max_metarefresh_delay:
-  a HTTP download
+from twisted.web.client import PartialDownloadError
-from scrapy.core.exceptions import HttpException
+from scrapy.utils.response import response_status_message
-                           ConnectionLost)
+                           ConnectionLost, PartialDownloadError)
-                        and (int(exception.status) in self.retry_http_codes)):
+        if isinstance(exception, self.EXCEPTIONS_TO_RETRY):
-            retries = request.meta.get('retry_times', 0) + 1
+    def _retry(self, request, reason, spider):
-                        domain=spider.domain_name, level=log.DEBUG)
+        if retries <= self.max_retry_times:
-from scrapy.core.exceptions import HttpException, NotConfigured
+from scrapy.core.exceptions import NotConfigured
-            
+
-        
+
-from scrapy.core.exceptions import HttpException, NotSupported
+from scrapy.core.exceptions import NotSupported
-    factory.deferred.addCallbacks(_on_success)
+    factory.deferred.addCallbacks(_create_response)
-        exc = HttpException('301', None, rsp)
+        rsp = Response(url, headers={'Location': url2}, status=301)
-        req2 = self.mw.process_exception(req, exc, self.spider)
+        req2 = self.mw.process_response(req, rsp, self.spider)
-        assert self.mw.process_exception(req, exc, self.spider) is None
+        assert self.mw.process_response(req, rsp, self.spider) is rsp
-        exc = HttpException('302', None, rsp)
+        rsp = Response(url, headers={'Location': url2}, status=302)
-        req2 = self.mw.process_exception(req, exc, self.spider)
+        req2 = self.mw.process_response(req, rsp, self.spider)
-        assert self.mw.process_exception(req, exc, self.spider) is None
+        assert self.mw.process_response(req, rsp, self.spider) is rsp
-        exc = HttpException('302', None, Response('http://www.scrapytest.org/302', headers={'Location': '/redirected'}))
+        rsp = Response('http://scrapytest.org/302', headers={'Location': '/redirected'}, status=302)
-        req = self.mw.process_exception(req, exc, self.spider)
+        req = self.mw.process_response(req, rsp, self.spider)
-        req = self.mw.process_exception(req, exc, self.spider)
+        req = self.mw.process_response(req, rsp, self.spider)
-        exc = HttpException('302', None, Response('http://www.scrapytest.org/302', headers={'Location': '/redirected'}))
+        rsp = Response('http://www.scrapytest.org/302', headers={'Location': '/redirected'}, status=302)
-        req = self.mw.process_exception(req, exc, self.spider)
+        req = self.mw.process_response(req, rsp, self.spider)
-        req = self.mw.process_exception(req, exc, self.spider)
+        req = self.mw.process_response(req, rsp, self.spider)
-        exc404 = HttpException('404', None, Response('http://www.scrapytest.org/404', body=''))
+    def test_404(self):
-        self.assertTrue(req is None)
+        assert self.mw.process_response(req, rsp, self.spider) is rsp
-        self._test_retry_exception(req503, exc503)
+    def test_503(self):
-    def test_process_exception_twistederrors(self):
+        # first retry
-        del self.worker.crawlers[self.pid]
+        self.worker.crawlers.pop(self.pid, None)
-class CrawlDebug(object):
+class DebugMiddleware(object):
-        log.msg("Crawling %s" % repr(request), domain=spider.domain_name, level=log.DEBUG)
+        log.msg("process_request %r" % request, domain=spider.domain_name, level=log.DEBUG)
-        log.msg("Crawl exception %s in %s" % (exception, repr(request)), domain=spider.domain_name, level=log.DEBUG)
+        log.msg("process_exception %s in %r" % (exception, request), domain=spider.domain_name, level=log.DEBUG)
-        log.msg("Fetched %s from %s" % (response, repr(request)), domain=spider.domain_name, level=log.DEBUG)
+        log.msg("process_response %s from %r" % (response, request), domain=spider.domain_name, level=log.DEBUG)
-    'scrapy.contrib.downloadermiddleware.common.CommonMiddleware': 550,
+    'scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware': 550,
-"""
+import warnings
-from scrapy.conf import settings
+from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware
-class CommonMiddleware(object):
+class CommonMiddleware(DefaultHeadersMiddleware):
-
+        warnings.warn("scrapy.contrib.downloadermiddleware.common.CommonMiddleware has been replaced by scrapy.contrib.downloadermiddleware.defaultheaders.DefaultHeadersMiddleware")
-#CACHE2_SECTORIZE = 1         # sectorize domains to distribute storage among servers
+#HTTPCACHE_DIR = '/tmp/cache2'  # if set, enables HTTP cache
-    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware': 900,
+    'scrapy.contrib.downloadermiddleware.cache.HttpCacheMiddleware': 900,
-from __future__ import with_statement
+# FIXME: code below is for backwards compatibility and should be removed before
-from pydispatch import dispatcher
+import warnings
-from scrapy.conf import settings
+from scrapy.contrib.downloadermiddleware.httpcache import HttpCacheMiddleware
-        dispatcher.connect(self.open_domain, signal=signals.domain_open)
+class CacheMiddleware(HttpCacheMiddleware):
-                f.write(request.body)
+    def __init__(self, *args, **kwargs):
-    """ Test connection wrapper """
+class MySqlTestCase(unittest.TestCase):
-class ProductComparisonTestCase(unittest.TestCase):
+class ProductComparisonTestCase(MySqlTestCase):
-
+import os
-        self.secret_key = settings['AWS_SECRET_ACCESS_KEY']
+        self.access_key = settings['AWS_ACCESS_KEY_ID'] or os.environ.get('AWS_ACCESS_KEY_ID')
-                "Enter a valid date in YYYY-MM-DD format.")
+            raise ValueError("Enter a valid date in YYYY-MM-DD format.")
-            raise FieldValueError("Invalid date: %s" % str(e))
+            raise ValueError("Invalid date: %s" % str(e))
-                raise FieldValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')
+                raise ValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')
-                    raise FieldValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')
+                    raise ValueError('Enter a valid date/time in YYYY-MM-DD HH:MM[:ss[.uuuuuu]] format.')
-            raise FieldValueError("This value must be a decimal number.")
+            raise ValueError("This value must be a decimal number.")
-            raise FieldValueError("This value must be a float.")
+            raise ValueError("This value must be a float.")
-            raise FieldValueError("This value must be an integer.")
+            raise ValueError("This value must be an integer.")
-        raise FieldValueError("This field must be a string.")
+        raise ValueError("This field must be a string.")
-           'FloatField', 'IntegerField', 'StringField']
+__all__ = ['MultiValuedField', 'BooleanField', 'DateField', 'DateTimeField',
-class Field(object):
+class BaseField(object):
-            return self.to_python(value)
+        return self.to_python(value)
-class MultiValuedField(Field):
+class MultiValuedField(BaseField):
-from scrapy.contrib_exp.newitem.fields import Field
+from scrapy.contrib_exp.newitem.fields import BaseField
-            if isinstance(v, Field):
+            if isinstance(v, BaseField):
-        raise FieldValueError("This value must be either True or False.")
+        return bool(value)
-CLUSTER_MASTER_CACHEFILE = ""
+CLUSTER_MASTER_STATEFILE = ""
-
+from traceback import format_exc
-levels = {
+SILENT, CRITICAL, ERROR, WARNING, INFO, DEBUG, TRACE = range(7)
-    """ Init logging """
+    """Initialize and start logging facility"""
-    setattr(sys.modules[__name__], 'started', True)
+    log_level = level
-    component = "%s/%s" % (BOT_NAME, domain) if domain else component
+    """Log message according to the level"""
-        msg_txt = unicode_to_str("%s: %s" % (levels[level], message))
+        msg_txt = unicode_to_str("%s: %s" % (level_names[level], message))
-    domain = kwargs.pop('domain', '')
+    domain = kwargs.pop('domain', None)
-    kwargs['system'] = "%s/%s" % (BOT_NAME, domain) if domain else component
+    kwargs['system'] = "%s/%s" % (component, domain) if domain else component
-    utf8body = body_as_utf8(response)
+    utf8body = body_as_utf8(response)
-
+"""
-and scrapy.link.extractors.
+This module implements the HtmlImageLinkExtractor for extracting 
-from scrapy.link.extractors import RegexLinkExtractor
+from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
-            Rule(RegexLinkExtractor(allow_domains=(domain_name,)), self.parse_note, follow=True),
+            Rule(SgmlLinkExtractor(allow_domains=(domain_name,)), self.parse_note, follow=True),
-# not a singleton
+# not a singleton
-from scrapy.contrib.link_extractors import HTMLImageLinkExtractor
+from scrapy.contrib.linkextractors.image import HTMLImageLinkExtractor
-LinkExtractor provides en efficient way to extract links from pages
+This module defines the Link object used in Link extractors.
-See documentation in docs/ref/link-extractors.rst
+For actual link extractors implementation see scrapy.contrib.linkextractor, or
-    Link objects represent an extracted link by the LinkExtractor.
+    """Link objects represent an extracted link by the LinkExtractor.
-(scrapy.link.LinkExtractor) with some additional useful features.
+# FIXME: code below is for backwards compatibility and should be removed before
-"""
+import warnings
-import re
+from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
-from scrapy.utils.misc import arg_to_iter
+class RegexLinkExtractor(SgmlLinkExtractor):
-_re_type = type(re.compile("", 0))
+    def __init__(self, *args, **kwargs):
-from scrapy.contrib.link_extractors import HTMLImageLinkExtractor
+from scrapy.link import Link
-        lx = LinkExtractor()  # default: tag=a, attr=href
+        lx = BaseSgmlLinkExtractor()  # default: tag=a, attr=href
-        lx = LinkExtractor()  # default: tag=a, attr=href
+        lx = BaseSgmlLinkExtractor()  # default: tag=a, attr=href
-        lx = LinkExtractor()
+        lx = BaseSgmlLinkExtractor()
-        lx = LinkExtractor()
+        lx = BaseSgmlLinkExtractor()
-class RegexLinkExtractorTestCase(unittest.TestCase):
+class SgmlLinkExtractorTestCase(unittest.TestCase):
-        body = get_testdata('link_extractor', 'regex_linkextractor.html')
+        body = get_testdata('link_extractor', 'sgml_linkextractor.html')
-        lx = RegexLinkExtractor()
+        lx = SgmlLinkExtractor()
-        lx = RegexLinkExtractor()
+        lx = SgmlLinkExtractor()
-        lx = RegexLinkExtractor(allow=('sample', ))
+        lx = SgmlLinkExtractor(allow=('sample', ))
-        lx = RegexLinkExtractor(allow=('sample', ), unique=False)
+        lx = SgmlLinkExtractor(allow=('sample', ), unique=False)
-        lx = RegexLinkExtractor(allow=('sample', ), deny=('3', ))
+        lx = SgmlLinkExtractor(allow=('sample', ), deny=('3', ))
-        lx = RegexLinkExtractor(allow_domains=('google.com', ))
+        lx = SgmlLinkExtractor(allow_domains=('google.com', ))
-        lx = RegexLinkExtractor(tags=('img', ), attrs=('src', ))
+        lx = SgmlLinkExtractor(tags=('img', ), attrs=('src', ))
-        lx = RegexLinkExtractor(allow='sample')
+        lx = SgmlLinkExtractor(allow='sample')
-        lx = RegexLinkExtractor(allow='sample', deny='3')
+        lx = SgmlLinkExtractor(allow='sample', deny='3')
-        lx = RegexLinkExtractor(allow_domains='google.com')
+        lx = SgmlLinkExtractor(allow_domains='google.com')
-        lx = RegexLinkExtractor(deny_domains='example.com')
+        lx = SgmlLinkExtractor(deny_domains='example.com')
-        lx = RegexLinkExtractor(allow=(r'stuff1', ))
+        lx = SgmlLinkExtractor(allow=(r'stuff1', ))
-        lx = RegexLinkExtractor(deny=(r'uglystuff', ))
+        lx = SgmlLinkExtractor(deny=(r'uglystuff', ))
-        lx = RegexLinkExtractor(allow_domains=('evenmorestuff.com', ))
+        lx = SgmlLinkExtractor(allow_domains=('evenmorestuff.com', ))
-        lx = RegexLinkExtractor(deny_domains=('lotsofstuff.com', ))
+        lx = SgmlLinkExtractor(deny_domains=('lotsofstuff.com', ))
-        lx = RegexLinkExtractor(allow=('blah1', ), deny=('blah2', ),
+        lx = SgmlLinkExtractor(allow=('blah1', ), deny=('blah2', ),
-        lx = RegexLinkExtractor(restrict_xpaths=('//div[@id="subwrapper"]', ))
+        lx = SgmlLinkExtractor(restrict_xpaths=('//div[@id="subwrapper"]', ))
-        lx = RegexLinkExtractor(restrict_xpaths="//div[@class='links']") 
+        lx = SgmlLinkExtractor(restrict_xpaths="//div[@class='links']") 
-        lx = RegexLinkExtractor(process_value=process_value)
+        lx = SgmlLinkExtractor(process_value=process_value)
-from scrapy.link import LinkExtractor
+from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
-        xlink = LinkExtractor()
+        xlink = SgmlLinkExtractor()
-To run all Scrapy unittests type: 
+To run all Scrapy unittests go to Scrapy main dir and type: 
-    python -m scrapy.tests.run
+    bin/runtests.sh
-modules available like MySQLdb or simplejson.
+modules available like MySQLdb or simplejson, but that's not a problem.
-    return open(path).read()
+    return open(path, 'rb').read()
-    import os
+    import os, sys
-    suite = loader.loadByNames(['scrapy'], recurse=True)
+    suite = loader.loadByNames(tests_to_run, recurse=True)
-from scrapy.link import Link
+from scrapy.tests import get_testdata
-        body = file(sample_filename).read()
+        body = get_testdata('adaptors', sample_filename)
-import os
+from scrapy.tests import get_testdata
-
+    uncompressed_body = get_testdata('compressed', 'feed-sample1.xml')
-        fd.close()
+        body = get_testdata('compressed', 'feed-sample1.' + format)
-SAMPLEDIR = join(abspath(dirname(__file__)), 'sample_data/compressed')
+SAMPLEDIR = join(tests_datadir, 'compressed')
-    root_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), "sample_data", "test_site")
+    root_dir = os.path.join(tests_datadir, "test_site")
-import os
+from scrapy.tests import get_testdata
-        body = open(os.path.join(base_path, 'linkextractor_noenc.html'), 'r').read()
+        body = get_testdata('link_extractor', 'linkextractor_noenc.html')
-        body = open(os.path.join(base_path, 'linkextractor_latin1.html'), 'r').read()
+        body = get_testdata('link_extractor', 'linkextractor_latin1.html')
-        body = open(os.path.join(base_path, 'regex_linkextractor.html'), 'r').read()
+        body = get_testdata('link_extractor', 'regex_linkextractor.html')
-        body = open(os.path.join(base_path, 'image_linkextractor.html'), 'r').read()
+        body = get_testdata('link_extractor', 'image_linkextractor.html')
-
+        body = get_testdata('feeds', 'feed-sample3.csv')
-
+        body = get_testdata('feeds', 'feed-sample3.csv').replace(',', '\t')
-        sample = open(self.sample_feed_path).read().splitlines()
+        sample = get_testdata('feeds', 'feed-sample3.csv').splitlines()
-        body = open(self.sample_feed_path).read()
+        body = get_testdata('feeds', 'feed-sample3.csv')
-        body = open(self.sample_feed_path).read()
+        body = get_testdata('feeds', 'feed-sample3.csv')
-        body2 = open(self.sample_feed3_path).read()
+        body1 = get_testdata('feeds', 'feed-sample4.csv')
-class UtilsXmlTestCase(unittest.TestCase):
+class UtilsIteratorsTestCase(unittest.TestCase):
-    def test_iterator(self):
+    def test_xmliter(self):
-    def test_iterator_text(self):
+    def test_xmliter_text(self):
-    def test_iterator_namespaces(self):
+    def test_xmliter_namespaces(self):
-    def test_iterator_exception(self):
+    def test_xmliter_exception(self):
-    def test_iterator_defaults(self):
+    def test_csviter_defaults(self):
-    def test_iterator_delimiter(self):
+    def test_csviter_delimiter(self):
-    def test_iterator_headers(self):
+    def test_csviter_headers(self):
-    def test_iterator_falserow(self):
+    def test_csviter_falserow(self):
-    def test_iterator_exception(self):
+    def test_csviter_exception(self):
-    def test_iterator_encoding(self):
+    def test_csviter_encoding(self):
-            response = TextResponse(url=None, body=unicode_to_str(text))
+            response = TextResponse(url=None, body=unicode_to_str(text), encoding='utf-8')
-from scrapy.utils.misc import memoize
+from scrapy.utils.misc import memoize, arg_to_iter
-        guids = [i.guid for i in items if isinstance(i, ScrapedItem)]
+        guids = [i.guid for i in arg_to_iter(items) if isinstance(i, ScrapedItem)]
-DOWNLOADER_MIDDLEWARES = [
+DOWNLOADER_MIDDLEWARES = {}
-    'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',
+    'scrapy.contrib.downloadermiddleware.robotstxt.RobotsTxtMiddleware': 100,
-]
+}
-        ]
+SCHEDULER_MIDDLEWARES = {}
-SPIDER_MIDDLEWARES = [
+SPIDER_MIDDLEWARES = {}
-    'scrapy.contrib.spidermiddleware.depth.DepthMiddleware',
+    'scrapy.contrib.itemsampler.ItemSamplerMiddleware': 100,
-]
+}
-        for mwpath in settings.getlist('DOWNLOADER_MIDDLEWARES') or ():
+        mwlist = build_middleware_list(settings['DOWNLOADER_MIDDLEWARES_BASE'],
-        for mwpath in settings.getlist('SCHEDULER_MIDDLEWARES') or ():
+        mwlist = build_middleware_list(settings['SCHEDULER_MIDDLEWARES_BASE'],
-        for mwpath in settings.getlist('SPIDER_MIDDLEWARES') or ():
+        mwlist = build_middleware_list(settings['SPIDER_MIDDLEWARES_BASE'],
-        if status in [302, 303]:
+        if status in [302, 303] and 'Location' in response.headers:
-        if status in [301, 307]:
+        if status in [301, 307] and 'Location' in response.headers:
-from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, remove_escape, unquote
+from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, replace_escape, unquote
-from scrapy.utils.markup import replace_tags, remove_escape_chars, unquote_markup
+from scrapy.utils.markup import replace_tags, replace_escape_chars, unquote_markup
-def remove_escape(which_ones=('\n','\t','\r'), replace_by=u''):
+def replace_escape(which_ones=('\n', '\t', '\r'), replace_by=u''):
-
+    def _replace_escape(value):
-from scrapy.utils.markup import remove_tags_with_content, remove_escape_chars, remove_tags
+from scrapy.utils.markup import remove_tags_with_content, replace_escape_chars, remove_tags
-    def test_remove_escape_chars(self):
+    def test_replace_escape_chars(self):
-        assert isinstance(remove_escape_chars('no ec', which_ones=('\n','\t',)), unicode)
+        assert isinstance(replace_escape_chars('no ec'), unicode)
-        self.assertEqual(remove_escape_chars(u'no ec', which_ones=('\n',)), u'no ec')
+        self.assertEqual(replace_escape_chars(u'no ec'), u'no ec')
-        self.assertEqual(remove_escape_chars(u'escape\tchars\n', replace_by=' '), 'escape chars ')
+        self.assertEqual(replace_escape_chars(u'escape\n\n'), u'escape')
-def remove_escape_chars(text, which_ones=('\n','\t','\r'), replace_by=u''):
+def replace_escape_chars(text, which_ones=('\n','\t','\r'), replace_by=u''):
-def remove_escape(which_ones=('\n','\t','\r'), replace_str=u''):
+def remove_escape(which_ones=('\n','\t','\r'), replace_by=u''):
-    string, else they're removed.
+    If `replace_by` is given, escape characters are replaced by that
-        return remove_escape_chars(value, which_ones, replace_str)
+        return remove_escape_chars(value, which_ones, replace_by)
-def remove_escape_chars(text, which_ones=('\n','\t','\r'), replace_str=u''):
+def remove_escape_chars(text, which_ones=('\n','\t','\r'), replace_by=u''):
-        text = text.replace(ec, replace_str)
+        text = text.replace(ec, str_to_unicode(replace_by))
-from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, unquote
+from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, remove_escape, unquote
-from scrapy.utils.markup import replace_tags, unquote_markup
+from scrapy.utils.markup import replace_tags, remove_escape_chars, unquote_markup
-def remove_escape_chars(text, which_ones=('\n','\t','\r')):
+def remove_escape_chars(text, which_ones=('\n','\t','\r'), replace_str=u''):
-        text = text.replace(ec, u'')
+        text = text.replace(ec, replace_str)
-    def __init__(self, tag="a", attr="href", unique=False):
+    def __init__(self, tag="a", attr="href", unique=False, process_value=None):
-                    self.current_link = link
+                    url = self.process_value(value)
-                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):
+                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True, process_value=None):
-        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func, unique=unique)
+        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func, 
-(scrapy.link.LinkExtractor) with some addutional useful features.
+(scrapy.link.LinkExtractor) with some additional useful features.
-from scrapy.utils.python import FixedSGMLParser, unique as unique_list
+from scrapy.utils.python import FixedSGMLParser, unique as unique_list, str_to_unicode
-            link.text = link.text.decode(response_encoding)
+            link.text = str_to_unicode(link.text, response_encoding)
-
+    def test_restrict_xpaths(self):
-        scrapyengine.removetask(self.tasks[domain])
+        if domain in self.tasks:
-            c, "http://www.acme.com/foo%2f%25/<<%0anew/",
+            c, "http://www.acme.com/foo%2f%25/<<%0anew\xc3\xa5/\xc3\xa6\xc3\xb8\xc3\xa5",
-            c, "http://www.acme.com/foo/%25/<<%0anew/")
+            c, "http://www.acme.com/foo/%25/<<%0anew\xc3\xa5/\xc3\xa6\xc3\xb8\xc3\xa5")
-        cookie = interact_2965(c, u"http://www.acme.com/\xfc")
+        ## NOTE: this test is commented because it doesn't really test anything
-            nodes = XmlXPathSelector(response).x('//%s' % self.itertag)
+            selector = XmlXPathSelector(response)
-            nodes = HtmlXPathSelector(response).x('//%s' % self.itertag)
+            selector = HtmlXPathSelector(response)
-        ua = getattr(spider, 'user_agent', self.default_useragent)
+        ua = getattr(spider, 'user_agent', None) or self.default_useragent
-                    log.msg("Scraped %s in <%s>" % (item, request.url), log.DEBUG, domain=domain)
+                    log.msg("Scraped %s in <%s>" % (item, request.url), log.INFO, domain=domain)
-        factory = client.HTTPClientFactory(url)
+        factory = client.ScrapyHTTPClientFactory(url)
-        self.assertEquals(factory.response_headers['content-length'][0], '10')
+        self.assertEquals(factory.response_headers['content-length'], '10')
-default_agent = settings.get('USER_AGENT')
+    timeout = getattr(spider, "download_timeout", None) or default_timeout
-                                postdata=request.body or None, # see http://dev.scrapy.org/ticket/60
+                                body=request.body or None, # see http://dev.scrapy.org/ticket/60
-                                followRedirect=False)
+                                timeout=timeout)
-        return _create_response(body)
+        response = _create_response(body)
-    factory.deferred.addCallbacks(_on_success, _on_error)
+    factory.deferred.addCallbacks(_on_success)
-from twisted.web import error
+from twisted.web.client import HTTPClientFactory, PartialDownloadError
-def _parse(url, defaultPort=None):
+
-    return scheme, host, port, path
+        parsed = Url(url.strip()).parsedurl
-    failed = 0
+    if port is None:
-    _specialHeaders = set(('host', 'user-agent', 'content-length'))
+    return scheme, host, port, path
-        self.sendHeader('User-Agent', headers.get('User-Agent', self.factory.agent))
+class ScrapyHTTPPageGetter(HTTPClient):
-            self.sendHeader("Content-Length", str(len(data)))
+    def connectionMade(self):
-            if key.lower() not in self._specialHeaders:
+        # Method command
-            self.transport.write('%s: %s\r\n' % (name, v))
+        # Body
-            self.factory.noPage(reason)
+        HTTPClient.connectionLost(self, reason)
-                PartialDownloadError(self.status, self.message, response)))
+                PartialDownloadError(self.factory.status, None, response)))
-    cookies.
+    result.
-    def setURL(self, url):
+    def __init__(self, url, method='GET', body=None, headers=None, timeout=0):
-        self.path = path
+        self.method = method
-        """
+
-from twisted.web import server, static
+from twisted.web import server, static, error, util
-        f = ScrapyHTTPClientFactory(Url(url))
+        f = client.ScrapyHTTPClientFactory(Url(url))
-        self.assertEquals(path, "/")
+        tests = (
-            request.headers.setdefault('User-Agent', spider.user_agent)
+    default_useragent = settings.get('USER_AGENT')
-from scrapy.utils.cookies import CookieJar
+from scrapy.http.cookies import CookieJar
-from scrapy.utils.cookies import WrappedRequest, WrappedResponse, CookieJar, DefaultCookiePolicy
+from scrapy.http.cookies import WrappedRequest, WrappedResponse, CookieJar, DefaultCookiePolicy
-class WrapperRequestResponse(TestCase):
+class CookielibWrappersTest(TestCase):
-            return
+            return response
-
+        assert self.mw.process_request(req, self.spider) is None
-        self.mw.process_response(req, res, self.spider)
+        assert self.mw.process_response(req, res, self.spider) is res
-        self.mw.process_request(req2, self.spider)
+        assert self.mw.process_request(req2, self.spider) is None
-        self.mw.process_request(req, self.spider)
+        assert self.mw.process_request(req, self.spider) is None
-        self.mw.process_exception(req, exc, self.spider)
+        headers = {'Set-Cookie': 'C1=value1; path=/'}
-        self.mw.process_request(req2, self.spider)
+        assert self.mw.process_request(req2, self.spider) is None
-        self.mw.process_response(req, res, self.spider)
+        assert self.mw.process_response(req, res, self.spider) is res
-        self.mw.process_request(req, self.spider)
+        assert self.mw.process_request(req, self.spider) is None
-        self.mw.process_response(req, res, self.spider)
+        assert self.mw.process_response(req, res, self.spider) is res
-        self.mw.process_request(req, self.spider)
+        assert self.mw.process_request(req, self.spider) is None
-        self.mw.process_request(req, self.spider)
+        assert self.mw.process_request(req, self.spider) is None
-        self.mw.process_response(req, res, self.spider)
+        assert self.mw.process_response(req, res, self.spider) is res
-        self.mw.process_request(req2, self.spider)
+        assert self.mw.process_request(req2, self.spider) is None
-        self.jar._cookies_jar = _DummyLock()
+        self.jar._cookies_lock = _DummyLock()
-from scrapy.contrib_exp.downloadermiddleware.cookies import CookiesMiddleware
+from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware
-    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',
+    'scrapy.contrib.downloadermiddleware.cookies.CookiesMiddleware',
-from scrapy.utils.misc import dict_updatedefault
+from scrapy.http import Response
-        dispatcher.connect(self.domain_open, signals.domain_open)
+        self.jars = defaultdict(CookieJar)
-            dict_updatedefault(request.cookies, self.cookies[spider.domain_name])
+        if request.meta.get('dont_merge_cookies', False):
-        cookies.update(request.cookies)
+        if request.meta.get('dont_merge_cookies', False):
-        self.cookies[domain] = {}
+    # cookies should be set on non-200 responses too
-        del self.cookies[domain]
+        self.jars.pop(domain, None)
-from twisted.web.client import HTTPClientFactory
+from twisted.web.client import HTTPClientFactory, HTTPPageGetter
-from scrapy.http import Url
+from scrapy.http import Url, Headers
-        self.setdefault(key, default_list)
+        return self.setdefault(key, default_list)
-        return ((k, self[k]) for k in self.keys())
+        return ((k, self.getlist(k)) for k in self.keys())
-        return self.__class__(self.lists())
+        return self.__class__(self)
-        self.assertEqual(h.items(), [('X-Forwarded-For', 'ip2'), ('Content-Type', 'text/html')])
+        self.assertEqual(h.items(), [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])
-                [('X-Forwarded-For', 'ip2'), ('Content-Type', 'text/html')])
+                [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])
-            self.assert_(isinstance(v, str))
+            for s in v:
-        self.request.headers[name] = value
+        self.request.headers.appendlist(name, value)
-                fp.update(request.headers.get(hdr, ''))
+                for v in request.headers.getlist(hdr):
-
+import operator
-        for name, cookie in request.cookies.items():
+        cookies = _get_cookies(jar, request)
-        headers = {'Set-Cookie': 'C1=value1; path=/'}
+        headers = {'Set-Cookie': 'C1=value1; path=/'}
-        self.assertEquals(req2.headers.get('Cookie'), "C1=value1")
+        self.assertEquals(req2.headers.get('Cookie'), "C1=value1; galleta=salada")
-        self.cookies = defaultdict(CookieJar)
+        self.jars = defaultdict(CookieJar)
-        jar = self.cookies[spider.domain_name]
+        jar = self.jars[spider.domain_name]
-        jar = self.cookies[spider.domain_name]
+        jar = self.jars[spider.domain_name]
-
+        self.jars.pop(domain, None)
-        jar.add_cookie_header(wreq)
+        jar.add_cookie_header(request)
-        jar.extract_cookies(wrsp, wreq)
+        jar.extract_cookies(response, request)
-from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy
+from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy, Cookie
-        self.getlist(key).extend(self.normvalue(value))
+        lst = self.getlist(key)
-            redirected = request.replace(url=redirected_url, method='GET', body=None)
+            redirected = request.replace(url=redirected_url, method='GET', body='')
-    def test_process_exception(self):
+    def test_redirect_301(self):
-        req = Request(url, method='POST')
+        req = Request(url, method='POST', body='test', 
-    def test_process_response(self):
+        assert 'Content-Type' not in req2.headers, \
-from scrapy.utils.misc import dict_updatedefault
+from scrapy.utils.cookies import CookieJar
-        wreq = _WrappedRequest(request)
+        for name, cookie in request.cookies.items():
-        # set Cookie header with cookies in jar
+        # set Cookie header
-        print request.url.netloc, 'Set-Cookie:', response.headers.get('Set-Cookie'), request.cookies
+        if request.meta.get('dont_merge_cookies', False):
-
+    # cookies should be set on non-200 responses too
-        return self.response.headers.getlist(name)
+from cookielib import CookieJar as _CookieJar, DefaultCookiePolicy
-        insert_pos = len([p for p in self.pending if ['priority'] <= priority])
+        insert_pos = len([p for p in self.pending if p['priority'] <= priority])
-        self.pending = [p for p in self.pending if ['domain'] not in domains]
+        self.pending = [p for p in self.pending if p['domain'] not in domains]
-        request = cls(url, body=body, headers=headers, **kwargs)
+        request = cls(url, method=form.method, body=body, headers=headers, **kwargs)
-            return True
+        return not (pending or downloading or haspipe or oninit or scraping)
-
+            self.next_request(spider)
-        self.cookies = defaultdict(self.defaultdict_factory)
+        self.cookies = defaultdict(CookieJar)
-
+        # TODO: Merge cookies in request with jar here
-
+from pydispatch import dispatcher
-from scrapy.contrib_exp.cluster.worker.manager import ResponseCode
+from scrapy.contrib.cluster.worker.manager import ResponseCode
-from scrapy.contrib_exp.cluster.master.manager import ClusterMaster
+from scrapy.contrib.cluster.master.manager import ClusterMaster
-        methodname = kwargs.pop('methodname')
+        if 'body' not in kwargs:
-        self.body = xmlrpclib.dumps(params, methodname)
+    def test_copy(self):
-            dfd = scrapyengine.schedule(robotsreq, spiders.fromdomain(spiderdomain), priority=0)
+            dfd = scrapyengine.schedule(robotsreq, spiders.fromdomain(spiderdomain), priority=self.DOWNLOAD_PRIORITY)
-        return scrapyengine.schedule(request, info.spider, priority=0)
+        return scrapyengine.schedule(request, info.spider, priority=self.DOWNLOAD_PRIORITY)
-                schd = self.schedule(redirected, spider, priority=-1)
+                schd = self.schedule(redirected, spider, priority=self.REDIRECTION_PRIORITY)
-    def crawl(self, request, spider, priority=1, domain_priority=1):
+    def crawl(self, request, spider, priority=0, domain_priority=0):
-    def schedule(self, request, spider, priority=1, domain_priority=1):
+    def schedule(self, request, spider, priority=0, domain_priority=0):
-                schd = self.schedule(redirected, spider, priority=4)
+                schd = self.schedule(redirected, spider, priority=-1)
-    def add_domain(self, domain, priority=1):
+    def add_domain(self, domain, priority=0):
-    def enqueue_request(self, domain, request, priority=1):
+    def enqueue_request(self, domain, request, priority=0):
-            raise ValueError("No form control found in %s" % response)
+            raise ValueError("No <form> element found in %s" % response)
-            raise IndexError("No such form number: %d" % formnumber)
+            raise IndexError("Form number %d not found in %s" % (formnumber, response))
-(that Request) to generate Requests based on form data.
+(than Request) to generate Requests based on form data.
-from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, Url
+from cStringIO import StringIO
-from scrapy.contrib.pipeline.images import BaseImagesPipeline
+from twisted.trial import unittest
-    python -m scrapy.tests
+    python -m scrapy.tests.run
-up and configured appropriately.
+Keep in mind that some tests may be skipped if you don't have some (optional)
-    runner.run(suite)
+"""
-from scrapy.utils.serialization import serialize, unserialize, serialize_funcs
+from twisted.trial import unittest
-from scrapy.contrib.item import RobustScrapedItem
+from scrapy.item import ScrapedItem
-        }
+class GoogledirItem(ScrapedItem):
-        Rule(RegexLinkExtractor(allow=('google.com/[A-Z][a-zA-Z_/]+$',),),
+        Rule(RegexLinkExtractor(allow='google.com/[A-Z][a-zA-Z_/]+$'),
-            items_to_csv(self.csv_file, [item])
+            item.name = link.x('a/text()').extract()
-        '${project_name}/pipelines.py',
+        '${project_name}/settings.py.tmpl',
-    with open(path, 'wb') as file:
+    with open(path.rstrip('.tmpl'), 'wb') as file:
-import os, os.path, glob
+# Scrapy setup.py script 
-    return matches
+from distutils.core import setup
-    license = '',
+    name = 'Scrapy',
-    },
+    download_url = "http://scrapy.org/releases/scrapy-0.7.0.tar.gz",
-            u = urlparse.urlparse(url)
+        self.response = None
-            return
+        url = url.strip()
-            return []
+            if u.scheme not in ('http', 'https', 'file'):
-            self.generate_vars(url, self.result)
+        print "Fetching %s..." % request
-    def generate_vars(self, url, response):
+    def generate_vars(self, url=None, response=None, request=None):
-        print "   get <url>: Fetches a new page and updates all Scrapy objects."
+        print "   get [url]: Fetch a new URL or re-fetch current Request"
-                self.get_url(arg.strip())
+                self.get_url(arg)
-                    self.generate_vars(None, None)
+                    self.generate_vars()
-                self.generate_vars(None, None)
+                self.generate_vars()
-from twisted.internet import reactor
+import os
-        """ You can use this function to update the local variables that will be available in the scrape console """
+        """ You can use this function to update the Scrapy objects that will be available in the shell"""
-        print "Downloading URL...           ",
+        print "Downloading URL..."
-        blockingCallFromThread(reactor, scrapyengine.crawl, r, spider)
+        threads.blockingCallFromThread(reactor, scrapyengine.crawl, r, spider)
-        print "Available local variables:"
+        print '-' * 60
-                print "   %s: %s" % (key, val.__class__)
+            print "   %s: %s" % (key, val)
-        print '-' * 78
+        print "   get <url>: Fetches a new page and updates all Scrapy objects."
-        print "Scrapy %s - Interactive scraping console\n" % scrapy.__version__
+        print "Welcome to Scrapy shell!"
-                ip.expose_magic("scrapehelp", _help_magic)
+                ip.expose_magic("shelp", _help_magic)
-These are generally a bad idea.
+Monkey patches for supporting Twisted 2.5.0
-#sys.setrecursionlimit(7400)
+import twisted
-from scrapy.utils.misc import load_object
+from scrapy.utils.misc import load_object, gzip_file
-import sys
+import sys
-            log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \
+            log.msg('Image (uptodate): Downloaded %s from <%s> referred in <%s>' % \
-from scrapy.contrib.spiders.crawl import CrawlSpider, Rule, rule
+from scrapy.contrib.spiders.crawl import CrawlSpider, Rule
-        self.getlist(key).append(self.normvalue(value))
+        self.getlist(key).extend(self.normvalue(value))
-    #    assert h1.getlist('header1') is not h2.getlist('header1')
+    def test_copy(self):
-from scrapy.utils.datatypes import PriorityQueue, PriorityStack
+from scrapy.utils.datatypes import PriorityQueue, PriorityStack, CaselessDict
-    
+        self.assertEquals(result, self.output)
-        self.assertEquals(bool(pq), True) 
+        self.assertEquals(bool(pq), True)
-        self.assertEquals(bool(pq), True) 
+        self.assertEquals(bool(pq), True)
-        self.assertEquals(bool(pq), False) 
+        self.assertEquals(bool(pq), False)
-        self.assertEquals(len(pq), 3) 
+        self.assertEquals(len(pq), 3)
-        self.assertEquals(len(pq), 2) 
+        self.assertEquals(len(pq), 2)
-        self.assertEquals(len(pq), 1) 
+        self.assertEquals(len(pq), 1)
-        self.assertEquals(len(pq), 0) 
+        self.assertEquals(len(pq), 0)
-    
+        self.assertEquals(result, self.output)
-        dict.__init__(self)
+        super(CaselessDict, self).__init__()
-        self.restrict_xpaths = restrict_xpaths
+        self.allow_res = [x if isinstance(x, _re_type) else re.compile(x) for x in arg_to_iter(allow)]
-from scrapy.contrib.pipeline.images import thumbnail_name, image_path
+from scrapy.contrib.pipeline.images import BaseImagesPipeline
-                         'dev.mydeco.com/mydeco.gif')
+                         'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')
-                         'www.maddiebrown.co.uk/catalogue-items/image_54642_12175_95307.jpg')
+                         'full/0ffcd85d563bca45e2f90becd0ca737bc58a00b2.jpg')
-                         'dev.mydeco.com/two/dirs/with spaces+signs.gif')
+                         'full/b250e3a74fff2e4703e310048a5b13eba79379d2.jpg')
-                         'www.dfsonline.co.uk/4507be485f38b0da8a0be9eb2e1dfab8a19223f2')
+                         'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')
-                         'www.dorma.co.uk/images/product_details/2532.jpg')
+                         'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg')
-                         'www.dorma.co.uk/images/product_details/2532')
+                         'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg')
-                         '/tmp/foo_50.jpg')
+                         'thumbs/50/271f172bb4727281011c80fe763e93a47bb6b3fe.jpg')
-                         'foo_50.jpg')
+                         'thumbs/50/0945c699b5580b99e4f40dffc009699b2b6830a7.jpg')
-                         '/tmp/foo_50.jpg')
+                         'thumbs/50/469150566bd728fc90b4adf6495202fd70ec3537.jpg')
-                         '/tmp/some.name/foo_50.jpg')
+                         'thumbs/50/92dac2a6a2072c5695a5dff1f865b3cb70c657bb.jpg')
-import urlparse
+import Image
-import Image
+from twisted.internet import defer
-    MEDIA_TYPE = 'image'
+    """Abstract pipeline that implement the image downloading and thumbnail generation logic
-        mtype = self.MEDIA_TYPE
+        mtype = self.MEDIA_NAME
-        return self.image_downloaded(response, request, info)
+
-                % (self.MEDIA_TYPE, request, referer, errmsg)
+                % (self.MEDIA_NAME, request, referer, errmsg)
-    def image_downloaded(self, response, request, info):
+    def convert_image(self, image, size=None):
-     )
+    """Images pipeline with filesystem support as image's store backend
-            super(ImagesPipeline.DomainInfo, self).__init__(domain)
+            super(ImagesPipeline.DomainInfo, self).__init__(domain)
-        self.mkdir(dirname, info)
+    def store_image(self, key, image, buf, info):
-            raise ImageException(msg)
+            last_modified = os.path.getmtime(absolute_path)
-        return relpath # success value sent as input result for item_media_downloaded
+        return {'last_modified': last_modified, 'checksum': checksum}
-        return relative, absolute
+    def get_filesystem_path(self, key):
-from scrapy.contrib.aws import sign_request
+from scrapy.contrib.pipeline.images import BaseImagesPipeline, md5sum
-from .images import BaseImagesPipeline, ImageException
+class S3ImagesPipeline(BaseImagesPipeline):
-    return m.hexdigest()
+    This pipeline tries to minimize the PUT requests made to amazon doing a
-    )
+        from scrapy.spider import BaseSpider
-    sign_requests = True
+    Commonly uploading images to S3 requires requests to be signed, the
-    s3_custom_spider = None
+    More info about amazon S3 at http://docs.amazonwebservices.com/AmazonS3/2006-03-01/
-        MediaPipeline.__init__(self)
+        super(S3ImagesPipeline, self).__init__()
-        url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.prefix, key)
+        url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.key_prefix, key)
-
+    def stat_key(self, key, info):
-            # check if last modified date did not expires
+            checksum = response.headers['Etag'].strip('"')
-                    (self.MEDIA_TYPE, request.url, referer), level=log.DEBUG, domain=info.domain)
+            return {'checksum': checksum, 'last_modified': modified_stamp}
-        dfd.addErrback(log.err, 'S3ImagesPipeline.media_to_download')
+        dfd.addCallback(_onsuccess)
-    def s3_store_image(self, response, url, info):
+    def store_image(self, key, image, buf, info):
-        return self.s3_download(req, info), buf
+        self.s3_download(req, info)
-            return scrapyengine.schedule(request, self.s3_custom_spider)
+        if self.AmazonS3Spider:
-                 cookies=None, meta=None, encoding='utf-8', dont_filter=False):
+                 cookies=None, meta=None, encoding='utf-8', dont_filter=False,
-            callback = defer.Deferred().addCallback(callback)
+            callback = defer.Deferred().addCallbacks(callback, errback)
-from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate
+from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_imap
-                return deferred_degenerate(result, _ResultContainer())
+                return deferred_imap(_onsuccess_per_item, result)
-    generator = iter(generator or [])
+def deferred_imap(function, *sequences, **kwargs):
-    container = container or []
+    container = []
-            container.append(generator.next())
+            value = iterator.next()
-from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_list, arg_to_iter
+from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_iter
-err = log.err
+def err(*args, **kwargs):
-        return self.__class__(url=self.url if url is None else self.url,
+        return self.__class__(url=self.url if url is None else url,
-        r4 = r3.replace(body='', meta={}, dont_filter=False)
+        r4 = r3.replace(url="http://www.example.com/2", body='', meta={}, dont_filter=False)
-                 cookies=None, meta=None, encoding='utf-8', dont_filter=None):
+                 cookies=None, meta=None, encoding='utf-8', dont_filter=False):
-        self._url = Url(safe_url_string(decoded_url, self.encoding))
+        if isinstance(url, basestring):
-    def replace(self, url=None, method=None, headers=None, body=None, cookies=None, meta=None):
+        """Return a copy of this Request"""
-                              meta=meta or self.meta)
+        return self.__class__(url=self.url if url is None else self.url,
-                  flags=flags or self.flags,
+        new = cls(url=self.url if url is None else url,
-                            help="Override Scrapy setting SETTING with VALUE")
+                            help="Override Scrapy setting SETTING with VALUE. May be repeated.")
-                name, val = setting.split(':')
+                name, val = setting.split(':', 1)
-                sys.stderr.write("%s: invalid argument --set=%s - proper format is -set=SETTING:VALUE'\n" % (sys.argv[0], setting))
+                sys.stderr.write("%s: invalid argument --set=%s - proper format is --set=SETTING:VALUE'\n" % (sys.argv[0], setting))
-        sys.exit()
+        sys.exit(2)
-    pass
+from scrapy.conf import settings
-        return redirected
+        ttl = request.meta.setdefault('redirect_ttl', self.max_redirect_times)
-        req2 = mw.process_exception(req, exc, self.spider)
+        req2 = self.mw.process_exception(req, exc, self.spider)
-        req2 = mw.process_exception(req, exc, self.spider)
+        req2 = self.mw.process_exception(req, exc, self.spider)
-        req2 = mw.process_response(req, rsp, self.spider)
+        req2 = self.mw.process_response(req, rsp, self.spider)
-        rsp2 = mw.process_response(req, rsp, self.spider)
+        rsp2 = self.mw.process_response(req, rsp, self.spider)
-        self.retry_times = settings.getint('RETRY_TIMES')
+        self.max_retry_times = settings.getint('RETRY_TIMES')
-            self.failed_count[fp] = self.failed_count.get(fp, 0) + 1
+        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \
-                log.msg("Retrying %s (failed %d times): %s" % (request, self.failed_count[fp], exception), domain=spider.domain_name, level=log.DEBUG)
+            retries = request.meta.get('retry_times', 0) + 1
-                log.msg("Discarding %s (failed %d times): %s" % (request, self.failed_count[fp], exception), domain=spider.domain_name, level=log.DEBUG)
+                log.msg("Discarding %s (failed %d times): %s" % (request, retries, exception),
-        exception_503 = (Request('http://www.scrapytest.org/503'), HttpException('503', None, Response('http://www.scrapytest.org/503', body='')), self.spider)
+    def _test_retry_exception(self, req, exception):
-        mw.retry_times = 1
+        # second retry
-        self.assertTrue(mw.process_exception(*exception_404) is None)
+        # discard it
-        self.assertTrue(mw.process_exception(*exception_503) is None)
+class PriorityQueue4b(object):
-
+       ("deque+defaultdict+deque+cache", PriorityQueue4b),
-        randomprio.append(prio)
+for line in open('%(samplefile)s'):
-TESTCASES = [
+TESTCASES = (
-       ]
+       )
-    return int(random.random() * priorities) - (priorities / 2)
+def _distribution(priorities, distribution):
-    return min(max(prio, -priorities/2), priorities/2)
+    sigma = priorities / 4.0
-    samplefile = samplefile or gen_samples(pushpops, priorities)
+    samplefile = samplefile or gen_samples(pushpops, priorities, priority_distribution)
-    righ_time = 0
+    right_time = 0
-            heappush(self.positems, (priority, self.righ_time, item))
+            self.right_time += 1
-        return bool(len(self))
+        return any(d for d in self.priolist)
-    """A simple priority queue"""
+    """heapq
-class PriorityQueue2(object):
+
-    Faster priority queue implementation, based on a dictionary of lists
+    time = 0
-    """Priority queue using a deque for priority 0"""
+    """deque+heapq"""
-    """Priority queue using a deque for priority 0"""
+    """deque+defaultdict+deque"""
-    """Priority queue using a deque for priority 0"""
+    """list+deque"""
-        self.priolist = [deque() for _ in range(size)]
+class PriorityQueue5b(PriorityQueue5):
-    """Priority queue using a deque for priority 0"""
+class PriorityQueue5c(PriorityQueue5b):
-       ('list+deque+cache+islice', PriorityQueue6b),
+       #('list+deque', PriorityQueue5),
-        self.priolist[final].appendleft(item)
+        i = priority + self.zero
-        if len(cached):
+        if cached:
-                return (queue.pop(), final)
+            if queue:
-    
+class TestPriorityQueue(object):
-    
+        self.assertEquals(result, OUTPUT)
-        self.assertEquals(bool(pq), True) 
+        self.assertEquals(bool(pq), True)
-        self.assertEquals(bool(pq), True) 
+        self.assertEquals(bool(pq), True)
-        self.assertEquals(bool(pq), False) 
+        self.assertEquals(bool(pq), False)
-        self.assertEquals(len(pq), 3) 
+        self.assertEquals(len(pq), 3)
-        self.assertEquals(len(pq), 2) 
+        self.assertEquals(len(pq), 2)
-        self.assertEquals(len(pq), 1) 
+        self.assertEquals(len(pq), 1)
-        self.PriorityQueue = PriorityQueue3
+        self.assertEquals(len(pq), 0)
-        self.PriorityQueue = PriorityQueue5
+# automatically test any PriorityQueueN class defined at pq_classes
-        self.PriorityQueue = PriorityQueue6
+from __future__ import with_statement
-    q.push(n, int(random.random() * %(priorities)s) - %(priorities)s / 2)
+stmt_fmt = """
-    q.pop()
+try:
-import random
+from collections import deque
-    print "\n== With %s priorities ==\n" % priorities
+def normal_priority(priorities):
-    stmt = stmt_fmt % {'priorities': priorities, 'pushpops': pushpops}
+    stmt = stmt_fmt
-        setup = setup_fmt % {'PriorityClass': cls.__name__, 'priorities': priorities}
+        setup = setup_fmt % {
-    runtests(priorities=100)
+    o = OptionParser()
-class DatatypesTestCase(unittest.TestCase):
+# (ITEM, PRIORITY)
-    def test_priority_queue(self):
+class PriorityQueueTestCase(unittest.TestCase):
-        output = [('one', 1), ('three-1', 3), ('three-2', 3), ('five', 5), ('six', 6)]
+    output = [(1, -5), (80, -3), (30, -1), (50, -1), (20, 0), (4, 1), (6, 3)]
-        self.assertEqual(len(pq), len(input))
+        for item, pr in INPUT:
-        self.assertEqual(out, output)
+            l.append(pq.pop())
-    def test_priority_stack(self):
+    def test_iter(self):
-        output = [('one', 1), ('three-2', 3), ('three-1', 3), ('five', 5), ('six', 6)]
+class PriorityStackTestCase(unittest.TestCase):
-        self.assertEqual(len(pq), len(input))
+        for item, pr in INPUT:
-        self.assertEqual(out, output)
+            l.append(pq.pop())
-    """
+    """Priority queue using a deque for priority 0"""
-        self.items = defaultdict(deque)
+        self.negitems = defaultdict(deque)
-        self.items[priority].appendleft(item)
+        if priority == 0:
-        raise IndexError
+        if self.negitems:
-        return totlen
+        total = sum(len(v) for v in self.negitems.values()) + \
-            
+        gen_negs = ((i, priority) 
-        return False
+        return bool(self.negitems or self.pzero or self.positems)
-        self.items[priority].append(item)
+        if priority == 0:
-                    log.msg("Spider can return Request, ScrapedItem or None, got '%s' while processing %s" % (type(item).__name__, request), log.WARNING, domain=domain)
+                    log.msg("Spider must return Request, ScrapedItem or None, got '%s' while processing %s" % (type(item).__name__, request), log.WARNING, domain=domain)
-                    log.msg("Spider must return Request or ScrapedItem objects, got '%s' while processing %s" % (type(item).__name__, request), log.WARNING, domain=domain)
+                    log.msg("Spider can return Request, ScrapedItem or None, got '%s' while processing %s" % (type(item).__name__, request), log.WARNING, domain=domain)
-    return res
+
-    default_adaptor = lambda v: v.title()
+    default_adaptor = lambda v: v[:-1]
-        assert dta.name == 'Marta'
+        self.assertEqual(dta.name, 'mart')
-        assert ida.name == 'Marta'
+        assert ida.name == 'mart'
-        self.assertEqual(dia.name, 'mARTA')
+        self.assertEqual(dia.name, 'MART')
-from heapq import heappush, heappop
+from collections import deque, defaultdict
-    """A simple priority queue"""
+class PriorityQueue(object):
-        self.items = []
+        self.items = defaultdict(deque)
-        heappush(self.items, (priority, time.time(), item))
+        self.items[priority].appendleft(item)
-        return item, priority
+        priorities = self.items.keys()
-        return len(self.items)
+        totlen = 0
-
+        priorities = self.items.keys()
-        return bool(self.items)
+        for q in self.items.values():
-        heappush(self.items, (priority, -time.time(), item))
+        self.items[priority].append(item)
-        cls.field_adaptors = cls.field_adaptors.copy()
+        cls._field_adaptors = cls._field_adaptors.copy()
-                    cls.field_adaptors[item_field] = adaptor
+                    cls._field_adaptors[item_field] = adaptor
-    field_adaptors = {}
+    _field_adaptors = {}
-            or name == 'default_adaptor' or name == 'field_adaptors'):
+            or name == 'default_adaptor'):
-            fa = self.field_adaptors[name]
+            fa = self._field_adaptors[name]
-            or name == 'default_adaptor' or name == 'field_adaptors'):
+            or name == 'default_adaptor'):
-        assert 'name' in ia.field_adaptors
+        assert 'url' in ia._field_adaptors
-        assert 'summary' in ia.field_adaptors
+        assert 'url' in ia._field_adaptors
-from scrapy.contrib_exp.newitem import Item, StringField
+from scrapy.contrib_exp.newitem import * 
-        stats.incpath('_globl/downloader/exception_count')
+        stats.incpath('_global/downloader/exception_count')
-__all__ = ['BooleanField', 'DateField', 'DecimalField',
+__all__ = ['MultiValuedField', 'BooleanField', 'DateField', 'DecimalField',
-    fields = {}
+class ItemMeta(type):
-    def __classinit__(cls, attrs):
+    def __new__(meta, class_name, bases, attrs):
-        attrs['default_adaptor'] = staticmethod(adaptize(da))
+        da = attrs.get('default_adaptor')
-_IDENTITY = lambda v: v
+def IDENTITY(v):
-        da = attrs.get('default_adaptor') or _IDENTITY
+        da = attrs.get('default_adaptor') or IDENTITY
-class ItemAdaptorMeta(type):
+_IDENTITY = lambda v: v
-        cls = type.__new__(meta, class_name, bases, attrs)
+        da = attrs.get('default_adaptor') or _IDENTITY
-            cls.default_adaptor = staticmethod(default_adaptor)
+        cls = type.__new__(meta, class_name, bases, attrs)
-    IDENTITY = lambda v: v
+    IDENTITY = _IDENTITY
-    default_adaptor = IDENTITY
+
-        cls.__classinit__.im_func(cls, attrs)
+
-class ItemAdaptor(Declarative):
+class ItemAdaptorMeta(type):
-    default_adaptor = None
+    default_adaptor = IDENTITY
-            setattr(cls, name, staticmethod(adaptor))
+        if 'default_adaptor' in attrs:
-                        set_adaptor(cls, field, cls.default_adaptor.im_func)
+            for item_field in cls.item_class.fields.keys():
-            return setattr(self.item_instance, name, value)
+            fa = self.default_adaptor
-class TestItem(Item):
+class BaseItem(Item):
-class TestAdaptor(ItemAdaptor):
+class BaseAdaptor(ItemAdaptor):
-        dta = DefaultedTestAdaptor()
+        dta = DefaultedAdaptor()
-            name = adaptor(ParentAdaptor.name, string.swapcase)
+        class ChildAdaptor(TestAdaptor):
-            name = adaptor(ParentAdaptorDefaulted.name, string.swapcase)
+        class ChildAdaptorDefaulted(DefaultedAdaptor):
-    return hasattr(func, '__call__') and 'adaptor_args' in get_func_args(func)
+
-                    cls.set_adaptor(n, v)
+                if n in cls.item_class.fields.keys():
-        setattr(cls, name, staticmethod(func))
+                        set_adaptor(cls, field, cls.default_adaptor.im_func)
-    name = lambda v, adaptor_args: v.title()
+    name = lambda v: v.title()
-            default_adaptor = lambda v, adaptor_args: v.title()
+            default_adaptor = lambda v: v.title()
-            url = lambda v, adaptor_args: v.lower()
+            url = lambda v: v.lower()
-            summary = lambda v, adaptor_args: v
+            url = lambda v: v.upper()
-            name = adaptor(lambda v, adaptor_args: v)
+            name = adaptor(lambda v: v)
-            default_adaptor = lambda v, adaptor_args: v.title()
+            default_adaptor = lambda v: v.title()
-""" """
+"""
-            content_encoding = response.headers.get('Content-Encoding')
+            content_encoding = response.headers.getlist('Content-Encoding')
-                decoded_body = self._decode(raw_body, encoding)
+                encoding = content_encoding.pop()
-                response.headers['Content-Encoding'] = content_encoding[1:]
+                if not content_encoding:
-    return callable(func) and 'adaptor_args' in get_func_args(func)
+    return hasattr(func, '__call__') and 'adaptor_args' in get_func_args(func)
-        pipe_kwargs = { 'adaptor_args': aargs }
+        aargs = dict(t for d in [pipe_adaptor_args, adaptor_args or {}] for t in d.items())
-                setattr(cls, n, staticmethod(v))
+        cls.field_adaptors = cls.field_adaptors.copy()
-                or name == 'default_adaptor':
+        if (name.startswith('_') or name == 'item_instance' \
-            fa = self._field_adaptors[name]
+            fa = self.field_adaptors[name]
-                return setattr(self.item_instance, name, value)
+            return setattr(self.item_instance, name, value)
-        if name.startswith('_') or name.startswith('item_'):
+        if (name.startswith('_') or name.startswith('item_') \
-
+
-        assert dta._default_adaptor
+        assert dta.default_adaptor
-        assert 'name' in ia._field_adaptors
+        assert 'url' in ia.field_adaptors
-
+        assert 'url' in ia.field_adaptors
-        for field in self.item_instance._fields.keys():
+        for field in self.item_instance.fields.keys():
-            self._values[name] = self._fields[name].assign(value)
+        if name in self.fields.keys():
-        if name.startswith('_'):
+        if name.startswith('_') or name == 'fields':
-        if name in self._fields.keys():
+        if name in self.fields.keys():
-                return self._fields[name].default
+                return self.fields[name].default
-        reprdict = dict((field, getattr(self, field)) for field in self._fields)
+        reprdict = dict((field, getattr(self, field)) for field in self.fields)
-        return type.__new__(meta, name, bases, dct)
+class ItemAdaptor(Declarative):
-    __metaclass__ = ItemAdaptorMetaClass
+    def __classinit__(cls, attrs):
-                raise AttributeError(name)
+        if name.startswith('_'):
-            object.__setattr__(self, name, value)
+            raise AttributeError(name)
-        if not name.startswith('_') and name in self._fields.keys():
+        if name.startswith('_'):
-            return object.__getattribute__(self, name)
+            raise AttributeError(name)
-            raise IgnoreRequest('Skipped (already seen request')
+            raise IgnoreRequest('Skipped (already seen request)')
-    """
+class Item(Declarative, ScrapedItem):
-                    if isinstance(i[1], Field))
+    @property
-    keyvals = cgi.parse_qsl(parts[4], keep_blank_values)
+    scheme, netloc, path, params, query, fragment = urlparse.urlparse(url)
-    return urlparse.urlunparse(parts)
+    query = urllib.urlencode(keyvals)
-        self.assertEqual(canonicalize_url("http://www.example.com/do?b=&a=2"),
+        self.assertEqual(canonicalize_url("http://www.example.com/do?b=&a=2", keep_blank_values=False),
-        self.assertEqual(canonicalize_url("http://www.example.com/do?b=&a=2", keep_blank_values=True),
+        self.assertEqual(canonicalize_url("http://www.example.com/do?b=&a=2"),
-        self.assertEqual(canonicalize_url("http://www.example.com/do?b=&c&a=2"),
+        self.assertEqual(canonicalize_url("http://www.example.com/do?b=&c&a=2", keep_blank_values=False),
-        self.assertEqual(canonicalize_url("http://www.example.com/do?b=&c&a=2", keep_blank_values=True),
+        self.assertEqual(canonicalize_url("http://www.example.com/do?b=&c&a=2"),
-def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):
+def canonicalize_url(url, keep_blank_values=True, keep_fragments=False):
-    keyvals = cgi.parse_qsl(query, keep_blank_values)
+    parts = list(urlparse.urlparse(url))
-
+    parts[2] = urllib.quote(urllib.unquote(parts[2]))
-from scrapy.http import Request, FormRequest, Headers, Url
+from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, Url
-        # empty formdata
+class FormRequestTest(unittest.TestCase):
-        # using custom encoding
+    def test_custom_encoding(self):
-                                timeout=getattr(spider, "download_timeout", default_timeout),
+                                timeout=getattr(spider, "download_timeout", None) or default_timeout,
-class LinkExtractor(HTMLParser):
+class HtmlParserLinkExtractor(HTMLParser):
-      * a function which receives an attribute name and returns whether to scan it
+      * an attrsute name which is used to search for links (defaults to "href")
-      * a function wich receives the attribute value before assigning it
+      * a function wich receives the attrsute value before assigning it
-    def __init__(self, tag="a", attr="href", unique=False):
+    def __init__(self, tag="a", attr="href", process=None, unique=False):
-from scrapy.utils.misc import load_object
+from scrapy.utils.misc import load_object, arg_to_iter
-from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_list
+from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_list, arg_to_iter
-    
+    can be a None, single value, or an iterable.
-    
+
-    
+    """Hash a series of non-None values.
-from scrapy.core.exceptions import HttpException
+from scrapy.core.exceptions import HttpException, IgnoreRequest
-        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)
+        if isinstance(failure.value, (HttpException, IgnoreRequest)):
-from scrapy.core.exceptions import DropItem, NotConfigured, HttpException
+from scrapy.core.exceptions import NotConfigured
-from .images import BaseImagesPipeline, NoimagesDrop, ImageException
+from .images import BaseImagesPipeline, ImageException
-from scrapy.http import Request, Response
+from scrapy.http import Response
-        adaptor_args = {'response': self._response}
+        adaptor_args = {'response': self._response, 'item': self.item_instance}
-        if name.startswith('_') or name == 'item_instance':
+        if name.startswith('_') or name == 'item_instance' \
-            return setattr(self.item_instance, name, value)
+            if self._default_adaptor:
-                fa[field] = adaptor
+            try:
-from scrapy.utils.misc import hash_values, items_to_csv, load_object, to_list
+from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_list
-        self.assertEqual(to_list(('lala', 'poo')), ['lala', 'poo'])
+    def test_arg_to_list(self):
-    if obj is None:
+def arg_to_list(arg):
-        return list(obj)
+    elif hasattr(arg, '__iter__'):
-        return [obj]
+        return [arg]
-from scrapy.core.exceptions import UsageError, DropItem
+from scrapy.core.exceptions import DropItem
-    def __init__(self,problem,value=None):
+    def __init__(self, problem, value=None):
-            raise UsageError("You must specify at least one value when setting an attribute")
+            raise ValueError("You must specify at least one value when setting an attribute")
-        hash_.update("".join(["".join([n, str(v)]) for n,v in sorted(self.__dict__.iteritems())]))
+        hash_.update("".join(["".join([n, str(v)]) for n, v in sorted(self.__dict__.iteritems())]))
-from scrapy.core.exceptions import UsageError, NotConfigured, NotSupported
+from scrapy.core.exceptions import NotConfigured, NotSupported
-                raise UsageError('You cannot return an "%s" object from a spider' % type(ret).__name__)
+                raise TypeError('You cannot return an "%s" object from a spider' % type(ret).__name__)
-                raise UsageError('You cannot return an "%s" object from a spider' % type(ret).__name__)
+                raise TypeError('You cannot return an "%s" object from a spider' % type(ret).__name__)
-from twisted.internet import defer, reactor
+from twisted.internet import reactor
-from scrapy.core.exceptions import UsageError, HttpException, NotSupported
+from scrapy.http import Headers
-        raise UsageError("A site domain name is required")
+        raise ValueError("A site domain name is required")
-        raise UsageError("download_delay must be numeric")
+        raise ValueError("download_delay must be numeric")
-        raise UsageError("download_delay must be positive")
+        raise ValueError("download_delay must be positive")
-from scrapy.item.adaptors import AdaptorPipe, AdaptorFunc
+from scrapy.item.adaptors import AdaptorPipe
-        self.assertRaises(UsageError, self.item.attribute, 'foo')
+        self.assertRaises(ValueError, self.item.attribute, 'foo')
-        self.assertRaises(UsageError, hash_values, 'some', None, 'value')
+        self.assertRaises(ValueError, hash_values, 'some', None, 'value')
-        raise UsageError, '%s isn\'t a module' % path
+        raise ValueError, "Error loading object '%s': not a full path" % path
-        raise UsageError, 'Error importing %s: "%s"' % (module, e)
+        raise ImportError, "Error loading object '%s': %s" % (path, e)
-        raise UsageError, 'module "%s" does not define any object named "%s"' % (module, name)
+        raise NameError, "Module '%s' doesn't define any object named '%s'" % (module, name)
-    """Hash a series of values. 
+    """Hash a series of non-None values. 
-            raise UsageError(message)
+            message = "hash_values was passed None at argument index %d" % list(values).index(None)
-                return get_field_adaptor(field, class_)
+            try:
-from functools import wraps
+def is_adaptor(func):
-            self.item_instance = self.item_class()
+    __metaclass__ = ItemAdaptorMetaClass
-                    return get_field_adaptor(field, class_)
+            func = getattr(cls, field, None)
-#         self.assertEqual(ia.name, 'mARTA')
+    def test_staticmethods(self):
-    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',
+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',
-    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',
+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',
-    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',
+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',
-class HTTPCompressionMiddleware(object):
+class HttpCompressionMiddleware(object):
-    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',
+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',
-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',
+    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',
-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',
+    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',
-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',
+    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',
-class CompressionMiddleware(object):
+class HTTPCompressionMiddleware(object):
-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',
+    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',
-        self.assertEqual(ia.url, 'http://scrapy.org')
+        self.assertEqual(ia.url, 'HTTP://SCRAPY.ORG')
-                fa[field] = cls.__dict__[field]
+                return cls.__dict__[field]
-            get_field_adaptor(field, self.__class__)
+            adaptor = get_field_adaptor(field, self.__class__)
-                pass
+            for base in self.__class__.__bases__:
-            fa = bounded_fa.__func__.__call__ 
+            fa = self._field_adaptors[name]
-    
+
-            raise AttributeError(name)
+            return setattr(self.item_instance, name, value)
-            fieldadaptor = self._field_adaptors[name]
+            bounded_fa = self._field_adaptors[name]
-        setattr(self.item_instance, name, final)
+        ovalue = fa(value, adaptor_args=adaptor_args)
-            name = lambda v, adaptor_args: v.title()
+    def test_basic(self):
-        
+
-                        f_out.writelines(f_in)
+                f_in = open(self.logfile)
-            except Exception as e:
+            except Exception, e:
-            except e:
+            except Exception as e:
-        self._field_extractors = self._get_field_extractors()
+        self._field_adaptors = self._get_field_adaptors()
-        fe = {}
+    def _get_field_adaptors(self):
-        return fe
+                fa[field] = self.__class__.__dict__[field]
-            fieldextractor = self._field_extractors[name]
+            fieldadaptor = self._field_adaptors[name]
-        final = fieldextractor(value, adaptor_args=adaptor_args)
+        final = fieldadaptor(value, adaptor_args=adaptor_args)
-                log.msg("failed to compress %s exception=%s (domain=%s, pid=%s)" % (self.logfile, self.domain, self.pid))
+                log.msg("failed to compress %s exception=%s (domain=%s, pid=%s)" % (self.logfile, e, self.domain, self.pid))
-import sys, os, time, datetime, pickle
+from __future__ import with_statement
-            redirected_url = urljoin(request.url, response.headers['location'][0])
+            redirected_url = urljoin(request.url, response.headers['location'])
-            redirected_url = urljoin(request.url, response.headers['location'][0])
+            redirected_url = urljoin(request.url, response.headers['location'])
-            last_modified = response.headers['Last-Modified'][0]
+            last_modified = response.headers['Last-Modified']
-            etag = response.headers['Etag'][0].strip('"')
+            etag = response.headers['Etag'].strip('"')
-            cls = self.from_content_type(headers['Content-type'][0])
+            cls = self.from_content_type(headers['Content-type'])
-            cls = self.from_content_disposition(headers['Content-Disposition'][0])
+            cls = self.from_content_disposition(headers['Content-Disposition'])
-        return value
+
-            encoding = self._ENCODING_RE.search(content_type[0])
+            encoding = self._ENCODING_RE.search(content_type)
-            key = key.encode(self.encoding)
+            return key.title().encode(self.encoding)
-            raise UsageError("Initialize with dict, not %s" % data.__class__.__name__)
+            raise TypeError("Initialize with dict, not %s" % data.__class__.__name__)
-from scrapy.http.headers import headers_dict_to_raw
+from scrapy.utils.http import headers_dict_to_raw, headers_raw_to_dict
-        headers = Headers(responseheaders)
+        headers = Headers(headers_raw_to_dict(responseheaders))
-    return '\r\n'.join(raw_lines)
+from scrapy.utils.http import headers_dict_to_raw
-    def __init__(self, dictorstr=None, fromdict=None, fromstr=None, encoding='utf-8'):
+    def __init__(self, seq=None, encoding='utf-8'):
-            self.__setitem__(k.lower(), v) 
+        super(Headers, self).__init__(seq)
-    def __setitem__(self, key, value):
+        return key.title()
-        super(Headers, self).__setitem__(key, value)
+        return value
-        dict.__setitem__(self, self.normkey(key), value)
+        dict.__setitem__(self, self.normkey(key), self.normvalue(value))
-        return dict.get(self, self.normkey(key), def_val)
+        return dict.get(self, self.normkey(key), self.normvalue(def_val))
-        return dict.setdefault(self, self.normkey(key), def_val)
+        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))
-        dict.update(self, ((self.normkey(k), v) for k, v in items))
+        seq = seq.iteritems() if isinstance(seq, dict) else seq
-                    dict.__setitem__(self, self.normkey(k), v)
+    def __init__(self, seq=None):
-            dict.__setitem__(self, self.normkey(k), v)
+    def update(self, seq):
-        return d
+    @classmethod
-        return dict.pop(self, self.normkey(key), def_val)
+    def pop(self, key, *args):
-from scrapy.contrib_exp.newitem.extractors import adaptor, ItemExtractor
+from scrapy.contrib_exp.newitem.adaptors import adaptor, ItemAdaptor
-from scrapy.contrib_exp.newitem.extractors import ItemAdaptor, adaptor
+from scrapy.contrib_exp.newitem.adaptors import ItemAdaptor, adaptor
-from scrapy.contrib_exp.newitem.extractors import ItemExtractor, adaptor
+from scrapy.contrib_exp.newitem.extractors import ItemAdaptor, adaptor
-class GoogledirItemExtractor(ItemExtractor):
+class GoogledirItemAdaptor(ItemAdaptor):
-from googledir.items import GoogledirItem, GoogledirItemExtractor
+from googledir.items import GoogledirItem, GoogledirItemAdaptor
-            extractor = GoogledirItemExtractor()
+            extractor = GoogledirItemAdaptor()
-class ItemExtractor(object):
+class ItemAdaptor(object):
-        if (name.startswith('_') or name == 'item_instance'):
+        if name.startswith('_') or name == 'item_instance':
-        else:
+        if name.startswith('_') or name.startswith('item_'):
-    It takes multiples unnamed arguments used as functions of the pipe, and a
+    It takes multiples unnamed arguments used as functions of the pipe, and
-from scrapy.contrib_exp import adaptors
+from scrapy.contrib_exp.newitem.extractors import ItemExtractor, adaptor
-class GoogledirItemExtractor(extractors.ItemExtractor):
+class GoogledirItemExtractor(ItemExtractor):
-    description = extractors.ExtractorField([adaptors.extract, adaptors.strip])
+    name = adaptor(extract, strip)
-def treeadapt(*funcs, **adaptor_args):
+def adaptor(*funcs, **adaptor_args):
-        pipe_kwargs = {'adaptor_args': pipe_adaptor_args}
+        aargs = dict(t for d in [pipe_adaptor_args, adaptor_args or {}] for t in d.iteritems())
-        dict_updatedefault(request.cookies, self.cookies[spider.domain_name])
+        if not request.meta.get('dont_merge_cookies', False):
-DUPLICATESFILTER_FILTERCLASS = 'scrapy.core.filters.SimplePerDomainFilter'
+DUPEFILTER_FILTERCLASS = 'scrapy.dupefilter.SimplePerDomainFilter'
-from scrapy import log
+from scrapy.dupefilter import dupefilter
-        added = duplicatesfilter.add(domain, request)
+        added = dupefilter.add(domain, request)
-from scrapy.core.filters import duplicatesfilter
+from scrapy.dupefilter import dupefilter
-                has = duplicatesfilter.has(domain, req)
+                has = dupefilter.has(domain, req)
-    duplicatesfilter
+    dupefilter
-    clspath = settings.get('DUPLICATESFILTER_FILTERCLASS')
+    clspath = settings.get('DUPEFILTER_FILTERCLASS')
-    duplicatesfilter = cls()
+    dupefilter = cls()
-from scrapy.core.filters import duplicatesfilter
+from scrapy.dupefilter import dupefilter
-        duplicatesfilter.open('scrapytest.org')
+        dupefilter.open('scrapytest.org')
-from scrapy.core.filters import SimplePerDomainFilter, NullFilter
+from scrapy.dupefilter import SimplePerDomainFilter, NullFilter
-from scrapy.core.filters import duplicatesfilter
+from scrapy.dupefilter import dupefilter
-        duplicatesfilter.open('scrapytest.org')
+        dupefilter.open('scrapytest.org')
-        duplicatesfilter.close('scrapytest.org')
+        dupefilter.close('scrapytest.org')
-from scrapy.core.filters import duplicatesfilter
+from scrapy.dupefilter import dupefilter
-        duplicatesfilter.open('scrapytest.org')
+        dupefilter.open('scrapytest.org')
-        duplicatesfilter.close('scrapytest.org')
+        dupefilter.close('scrapytest.org')
-        duplicatesfilter.add('scrapytest.org', r2)
+        dupefilter.add('scrapytest.org', r0)
-from twisted.web.client import HTTPClientFactory
+from scrapy.core.downloader.webclient import ScrapyHTTPClientFactory as HTTPClientFactory
-
+# bugfix not present in twisted 2.5 for handling empty response of HEAD requests
-from scrapy.utils.misc import hash_values, items_to_csv, load_object
+from scrapy.utils.misc import hash_values, items_to_csv, load_object, to_list
-import inspect
+from scrapy.utils.python import get_func_args
-            object.__setattr__(self, name, value)
+        if (name.startswith('_') or name == 'item_instance'):
-
+            func_args = get_func_args(func)
-    def __call__(self, value, kwargs=None):
+    def __call__(self, value, adaptor_args=None):
-                val = func(val, kwargs) if takes_args else func(val)
+                val = func(val, adaptor_args) if takes_args else func(val)
-import fnmatch
+import fnmatch
-from scrapy.utils.misc import hash_values, items_to_csv
+from scrapy.utils.misc import hash_values, items_to_csv, load_object
-class Item(object):
+class Item(ScrapedItem):
-            return self._redirect(redirected, request, spider, 'meta refresh') or response
+            return self._redirect(redirected, request, spider, 'meta refresh')
-            return redirected
+        log.msg("Redirecting (%s) to %s from %s" % (reason, redirected, request), level=log.DEBUG, domain=domain)
-                if not (added or req.dont_filter):
+                has = duplicatesfilter.has(domain, req)
-from scrapy.core.scheduler import Scheduler
+from scrapy.core.scheduler import Scheduler, SchedulerMiddlewareManager
-        schd = self.scheduler.enqueue_request(domain, request, priority)
+        schd = self.schedulermiddleware.enqueue_request(domain, request, priority)
-        mw.process_spider_input(response, self.spider)
+        duplicatesfilter.add('scrapytest.org', r0)
-        assert r2 in filtered
+        assert r2 not in filtered
-        if status in set([302, 303]):
+        if status in [302, 303]:
-DUPLICATESFILTER_FILTERCLASS = 'scrapy.contrib.spidermiddleware.duplicatesfilter.SimplePerDomainFilter'
+DUPLICATESFILTER_FILTERCLASS = 'scrapy.core.filters.SimplePerDomainFilter'
-            return redirected
+            return self._redirect(redirected, request, spider, status)
-            return redirected
+            return self._redirect(redirected, request, spider, status)
-            return redirected
+            return self._redirect(redirected, request, spider, 'meta refresh') or response
-from scrapy.utils.misc import load_object
+from scrapy.core.filters import duplicatesfilter
-        dispatcher.connect(self.filter.close, signals.domain_closed)
+    """Filter out already seen requests to avoid visiting pages more than once."""
-        self.filter.add(spider.domain_name, response.request)
+        duplicatesfilter.add(spider.domain_name, response.request)
-                added = self.filter.add(domain, req)
+                added = duplicatesfilter.add(domain, req)
-        return False
+from scrapy.core.filters import duplicatesfilter
-        url2 = 'http://www.example.com/redirected'
+        url2 = 'http://www.example.com/redirected2'
-        
+
-from scrapy.contrib.spidermiddleware.duplicatesfilter import DuplicatesFilterMiddleware, SimplePerDomainFilter
+from scrapy.contrib.spidermiddleware.duplicatesfilter import DuplicatesFilterMiddleware
-                return response
+        if not isinstance(exception, HttpException):
-                return redirected
+        interval, url = get_meta_refresh(response)
-            raise IgnoreRequest("Skipped (already processed): %s" % response.request)
+        self.filter.add(spider.domain_name, response.request)
-from scrapy.core.exceptions import NotConfigured
+from scrapy.core.exceptions import NotConfigured, IgnoreRequest
-                    log.msg('Skipped (already visited): %s' % req, log.TRACE, domain=domain)
+                    log.msg('Skipped (already processed): %s' % req, log.TRACE, domain=domain)
-           'FloatItemField', 'IntegerItemField', 'StringItemField']
+__all__ = ['BooleanField', 'DateField', 'DecimalField',
-class ItemFieldValueError(Exception):
+class FieldValueError(Exception):
-class ItemField(object):
+class Field(object):
-class BooleanItemField(ItemField):
+class BooleanField(Field):
-        raise ItemFieldValueError("This value must be either True or False.")
+        raise FieldValueError("This value must be either True or False.")
-class DateItemField(ItemField):
+class DateField(Field):
-            raise ItemFieldValueError(
+            raise FieldValueError(
-            raise ItemFieldValueError("Invalid date: %s" % str(e))
+            raise FieldValueError("Invalid date: %s" % str(e))
-class DecimalItemField(ItemField):
+class DecimalField(Field):
-            raise ItemFieldValueError("This value must be a decimal number.")
+            raise FieldValueError("This value must be a decimal number.")
-class FloatItemField(ItemField):
+class FloatField(Field):
-            raise ItemFieldValueError("This value must be a float.")
+            raise FieldValueError("This value must be a float.")
-class IntegerItemField(ItemField):
+class IntegerField(Field):
-            raise ItemFieldValueError("This value must be an integer.")
+            raise FieldValueError("This value must be an integer.")
-class StringItemField(ItemField):
+class StringField(Field):
-        raise ItemFieldValueError("This field must be a string.")
+        raise FieldValueError("This field must be a string.")
-from scrapy.contrib_exp.newitem.fields import ItemField
+from scrapy.contrib_exp.newitem.fields import Field
-                    if isinstance(i[1], ItemField))
+                    if isinstance(i[1], Field))
-        self._default = default
+        self.default = default or self.to_python(None)
-
+__all__ = ['BooleanItemField', 'DateItemField', 'DecimalItemField',
-        self.default = default
+        self._default = default
-    def default_value(self):
+    @property
-        return self.default or self.to_python(None)
+        return self._default or self.to_python(None)
-                return self._fields[name].default_value()
+                return self._fields[name].default
-        return self.to_python(list.join(''))
+    def deiter(self, value):
-                self._values[name] = self._fields[name].to_python(value)
+                self._values[name] = self._fields[name].assign(value)
-        response.request = rq
+        response.request = Request('http://scrapytest.org/')
-        filtered = list(mw.process_spider_output(response, [r1, r2, r3], self.spider))
+        filtered = list(mw.process_spider_output(response, [r0, r1, r2, r3], self.spider))
-        self.assertFalse(r3 in filtered)
+        assert r0 not in filtered
-        self.assertTrue(domain in filter)
+        assert domain in filter
-        self.assertFalse(filter.add(domain, r3))
+        assert filter.add(domain, r1)
-        self.assertFalse(domain in filter)
+        assert domain not in filter
-                    isinstance(i[1], ItemField))
+        return dict(i for i in self.__class__.__dict__.iteritems() \
-            self._values[name] = self._fields[name].to_python(value)
+        if not name.startswith('_'):
-                return None
+                return self._fields[name].default_value()
-from scrapy.contrib.newitem.fields import *
+from scrapy.contrib_exp.newitem.models import Item
-from scrapy.contrib.newitem.fields import ItemField
+from scrapy.contrib_exp.newitem.fields import ItemField
-        #self.filter.add(domain, response.request)
+        self.filter.add(domain, response.request)
-                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True):
+                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):
-        response = Response('')
+        rq = Request('http://scrapytest.org/')
-        assert r3 not in filtered
+        self.assertFalse(rq in filtered)
-        assert domain in filter
+        self.assertTrue(domain in filter)
-        assert not filter.add(domain, r3)
+        self.assertTrue(filter.add(domain, r1))
-        assert domain not in filter
+        self.assertFalse(domain in filter)
-        self.filter.add(domain, response.request)
+        # FIXME there's a conflict between test_spidermiddleware_duplicatesfilter.py and test_engine.py
-                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):
+                 tags=('a', 'area'), attrs=('href',), canonicalize=True, unique=True):
-    def __init__(self, required=False):
+    def __init__(self, required=False, default=None):
-        pass
+        """
-from scrapy.contrib.spiders.crawl import CrawlSpider, Rule
+from scrapy.contrib.spiders.crawl import CrawlSpider, Rule, rule
-            
+
-        
+
-        
+        """Run the given domain.
-        dsettings  = domain_info['settings']
+        spider_settings  = domain_info['settings']
-            
+            self.master.reschedule([domain], spider_settings, newprio,
-                log.msg("ClusterMaster: No available slots at worker=%s when trying to run domain=%s" % (self.name, domain), log.WARNING)
+                log.msg("ClusterMaster: No available slots at worker=%s when trying to run domain=%s"
-                self.master.reschedule([domain], dsettings, newprio, reason="no available slots at worker=%s" % self.name)
+                self.master.reschedule([domain], spider_settings, newprio,
-                log.msg("ClusterMaster: Already running domain=%s at worker=%s" % (domain, self.name), log.WARNING)
+                log.msg("ClusterMaster: Already running domain=%s at worker=%s" %
-                self.master.reschedule([domain], dsettings, priority, reason="domain already running at worker=%s" % self.name)
+                self.master.reschedule([domain], spider_settings, priority,
-        
+
-            self.master.statistics["domains"]["running"].add(domain)
+            dstats["running"].add(domain)
-        log.msg("ClusterMaster: Changed status to <%s> for domain=%s at worker=%s" % (domain_status, domain, self.name))
+            dstats["running"].remove(domain)
-            # when there is no loading domain or in the next status update. This way also we load the nodes softly
+            # load domains by one, so to mix up better the domain loading between nodes.
-                # if domain already running in some node, reschedule with same priority (so it will be run later)
+                # If domain already running in some node, reschedule with same
-                    self.master.reschedule([pending['domain']], pending['settings'], pending['priority'], reason="domain already running in other worker")
+                    self.master.reschedule([pending['domain']], pending['settings'],
-        
+
-        
+
-        
+
-            
+
-            log.msg("ClusterMaster: Could not connect to worker=%s (%s): %s" % (name, hostport, err), log.ERROR)
+            log.msg("ClusterMaster: Could not connect to worker=%s (%s): %s" %
-                log.msg("ClusterMaster: Could not connect to worker=%s (%s): %s" % (name, hostport, failure.value), log.ERROR)
+                log.msg("ClusterMaster: Could not connect to worker=%s (%s): %s" %
-        
+
-        lost = self.statistics["domains"]["running"].difference(real_running)
+        lost = dstats["running"].difference(real_running)
-            
+            dstats["lost_count"][domain] = dstats["lost_count"].get(domain, 0) + 1
-        
+
-                self.pending.insert(insert_pos, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})
+                self.pending.insert(insert_pos,
-        log.msg("clustermaster: Scheduled domains=%s with priority=%s" % (','.join(domains), priority), log.DEBUG)
+        log.msg("clustermaster: Scheduled domains=%s with priority=%s" %
-        log.msg("clustermaster: Rescheduled domains=%s with priority=%s reason='%s'" % (','.join(domains), priority, reason), log.DEBUG)
+        log.msg("clustermaster: Rescheduled domains=%s with priority=%s reason='%s'" %
-higher than 2.5 which is the lower version support by Scrapy.
+higher than 2.5 which is the lowest version supported by Scrapy.
-from scrapy.utils.misc import ignore_patterns, copytree
+from scrapy.utils.python import ignore_patterns, copytree
-
+
-from shutil import copy2, copystat, WindowsError
+from shutil import copy2, copystat
-import shutil
+from scrapy.utils.misc import ignore_patterns, copytree
-            shutil.copytree(roottpl, project_name)
+            copytree(roottpl, project_name, ignore=IGNORE)
-            shutil.copytree(moduletpl, '%s/%s' % (project_name, project_name))
+            copytree(moduletpl, '%s/%s' % (project_name, project_name),
-SPIDER = GoogleDirectorySpider()
+# Define here the models for your scraped items
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-        itemcls = load_class(settings['DEFAULT_ITEM_CLASS'])
+        itemcls = load_object(settings['DEFAULT_ITEM_CLASS'])
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-        historycls = load_class(settings['MEMORYSTORE'])
+        historycls = load_object(settings['MEMORYSTORE'])
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-        self.filter = load_class(clspath)()
+        self.filter = load_object(clspath)()
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-        self.prerun_hooks = [load_class(f) for f in settings.getlist('CLUSTER_WORKER_PRERUN_HOOKS', [])]
+        self.prerun_hooks = [load_object(f) for f in settings.getlist('CLUSTER_WORKER_PRERUN_HOOKS', [])]
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-            cls = load_class(mwpath)
+            cls = load_object(mwpath)
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-            self.classes[mimetype] = load_class(cls)
+            self.classes[mimetype] = load_object(cls)
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-        scheduler = load_class(settings['SCHEDULER'])()
+        scheduler = load_object(settings['SCHEDULER'])()
-        self.prioritizer_class = load_class(settings['PRIORITIZER'])
+        self.prioritizer_class = load_object(settings['PRIORITIZER'])
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-                cls = load_class(extension_path)
+                cls = load_object(extension_path)
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-            cls = load_class(stage)
+            cls = load_object(stage)
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-                spider = load_class(spiderclassname)(domain)
+                spider = load_object(spiderclassname)(domain)
-from scrapy.utils.misc import load_class
+from scrapy.utils.misc import load_object
-            cls = load_class(mwpath)
+            cls = load_object(mwpath)
-    instantiating it"""
+def load_object(path):
-        dot = class_path.rindex('.')
+        dot = path.rindex('.')
-    module, classname = class_path[:dot], class_path[dot+1:]
+        raise UsageError, '%s isn\'t a module' % path
-        cls = getattr(mod, classname)
+        obj = getattr(mod, name)
-        raise UsageError, 'module "%s" does not define a "%s" class' % (module, classname)
+        raise UsageError, 'module "%s" does not define any object named "%s"' % (module, name)
-    return cls
+    return obj
-from scrapy.contrib.pbcluster.crawler.manager import ClusterCrawler
+from scrapy.contrib_exp.cluster.worker.manager import ClusterWorker
-from scrapy.contrib.pbcluster.worker.manager import ResponseCode
+from scrapy.contrib_exp.cluster.worker.manager import ResponseCode
-from scrapy.contrib.pbcluster.master.manager import ClusterMaster
+from scrapy.contrib_exp.cluster.master.manager import ClusterMaster
-                    log.msg('Skipped (already visited): %s' % req, log.DEBUG, domain=domain)
+                    log.msg('Skipped (already visited): %s' % req, log.TRACE, domain=domain)
-    def is_cached(self, domain, key):
+    def read_meta(self, domain, key):
-        if os.path.exists(requestpath):
+        try:
-            return False
+        except IOError, e:
-        if not self.is_cached(domain, key):
+        metadata = self.read_meta(domain, key)
-            metadata = pickle.load(f)
+        responsebody = responseheaders = None
-import MySQLdb
+try:
-    unittest.main()
+DUPLICATESFILTER_FILTERCLASS = 'scrapy.contrib.spidermiddleware.duplicatesfilter.SimplePerDomainFilter'
-    """Filter out duplicate requests to avoid visiting same page more than once"""
+    """Filter out duplicate requests to avoid visiting same page more than once.
-    discovered are added to the scheduler. 
+    """The scheduler decides what to scrape next. In other words, it defines the
-    all_domains contains the names of all domains that are to be scheduled.
+        * next_domain() called each time a domain slot is freed, and return
-            longer to reach the most relevant pages.
+        * BFO - breath-first order (default). Consumes more memory than DFO but reaches
-        self.groupfilter = GroupFilter()
+        """Check if scheduler's resources were allocated for a domain"""
-        """
+        """Return next domain available to scrape and remove it from available domains queue"""
-        domain can be scheduled twice, either with the same or with different
+        """This functions schedules a new domain to be scraped, with the given priority.
-        self.groupfilter.open(domain)
+        """Allocates resources for maintaining a schedule for domain."""
-        added = self.groupfilter.add(domain, requestid)
+        """Enqueue a request to be downloaded for a domain that is currently being scraped."""
-            return defer_fail(IgnoreRequest('Skipped (already visited): %s' % request))
+    def next_request(self, domain):
-        return self.groupfilter.has(domain, request_fingerprint(request))
+        Returns a pair ``(request, deferred)`` where ``deferred`` is the
-        Get the next request to be scraped.
+        ``(None, None)`` should be returned if there aren't requests pending
-        else:
+        try:
-        free any resources associated with the domain.
+        """Called once we are finished scraping a domain.
-        
+
-#DUPLICATESFILTER_FILTERCLASS = 'scrapy.contrib.spidermiddleware.duplicatesfilter.SimplePerDomainFilter'
+import unittest
-                                postdata=request.body or None,
+                                postdata=request.body or None, # see http://dev.scrapy.org/ticket/60
-                                postdata=request.body,
+                                postdata=request.body or None,
-                redirected.url = urljoin(request.url, m.group(2))
+            interval, url = get_meta_refresh(response)
-from scrapy.utils.response import body_or_str, get_base_url
+from scrapy.utils.response import body_or_str, get_base_url, get_meta_refresh
-    LostPound
+    'LostPound'
-    MissingImages
+    'MissingImages'
-        file = open(file, 'a+')
+        file = open(file, 'ab+')
-    """ Convert a word  to it CamelCase version
+    """ Convert a word  to its CamelCase version and remove invalid chars
-        otherwise None if the enrie domain was requested for scraping.
+        otherwise None if the entire domain was requested for scraping.
-         * Both
+
-    cleaner.
+    You can choose whether to parse the file using the 'iternodes' iterator, an
-        of results (Items or Requests)."""
+        the response which originated that results. It must return a list of
-        to into the feed before parsing it. This function must return a response."""
+        to into the feed before parsing it. This function must return a
-    Spider for parsing CSV feeds.
+    """Spider for parsing CSV feeds.
-sys.path.append(os.path.join(os.path.dirname(__file__), "_ext"))
+sys.path.append(path.join(path.dirname(__file__), "_ext"))
-extensions = ['scrapydocs']
+extensions = ['scrapydocs', 'sphinx.ext.autodoc']
-            self.body = urllib.urlencode(query)
+            query = [(unicode_to_str(k, self.encoding), _unicode_to_str(v, self.encoding))
-from scrapy.utils.misc import render_templatefile
+from scrapy.utils.misc import render_templatefile, string_camelcase
-        'scrapy_settings.py',
+        'scrapy-ctl.py',
-    
+
-            shutil.copytree(project_tplpath, project_name)
+            project_root_path = project_name
-                render_templatefile(os.path.join(project_name, path), project_name=project_name)
+                tplfile = os.path.join(project_root_path,
-from scrapy.utils.misc import render_templatefile
+from scrapy.utils.misc import render_templatefile, string_camelcase
-            'project_name': settings.get('BOT_NAME'),
+            'project_name': settings.get('PROJECT_NAME'),
-class MyItem(ScrapedItem):
+class ${ProjectName}Item(ScrapedItem):
-BOT_NAME = '$project_name'
+BOT_NAME = PROJECT_NAME
-#)
+ITEM_PIPELINES = (
-        self.settings = self._import(SETTINGS_MODULE)
+        if not SETTINGS_DISABLED:
-            return self.overrides[opt_name]
+        if not SETTINGS_DISABLED:
-            return os.environ['SCRAPY_' + opt_name]
+            if 'SCRAPY_' + opt_name in os.environ:
-            return getattr(self.settings, opt_name)
+            if hasattr(self.settings, opt_name):
-            return self.defaults[opt_name]
+            if opt_name in self.defaults:
-Common downloader middleare
+Common downloader middleware
-                # be sent by microsof servers. For more information, see:
+                # be sent by microsoft servers. For more information, see:
-You can change the behaviour of this moddileware by modifing the scraping settings:
+You can change the behaviour of this middleware by modifing the scraping settings:
-# -*- coding: utf8 -*-
+"""
-(scrapy.link.LinkExtractor) with some useful features.
+This module provides some LinkExtractors, which extend the base LinkExtractor
-Base class for scrapy spiders
+Base class for Scrapy spiders
-        return self.classes.get(mimetype, self.classes.get(mimetype.split('/')[0], Response))
+        if mimetype is not None:
-    def open_domain(self, domain):
+    def domain_open(self, domain):
-    def close_domain(self, domain):
+    def domain_closed(self, domain):
-        respcls = responsetypes.from_headers(headers)
+        respcls = responsetypes.from_args(headers=headers, url=url)
-            respcls = self._get_response_class(filename=tar_file.members[0].name, body=body)
+            respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)
-            respcls = self._get_response_class(filename=namelist[0], body=body)
+            respcls = responsetypes.from_args(filename=namelist[0], body=body)
-        respcls = self._get_response_class(body=decompressed_body)
+        respcls = responsetypes.from_args(body=decompressed_body)
-        respcls = self._get_response_class(body=decompressed_body)
+        respcls = responsetypes.from_args(body=decompressed_body)
-        respcls = responsetypes.from_headers(headers)
+        respcls = responsetypes.from_args(headers=headers, url=url)
-        respcls = responsetypes.from_body(body) if respcls is Response else respcls
+        respcls = responsetypes.from_args(filename=filepath, body=body)
-import mimetypes
+from os.path import abspath, dirname, join
-        mimetype, encoding = mimetypes.guess_type(filename)
+        mimetype, encoding = self.mimetypes.guess_type(filename)
-        return self.from_mimetype(mimetypes.guess_type(url)[0])
+        return self.from_mimetype(self.mimetypes.guess_type(url)[0])
-ROBOTSTXT_OBEY = True
+ROBOTSTXT_OBEY = False
-           raise NotImplementedError('You must override _add_single_attributes method in order to join %s values into a single value.' % attrtype.__name__)
+        raise NotImplementedError('You must override _add_single_attributes method in order to join %s values into a single value.' % attrtype.__name__)
-            if add and len(new_values) >= 1:
+            if add and len(new_values) > 1:
-        self.assertEqual(self.item.name, 'JohnDoe')
+        self.assertRaises(NotImplementedError, self.item.attribute, 'name', 'Doe', add=True)
-        response = Response(url=request.url, body=f.read())
+        body = f.read()
-        
+        """Try to guess the appropiate response based on the body content.
-        """
+        cannot be guess using more straightforward methods."""
-        reg_exp_remove_tags = '|'.join(tags)
+    if which_ones:
-    return re_tags.sub(u'', str_to_unicode(text))
+        regex = '<.*?>'
-    return re_tags_remove.sub(u'', str_to_unicode(text))
+    text = str_to_unicode(text)
-    return re_escape_chars.sub(u'', str_to_unicode(text))
+    for ec in which_ones:
-
+        self.assertEqual(remove_entities('x&#x2264;y'), u'x\u2264y')
-import htmlentitydefs
+from htmlentitydefs import name2codepoint as entity_defs
-_ent_re = re.compile(r'&(#?)([^&;]+);')
+_ent_re = re.compile(r'&(#?(x?))([^&;\s]+);')
-    It supports both numeric (&#nnnn;) and named (&nbsp; &gt;) entities.
+    It supports both numeric (&#nnnn; and &#hhhh;) and named (&nbsp; &gt;)
-        if m.group(1) == '#':
+        entity_body = m.group(3)
-                    return u''
+                if m.group(2):
-                return u''
+                    number = int(entity_body, 10)
-                return u'&%s;' % m.group(2)
+                number = entity_defs.get(entity_body)
-            ({'Content-Type': ['application/octet-stream'], 'Content-Disposition': ['attachment; filename=data.csv']}, TextResponse),
+            ({'Content-Type': ['application/octet-stream'], 'Content-Disposition': ['attachment; filename=data.txt']}, TextResponse),
-        return self.from_mimetype(mimetype) if encoding is None else Response
+        if mimetype and not encoding:
-    sample_feed_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds', 'feed-sample3.csv')
+    sample_feeds_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds')
-        return self.from_mimetype(mimetypes.guess_type(filename)[0])
+        mimetype, encoding = mimetypes.guess_type(filename)
-def csviter(obj, delimiter=None, headers=None):
+def csviter(obj, delimiter=None, headers=None, encoding=None):
-        return [str_to_unicode(field) for field in csv_r.next()]
+        return [str_to_unicode(field, encoding) for field in csv_r.next()]
-import urlparse
+import os, urllib, urlparse
-                    else:
+                    elif val is not None:
-            return Response
+            cls = self.from_content_type(headers['Content-type'][0])
-        
+
-BOT_NAME = 'scrapybot'
+BOT_NAME = '$project_name'
-            if status in ['302', '303']:
+
-                    log.msg("Redirecting (%s) to %s from %s" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)
+                    log.msg("Redirecting (%d) to %s from %s" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)
-                log.msg("Ignored redirecting (%s) to %s from %s (disabled by spider)" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)
+                log.msg("Ignored redirecting (%d) to %s from %s (disabled by spider)" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)
-            if status in ['301', '307']:
+            if status in [301, 307]:
-                    log.msg("Redirecting (%s) to %s from %s" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)
+                    log.msg("Redirecting (%d) to %s from %s" % (status, redirected, request), level=log.DEBUG, domain=spider.domain_name)
-                log.msg("Ignored redirecting (%s) to %s from %s (disabled by spider)" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)
+                log.msg("Ignored redirecting (%d) to %s from %s (disabled by spider)" % (status, redirected_url, request), level=log.DEBUG, domain=spider.domain_name)
-        self.status = status
+        self.status = int(status)
-        self.status = status
+        self.status = int(status)
-        s  = "HTTP/1.1 %s %s\r\n" % (self.status, RESPONSES[self.status])
+        s  = "HTTP/1.1 %d %s\r\n" % (self.status, RESPONSES[self.status])
-                        print "  %07s | input >" % adaptor.name, repr(val)
+                        print "%sinput | %s <" % (' ' * debug_padding, adaptor.name), repr(val)
-                        print "  %07s | output >" % adaptor.name, repr(val)
+                        print "%soutput | %s >" % (' ' * debug_padding, adaptor.name), repr(val)
-            errbackArgs=[to, cc, subject, len(attachs)])
+        # FIXME ---------------------------------------------------------------------
-                    log.msg('dropping old cached response from %s' % metadata['timestamp'])
+                    log.msg('dropping old cached response from %s' % metadata['timestamp'], level=log.DEBUG)
-        response = CachedResponse(url=url, headers=headers, status=status, body=responsebody)
+        respcls = responsetypes.from_headers(headers)
-            f.write(response.body.get_content())
+            f.write(response.body)
-                raw_body = response.body.get_content()
+                raw_body = response.body
-                    redirected.body = None
+                    redirected = request.replace(url=redirected_url, method='GET', body=None)
-                    redirected.url = redirected_url
+                    redirected = request.replace(url=redirected_url)
-            m = META_REFRESH_RE.search(response.body.to_string()[0:4096])
+            m = META_REFRESH_RE.search(response.body[0:4096])
-        key = hashlib.sha1(response.body.to_string()).hexdigest()
+        key = hashlib.sha1(response.body).hexdigest()
-        key = hashlib.sha1(response.body.to_string()).hexdigest()
+        key = hashlib.sha1(response.body).hexdigest()
-        if not response or not response.body.to_string():
+        if not response or not response.body:
-    memoryfile = StringIO(response.body.to_string())
+    memoryfile = StringIO(response.body)
-        f.write(response.body.to_string())
+        f.write(response.body)
-        buf = StringIO(response.body.to_string())
+        buf = StringIO(response.body)
-        body = response.body.to_string() if response.body is not None else ""
+        body = response.body if response.body is not None else ""
-    from StringIO import StringIO
+from cStringIO import StringIO
-            return response.replace(body=tar_file.extractfile(tar_file.members[0]).read())
+            body = body=tar_file.extractfile(tar_file.members[0]).read()
-            return response.replace(body=zip_file.read(namelist[0]))
+            body = zip_file.read(namelist[0])
-        return response.replace(body=decompressed_body)
+        respcls = self._get_response_class(body=decompressed_body)
-        return response.replace(body=decompressed_body)
+        respcls = self._get_response_class(body=decompressed_body)
-        self.body = response.body.to_string()
+        self.body = response.body
-        r = Response(url=request.url, status=status, headers=headers, body=body)
+        respcls = responsetypes.from_headers(headers)
-        self.encoding = encoding  # this one has to be set first
+        self._encoding = encoding  # this one has to be set first
-        elif isinstance(body, str):
+        if isinstance(body, str):
-            raise TypeError("Request body must either str, unicode or None. Got: '%s'" % type(body).__name__)
+            raise TypeError("Request body must either str or unicode. Got: '%s'" % type(body).__name__)
-            s += "\r\n"
+        s += self.body
-
+"""
-            response.body.get_real_encoding())
+        return self._extract_links(response.body, response.url, response.encoding)
-            links = self._extract_links(html_slice, response.url, response.body.get_real_encoding())
+            links = self._extract_links(html_slice, response.url, response.encoding)
-        key = hashlib.sha1(response.body.to_string()).hexdigest()
+        key = hashlib.sha1(response.body).hexdigest()
-from scrapy.http import Response, Headers
+from scrapy.http import HtmlResponse, Headers
-        response = Response(url=url, headers=Headers(headers), status=200, body=body)
+        response = HtmlResponse(url=url, headers=Headers(headers), status=200, body=body)
-        sample_response = Response('http://foobar.com/dummy', body=test_data)
+        sample_response = HtmlResponse('http://foobar.com/dummy', body=test_data)
-        assert r1.body is None
+        assert r1.body == ""
-from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware
+from scrapy.http import Response, XmlResponse
-        self.assertEqual(response.body.to_string(), self.uncompressed_body)
+        assert isinstance(response, XmlResponse)
-        self.assertEqual(response.body.to_string(), self.uncompressed_body)
+        assert isinstance(response, XmlResponse)
-        self.assertEqual(response.body.to_string(), self.uncompressed_body)
+        assert isinstance(response, XmlResponse)
-        self.assertEqual(response.body.to_string(), self.uncompressed_body)
+        assert isinstance(response, XmlResponse)
-from scrapy.core.scheduler import GroupFilter
+from scrapy.http import Request, FormRequest, Headers, Url
-        assert r1.body is None
+        assert r1.body == ''
-        self.assertEqual((r1.body, r2.body), (None, "New body"))
+        self.assertEqual((r1.body, r2.body), ('', "New body"))
-        self.assertEqual(r1.httprepr(), 'POST http://www.example.com HTTP/1.1\r\nHost: www.example.com\r\nContent-Type: text/html\r\n\r\nSome body\r\n')
+        self.assertEqual(r1.httprepr(), 'POST http://www.example.com HTTP/1.1\r\nHost: www.example.com\r\nContent-Type: text/html\r\n\r\nSome body')
-from scrapy.http.response import _ResponseBody
+from scrapy.http import Response, TextResponse, HtmlResponse, XmlResponse, Headers, Url
-        self.assertTrue(isinstance(Response('http://example.com/', body=None), Response))
+        self.assertTrue(isinstance(Response('http://example.com/', body=''), Response))
-        self.assertTrue(isinstance(Response('http://example.com/', headers={}, status=200, body=None), Response))
+        self.assertTrue(isinstance(Response('http://example.com/', headers={}, status=200, body=''), Response))
-        assert isinstance(r2.body, _ResponseBody)
+        assert r1.body == ''
-        self.assertEqual((r1.body, r2.body.to_string()), (None, "New body"))
+        self.assertEqual((r1.body, r2.body), ('', "New body"))
-    unicode_string = u'\u043a\u0438\u0440\u0438\u043b\u043b\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0435\u043a\u0441\u0442'
+        self.assertEqual(r1.httprepr(), 'HTTP/1.1 404 Not Found\r\nContent-Type: text/html\r\n\r\nSome body')
-        self.assertEqual(cp1251_body.to_string(), original_string)
+        unicode_string = u'\u043a\u0438\u0440\u0438\u043b\u043b\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0442\u0435\u043a\u0441\u0442'
-from unittest import TestCase, main
+import unittest
-        libxml2.debugMemory(1)
+from scrapy.http import TextResponse
-        assert leaked_bytes == 0, "libxml2 memory leak detected: %d bytes" % leaked_bytes
+class Libxml2Test(unittest.TestCase):
-        assert leaked_bytes == 0, "libxml2 memory leak detected: %d bytes" % leaked_bytes
+class ResponseLibxml2DocTest(unittest.TestCase):
-        scrapymanager.configure()
+        #this method shouldn't raise TypeError Exception
-        response = Response('http://example.com/catalog/product/blabla-123',
+        response = TextResponse('http://example.com/catalog/product/blabla-123',
-    main()
+    unittest.main()
-from scrapy.http.response import Response
+from scrapy.http import HtmlResponse
-        response = Response("http://example.org/somepage/index.html", body=html)
+        response = HtmlResponse("http://example.org/somepage/index.html", body=html)
-        response = Response("http://example.org/somepage/index.html", body=html)
+        response = HtmlResponse("http://example.org/somepage/index.html", body=html)
-        response_noenc = Response(url='http://example.com/noenc', body=body)
+        response_utf8 = HtmlResponse(url='http://example.com/utf8', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})
-        response_latin1 = Response(url='http://example.com/latin1', body=body)
+        response_latin1 = HtmlResponse(url='http://example.com/latin1', body=body)
-        self.response = Response(url='http://example.com/index', body=body)
+        self.response = HtmlResponse(url='http://example.com/index', body=body)
-        self.response = Response(url='http://example.com/index', body=body)
+        self.response = HtmlResponse(url='http://example.com/index', body=body)
-        m = self.name_re.search(response.body.to_string())
+        m = self.name_re.search(response.body)
-        m = self.price_re.search(response.body.to_string())
+        m = self.price_re.search(response.body)
-from scrapy.http import Response
+from scrapy.http import XmlResponse, TextResponse
-        response = Response(url="http://example.com", body=body)
+        response = XmlResponse(url="http://example.com", body=body)
-        response = Response(url='http://mydummycompany.com', body=body)
+        response = XmlResponse(url='http://mydummycompany.com', body=body)
-        response = Response(url="http://example.com/", body=body)
+        response = TextResponse(url="http://example.com/", body=body)
-        response = Response(url="http://example.com/", body=body)
+        response = TextResponse(url="http://example.com/", body=body)
-        response = Response(url="http://example.com/", body=body)
+        response = TextResponse(url="http://example.com/", body=body)
-        response = Response(url="http://example.com/", body=body)
+        response = TextResponse(url="http://example.com/", body=body)
-        response = Response(url="http://example.com/", body=body)
+        response = TextResponse(url="http://example.com/", body=body)
-from scrapy.utils.python import str_to_unicode, unicode_to_str
+from scrapy.utils.python import str_to_unicode, unicode_to_str, memoizemethod, isbinarytext
-from scrapy.http.response import Response
+from scrapy.http import Response, TextResponse
-    dummy_response = Response(url='http://example.org/', body='dummy_response')
+    dummy_response = TextResponse(url='http://example.org/', body='dummy_response')
-from scrapy.http import Response
+from scrapy.http import TextResponse, HtmlResponse, XmlResponse
-
+    @libxml2debug
-        response = Response(url="http://example.com", body=body)
+        response = TextResponse(url="http://example.com", body=body)
-        response = Response(url="http://example.com", body=body)
+        response = HtmlResponse(url="http://example.com", body=body)
-        response = Response(url="http://example.com", body=body)
+        response = HtmlResponse(url="http://example.com", body=body)
-        response = Response(url="http://example.com", body=body)
+        response = XmlResponse(url="http://example.com", body=body)
-        response = Response(url="http://example.com", body=body)
+        response = XmlResponse(url="http://example.com", body=body)
-        response = Response(url="http://example.com", headers=headers, body=html_utf8)
+        response = HtmlResponse(url="http://example.com", headers=headers, body=html_utf8)
-from scrapy.http import Response
+from scrapy.http import HtmlResponse
-        r1 = Response('http://www.example.com', body='<html><head></head><body></body></html>')
+        r1 = HtmlResponse('http://www.example.com', body='<html><head></head><body></body></html>')
-        return obj.body.to_unicode() if unicode else obj.body.to_string()
+        return obj.body_as_unicode() if unicode else obj.body
-        match = BASEURL_RE.search(response.body.to_string()[0:4096])
+        match = BASEURL_RE.search(response.body[0:4096])
-        lxdoc = libxml2.htmlReadDoc(response.body.to_string('utf-8'), response.url, 'utf-8', html_parser_options)
+        lxdoc = libxml2.htmlReadDoc(utf8body, response.url, 'utf-8', html_parser_options)
-        lxdoc = libxml2.htmlReadDoc(response.body.to_string('utf-8').replace("\x00", ""), response.url, 'utf-8', html_parser_options)
+        lxdoc = libxml2.htmlReadDoc(utf8body.replace("\x00", ""), response.url, 'utf-8', html_parser_options)
-        lxdoc = libxml2.readDoc(response.body.to_string('utf-8'), response.url, 'utf-8', xml_parser_options)
+        lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', xml_parser_options)
-        lxdoc = libxml2.readDoc(response.body.to_string('utf-8').replace("\x00", ""), response.url, 'utf-8', xml_parser_options)
+        lxdoc = libxml2.readDoc(utf8body.replace("\x00", ""), response.url, 'utf-8', xml_parser_options)
-from scrapy.http import Response
+from scrapy.http import TextResponse
-            response = Response(url=None, body=unicode_to_str(text))
+            response = TextResponse(url=None, body=unicode_to_str(text))
-
+        def _clean_values(values):
-        new_values = self._clean_values(new_values)
+        new_values = _clean_values(new_values)
-from scrapy.utils.python import flatten
+from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, unquote
-def remove_tags(value):
+def remove_tags(tags=()):
-      u'my header', u'my body'
+    Factory that returns an adaptor for removing each
-    return replace_tags(value)
+    def _remove_tags(value):
-class Unquote(object):
+def unquote(keep=None):
-    may have, and returns a new list.
+    This factory returns an adaptor that
-        self.keep = [] if keep is None else keep
+    default_keep = [] if keep is None else keep
-        keep = adaptor_args.get('keep_entities', self.keep)
+    def unquote(value, adaptor_args):
-    it to unicode, and returns a new list.
+    Receives a string and converts it to unicode
-      >> to_unicode(['it costs 20, or 30'])
+      >> to_unicode('it costs 20\xe2\x82\xac, or 30\xc2\xa3')
-    Output: list of unicodes
+    Input: string
-    in the provided iterable.
+    Converts multispaces into single spaces for the given string.
-      [u'Hello sir']
+      >> clean_spaces(u'Hello   sir')
-    Output: list of unicodes
+    Input: string/unicode
-class Delist(object):
+def delist(delimiter=''):
-    in the adaptor's constructor.
+    This factory returns and adaptor that joins
-        return delimiter.join(value)
+    def delist(value, adaptor_args):
-        unquote = adaptors.Unquote()
+        unquote = adaptors.unquote()
-        unquote = adaptors.Unquote(keep=['amp', 'lt'])
+        unquote = adaptors.unquote(keep=['amp', 'lt'])
-        self.assertEqual(adaptors.remove_tags('<div id="1"><table>dsadasf</table></div>'), 'dsadasf')
+        self.assertEqual(adaptors.remove_tags()('<a href="lala">adsaas<br /></a>'), 'adsaas')
-        delist = adaptors.Delist(' ')
+        delist = adaptors.delist(' ')
-        delist = adaptors.Delist()
+        delist = adaptors.delist()
-        self.assertEqual(self.item._adaptors_dict['name'][1].name, "Delist")
+        self.assertEqual(self.item._adaptors_dict['name'][1].name, "delist")
-        unquote = adaptors.Unquote()
+        unquote = adaptors.unquote()
-        self.assertEqual(self.item._adaptors_dict['name'][1].name, "Unquote")
+        self.assertEqual(self.item._adaptors_dict['name'][1].name, "unquote")
-        self.item.attribute('children', *['Johnny', 'Rodrigo'], add=True)
+        self.item.attribute('children', 'Johnny', 'Rodrigo', add=True)
-        return [ unquote_markup(v, keep=keep) for v in value ]
+from scrapy.item.adaptors import AdaptorPipe
-    def __init__(self, data=None):
+
-        if attr in self.ATTRIBUTES:  
+        if attr in self.ATTRIBUTES:
-    
+
-    
+
-        
+
-    
+
-            
+
-        
+
-    
+
-def to_unicode(value):
+def to_unicode(value, adaptor_args):
-        raise TypeError('to_unicode must receive an iterable.')
+    if not isinstance(value, basestring):
-    return [ _clean_spaces_re.sub(' ', str_to_unicode(v)) for v in value ]
+    return _clean_spaces_re.sub(' ', str_to_unicode(value))
-    of the provided string or list.
+    of the provided string.
-    Output: unicode or list of unicodes
+    Input: string/unicode
-        return [ unicode(v.strip()) for v in value ]
+    return value.strip()
-    Output: unicode
+    Input: iterable of strings/unicodes
-    def __init__(self, delimiter=' '):
+    def __init__(self, delimiter=''):
-        return self.delimiter.join(value)
+
-        super(AdaptorPipe, self).__init__(adaptors)
+        super(AdaptorPipe, self).__init__(map(_filter_adaptor, adaptors))
-    def __call__(self, value, **kwargs):
+    def __call__(self, value, kwargs=None):
-                func_args = []
+            next_round = []
-                name = adaptor.__class__.__name__ if hasattr(adaptor, '__class__') else adaptor.__name__
+                    val = adaptor(val, kwargs or {})
-                    print "  %07s | input >" % name, repr(value)
+                    if self.debug:
-        return value
+        return tuple(values)
-from scrapy.item.adaptors import AdaptorPipe
+import copy
-
+    def copy(self):
-from scrapy.contrib import adaptors
+from scrapy.contrib_exp import adaptors
-            'hi, this is my text: "foobarfoobar"')
+        self.assertEqual(sample_pipe(sample_value), ('hi, this is my text: "sample text 1"', ))
-            self.assertEqual(adaptors.extract(x.x("//span[@class='poundnum']/@value")), [u'\xa3'])
+                self.assertEqual(adaptors.extract(x.x("//span[@class='pound']/text()")), (u'\xa3', ))
-            self.assertEqual(adaptors.extract(x.x("//span[@class='euronum']/@value")), [u'\u20ac'])
+                self.assertEqual(adaptors.extract(x.x("//span[@class='euro']/text()")), (u'\u20ac', ))
-        self.assertEqual(adaptors.extract(x.x('//tag2/text()')), [u'blaheawfds<'])
+        self.assertEqual(adaptors.extract(x.x('//tag1/text()[2]')[0]), (u'more test text &amp; &gt;', ))
-        self.assertEqual(adaptors.extract(x.x('//tag2/text()'), {'use_unquote': False}), [u'blaheawfds&lt;'])
+            (u'test text &amp; &amp;', u'<![CDATA[more test text &amp; &gt;]]>', u'blah&amp;blah'))
-    def test_extract_links(self):
+    def test_extract_image_links(self):
-
+        sample_selector = HtmlXPathSelector(sample_response)
-
+        self.assertEqual(adaptors.to_unicode('lelelulu\xc3\xb1', {}), u'lelelulu\xf1')
-
+        unquote = adaptors.Unquote()
-
+        unquote = adaptors.Unquote(keep=['amp', 'lt'])
-
+        self.assertEqual(adaptors.remove_tags('<a href="lala">adsaas<br /></a>'), 'adsaas')
-
+        self.assertEqual(adaptors.remove_root('<div>lallaa<a href="coso">dsfsdfds</a>pepepep<br /></div>'),
-
+        self.assertEqual(adaptors.clean_spaces('  hello,  whats     up?'), ' hello, whats up?')
-
+            'hello there, this is my test')
-
+            [1, 2, 5, 6, 'hi'])
-                         'hi there fellas. this is my test.')
+        delist = adaptors.Delist(' ')
-        item_1.attribute('url', 'http://dummyurl.com')
+        item_1._hidden = '543565'
-        item_2.attribute('supplier', 'A random supplier')
+        item_2._hidden = 'lala'
-from scrapy.utils.response import body_or_str
+from scrapy.utils.response import body_or_str, get_base_url
-    def test_input(self):
+    def test_body_or_str_input(self):
-    def test_extraction(self):
+    def test_body_or_str_extraction(self):
-    def test_encoding(self):
+    def test_body_or_str_encoding(self):
-from scrapy.xpath import HtmlXPathSelector
+from scrapy.utils.python import unicode_to_str, flatten
-        self.locations = tuple(locations) if hasattr(locations, '__iter__') else tuple()
+        self.locations = flatten([locations])
-            alt = alt_sel.extract() if alt_sel else ('', )
+            url = flatten([url_sel.extract()])
-            for selector in selector_res:
+            if isinstance(location, basestring):
-#              Link(url='http://example.com/sample3.html', text=u'sample 3') ])
+class HTMLImageLinkExtractorTestCase(unittest.TestCase):
-You can change the behaviour of this moddileware by modifing the scraping settings: 
+You can change the behaviour of this moddileware by modifing the scraping settings:
-""" 
+  a HTTP download
-from twisted.internet.defer import TimeoutError as UserTimeoutError 
+                                   ConnectionRefusedError, ConnectionDone, ConnectError, \
-                           ConnectionRefusedError, ConnectionDone, ConnectError)
+    EXCEPTIONS_TO_RETRY = (ServerTimeoutError, UserTimeoutError, DNSLookupError,
-                r.append_callback(self._response_downloaded, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)
+                r.deferred.addCallback(self._response_downloaded, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)
-        self.set_url(url)
+                 cookies=None, meta=None, encoding='utf-8', dont_filter=None):
-        # dont_filter be filtered by scheduler
+        self.headers = Headers(headers or {}, encoding=encoding)
-        self.assertEqual(r.url, "http://www.scrapy.org/price/%C2%A3")
+        r1 = Request(url=u"http://www.scrapy.org/price/\xa3", encoding="utf-8")
-    return scheme in ['http', 'https']
+    return request.url.scheme in ['http', 'https']
-                'domain': response.domain,
+                'domain': domain,
-from scrapy.core.exceptions import UsageError, HttpException
+from scrapy.core.exceptions import UsageError, HttpException, NotSupported
-    elif u.scheme in ('http', 'https'):
+    scheme = request.url.scheme
-        raise UsageError("Unsupported scheme '%s' in URL: <%s>" % (u.scheme, request.url))
+        raise NotSupported("Unsupported URL scheme '%s' in: <%s>" % (request.url.scheme, request.url))
-    """This functions handles http/https downloads"""
+def create_factory(request, spider):
-    factory = HTTPClientFactory(url=str(url), # never pass unicode urls to twisted
+
-                                timeout=getattr(spider, "download_timeout", None) or settings.getint('DOWNLOAD_TIMEOUT'),
+                                timeout=getattr(spider, "download_timeout", default_timeout),
-    def _response(body):
+    def _create_response(body):
-        return _response(body)
+        return _create_response(body)
-            raise HttpException(ex.status, ex.message, _response(ex.response))
+            raise HttpException(ex.status, ex.message, _create_response(ex.response))
-        reactor.connectTCP(ip, port or 80, factory)
+def download_http(request, spider):
-        url_encoding='utf-8', dont_filter=None):
+    def __init__(self, url, callback=None, method='GET', headers=None, body=None, 
-            'cookies': self.cookies,
+            'cookies': self.cookies,
-        """Return a new request cloned from this one"""
+        """Return a copy a of this Request"""
-        """Create a new Response based on the current one"""
+        """Return a copy of this Response"""
-    def replace(self, url=None, status=None, headers=None, body=None):
+    def replace(self, url=None, status=None, headers=None, body=None, meta=None):
-                             body=body)
+                             meta=meta or self.meta)
-        new.meta = self.meta.copy()
+        else:
-        r1 = Request("http://www.example.com")
+        def somecallback():
-        r1 = Response("http://www.example.com")
+        r1 = Response("http://www.example.com", body="Some body")
-                spider = spiders.fromurl(request.url)
+            spider = spiders.fromurl(request.url)
-        url_encoding='utf-8', dont_filter=None, domain=None):
+        url_encoding='utf-8', dont_filter=None):
-        response = CachedResponse(domain=domain, url=url, headers=headers, status=status, body=responsebody)
+        response = CachedResponse(url=url, headers=headers, status=status, body=responsebody)
-        r = Response(domain=spider.domain_name, url=request.url, status=status, headers=headers, body=body)
+        r = Response(url=request.url, status=status, headers=headers, body=body)
-        response = Response(domain=spider.domain_name, url=request.url, body=f.read())
+        response = Response(url=request.url, body=f.read())
-        self.domain = domain
+    def __init__(self, url, status=200, headers=None, body=None, meta=None):
-                (repr(self.domain), repr(self.url), repr(self.headers), repr(self.status), repr(self.body))
+        return "Response(url=%s, headers=%s, status=%s, body=%s)" % \
-    def replace(self, domain=None, url=None, status=None, headers=None, body=None):
+    def replace(self, url=None, status=None, headers=None, body=None):
-                             url=url or self.url,
+        new = self.__class__(url=url or self.url,
-        response = Response(domain=domain, url=url, headers=Headers(headers), status=200, body=body)
+        response = Response(url=url, headers=Headers(headers), status=200, body=body)
-        sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=test_data)
+        sample_response = Response('http://foobar.com/dummy', body=test_data)
-        r1 = Response('example.com', 'http://www.example.com', body='')
+        r1 = Response('http://www.example.com', body='')
-        r1 = Response('example.com', 'http://www.example.com', body='')
+        r1 = Response('http://www.example.com', body='')
-            self.assertEqual(response.domain, spider.domain_name)
+        # Request requires url in the constructor
-        # Response requires domain and url
+        # Response requires url in the consturctor
-        self.assertTrue(isinstance(Response('example.com', 'http://example.com/'), Response))
+        self.assertTrue(isinstance(Response('http://example.com/'), Response))
-        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', body='body'), Response))
+        self.assertTrue(isinstance(Response('http://example.com/', body=None), Response))
-        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', headers={}, status=200, body=None), Response))
+        self.assertTrue(isinstance(Response('http://example.com/', headers={}, status=200, body=None), Response))
-        r = Response("domain.com", "http://www.example.com")
+        r = Response("http://www.example.com")
-        r = Response("example.com", "http://www.example.com", meta=meta, headers=headers, body="a body")
+        r = Response("http://www.example.com", meta=meta, headers=headers, body="a body")
-        r1 = Response('example.com', "http://www.example.com")
+        r1 = Response("http://www.example.com")
-        r1 = CustomResponse('example.com', 'http://www.example.com')
+        r1 = CustomResponse('http://www.example.com')
-        r1 = Response('example.com', "http://www.example.com")
+        r1 = Response("http://www.example.com")
-        r1 = Response('example.com', "http://www.example.com", status=404, headers={"Content-type": "text/html"}, body="Some body")
+        r1 = Response("http://www.example.com", status=404, headers={"Content-type": "text/html"}, body="Some body")
-        response = Response('example.com', 'http://example.com/catalog/product/blabla-123',
+        response = Response('http://example.com/catalog/product/blabla-123',
-        response = Response("example.org", "http://example.org/somepage/index.html", body=html)
+        response = Response("http://example.org/somepage/index.html", body=html)
-        response = Response("example.org", "http://example.org/somepage/index.html", body=html)
+        response = Response("http://example.org/somepage/index.html", body=html)
-        response_noenc = Response(url='http://example.com/noenc', domain='example.com', body=body)
+        response_utf8 = Response(url='http://example.com/utf8', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})
-        response_latin1 = Response(url='http://example.com/latin1', domain='example.com', body=body)
+        response_latin1 = Response(url='http://example.com/latin1', body=body)
-        self.response = Response(url='http://example.com/index', domain='example.com', body=body)
+        self.response = Response(url='http://example.com/index', body=body)
-#        self.response = Response(url='http://example.com/index', domain='example.com', body=body)
+#        self.response = Response(url='http://example.com/index', body=body)
-        test_responses[format] = Response('foo.com', 'http://foo.com/bar', body=body)
+        test_responses[format] = Response('http://foo.com/bar', body=body)
-        exception_503 = (Request('http://www.scrapytest.org/503'), HttpException('503', None, Response('scrapytest.org', 'http://www.scrapytest.org/503', body='')), self.spider)
+        exception_404 = (Request('http://www.scrapytest.org/404'), HttpException('404', None, Response('http://www.scrapytest.org/404', body='')), self.spider)
-        response = Response(domain="example.com", url="http://example.com", body=body)
+        response = Response(url="http://example.com", body=body)
-        response = Response(domain='mydummycompany.com', url='http://mydummycompany.com', body=body)
+        response = Response(url='http://mydummycompany.com', body=body)
-        response = Response(domain="example.com", url="http://example.com/", body=body)
+        response = Response(url="http://example.com/", body=body)
-        response = Response(domain="example.com", url="http://example.com/", body=body)
+        response = Response(url="http://example.com/", body=body)
-        response = Response(domain="example.com", url="http://example.com/", body=body)
+        response = Response(url="http://example.com/", body=body)
-        response = Response(domain="example.com", url="http://example.com/", body=body)
+        response = Response(url="http://example.com/", body=body)
-        response = Response(domain="example.com", url="http://example.com/", body=body)
+        response = Response(url="http://example.com/", body=body)
-    dummy_response = Response(domain='example.org', url='http://example.org/', body='dummy_response')
+    dummy_response = Response(url='http://example.org/', body='dummy_response')
-        response = Response(domain="example.com", url="http://example.com", body=body)
+        response = Response(url="http://example.com", body=body)
-        response = Response(domain="example.com", url="http://example.com", body=body)
+        response = Response(url="http://example.com", body=body)
-        response = Response(domain="example.com", url="http://example.com", body=body)
+        response = Response(url="http://example.com", body=body)
-        response = Response(domain="example.com", url="http://example.com", body=body)
+        response = Response(url="http://example.com", body=body)
-        response = Response(domain="example.com", url="http://example.com", body=body)
+        response = Response(url="http://example.com", body=body)
-        response = Response(domain="example.com", url="http://example.com", headers=headers, body=html_utf8)
+        response = Response(url="http://example.com", headers=headers, body=html_utf8)
-        r1 = Response('example.com', 'http://www.example.com', body='<html><head></head><body></body></html>')
+        r1 = Response('http://www.example.com', body='<html><head></head><body></body></html>')
-            response = Response(domain=None, url=None, body=unicode_to_str(text))
+            response = Response(url=None, body=unicode_to_str(text))
-        body=None, headers=None, cookies=None,
+        body=None, headers=None, cookies=None, meta=None,
-        self.meta = {}
+        self.meta = {} if meta is None else dict(meta)
-    def __init__(self, domain, url, status=200, headers=None, body=None):
+    def __init__(self, domain, url, status=200, headers=None, body=None, meta=None):
-        self.meta = {}
+        self.meta = {} if meta is None else dict(meta)
-from scrapy.http import Request, Headers
+from scrapy.http import Request, Headers, Url
-from scrapy.http import Response
+from scrapy.http import Response, Headers, Url
-            request.context['history_response_version'] = version
+            request.meta['history_response_version'] = version
-        version = request.context.get('history_response_version')
+        version = request.meta.get('history_response_version')
-    def __init__(self, url, callback=None, context=None, method='GET',
+    def __init__(self, url, callback=None, method='GET',
-        """Clone request except `context` attribute"""
+        """Return a new request cloned from this one"""
-            if att not in ['cache', 'context', 'url', 'deferred']:
+            if att not in ['cache', 'url', 'deferred']:
-        reqlen = len(request)
+        reqlen = len(request.httprepr())
-        reslen = len(response)
+        reslen = len(response.httprepr())
-        elif max_response_size and max_response_size > len(response):  
+        elif max_response_size and max_response_size > len(response.httprepr()):  
-    def to_string(self):
+    def httprepr(self):
-    def to_string(self):
+    def httprepr(self):
-    def test_to_string(self):
+    def test_httprepr(self):
-        self.assertEqual(r1.to_string(), 'GET http://www.example.com HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
+        self.assertEqual(r1.httprepr(), 'GET http://www.example.com HTTP/1.1\r\nHost: www.example.com\r\n\r\n')
-        self.assertEqual(r1.to_string(), 'POST http://www.example.com HTTP/1.1\r\nHost: www.example.com\r\nContent-Type: text/html\r\n\r\nSome body\r\n')
+        self.assertEqual(r1.httprepr(), 'POST http://www.example.com HTTP/1.1\r\nHost: www.example.com\r\nContent-Type: text/html\r\n\r\nSome body\r\n')
-    def test_to_string(self):
+    def test_httprepr(self):
-        self.assertEqual(r1.to_string(), 'HTTP/1.1 200 OK\r\n\r\n')
+        self.assertEqual(r1.httprepr(), 'HTTP/1.1 200 OK\r\n\r\n')
-        self.assertEqual(r1.to_string(), 'HTTP/1.1 404 Not Found\r\nContent-Type: text/html\r\n\r\nSome body\r\n')
+        self.assertEqual(r1.httprepr(), 'HTTP/1.1 404 Not Found\r\nContent-Type: text/html\r\n\r\nSome body\r\n')
-        if isinstance(response, Response) and not response.cached:
+        if isinstance(response, Response) and not response.meta.get('cached'):
-        response.cached = True
+        response = CachedResponse(domain=domain, url=url, headers=headers, status=status, body=responsebody)
-        self.enabled = settings.getbool('CRAWL_DEBUG')
+        raise NotConfigured
-            log.msg("Crawling %s" % repr(request), domain=spider.domain_name, level=log.DEBUG)
+        log.msg("Crawling %s" % repr(request), domain=spider.domain_name, level=log.DEBUG)
-            log.msg("Crawl exception %s in %s" % (exception, repr(request)), domain=spider.domain_name, level=log.DEBUG)
+        log.msg("Crawl exception %s in %s" % (exception, repr(request)), domain=spider.domain_name, level=log.DEBUG)
-            log.msg("Fetched %s from %s" % (response.info(), repr(request)), domain=spider.domain_name, level=log.DEBUG)
+        log.msg("Fetched %s from %s" % (response, repr(request)), domain=spider.domain_name, level=log.DEBUG)
-        if version == response.version():
+        if version == self.get_version(response):
-            version = response.version()
+            version = self.get_version(response)
-            if version == pagedata.version():
+            
-                              pagedata.parent,  pagedata.version(),
+                              pagedata.parent,  self.get(pagedata),
-        status = factory.status
+        status = int(factory.status)
-                log.msg("Crawled %s <%s> from <%s>" % (cached, response.url, request.headers.get('referer')), level=log.DEBUG, domain=domain)
+                log.msg("Crawled %s from <%s>" % (response, request.headers.get('referer')), level=log.DEBUG, domain=domain)
-Request, Response, ResponseBody and Url outside this module.
+Request, Response and Url outside this module.
-from scrapy.http.response import Response, ResponseBody
+from scrapy.http.response import Response
-        return "<%s>" % self.url
+        if self.method == 'GET':
-        s += self.headers.to_string() + "\r\n"
+        if self.headers:
-import hashlib
+from types import NoneType
-        self.body = ResponseBody(body, self.headers_encoding())
+        if body is not None:
-        self.request = None # request which originated this response
+        self.request = None
-        return "<Response: %s %s (%s)>" % (self.status, self.url, version)
+        if self.status == 200:
-        """Create a new Response with the same attributes except for those given new values.
+    def replace(self, domain=None, url=None, status=None, headers=None, body=None):
-        Example: newresp = oldresp.replace(body="New body")
+        >>> newresp = oldresp.replace(body="New body")
-        return newresp
+        new = self.__class__(domain=domain or self.domain,
-        s += self.headers.to_string() + "\r\n"
+        s  = "HTTP/1.1 %s %s\r\n" % (self.status, RESPONSES[self.status])
-    """The body of an HTTP response
+class _ResponseBody(object):
-    This handles conversion to unicode and various character encodings.
+    Currently, the main purpose of this class is to handle conversion to
-        return "ResponseBody(content=%s, declared_encoding=%s)" % (repr(self._content), repr(self.declared_encoding))
+        return "_ResponseBody(content=%s, declared_encoding=%s)" % (repr(self._content), repr(self.declared_encoding))
-        key = response.version()
+        key = hashlib.sha1(response.body.to_string()).hexdigest()
-        response = Response(domain=domain, url=url, headers=Headers(headers), status='200', body=body)
+        response = Response(domain=domain, url=url, headers=Headers(headers), status=200, body=body)
-                self.assertEqual('404', response.status)
+                self.assertEqual(404, response.status)
-                self.assertEqual('302', response.status)
+                self.assertEqual(302, response.status)
-from scrapy.http import Response, ResponseBody
+from scrapy.http import Response
-        self.assertRaises(AssertionError, Response, 'example.com', 'http://example.com/', body=ResponseBody('body', 'utf-8'))
+    def test_copy_inherited_classes(self):
-        cp1251_body     = ResponseBody(original_string, 'cp1251')
+        cp1251_body     = _ResponseBody(original_string, 'cp1251')
-        response = Response(domain=domain, url=url, original_url=original_url, headers=headers, status=status, body=responsebody)
+        response = Response(domain=domain, url=url, headers=headers, status=status, body=responsebody)
-        r = Response(domain=spider.domain_name, url=request.url, headers=headers, status=status, body=body)
+        r = Response(domain=spider.domain_name, url=request.url, status=status, headers=headers, body=body)
-responses in Scrapy.
+esponses in Scrapy.
-    """HTTP responses
+class Response(object):
-    def __init__(self, domain, url, original_url=None, headers=None, status=200, body=None):
+    def __init__(self, domain, url, status=200, headers=None, body=None):
-                (repr(self.domain), repr(self.url), repr(self.original_url), repr(self.headers), repr(self.status), repr(self.body))
+        return "Response(domain=%s, url=%s, headers=%s, status=%s, body=%s)" % \
-        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', original_url='http://example.com/None', headers={}, status=200, body=None), Response))
+        self.assertTrue(isinstance(Response('example.com', 'http://example.com/', headers={}, status=200, body=None), Response))
-                    log.msg('Garbage found in spider output while processing %s, got type %s' % (request, type(item)), log.DEBUG, domain=domain)
+                    log.msg("Spider must return Request or ScrapedItem objects, got '%s' while processing %s" % (type(item).__name__, request), log.WARNING, domain=domain)
-    if not hasattr(response, '_soup'):
+    if 'soup' not in response.cache:
-    return response._soup
+        response.cache['soup'] = BeautifulSoup(body, **kwargs)
-                if self.maxdepth and request.depth > self.maxdepth:
+                depth = response.request.meta['depth'] + 1
-                        stats.setpath('%s/request_depth_max' % spider.domain_name, request.depth)
+                    stats.incpath('%s/request_depth_count/%s' % (spider.domain_name, depth))
-        if self.stats and response.request.depth == 0: # otherwise we loose stats for depth=0 
+        if self.stats and 'depth' not in response.request.meta: # otherwise we loose stats for depth=0 
-                r = Request(url=link.url, link_text=link.text)
+                r = Request(url=link.url)
-from copy import copy
+import copy
-        url_encoding='utf-8', link_text='', dont_filter=None, domain=None):
+        url_encoding='utf-8', dont_filter=None, domain=None):
-        self._cache = {}
+        self.meta = {}
-        new._cache = {}
+        new = copy.copy(self)
-            if att not in ['_cache', 'context', 'url', 'deferred']:
+            if att not in ['cache', 'context', 'url', 'deferred']:
-                setattr(new, att, copy(value))
+                setattr(new, att, copy.copy(value))
-from unittest import TestCase, main
+import unittest
-class ResponseTest(TestCase):
+class ResponseTest(unittest.TestCase):
-class ResponseBodyTest(TestCase):
+    def test_copy(self):
-    main()
+    unittest.main()
-        self.assertEqual(request_fingerprint(r1), r1._cache['fingerprint'])
+        self.assertEqual(request_fingerprint(r1), r1.cache['fingerprint'])
-        return request._cache[cachekey]
+        return request.cache[cachekey]
-        request._cache[cachekey] = fphash
+        request.cache[cachekey] = fphash
-    if not hasattr(response, attr):
+    cachekey = 'lx2doc_%s' % constructor.__name__
-    return getattr(response, attr)
+        response.cache[cachekey] = lx2doc
-        body=None, headers=None, cookies=None, referer=None,
+        body=None, headers=None, cookies=None,
-    def process_item(self, domain, response, item):
+    def process_item(self, domain, item):
-    def process_item(self, domain, response, item):
+    def process_item(self, domain, item):
-    def process_item(self, domain, response, item):
+    def process_item(self, domain, item):
-    def process_item(self, domain, response, item):
+    def process_item(self, domain, item):
-    def process_item(self, domain, response, item):
+    def process_item(self, domain, item):
-                    piped = self.pipeline.pipe(item, spider, response) # TODO: remove response
+                    piped = self.pipeline.pipe(item, spider)
-    def pipe(self, item, spider, response):
+    def pipe(self, item, spider):
-            d = mustbe_deferred(current_stage.process_item, domain, response, item)
+            d = mustbe_deferred(current_stage.process_item, domain, item)
-    def __init__(self, url, callback=None, context=None, method=None,
+    def __init__(self, url, callback=None, context=None, method='GET',
-             'Request method argument must be str or unicode, got %s: %s' % (type(method), method)
+        self.method = method.upper()
-        request.prepend_callback(d)
+        # prepend_callback Request method was removed (it never worked properly anyways) 
-        assert isinstance(func, defer.Deferred), 'prepend_callback expects a callable or defer.Deferred instance, got %s' % type(func)
+        assert isinstance(func, defer.Deferred), \
-        assert isinstance(url, basestring), 'Request url argument must be str or unicode, got %s:' % type(url).__name__
+        assert isinstance(url, basestring), \
-
+        http_user = getattr(spider, 'http_user', '')
-            domain=None):
+
-        assert isinstance(self.method, basestring), 'Request method argument must be str or unicode, got %s: %s' % (type(method), method)
+        assert isinstance(self.method, basestring), \
-from scrapy.utils.request import request_fingerprint
+from scrapy.utils.request import request_fingerprint, request_authenticate
-
+from base64 import urlsafe_b64encode
-        reactor.connectSSL(u.hostname, u.port or 443, factory, contextFactory)
+        reactor.connectSSL(ip, port or 443, factory, contextFactory)
-        reactor.connectTCP(u.hostname, u.port or 80, factory)
+        reactor.connectTCP(ip, port or 80, factory)
-        self.image_refresh_days = settings.getint('IMAGE_EXPIRES', 90)
+        self.image_refresh_days = settings.getint('IMAGES_EXPIRES', 90)
-        self.image_refresh_days = settings.getint('IMAGES_REFRESH_DAYS', 90)
+        self.image_refresh_days = settings.getint('IMAGE_EXPIRES', 90)
-from scrapy.core.exceptions import UsageError, NotConfigured, HttpException, IgnoreRequest
+from scrapy.core.exceptions import NotConfigured, HttpException, IgnoreRequest
-        key = request.fingerprint()
+        key = request_fingerprint(request)
-            key = request.fingerprint()
+            key = request_fingerprint(request)
-            key = request.fingerprint()
+            key = request_fingerprint(request)
-            self.cache.store(domain,key, request, exception.response)
+            self.cache.store(domain, key, request, exception.response)
-            fp = request.fingerprint()
+            fp = request_fingerprint(request)
-        requestid = request.fingerprint()
+        requestid = request_fingerprint(request)
-        fp = request.fingerprint()
+        fp = request_fingerprint(request)
-    """Maintain many concurrent downloads and provide an HTTP abstraction.
+    """Mantain many concurrent downloads and provide an HTTP abstraction.
-        log.msg('Activating %s' % request.traceinfo(), log.TRACE)
+        if self.debug_mode:
-            log.msg('Deactivating %s' % request.traceinfo(), log.TRACE)
+            if self.debug_mode:
-        log.msg("Downloader closing domain %s" % domain, log.TRACE, domain=domain)
+        if self.debug_mode:
-            log.msg('Domain %s already closed' % domain, log.TRACE, domain=domain)
+            if self.debug_mode:
-        return len(self.sites) < settings.getint('CONCURRENT_DOMAINS')
+        return len(self.sites) < self.concurrent_domains
-scheduler, downloader and spiders.
+This is the Scrapy engine which controls the Scheduler, Downloader and Spiders.
-                    log.msg('Garbage found in spider output while processing %s, got type %s' % (request, type(item)), log.TRACE, domain=domain)
+                    log.msg('Garbage found in spider output while processing %s, got type %s' % (request, type(item)), log.DEBUG, domain=domain)
-                log.msg('Scheduling %s (delayed)' % request.traceinfo(), log.TRACE)
+                log.msg('Scheduling %s (delayed)' % request_info(request), log.DEBUG)
-            log.msg('Scheduling %s (now)' % request.traceinfo(), log.TRACE)
+            log.msg('Scheduling %s (now)' % request_info(request), log.DEBUG)
-        log.msg('Downloading %s' % request.traceinfo(), log.TRACE)
+        if self.debug_mode:
-            log.msg("Requested %s" % request.traceinfo(), level=log.TRACE, domain=domain)
+            if self.debug_mode:
-        requestid = request.fingerprint()
+        requestid = request_fingerprint(request)
-        return self.groupfilter.has(domain, request.fingerprint())
+        return self.groupfilter.has(domain, request_fingerprint(request))
-            fingerprint_params=None, domain=None):
+            domain=None):
-        self._fingerprint = None
+
-
+        new._cache = {}
-            if att not in ['context', 'url', 'deferred', '_fingerprint']:
+            if att not in ['_cache', 'context', 'url', 'deferred']:
-        self.assertEqual(k1, k2)
+        k1 = "id1"
-        for k,v in h.iteritems():
+        for k, v in h.iteritems():
-
+import unittest
-                        (dropped_count, max_pending), level=log.WARNING, domain=spider.domain_name)
+                        (dropped_count, max_pending), level=log.DEBUG, domain=spider.domain_name)
-CLOSEDOMAIN_NOTIFY = ['pablo@insophia.com']
+CLOSEDOMAIN_NOTIFY = []
-from cStringIO import StringIO
+    'scrapy.contrib.webconsole.scheduler.SchedulerQueue',
-Scheduler information module for Scrapy webconsole
+Scheduler queue web console module 
-like that.
+See documentation in docs/ref/extensions.rst
-class SchedulerStats(object):
+class SchedulerQueue(object):
-        for domain, requests in scrapyengine.scheduler.pending_requests.iteritems():
+        for domain, request_queue in scrapyengine.scheduler.pending_requests.iteritems():
-            s += "%s (<b>%s</b> pages)\n" % (domain, len(requests))
+            s += "%s (<b>%s</b> requests)\n" % (domain, len(request_queue))
-                #    s += "<li>%s</li>\n" % (repr(r))
+            for ((req, _), prio) in request_queue:
-
+        assert not bool(pq)
-        return ((priority, item) for priority, _, item in self.items)
+        return ((item, priority) for priority, _, item in self.items)
-    multiple times within the same batch.
+    The scheduler decides what to scrape next. In other words, it defines the
-    * begin_domain() called to commence scraping a website
+    * next_available_domain() called to find out when there is something to do
-            return not self.pending_requests[domain].empty()
+            return bool(self.pending_requests[domain])
-            priority, domain = self.domains_queue.get_nowait()
+            domain, priority = self.domains_queue.pop()
-        self.domains_queue.put(domain, priority=priority)
+        self.domains_queue.push(domain, priority)
-            self.pending_requests[domain].put((request, deferred), priority)
+            self.pending_requests[domain].push((request, deferred), priority)
-            return pending_list.get_nowait()[1]
+        if pending_list:
-from scrapy.utils.datatypes import PriorityQueue
+
-        self.assertEqual(p, [0, 1, 1, 1, 2])
+        pq = PriorityStack()
-import bisect
+from heapq import heappush, heappop
-        Queue.Queue.put(self, (priority, item), block, timeout)
+    def __init__(self):
-        priority = int(item[0])
+    def push(self, item, priority=0):
-        self.queue.insert(bisect.bisect_left(self.queue, (priority+1,)), item)
+    def pop(self):
-        return self.queue.pop(0)
+    def __iter__(self):
-        self.queue.insert(bisect.bisect_left(self.queue, (priority,)), item)
+    """A simple priority stack which is similar to PriorityQueue but pops its
-    
+
-    
+
-    
+
-        return []
+        return [Request(url, callback=self.parse, dont_filter=True) for url in urls]
-    keyvals = cgi.parse_qsl(parts[4], keep_blank_values)
+    scheme, netloc, path, params, query, fragment = urlparse.urlparse(url)
-    return urlparse.urlunparse(parts)
+    query = urllib.urlencode(keyvals)
-    be enabled"""
+
-        request.headers.setdefault('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')
+        request.headers.setdefault('Accept', self.header_accept)
-
+from cStringIO import StringIO
-        msg = MIMEMultipart()
+    def send(self, to, subject, body, cc=None, attachs=()):
-        msg.attach(MIMEText(body))
+        if attachs:
-            msg.attach(part)
+    def _sent_failed(self, failure, to, cc, subject, nattachs):
-        smtp.close()
+    def _sendmail(self, smtphost, from_addr, to_addrs, msg, port=25):
-    'scrapy.contrib.debug.StackTraceDebug',
+    'scrapy.contrib.debug.StackTraceDump',
-class StackTraceDebug(object):
+class StackTraceDump(object):
-Crawler logging functionality
+Scrapy logging facility
-See documentation in docs/topics/email.rst
+See documentation in docs/ref/email.rst
-    'scrapy.contrib.webconsole.schedstats.SchedulerStats',
+"""
-you can do with the getsoup() method).
+ResponseSoup extension 
-http://www.crummy.com/software/BeautifulSoup/documentation.html
+See documentation in docs/ref/extensions.rst
-from scrapy.management.web import banner
+from scrapy.management.web import banner, webconsole_discover_module
-        pstats.started = datetime.now()
+        pstats.started = datetime.now().replace(microsecond=0)
-        self.domains[spider.domain_name].finished = datetime.now()
+        self.domains[spider.domain_name].finished = datetime.now().replace(microsecond=0)
-        #for example from scrapy shell
+        # sometimes we download responses without opening/closing domains,
-from scrapy.management.web import banner
+from scrapy.management.web import banner, webconsole_discover_module
-        from scrapy.management.web import webconsole_discover_module
+    def webconsole_discover_module(self):
-        return self
+        enabled_domains = spiders.asdict(include_disabled=False).keys()
-            scrapyengine.scheduler.pending_domains = [d for d in execengine.scheduler.pending_domains if d not in removed]
+            scrapyengine.scheduler.pending_domains = [d for d in scrapyengine.scheduler.pending_domains if d not in removed]
-    s += "<p>Bot: <b>%s</b> | Host: <b>%s</b> | Uptime: <b>%s</b> | Time: <b>%s</b> </p>\n" % (settings['BOT_NAME'], socket.gethostname(), str(uptime), str(now))
+    s += "<p>Bot: <b>%s</b> | Host: <b>%s</b> | Uptime: <b>%s</b> | Time: <b>%s</b> </p>\n" % (settings['BOT_NAME'], socket.gethostname(), str(uptime), str(now.replace(microsecond=0)))
-            return self.modules[m.group(1)].webconsole_render(request)
+            module = m.group(1)
-                or request.url.hostname.endswith('s3.amazonaws.com'):
+                or (request.url.hostname and request.url.hostname.endswith('s3.amazonaws.com')):
-        log.msg("%s: %s" % (levels[level], message), system=component)
+        msg_txt = unicode_to_str("%s: %s" % (levels[level], message))
-        raise TypeError('str_to_unicode can only receive a string object')
+        raise TypeError('str_to_unicode must receive a str or unicode object, got %s' % type(text).__name__)
-        raise TypeError('unicode_to_str can only receive a unicode object')
+        raise TypeError('unicode_to_str must receive a unicode or str object, got %s' % type(text).__name__)
-from scrapy.http import ResponseBody
+from scrapy.http import Response, ResponseBody
-        self.assertEqual(isinstance(body.to_string('utf-8'), str), True)
+class ResponseBodyTest(TestCase):
-    uncompressed_body = ''
+def setUp():
-    middleware = DecompressionMiddleware()
+    for format in formats:
-            self.test_responses[format] = Response('foo.com', 'http://foo.com/bar', body=body)
+class ScrapyDecompressionTest(TestCase):
-        newresp.body = kw.get('body', samebody())
+                           status=kw.get('status', self.status),
-                           body=kw.get('body', samebody()))
+                           status=kw.get('status', self.status))
-    def process_spider_output(self, response, results):
+    def process_results(self, response, results):
-            cb_res = self.process_spider_output(response, cb_res)
+            cb_res = self.process_results(response, cb_res)
-    def process_spider_output(self, response, results):
+    def process_results(self, response, results):
-            for result_item in self.process_spider_output(response, ret):
+            for result_item in self.process_results(response, ret):
-    def process_spider_output(self, response, results):
+    def process_results(self, response, results):
-            for result_item in self.process_spider_output(response, ret):
+            for result_item in self.process_results(response, ret):
-html_style = 'default.css'
+html_style = 'scrapydoc.css'
-    We will have a limited number of connections per domain and scrape many domains in
+    """Maintain many concurrent downloads and provide an HTTP abstraction.
-    def __init__(self) :
+    def __init__(self, engine):
-            scrapyengine.closed_domain(domain=domain)
+            self.engine.closed_domain(domain)
-        self.downloader = downloader or Downloader()
+        self.downloader = downloader or Downloader(self)
-              Link(url='http://example.com/sample2.html', text=u'sample 2') ])
+#        lx = RegexLinkExtractor(restrict_xpaths=('//div[@id="subwrapper"]', ))
-            link.url = urljoin(base_url, link.url).strip()
+            link.url = urljoin(base_url, link.url)
-            self.current_link.text = data
+            self.current_link.text = data.strip()
-              Link(url='http://www.google.com/something', text='') ])
+            [ Link(url='http://example.com/sample1.html', text=u''),
-              Link(url='http://example.com/sample3.html', text='sample 3 text') ])
+            [ Link(url='http://example.com/sample1.html', text=u''),
-              Link(url='http://example.com/sample3.html', text='sample 3 repetition') ])
+            [ Link(url='http://example.com/sample1.html', text=u''),
-              Link(url='http://example.com/sample2.html', text='') ])
+            [ Link(url='http://example.com/sample1.html', text=u''),
-            [ Link(url='http://www.google.com/something', text='') ])
+            [ Link(url='http://www.google.com/something', text=u'') ])
-            [ Link(url='http://example.com/sample2.jpg', text='') ])
+            [ Link(url='http://example.com/sample2.jpg', text=u'') ])
-    def process_spider_output(self, results, response):
+    def process_spider_output(self, response, results):
-            cb_res = self.process_spider_output(cb_res, response)
+            cb_res = self.process_spider_output(response, cb_res)
-    def process_results(self, results, response):
+    def process_spider_output(self, response, results):
-            for result_item in self.process_results(ret, response):
+            for result_item in self.process_spider_output(response, ret):
-    def process_results(self, results, response):
+    def process_spider_output(self, response, results):
-            for result_item in self.process_results(ret, response):
+            for result_item in self.process_spider_output(response, ret):
-            update_defaults(settings.overrides, module)
+            update_defaults(settings.defaults, module)
-            links = self._extract_links(html_slice, response.url)
+            links = self._extract_links(html_slice, response.url, response.body.get_real_encoding())
-            update_defaults(settings.defaults, module)
+            update_defaults(settings.overrides, module)
-def extract(locations, use_unquote=True):
+def extract(locations, adaptor_args=None):
-
+    use_unquote = adaptor_args.get('use_unquote', True) if adaptor_args else True
-            return AdaptorPipe(self + [other])
+        if callable(other):
-from scrapy.contrib.adaptors.extraction import extract, extract_unquoted, ExtractImages
+from scrapy.contrib.adaptors.extraction import extract, ExtractImages
-def extract(locations):
+def extract(locations, use_unquote=True):
-    or an XPathSelector/XPathSelectorList.
+    This adaptor tries to extract data from the given locations.
-    return _extract(locations)
+    If an XPathSelector is a text/cdata node, and `use_unquote`
-def extract_unquoted(locations):
+    Input: anything
-    return _extract(locations, 'extract_unquoted')
+    locations = flatten([locations])
-        self.assertEqual(adaptors.extract_unquoted(x.x('//tag2/text()')), [u'blaheawfds<'])
+        self.assertEqual(adaptors.extract(x.x('//tag1/text()')), [u'test text & &', u'more test text &amp; &gt;', u'blah&blah'])
-from scrapy.utils.python import flatten
+from scrapy.utils.python import flatten, unicode_to_str
-            response = Response(domain=None, url=None, body=str(text))
+            response = Response(domain=None, url=None, body=unicode_to_str(text))
-    domains = [spider.domain_name] + spider.extra_domain_names
+    domains = [spider.domain_name]
-            cb_res = self.process_results(cb_res, response)
+            cb_res = self.process_spider_output(cb_res, response)
-    def process_scrape(self, response, spider):
+    def process_spider_input(self, response, spider):
-    def process_result(self, response, result, spider):
+    def process_spider_output(self, response, result, spider):
-    def process_result(self, response, result, spider):
+    def process_spider_output(self, response, result, spider):
-    def process_result(self, response, result, spider):
+    def process_spider_output(self, response, result, spider):
-    def process_result(self, response, result, spider):
+    def process_spider_output(self, response, result, spider):
-    def process_result(self, response, result, spider):
+    def process_spider_output(self, response, result, spider):
-    def process_result(self, response, result, spider):
+    def process_spider_output(self, response, result, spider):
-    def process_result(self, response, result, spider):
+    def process_spider_output(self, response, result, spider):
-    def process_result(self, response, result, spider):
+    def process_spider_output(self, response, result, spider):
-    def process_results(self, results, response):
+    def process_spider_output(self, results, response):
-Spider middleware manager
+This module implements the Spider Middleware manager. For more information see
-            self.exception_middleware.insert(0, mw.process_exception)
+        if hasattr(mw, 'process_spider_input'):
-        def process_scrape(response):
+        def process_spider_input(response):
-        def process_result(result):
+        def process_spider_output(result):
-        def process_exception(_failure):
+        def process_spider_exception(_failure):
-        dfd.addCallback(process_result)
+        dfd = mustbe_deferred(process_spider_input, response)
-
+DEFAULT_SPIDER = None
-version = '0.7'
+version = '0.7.0'
-release = '0.7'
+release = version
-html_copy_source = False
+html_copy_source = True
-
+        super(RobustScrapedItem, self).__init__(data)
-    _adaptors_dict = {}
+
-        add = kwargs.pop('add', False)
+    def attribute(self, attrname, value, override=False, add=False, **kwargs):
-        elif value:
+            value = pipe(value, **kwargs)
-        raise NotImplementedError
+# -*- coding: utf8 -*-
-__version__ = "0.1.0"
+__version__ = "0.7.0"
-    url(r"^docs/", include("scrapyorg.docs.urls")),
+#    url(r"^docs/", include("scrapyorg.docs.urls")),
-from scrapy.http import Response, ResponseBody
+from scrapy.http import Response
-                response.body = ResponseBody(decoded_body, declared_encoding)
+                response = response.replace(body=decoded_body)
-from scrapy.http import Response, ResponseBody
+from scrapy.http import Response
-            return response.replace(body=ResponseBody(tar_file.extractfile(tar_file.members[0]).read()))
+            return response.replace(body=tar_file.extractfile(tar_file.members[0]).read())
-            return response.replace(body=ResponseBody(zip_file.read(namelist[0])))
+            return response.replace(body=zip_file.read(namelist[0]))
-        return response.replace(body=ResponseBody(decompressed_body))
+        return response.replace(body=decompressed_body)
-            self.body = ResponseBody(body, self.headers_encoding())
+        # ResponseBody is not meant to be used directly (use .replace instead)
-from scrapy.http import Response, ResponseBody, Headers
+from scrapy.http import Response, Headers
-        sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=ResponseBody(test_data))
+        sample_response = Response('foobar.com', 'http://foobar.com/dummy', body=test_data)
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-    
+
-from scrapy.http import ResponseBody,  Response
+from scrapy.http import Response
-        response = Response('example.com', 'http://example.com/catalog/product/blabla-123', body=self.problematic_body)
+        response = Response('example.com', 'http://example.com/catalog/product/blabla-123',
-from scrapy.http.response import Response, ResponseBody
+from scrapy.http.response import Response
-        response_noenc = Response(url='http://example.com/noenc', domain='example.com', body=ResponseBody(body))
+        response_utf8 = Response(url='http://example.com/utf8', domain='example.com', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})
-        response_latin1 = Response(url='http://example.com/latin1', domain='example.com', body=ResponseBody(body))
+        response_latin1 = Response(url='http://example.com/latin1', domain='example.com', body=body)
-        self.response = Response(url='http://example.com/index', domain='example.com', body=ResponseBody(body))
+        self.response = Response(url='http://example.com/index', domain='example.com', body=body)
-#        self.response = Response(url='http://example.com/index', domain='example.com', body=ResponseBody(body))
+#        self.response = Response(url='http://example.com/index', domain='example.com', body=body)
-from scrapy.http import Response, ResponseBody
+from scrapy.http import Response
-    
+
-        
+
-            body = ResponseBody(fd.read())
+            body = fd.read()
-    
+
-        
+
-    
+
-        
+
-        self.assertEqual(adaptors.Unquote(keep=[])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&welcome', u'<br />&'])
+        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&welcome', u'<br />&'])
-        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
+        self.assertEqual(adaptors.Unquote(keep=['amp', 'lt'])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
-        self.keep = ['amp', 'lt'] if keep is None else keep
+        self.keep = [] if keep is None else keep
-        return [ unquote_markup(v, keep=self.keep) for v in value ]
+        keep = keep if keep is not None else self.keep
-documentation in docs/reference/settings.rst
+documentation in docs/ref/settings.rst
-  (docs/reference/settings.rst)
+  (docs/ref/settings.rst)
-subject.
+This is a middleware to respect robots.txt policies. To active it you must
-from scrapy import log
+from scrapy.core.engine import scrapyengine
-class RobotsMiddleware(object):
+class RobotsTxtMiddleware(object):
-        urldomain = urlparse.urlparse(url).hostname
+        parsedurl = urlparse.urlparse(url)
-            rp = self._parsers[urldomain]
+            return self._parsers[urldomain]
-            self._parsers[urldomain] = rp
+            self._parsers[urldomain] = None
-        return rp
+
-request-response middleware extension
+This module implements the Downloader Middleware manager. For more information
-copyright = u'2008, Insophia'
+copyright = u'2008-2009, Insophia'
-#html_last_updated_fmt = '%b %d, %Y'
+html_last_updated_fmt = '%b %d, %Y'
-#html_use_smartypants = True
+html_use_smartypants = True
-from scrapy.utils.python import FixedSGMLParser
+from scrapy.utils.python import FixedSGMLParser, unique as unique_list
-            links = [link for link in links if not _seen(link.url)]
+        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links
-        offset = text_len = len(text)
+        offset = len(text)
-def unique(list_):
+def unique(list_, key=lambda x: x):
-        if item in seen: 
+        seenkey = key(item)
-        seen[item] = 1
+        seen[seenkey] = 1
-from scrapy.item.pipeline import ItemPipeline
+from scrapy.item.pipeline import ItemPipelineManager
-        self.pipeline = ItemPipeline()
+        self.pipeline = ItemPipelineManager()
-class ItemPipeline(object):
+class ItemPipelineManager(object):
-    pass
+    pass
-version = '1.0'
+version = '0.7'
-release = '1.0'
+release = '0.7'
-def str_to_unicode(text):
+def str_to_unicode(text, encoding='utf-8'):
-        return text.decode('utf-8')
+        return text.decode(encoding)
-def unicode_to_str(text):
+def unicode_to_str(text, encoding='utf-8'):
-        return text.encode('utf-8')
+        return text.encode(encoding)
-    s = url.encode(use_encoding) if isinstance(url, unicode) else url
+    s = unicode_to_str(url, use_encoding)
-from scrapy.utils.url import urljoin_rfc as urljoin
+from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin
-    this case the values are the text of the hyperlink.
+    method which must receive a Response object and return a list of Link objects
-    links to follow as its keys.
+    extract_links method that receives a Response and returns a list of Link objects.
-    def _extract_links(self, response_text, response_url):
+    def _extract_links(self, response_text, response_url, response_encoding):
-        base_url = self.base_url if self.base_url else response_url
+        links = self.links
-        for link in self.links:
+        ret = []
-        return links
+            link.url = safe_url_string(link.url, response_encoding)
-        return self._extract_links(response.body.to_string(), response.url)
+        return self._extract_links(response.body.to_string(),
-                        self.current_link = link
+                    link = Link(url=value)
-        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func)
+        LinkExtractor.__init__(self, tag=tag_func, attr=attr_func, unique=unique)
-#        self.response = Response(url='http://examplesite.com/index', domain='examplesite.com', body=ResponseBody(body))
+#        base_path = os.path.join(os.path.dirname(__file__), 'sample_data', 'link_extractor')
-#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])
+#            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),
-#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4 repetition') ])
+#            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),
-#              Link(url='http://examplesite.com/sample4.jpg', text=u'sample 4') ])
+#            [ Link(url='http://example.com/sample1.jpg', text=u'sample 1'),
-#              Link(url='http://examplesite.com/sample3.html', text=u'sample 3') ])
+#            [ Link(url='http://example.com/sample2.jpg', text=u'sample 2'),
-    """Convert a unicode (or utf8 string) object into a legal URL.
+    """Convert a unicode object (using 'use_encoding' as the encoding), or an already
-    function strings in encodings other than utf8.
+    It is safe to call this function multiple times.
-    s = url.encode(use_encoding)
+    s = url.encode(use_encoding) if isinstance(url, unicode) else url
-
+import unittest
-        else:
+        # FIXME this should be replaced with assert(isinstance(body, basestring)) since ResponseBody is not meant to be used directly
-            yield link
+            links.append(link)
-        links = (link for link in links if _is_valid_url(link.url))
+        links = [link for link in links if _is_valid_url(link.url)]
-            links = (link for link in links if _matches(link.url, self.allow_res))
+            links = [link for link in links if _matches(link.url, self.allow_res)]
-            links = (link for link in links if not _matches(link.url, self.deny_res))
+            links = [link for link in links if not _matches(link.url, self.deny_res)]
-            links = (link for link in links if url_is_from_any_domain(link.url, self.allow_domains))
+            links = [link for link in links if url_is_from_any_domain(link.url, self.allow_domains)]
-            links = (link for link in links if not url_is_from_any_domain(link.url, self.deny_domains))
+            links = [link for link in links if not url_is_from_any_domain(link.url, self.deny_domains)]
-            links = (canonicalize_url(link.url) for link in links)
+            for link in links:
-        self.assertEqual(links,
+        self.assertEqual(lx.extract_links(response),
-        self.assertEqual(links,
+        self.assertEqual(lx.extract_links(response),
-    def __init__(self, tag="a", attr="href"):
+    def __init__(self, tag="a", attr="href", unique=False):
-    def _extract_links(self, response_text, response_url, unique):
+    def _extract_links(self, response_text, response_url):
-    def extract_links(self, response, unique=False):
+    def extract_links(self, response):
-        return self._extract_links(response.body.to_string(), response.url, unique)
+        return self._extract_links(response.body.to_string(), response.url)
-from scrapy.link import LinkExtractor, Link
+from scrapy.link import LinkExtractor
-                 tags=('a', 'area'), attrs=('href'), canonicalize=True):
+                 tags=('a', 'area'), attrs=('href'), canonicalize=True, unique=True):
-    def extract_links(self, response, unique=True):
+    def extract_links(self, response):
-            links = self._extract_links(html_slice, response.url, unique)
+            links = self._extract_links(html_slice, response.url)
-            links = LinkExtractor.extract_links(self, response, unique)
+            links = LinkExtractor.extract_links(self, response)
-from scrapy.link.extractors import RegexLinkExtractor, ImageLinkExtractor
+from scrapy.link.extractors import RegexLinkExtractor
-#class ImageLinkExtractorTestCase(unittest.TestCase):
+#class HTMLImageLinkExtractorTestCase(unittest.TestCase):
-#        links_1 = lx.extract_links(self.response) # using default locations (//img)
+#        lx = HTMLImageLinkExtractor(locations=('//img', ))
-#        links_2 = lx.extract_links(self.response, unique=False) # using default locations and unique=False
+#        lx = HTMLImageLinkExtractor(locations=('//img', ), unique=False)
-#        links_3 = lx.extract_links(self.response, locations=('//div[@id="wrapper"]', ))
+#        lx = HTMLImageLinkExtractor(locations=('//div[@id="wrapper"]', )
-#        links_4 = lx.extract_links(self.response, locations=('//a', ))
+#        lx = HTMLImageLinkExtractor(locations=('//a', )
-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector, XPathSelectorList
+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector
-    def extract_links(self, response, unique=False):
+    def _extract_links(self, response_text, response_url, unique):
-        self.feed(response.body.to_string())
+        self.feed(response_text)
-        ret = []
+
-        return ret
+            yield link
-from scrapy.xpath.selector import HtmlXPathSelector
+from scrapy.xpath import HtmlXPathSelector
-    
+
-            response = new_response_from_xpaths(response, self.restrict_xpaths)
+            hxs = HtmlXPathSelector(response)
-        links = [link for link in links if _is_valid_url(link.url)]
+        links = (link for link in links if _is_valid_url(link.url))
-            links = [link for link in links if _matches(link.url, self.allow_res)]
+            links = (link for link in links if _matches(link.url, self.allow_res))
-            links = [link for link in links if not _matches(link.url, self.deny_res)]
+            links = (link for link in links if not _matches(link.url, self.deny_res))
-            links = [link for link in links if url_is_from_any_domain(link.url, self.allow_domains)]
+            links = (link for link in links if url_is_from_any_domain(link.url, self.allow_domains))
-        
+            links = (link for link in links if not url_is_from_any_domain(link.url, self.deny_domains))
-                link.url = canonicalize_url(link.url)
+            links = (canonicalize_url(link.url) for link in links)
-            
+
-        self.assertEqual(lx.extract_links(response),
+        links = [link for link in lx.extract_links(response)]
-        self.assertEqual(lx.extract_links(response),
+        links = [link for link in lx.extract_links(response)]
-        return obj if unicode else obj.encode('utf-8')
+from scrapy.utils.python import re_rsearch, str_to_unicode
-    text = _normalize_input(obj)
+    text = body_or_str(obj)
-    
+
-    lines = _normalize_input(obj, unicode=False).splitlines(True)
+    lines = body_or_str(obj, unicode=False).splitlines(True)
-from scrapy.http.response import ResponseBody
+from scrapy.http.response import Response
-    return response.replace(body=ResponseBody(content=new_body_content, declared_encoding=response.body.get_declared_encoding()))
+def body_or_str(obj, unicode=True):
-              Link(url='http://examplesite.com/sample3.html', text=u'sample 3') ])
+#class ImageLinkExtractorTestCase(unittest.TestCase):
-            links = [l for l in rule.link_extractor.extract_urls(response) if l not in seen]
+            links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]
-    instantiated and later "applied" to a Response using the extract_urls
+    instantiated and later "applied" to a Response using the extract_links
-    extract_urls method that receives a Response and returns a dict with the
+    extract_links method that receives a Response and returns a dict with the
-    def extract_urls(self, response, unique=False):
+    def extract_links(self, response, unique=False):
-    def extract_urls(self, response, unique=True):
+    def extract_links(self, response, unique=True):
-        links = LinkExtractor.extract_urls(self, response, unique)
+        links = LinkExtractor.extract_links(self, response, unique)
-        self.assertEqual(lx.extract_urls(response), 
+        self.assertEqual(lx.extract_links(response),
-        self.assertEqual(lx.extract_urls(response), 
+        self.assertEqual(lx.extract_links(response),
-        for link in xlink.extract_urls(response):
+        for link in xlink.extract_links(response):
-from scrapy.link import LinkExtractor
+from scrapy.link import LinkExtractor, Link
-from scrapy.utils.misc import dict_updatedefault
+from scrapy.utils.python import unicode_to_str
-from scrapy.http import Response
+from scrapy.http.response import Response, ResponseBody
-from scrapy.link.extractors import RegexLinkExtractor
+from scrapy.link.extractors import RegexLinkExtractor, ImageLinkExtractor
-from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector
+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector, XPathSelectorList
-        self._enabled_spiders = self._enabled_spiders()
+        self._enabled_spiders_set = self._enabled_spiders()
-            if spider.domain_name in self._enabled_spiders:
+            if spider.domain_name in self._enabled_spiders_set:
-        return self._alldict.get(self.default_domain)
+        spider = self._alldict.get(self.default_domain)
-        enabled_spiders = self._enabled_spiders()
+        self._enabled_spiders = self._enabled_spiders()
-                    print "WARNING: Could not load spider %s: %s" % (spider, e)
+                self.add_spider(spider)
-    core = None
+    global_defaults = None
-        self.core = self._import('scrapy.conf.core_settings')
+        self.global_defaults = self._import('scrapy.conf.default_settings')
-            return getattr(self.core, opt_name)
+        if hasattr(self.global_defaults, opt_name):
-RETRY_HTTP_CODES = ['500', '503', '504', '400', '408', '200']
+"""
-Default values are located in scrapy.conf.core_settings
+Default values are located in scrapy.conf.default_settings, like any other
-    app.add_description_unit(
+    app.add_crossref_type(
-    url(r"^blog/", include("scrapyorg.blog.urls")),
+    # news
-from scrapy.utils.python import flatten, str_to_unicode
+from scrapy.utils.python import flatten, str_to_unicode, unicode_to_str
-    return [ _clean_spaces_re.sub(' ', v.decode('utf-8')) for v in value ]
+    return [ _clean_spaces_re.sub(' ', str_to_unicode(v)) for v in value ]
-        return [canonicalize_url(str(url)) for url in value]
+        return [canonicalize_url(unicode_to_str(url)) for url in value]
-        return canonicalize_url(str(value))
+        return canonicalize_url(unicode_to_str(value))
-        check_encoding(x, pound=False, euro=False)
+        check_extractor(x, pound=False, euro=False)
-from scrapy.utils.python import flatten
+from scrapy.utils.python import flatten, str_to_unicode
-        return [ unicode(v) for v in value ]
+        return [ str_to_unicode(v) if isinstance(v, basestring) else str_to_unicode(str(v)) for v in value ]
-from scrapy.xpath.selector import XmlXPathSelector
+from scrapy.http import Response, ResponseBody, Headers
-        
+        def check_extractor(x, pound=True, euro=True):
-        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'lulu', 1, '']),
+        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'lulu\xc3\xb1', 1, '\xc3\xa1\xc3\xa9']),
-from scrapy.utils.python import re_rsearch
+from scrapy.utils.python import re_rsearch, str_to_unicode, unicode_to_str
-def _normalize_input(obj):
+def _normalize_input(obj, unicode=True):
-        return obj.body.to_unicode()
+        return obj.body.to_unicode() if unicode else obj.body.to_string()
-        return obj.decode('utf-8')
+        return obj.decode('utf-8') if unicode else obj
-        return obj
+        return obj if unicode else obj.encode('utf-8')
-        return [field.decode() for field in csv_r.next()]
+        return [str_to_unicode(field) for field in csv_r.next()]
-    lines = _normalize_input(obj).splitlines(True)
+    lines = _normalize_input(obj, unicode=False).splitlines(True)
-    url = url.encode('utf-8')
+    url = unicode_to_str(url)
-                text = unicode(self.xmlNode.content, errors='ignore')
+                text = unicode(self.xmlNode.content, 'utf-8', errors='ignore')
-                text = unicode(self.xmlNode, errors='ignore')
+                text = unicode(self.xmlNode, 'utf-8', errors='ignore')
-            return unicode(self.xmlNode.getContent(), errors='ignore')
+            return unicode(self.xmlNode.getContent(), 'utf-8', errors='ignore')
-from scrapy.core.engine import scrapyengine
+    from scrapy.core.engine import scrapyengine
-            perdomain.setdefault(domain, []).append(request)
+            perdomain.setdefault(spider.domain_name, []).append(request)
-                perdomain.setdefault(domain, []).extend(reqs)
+                perdomain.setdefault(spider.domain_name, []).extend(reqs)
-                _add(domain, request)
+            reqs = spider.start_requests()
-                _add(spider.domain_name, request)
+                reqs = spider.start_requests([url])
-            _add(spider.domain_name, request)
+            perdomain.setdefault(domain, []).append(request)
-        
+from scrapy.http import Request
-
+    class.
-            fragments.append(match)
+            yield txt[offset:match_s]
-        return fragments
+        yield txt[offset:]
-def items_to_csv(file, items, delimiter=';'):
+def items_to_csv(file, items, delimiter=';', headers=None):
-    if it lacks any attribute that other item has, that attribute will be missing.
+    The saved attributes are either the ones found in the 'headers' parameter
-    header = sorted([key for key in items[0].__dict__.keys() if not key.startswith('_')])
+    header = headers or sorted([key for key in items[0].__dict__.keys() if not key.startswith('_')])
-from scrapy.utils.misc import hash_values
+from scrapy.utils.misc import hash_values, items_to_csv
-from scrapy.utils.python import flatten
+from scrapy.utils.python import flatten, unicode_to_str
-from scrapy.core.exceptions import UsageError, NotConfigured
+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector
-    or not using it (which just splits the tags using xpath)
+    You can choose whether to parse the file using the 'iternodes' iterator,
-    iternodes = True
+    iterator = 'iternodes'
-        if self.iternodes:
+        if self.iterator == 'iternodes':
-        else:
+        elif self.iterator == 'xml':
-    return _ent_re.sub(convert_entity, text)
+    return _ent_re.sub(convert_entity, str_to_unicode(text))
-    return bool(_ent_re.search(text))
+    return bool(_ent_re.search(str_to_unicode(text)))
-    return _tag_re.sub(token, text)
+    return _tag_re.sub(token, str_to_unicode(text))
-    return re.sub('<!--.*?-->', u'', text, re.DOTALL)
+    return re.sub('<!--.*?-->', u'', str_to_unicode(text), re.DOTALL)
-    return re_tags.sub(u'', text)
+    return re_tags.sub(u'', str_to_unicode(text))
-    return re_tags_remove.sub(u'', text)
+    return re_tags_remove.sub(u'', str_to_unicode(text))
-    return re_escape_chars.sub(u'', text)
+    return re_escape_chars.sub(u'', str_to_unicode(text))
-
+    text = str_to_unicode(text)
-    'text' must be a unicode string.
+    'text' can be a unicode string or a regular string encoded as 'utf-8'
-    'text' must be a unicode string.
+    'text' can be a unicode string or a regular string encoded as 'utf-8'
-    This function receives markup as a text (always a unicode string) and does the following:
+    This function receives markup as a text (always a unicode string or a utf-8 encoded string) and does the following:
-    'text' can be a unicode string or a regular string encoded as 'utf-8'
+    'text' must be a unicode string.
-    return _ent_re.sub(convert_entity, text.decode('utf-8'))
+    return _ent_re.sub(convert_entity, text)
-    'text' can be a unicode string or a regular string encoded as 'utf-8'
+    'text' must be a unicode string.
-    return _tag_re.sub(token, text.decode('utf-8'))
+    return _tag_re.sub(token, text)
-    return re.sub('<!--.*?-->', u'', text.decode('utf-8'), re.DOTALL)
+    return re.sub('<!--.*?-->', u'', text, re.DOTALL)
-    return re_tags.sub(u'', text.decode('utf-8'))
+    return re_tags.sub(u'', text)
-    return re_tags_remove.sub(u'', text.decode('utf-8'))
+    return re_tags_remove.sub(u'', text)
-    return re_escape_chars.sub(u'', text.decode('utf-8'))
+    return re_escape_chars.sub(u'', text)
-    This function receives markup as a text and does the following:
+    This function receives markup as a text (always a unicode string) and does the following:
-    for fragment in _get_fragments(text.decode('utf-8'), _cdata_re):
+    ret_text = u''
-    return unicode(ret_text)
+    return ret_text
-        if spider.domain_name in 's3.amazonaws.com':
+        if spider.domain_name == 's3.amazonaws.com' \
-from scrapy.core.downloader.handlers import download_any
+from scrapy.core.engine import scrapyengine
-    def get_url(self, url, decompress=False):
+    def get_url(self, url):
-        r = Request(url)
+        r = Request(url, callback=_get_response)
-            print "Error: %s" % e
+        blockingCallFromThread(reactor, scrapyengine.crawl, r, spider)
-        scrapymanager.start()
+        scrapymanager.start()
-        """
+        body = """<?xml version="1.0" encoding="UTF-8"?>\
-        body = """
+        body = """\
-        self.assertRaises(libxml2.xpathError, node.x, 'g:price/text()')
+        self.assertEqual(node.x('image_link/text()').extract(), [])
-from scrapy.utils.markup import replace_tags, remove_entities
+from scrapy.utils.markup import replace_tags, unquote_markup
-    a new list
+    CDATAs and entities (except the ones in CDATAs) the strings
-        return [ remove_entities(v, keep=self.keep) for v in value ]
+        return [ unquote_markup(v, keep=self.keep) for v in value ]
-        to into the feed before parsing it. This function must return a response."""
+        """This method has the same purpose as the one in XMLFeedSpider"""
-        yield XmlXPathSelector(text=nodetext).x('/' + nodename)[0]
+        nodetext = header_start + match.group() + header_end
-
+import re
-def unquote_markup(text):
+def unquote_markup(text, keep=(), remove_illegal=True):
-     - removes entities from any part of it that it's not inside a CDATA
+     - removes entities (except the ones in 'keep') from any part of it that it's not inside a CDATA
-            ret_text += remove_entities(fragment)
+            ret_text += remove_entities(fragment, keep=keep, remove_illegal=remove_illegal)
-        _adaptors_dict = dict(item for item in adaptors_dict.items() if isinstance(item[1], AdaptorPipe))
+        _adaptors_dict = dict((attrib, AdaptorPipe(pipe)) for attrib, pipe in adaptors_dict.items() if hasattr(pipe, '__iter__'))
-            pprint.pprint(list(ddh.getall(domain, opts.path)))
+            pprint.pprint(list(ddh.get(domain, path=opts.path, count=int(opts.limit))))
-html_static_path = ['.static']
+html_static_path = ['_static']
-#sys.path.append(os.path.abspath('.'))
+sys.path.append(os.path.join(os.path.dirname(__file__), "_ext"))
-extensions = []
+extensions = ['scrapydocs']
-
+import sys, os
-    
+from scrapy import log
-    
+        self.decompressors = {
-    
+
-            
+
-            
+
-    def extract_winfo(self, response):
+
-        
+
-        return self.extract_winfo(response)[0]
+                return (new_response, decompressor)
-from scrapy.utils.decompressor import Decompressor
+from scrapy.contrib.downloadermiddleware.decompression import DecompressionMiddleware
-class ScrapyDecompressTest(TestCase):
+class ScrapyDecompressionTest(TestCase):
-    decompressor = Decompressor()
+    middleware = DecompressionMiddleware()
-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)
+        response, format = self.middleware.extract(self.test_responses['tar'])
-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)
+        response, format = self.middleware.extract(self.test_responses['zip'])
-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)
+        response, format = self.middleware.extract(self.test_responses['xml.gz'])
-            self.assertEqual(ret.body.to_string(), self.uncompressed_body)
+        response, format = self.middleware.extract(self.test_responses['xml.bz2'])
-                stats.incpath('%s/http_error_count/%d' % (spider.domain_name, int(exception.status)))
+from scrapy.core.exceptions import HttpException, NotConfigured
-        
+from scrapy.stats import stats
-RETRY_TIMES = 3
+RETRY_TIMES = 2 # initial response + 2 retries = 3 requests
-        self.max_retries = settings.getint('RETRY_TIMES')
+        self.retry_times = settings.getint('RETRY_TIMES')
-        if retry:
+        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) or (isinstance(exception, HttpException) and (int(exception.status) in self.retry_http_codes)):
-            count = self.failed_count[fp] = self.failed_count.get(fp, 0) + 1
+            self.failed_count[fp] = self.failed_count.get(fp, 0) + 1
-                log.msg("Retrying %s (failed %d times): %s" % (request, count, exception), level=log.DEBUG, domain=spider.domain_name)
+            if self.failed_count[fp] <= self.retry_times:
-                log.msg("Discarding %s (failed %d times): %s" % (request, count, exception), domain=spider.domain_name, level=log.DEBUG)
+                log.msg("Discarding %s (failed %d times): %s" % (request, self.failed_count[fp], exception), domain=spider.domain_name, level=log.DEBUG)
-        self.stores[domain][guid] = item
+        if not status == 'old':
-New is a "new generation" shelve. For more information see: 
+Shove is a "new generation" shelve. For more information see: 
-        self.stores[domain][str(item.guid)] = item
+        guid = str(item.guid)
-        self.assertEqual(sample_adaptor(sample_xsel.x('//@href')),
+        self.assertEqual(sample_adaptor(None), [])
-                          u'http://foobar.com/pepepe/papapa/lala3.html', u'http://foobar.com/lala4.html'])
+                          u'http://foobar.com/pepepe/papapa/lala3.html', u'http://foobar.com/lala4.html', u'http://foobar.com/my_image.gif'])
-        if isinstance(locations, basestring):
+        if not locations:
-        return self.download(request, info.spider)
+        return self.download(request, info)
-    def print_results(self, items, links, opts):
+    def print_results(self, items, links, cb_name, opts):
-            print "# Scraped Items", "-"*60
+            print "# Scraped Items - callback: %s" % cb_name, "-"*60
-            print "# Links", "-"*68
+            print "# Links - callback: %s" % cb_name, "-"*68
-                continue
+                    self.print_results(items, links, callback, opts)
-                rules = getattr(spider, 'rules')
+                rules = getattr(spider, 'rules', None)
-                    items, links = self.run_callback(spider, response, 'parse', args, opts)
+                    log.msg('No rules found for spider "%s", please specify a callback for parsing' % spider.domain_name)
-        self.print_results(ret_items, ret_links, opts)
+                self.print_results(items, links, 'parse', opts)
-from twisted.trial import unittest
+from scrapy.store.db import DomainDataHistory
-    
+    test_db = settings.get('TEST_SCRAPING_DB')
-        
+
-def mysql_connect(db_uri, **kwargs):
+class URIValidationError(Exception):
-    Connects to a MySQL DB given a mysql URI
+    Parse mysql URI and return settings dict
-    import MySQLdb
+    if not (isinstance(db_uri, basestring) and db_uri.startswith('mysql://')):
-        raise Exception("Incorrect MySQL URI: %s" % db_uri)
+        return d
-        
+
-        
+
-        
+
-        
+
-                    links.extend(r_links)
+                    items, links = self.run_callback(spider, response, callback, args, opts)
-                        break
+                rules = getattr(spider, 'rules')
-                    continue
+                    log.msg('No rules found for spider "%s", calling default method "parse"' % spider.domain_name)
-        parser.add_option("-c", "--callback", dest="callback", action="store", help="use the provided callback for parsing the url")
+        parser.add_option("-c", "--callbacks", dest="callbacks", action="store", help="use the provided callback(s) for parsing the url (separated with commas)")
-    def run_method(self, spider, response, method, args, opts):
+    def run_callback(self, spider, response, callback, args, opts):
-                log.msg('Cannot find method %s in %s spider' % (method, spider.domain_name))
+        if callback:
-            result = method_fcn(response)
+            result = callback_fcn(response)
-        return items, links
+        return (), ()
-                items, links = self.run_method(spider, response, opts.callback, args, opts)
+            if self.callbacks:
-                        items, links = self.run_method(spider, response, rule.callback, args, opts)
+                        items, links = self.run_callback(spider, response, rule.callback, args, opts)
-                    log.msg('No rules found for spider "%s", please specify a parsing method' % spider.domain_name)
+                    log.msg('No rules found for spider "%s", please specify a parsing callback' % spider.domain_name)
-                items, links = self.run_method(spider, response, 'parse', args, opts)
+                items, links = self.run_callback(spider, response, 'parse', args, opts)
-    def s3request(self, key, method, body=None, headers=None):
+    def s3_request(self, key, method, body=None, headers=None):
-        dfd = self.download(req, info)
+        req = self.s3_request(key, method='HEAD')
-        return self.download(req, info), buf
+        req = self.s3_request(key, method='PUT', body=buf.read(), headers=headers)
-                    log.msg('Could not found spider for %s' % a, log.ERROR)
+                    log.msg('Cannot find spider for %s' % a, log.ERROR)
-                log.msg('Couldnt create any requests from the provided arguments', log.ERROR)
+                log.msg('Cannot create any requests with the provided arguments', log.ERROR)
-            log.msg('Couldnt find spider for url: %s' % response.url, level=log.ERROR)
+            log.msg('Cannot find spider for url: %s' % response.url, level=log.ERROR)
-                log.msg('Couldnt find method %s in %s spider' % (method, spider.domain_name))
+                log.msg('Cannot find method %s in %s spider' % (method, spider.domain_name))
-                log.msg('Couldnt find spider for "%s"' % response.url)
+                log.msg('Cannot find spider for "%s"' % response.url)
-                log.msg('Could not found spider for %s' % domain, log.ERROR)
+                log.msg('Could not find spider for %s' % domain, log.ERROR)
-                log.msg('Could not found spider for <%s>' % url, log.ERROR)
+                log.msg('Could not find spider for <%s>' % url, log.ERROR)
-                log.msg('Could not found spider for %s' % request, log.ERROR)
+                log.msg('Could not find spider for %s' % request, log.ERROR)
-            return len(site.active) + len(site.queue)
+            return site.outstanding()
-        if outstanding > self.DOWNLOADER_BACKLOG:
+        # backout enqueing downloads if domain needs it
-from scrapy.contrib.aws import canonical_string, sign_request
+from scrapy.contrib.aws import sign_request
-        sign_request(req, self.access_key, self.secret_key)
+        if self.sign_requests:
-        return "[options] <url> <method>"
+        return "[options] <url>"
-        return "Parse the URL with the given spider method and print the results"
+        return "Parse the given URL and print the results"
-        parser.add_option("--matches", dest="matches", action="store_true", help="try to match and parse the url with the defined rules (if any)")
+        parser.add_option("-r", "--rules", dest="rules", action="store_true", help="try to match and parse the url with the defined rules (if any)")
-                       item.__dict__.pop(key, None)
+                    if key.startswith('_'):
-                url, method = args[:2]
+        if not args:
-        for response in fetch([url]):
+        ret_items, ret_links = [], []
-                            links.extend(ret_links)
+            if opts.callback:
-        self.print_results(items, links, opts)
+        self.print_results(ret_items, ret_links, opts)
-        return self._response_downloaded(response, None, cb_kwargs={}, follow=True)
+        return self._response_downloaded(response, self.parse_start_url, cb_kwargs={}, follow=True)
-            method = None            
+            method = None
-        links = set()
+        items = []
-                links = links.union(ret_links)
+                items.extend(ret_items)
-                           links.add(Request(url=link.url, link_text=link.text))
+                    already_parsed = False
-                        if rule.link_extractor.matches(response.url):
+                    for rule in spider.rules:
-                            links = links.union(ret_links)
+                            items.extend(ret_items)
-
+                            extracted_links = rule.link_extractor.extract_urls(response)
-        parser.add_option("--matches", dest="matches", action="store_true", help="avoid using pygments to colorize the output")
+        parser.add_option("--matches", dest="matches", action="store_true", help="try to match and parse the url with the defined rules (if any)")
-        return []
+        for rule in self._rules:
-            if rule.process_links:
+            if links and rule.process_links:
-        if follow:
+
-            link.url = urljoin(base_url, link.url)
+            link.url = urljoin(base_url, link.url).strip()
-        links = [link.strip() for link in links if _is_valid_url(link.url)]
+        links = [link for link in links if _is_valid_url(link.url)]
-        links = [link for link in links if _is_valid_url(link.url)]
+        links = [link.strip() for link in links if _is_valid_url(link.url)]
-        return True if any(allowed) and not any(denied) else False
+        return any(allowed) and not any(denied)
-        return any(allowed) and not any(denied)
+        allowed = [regex.search(url) for regex in self.allow_res] if self.allow_res else [True]
-from scrapy.command.commands.parse_method import Command as ScrapyCommand
+from scrapy.command import ScrapyCommand
-        return "[options] <url>"
+        return "[options] <url> <method>"
-        return "Parse the URL and print its results"
+        return "Parse the URL with the given spider method and print the results"
-        parser.add_option("--identify", dest="identify", action="store_true", help="try to use identify instead of parse")
+        parser.add_option("--nolinks", dest="nolinks", action="store_true", help="don't show extracted links")
-            return
+        if opts.matches:
-        for response in responses:
+        for response in fetch([url]):
-            else:
+            if not spider:
-            self.print_results(items, links, opts)
+            if method:
-
+    def matches(self, url):
-        return any(regex.search(url) for regex in self.allow_res) and not any(regex.search(url) for regex in self.deny_res)
+    def matches(self, url):
-from twisted.web.client import HTTPClientFactory, HTTPPageGetter
+from twisted.web.client import HTTPClientFactory
-from twisted.web.client import HTTPClientFactory
+from twisted.web.client import HTTPClientFactory, HTTPPageGetter
-                log.msg('Couldnt find method %s in spider %s' % (method, spider.__name__))
+                log.msg('Couldnt find spider for "%s"' % response.url)
-        self.print_results(items, links, opts)
+        if items or links:
-from scrapy.command import ScrapyCommand
+from scrapy.command.commands.parse_method import Command as ScrapyCommand
-        return "Parse the URL and print their results"
+        return "Parse the URL and print its results"
-        return item
+        items = set()
-                    result = spider.identify_products(response)
+                    ret_items, ret_links = ScrapyCommand.run_method(self, response, 'identify_products', args, opts)
-                    item.__dict__.pop('_adaptors_dict', None)
+                    if hasattr(spider, 'rules'):
-                    display.pprint(items)
+        self.print_results(items, links, opts)
-                log.msg('cannot find spider for url: %s' % response.url, level=log.ERROR)
+from scrapy.command import ScrapyCommand
-            raise AttributeError('You must specify either a response or a base_url to the ExtractImages adaptor.')
+            log.msg('No base URL was found for ExtractImages adaptor, will only extract absolute URLs', log.WARNING)
-              if len(children) > 1:
+            if selector.xmlNode.name == 'a':
-        elif selector.xmlNode.type == 'attribute' and selector.xmlNode.name in ['href', 'src']:
+                ret.extend(selector.x('.//text()'))
-        rel_links = []
+        rel_urls = []
-                rel_links.extend(self.extract_from_xpath(location))
+                rel_urls.extend(self.extract_from_xpath(location))
-        return [urlparse.urljoin(self.base_url, link) for link in rel_links]
+                rel_urls.append(location)
-
+from scrapy.contrib.spiders.crawl import CrawlSpider, Rule
-        url = 'http://%s.s3.amazonaws.com/%s' % (self.bucket_name, key)
+        url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.prefix, key)
-        return '%s/full/%s.jpg' % (self.prefix, image_guid)
+        return 'full/%s.jpg' % (image_guid)
-        return '%s/thumbs/%s/%s.jpg' % (self.prefix, thumb_id, image_guid)
+        return 'thumbs/%s/%s.jpg' % (thumb_id, image_guid)
-    link_filter (optional)
+    process_links (optional)
-    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, link_filter=None):
+    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None):
-        self.link_filter = link_filter
+        self.process_links = process_links
-
+        """Constructor takes care of compiling rules"""
-            rule.link_filter = _get_method(rule.link_filter)
+        self._compile_rules()
-            return self.parse_url(response)
+        """This function is called by the framework core for all the
-        of ScrapedItems and/or Requests."""
+        """Overrideable callback function for processing start_urls. It must
-        return item
+    def process_results(self, results, response):
-                links = rule.link_filter(links)
+        for rule in self._rules:
-                r.append_callback(self._parse_wrapper, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)
+                r.append_callback(self._response_downloaded, rule.callback, cb_kwargs=rule.cb_kwargs, follow=rule.follow)
-    def _parse_wrapper(self, response, callback, cb_kwargs, follow):
+    def _response_downloaded(self, response, callback, cb_kwargs, follow):
-                    entry = self.scraped_item(response, entry)
+            cb_res = self.process_results(cb_res, response)
-        return list(ret)
+    def _compile_rules(self):
-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url, check_valid_urlencode
+from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url
-from scrapyorg.docs.views import index, document
+from scrapyorg.docs.views import document
-    (r'^(?P<url>[\w./-]*)/$', document),
+    url(r'^$', document, {'url': ''}),
-
+    url = url.encode('utf-8')
-        # no query arguments
+        # simplest case
-                         "http://www.example.com")
+                                          "http://www.example.com")
-                         "http://www.example.com/do?a=50&b=2&b=5&c=3")
+                                          "http://www.example.com/do?a=50&b=2&b=5&c=3")
-    """Canonicalize url by applying the following procedures:
+    """Canonicalize the given url by applying the following procedures:
-        self.assertTrue(check_valid_urlencode('http://www.example.com/product%2Clittle%3Alondon%24set%25'))
+        self.assertFalse(check_valid_urlencode('http://www.example.com/.,:;!@$%^*()_-[]{}|'))
-        return all((s_plus == urllib.quote(urllib.unquote(s_plus)) for s_plus in s.split('+')))
+        for ignore_char in ',:;!@*()':
-        return all((check_str(s) for s in p.split('=', 1)))
+        return all(check_str(s) for s in p.split('=', 1))
-    return check_str(split_result[2]) and all((check_param(p) for p in split_result[3].split('&')))
+    return check_str(split_result.path) and all(check_param(p) for p in split_result.query.split('&'))
-            if age_days < self.image_refresh_days:
+            if age_days > self.image_refresh_days:
-    url = str(url) # URLs *must* be a string; otherwise urllib may fail.
+    url = str(url) # URLs *must* be a string; otherwise urllib may fail.
-                    newval = val
+                    setattr(self, attrname, val)
-                        newval = val
+                        setattr(self, attrname, val)
-                            newval = '%s %s' % (curr_val, val)
+                            setattr(self, attrname, '%s\t%s' % (curr_val, val))
-                setattr(self, attrname, newval)
+                            setattr(self, attrname, curr_val + val)
-                    setattr(self, attrname, val)
+                    newval = val
-                        setattr(self, attrname, newval)
+                        newval = val
-        be parsed or not.
+        This is were any response arrives, and were it's decided whether
-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline, SetGUIDPipeline
+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline
-            self.follow = follow 
+from scrapy.contrib.spiders2.crawl import CrawlSpider, Rule
-from scrapy.item.adaptors import AdaptorPipe
+from scrapy.item.adaptors import AdaptorPipe
-
+        self.assertFalse(check_valid_urlencode('http://www.example.com/pictures detail CAN43664.jpg'))
-    TODO should also check the parameters """
+    """ check that the url-path and arguments are properly quoted """
-    return split_result[2] == urllib.quote(urllib.unquote(split_result[2]))
+    return check_str(split_result[2]) and all((check_param(p) for p in split_result[3].split('&')))
-            if age_days > self.image_refresh_days:
+            if age_days < self.image_refresh_days:
-        buf.seek(0)
+        width, height = image.size
-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url
+from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url, check_valid_urlencode
-    m = md5.new()
+    m = hashlib.md5()
-            self.s3_store_image(response, request.url, info)
+            etag = self.s3_store_image(response, request.url, info)
-        return key # success value sent as input result for item_media_downloaded
+        return '%s#%s' % (key, etag) # success value sent as input result for item_media_downloaded
-        def _on200(response):
+        def _onsuccess(response):
-                return True
+                return # returning None force download
-            return True
+            if age_days > self.image_refresh_days:
-                return key
+            self.inc_stats(info.domain, 'uptodate')
-        dfd.addCallback(_evaluate)
+        dfd.addCallbacks(_onsuccess, lambda _:None)
-        self._s3_put_image(image, key, info)
+        _, jpegbuf = self._s3_put_image(image, key, info)
-        return self.download(req, info)
+        return self.download(req, info), buf
-        super(AdaptorPipe, self).__init__([adaptor for adaptor in adaptors if callable(adaptor)])
+        for adaptor in adaptors:
-                if debug:
+                if self.debug:
-                if debug:
+                if self.debug:
-from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, Delist, Regex
+from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, canonicalize_urls, Delist, Regex
-def strip_list(value):
+def strip(value):
-    of each string in the provided list.
+    of the provided string or list.
-      >> strip_list([' hi   ', 'buddies  '])
+      >> strip([' hi   ', 'buddies  '])
-    Output: list of unicodes
+    Input: unicode or iterable of unicodes
-    return [ unicode(v.strip()) for v in value ]
+    if isinstance(value, basestring):
-        self.assertEqual(adaptors.strip_list([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),
+    def test_strip(self):
-        ("270", (270, 270))
+#             ('50', (50, 50)),
-        if lk in set('content-md5', 'content-type', 'date') or lk.startswith(AMAZON_HEADER_PREFIX):
+        if lk in set(['content-md5', 'content-type', 'date']) or lk.startswith(AMAZON_HEADER_PREFIX):
-        sign_request(req, self.access_key, self.secret_keself.secret_keyy)
+        sign_request(req, self.access_key, self.secret_key)
-        if lk in ['content-md5', 'content-type', 'date'] or lk.startswith(AMAZON_HEADER_PREFIX):
+        if lk in set('content-md5', 'content-type', 'date') or lk.startswith(AMAZON_HEADER_PREFIX):
-        interesting_headers['content-md5'] = ''
+    interesting_headers.setdefault('content-type', '')
-    if interesting_headers.has_key('x-amz-date'):
+    if 'x-amz-date' in interesting_headers:
-    return metadata
+def sign_request(req, accesskey, secretkey):
-    _hmac = hmac.new(settings.AWS_SECRET_ACCESS_KEY, digestmod=hashlib.sha1)
+    c_string = canonical_string(req.method, fqkey, req.headers)
-    headers['Authorization'] = "AWS %s:%s" % (settings.AWS_ACCESS_KEY_ID, b64_hmac)
+    req.headers['Authorization'] = "AWS %s:%s" % (accesskey, b64_hmac)
-from scrapy.contrib.aws import canonical_string
+from scrapy.contrib.aws import canonical_string, sign_request
-        req.headers['Authorization'] = "AWS %s:%s" % (self.access_key, b64_hmac)
+        sign_request(req, self.access_key, self.secret_keself.secret_keyy)
-        self.base_url = response.url if response else base_url
+        BASETAG_RE = re.compile(r'<base\s+href\s*=\s*[\"\']\s*([^\"\'\s]+)\s*[\"\']', re.I)
-        
+
-                        del item._adaptors_dict
+                    item.__dict__.pop('_adaptors_dict', None)
-    def _adaptor(value, **adaptor_args):
+    def _adaptor(value, adaptor_args):
-                    del item._adaptors_dict
+                    if hasattr(item, '_adaptors_dict'):
-                items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]
+                items = [self.pipeline_process(i, opts) for i in result if isinstance(i, ScrapedItem)]
-        Set the adaptors (from a list or tuple) to be used for a specific attribute.
+        Add an adaptor for the specified attribute at the given position.
-        self._adaptors_dict[attrib] = AdaptorPipe(pipe) if hasattr(pipe, '__iter__') else None
+        if callable(adaptor):
-        self.keep = keep
+    def __init__(self, keep=None):
-    def __call__(self, value):
+    def __call__(self, value, keep=None):
-
+from scrapy.item.adaptors import adaptize
-        return [canonicalize_url(url) for url in value]
+        return [canonicalize_url(str(url)) for url in value]
-    return u''
+        return canonicalize_url(str(value))
-    def __call__(self, value):
+    def __call__(self, value, delimiter=None):
-class AdaptorDict(dict):
+class AdaptorPipe(list):
-    of adaptors to be run for filtering the input before storing.
+    This class is itself a list, filled by adaptors to be run
-    def execute(self, attrname, value, debug=False):
+    def __call__(self, value, **kwargs):
-        Execute pipeline for attribute name "attrname" and value "value".
+        Execute the adaptor pipeline for this attribute.
-        debug = debug or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])
+        debug = kwargs.pop('debug', all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE']))
-                value = adaptor(value)
+
-        
+
-        return "<AdaptorDict [ %d attributes ] >" % len(self.keys())
+    def _adaptor(value, **adaptor_args):
-from scrapy.item.adaptors import AdaptorDict
+from scrapy.item.adaptors import AdaptorPipe
-    def set_adaptors(self, adaptors_dict, **kwargs):
+    def set_adaptors(self, adaptors_dict):
-        setattr(self, '_adaptors_dict', AdaptorDict(adaptors_dict))
+        _adaptors_dict = dict(item for item in adaptors_dict.items() if isinstance(item[1], AdaptorPipe))
-    def set_attrib_adaptors(self, attrib, adaptors, **kwargs):
+    def set_attrib_adaptors(self, attrib, pipe):
-        self._adaptors_dict[attrib] = adaptors
+        self._adaptors_dict[attrib] = AdaptorPipe(pipe) if hasattr(pipe, '__iter__') else None
-                if override:
+    def attribute(self, attrname, value, **kwargs):
-                    setattr(self, attrname, newval)
+                else:
-import boto
+from scrapy.http import Request
-from scrapy.core.exceptions import HttpException
+from scrapy.core.exceptions import DropItem, NotConfigured, HttpException
-    """Product with no images exception"""
+from .images import BaseImagesPipeline, NoimagesDrop, ImageException
-class S3ImagesPipeline(MediaPipeline):
+class S3ImagesPipeline(BaseImagesPipeline):
-
+        self.access_key = settings['AWS_ACCESS_KEY_ID']
-    def save_image(self, response, request, info):
+    def s3request(self, key, method, body=None, headers=None):
-            self.s3_store_image(response, request.url)
+            self.s3_store_image(response, request.url, info)
-    def s3_should_download(self, url):
+    def media_to_download(self, request, info):
-        if k is None:
+
-    def s3_store_image(self, response, url):
+        def _evaluate(should):
-        self.s3_store_thumbnails(image, url)
+        self._s3_put_image(image, key, info)
-    def s3_store_thumbnails(self, image, url):
+    def s3_store_thumbnails(self, image, url, info):
-            self._s3_put_image(thumb, key)
+            self._s3_put_image(thumb, key, info)
-    def _s3_put_image(self, image, key):
+    def _s3_put_image(self, image, key, info):
-        log.msg("Uploaded to S3: %s" % self.s3_public_url(key), level=log.DEBUG)
+
-    pass
+    """Product with no images exception"""
-class ImagesPipeline(MediaPipeline):
+class BaseImagesPipeline(MediaPipeline):
-#     )
+
-        MediaPipeline.__init__(self)
+        super(ImagesPipeline, self).__init__()
-    def save_image(self, response, request, info):
+    def image_downloaded(self, response, request, info):
-            msg = 'Image (processing-error): Error thumbnailing %s from %s referred in <%s>: %s' % (mtype, request, referer, ex)
+            msg = 'Image (processing-error): Error thumbnailing %s from %s referred in <%s>: %s' \
-        already_created = info.extra.setdefault('created_directories', set()) if info else set()
+        already_created = info.created_directories if info else set()
-        self.extra = {}
+class MediaPipeline(object):
-        self.domaininfo[domain] = DomainInfo(domain)
+        self.domaininfo[domain] = self.DomainInfo(domain)
-from scrapy.utils.defer import mustbe_deferred
+from scrapy.utils.defer import mustbe_deferred, defer_result
-    def __init__(self, callback=None, errback=None):
+    def __init__(self):
-        request.deferred.callback(response)
+        defer_result(response).chainDeferred(request.deferred)
-            return wad # break
+        # if request is not downloading, download it.
-        return dfd
+        return wad
-        dlst.addBoth(lambda _: self.item_completed(item, info))
+        dlst.addBoth(self.item_completed, item, info)
-    def item_completed(self, item, info):
+    def item_completed(self, results, item, info):
-    # This function may return either a response or a string.
+        """You can override this function in order to make any changes you want
-            log.msg('Unhandled ERROR in MediaPipeline.item_media_{downloaded,failed} for %s: %s' % (request, _failure), log.ERROR, domain=domain)
+            log.msg('Unhandled ERROR in MediaPipeline.item_media_{downloaded,failed} for %s: %s' \
-
+
-                self._download(request, info, fp)
+            return wad # break
-                    )
+        # add to pending list for this request, and wait for result like the others.
-        info.downloading[fp] = (request, dwld)
+        # if request is already downloading, just wait.
-            defer_result(result).chainDeferred(wad)
+        # if not, this is the first time for request, try to download it.
-        name = args[0]
+        name = self.normalize_name(args[0])
-            print "Spider '%s' already exists" % name
+        if site in spiders_dict.keys():
-            response = responses[0]
+        responses = fetch(args)
-                result = spider.parse_url(response)
+            if spider:
-
+                log.msg('cannot find spider for url: %s' % response.url, level=log.ERROR)
-from scrapy.http import Request, Response, ResponseBody
+from scrapy.http import Request
-from scrapy.core.exceptions import UsageError
+from scrapy.core.exceptions import NotConfigured
-from scrapy.utils.misc import hash_values
+
-        super(BaseSpider, self).__init__()
+        super(CrawlSpider, self).__init__()
-               self.set_guid(entry)
+                self.set_guid(entry)
-
+    log = _log
-            raise NotConfigured('You must define parse_item method in order to scrape this feed')
+            raise NotConfigured('You must define parse_item method in order to scrape this XML feed')
-            settings.overrides['FOLLOW_LINKS'] = False
+            settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False
-        if settings.getbool('FOLLOW_LINKS', True):
+        if settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):
-            if settings.getbool('FOLLOW_LINKS', True):
+            if settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True):
-        return self._parse_wrapper(response, self.parse_start_url)
+        if response.url in self.start_urls:
-        res += callback(response) if callback else ()
+        res = []
-            ret.extend(self._links_to_follow(response))
+            if settings.getbool('FOLLOW_LINKS', True):
-        parser.add_option("--nolinks", dest="nolinks", action="store_true", help="don't show extracted links")
+        parser.add_option("--links", dest="links", action="store_true", help="show extracted links")
-            if not opts.nolinks:
+            if opts.links:
-IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 15)
+IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)
-            request.headers['If-Modified-Since'] = mod_date.strftime('%a, %d %b %Y %H:%M:%S GMT')
+        if not should_download(absolute):
-    Is the image older than the expiration time?
+def should_download(path):
-        return (age_days > IMAGE_EXPIRES, mtime)
+        return age_days > IMAGE_EXPIRES
-        return (True, 0)
+        return True
-IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)
+IMAGE_EXPIRES = settings.getint('IMAGES_EXPIRES', 15)
-            return relative
+        expired, mtime = image_expired(absolute)
-    """Should the image downloader download the image to the location specified
+def image_expired(path):
-        return age_days > IMAGE_EXPIRES
+        return (age_days > IMAGE_EXPIRES, mtime)
-        return True
+        return (True, 0)
-        return "<spider_name> <spider_domain_name>"
+        return "<spider_name> <spider_domain_name> [--template=template_name]"
-        return "Generate new spider based on predefined template"
+        return "Generate new spider based on a predefined template"
-        if len(args) != 2:
+        if len(args) < 2:
-            self._genspider(name, site)
+        if not site in spiders_dict.keys():
-            print "Spider '%s' exist" % name
+            print "Spider '%s' already exists" % name
-    def _genspider(self, name, site):
+    def normalize_name(self, name):
-            'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('-')])
+            'classname': '%sSpider' % ''.join([s.capitalize() for s in name.split('_')])
-        self._genfiles(self.template_name, '%s/%s.py' % (spidersdir, name), tvars)
+        spiders_dir = os.path.abspath(os.path.dirname(spiders_module.__file__))
-import string
+from scrapy.utils.misc import render_templatefile
-
+from __future__ import with_statement
-    Output: list of strings
+    Output: list of unicodes
-        return value
+        return unicode(value)
-    Input: iterable with strings
+    Input: iterable of strings
-    E.g. "Hello   sir" would turn into "Hello sir".
+    Converts multispaces into single spaces for each string
-    Input: list of unicodes
+    Input: iterable of unicodes
-    return [ _clean_spaces_re.sub(' ', v) for v in value ]
+    return [ _clean_spaces_re.sub(' ', v.decode('utf-8')) for v in value ]
-    Input: list of unicodes
+    Input: iterable of unicodes
-    return [ v.strip() for v in value ]
+    return [ unicode(v.strip()) for v in value ]
-    Tries to canonicalize each url in the list you provide.
+    Canonicalizes each url in the list you provide.
-    Input: list of unicodes(urls)
+    Input: iterable of unicodes(urls)
-    return ''
+    return u''
-    Input: iterable with strings
+    Joins a list with the specified delimiter
-        self.assertEqual(adaptors.drop_empty([1, 2, None, 5, None, 6, None, 'hi']),
+        self.assertEqual(adaptors.drop_empty([1, 2, None, 5, 0, 6, False, 'hi']),
-    links to be resolved.
+    This adaptor may receive either an XPathSelector containing
-class CrawlSpider(BasicSpider):
+class CrawlSpider(BaseSpider):
-class XMLFeedSpider(BasicSpider):
+    def set_guid(self, item):
-import sha
+import hashlib
-    hash = sha.new()
+    hash = hashlib.sha1()
-def delist(value, join_delimiter=' '):
+class Delist(object):
-    return join_delimiter.join(value)
+    def __init__(self, delimiter=' '):
-from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, delist, Regex
+from scrapy.contrib.adaptors.markup import remove_tags, remove_root, Unquote
-def unquote(value, keep_entities=None):
+class Unquote(object):
-    return [ remove_entities(v, keep=keep_entities) for v in value ]
+    def __init__(self, keep=['lt', 'amp']):
-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline
+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline, SetGUIDPipeline
-from scrapy.item import ScrapedItem, ItemDelta, ItemAttribute
+from scrapy.item import ScrapedItem, ItemDelta
-        'url': ItemAttribute(attrib_name='url', attrib_type=basestring),  # the main URL where this item was scraped from
+        'guid': basestring, # a global unique identifier
-from scrapy.item.models import ScrapedItem, ItemDelta, ItemAttribute
+from scrapy.item.models import ScrapedItem, ItemDelta
-from scrapy import log
+from scrapy.item.adaptors import AdaptorDict
-        if not val is None:
+    def set_adaptors(self, adaptors_dict, **kwargs):
-        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;'], keep_entities=[]), [u'hello\xa9&welcome', u'<br />&'])
+        self.assertEqual(adaptors.Unquote(keep=[])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&welcome', u'<br />&'])
-        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
+        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
-        self.assertEqual(adaptors.delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),
+        self.assertEqual(adaptors.Delist()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),
-            return () if hasattr(self.ATTRIBUTES[attr].attrib_type, '__iter__') else None
+            return None
-        self.assertEqual(adaptors.Unquote(keep=[])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&welcome', u'<br />&'])
+        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;'], keep_entities=[]), [u'hello\xa9&welcome', u'<br />&'])
-        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
+        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
-        self.assertEqual(adaptors.Delist()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),
+        self.assertEqual(adaptors.delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),
-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline, SetGUIDPipeline
+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline
-from scrapy.item import ScrapedItem, ItemDelta
+from scrapy.item import ScrapedItem, ItemDelta, ItemAttribute
-        'url': basestring,  # the main URL where this item was scraped from
+        'guid': ItemAttribute(attrib_name='guid', attrib_type=basestring), # a global unique identifier
-            return None
+            return () if hasattr(self.ATTRIBUTES[attr].attrib_type, '__iter__') else None
-        self.__dict__[attr] = value
+        super(RobustScrapedItem, self).__setattr__(attr, value)
-from scrapy.item.models import ScrapedItem, ItemDelta
+from scrapy.item.models import ScrapedItem, ItemDelta, ItemAttribute
-from scrapy.item.adaptors import AdaptorDict
+import types
-        if val or val is False:
+    ATTRIBUTES = { 'guid': ItemAttribute(attrib_name='guid', attrib_type=basestring) }
-    pass
+import inspect
-from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, Delist, Regex
+from scrapy.contrib.adaptors.markup import remove_tags, remove_root, unquote
-class Unquote(object):
+def unquote(value, keep_entities=None):
-        return [ remove_entities(v, keep=self.keep) for v in value ]
+    if keep_entities is None:
-class Delist(object):
+def delist(value, join_delimiter=' '):
-        return self.delimiter.join(value)
+    return join_delimiter.join(value)
-                self._links_callback.append((suffix, extractor))
+                suffix = attr.split('_', 1)[1]
-        return self._parse_wrapper(response, callback=self.parse_start_url)
+        return self._parse_wrapper(response, self.parse_start_url)
-        for suffix, lx in self._links_callback:
+        for lx, callback in self._links_callback:
-                res.append(request)
+                links_to_follow[link.url] = (callback, link.text)
-        callback = kwargs.get('callback')
+    def _parse_wrapper(self, response, callback):
-
+        res += callback(response) if callback else ()
-                ret.extend(getattr(self, parse_name)(response))
+            callback_name = 'parse_%s' % name[6:]
-                self._links_callback.append((value, callback))
+                suffix = attr[6:]
-        return self._parse_wrapper(response, self.parse_start_url)
+        return self._parse_wrapper(response, callback=self.parse_start_url)
-        for lx, callback in self._links_callback:
+        for suffix, lx in self._links_callback:
-            res.append(request)
+                request = Request(url=link.url, link_text=link.text)
-    def _parse_wrapper(self, response, callback):
+    def _parse_wrapper(self, response, **kwargs):
-        res += callback(response) if callback else ()
+
-                    ret.extend(getattr(self, callback_name)(response))
+            check_name = 'check_%s' % name[6:]
-
+from unittest import TestCase, main
-    return response.replace(body=new_body)
+    new_body_content = ''.join(parts)
-        if not val is None:
+        if val or val is False:
-                result = spider.identify(response)
+                result = spider.identify_products(response)
-    itertag = 'product'
+    itertag = 'item'
-        if val:
+        if not val is None:
-        item.guid = hash_values(*[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])
+        item.guid = hash_values(self.domain_name, *[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])
-        return (self.parse_item(response, xSel) for xSel in nodes)
+        return (self.parse_item_wrapper(response, xSel) for xSel in nodes)
-            if override:
+        if val:
-                setattr(self, attrname, newval)
+            else:
-            nodes = xpathselector_iternodes(response, self.itertag)
+            nodes = xmliter(response, self.itertag)
-import csv
+import re, csv
-def csv_iter(response, delimiter=None, headers=None):
+def _normalize_input(obj):
-        csv_r = csv.reader(response.body.to_unicode().split('\n'), delimiter=delimiter)
+        csv_r = csv.reader(lines, delimiter=delimiter)
-        csv_r = csv.reader(response.body.to_unicode().split('\n'))
+        csv_r = csv.reader(lines)
-        headers = csv_r.next()
+        headers = _getrow(csv_r)
-            log.msg("ignoring node %d (length: %d, should be: %d)" % (csv_r.line_num, len(node), len(headers)), log.WARNING)
+        row = _getrow(csv_r)
-        yield dict(zip(headers, node))
+        else:
-from scrapy.xml.selector import XmlXPathSelector
+from scrapy.xpath.selector import XmlXPathSelector
-            nodes = HtmlXPathSelector(response).x('//%s' % self.itertag)
+            nodes = XmlXPathSelector(response).x('//%s' % self.itertag)
-from scrapy.contrib.codecs import x_mac_roman
+from scrapy.contrib.codecs.x_mac_roman import XMacRomanCodec
-codecs.register(x_mac_roman_search_function)
+class XMacRomanCodec(object):
-            result = spider.parse(response) if not opts.identify else spider.identify(response)
+            if opts.identify:
-basic crawling.
+This module contains some basic spiders for scraping websites (CrawlSpider)
-from scrapy.http import Request
+from scrapy.http import Request, Response, ResponseBody
-    more information refer to the Scrapy tutorial
+    This class is basically a BaseSpider with support for GUID generating
-
+    def set_guid(self, item):
-                links_to_follow[url] = (callback, link_text)
+            links = lx.extract_urls(response)
-        item.guid = hash_values(*[str(getattr(item, aname) or '') for aname in self.gen_guid_attribs])
+    def parse_url(self, response):
-from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, Delist, Regex
+from scrapy.contrib.adaptors.misc import to_unicode, clean_spaces, strip_list, drop_empty, canonicalize_urls, Delist, Regex
-def normalize_urls(value):
+def canonicalize_urls(value):
-        self.spider.set_guid(item)
+        spiders.fromdomain(domain).set_guid(item)
-    def attribute(self, attrname, value, debug=False, override=False):
+    def attribute(self, attrname, value, override=False, add=False, debug=False):
-        if not getattr(self, attrname, None) or override:
+        curr_val = getattr(self, attrname, None)
-                yield Request(url=url, callback=self.parse_item)
+        for link in xlink.extract_urls(response):
-        self.inside_link = False
+        self.current_link = None
-    def extract_urls(self, response):
+    def extract_urls(self, response, unique=False):
-        return urls
+        ret = []
-        self.links = {}
+        self.links = []
-                    self.inside_link = value
+                    if not self.unique or not value in [link.url for link in self.links]:
-        self.inside_link = False
+        self.current_link = None
-            self.links[self.inside_link] = data
+        if self.current_link and not self.current_link.text:
-    def extract_urls(self, response):
+    def extract_urls(self, response, unique=True):
-        urls = [u for u in url_text.iterkeys() if _is_valid_url(u)]
+        links = LinkExtractor.extract_urls(self, response, unique)
-            urls = [u for u in urls if _matches(u, self.allow_res)]
+            links = [link for link in links if _matches(link.url, self.allow_res)]
-            urls = [u for u in urls if not _matches(u, self.deny_res)]
+            links = [link for link in links if not _matches(link.url, self.deny_res)]
-            urls = [u for u in urls if url_is_from_any_domain(u, self.allow_domains)]
+            links = [link for link in links if url_is_from_any_domain(link.url, self.allow_domains)]
-        res = {}
+            links = [link for link in links if not url_is_from_any_domain(link.url, self.deny_domains)]
-        return res
+            for link in links:
-from scrapy.link import LinkExtractor
+from scrapy.link import LinkExtractor, Link
-                          'http://example.org/': ''})
+                         [Link(url='http://example.org/somepage/item/12.html', text='Item 12'), 
-                         {'http://otherdomain.com/base/item/12.html': 'Item 12'})
+                         [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])
-from scrapy.core import signals
+from scrapy.conf import settings
-from scrapy.http import Response, Request
+from scrapy.http import Response, Request
-
+from scrapy.contrib import codecs
-        self.base_url = base_url.url if isinstance(base_url, Response) else base_url
+    def __call__(self, locations):
-from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline
+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline, SetGUIDPipeline
-
+from scrapy.utils.url import canonicalize_url
-        self.base_url = base_url
+        self.base_url = response.url if response else base_url
-        if not self.response and not self.base_url:
+    def __call__(self, (locations, base_url)):
-        return [urlparse.urljoin(base_url, link) for link in rel_links]
+        return [urlparse.urljoin(self.base_url, link) for link in rel_links]
-    """BasicSpider extends BaseSpider by providing support for simple crawling
+    """
-    more information refer to the Scrapy tutorial"""
+    more information refer to the Scrapy tutorial
-                                     (type(self).__name__, suffix, suffix))
+                callback = getattr(self, 'parse_%s' % suffix, None)
-        for url, cb_link in links_to_follow.iteritems():
+        for url, (callback, link_text) in links_to_follow.iteritems():
-        res += callback(response) or ()
+        res += callback(response) if callback else ()
-    def __init__(self, data=None, adaptors_pipe={}):
+    def __init__(self, data=None):
-            return object.__setattr__(self, '_adaptors_pipe', value)
+        if attr == '_adaptors_dict':
-class AdaptorPipe(object):
+class AdaptorDict(dict):
-    def execute(self, attrname, value, kwargs):
+    def execute(self, attrname, value, debug=False):
-        debug = kwargs.get('debug') or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])
+        debug = debug or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])
-        for adaptor in self.pipes.get(attrname, []):
+        for adaptor in self.get(attrname, []):
-                value = adaptor(kwargs)(value)
+                    print "  %07s | input >" % name, repr(value)
-                    print "  %07s | output >" % adaptor.__name__, repr(value)
+                    print "  %07s | output >" % name, repr(value)
-                print "Error in '%s' adaptor. Traceback text:" % adaptor.__name__
+            except Exception:
-from scrapy.item.adaptors import AdaptorPipe
+from scrapy.item.adaptors import AdaptorDict
-    def set_adaptors(self, adaptors_pipe):
+    def set_adaptors(self, adaptors_dict, **kwargs):
-        setattr(self, '_adaptors_pipe', AdaptorPipe(adaptors_pipe))
+        setattr(self, '_adaptors_dict', AdaptorDict(adaptors_dict))
-    def set_attrib_adaptors(self, attrib, adaptors):
+    def set_attrib_adaptors(self, attrib, adaptors, **kwargs):
-        self._adaptors_pipe.set_adaptors(attrib, adaptors)
+        self._adaptors_dict[attrib] = adaptors
-        if not getattr(self, attrname, None) or kwargs.get('override'):
+    def attribute(self, attrname, value, debug=False, override=False):
-from scrapy.xpath.selector import XmlXPathSelector
+from scrapy.contrib import adaptors
-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('/')),
+        self.assertEqual(adaptors.extract(sample_xsel.x('/')),
-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('xml/*')),
+        self.assertEqual(adaptors.extract(sample_xsel.x('xml/*')),
-        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//tag3/@value')), ['mytag'])
+        self.assertEqual(adaptors.extract(sample_xsel.x('xml/@id')), ['2'])
-                           <a href="lala1.html">lala1</a>
+                           <a href="lala1/lala1.html">lala1</a>
-                           <a href="lala4.html"><img src="lala4.jpg" /></a>
+                           <a href="http://foobar.com/pepepe/papapa/lala3.html">lala3</a>
-                           <a onclick="javascript: opensomething('my_html2.html');">something2</a>
+                           <a onclick="javascript: opensomething('dummy/my_html2.html');">something2</a>
-        sample_adaptor = adaptors.ExtractImagesAdaptor({'response': sample_response})
+        sample_adaptor = adaptors.ExtractImages(response=sample_response)
-                          u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.html'])
+                         [u'http://foobar.com/lala1/lala1.html', u'http://foobar.com/lala2.html',
-                          u'http://foobar.com/lala3.html', u'http://foobar.com/dummy/lala4.jpg'])
+                         [u'http://foobar.com/lala1/lala1.html', u'http://foobar.com/lala2.html',
-        self.assertEqual(adaptors.ToUnicodeAdaptor()(['lala', 'lele', 'lulu', 1, '']),
+        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'lulu', 1, '']),
-        adaptor = adaptors.RegexAdaptor({'regex': r'href="(.*?)"'})
+        adaptor = adaptors.Regex(regex=r'href="(.*?)"')
-        self.assertEqual(adaptors.UnquoteAdaptor({'keep': []})([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&welcome', u'<br />&'])
+        self.assertEqual(adaptors.Unquote(keep=[])([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&welcome', u'<br />&'])
-        self.assertEqual(adaptors.UnquoteAdaptor()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
+        self.assertEqual(adaptors.Unquote()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
-        self.assertEqual(adaptors.RemoveTagsAdaptor()(test_data), ['adsaas', 'dsadasf'])
+        self.assertEqual(adaptors.remove_tags(test_data), ['adsaas', 'dsadasf'])
-        self.assertEqual(adaptors.RemoveRootAdaptor()(['<div>lallaa<a href="coso">dsfsdfds</a>pepepep<br /></div>']),
+        self.assertEqual(adaptors.remove_root(['<div>lallaa<a href="coso">dsfsdfds</a>pepepep<br /></div>']),
-        self.assertEqual(adaptors.CleanSpacesAdaptor()(['  hello,  whats     up?', 'testing testingtesting      testing']),
+        self.assertEqual(adaptors.clean_spaces(['  hello,  whats     up?', 'testing testingtesting      testing']),
-        self.assertEqual(adaptors.StripAdaptor()([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),
+    def test_strip_list(self):
-        self.assertEqual(adaptors.DropEmptyAdaptor()([1, 2, None, 5, None, 6, None, 'hi']),
+        self.assertEqual(adaptors.drop_empty([1, 2, None, 5, None, 6, None, 'hi']),
-        self.assertEqual(adaptors.DelistAdaptor()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),
+        self.assertEqual(adaptors.Delist()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),
-from scrapy.core.exceptions import NotConfigured, DropItem
+from scrapy.core.exceptions import NotConfigured
-            log.msg("Empty domains (no items scraped) found: %s" % " ".join(self.empty_domains), level=log.WARNING)
+            log.msg("No products sampled for: %s" % " ".join(self.empty_domains), level=log.WARNING)
-            return result
+            # TODO: this needs some revision, as keeping only the first item
-(generated by the spider) will be ignored.
+RequestLimitMiddleware: Limits the scheduler request queue size. When spiders
-will be applied. If given a value of 0, no limit will be applied.
+The limit can be set using the spider attribue `requests_queue_size` or the
-    #_last_queue_size = 0
+
-        [requests.append(r) if isinstance(r, Request) else other.append(r) for r in result]
+        items = []
-        return accepted + other
+        max_pending = getattr(spider, 'requests_queue_size', self.max_queue_size)
-        #self.set_adaptors(adaptors_pipe)
+        self.set_adaptors(adaptors_pipe)
-from scrapy.item import ScrapedItem
+from scrapy.item import ScrapedItem, ItemDelta
-    def __init__(self, data=None):
+    def __init__(self, data=None, adaptors_pipe={}):
-        if self._version:
+        if getattr(self, '_version', None):
-class RobustItemDelta(object):
+class RobustItemDelta(ItemDelta):
-from scrapy.item.models import ScrapedItem
+from scrapy.item.models import ScrapedItem, ItemDelta
-from scrapy.core import signals
+from scrapy.core import signals
-            self.scraped_old[str(item.guid)] = item.copy()
+            self.scraped_old[str(item.guid)] = item
-            self.scraped_new[str(item.guid)] = item.copy()
+            self.scraped_new[str(item.guid)] = item
-            self.passed_old[str(item.guid)] = item.copy()
+            self.passed_old[str(item.guid)] = item
-            self.passed_new[str(item.guid)] = item.copy()
+            self.passed_new[str(item.guid)] = item
-    def append_adaptor(self, attrname, adaptor):
+    def set_adaptors(self, attr, adaptors):
-        Add an adaptor at the end of the provided attribute's pipeline
+        Set the adaptor pipeline that will be used for the specified attribute
-    def execute(self, attrname, value, debug=False):
+        self.pipes[attr] = adaptors
-        for function in self.pipes.get(attrname, []):
+        debug = kwargs.get('debug') or all([settings.getbool('LOG_ENABLED'), settings.get('LOGLEVEL') == 'TRACE'])
-                value = function(value)
+                    print "  %07s | input >" % adaptor.__name__, repr(value)
-
+                    print "  %07s | output >" % adaptor.__name__, repr(value)
-                print "Error in '%s' adaptor. Traceback text:" % function.func_name
+                print "Error in '%s' adaptor. Traceback text:" % adaptor.__name__
-
+        
-def extract(location):
+class ExtractAdaptor(AdaptorFunc):
-    This function *always* returns a list.
+    This adaptor *always* returns a list.
-def extract_links(locations):
+
-    In any case, this adaptor returns a list of absolute urls extracted.
+    In any case, this adaptor returns a list containing the absolute urls extracted.
-                ret = locations.re(regexp)
+
-def to_unicode(value):
+                rel_links.append(location)
-    expression to each of its members.
+    def __call__(self, value):
-    This adaptor always returns a list of strings.
+class RegexAdaptor(AdaptorFunc):
-            return flatten([extract_regex(expr, string, 'utf-8') for string in value])
+    
-def unquote_all(value):
+class UnquoteAdaptor(AdaptorFunc):
-    return [ remove_entities(v, keep=['lt', 'amp']) for v in value ]
+    def __init__(self, kwargs={}):
-    return [ replace_tags(v) for v in value ]
+class RemoveTagsAdaptor(AdaptorFunc):
-    return [ _remove_root(v) for v in value ]
+class RemoveRootAdaptor(AdaptorFunc):
-    return [ _clean_spaces_re.sub(' ', v) for v in value ]
+class CleanSpacesAdaptor(AdaptorFunc):
-    return [ v.strip() for v in value ]
+class DropEmptyAdaptor(AdaptorFunc):
-    return [ v for v in value if v ]
+class DelistAdaptor(AdaptorFunc):
-    return ' '.join(value)
+    def __call__(self, value):
-             strip,
+def single_pipeline(do_remove_root=True, do_remove_tags=True, do_unquote=True):
-        pipe.insert(5, remove_tags)
+
-    return pipe + [delist]
+        pipe.append(UnquoteAdaptor)
-             drop_empty_elements,
+    return [ ExtractImagesAdaptor,
-                  ]
+list_pipeline = [ ExtractAdaptor,
-
+list_join_pipeline = list_pipeline + [DelistAdaptor]
-    def setadaptors(self, adaptors_pipe):
+    def __init__(self, adaptors_pipe={}):
-        returns the item itself.
+        Set the adaptors to use for this item. Receives a dict of the adaptors
-        object.__setattr__(self, '_adaptors_pipe', AdaptorPipe(adaptors_pipe))
+        setattr(self, '_adaptors_pipe', AdaptorPipe(adaptors_pipe))
-            setattr(self, attrname, value)
+    def set_attrib_adaptors(self, attrib, adaptors):
-        self.assertEqual(adaptors.extract(sample_xsel.x('/')),
+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('/')),
-        self.assertEqual(adaptors.extract(sample_xsel.x('xml/*')),
+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('xml/*')),
-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag1//text()')),
+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('xml/@id')), ['2'])
-        self.assertEqual(adaptors.extract(sample_xsel.x('//text()')),
+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//text()')),
-        self.assertEqual(adaptors.extract(sample_xsel.x('//tag3/@value')), ['mytag'])
+        self.assertEqual(adaptors.ExtractAdaptor()(sample_xsel.x('//tag3/@value')), ['mytag'])
-        self.assertEqual(adaptors.extract_links(sample_xsel.x('//@href')),
+        sample_adaptor = adaptors.ExtractImagesAdaptor({'response': sample_response})
-        self.assertEqual(adaptors.extract_links(sample_xsel.x('//a')),
+        self.assertEqual(sample_adaptor(sample_xsel.x('//a')),
-        self.assertEqual(adaptors.extract_links((sample_xsel.x('//a[@onclick]'), r'opensomething\(\'(.*?)\'\)')),
+        self.assertEqual(sample_adaptor(sample_xsel.x('//a[@onclick]').re(r'opensomething\(\'(.*?)\'\)')),
-        self.assertEqual(adaptors.to_unicode(['lala', 'lele', 'lulu', 1, '']),
+        self.assertEqual(adaptors.ToUnicodeAdaptor()(['lala', 'lele', 'lulu', 1, '']),
-                         ['lala.com', 'pepe.co.uk', 'das.biz', 'lelelel.net'])
+        adaptor = adaptors.RegexAdaptor({'regex': r'href="(.*?)"'})
-        self.assertEqual(adaptors.unquote_all([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&welcome', u'<br />&'])
+        self.assertEqual(adaptors.UnquoteAdaptor({'keep': []})([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&welcome', u'<br />&'])
-        self.assertEqual(adaptors.unquote([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
+        self.assertEqual(adaptors.UnquoteAdaptor()([u'hello&copy;&amp;welcome', u'&lt;br /&gt;&amp;']), [u'hello\xa9&amp;welcome', u'&lt;br />&amp;'])
-        self.assertEqual(adaptors.remove_tags(test_data), ['adsaas', 'dsadasf'])
+        self.assertEqual(adaptors.RemoveTagsAdaptor()(test_data), ['adsaas', 'dsadasf'])
-        self.assertEqual(adaptors.remove_root(['<div>lallaa<a href="coso">dsfsdfds</a>pepepep<br /></div>']),
+        self.assertEqual(adaptors.RemoveRootAdaptor()(['<div>lallaa<a href="coso">dsfsdfds</a>pepepep<br /></div>']),
-        self.assertEqual(adaptors.remove_multispaces(['  hello,  whats     up?', 'testing testingtesting      testing']),
+        self.assertEqual(adaptors.CleanSpacesAdaptor()(['  hello,  whats     up?', 'testing testingtesting      testing']),
-        self.assertEqual(adaptors.strip([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),
+        self.assertEqual(adaptors.StripAdaptor()([' hi there, sweety ;D ', ' I CAN HAZ TEST??    ']),
-        self.assertEqual(adaptors.drop_empty_elements([1, 2, None, 5, None, 6, None, 'hi']),
+        self.assertEqual(adaptors.DropEmptyAdaptor()([1, 2, None, 5, None, 6, None, 'hi']),
-        self.assertEqual(adaptors.delist(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),
+        self.assertEqual(adaptors.DelistAdaptor()(['hi', 'there', 'fellas.', 'this', 'is', 'my', 'test.']),
-        self._worker = remote
+    def __init__(self, worker, name, master):
-            self.master.schedule([domain], dsettings, newprio)
+            self.master.reschedule([domain], dsettings, newprio, reason="error while try to run it")
-                self.master.schedule([domain], dsettings, newprio)
+                self.master.reschedule([domain], dsettings, newprio, reason="no available slots at worker=%s" % self.name)
-                self.master.schedule([domain], dsettings, priority)
+                self.master.reschedule([domain], dsettings, priority, reason="domain already running at worker=%s" % self.name)
-                # if domain already running in some node, reschedule with same priority (so will be moved to run later)
+                # if domain already running in some node, reschedule with same priority (so it will be run later)
-                    self.master.schedule([pending['domain']], pending['settings'], pending['priority'])
+                    self.master.reschedule([pending['domain']], pending['settings'], pending['priority'], reason="domain already running in other worker")
-                self._logfailure("Error while loading worker node", failure)
+                log.msg("ClusterMaster: Could not connect to worker=%s (%s): %s" % (name, hostport, failure.value), log.ERROR)
-        """Schedule the given domains, with the given priority"""
+    def _schedule(self, domains, spider_settings=None, priority=20):
-                log.msg("ClusterMaster: Scheduled domain=%s priority=%s" % (domain, priority), log.DEBUG)
+
-            deferred.addCallbacks(callback=lambda x: x, errback=lambda reason: log.msg(reason, log.ERROR))
+            def _eb(failure):
-        server.Site.__init__(self, WebConsoleResource())
+        logfile = settings['WEBCONSOLE_LOGFILE']
-            log.msg("ClusterMaster: lost connection to node %s." % (self.name), log.ERROR)
+            log.msg("ClusterMaster: Lost connection to node %s." % self.name, log.ERROR)
-            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))
+            def _eb(failure):
-
+        """Update status from this worker. This is called periodically."""
-            log.msg("ClusterMaster: Lost connection to node %s." % (self.name), log.ERROR)
+            log.msg("ClusterMaster: Lost connection to worker=%s." % self.name, log.ERROR)
-            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))
+            def _eb(failure):
-            log.msg("ClusterMaster: Lost connection to node %s." % (self.name), log.ERROR)
+            log.msg("ClusterMaster: Lost connection to worker=%s." % self.name, log.ERROR)
-            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))
+            def _eb(failure):
-        domain_info keys:
+        domain_info is a dict of keys:
-            log.msg("ClusterMaster: Domain %s rescheduled: lost connection to node." % domain_info['domain'], log.WARNING)
+        domain = domain_info['domain']
-                log.msg("ClusterMaster: Domain %s rescheduled: no availble processes in worker" % domain_info['domain'], log.WARNING)
+                log.msg("ClusterMaster: No available slots at worker=%s when trying to run domain=%s" % (self.name, domain), log.WARNING)
-                log.msg("ClusterMaster: Domain %s rescheduled: already running in node." % domain_info['domain'], log.WARNING)
+                log.msg("ClusterMaster: Already running domain=%s at worker=%s" % (domain, self.name), log.WARNING)
-            deferred = self._worker.callRemote("run", domain_info["domain"], domain_info["settings"])
+            log.msg("ClusterMaster: Running domain=%s at worker=%s" % (domain, self.name), log.DEBUG)
-            log.msg("ClusterMaster: Lost connection to node %s." % (self.name), log.ERROR)
+            log.msg("ClusterMaster: Lost connection to worker=%s." % self.name, log.ERROR)
-        self._set_status(status)
+    def remote_update(self, worker_status, domain, domain_status):
-        elif domain_status == "scraped":
+        elif domain_status in ("done", "terminated"):
-        log.msg("ClusterMaster: Lost connection to %s. Node removed" % self.nodename )
+        self.master.remove_node(self.nodename)
-        if not settings['CLUSTER_MASTER_STATEFILE']:
+
-            statefile = open(settings["CLUSTER_MASTER_STATEFILE"], "r")
+            statefile = open(self.statefile, "r")
-        for name, hostport in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():
+        for name, hostport in self.nodesconf.iteritems():
-        log.msg("ClusterMaster: Connecting to worker %s (%s)..." % (name, hostport))
+        log.msg("ClusterMaster: Connecting to worker=%s (%s)..." % (name, hostport))
-            log.msg("ClusterMaster: Could not connect to worker %s (%s): %s" % (name, hostport, err), log.ERROR)
+            log.msg("ClusterMaster: Could not connect to worker=%s (%s): %s" % (name, hostport, err), log.ERROR)
-                log.msg("ClusterMaster: Could not connect to worker %s (%s): %s" % (name, hostport, _reason), log.ERROR)
+            def _eb(failure):
-            d.addCallbacks(callback=lambda obj: self.add_node(obj, name), errback=_errback)
+            d.addCallbacks(callback=lambda obj: self.add_node(obj, name), errback=_eb)
-        for name, hostport in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():
+        for name, hostport in self.nodesconf.iteritems():
-        log.msg("ClusterMaster: Added cluster worker %s" % name)
+        log.msg("ClusterMaster: Added worker=%s" % name)
-    def schedule(self, domains, spider_settings=None, priority=DEFAULT_PRIORITY):
+    def schedule(self, domains, spider_settings=None, priority=20):
-        scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))
+        scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL', 60))
-        with open(settings["CLUSTER_MASTER_STATEFILE"], "w") as f:
+        with open(self.statefile, "w") as f:
-            log.msg("ClusterMaster: state saved in %s" % settings["CLUSTER_MASTER_STATEFILE"])
+            log.msg("ClusterMaster: Saved state in %s" % self.statefile)
-from scrapy.contrib.pbcluster.master.manager import ClusterMaster, DEFAULT_PRIORITY
+from scrapy.contrib.pbcluster.master.manager import ClusterMaster
-            priority = int(args.get("priority", [DEFAULT_PRIORITY])[0])
+            priority = int(args.get("priority", [20])[0])
-        s += "<input type='text' name='priority'>%s</input>" % DEFAULT_PRIORITY
+        s += "<input type='text' name='priority'>%s</input>" % 20
-        self.procman = procman
+    def __init__(self, worker, domain, logfile=None, spider_settings=None):
-        self.procman.update_master(self.domain, "running")
+        self.worker.update_master(self.domain, "running")
-        self.procman.update_master(self.domain, "scraped")
+        del self.worker.running[self.domain]
-            deferred = self.__master.callRemote("update", self.status(), domain, domain_status)
+            deferred = self._master.callRemote("update", self.status(), domain, domain_status)
-            self.__master = None
+            self._master = None
-        self.__master = master
+        self._master = master
-                self.running[domain] = scrapy_proc
+                scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)
-        log.msg("ClusterWorker: started domain=%s, pid=%d, log=%s" % (self.domain, self.pid, self.logfile))
+        log.msg("ClusterWorker: started domain=%s pid=%d log=%s" % (self.domain, self.pid, self.logfile))
-        log.msg("ClusterWorker: finished domain=%s, status=%s, pid=%d, log=%s%s" % (self.domain, st, self.pid, self.logfile, er))
+        log.msg("ClusterWorker: finished domain=%s status=%s pid=%d log=%s%s" % (self.domain, st, self.pid, self.logfile, er))
-            log.msg("ClusterWorker: Sending shutdown signal to domain=%s, pid=%d" % (domain, proc.pid))
+            log.msg("ClusterWorker: Sending shutdown signal to domain=%s pid=%d" % (domain, proc.pid))
-class ClusterMasterBroker(pb.Referenceable):
+class ClusterNodeBroker(pb.Referenceable):
-        self.__remote = remote
+        self._worker = remote
-            deferred = self.__remote.callRemote("set_master", self)
+            deferred = self._worker.callRemote("set_master", self)
-            log.msg("Lost connection to node %s." % (self.name), log.ERROR)
+            log.msg("ClusterMaster: lost connection to node %s." % (self.name), log.ERROR)
-            #when there is no loading domain or in the next status update. This way also we load the nodes softly
+            # load domains by one, so to mix up better the domain loading between nodes. The next one in the same node will be loaded
-                #if domain already running in some node, reschedule with same priority (so will be moved to run later)
+                # if domain already running in some node, reschedule with same priority (so will be moved to run later)
-            deferred = self.__remote.callRemote("status")
+            deferred = self._worker.callRemote("status")
-            log.msg("Lost connection to node %s." % (self.name), log.ERROR)
+            log.msg("ClusterMaster: Lost connection to node %s." % (self.name), log.ERROR)
-            deferred = self.__remote.callRemote("stop", domain)
+            deferred = self._worker.callRemote("stop", domain)
-            log.msg("Lost connection to node %s." % (self.name), log.ERROR)
+            log.msg("ClusterMaster: Lost connection to node %s." % (self.name), log.ERROR)
-    def run(self, pending):
+    def run(self, domain_info):
-            log.msg("Domain %s rescheduled: lost connection to node." % pending['domain'], log.WARNING)
+            self.master.loading.remove(domain_info['domain'])
-                log.msg("Domain %s rescheduled: already running in node." % pending['domain'], log.WARNING)
+            if status['callresponse'][0] == ResponseCode.NO_FREE_SLOT:
-            deferred = self.__remote.callRemote("run", pending["domain"], pending["settings"])
+            deferred = self._worker.callRemote("run", domain_info["domain"], domain_info["settings"])
-            log.msg("Lost connection to node %s." % (self.name), log.ERROR)
+            log.msg("ClusterMaster: Lost connection to node %s." % (self.name), log.ERROR)
-        log.msg("Lost connection to %s. Node removed" % self.nodename )
+        log.msg("ClusterMaster: Lost connection to %s. Node removed" % self.nodename )
-        # for more info about statistics see self.update_nodes() and ClusterMasterBroker.remote_update()
+        # for more info about statistics see self.update_nodes() and ClusterNodeBroker.remote_update()
-        log.msg("Server: %s, Port: %s" % (server, port))
+        log.msg("ClusterMaster: Connecting to worker %s (%s)..." % (name, hostport))
-            log.msg("Could not connect to node %s in %s: %s." % (name, hostport, err), log.ERROR)
+            log.msg("ClusterMaster: Could not connect to worker %s (%s): %s" % (name, hostport, err), log.ERROR)
-                log.msg("Could not connect to remote node %s (%s): %s." % (name, hostport, _reason), log.ERROR)
+                log.msg("ClusterMaster: Could not connect to worker %s (%s): %s" % (name, hostport, _reason), log.ERROR)
-                log.msg("Updating node. name: %s, host: %s" % (name, hostport) )
+                log.msg("ClusterMaster: Updating stats from worker node %s (%s)" % (name, hostport))
-                log.msg("Reloading node. name: %s, host: %s" % (name, hostport) )
+                log.msg("ClusterMaster: Reloading worker node %s (%s)" % (name, hostport))
-        node = ClusterMasterBroker(cworker, name, self)
+        node = ClusterNodeBroker(cworker, name, self)
-        log.msg("Added cluster worker %s" % name)
+        log.msg("ClusterMaster: Added cluster worker %s" % name)
-                break
+        """Schedule the given domains, with the given priority"""
-            if pd: #domain already pending, so just change priority if new is higher
+            pd = self.get_first_pending(domain)
-                    self.pending.insert(i, pd)
+                    self.pending.insert(insert_pos, pd)
-                self.pending.insert(i, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})
+                self.pending.insert(insert_pos, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})
-        started yet). Otherwise use stop()"""
+        """Remove all scheduled instances of the given domains (if they haven't
-                self.pending.remove(p)
+        self.pending = [p for p in self.pending if ['domain'] not in domains]
-    def find_inpending(self, domain):
+    def get_first_pending(self, domain):
-    def print_pending(self, verbosity=1):
+    def get_pending(self, verbosity=1):
-            log.msg("Cluster master state saved in %s" % settings["CLUSTER_MASTER_STATEFILE"])
+            log.msg("ClusterMaster: state saved in %s" % settings["CLUSTER_MASTER_STATEFILE"])
-            status["pending"] = self.print_pending(verbosity)
+            status["pending"] = self.get_pending(verbosity)
-            return self.status(0, "Stopped process %s" % proc)
+            return self.status(ResponseCode.DOMAIN_STOPPED, "Stopped process %s" % proc)
-            return self.status(1, "%s: domain not running" % domain)
+            return self.status(ResponseCode.DOMAIN_NOT_RUNNING, "%s: domain not running" % domain)
-                return self.status(0, "Started process %s" % scrapy_proc)
+                return self.status(ResponseCode.DOMAIN_STARTED, "Started process %s" % scrapy_proc)
-                return self.status(2, "Domain %s already running" % domain )
+                return self.status(ResponseCode.DOMAIN_ALREADY_RUNNING, "Domain %s already running" % domain )
-            return self.status(1, "No free slot to run another process")
+            return self.status(ResponseCode.NO_FREE_SLOT, "No free slot to run another domain")
-        scrapyengine.listenTCP(port, pb.PBServerFactory(self))
+        factory = pb.PBServerFactory(self, unsafeTracebacks=True)
-        log.msg("Reason type: %s. value: %s" % (reason.type, reason.value) )
+    def processEnded(self, status):
-            return self.status(1, "%s: domain not running." % domain)
+            return self.status(1, "%s: domain not running" % domain)
-                    log.msg("pysvn module not available.", level=log.WARNING)
+
-                return self.status(0, "Started process %s." % scrapy_proc)
+                return self.status(0, "Started process %s" % scrapy_proc)
-                return self.status(2, "Domain %s already running." % domain )
+                return self.status(2, "Domain %s already running" % domain )
-            return self.status(1, "No free slot to run another process.")
+            return self.status(1, "No free slot to run another process")
-        d.update(settings.get("MYSQL_CONNECTION_SETTINGS"))
+        d['charset'] = 'utf8'
-if not sys.argv:
+if not sys.argv or sys.argv[0] == '--status':
-elif sys.argv[0] == "-s":
+elif sys.argv[0] == "--stop":
-elif sys.argv[0] == "-r":
+elif sys.argv[0] == "--run":
-d.addCallbacks(callback = util.println, errback = lambda reason: 'error: '+str(reason.value))
+d.addCallbacks(callback=pprint.pprint, errback=lambda reason:'error: ' + str(reason.value))
-        """Return this scrapy process status as a dict.
+    def info(self):
-        status["running"] = [self.running[k].status() for k in self.running.keys()]
+        status["running"] = [self.running[k].info() for k in self.running.keys()]
-class Broker(pb.Referenceable):
+class ClusterCrawlerBroker(pb.Referenceable):
-            deferred.addCallbacks(callback=lambda x: None, errback=lambda reason: log.msg(reason, log.ERROR))
+        deferred = self.__remote.callRemote("register_crawler", os.getpid(), self)
-class ClusterCrawler:
+class ClusterCrawler(object):
-            self.worker = Broker(self, obj)
+            self.worker = ClusterCrawlerBroker(self, obj)
-import pickle
+from __future__ import with_statement
-from pydispatch import dispatcher
+import datetime
-class Broker(pb.Referenceable):
+class ClusterMasterBroker(pb.Referenceable):
-                #dont show spider settings
+                # dont show spider settings
-        log.msg("Removed node %s." % self.nodename )
+        log.msg("Lost connection to %s. Node removed" % self.nodename )
-class ClusterMaster:
+class ClusterMaster(object):
-        if not (settings.getbool('CLUSTER_MASTER_ENABLED')):
+        if not settings.getbool('CLUSTER_MASTER_ENABLED'):
-        #import groups settings
+        # import groups settings
-        #load pending domains
+        # load pending domains
-            self.pending = pickle.load( open(settings["CLUSTER_MASTER_CACHEFILE"], "r") )
+            statefile = open(settings["CLUSTER_MASTER_STATEFILE"], "r")
-        #on how statistics works, see self.update_nodes() and Broker.remote_update()
+        # for more info about statistics see self.update_nodes() and ClusterMasterBroker.remote_update()
-        #load cluster global settings
+        # load cluster global settings
-            self.load_node(name, url)
+        for name, hostport in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():
-        server, port = url.split(":")
+    def load_node(self, name, hostport):
-            log.msg("Could not connect to node %s in %s: %s." % (name, url, reason), log.ERROR)
+            log.msg("Could not connect to node %s in %s: %s." % (name, hostport, err), log.ERROR)
-            _make_callback(factory, name, url)
+            def _errback(_reason):
-        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():
+        """Update worker nodes statistics"""
-                log.msg("Updating node. name: %s, url: %s" % (name, url) )
+                log.msg("Updating node. name: %s, host: %s" % (name, hostport) )
-                self.load_node(name, url)
+                log.msg("Reloading node. name: %s, host: %s" % (name, hostport) )
-        node = Broker(cworker, name, self)
+        node = ClusterMasterBroker(cworker, name, self)
-        log.msg("Pending saved in %s" % settings["CLUSTER_MASTER_CACHEFILE"])
+        with open(settings["CLUSTER_MASTER_STATEFILE"], "w") as f:
-            #spider settings
+            # spider settings
-        #spider settings
+        # spider settings
-        return content
+        return content
-import sys, os, time, datetime, pickle
+import sys
-        #We conserve original setting format for info purposes (avoid lots of unnecesary "SCRAPY_")
+        # We preserve the original settings format for info purposes (avoid
-        self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': 0, 'CLUSTER_CRAWLER_ENABLED': 1, 'WEBCONSOLE_ENABLED': 0})
+        self.scrapy_settings.update({'LOGFILE': self.logfile, 
-        self.env["PYTHONPATH"] = ":".join(sys.path)#this is need so this crawl process knows where to locate local_scrapy_settings.
+        # we nee to pass the worker python path to the crawling process so it
-        return {"domain": self.domain, "pid": self.pid, "status": self.status, "settings": self.scrapy_settings, "logfile": self.logfile, "starttime": self.start_time}
+    def status(self):
-        self.crawlers = {}#a dict pid->scrapy process remote pb connection
+        self.running = {} # dict of domain->ScrapyProcessControl 
-        log.msg("PYTHONPATH: %s" % repr(sys.path))
+        log.msg("Using sys.path: %s" % repr(sys.path), level=log.DEBUG)
-        status["running"] = [ self.running[k].as_dict() for k in self.running.keys() ]
+        status["running"] = [self.running[k].status() for k in self.running.keys()]
-        status["callresponse"] = (rcode, rstring) if rstring else (0, "Status Response.")
+        status["callresponse"] = (rcode, rstring) if rstring else (0, "No request")
-            log.msg("Lost connection to node %s." % (self.name), log.ERROR)
+            log.msg("Lost connection to master", log.ERROR)
-        """Stop running domain."""
+        """Stop a running domain"""
-        """Spawn process to run the given domain."""
+        """Start scraping the given domain by spawning a process"""
-                proc = reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)
+                reactor.spawnProcess(scrapy_proc, sys.executable, args=args, env=scrapy_proc.env)
-        return self.status(1, "No free slot to run another process.")
+            else:
-import os, pickle
+import os
-import pickle
+import cPickle as pickle
-from twisted.internet import defer
+
-
+serialize = lambda x: _serialize(x, 'json')
-def single_pipeline(do_unquote=True):
+def single_pipeline(remove_root=True, remove_tags=True, do_unquote=True):
-             remove_root,
+    if remove_root:
-def list_pipeline(extract=True):
+def list_pipeline(do_extract=True):
-                       ])
+    if do_extract:
-    
+
-    elif isinstance(location, (list, tuple)):
+    elif isinstance(location, XPathSelectorList):
-    elif isinstance(location, str):
+    elif isinstance(location, (list, tuple)):
-            current_url, base_url = re.search(r'((http://(?:www\.)?[\w\d\.-]+?)(?:/|$).*)/', locations[0].response.url).groups()
+            current_url, base_url = re.search(r'((http://(?:www\.)?[\w\d\.-]+?)(?:/|$).*)', locations[0].response.url).groups()
-            return extract_regex(expr, value, 'utf-8')
+            return flatten([extract_regex(expr, string, 'utf-8') for string in value])
-           ]
+def list_pipeline(extract=True):
-class AdaptorPipe:
+from scrapy.xpath.selector import XPathSelector, XPathSelectorList
-    def __init__(self, adaptors_dict=None):
+class AdaptorPipe:
-        self.pipes = adaptors_dict or {}
+        self.pipes = adaptors_pipe or {}
-        self.adaptors_dict = AdaptorPipe(adaptors_dict)
+    def setadaptors(self, adaptors_pipe):
-
+    
-        if not hasattr(self, attrname):
+        value = self._adaptors_pipe.execute(attrname, value)
-                
+        self.response = response
-                return XPathSelectorList([cls(node=node, parent=self, expr=xpath) for node in xpath_result])
+                return XPathSelectorList([cls(node=node, parent=self, expr=xpath, response=self.response)
-                return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath)])
+                return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath, response=self.response)])
-            raise unittest.SkipTest("TEST_DB not configured")
+        self.test_db = settings.get('TEST_SCRAPING_DB')
-            mysql_connect(TEST_DB)
+            mysql_connect(self.test_db)
-            raise unittest.SkipTest("Test database not available at: %s" % TEST_DB)
+            raise unittest.SkipTest("Test database not available at: %s" % self.test_db)
-        ddh = DomainDataHistory(TEST_DB, 'domain_data_history')
+        ddh = DomainDataHistory(self.test_db, 'domain_data_history')
-
+        TEST_DB = settings.get('TEST_DB')
-from scrapy.contrib.item.models import RobustScrapedItem, ValidationError, ValidationPipeline
+from scrapy.contrib.item.models import RobustScrapedItem, RobustItemDelta, ValidationError, ValidationPipeline
-        if other:
+        if isinstance(other, type(self)):
-        
+    
-        pass
+                
-    return re.sub('<!--.*?-->', '', text.decode('utf-8'), re.DOTALL)
+    return re.sub('<!--.*?-->', u'', text.decode('utf-8'), re.DOTALL)
-    return re_tags.sub('', text.decode('utf-8'))
+    return re_tags.sub(u'', text.decode('utf-8'))
-    return re_tags_remove.sub('', text.decode('utf-8'))
+    return re_tags_remove.sub(u'', text.decode('utf-8'))
-    return re_escape_chars.sub('', text.decode('utf-8'))
+    return re_escape_chars.sub(u'', text.decode('utf-8'))
-                diff[attr]['new'] = newval
+        delta = new - old
-        if diff:
+        if delta.diff:
-            s += display.pformat(diff) + "\n"
+            s += display.pformat(delta.diff) + "\n"
-        return self.version == other.version
+        if other:
-COMMANDS_SETTINGS_MODULE = '$project_name.conf.commands'
+# uncomment if you want to add your own custom scrapy commands
-            self.adaptors_pipe = adaptors_pipe
+    def setadaptors(self, adaptors_dict):
-import sha
+import hashlib
-from scrapy.http import Request
+            referer = request.headers.get('Referer')
-        img_path = os.path.join(netloc, sha.sha(url).hexdigest())
+        img_path = os.path.join(netloc, hashlib.sha1(url).hexdigest())
-
+"""
-"""Scrapy admin script"""
+"""Scrapy admin script is used to create new scrapy projects and similar
-import os, stat
+import os
-Usage: scrapy-admin.py [options] [command]
+scrapy-admin.py [options] [command]
-    parser = OptionParser(usage = usage)
+    parser = OptionParser(usage=usage)
-    parser.print_help()
+    if not args:
-    main()
+    main()
-import __project_name__
+import $project_name
-# - Scrapy settings for __project_name__                                    -
+# - Scrapy settings for $project_name                                    -
-BOT_NAME = 'scrapy-bot'
+BOT_NAME = 'scrapybot'
-ENABLED_SPIDERS_FILE = '%s/conf/enabled_spiders.list' % __project_name__.__path__[0]
+COMMANDS_MODULE = '$project_name.commands'
-    #'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',
+    'scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware',
-    #'scrapy.contrib.spidermiddleware.urlfilter.UrlFilterMiddleware',
+    'scrapy.contrib.spidermiddleware.limit.RequestLimitMiddleware',
-
+    def __nonzero__(self):
-    
+
-from scrapy.utils.markup import remove_entities, replace_tags
+from scrapy.utils.markup import remove_entities, replace_tags, remove_comments
-    def test_remove_tags(self):
+    def test_replace_tags(self):
-    return re.sub('<!--.*?-->', '', text, re.DOTALL)
+    return re.sub('<!--.*?-->', '', text.decode('utf-8'), re.DOTALL)
-    return re_tags.sub('', text)
+    return re_tags.sub('', text.decode('utf-8'))
-    return re_tags_remove.sub('', text)
+    return re_tags_remove.sub('', text.decode('utf-8'))
-                      if is empty do nothing.
+                      By default removes \n, \t, \r.
-    return re_escape_chars.sub('', text)
+    return re_escape_chars.sub('', text.decode('utf-8'))
-
+        self.assertEqual(safe_url_string("http://www.example.com/Brochures_&_Paint_Cards&PageSize=200"),
-            for request, spider, priority in _issue():
+        for domain in requests or ():
-        delay = getattr(spider, 'download_delay', None)
+        delay = getattr(spider, 'download_delay', None) or settings.getint("DOWNLOAD_DELAY", 0)
-            for request in requests[domain]:
+        
-        """ Parse crawl arguments and return a tuple of (domains, urls) """
+        """ Parse crawl arguments and return a dict domains -> [requests] """
-            spider = spiders.fromurl(request.url)
+            if request.domain:
-
+        
-            fingerprint_params=None):
+            fingerprint_params=None, domain=None):
-
+        #allows to directly specify the spider for the request
-        self.assertEqual(safe_url_string("http://clkuk.tradedoubler.com/click?p(28639)a(1424750)g(16962058)epi(59fc2ea58da61c7bce606df72a4a10fb3a0bd2ad)url(http://www.bedworld.net/beds_mattresses/112AURHB--/Aurora_Headboard.html.html)"), 'http://clkuk.tradedoubler.com/click?p(28639)a(1424750)g(16962058)epi(59fc2ea58da61c7bce606df72a4a10fb3a0bd2ad)url(http://www.bedworld.net/beds_mattresses/112AURHB--/Aurora_Headboard.html.html)')
+        self.assertEqual(safe_url_string("http://www.example.com/test?p(29)url(http://www.another.net/page)"),
-                    if isinstance(redirected.dont_filter, int) and redirected.dont_filter > 0:
+                    if isinstance(redirected.dont_filter, int):
-                    if isinstance(redirected.dont_filter, int):
+                    if isinstance(redirected.dont_filter, int) and redirected.dont_filter > 0:
-_safe_chars = urllib.always_safe + '%' + _reserved
+_unreserved_marks = "-_.!~*'()" #RFC 2396 sec 2.3
-            #print _failure
+                result.request = r
-                              self.is_gzip, self.is_bzip2]        
+        self.decompressors = {'tar': self.is_tar, 'zip': self.is_zip,
-    def extract(self, response):
+    
-        for decompressor in self.decompressors:
+        
-        return response
+            new_response = self.decompressors[decompressor](response)
-ent_re = re.compile(r'&(#?)([^&;]+);')
+_ent_re = re.compile(r'&(#?)([^&;]+);')
-    return ent_re.sub(convert_entity, text.decode('utf-8'))
+    return _ent_re.sub(convert_entity, text.decode('utf-8'))
-_ent_re = re.compile(r'&(#?)([^&;]+);')
+ent_re = re.compile(r'&(#?)([^&;]+);')
-        if m.group(1)=='#':
+        if m.group(1) == '#':
-    return _ent_re.sub(convert_entity, text.decode('utf-8'))
+    return ent_re.sub(convert_entity, text.decode('utf-8'))
-        return dammit.unicode
+        return self._unicode_content
-    _template = r'%s\s*=\s*[\"\']?\s*%s\s*[\"\']?'
+    _template = r'''%s\s*=\s*["']?\s*%s\s*["']?'''
-        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else failure.getTraceback()
+        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)
-    adaptors_pipe = AdaptorPipe([])
+    adaptors_pipe = AdaptorPipe()
-    CHARSET_RE = re.compile(r'<meta\s+http-equiv\s*=\s*[\"\']?\s*Content-Type\s*[\"\']?\s+content\s*=(?P<mime>[^;]+);\s*charset=(?P<charset>[\w-]+)', re.I)
+    _template = r'%s\s*=\s*[\"\']?\s*%s\s*[\"\']?'
-            match = self.CHARSET_RE.search(self._content[:5000])
+            chunk = self._content[:5000]
-    * if the refex doesn't contain any group the entire regex matching is returned
+    * if the regex doesn't contain any group the entire regex matching is returned
-
+        self.assertEqual(remove_entities('redirectTo=search&searchtext=MR0221Y&aff=buyat&affsrc=d_data&cm_mmc=buyat-_-ELECTRICAL & SEASONAL-_-MR0221Y-_-9-carat gold &frac12;oz solid crucifix pendant'),
-_ent_re = re.compile(r'&(#?)(.+?);')
+_ent_re = re.compile(r'&(#?)([^&;]+);')
-    
+
-    
+
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import signals, log
+from scrapy.core import signals
-from scrapy.core import signals, log
+from scrapy.core import signals
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log, signals
+from scrapy.core import signals
-from scrapy.core import log, signals
+from scrapy.core import signals
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log, signals
+from scrapy.core import signals
-from scrapy.core import log
+from scrapy import log
-        
+        
-from scrapy.core import log, signals
+from scrapy.core import signals
-from scrapy.core import log
+from scrapy import log
-        self.crawlers[pid] = crawler
+        self.crawlers[pid] = crawler
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log 
+from scrapy import log 
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log, signals
+from scrapy.core import signals
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import signals, log
+from scrapy.core import signals
-from scrapy.core import signals, log
+from scrapy.core import signals
-from scrapy.core import log
+from scrapy import log
-from scrapy.utils.defer import load_class
+from scrapy.utils.misc import load_class
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from twisted.python import log
+
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import signals, log
+from scrapy.core import signals
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import log
+from scrapy import log
-from scrapy.core import signals, log
+from scrapy.core import signals
-from scrapy.core import log
+from scrapy import log
-        return conn
+        return conn
-from scrapy.core.mail import MailSender
+from scrapy.mail import MailSender
-from scrapy.core.mail import MailSender
+from scrapy.mail import MailSender
-from scrapy.core.mail import MailSender
+from scrapy.mail import MailSender
-from scrapy.utils.misc import mustbe_deferred, defer_result
+from scrapy.utils.defer import mustbe_deferred, defer_result
-from scrapy.utils.misc import mustbe_deferred
+from scrapy.utils.defer import mustbe_deferred
-from scrapy.utils.misc import mustbe_deferred
+from scrapy.utils.defer import mustbe_deferred
-from scrapy.utils.misc import defer_succeed
+from scrapy.utils.defer import defer_succeed
-from scrapy.utils.misc import chain_deferred
+from scrapy.utils.defer import chain_deferred, mustbe_deferred
-from scrapy.utils.misc import load_class, mustbe_deferred
+from scrapy.utils.misc import load_class
-from scrapy.utils.misc import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate
+from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate
-from scrapy.utils.misc import load_class
+from scrapy.utils.defer import load_class
-from scrapy.utils.misc import defer_fail
+from scrapy.utils.defer import defer_fail
-from scrapy.utils.misc import chain_deferred
+from scrapy.utils.defer import chain_deferred
-from scrapy.utils.misc import load_class, defer_succeed, mustbe_deferred
+from scrapy.utils.misc import load_class
-from scrapy.utils.misc import load_class, mustbe_deferred
+from scrapy.utils.misc import load_class
-from twisted.python import failure
+from twisted.internet import defer
-
+from scrapy.utils.markup import remove_entities
-        return [unquote_html(s, keep_reserved=True) for s in strings]
+        return [remove_entities(s, keep=['lt', 'amp']) for s in strings]
-        return [unquote_html(unicode(s, encoding), keep_reserved=True) for s in strings]
+        return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]
-)
+from django.contrib import admin
-                                 default=True)
+    public = models.BooleanField(_("public"), blank=False, default=True)
-    updated = models.DateTimeField(_("updated"), core=True, editable=False)
+    created = models.DateTimeField(_("created"), editable=False)
-        save_on_top = True
+#     class Admin:
-from django.views.generic.simple import direct_to_template
+from django.conf.urls.defaults import *
-    (r"^weblog/", include("scrapyorg.blog.urls")),
+    url(r"^weblog/", include("scrapyorg.blog.urls")),
-    (r"^admin/", include("django.contrib.admin.urls")),
+    url(r"^admin/download/downloadlink/", include("scrapyorg.download.urls")),
-        (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:], 'django.views.static.serve', {'document_root': settings.MEDIA_ROOT}),
+        (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:],
-    (r"", include("scrapyorg.article.urls")),
+    url(r"", include("scrapyorg.article.urls")),
-from decobot.utils.link_extraction import follow_link_pattern
+from scrapy.link import LinkExtractor
-        return follow_link_pattern(response.body.to_string(), self.parse_item, response.url, self.itemurl_re)
+        xlink = LinkExtractor()
-            print "Could not open file %s for writing. Output dumped to /tmp/decobot-schedule.tmp instead." % opts.output
+            open("/tmp/scrapy-cluster-schedule.tmp", "w").write(output)
-    main()
+    main()
-        if fp not in info.downloaded:
+        if fp in info.downloaded:
-        delay = settings.getint('DOWNLOAD_DELAY') or getattr(spider, 'download_delay', None)
+        delay = getattr(spider, 'download_delay', None)
-from scrapy.http import Request, Response, Headers, HTTPClientFactory
+from scrapy.http import Request, Response, Headers
-        delay = settings.getint('DOWNLOAD_DELAY', None) or getattr(spider, 'download_delay', None)
+        delay = settings.getint('DOWNLOAD_DELAY') or getattr(spider, 'download_delay', None)
-from scrapy.http import Request, Response, Headers
+from scrapy.http import Request, Response, Headers, HTTPClientFactory
-        delay = getattr(spider, 'download_delay', None)
+        delay = settings.getint('DOWNLOAD_DELAY', None) or getattr(spider, 'download_delay', None)
-    version = '0.1',
+    name = 'scrapy',
-        '': ['*.tmpl'],
+        'scrapy': ['templates/*.tmpl'],
-setup (
+setup(
-        findfiles('*.tmpl', 'scrapy/templates')
+    packages = find_packages(),
-    data_files = [],
+from setuptools import setup, find_packages
-            return defer_result(info.downloaded[fp])
+        wad = request.deferred or defer.Deferred()
-            self._download(request, info, fp)
+        fp = request.fingerprint()
-        wad = defer.Deferred()
+        wad = request.deferred or defer.Deferred()
-                )
+        result = self.media_to_download(request, info)
-        This method is called every time an item media is enqueue for download.
+        This method is called every time a media is requested for download, and
-        result, and if you want to store returned valued as result for request.
+            - the return value is cached and piped into `item_media_downloaded` or `item_media_failed`
-        self.cache = {}
+        self.domaininfo = {}
-        self.cache[domain] = DomainInfo(domain)
+        self.domaininfo[domain] = DomainInfo(domain)
-        del self.cache[domain]
+        del self.domaininfo[domain]
-        info = self.cache[domain]
+        info = self.domaininfo[domain]
-sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), "lib"))
+sys.path.insert(0, os.path.join(os.path.abspath(os.path.dirname(__file__)), "xlib"))
-                'get_urls_from_item should return None or iterable'
+        requests = self.get_media_requests(item, info)
-            log.msg('Unhandled ERROR in MediaPipeline.{new,failed}_item_media for %s: %s' % (request, _failure), log.ERROR, domain=domain)
+            log.msg('Unhandled ERROR in MediaPipeline.item_media_{downloaded,failed} for %s: %s' % (request, _failure), log.ERROR, domain=domain)
-            request = url if isinstance(url, Request) else Request(url=url)
+        for request in requests or ():
-                    callback=self.new_item_media,
+                    callback=self.item_media_downloaded,
-                    errback=self.failed_item_media,
+                    errback=self.item_media_failed,
-                errback=self.media_failure,
+                errback=self.media_failed,
-            - call `failed_item_media` if value is Failure instance
+            - call `item_media_downloaded` with this value as input unless value is Failure instance
-        """ Return the urls or Request objects to download for this item
+    def get_media_requests(self, item, info):
-        Return value is cached and used as input for `new_item_media` method.
+        Return value is cached and used as input for `item_media_downloaded` method.
-    def media_failure(self, failure, request, info):
+    def media_failed(self, failure, request, info):
-        Return value is cached and used as input for `failed_item_media` method.
+        Return value is cached and used as input for `item_media_failed` method.
-    def new_item_media(self, result, item, request, info):
+    def item_media_downloaded(self, result, item, request, info):
-        returned by `media_failure` hook.
+        returned by `media_failed` hook.
-    def failed_item_media(self, failure, item, request, info):
+    def item_media_failed(self, failure, item, request, info):
-        result is the returned Failure instance of `media_failure` hook, or Failure instance
+        result is the returned Failure instance of `media_failed` hook, or Failure instance
-            log.msg('MediaPipeline Unhandled ERROR in %s: %s' % (request, _failure), log.ERROR, domain=domain)
+            log.msg('Unhandled ERROR in MediaPipeline.{new,failed}_item_media for %s: %s' % (request, _failure), log.ERROR, domain=domain)
-        dwld.addErrback(_bugtrap, request)
+        def _bugtrap(_failure):
-        dwld = mustbe_deferred(self.download(request, info))
+        def _bugtrap(_failure, request):
-        urls = self.get_urls_from_item(item)
+        urls = self.get_urls_from_item(item, info)
-                    callbackArgs=(item, request),
+                    callbackArgs=(item, request, info),
-                    errbackArgs=(item, request),
+                    errbackArgs=(item, request, info),
-        dlst.addBoth(lambda _: self.item_completed(item))
+        dlst.addBoth(lambda _: self.item_completed(item, info))
-        dwld.addCallbacks(self.media_downloaded, self.media_failure)
+        dwld.addCallbacks(
-    def get_urls_from_item(self, item):
+    def get_urls_from_item(self, item, info):
-    def media_downloaded(self, response):
+    def media_downloaded(self, response, request, info):
-    def media_failure(self, failure):
+    def media_failure(self, failure, request, info):
-    def new_item_media(self, result, item, request):
+    def new_item_media(self, result, item, request, info):
-    def failed_item_media(self, failure, item, request):
+    def failed_item_media(self, failure, item, request, info):
-    def item_completed(self, item):
+    def item_completed(self, item, info):
-from twisted.python import failure
+from scrapy.utils.misc import mustbe_deferred, defer_result
-from scrapy.utils.misc import chain_deferred, mustbe_deferred, defer_result
+from scrapy.http import Request
-        urls = self.get_urls_from_item(item)
+        urls = self.get_urls_from_item(item)
-            dfd = self._enqueue(url, info)
+            request = url if isinstance(url, Request) else Request(url=url)
-                    callbackArgs=(item, url),
+                    callbackArgs=(item, request),
-                    errbackArgs=(item, url),
+                    errbackArgs=(item, request),
-            return defer_result(request)
+    def _enqueue(self, request, info):
-        dwld = mustbe_deferred(self.request(request, info))
+        dwld = mustbe_deferred(self.download(request, info))
-        dwld.addBoth(self._download_finished, info, fp)
+        dwld.addBoth(self._downloaded, info, fp)
-    def _download_finished(self, result, info, fp):
+    def _downloaded(self, result, info, fp):
-
+        del info.downloading[fp]
-    def request(self, request, info):
+
-        self.cache[domain] = DomainInfo(domain)
+    def media_to_download(self, request, info):
-        del self.cache[domain]
+        This method is called every time an item media is enqueue for download.
-        return item.image_urls
+        """ Return the urls or Request objects to download for this item
-        return Request(url=url)
+        """
-        pass
+        """ Method called on success download of media request
-        return _failure
+    def new_item_media(self, result, item, request):
-        pass
+        result is the return value of `media_downloaded` hook, or the non-Failure instance
-        pass
+        return value of this method isn't important and is recommended to return None.
-        return item
+        """ Method called when all media requests for a single item has returned a result or failure.
-from scrapy.utils.misc import chain_deferred, mustbe_deferred
+from scrapy.utils.misc import chain_deferred, mustbe_deferred, defer_result
-        ulist = []
+        lst = []
-                    callbackArgs=(item,),
+                    callbackArgs=(item, url),
-                    errbackArgs=(item,),
+                    errbackArgs=(item, url),
-            ulist.append(dfd)
+            lst.append(dfd)
-        return dlist
+        dlst = defer.DeferredList(lst, consumeErrors=False)
-        wad = defer.Deferred()
+        request = self.media_to_download(url, info)
-        waiting = info.waiting.setdefault(fp, []).append(wad)
+        if fp in info.downloaded:
-            info.downloading[fp] = (request, dwld)
+            self._download(request, info, fp)
-        waiting = info.waiting[fp]
+        del info.downloading[fp]
-        return result
+            defer_result(result).chainDeferred(wad)
-        return self.item_completed(item) #dummy as hell
+    def request(self, request, info):
-        pass
+    def media_to_download(self, url, info):
-    def new_item_media(self, result, item):
+    def new_item_media(self, result, item, url):
-    def failed_item_media(self, _failure, item):
+    def failed_item_media(self, _failure, item, url):
-    return deferred
+        return defer_result(result)
-            log.msg("_%s_ Pipeline stage: %s" % (item.guid, type(current_stage).__name__), log.TRACE, domain=domain)
+            log.msg("_%s_ Pipeline stage: %s" % (item, type(current_stage).__name__), log.TRACE, domain=domain)
-            log.msg("_%s_ Pipeline finished" % item.guid, log.TRACE, domain=domain)
+            log.msg("_%s_ Pipeline finished" % item, log.TRACE, domain=domain)
-from pprint import pprint
+
-        for url in urls:
+        for url in urls or ():
-        info.downloading[fp] = (request, dwld)
+        if fp not in info.downloading:
-    def status_as_dict(self, verbosity=0):
+    def status_as_dict(self, verbosity=1):
-            if verbosity == 0:
+            if verbosity == 1:
-            else:
+            elif verbosity == 2:
-        if verbosity == 0:
+    def print_pending(self, verbosity=1):
-        else:
+        elif verbosity == 2:
-        
+        return
-                return self.ws_status(wc_request)
+                return self.ws_status(wc_request, verbosity=0)
-    def ws_status(self, wc_request):
+    def ws_status(self, wc_request, verbosity=1):
-        verbosity = wc_request.args['verbosity'][0] if 'verbosity' in wc_request.args else '0'
+        verbosity = int(wc_request.args['verbosity'][0]) if 'verbosity' in wc_request.args else verbosity
-        
+        if verbosity > 0:
-    parser.add_option("--verbosity", dest="verbosity", type="int", help="Sets the report status verbosity.", default=0)
+    parser.add_option("--verbosity", dest="verbosity", type="int", help="Sets the report status verbosity.")
-    post = {"format":opts.format, "verbosity":opts.verbosity}
+    post = {"format":opts.format}
-    
+
-
+    if not output:
-        chcount, chreport = self._report_differences(self.before_db, self.now_db, guids_both)
+        changed_items, chreport = self._report_differences(self.before_db, self.now_db, guids_both)
-        if chcount == 0 and opts.quiet:
+        ok_items = len(guids_both) - changed_items
-            s += "  Changed items : %d\n" % chcount
+            s += "Total items     : %d\n" % (len(guids_both) + new_items + missing_items)
-            s += "- NEW ITEMS (%d) -----------------------------------------\n" % len(guids_new)
+            s += "- NEW ITEMS (%d) -----------------------------------------\n" % new_items
-            s += "- MISSING ITEMS (%d) -------------------------------------\n" % len(guids_missing)
+            s += "- MISSING ITEMS (%d) -------------------------------------\n" % missing_items
-            s += "- CHANGED ITEMS (%d) -------------------------------------\n" % chcount
+            s += "- CHANGED ITEMS (%d) -------------------------------------\n" % changed_items
-        )
+    def __init__(self, adaptors_dict=None):
-    def insertadaptor(self, function, name, match_function=lambda x: True, after=None, before=None, compile_pipe=True):
+        Receives a dictionary that maps attribute_name to a list of adaptor functions
-        self._compile_pipe()
+        self.pipes = adaptors_dict or {}
-        for adaptor in self.pipes.get(attrname, []):
+        for function in self.pipes.get(attrname, []):
-                value = adaptor.function(value)
+                    print "  %07s | input >" % function.func_name, repr(value)
-                    print "  %07s | output>" % adaptor.name, repr(value)
+                    print "  %07s | output>" % function.func_name, repr(value)
-                print "Error in '%s' adaptor. Traceback text:" % adaptor.name
+                print "Error in '%s' adaptor. Traceback text:" % function.func_name
-            args = ['decobot-ctl.py', 'crawl']
+            args = ['scrapy-crawl', 'crawl']
-        s = "CRAWLING DIFFERENCES REPORT\n\n"
+        if chcount == 0 and opts.quiet:
-        s += "  Changed items : %d\n" % chcount
+            s += "Total items     : %d\n" % (len(guids_both) + len(guids_new) + len(guids_missing))
-        s += self._format_items([self.now_db[g] for g in guids_new])
+            s += "\n"
-        s += self._format_items([self.before_db[g] for g in guids_missing])
+            s += "\n"
-        s += chreport
+            s += "\n"
-
+
-from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector
+from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector
-        xmldoc='\n'.join((
+        xmldoc = '\n'.join((
-    unittest.main()   
+    def test_unquote(self):
-    
+
-    def __init__(self, adaptors=None, adaptorclass=None):
+    def __init__(self, attribute_names, adaptors=None, adaptorclass=None):
-        . Example:
+        Example:
-                self.insertadaptor(*entry)
+                self.insertadaptor(compile_pipe=False, *entry)
-    def insertadaptor(self, function, name, match_function=lambda x: True, after=None, before=None):
+    def insertadaptor(self, function, name, match_function=lambda x: True, after=None, before=None, compile_pipe=True):
-    def execute(self, match_condition, value, debug=False):
+    def removeadaptor(self, adaptorname):
-                        print "  %07s | output>" % adaptor.name, repr(value)
+        for adaptor in self.pipes.get(attrname, []):
-                    return
+            except Exception, e:
-    adaptors_pipe = AdaptorPipe()
+    adaptors_pipe = AdaptorPipe([])
-        value = self.adaptors_pipe.execute(match_condition or attrname, value)
+    def attribute(self, attrname, value):
-from cStringIO import StringIO
+try:
-            tar_file = tarfile.open(fileobj=self.archive)
+            tar_file = tarfile.open(name='tar.tmp', fileobj=self.archive)
-    def get_url(self, url):
+    def get_url(self, url, decompress=False):
-        url = url.strip()
+                if decompress:
-            
+
-                self.get_url(url)
+            def _get_magic(shell, arg):
-        self.archive.seek(0)
+from cStringIO import StringIO
-           
+    def __init__(self):
-            return response
+            raise self.ArchiveIsEmpty
-        self._genfiles('spider.tmpl', '%s/%s.py' % (spidersdir, name), tvars)
+        self._genfiles(self.template_name, '%s/%s.py' % (spidersdir, name), tvars)
-DEFAULT_PRIORITY = settings.getint("DEFAULT_PRIORITY") or 20
+DEFAULT_PRIORITY = settings.getint("DEFAULT_PRIORITY", 20)
-    setattr(sys.modules[__name__], "PRIORITY_%s" % attr, val )
+DEFAULT_PRIORITY = settings.getint("DEFAULT_PRIORITY") or 20
-    def schedule(self, domains, spider_settings=None, priority=PRIORITY_NORMAL):
+    def schedule(self, domains, spider_settings=None, priority=DEFAULT_PRIORITY):
-from scrapy.contrib.pbcluster.master.manager import *
+from scrapy.contrib.pbcluster.master.manager import ClusterMaster, DEFAULT_PRIORITY
-            priority = int(args.get("priority", ["PRIORITY_NORMAL"])[0])
+            priority = int(args.get("priority", [DEFAULT_PRIORITY])[0])
-        s += "</select>\n"
+        s += "<input type='text' name='priority'>%s</input>" % DEFAULT_PRIORITY
-            post["priority"] = "PRIORITY_NOW"
+            post["priority"] = "0"
-    def __init__(self, function, name, attribute_re=None, attribute_list=None, negative_match=False):
+    def __init__(self, function, name, match_function):
-        return self.basefunction(value, **pipeargs)
+        self.match_function = match_function
-        "define_from" is an ordered tuple of 4-elements tuples, each of which
+        "adaptors" is an ordered tuple of 3-elements tuples, each of which
-          ("my_function", "my_function", None, "name")
+          (my_function, "my_function", lambda x: x in my_list)
-    def insertadaptor(self, function, name, attrs_re=None, attrs_list=None, negative_match=False, after=None, before=None):
+    def insertadaptor(self, function, name, match_function=lambda x: True, after=None, before=None):
-        negative_match is False (i.e. match -> apply)
+        Inserts a "function" as an adaptor that will apply when match_function returns True (by
-        argument to ignore unused keywords. "name" is the name of the adaptor.
+        the adaptor of the given name. "name" is the name of the adaptor.
-            adaptor = self.__adaptorclass(function, name, attrs_re, attrs_list, negative_match)
+            adaptor = self.__adaptorclass(function, name, match_function)
-    def execute(self, attrname, value, debug=False, **pipeargs):
+    def execute(self, match_condition, value, debug=False):
-            if adapt:
+            if adaptor.match_function(match_condition):
-                    value = adaptor.function(value, **pipeargs)
+                    value = adaptor.function(value)
-                    
+
-        value = self.adaptors_pipe.execute(attrname, value, **pipeargs)
+    def attribute(self, attrname, value, match_condition=None):
-from .metrics import tagdepth
+from scrapy.contrib.rulengine.responseWrapper import ResponseWrapper
-        self.reportfile = open(repfilename, "a") if repfilename else None
+        self.reportfile = open(repfilename, "a") if repfilename else None        
-            group, simrate = self.get_similarity_group(response)
+            group, simrate, simhash = self.get_similarity_group(response)
-                self.sim_groups[group]['similar_urls'].append((response.url, simrate))
+                wres = ResponseWrapper(response)
-        sh = self.metric.simhash(response)
+        sh = self.metric.simhash(response, symnumbers=True)
-        return (None, 0)
+                return (group, simrate, data['simhash'])
-        data['simhash'] = self.metric.simhash(response)
+        data['simhash'] = self.metric.simhash(response, symnumbers=True)
-1. simhash(response)
+1. simhash(response, *args)
-relevant_tags = set(['div', 'table', 'td', 'tr', 'h1'])
+relevant_tags = set(['div', 'table', 'td', 'tr', 'h1','p'])
-def simhash(response):
+def simhash(response, symnumbers=False):
-    return set(symdict.keys())
+    symdict = get_symbol_dict(soup.find('body'), relevant_tags)
-    return len(sh1 & sh2) / len(sh1 | sh2)
+    if sh1 == sh2:
-    def __init__(self, function, name, attribute_re=None, attribute_list=None):
+    def __init__(self, function, name, attribute_re=None, attribute_list=None, negative_match=False):
-    def insertadaptor(self, function, name, attrs_re=None, attrs_list=None, after=None, before=None):
+    def insertadaptor(self, function, name, attrs_re=None, attrs_list=None, negative_match=False, after=None, before=None):
-        
+        If negative_match is True, the adaptor will be applied only if there is no match. By default,
-            adaptor = self.__adaptorclass(function, name, attrs_re, attrs_list)
+            adaptor = self.__adaptorclass(function, name, attrs_re, attrs_list, negative_match)
-            adapt = False
+            match = False
-                        adapt = attrname in adaptor.attribute_list
+                        match = attrname in adaptor.attribute_list
-                        adapt = True
+                        match = True
-                adapt = attrname in adaptor.attribute_list
+                match = attrname in adaptor.attribute_list
-                adapt = True
+                match = True
-        port = eval(port)
+        port = int(port)
-        return strings
+from traceback import format_exc
-    def execute(self, attrname, value, **pipeargs):
+    def execute(self, attrname, value, debug=False, **pipeargs):
-                value = adaptor.function(value, **pipeargs)
+                try:
-            priority = eval(args.get("priority", ["PRIORITY_NORMAL"])[0])
+            priority = int(args.get("priority", ["PRIORITY_NORMAL"])[0])
-                total += rule_result
+"""
-    def __init__(self, define_from=None, adaptorclass=None):
+    def __init__(self, adaptors=None, adaptorclass=None):
-        path to the adaptor function. Example:
+        If "adaptors" is given, constructs pipeline from this.
-          (".*", "replace_tags", "scrapy.utils.markup.replace_tags")
+          ("my_function", "my_function", None, "name")
-        self.attribute_re = re.compile(attribute_re)
+        self.attribute_re = re.compile(attribute_re) if attribute_re else None
-
+        If both, attrs_re and attrs_list are given, apply both. Else if only one is given, apply those.
-            elif adaptor.attribute_re.search(attrname) and attrname in adaptor.attribute_list:
+            if adaptor.attribute_re:
-    generator = iter(generator)
+    generator = iter(generator or [])
-        url = canonicalize_url(url.strip())
+        url = url.strip()
-        from scrapy.fetcher import get_or_create_spider
+        url = canonicalize_url(url.strip())
-                print "OK"
+                print "Done."
-    def __init__(self, function, name, attribute_re=None):
+    def __init__(self, function, name, attribute_re=None, attribute_list=None):
-            attribute_re = ".*"
+        attribute_re = attribute_re or ".*"
-    def insertadaptor(self, function, name, attrs_re=None, after=None, before=None):
+    def insertadaptor(self, function, name, attrs_re=None, attrs_list=None, after=None, before=None):
-        adaptor of name "before". The "function" must always have a **keyword
+        which matches regex given in "attrs_re" (None matches all), or are included in "attrs_list" list.
-            adaptor = self.__adaptorclass(function, name, attrs_re)
+            adaptor = self.__adaptorclass(function, name, attrs_re, attrs_list)
-            if adaptor.attribute_re.search(attrname):
+            adapt = False
-        ScrapedItem.adaptors_pipes = adaptors_pipes
+    adaptors_pipe = AdaptorPipe()
-        value =ScrapedItem.adaptors_pipe.execute(attrname, value, **pipeargs)
+        value = self.adaptors_pipe.execute(attrname, value, **pipeargs)
-            setattr(self, attrname, value)
+            setattr(self, attrname, value)
-        return name
+class DuplicatedAdaptorName(Exception): pass
-        return value
+        return value
-class CrawlMiddleware(object):
+class RefererMiddleware(object):
-spider will be ignored.
+RequestLimitMiddleware: Limits the scheduler request queue from the point of
-Scrapemiddlware to restrict crawling to only some particular URLs
+RestrictMiddleware: restricts crawling to fixed set of particular URLs
-
+"""
-        assert isinstance(url, basestring), 'Request url argument must be str or unicode, got %s:' % (type(url), url)
+        assert isinstance(url, basestring), 'Request url argument must be str or unicode, got %s:' % type(url).__name__
-            return self._fingerprint
+        """Returns unique resource fingerprint with caching support"""
-    Illegal characters are escaped.  See rfc3968.
+    Illegal characters are escaped (RFC-3986)
-        self.assertEqual(canonicalize_url(u"http://user:pass@www.example.com/do?a=1#frag", remove_fragments=False),
+        self.assertEqual(canonicalize_url(u"http://user:pass@www.example.com/do?a=1#frag", keep_fragments=True),
-def canonicalize_url(url, keep_blank_values=False, remove_fragments=True):
+def canonicalize_url(url, keep_blank_values=False, keep_fragments=False):
-    - remove fragments (if remove_fragments is True)
+    - remove fragments (unless keep_fragments is True)
-    if remove_fragments:
+    if not keep_fragments:
-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner
+from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner, canonicalize_url
-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url
+from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner
-    """
+    """Return the value of a url parameter, given the url and parameter name"""
-     
+def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):
-    """
+    """Add or remove a parameter to a given url"""
-        if _has_querystring(url):
+        if has_querystring(url):
-        raise NotImplemented
+from scrapy.item.adaptors import AdaptorPipe
-        return value
+def extract(value):
-    adaptors_pipe = [ExtractAdaptor()]
+    adaptors_pipe = standardpipe
-            setattr(item, name, value)
+    def attribute(self, attrname, value, **pipeargs):
-    scheme, netloc, path, query, fragment = urlparse.urlsplit(safe_url)
+    scheme, netloc, path, query, _ = urlparse.urlsplit(safe_url)
-            settings.overrides['LOG_ENABLED'] = False
+        if args:
-        self.vars['scrape_help'] = self.print_vars
+        print "Available commands:"
-            if url:
+            
-            
+                self.generate_vars(None, None)
-
+        #sometimes we download responses without opening/closing domains,
-        fp = self.metric.simhash(response)
+        sh = self.metric.simhash(response)
-            simrate = self.metric.compare(fp, data['simhash'])
+            simrate = self.metric.compare(sh, data['simhash'])
-#!/usr/bin/env python
+"""
-from BeautifulSoup import BeautifulSoup, Tag
+from BeautifulSoup import Tag
-relevant_tags = ['div', 'table', 'td', 'tr', 'h1']
+relevant_tags = set(['div', 'table', 'td', 'tr', 'h1'])
-            symbol = str("%d%s" % (depth, tag.name))
+            symbol = "%d%s" % (depth, str(tag.name))
-    symdict = get_symbol_dict(soup.find('body'), relevant_tags)
+    soup = response.soup
-    return len(fp1 & fp2) / len(fp1 | fp2)
+def compare(sh1, sh2):
-from scrapy.fetcher import fetch
+from scrapy.core.manager import scrapymanager
-    print '-' * 78
+    @param reactor: The L{IReactorThreads} provider which will be used to
-    def update_vars(self, vars):
+    def update_vars(self):
-    def run(self, args, opts):
+    def get_url(self, url):
-                pass
+
-            code.interact(local=vars)
+                self.generate_vars(url, None)
-    vars['spider'] = spiders.fromurl(url)
+    if url:
-        return "<url>"
+        return "[url]"
-        url = args[0]
+        
-        response = responses[0]
+        response = None
-        return clean_markup(string, **pipeargs)
+        return strings
-    return string
+from scrapy.utils.markup import clean_markup
-    def function(self, attrname, location, **pipeargs):
+    def function(self, location, **pipeargs):
-    def function(self, attrname, string, **pipeargs):
+    def function(self, string, **pipeargs):
-        return xml
+    def clean(self, string, **pipeargs):
-        return xml
+    def clean(self, string, **pipeargs):
-    def function(self, item, location, **pipeargs):
+    def function(self, attrname, location, **pipeargs):
-    def function(self, item, location, **pipeargs):
+    def function(self, attrname, string, **pipeargs):
-        return self.extract(location, **kwargs)
+    def _extract(self, location, **kwargs):
-        pre/post functions. Also prints a lot of useful debugging info."""
+        """Executes an adaptor printing a lot of useful debugging info."""
-        return self.do(self._extract, location, **pipeargs)
+    def function(self, item, location, **pipeargs):
-    def _extract(self, location, **kwargs):
+    def extract(self, location, **kwargs):
-            strings = flatten([self._extract(o, **kwargs) for o in location])
+            strings = flatten([self.extract(o, **kwargs) for o in location])
-    def function(self, item, attrname, value, **pipeargs):
+    def function(self, item, value, **pipeargs):
-    def function(self, item, attrname, value, **pipeargs):
+    def function(self, item, value, **pipeargs):
-    adaptors_pipe = [ExtractAdaptor(), AssignAdaptor()]
+    adaptors_pipe = [ExtractAdaptor()]
-            value = adaptor.function(self, name, value, **pipeargs)
+            value = adaptor.function(self, value, **pipeargs)
-Adaptors for item adaptation pipe.
+Generic Adaptors for item adaptation pipe. Performs tasks
-class BaseAdaptor:
+class BaseAdaptor(object):
-    for arg in sys.argv[1:]:
+def getcmdname(argv):
-    s += "       %s <subcommand> -h\n\n" % sys.argv[0]
+def usage(argv):
-    cmdname = getcmdname()
+    cmdname = getcmdname(argv)
-        print usage()
+        print usage(argv)
-        print 'Type "%s -h" for help' % sys.argv[0]
+        print 'Type "%s -h" for help' % argv[0]
-    (opts, args) = parser.parse_args()
+    (opts, args) = parser.parse_args(args=argv[1:])
-        self.action_crawl(opts)
+        self.replay.update(args=opts.targets, opts=opts.__dict__)
-        self.play(args, opts)
+        if (opts['pages']):
-will be filtered out.
+That is, if the scheduler queue contains an equal or greater ammount of
-        s += ">>> Total: %d responses downloaded\n" % len(self.replay.responses_old)
+        s += ">>> Total: %d responses received\n" % len(self.replay.responses_old)
-        dispatcher.connect(self.response_downloaded, signal=signals.response_downloaded)
+        dispatcher.connect(self.response_received, signal=signals.response_received)
-            log.msg("Replay: recorded in %s: %d/%d scraped/passed items, %d downloaded responses" % \
+            log.msg("Replay: recorded in %s: %d/%d scraped/passed items, %d received responses" % \
-    def response_downloaded(self, response, spider):
+    def response_received(self, response, spider):
-        if self.recording and key:
+        if (self.recording or self.updating) and key:
-from scrapy.core import log
+from scrapy.core import signals, log
-        request or exception middleware, or the appropriate download function;
+        request or exception middleware, or the appropiate download function;
-                        'Middleware %s.process_request must returns None, Response or Request, got %s ' % \
+                        'Middleware %s.process_request must return None, Response or Request, got %s' % \
-                    'Middleware %s.process_response must returns Response or Request, got %s ' % \
+                    'Middleware %s.process_response must return Response or Request, got %s' % \
-                    'Middleware %s.process_exception must returns None, Response or Request, got %s ' % \
+                    'Middleware %s.process_exception must return None, Response or Request, got %s' % \
-        extract its data """
+    def attribute(self, name, value, **pipeargs):
-        setattr(self, name, value)
+        for adaptor in ScrapedItem.adaptors_pipe:
-        tsk.stop()
+        if tsk.running:
-                tsk.stop()
+                if tsk.running:
-        """Resume the execution enine"""
+        """Resume the execution engine"""
-will be applied.If given a value of 0, no limit will be applied.
+will be applied. If given a value of 0, no limit will be applied.
-will be applied. If given a value of 0, no limit will be applied.
+will be applied.If given a value of 0, no limit will be applied.
-    _last_queue_size = 0
+    #_last_queue_size = 0
-        self._last_queue_size = actual_size
+        #actual_size = len(scrapyengine.scheduler.pending_requests[spider.domain_name])
-from scrapy.contrib.pbcluster.master.web import ClusterMasterWeb
+from scrapy.contrib.pbcluster.master.web import ClusterMasterWeb
-            deferred = self.__remote.callRemote("register_crawler", domain, self)
+            deferred = self.__remote.callRemote("register_crawler", os.getpid(), self)
-            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))
+            deferred.addCallbacks(callback=lambda x: None, errback=lambda reason: log.msg(reason, log.ERROR))
-        d.addCallbacks(callback=lambda obj: self.worker=Node(self, obj), errback=lambda reason: log.msg(reason, log.ERROR))
+        def _set_worker(obj):
-        del self.procman.crawlers[self.domain]
+        del self.procman.crawlers[self.pid]
-        self.crawlers = {}#a dict domain->scrapy process remote pb connection
+        self.crawlers = {}#a dict pid->scrapy process remote pb connection
-            d = self.crawler["domain"].callRemote("stop")
+            d = self.crawlers[proc.pid].callRemote("stop")
-        self.crawlers['domain'] = crawler
+    def remote_register_crawler(self, pid, crawler):
-            d.addCallbacks(callback=lambda x: proc.status="closing", errback=lambda reason: log.msg(reason, log.ERROR))
+            def _close():
-class Node(pb.Referenceable):
+class Broker(pb.Referenceable):
-        #on how statistics works, see self.update_nodes() and Nodes.remote_update()
+        #on how statistics works, see self.update_nodes() and Broker.remote_update()
-        node = Node(cworker, name, self)
+        node = Broker(cworker, name, self)
-        self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': '0', 'WEBCONSOLE_ENABLED': '0'})
+        self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': 0, 'CLUSTER_CRAWLER_ENABLED': 1, 'WEBCONSOLE_ENABLED': 0})
-        self.running = {}
+        self.running = {}#a dict domain->ScrapyProcessControl 
-            proc.status = "closing"
+            d = self.crawler["domain"].callRemote("stop")
-        return MySQLdb.connect(**d)
+        conn = MySQLdb.connect(**d)
-        log.msg("Connecting db with settings %s" % d )
+        
-        if not settings['WS_ENABLED']:
+        if not settings.getbool('WS_ENABLED'):
-        #d['reconnect'] = 1
+        d.update(settings.get("MYSQL_CONNECTION_SETTINGS"))
-        d['reconnect'] = 1
+        #d['reconnect'] = 1
-                    redirected.dont_filter = False
+                    if isinstance(redirected.dont_filter, int):
-                self.dropped_file.close()
+        settings.overrides['CACHE2_EXPIRATION_SECS'] = -1
-        self.total = { 'passed': 0, 'dropped': 0 }
+        self.total = { 'passed': 0, 'dropped': 0, 'variants': 0 }
-                self.passed_file.write('\n--- Total scraped products: %d\n' % self.total['passed'])
+                self.passed_file.write('\n--- Total scraped: %d products + %d variants\n' % (self.total['passed'], self.total['variants']))
-            product_text = '%s\n##Variants\n%s' % (product_text, ''.join([self.get_product_attribs(variant) for variant in product.variants]))
+            product_text = '%s\n##Variants\n%s' % (product_text, '\n'.join([self.get_product_attribs(variant) for variant in product.variants]))
-            self.passed_file.close()
+            if self.passed_file:
-            self.dropped_file.close()
+            if self.dropped_file:
-    (r"^(?P<article_id>\d+)/publish/toggle/$", publish_toggle),
+    (r"^$", render_template, { "path": "home" }),
-from django.contrib.admin.views.decorators import staff_member_required
+from os.path import join
-from scrapyorg.article.models import Article
+from django.template import TemplateDoesNotExist
-    return HttpResponseRedirect("/admin/article/article/")
+ARTICLES_TEMPLATES_DIR = "articles"
-    return HttpResponseRedirect("/admin/article/article/")
+def render_template(request, path):
-    (r"^admin/article/article/", include("scrapyorg.article.urls")),
+    
-        return [r for r in result or () if _filter(r)]
+        return (r for r in result or () if _filter(r))
-        return [r for r in result or () if _filter(r)]
+        return (r for r in result or () if _filter(r))
-        return [_set_referer(r) for r in result or ()]
+        return (_set_referer(r) for r in result or ())
-        return [r for r in result or () if _filter(r)]
+        return (r for r in result or () if _filter(r))
-        return [r for r in result or () if _filter(r)]
+        return (r for r in result or () if _filter(r))
-from scrapy.utils.misc import chain_deferred, defer_succeed, mustbe_deferred
+from scrapy.utils.misc import chain_deferred, defer_succeed, mustbe_deferred, deferred_degenerate
-                        log.msg('Garbage found in spider output while processing %s, got type %s' % (request, type(item)), log.TRACE, domain=domain)
+                return deferred_degenerate(result, _ResultContainer())
-            scd.addBoth(lambda _:response)
+
-        return scd.addBoth(_remove)
+        return self.spidermiddleware.scrape(request, response, spider)
-from scrapy.utils.misc import load_class, mustbe_deferred, deferred_degenerate
+from scrapy.utils.misc import load_class, mustbe_deferred
-                    'Middleware %s must returns None, list or tuple, got %s ' % \
+                assert result is None or _isiterable(result), \
-                    'Middleware %s must returns list or tuple, got %s ' % \
+                assert _isiterable(result), \
-                    'Middleware %s must returns None, list or tuple, got %s ' % \
+                assert result is None or _isiterable(result), \
-def deferred_degenerate(generator):
+def deferred_degenerate(generator, container=None, next_delay=0):
-    result = []
+    container = container or []
-            result.append(generator.next())
+            container.append(generator.next())
-            reactor.callLater(0, deferred.callback, result)
+            reactor.callLater(0, deferred.callback, container)
-            reactor.callLater(0, _next)
+            reactor.callLater(next_delay, _next)
-        parser.add_option("--report-dropped", dest="doreport_dropped", action="store_true", help="choose whether to report dropped products or not")
+        parser.add_option("--report-dropped", dest="doreport_dropped", action="store_true", help="generate a report of the dropped products in a text file")
-            self.report = Report(dropped=opts.doreport_dropped)
+        if opts.doreport or opts.doreport_dropped:
-    def __init__(self, dropped):
+    def __init__(self, passed, dropped):
-        dispatcher.connect(self.item_passed, signal=signals.item_passed)
+        if self.passed:
-            product_text = '%sdropping reason: %s\n' % (product_text, dropped)
+            product_text = '%s--- Dropping reason: %s ---\n' % (product_text, dropped)
-          '### Products scraped correctly ###\n', '##################################\n'))
+        if self.passed:
-        self.passed_file.close()
+        if self.passed:
-        product_text = ''
+    def get_product_attribs(self, product):
-            product_text = '%s%s: %s\n' % (product_text, attrib, value)
+            product_attribs = '%s%s: %s\n' % (product_attribs, attrib, value)
-                for subdir in ["spiders", "conf", "commands"]:
+                for subdir in ["spiders", "conf", "commands", "templates"]:
-from decobot.utils.text_extraction import unquote_html
+def unquote_html(s, keep_reserved=False):
-            sys.exit(1)
+        return __import__(modulepath, {}, {}, [''])
-        return __import__(modulepath, {}, {}, [''])
+        try:
-            sys.exit(1)
+        return __import__(modulepath, {}, {}, [''])
-import shelve
+import cPickle as pickle
-        self._opendb(repfile, usedir)
+        #self._opendb(repfile, usedir)
-        self._closedb()
+        self._save()
-    def _opendb(self, repfile, usedir):
+
-            #self.responses_old.update(shelve.open(self.responses_path))
+            with open(self.options_path, 'r') as f:
-    def _closedb(self):
+            with open(self.options_path, 'r') as f:
-            self._persistdb(self.responses_old, self.responses_path)
+            with open(self.options_path, 'w') as f:
-
+    publish = models.BooleanField(_("publish"), core=True, default=False)
-    
+
-    
+
-        list_filter = ("main", "created")
+        list_display = ("title", "main", "position_link", "publish_link",
-        articles = Article.objects.all()
+        articles = Article.objects.filter(publish=True)
-               (self.public and _("Yes") or _("No"), self.id )
+        img_url = "/media/img/admin/icon-%s.gif" % \
-               " | <a href='/article/%(id)s/position/down/'>Down</a>)") % \
+        return _("%(position)s (<a href='/admin/article/article/%(id)s/position/up/'>Up</a>" \
-
+@staff_member_required
-                 'id': self.id }
+        return _("%s (<a href='%s/toggle/'>toggle</a>)") % \
-    (r"^(?P<link_id>\d+)/public/toggle/$", toggle_public),
+    (r"^(?P<link_id>\d+)/toggle/$", toggle_public),
-    (r"^download/", include("scrapyorg.download.urls")),
+    # admin
-    (r"^link/", include("scrapyorg.link.urls")),
+def get_url(url):
-        response = Response(domain=domain, url=url, original_url=original_url, headers=headers, status=status, body=responsebody, parent=parent)
+        response = Response(domain=domain, url=url, original_url=original_url, headers=headers, status=status, body=responsebody)
-            parentkey = urlkey(response.parent) if response.parent else None
+            parentkey = urlkey(response.request.headers.get('referer')) if response.request else None
-        r = Response(domain=spider.domain_name, url=request.url, headers=headers, status=status, body=body, parent=parent)
+        r = Response(domain=spider.domain_name, url=request.url, headers=headers, status=status, body=body)
-                log.msg("Crawled %s <%s> from <%s>" % (cached, response.url, response.parent), level=log.DEBUG, domain=domain)
+                log.msg("Crawled %s <%s> from <%s>" % (cached, response.url, request.headers.get('referer')), level=log.DEBUG, domain=domain)
-    def __init__(self, domain, url, original_url=None, headers=None, status=200, body=None, parent=None):
+    def __init__(self, domain, url, original_url=None, headers=None, status=200, body=None):
-                (repr(self.domain), repr(self.url), repr(self.original_url), repr(self.headers), repr(self.status), repr(self.body), repr(self.parent))
+        return "Response(domain=%s, url=%s, original_url=%s, headers=%s, status=%s, body=%s)" % \
-                           parent=kw.get('parent', self.parent))
+                           body=kw.get('body', samebody()))
-    def remote_update(self, status):
+    def remote_update(self, status, domain, domain_status):
-        self.statistics = {"domains": {}, "scraped_total": 0, "start_time": datetime.datetime.utcnow()}
+        self.start_time = datetime.datetime.utcnow()
-        factory = pb.PBClientFactory()
+        factory = ScrapyPBClientFactory(self, name)
-
+        
-        self.procman.update_master()
+        self.procman.update_master(self.domain, "running")
-    def processEnded(self, status_object):
+    def processEnded(self, reason):
-        self.procman.update_master()
+        self.procman.update_master(self.domain, "scraped")
-    def update_master(self):
+    def update_master(self, domain, domain_status):
-            deferred = self.__master.callRemote("update", self.status())
+            deferred = self.__master.callRemote("update", self.status(), domain, domain_status)
-CACHE2_EXPIRATION_SECS = 48 * 60 * 60 # seconds while cached response is still valid
+CACHE2_EXPIRATION_SECS = 48 * 60 * 60 # seconds while cached response is still valid (a negative value means "never expires")
-        response = self.cache.retrieve_response(domain, key)
+        try:
-                return True
+            expiration_secs = settings.getint('CACHE2_EXPIRATION_SECS')
-                return False
+                # disabled cache expiration
-    def __init__(self, remote, status, name, master):
+class Node(pb.Referenceable):
-        self._set_status(status)
+        self.alive = False
-
+        try:
-        
+    def remote_update(self, status):
-class ClusterMaster(pb.Root):
+class ClusterMaster:
-
+        """Loads nodes listed in CLUSTER_MASTER_NODES setting"""
-
+        """Creates the remote reference for each worker node"""
-        node.update_status()
+        node = Node(cworker, name, self)
-            pd = self.find_ifpending(domain)
+            pd = self.find_inpending(domain)
-    def find_ifpending(self, domain):
+    def find_inpending(self, domain):
-        self.procman.statistics["domains"][self.domain] = {"status": "running", "last_start_time": self.start_time}
+        self.procman.update_master()
-        self.procman.statistics["scraped_total"] += 1
+        self.procman.update_master()
-        self.statistics = {"domains": {}, "scraped_total": 0}
+    def status(self, rcode=0, rstring=None):
-                    log.msg("Unable to svn update: %s" % e) 
+                    log.msg("Unable to svn update: %s" % e, level=log.WARNING)
-    parser.add_option("--port", dest="port", type="int", help="Cluster master port. Default: 8080.", default=8080)
+    parser.add_option("--port", dest="port", type="int", help="Cluster master port. Default: 8060.", default=8060)
-                logfile = os.path.join(self.logdir, time.strftime("%F"), domain, time.strftime("%FT%T.log"))
+                logfile = os.path.join(self.logdir, domain, time.strftime("%FT%T.log"))
-                    pass
+                    r = c.update(settings.get("CLUSTER_WORKER_SVNWORKDIR", "."))
-    file = open(logfile, 'w') if logfile else sys.stderr
+    file = open(logfile, 'a') if logfile else sys.stderr
-        self.statistics = {"domains": {}, "scraped_total": 0, "start_time": datetime.datetime.utcnow(), "pending": self.print_pending() }
+        self.statistics = {"domains": {}, "scraped_total": 0, "start_time": datetime.datetime.utcnow()}
-            with open(pickled_meta, 'rb') as f:
+        requestpath = self.requestpath(domain, key)
-        with open(os.path.join(requestpath, 'pickled_meta'), 'rb') as f:
+        with open(os.path.join(requestpath, 'pickled_meta'), 'r') as f:
-            pickle.dump(metadata, f, -1)
+        with open(os.path.join(requestpath, 'pickled_meta'), 'w') as f:
-import sys
+import sys, datetime
-    def get_status(self):
+    def update_status(self):
-        
+        self.statistics = {"domains": {}, "scraped_total": 0, "start_time": datetime.datetime.utcnow(), "pending": self.print_pending() }
-                self.nodes[name].get_status()
+                self.nodes[name].update_status()
-        node.get_status()
+        node.update_status()
-            
+        if "statistics" in args:
-        self.starttime = time.time()
+        self.starttime = datetime.datetime.utcnow()
-        status["timestamp"] = time.time()
+        status["timestamp"] = datetime.datetime.utcnow()
-import cPickle
+import cPickle as pickle
-                metadata = cPickle.load(f)
+                metadata = pickle.load(f)
-            metadata = cPickle.load(f)
+            metadata = pickle.load(f)
-            cPickle.dump(metadata, f, -1)
+            pickle.dump(metadata, f, -1)
-            with open(pickled_meta) as f:
+            with open(pickled_meta, 'rb') as f:
-        with open(os.path.join(requestpath, 'pickled_meta')) as f:
+        with open(os.path.join(requestpath, 'pickled_meta'), 'rb') as f:
-        if os.path.exists(requestpath):
+        if os.path.exists(pickled_meta):
-        if not os.path.exists(requestpath):
+        if not self.is_cached(domain, key):
-        return "<%s (%s) xpath=%s>" % (type(self).__name__, getattr(self.xmlNode, 'name'), self.expr)
+        return "<%s (%s) xpath=%s>" % (type(self).__name__, getattr(self.xmlNode, 'name', type(self.xmlNode).__name__), self.expr)
-        requestpath = self.requestpath(domain, key)
+        pickled_meta = os.path.join(self.requestpath(domain, key), 'pickled_meta')
-            if datetime.datetime.now() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):
+            with open(pickled_meta) as f:
-                log.msg('dropping old cached response from %s' % metadata['timestamp'], log.INFO)
+                log.msg('dropping old cached response from %s' % metadata['timestamp'])
-            metadata = f.read()
+        with open(os.path.join(requestpath, 'pickled_meta')) as f:
-                'timestamp': datetime.datetime.now(),
+                'timestamp': datetime.datetime.utcnow(),
-import sha
+import hashlib
-        sector = sha.sha(domain).hexdigest()[0]
+        sector = hashlib.sha1(domain).hexdigest()[0]
-import sha
+import hashlib
-    return sha.new(value).hexdigest() if value else None
+    return hashlib.sha1(value).hexdigest() if value else None
-import sha
+import hashlib
-        hash_ = sha.new()
+        hash_ = hashlib.sha1()
-from sha import sha
+import hashlib
-    h = sha()
+    h = hashlib.sha1()
-from hashlib import sha1
+import hashlib
-        fp = sha1()
+        fp = hashlib.sha1()
-import sha
+import hashlib
-            self._version = sha.new(self.body.to_string()).hexdigest()
+            self._version = hashlib.sha1(self.body.to_string()).hexdigest()
-                self.master.loading.append(pending['domain'])
+            #load domains by one, so to mix up better the domain loading between nodes. The next one in the same node will be loaded
-            self.master.loading.remove(pending['domain'])
+                    self.pending.remove(pd)
-            return datetime.datetime.now() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS'))
+            if datetime.datetime.now() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):
-        return os.path.exists(requestpath)
+        if os.path.exists(requestpath):
-from sha import sha
+from hashlib import sha1
-        fp = sha(canonicalize(self.url))
+        fp = sha1()
-            spiders.default_domain = opts.spider
+            spiders.default_domain = opts.default_spider
-        parser.add_option("--spider", dest="spider", default=None, help="Force to use the given spider")
+        parser.add_option("--spider", dest="spider", default=None, help="Force using the given spider when the arguments are urls")
-        parser.add_option("--spider", dest="spider", default=None, help="default spider (domain) to use if no spider is found")
+        parser.add_option("--default-spider", dest="default_spider", default=None, help="default spider (domain) to use if no spider is found")
-        if opts.spider:
+        if opts.default_spider:
-    main()
+#!/usr/bin/env python
-    def status_as_dict(self):
+    def status_as_dict(self, verbosity=0):
-            status["running"] = self.running
+            if verbosity == 0:
-            nodes_status[d] = n.status_as_dict
+            nodes_status[d] = n.status_as_dict(int(verbosity))
-        status["pending"] = self.pending
+        status["pending"] = self.print_pending(int(verbosity))
-                logfile = os.path.join(self.logdir, domain, time.strftime("%FT%T.log"))
+                logfile = os.path.join(self.logdir, time.strftime("%F"), domain, time.strftime("%FT%T.log"))
-import pickle, socket
+import pickle
-from twisted.python import util
+        self.available = True
-            self.available = False
+            self.alive = False
-            self.available = True
+            self.alive = True
-                    self.master.loading.append(pending['domain'])
+        #load domains by one, so to mix up better the domain loading between nodes. The next one in the same node will be loaded
-            deferred.addCallbacks(callback=_run_callback, errback=lambda reason:log.msg(reason, log.ERROR))
+            deferred.addCallbacks(callback=_run_callback, errback=_run_errback)
-                _make_callback(factory, name, url)
+        server, port = url.split(":")
-            node.get_status()
+        for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():
-
+        if "disable_node" in args:
-            to_reschedule = []
+            #load domains by one, so to mix up better the domain loading between nodes. The next one in the same node will be loaded
-            while free_slots > 0 and self.master.pending:
+            if free_slots > 0 and self.master.pending:
-                    to_reschedule.append(pending)
+                    self.master.schedule([pending['domain']], pending['settings'], pending['priority'])
-                #slots are complete. Reschedule in master with priority reduced by one. This is a security issue because offen happens that the slots were completed and not yet notified because of the asynchronous response from worker.
+                #slots are complete. Reschedule in master with priority reduced by one.
-                #because the first check in self.master.loading should avoid to reach this point.
+                #domain already running in node. Reschedule with same priority.
-                self._set_status(status)
+            self._set_status(status)
-                free_slots -= 1
+                #if domain already running in some node, reschedule with same priority (so will be moved to run later)
-            self._set_status(status)
+            elif status['callresponse'][0] == 2:
-
+        self.loading = []
-            self.pending.insert(i, {'domain': domain, 'settings': final_spider_settings, 'priority': priority})
+            pd = self.find_ifpending(domain)
-                self.running[domain] = scrapy_proc
+                self.running[domain] = scrapy_proc
-                    c=pysvn.Client()
+                    c = pysvn.Client()
-            return self.status(0, "Started process %s." % scrapy_proc)
+            if not domain in self.running:
-        pickled_settings = os.environ.get("SCRAPY_PICKLED_SETTINGS")
+        pickled_settings = os.environ.get("SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE")
-        self.env["SCRAPY_PICKLED_SETTINGS"] = pickled_settings
+        self.env["SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE"] = pickled_settings
-        self.overrides = {}
+        pickled_settings = os.environ.get("SCRAPY_PICKLED_SETTINGS")
-        self.defaults = pickle.loads(pickled_defaults) if pickled_defaults else {}
+        self.defaults = {}
-
+        text = obj.body.to_unicode()
-        nodetext = match.group().decode(enc)
+        nodetext = match.group()
-            except NotConfigured:
+            except NotConfigured, e:
-                pass
+                if e.args:
-            raise NotConfigured("Requires SCRAPING_DB setting")
+            raise NotConfigured("SpiderStats: Requires SCRAPING_DB setting")
-        log.msg("PYTHON_PATH: %s" % repr(sys.path))
+        log.msg("PYTHONPATH: %s" % repr(sys.path))
-        self.logdir = settings['CLUSTER_WORKER_LOGDIR']
+        self.logdir = settings['CLUSTER_LOGDIR']
-                self.master.schedule([pending['domain']], pending['settings'], PRIORITY_NOW)
+                #slots are complete. Reschedule in master with priority reduced by one. This is a security issue because offen happens that the slots were completed and not yet notified because of the asynchronous response from worker.
-class ClusterMaster(object):
+class ClusterMaster(pb.Root):
-        """Loads nodes from the CLUSTER_MASTER_NODES setting"""
+        if name not in self.nodes:
-                    _make_callback(factory, name, url)
+    def remote_connect(self, name, url):
-import datetime
+import sys, os, time, datetime, pickle
-            self.env["SCRAPY_%s" % k] = str(self.scrapy_settings[k])
+        pickled_settings = pickle.dumps(self.scrapy_settings)
-        pickled_defaults = os.environ.get("SCRAPY_PICKLED_DEFAULT_SETTINGS")
+        pickled_defaults = os.environ.get("SCRAPY_PICKLED_SETTINGS")
-import os
+import os, pickle
-        self.defaults = {}
+        pickled_defaults = os.environ.get("SCRAPY_PICKLED_DEFAULT_SETTINGS")
-                log.msg("Crawled <%s> from <%s>" % (response.url, response.parent), level=log.DEBUG, domain=domain)
+                cached = 'cached' if response.cached else 'live'
-                self.master.schedule(pending['domain'], pending['settings'], pending['priority'])
+                self.master.schedule([pending['domain']], pending['settings'], PRIORITY_NOW)
-            self.pending = pickle.load( open("pending_cache_%s" % socket.gethostname(), "r") )
+            self.pending = pickle.load( open(settings["CLUSTER_MASTER_CACHEFILE"], "r") )
-        pickle.dump( self.pending, open("pending_cache_%s" % socket.gethostname(), "w") )
+        pickle.dump( self.pending, open(settings["CLUSTER_MASTER_CACHEFILE"], "w") )
-        self.env["PYTHONPATH"] = ":".join(sys.path)
+        self.env["PYTHONPATH"] = ":".join(sys.path)#this is need so this crawl process knows where to locate local_scrapy_settings.
-                r = c.update(settings["SVN_WORKDIR"] or ".")
+                r = c.update(settings["CLUSTER_WORKER_SVNWORKDIR"] or ".")
-        scrapyengine.start()
+        scrapyengine.start()# blocking call
-            reactor.stop()
+            if reactor.running:
-        self.env["PYTHONPATH"]=":".join(sys.path)
+        self.env["PYTHONPATH"] = ":".join(sys.path)
-            self.running[domain] = scrapy_proc
+        log.msg("Response from worker: %s" % status)
-
+import os
-
+        parser.add_option("--pidfile", dest="pidfile", help="Write process pid to file FILE", metavar="FILE")
-                self.master.schedule(pending['domain'], pending['settings'], pending['priority'], pending["env"])
+                self.master.schedule(pending['domain'], pending['settings'], pending['priority'])
-            deferred = self.__remote.callRemote("run", pending["domain"], pending["settings"], pending["env"])
+            deferred = self.__remote.callRemote("run", pending["domain"], pending["settings"])
-        if not settings.getbool('CLUSTER_MASTER_ENABLED'):
+
-    def schedule(self, domains, spider_settings=None, env=None, priority=PRIORITY_NORMAL):
+    def schedule(self, domains, spider_settings=None, priority=PRIORITY_NORMAL):
-            self.pending.insert(i, {'domain': domain, 'settings': spider_settings, 'env': env, 'priority': priority})
+            final_spider_settings = self.get_spider_groupsettings(domain)
-            self.schedule(domains, spider_settings, env, priority)
+
-        s += "<textarea name='settings' rows='6'>\n"
+        s += "Overrided spider settings:<br />\n"
-    def __init__(self, procman, domain, logfile=None, spider_settings=None, env=None):
+    def __init__(self, procman, domain, logfile=None, spider_settings=None):
-        self.env = self.env_original.copy()
+        self.env = {}
-            self.env["SCRAPY_%s" % k] = self.scrapy_settings[k]
+            self.env["SCRAPY_%s" % k] = str(self.scrapy_settings[k])
-        return {"domain": self.domain, "pid": self.pid, "status": self.status, "settings": self.scrapy_settings, "logfile": self.logfile, "starttime": self.start_time, "env": self.env_original}
+        return {"domain": self.domain, "pid": self.pid, "status": self.status, "settings": self.scrapy_settings, "logfile": self.logfile, "starttime": self.start_time}
-    def remote_run(self, domain, spider_settings=None, env=None):
+    def remote_run(self, domain, spider_settings=None):
-            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings, env)
+            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)
-
+import pickle, socket
-
+        dispatcher.connect(self._engine_stopped, signal=signals.engine_stopped)
-                self.master.schedule(pending['domain'], pending['settings'], pending['priority'])
+                self.master.schedule(pending['domain'], pending['settings'], pending['priority'], pending["env"])
-            deferred = self.__remote.callRemote("run", pending["domain"], pending["settings"])
+            deferred = self.__remote.callRemote("run", pending["domain"], pending["settings"], pending["env"])
-    def schedule(self, domains, spider_settings=None, priority=PRIORITY_NORMAL):
+    def schedule(self, domains, spider_settings=None, env=None, priority=PRIORITY_NORMAL):
-            self.pending.insert(i, {'domain': domain, 'settings': spider_settings, 'priority': priority})
+            self.pending.insert(i, {'domain': domain, 'settings': spider_settings, 'env': env, 'priority': priority})
-            self.schedule(domains, spider_settings, priority)
+            
-    def __init__(self, procman, domain, logfile=None, spider_settings=None):
+    def __init__(self, procman, domain, logfile=None, spider_settings=None, env=None):
-        env = {'SCRAPY_LOGFILE': self.logfile, 'SCRAPY_CLUSTER_WORKER_ENABLED': '0', 'SCRAPY_WEBCONSOLE_ENABLED': '0'}
+        self.env_original = env or {}
-        self.env = env
+        self.scrapy_settings = spider_settings or {}
-        return {"domain": self.domain, "pid": self.pid, "status": self.status, "settings": self.settings, "logfile": self.logfile, "starttime": self.start_time}
+        return {"domain": self.domain, "pid": self.pid, "status": self.status, "settings": self.scrapy_settings, "logfile": self.logfile, "starttime": self.start_time, "env": self.env_original}
-    def remote_run(self, domain, spider_settings=None):
+    def remote_run(self, domain, spider_settings=None, env=None):
-            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings)
+            scrapy_proc = ScrapyProcessProtocol(self, domain, logfile, spider_settings, env)
-from scrapy.contrib.pbcluster.master.manager import ClusterMaster, priorities
+from scrapy.contrib.pbcluster.master.manager import *
-    def webconsole_control(self, wc_request):
+    def webconsole_control(self, wc_request, ws=False):
-
+            if ws:
-            self.schedule(args["schedule"], priority=eval(args["priority"][0]))
+            if ws:
-            self.stop(args["stop"])
+            if ws:
-            self.remove(args["remove"])
+            if ws:
-        return ""
+        if ws:
-        s += "</br>\n"
+        s += "<br />\n"
-            assert isinstance(x, XPathSelector)
+            assert isinstance(x, HtmlXPathSelector)
-                return XPathSelectorList([XPathSelector(node=node, parent=self, expr=xpath) for node in xpath_result])
+                return XPathSelectorList([cls(node=node, parent=self, expr=xpath) for node in xpath_result])
-                return XPathSelectorList([XPathSelector(node=xpath_result, parent=self, expr=xpath)])
+                return XPathSelectorList([cls(node=xpath_result, parent=self, expr=xpath)])
-        return "<XPathSelector (%s) xpath=%s>" % (getattr(self.xmlNode, 'name'), self.expr)
+        return "<%s (%s) xpath=%s>" % (type(self).__name__, getattr(self.xmlNode, 'name'), self.expr)
-        XPathSelector.__init__(self, response=response, text=text, constructor=xmlDoc_from_xml)
+    def __init__(self, *args, **kwargs):
-        XPathSelector.__init__(self, response=response, text=text, constructor=xmlDoc_from_html)
+    def __init__(self, *args, **kwargs):
-
+import re
-    def __init__(self, remote, status, name):
+    def __init__(self, remote, status, name, master):
-            self.pending = status['pending']
+            self.closing = status['closing']
-            self.lastcallresponse = status['callresponse']
+            print "Running: %s" % self.running
-    def _remote_call(self, function, *args):
+    def get_status(self):
-            deferred = self.__remote.callRemote(function, *args)
+            deferred = self.__remote.callRemote("status")
-        self._remote_call("status")
+    def stop(self, domain):
-        self._remote_call("schedule", domains, spider_settings, priority)
+    def run(self, pending):
-        self._remote_call("stop", domains)
+        def _run_callback(status):
-        self._remote_call("remove", domains)
+        try:
-        self.queue = []
+        self.pending = []
-        
+
-                    
+
-        node = Node(cworker, None, name)
+        node = Node(cworker, None, name, self)
-            self._dispatch_domains(domains, spider_settings, priority)
+    def schedule(self, domains, spider_settings=None, priority=PRIORITY_NORMAL):
-            self.nodes[nodename].stop(domains)
+            for domain in domains:
-                to_remove[node.name].append(domain)
+        """Remove all scheduled instances of the given domains (if it hasn't
-            self.nodes[nodename].remove(domains)
+        for domain in domains:
-        s += "<tr><th>&nbsp;</th><th>Name</th><th>Available</th><th>Running</th><th>Pending</th><th>Load.avg</th></tr>\n"
+        s += "<tr><th>&nbsp;</th><th>Name</th><th>Available</th><th>Running</th><th>Load.avg</th></tr>\n"
-                 (chkbox, nodelink, node.available, len(node.running), node.maxproc, len(node.pending), loadavg)
+            s += "<tr><td>%s</td><td>%s</td><td>%s</td><td>%d/%d</td><td>%s</td></tr>\n" % \
-            self.schedule(args["schedule"], nodename=node, priority=eval(args["priority"][0]))
+            self.schedule(args["schedule"], priority=eval(args["priority"][0]))
-        inactive_domains = enabled_domains - set(self.running.keys() + self.pending.keys())
+        inactive_domains = enabled_domains - set(self.running.keys() + [p['domain'] for p in self.pending])
-        s += self._domains_table(self.pending, 'pending')
+        # pending domains
-        return self.status(responses)
+    def remote_stop(self, domain):
-    def status(self, response="Status Response"):
+    def status(self, rcode=0, rstring=None):
-        status["callresponse"] = response
+        status["callresponse"] = (rcode, rstring) if rstring else (0, "Status Response.")
-        self.running[domain] = scrapy_proc
+    def remote_run(self, domain, spider_settings=None):
-    d.addCallback(lambda object: object.callRemote("stop", [sys.argv[1]]))
+    d.addCallback(lambda object: object.callRemote("stop", sys.argv[1]))
-    d.addCallback(lambda object: object.callRemote("remove", [sys.argv[1]]))
+    d.addCallback(lambda object: object.callRemote("run", sys.argv[1]))
-from scrapy.xpath import XPathSelector
+from scrapy.xpath import XmlXPathSelector, HtmlXPathSelector
-    vars['xs'] = XPathSelector(response)
+    vars['xxs'] = XmlXPathSelector(response)
-from scrapy.xpath.selector import XPathSelector
+from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector
-from scrapy.xpath.constructors import xmlDoc_from_xml
+from scrapy.xpath.selector import XPathSelector, XmlXPathSelector, HtmlXPathSelector
-        xpath = XPathSelector(response)
+        xpath = HtmlXPathSelector(response)
-        x = XPathSelector(response)
+        x = HtmlXPathSelector(response)
-        x = XPathSelector(response)
+        x = HtmlXPathSelector(response)
-        x = XPathSelector(response, constructor=xmlDoc_from_xml)
+        x = XmlXPathSelector(response)
-        x = XPathSelector(response, constructor=xmlDoc_from_xml)
+        x = XmlXPathSelector(response)
-        x = XPathSelector(response)
+        x = HtmlXPathSelector(response)
-
+    try:
-from scrapy.xpath.selector import XPathSelector
+from scrapy.xpath.constructors import xml_parser_options
-            selector = XPathSelector(text=string, constructor=xmlDoc_from_xml).x('/' + self.requested_nodename)[0]
+            selector = XmlXPathSelector(text=string).x('/' + self.requested_nodename)[0]
-from scrapy.xpath.constructors import xmlDoc_from_html
+from scrapy.xpath.constructors import xmlDoc_from_html, xmlDoc_from_xml
-    Usage example (untested code):
+    """The XPathSelector class provides a convenient way for selecting document
-    i.assign("features", x.x("//div[@class='features']).x("./span/text()")
+    - XmlXPathSelector (for XML content)
-
+        """Perform the given XPath query on the current XPathSelector and
-    def extract(self, **kwargs): 
+    def extract(self):
-            if isinstance(self.xmlNode, libxml2.xmlAttr):
+        elif hasattr(self.xmlNode, 'serialize'):
-        return [x.extract(**kwargs) if isinstance(x, XPathSelector) else x for x in self]
+    """List of XPathSelector objects"""
-    unittest.main()
+"""
-from lib.templatetags import *
+from scrapyorg.lib.templatetags import *
-from article.models import Article
+from scrapyorg.article.models import Article
-from article.views import *
+from scrapyorg.article.views import *
-from article.models import Article
+from scrapyorg.article.models import Article
-from blog.models import Entry
+from scrapyorg.blog.models import Entry
-from models import Entry # relative import
+from scrapyorg.blog.models import Entry # relative import
-from lib.templatetags import *
+from scrapyorg.lib.templatetags import *
-from download.models import DownloadLink
+from scrapyorg.download.models import DownloadLink
-from download.views import *
+from scrapyorg.download.views import *
-from download.models import DownloadLink
+from scrapyorg.download.models import DownloadLink
-from lib.templatetags import *
+from scrapyorg.lib.templatetags import *
-from link.models import GroupLink
+from scrapyorg.link.models import GroupLink
-from link.views import *
+from scrapyorg.link.views import *
-from link.models import GroupLink
+from scrapyorg.link.models import GroupLink
-import sys
+#from os.path import abspath, dirname, basename, join
-PROJECT_NAME = basename(PROJECT_ABSOLUTE_DIR)
+PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
-MEDIA_ROOT = join(PROJECT_ABSOLUTE_DIR, "static")
+MEDIA_ROOT = os.path.join(PROJECT_ROOT, "static")
-ROOT_URLCONF = '%s.urls' % PROJECT_NAME
+ROOT_URLCONF = 'scrapyorg.urls'
-    join(PROJECT_ABSOLUTE_DIR, "templates"),
+    os.path.join(PROJECT_ROOT, "templates"),
-    'blog',
+    'scrapyorg.link',
-    (r"^weblog/", include("blog.urls")),
+    (r"^article/", include("scrapyorg.article.urls")),
-        (r"^site-media/(?P<path>.*)$", "django.views.static.serve", { "document_root": settings.MEDIA_ROOT }),
+        (r'^%s/(?P<path>.*)$' % settings.MEDIA_URL[1:], 'django.views.static.serve', {'document_root': settings.MEDIA_ROOT}),
-    order = models.IntegerField(_("order"), core=True, blank=False, default=0)
+    position = models.IntegerField(_("position"), core=True, blank=False,
-        self.order += 1
+    def position_up(self):
-        self.order -= 1
+    def position_down(self):
-    order_link.allow_tags = True
+    def position_link(self):
-        list_display = ("title", "main", "order_link", "updated")
+        list_display = ("title", "main", "position_link", "updated")
-        ordering = [ "-order", ]
+        ordering = [ "-position", ]
-    (r"^(?P<article_id>\d+)/order/down/$", order_down),
+    (r"^(?P<article_id>\d+)/position/up/$", position_up),
-def order_up(request, article_id):
+def position_up(request, article_id):
-    article.order_up()
+    article.position_up()
-def order_down(request, article_id):
+def position_down(request, article_id):
-    article.order_down()
+    article.position_down()
-from django_website.apps.blog.models import Entry
+from django import template
-        list_filter = ("group", )
+
-from link.models import Group
+from link.models import GroupLink
-        context[self.var_name] = group and group.link_set.all() or []
+        context[self.var_name] = GroupLink.objects.filter(group__slug=self.slug)
-# Create your views here.
+from django.shortcuts import get_object_or_404
