-        assert_almost_equal(y[0].asnumpy()[0,0,0], 0.3376348)
+        assert_almost_equal(y[0].asnumpy()[0, 0, 0], 0.3376348)
-    def test_dot_determinism(lhs_stype, rhs_stype, lhs_density, rhs_density, transpose_a, transpose_b):
+    def test_dot_determinism(lhs_stype, rhs_stype, lhs_density, rhs_density, transpose_a, transpose_b, forward_stype):
-    test_dot_determinism('default', 'csr', 1.0, 0.1, False, True)
+
-        return list(self._children.values())[key]
+        layers = list(self._children.values())[key]
-        return list(self._children.values())[key]
+        layers = list(self._children.values())[key]
-        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, int(shape[0] / self._gates))
+        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0] // self._gates)
-        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0] / self._gates)
+        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, int(shape[0] / self._gates))
-            assert_almost_equal(out / max_val, gpu_out.asnumpy() / max_val, rtol=1e-3, atol=1e-3)
+        # Run inference.
-        assert_almost_equal(out / max_val, gpu_out.asnumpy() / max_val, rtol=1e-3, atol=1e-3)
+        for i in range(5):
-MobileNetV2, implemented in Gluon.
+MobileNetV2, implemented in built-in symbols.
-__date__ = '18/1/31'
+__author__ = 'liangfu'
-from mxnet.gluon.model_zoo.vision.mobilenet import MobileNetV2
+def relu6(data, prefix):
-__all__ = ['MobileNetV2', 'get_symbol']
+def mobilenet_unit(data, num_filter=1, kernel=(1, 1), stride=(1, 1), pad=(0, 0), num_group=1, if_act=True, prefix=''):
-    <https://arxiv.org/abs/1801.04381>`_ paper.
+    channel_expand = mobilenet_unit(
-    net.hybridize()
+def inverted_residual_blocks(data, in_c, t, c, n, s, prefix):
-    return sym
+    last_residual_block = first_block
-    sym = get_symbol(1000, prefix='mob_')
+MNETV2_CONFIGS_MAP = {
-                        node_attrs={'shape': 'oval', 'fixedsize': 'fasl==false'}).view()
+class MobileNetV2(object):
-    plot_net()
+        internals = sym.get_internals()
-            len(params),
+            len(param_names),
-            atol = 1e-4
+            atol = 1e-3
-        check_symbolic_backward(y, [xa], [np.ones(shape)], [grad_xa], rtol=rtol, atol=atol)
+        if dtype is not np.float16:
-    assert_almost_equal(out1.asnumpy(), out3.asnumpy(), rtol=1e-3)
+    assert_almost_equal(out1.asnumpy(), out2.asnumpy(), rtol=1e-3, atol=1e-3)
-        out_dns = mx.nd.dot(lhs_nd, rhs_dns, transpose_a=trans_lhs, transpose_b=trans_rhs)
+        if default_context() == mx.cpu():
-        out = mx.symbol.sparse.dot(lhs, rhs, transpose_a=trans_lhs, transpose_b=trans_rhs)
+        out = mx.symbol.sparse.dot(lhs, rhs, transpose_a=trans_lhs, transpose_b=trans_rhs, forward_stype=forward_stype)
-                                rtol=1e-3, atol=1e-4)
+        if default_context() == mx.cpu():
-    density = [1.00, 0.50, 0.01]
+    density = [1.00, 0.5, 0.01]
-
+            test_infer_forward_stype(lhs_shape, (lhs_shape[1], rnd.randint(10, 20)),
-
+    kernel = new_attrs['kernel']
-    """Compute N-D convolution on (N+2)-D input."""
+    """Computes transposed convolution of the input tensor."""
-
+    kernel = new_attrs['kernel']
-    assert len(attrs_keys) == 19
+    assert len(attrs_keys) == 23
-                                  'reshape5', '8']):
+                                  'reshape5', '8', 'pad1', 'pad0', 'pad3',
-# Reaching timeout causes a test failure
+# Reaching timeout causes test failure
-    >>> print(dx.grad)
+    >>> print(dx)
-# Maximum 7 minutes per test
+# Maximum 10 minutes per test
-TIME_OUT = 7*60
+TIME_OUT = 10*60
-    def __init__(self, sym, flags=()):
+    def __init__(self, sym, flags=(), inputs=None, params=None):
-            assert name in params or name in input_idx, \
+        input_names = [i.name for i in inputs]
-    def _finish_deferred_init(self, hybrid, *args):
+
-                        " cannot be inferred \n {}".format(e)
+                        " cannot be inferred. {}".format(e)
-        out = self._cached_op(*cargs)
+        out = self._cached_op(*args)
-        self._cached_op_args = None
+                if self._active:
-                    self._finish_deferred_init(self._active, x, *args)
+                    self._deferred_infer_shape(x, *args)
-
+# Licensed to the Apache Software Foundation (ASF) under one
-        >>> x = mx.nd.ones('row_sparse', (3,3)).tostype('csr')
+        >>> x = mx.nd.ones((3,3)).tostype('csr')
-from __future__ import print_function
+import logging
-        self.mod = mx.mod.Module(symbol, label_names=None, context=ctx)
+        self.mod = mx.mod.Module(symbol, label_names=None, context=self.ctx)
-            print("Detection time for {} images: {:.4f} sec".format(
+            logging.info("Detection time for {} images: {:.4f} sec".format(
-            result.append(res)
+        result = Detector.filter_positive_detections(detections)
-                                    bbox=dict(facecolor=colors[cls_id], alpha=0.5),
+        for det in dets:
-            img[:, :, (0, 1, 2)] = img[:, :, (2, 1, 0)]
+            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
-import argparse
+import mxnet as mx
-
+
-from collections import Counter
+import re
-from collections import Counter
+import re
-        self.aux_params = {k : nd.zeros(s) for k, s in zip(aux_names, aux_shapes)}
+        self.aux_params = {k: mx.nd.zeros(s) for k, s in zip(aux_names, aux_shapes)}
-    
+
-import numpy as np
+import os
-from tqdm import tqdm
+
-        vec = [BOS] if add_bos else []
+        vec = [Vocab.BOS] if add_bos else []
-            vec.append(EOS)
+            vec.append(Vocab.EOS)
-import cv2
+
-if is_py3:
+import time
-else:
+except ImportError:
-            except Empty:
+            except queue.Empty:
-            if isinstance(value, nd.NDArray):
+            if isinstance(value, ndarray.NDArray):
-                 weight_initializer=None, bias_initializer='zeros',
+                 dtype='float32', weight_initializer=None, bias_initializer='zeros',
-                                          init=weight_initializer,
+                                          init=weight_initializer, dtype=dtype,
-                                            init=bias_initializer,
+                                            init=bias_initializer, dtype=dtype,
-                                      init=weight_initializer,
+                                      init=weight_initializer, dtype=dtype,
-    assert 'numpy.float32' in lines[1]
+    assert 'float32' in lines[1]
-            'composite', output_names=output_names, label_names=label_names)
+            name, output_names=output_names, label_names=label_names)
-                    assert_almost_equal(outputs[0].asnumpy(), c_npy, 
+                    assert_almost_equal(outputs[0].asnumpy(), c_npy,
-                    assert_almost_equal(exe.grad_dict['a'].asnumpy(), agrad_npy, 
+                    assert_almost_equal(exe.grad_dict['a'].asnumpy(), agrad_npy,
-                    assert_almost_equal(exe.grad_dict['b'].asnumpy(), bgrad_npy, 
+                    assert_almost_equal(exe.grad_dict['b'].asnumpy(), bgrad_npy,
-def container_run(platform: str, docker_binary: str, command: List[str], dry_run: bool = False, into_container: bool = False) -> str:
+def container_run(platform: str,
-    runlist = [docker_binary, 'run', '--rm',
+    runlist = [docker_binary, 'run', '--rm', '-t',
-            container_run(platform, docker_binary, command)
+            container_run(platform, docker_binary, shared_memory_size, command)
-            print(container_run(platform, docker_binary, [], True))
+            print(container_run(platform, docker_binary, shared_memory_size, [], True))
-            container_run(platform, docker_binary, [], False, True)
+            container_run(platform, docker_binary, shared_memory_size, [], False, True)
-            container_run(platform, docker_binary, cmd)
+            container_run(platform, docker_binary, shared_memory_size, cmd)
-            container_run(platform, docker_binary, cmd)
+            container_run(platform, docker_binary, shared_memory_size, cmd)
-    app.connect("builder-inited", build_mxnet)
+
-
+# Licensed to the Apache Software Foundation (ASF) under one
-        if lstype == 'default' and rstype == 'default':
+    def elemwise_mul_stype(lstype, rstype):
-                                expected_result_storage_type=most_dense(lhs_stype, rhs_stype),
+                                elemwise_mul_stype(lhs_stype, rhs_stype),
-
+    def __bool__(self):
-from common import models
+from common import assertRaises, models
-                sgd_update(weight, grad, out=weight,
+                sgd_update(weight, grad, out=weight, lazy_update=self.lazy_update,
-    mom_options = [{'momentum': 0.9}]
+    mom_options = [{'momentum': 0.0}, {'momentum': 0.9}]
-        self.data = [np.asarray(i, dtype=dtype) for i in self.data]
+        self.data = [np.asarray(i, dtype=dtype) for i in self.data if i]
-    invalid_label = 0
+    invalid_label = -1
-        len_sentence = randint(1, max(buckets) + 10)
+        len_sentence = randint(1, max(buckets)-1) # leave out the two last buckets empty
-    data_train = DummySentenceIter(train_sent, batch_size, buckets=buckets,
+    data_train = mx.rnn.BucketSentenceIter(train_sent, batch_size, buckets=buckets,
-    data_val = DummySentenceIter(val_sent, batch_size, buckets=buckets,
+    data_val =  mx.rnn.BucketSentenceIter(val_sent, batch_size, buckets=buckets,
-        label = mx.sym.Variable('l2_label')
+        label = mx.sym.Variable('softmax_label')
-        loss = mx.sym.LinearRegressionOutput(pred, label, name='l2_loss')
+        pred = mx.sym.FullyConnected(data=pred, num_hidden=len_vocab, name='pred')
-        return loss, ('data',), ('l2_label',)
+        return loss, ('data',), ('softmax_label',)
-        eval_metric=mx.metric.MSE(),
+        eval_metric=mx.metric.Perplexity(invalid_label), # Use Perplexity for multiclass classification.
-    assert model.score(data_val, mx.metric.MSE())[0][1] < 350, "High mean square error."
+    # This test forecasts random sequence of words to check bucketing.
-    def check_sparse_embedding(in_dim, out_dim, batch, densities, deterministic):
+    def check_sparse_embedding(in_dim, out_dim, batch, densities, deterministic, weight_stype):
-        weight = mx.sym.Variable("embed_weight", stype='row_sparse')
+        weight = mx.sym.Variable("embed_weight", stype=weight_stype)
-            weight[:] = rand_ndarray(weight.shape, 'row_sparse', density=density)
+            weight[:] = rand_ndarray(weight.shape, weight_stype, density=density)
-
+    stypes = ['default', 'row_sparse']
-__modified_date__ = '18/01/31'
+__modified_date__ = '18/04/18'
-def _add_conv(out, channels=1, kernel=1, stride=1, pad=0, num_group=1, active=True):
+def _add_conv(out, channels=1, kernel=1, stride=1, pad=0,
-        out.add(nn.Activation('relu'))
+        out.add(RELU6() if relu6 else nn.Activation('relu'))
-    _add_conv(out, channels=channels)
+def _add_conv_dw(out, dw_channels, channels, stride, relu6=False):
-            _add_conv(self.out, in_channels * t)
+            _add_conv(self.out, in_channels * t, relu6=True)
-            _add_conv(self.out, channels, active=False)
+                      pad=1, num_group=in_channels * t, relu6=True)
-                          stride=2, pad=1)
+                          stride=2, pad=1, relu6=True)
-                strides = [1, 2] * 2 + [1] * 6 + [2, 1, 1] * 2 + [1]
+                strides = [1, 2] * 2 + [1, 1, 2] + [1] * 6 + [2] + [1] * 3
-                _add_conv(self.features, last_channels)
+                _add_conv(self.features, last_channels, relu6=True)
-@unittest.skip("test fails intermittently. temporarily disabled till it gets fixed. tracked at https://github.com/apache/incubator-mxnet/issues/8712")
+parser.add_argument('--tag_default', type=str, default='master', help='Default Tag')
-        url = root_url if i == 0 else root_url + 'versions/%s/index.html' % (tag)
+        url = root_url if tag == args.tag_default else root_url + 'versions/%s/index.html' % (tag)
-def _flatten(args):
+def _flatten(args, inout_str):
-        "but got %s of type %s"%(str(args), str(type(args)))
+        "HybridBlock %s must be (nested) list of Symbol or NDArray, " \
-        arg, fmt = _flatten(i)
+        arg, fmt = _flatten(i, inout_str)
-            args, self._in_format = _flatten(args)
+            args, self._in_format = _flatten(args, "input")
-            out, self._out_format = _flatten(out)
+            out, self._out_format = _flatten(out, "output")
-        args, fmt = _flatten(args)
+        args, fmt = _flatten(args, "input")
-        args, _ = _flatten(args)
+        args, _ = _flatten(args, "input")
-        out, self._out_format = _flatten(outputs)
+        syms, self._in_format = _flatten(inputs, "input")
-        args, in_fmt = _flatten([x] + list(args))
+        args, in_fmt = _flatten([x] + list(args), "input")
-                    "shape incompatible expacted %s vs saved %s"%(
+                    "shape incompatible expected %s vs saved %s"%(
-                "dtype incompatible expacted %s vs saved %s"%(
+                "dtype incompatible expected %s vs saved %s"%(
-        return self._children[str(key)]
+        return list(self._children.values())[key]
-        return self._children[str(key)]
+        return list(self._children.values())[key]
-                    assert_almost_equal(exe.grad_dict['b'].asnumpy(), bgrad_npy, rtol=1e-3)
+                    assert_almost_equal(outputs[0].asnumpy(), c_npy, 
-                        assert_almost_equal(outputs[0].asnumpy(), c_npy, rtol=1e-3, atol=1e-4)
+                        assert_almost_equal(outputs[0].asnumpy(), c_npy,
-                        assert_almost_equal(exe.grad_dict['b'].asnumpy(), bgrad_npy, rtol=1e-3, atol=1e-4)
+                        assert_almost_equal(exe.grad_dict['a'].asnumpy(), agrad_npy,
-                            agrad_npy + a_init_grad_npy, rtol=1e-3, atol=1e-4)
+                                            agrad_npy + a_init_grad_npy,
-                            bgrad_npy + b_init_grad_npy, rtol=1e-3, atol=1e-4)
+                                            bgrad_npy + b_init_grad_npy,
-    if any(not isinstance(sym, Symbol) for sym in symbols):
+    if not symbols or any(not isinstance(sym, Symbol) for sym in symbols):
-    from .gen_iamge import *
+    from .gen_image import *
-            net.collect_params().save('%s.params' % opt.save_model_prefix)
+            net.save_params('%s.params' % opt.save_model_prefix)
-    net.collect_params().initialize(force_reinit=True)
+    net.initialize(force_reinit=True)
-net.collect_params().initialize()
+net.initialize()
-net.collect_params().initialize(mx.init.Xavier(magnitude=2.24),
+net.initialize(mx.init.Xavier(magnitude=2.24),
-    print(para_name, para_value.data().asnumpy()[0])
+    print(para_name, para_value.data().asnumpy()[0])
-model.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=mx.cpu())
+model.initialize(mx.init.Xavier(magnitude=2.24), ctx=mx.cpu())
-        style_model.collect_params().load(args.resume, ctx=ctx)
+        style_model.load_params(args.resume, ctx=ctx)
-                
+
-            
+
-                style_model.collect_params().save(save_model_path)
+                style_model.save_params(save_model_path)
-    style_model.collect_params().save(save_model_path)
+    style_model.save_params(save_model_path)
-    style_model.collect_params().load(args.model, ctx=ctx)
+    style_model.load_params(args.model, ctx=ctx)
-        
+
-        # Training the model 
+        # Training the model
-    net.conv4.collect_params().initialize(mx.init.Orthogonal(scale=1), force_reinit=True, ctx=ctx)
+    net.conv4.initialize(mx.init.Orthogonal(scale=1), force_reinit=True, ctx=ctx)
-            net.collect_params().save('childsum_tree_lstm_{}.params'.format(num_iter))
+            net.save_params('childsum_tree_lstm_{}.params'.format(num_iter))
-    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx[0])
+    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx[0])
-from .utils import _indent
+from .utils import _indent, _brief_print_list
-        Prefix should be unique within one model to prevent name collisions.
+        Prefix acts like a name space. All children blocks created in parent block's
-        self._children = []
+        self._children = OrderedDict()
-                        modstr=modstr)
+        return s.format(name=self.__class__.__name__, modstr=modstr)
-            self.register_child(value)
+                                'is not allowed.'.format(
-        for cld in self._children:
+        for cld in self._children.values():
-        self.collect_params().save(filename, strip_prefix=self.prefix)
+        params = self._collect_params_with_prefix()
-    def load_params(self, filename, ctx=cpu(), allow_missing=False,
+    def load_params(self, filename, ctx=None, allow_missing=False,
-                                   self.prefix)
+        loaded = ndarray.load(filename)
-    def register_child(self, block):
+        if not any('.' in i for i in loaded.keys()):
-        self._children.append(block)
+        if name is None:
-    def initialize(self, init=initializer.Uniform(), ctx=None, verbose=False):
+    def initialize(self, init=initializer.Uniform(), ctx=None, verbose=False,
-
+
-        self.collect_params().initialize(init, ctx, verbose)
+        self.collect_params().initialize(init, ctx, verbose, force_reinit)
-        for cld in self._children:
+        for cld in self._children.values():
-        for child in self._children:
+        for child in self._children.values():
-    def register_child(self, block):
+    def register_child(self, block, name=None):
-                "please try HybridSequential instead"%(
+                "please try HybridSequential instead."%(
-        super(HybridBlock, self).register_child(block)
+        super(HybridBlock, self).register_child(block, name)
-        for block in self._children:
+        for block in self._children.values():
-        for block in self._children:
+        for block in self._children.values():
-        for block in self._children:
+        for block in self._children.values():
-                            if isinstance(block, Block)])
+                            for key, block in self._children.items()])
-        return self._children[key]
+        return self._children[str(key)]
-                          'using HybridSequential for the best performance.', stacklevel=2)
+        if self._children and all(isinstance(c, HybridBlock) for c in self._children.values()):
-        for block in self._children:
+        for block in self._children.values():
-                            if isinstance(block, Block)])
+                            for key, block in self._children.items()])
-        return self._children[key]
+        return self._children[str(key)]
-from ..context import Context
+from ..context import Context, cpu
-from .utils import _indent
+from .utils import _indent, _brief_print_list
-                assert set(ctx) == set(self._deferred_init[1]), \
+                assert ctx is None or set(ctx) == set(self._deferred_init[1]), \
-            assert set(ctx) == set(self.list_ctx()), \
+            assert ctx is None or set(ctx) == set(self.list_ctx()), \
-    return ', '.join(["'%s'"%str(i) for i in lst])
+    def __repr__(self):
-                    "Block.collect_params.load instead."%(
+                    "Prefix '%s' is to be striped before saving, but Parameter's "
-    def load(self, filename, ctx, allow_missing=False,
+    def load(self, filename, ctx=None, allow_missing=False,
-        for cell in self._children:
+        for cell in self._children.values():
-                                          for i, m in enumerate(self._children)]))
+                                          for i, m in self._children.items()]))
-        return _cells_state_info(self._children, batch_size)
+        return _cells_state_info(self._children.values(), batch_size)
-        return _cells_begin_state(self._children, **kwargs)
+        return _cells_begin_state(self._children.values(), **kwargs)
-        for cell in self._children:
+        for cell in self._children.values():
-        for i, cell in enumerate(self._children):
+        for i, cell in enumerate(self._children.values()):
-        return self._children[i]
+        return self._children[str(i)]
-        self.register_child(r_cell)
+        self.register_child(l_cell, 'l_cell')
-                        r_cell=self._children[1])
+                        l_cell=self._children['l_cell'],
-        return _cells_state_info(self._children, batch_size)
+        return _cells_state_info(self._children.values(), batch_size)
-        return _cells_begin_state(self._children, **kwargs)
+        return _cells_begin_state(self._children.values(), **kwargs)
-        l_cell, r_cell = self._children
+        l_cell, r_cell = self._children.values()
-    assert b.c is c2 and b._children[0] is c2
+    assert b.c is c2 and list(b._children.values())[0] is c2
-                self._params[k] = v
+
-        self.infer_shape(*args)
+        try:
-            **{i.name: getattr(j, attr) for i, j in zip(inputs, args)})
+        with warnings.catch_warnings(record=True) as w:
-                      zip(out.list_auxiliary_states(), aux_attrs)})
+             zip(out.list_auxiliary_states(), aux_attrs)})
-
+import shutil
-def test_tutorial_nb(file_path):
+def test_tutorial_nb(file_path, workingdir, kernel=None):
-        path of tutorial markdown file
+        path of tutorial .ipynb file
-    eprocessor = ExecutePreprocessor(timeout=1800)
+    if kernel:
-        eprocessor.preprocess(notebook, {'metadata': {}})
+        eprocessor.preprocess(notebook, {'metadata': {'path':workingdir}})
-        output_nb = open("output.txt", mode='w')
+        output_file = os.path.join(workingdir, "output.txt")
-        output_nb = open("output.txt", mode='r')
+        output_nb = open(output_file, mode='r')
-                return
+                success = False
-            test_tutorial_nb(file_dir)
+    tutorial_dir = os.path.join('..','..','docs', '_build', 'html', 'tutorials')
-        print "Stats end"
+    fail_num = len(fail_dict)
-            exit(1)
+    if fail_num > 0:
-from subprocess import check_call, call
+import logging
-from copy import deepcopy
+import subprocess
-    platforms = list(map(lambda x: os.path.split(x)[1], files))
+    platforms = list(map(lambda x: os.path.split(x)[1], sorted(files)))
-def get_dockerfile(platform: str, path="docker"):
+def get_dockerfile(platform: str, path="docker") -> str:
-        return "docker"
+
-    print()
+
-            cmd = ["/work/mxnet/ci/docker/runtime_functions.sh", "build_{}".format(platform)]
+            build_platform = "build_{}".format(platform)
-            plat_buildir = os.path.join(get_mxnet_root(), "build_{}".format(platform))
+            plat_buildir = os.path.join(get_mxnet_root(), build_platform)
-
+    def slice_like(self, *args, **kwargs):
-                    'one_hot', 'pick', 'sort', 'topk', 'argsort', 'argmax', 'argmin',
+                    'broadcast_axes', 'pad', 'swapaxes', 'slice', 'slice_axis', 'slice_like',
-    
+
-                    'one_hot', 'pick', 'sort', 'topk', 'argsort', 'argmax', 'argmin',
+                    'broadcast_axes', 'pad', 'swapaxes', 'slice', 'slice_axis', 'slice_like',
-            [in_stype[0]]*len(self.list_auxiliary_states())
+            "in your custom operator if you have non-default input/output stypes" % (stype, i)
-context = mx.cpu()  # train on cpu because maclyfe
+# !/usr/bin/env python
-
+            if update_on_kvstore:
-                kvstore.set_optimizer(self._optimizer)
+def _get_kvstore_server_command_type(command):
-            self._send_command_to_servers(0, optim_str)
+            cmd = _get_kvstore_server_command_type('kController')
-
+            if update_on_kvstore:
-        else:
+
-    assert(np.sum(np.abs((A - x).asnumpy())) == 0), (rank, A.asnumpy(), x)
+def check_diff(A, x, rank=None):
-rate = 2
+keys_shape = ['3', '5', '7']
-    nworker = kv.num_workers
+    # # init kv dns keys
-    return kv, my_rank, nworker
+    kv.set_optimizer(mx.optimizer.create('test', rescale_grad=rate, multi_precision=use_multiprecision))
-    kv.set_gradient_compression({'type': '2bit', 'threshold':threshold})
+    kv.set_gradient_compression({'type': '2bit', 'threshold': threshold})
-    kv.init('1121', mx.nd.zeros(shape))
+    for k, s in compr_keys_shapes:
-    kv.init('1122', mx.nd.ones(shape))
+    for k, s in compr_init_keys_shapes:
-        nrepeat = 3
+def test_sync_push_pull(nrepeat):
-        nrepeat = 3
+        ks = keys_shapes if dtype == 'float32' else fp16_keys_shapes
-        v = mx.nd.zeros(shape)
+        v = mx.nd.zeros(shape, dtype=dtype)
-            kv.push('9', v.tostype('row_sparse'))
+            kv.push(k, v.tostype('row_sparse'))
-            num_rows = shape[0]
+            num_rows = s[0]
-            row_ids = mx.nd.array(row_ids_np).reshape((num_rows/2, 2))
+            row_ids = mx.nd.array(row_ids_np).reshape((num_rows/2, 2)).astype(dtype)
-            kv.row_sparse_pull('9', out=val, row_ids=row_ids)
+            val = mx.nd.zeros(s, stype='row_sparse', dtype=dtype)
-            updated_val = mx.nd.ones(shape)
+            updated_val = mx.nd.ones(s, dtype=dtype)
-                row = rank % shape[0]
+                row = rank % s[0]
-            expected = mx.nd.zeros(shape)
+            expected = mx.nd.zeros(s, dtype=dtype)
-            check_diff_to_scalar(val, expected)
+            check_diff(val, expected, kv.rank)
-        nrepeat = 3
+    def check_row_sparse_keys_with_zeros(dtype, nrepeat):
-        big_v = mx.nd.sparse.zeros('row_sparse', big_shape)
+        v = mx.nd.sparse.zeros('row_sparse', shape, dtype=dtype)
-            kv.push('100', big_v)
+            kv.push(k1, v)
-            kv.row_sparse_pull('11', out=val, row_ids=mx.nd.array(all_row_ids))
+            kv.row_sparse_pull(k1, out=val, row_ids=mx.nd.array(all_row_ids))
-            kv.row_sparse_pull('100', out=big_val, row_ids=mx.nd.array(big_all_row_ids))
+            kv.row_sparse_pull(k2, out=big_val, row_ids=mx.nd.array(big_all_row_ids))
-            check_diff_to_scalar(big_val, 1)
+            check_diff(val, 1)
-            check_diff_to_scalar(big_val, 0)
+            kv.row_sparse_pull(k1, out=val, row_ids=mx.nd.array([]))
-        v = mx.nd.zeros(big_shape)
+        v = mx.nd.zeros(big_shape, dtype=dtype)
-
+            kv.push(k, v.tostype('row_sparse'))
-            row_ids = mx.nd.array(row_ids_np).reshape((num_rows/2, 2))
+            row_ids = mx.nd.array(row_ids_np).reshape((num_rows/2, 2)).astype(dtype)
-            kv.row_sparse_pull('100', out=val, row_ids=row_ids)
+            val = mx.nd.zeros(big_shape, stype='row_sparse', dtype=dtype)
-            updated_val = mx.nd.ones(big_shape)
+            updated_val = mx.nd.ones(big_shape, dtype=dtype)
-            expected = mx.nd.zeros(big_shape)
+            expected = mx.nd.zeros(big_shape, dtype=dtype)
-            check_diff_to_scalar(val, expected, rank=my_rank)
+            check_diff(val, expected.astype(dtype), rank=my_rank)
-        for k,s in [('1121', shape),('112221',irregular_shape),('11221', big_shape)]:
+    for dtype in ['float16', 'float32']:
-            val=mx.nd.zeros(s)
+            kv.push(k, mx.nd.ones(s) * 0.4)
-            check_diff_to_scalar(val, 0)
+            check_diff(val, 0)
-            kv.push(k, mx.nd.ones(s)*(threshold - 0.4))
+            kv.push(k, mx.nd.ones(s) * (threshold - 0.4))
-            check_diff_to_scalar(val2, curval)
+            check_diff(val2, curval)
-            val3= mx.nd.zeros(s)
+            kv.push(k, mx.nd.ones(s) * 0.2)
-            check_diff_to_scalar(val3, curval)
+            check_diff(val3, curval)
-            kv.push(k, mx.nd.ones(s)*(threshold-0.2))
+            kv.push(k, mx.nd.ones(s) * (threshold-0.2))
-            check_diff_to_scalar(val4, curval)
+            kv.pull(k, val4)
-        for k,s in [('1121', shape),('112221',irregular_shape),('11221', big_shape)]:
+    def check_compr_ones(threshold):
-            kv.push(k,mx.nd.ones(s)*threshold)
+            kv.push(k,mx.nd.ones(s) * threshold)
-            check_diff_to_scalar(val2, newval)
+            newval = curval + rate * nworker * threshold
-                check_diff_to_scalar(val, 0)
+    def check_compr_pull_before_push():
-        for k,s in [('1121', shape),('112221',irregular_shape),('11221', big_shape)]:
+    def check_compr_zero():
-            check_diff_to_scalar(val, 0)
+            check_diff(val, 0)
-    def check_compr_random(kv, threshold, nworker):
+    def check_compr_random(threshold, nrepeat):
-        compr_random_keys_shapes = [('2121', shape),('212221',irregular_shape),('21221', big_shape)]
+
-    check_compr_random(kv, threshold, nworker)
+    print ('worker ' + str(my_rank) + ' started with compression tests')
-def test_sync_init():
+def test_sync_init(gpu_tests=False):
-        val = [mx.nd.zeros(cur_shape, ctx) for i in cur_keys]
+        val = [mx.nd.zeros(cur_shape, ctx=ctx, dtype=get_dtype(i, cur_keys)) for i in range(len(cur_keys))]
-            kv.init(cur_keys[i], [mx.nd.ones(cur_shape, ctx) * i])
+            kv.init(cur_keys[i], [mx.nd.ones(cur_shape, ctx=ctx, dtype=get_dtype(i, cur_keys)) * i])
-            check_diff_to_scalar(val[i], expected)
+            check_diff(val[i], expected)
-    print('worker ' + str(my_rank) + ' is initialized')
+    if gpu_tests:
-    test_sync_push_pull()
+    parser = argparse.ArgumentParser(description='test distributed kvstore in dist_sync mode')
-            label = np.fromstring(fin.read(), dtype=np.uint8).astype(np.int32)
+            label = np.frombuffer(fin.read(), dtype=np.uint8).astype(np.int32)
-            data = np.fromstring(fin.read(), dtype=np.uint8)
+            data = np.frombuffer(fin.read(), dtype=np.uint8)
-            data = np.fromstring(fin.read(), dtype=np.uint8).reshape(-1, 3072+1)
+            data = np.frombuffer(fin.read(), dtype=np.uint8).reshape(-1, 3072+1)
-            data = np.fromstring(fin.read(), dtype=np.uint8).reshape(-1, 3072+2)
+            data = np.frombuffer(fin.read(), dtype=np.uint8).reshape(-1, 3072+2)
-        header = header._replace(label=np.fromstring(s, np.float32, header.flag))
+        header = header._replace(label=np.frombuffer(s, np.float32, header.flag))
-            label = np.fromstring(flbl.read(), dtype=np.int8)
+            label = np.frombuffer(flbl.read(), dtype=np.int8)
-            image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)
+            image = np.frombuffer(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)
-            self.num_inst += len(pred_label.flat)
+            self.sum_metric += (pred_label == label).sum()
-        tag_list.append('master')
+    # To fetch the data names of the input to the model we list the inputs of the symbol graph
-    mod.bind(for_training=False, data_shapes=[('input_0', input_img.shape)])
+    mod = mx.mod.Module(symbol=sym, data_names=data_names, label_names=None)
-                self._renames[i.name] = name_param
+                self._nodes[i.name] = symbol.Variable(name=i.name,
-                self._renames[i.name] = name_input
+                self._nodes[i.name] = symbol.Variable(name=i.name)
-            inputs = [self._nodes[self._renames.get(i, i)] for i in node.input]
+            inputs = [self._nodes[i] for i in node.input]
-        mod = mx.mod.Module(symbol=self.symbol, data_names=['input_0'], context=ctx,
+        # To fetch the data names of the input to the model we list the inputs of the symbol graph
-        mod.bind(for_training=False, data_shapes=[('input_0', input_data.shape)],
+        mod.bind(for_training=False, data_shapes=data_shapes,
-                                     'input_0', 'param_0', 'param_2', 'param_4', 'param_6']):
+    for i, input_param in enumerate(['9', '7', '5', '3', '1', '2', '4', '6', '8']):
-                                  'transpose0']):
+    for i, key_item in enumerate(['reshape4', 'convolution2', 'convolution0',
-                                    'param_1', 'param_0', 'param_3', 'param_2']):
+    for i, param_item in enumerate(['3', '2', '5', '4', '7', '6', '9', '8']):
-        mod.bind(for_training=False, data_shapes=[('input_0', input_data.shape)], label_shapes=None)
+        data_names = [graph_input for graph_input in sym.list_inputs()
-        mod.bind(for_training=False, data_shapes=[('input_0', input_data.shape)], label_shapes=None)
+        data_names = [graph_input for graph_input in sym.list_inputs()
-        mod.bind(for_training=False, data_shapes=[('input_0', input_data.shape)], label_shapes=None)
+        data_names = [graph_input for graph_input in sym.list_inputs()
-# pylint: disable=invalid-name, protected-access, too-many-arguments, no-self-use, too-many-locals, broad-except
+# pylint: disable=invalid-name, protected-access, too-many-arguments, no-self-use, too-many-locals, broad-except, too-many-lines
-                        "shapes, got %d."%(n_out, len(otype))
+                        "types, got %d."%(n_out, len(otype))
-                        "shapes, got %d."%(n_in, len(itype))
+                        "types, got %d."%(n_in, len(itype))
-                        "shapes, got %d."%(n_aux, len(atype))
+                        "types, got %d."%(n_aux, len(atype))
-                                                                    writable=True))
+                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],
-                                                                    writable=False))
+                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],
-                                                                    writable=True))
+                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],
-                                                                    writable=False))
+                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],
-                         infertype_functype(infer_type_entry)]
+                         infertype_functype(infer_type_entry),
-            aux[0][:] = 1
+            if in_data[0].stype == 'default':
-            assert (aux[0].asnumpy() == 1).all()
+            self.assign(in_grad[0], req[0], 2 * mx.nd.sparse.elemwise_mul(in_data[0], out_grad[0]))
-        y = mx.nd.Custom(x, aux, op_type='sqr')
+    data = mx.symbol.Variable('data', stype='csr')
-def seed(seed_state):
+def seed(seed_state, ctx="all"):
-        The random number seed to set to all devices.
+        The random number seed.
-    generated from two devices can be different even if they are seeded using the same seed.
+    Random number generators in MXNet are device specific.
-    >>>
+    # Same results on the same device with the same seed
-    check_call(_LIB.MXRandomSeed(seed_state))
+        raise ValueError('seed_state must be int')
-from mxnet.base import py_str
+from mxnet.base import py_str, MXNetError
-                shape += (np.random.randint(1, size_max+1), )
+        ndim_min = 1
-            b = mx.nd.array(a, ctx=default_context(), dtype=a.dtype)
+            b = mx.nd.array(a, dtype=a.dtype)
-            reps_len = np.random.randint(0, length_max+1)
+            reps_len = np.random.randint(1, length_max+1)
-                reps_tuple += (np.random.randint(0, rep_max), )
+                reps_tuple += (np.random.randint(1, rep_max), )
-
+    def test_invalid_reps():
-    test_zero_reps()
+    test_invalid_reps()
-                end.append(slice_i+1)
+                end.append(slice_i+1 if slice_i != -1 else self.shape[i])
-    x[:, 1:3, 1] = 1
+    x[:, 1:3, 1:2] = 1
-                  (np_int((slice(None), slice(None), 1, 8), np.int64), False),
+                  (np_int((slice(None), slice(None), -1, -8)), False),
-    The optimizer updates the weight by::
+    The optimizer updates the weight by:
-            grad = grad * self.rescale_grad + weight * wd
+            grad = grad * self.rescale_grad
-            weight[:] += div * -lr
+            weight[:] += (div + weight * wd) * -lr
-        weight[:] -= current_delta
+        weight[:] -= current_delta + wd * weight
-        grad = grad * self.rescale_grad + weight * wd
+        grad = grad * self.rescale_grad
-        weight[:] += div * -lr
+        weight[:] += (div + weight * wd) * -lr
-    wd_options = [{}, {'wd': 0.1}]
+    wd_options = [{}, {'wd': 0.0}]
-
+        ctx_list.append({'ctx': mx.cpu(0), 'pool_data': data, 'type_dict': {'pool_data': np.float32}})
-    The optimizer updates the weight by:
+    The optimizer updates the weight by::
-            grad = grad * self.rescale_grad
+            grad = grad * self.rescale_grad + weight * wd
-            weight[:] += (div + weight * wd) * -lr
+            weight[:] += div * -lr
-        weight[:] -= current_delta + wd * weight
+        weight[:] -= current_delta
-        grad = grad * self.rescale_grad
+        grad = grad * self.rescale_grad + weight * wd
-        weight[:] += (div + weight * wd) * -lr
+        weight[:] += div * -lr
-    wd_options = [{}, {'wd': 0.0}]
+    wd_options = [{}, {'wd': 0.1}]
-            for name, val in eval_metric.get_name_value():
+            for name, val in eval_name_vals:
-    integer_types = int
+    integer_types = (int, np.int32, np.int64)
-    integer_types = (int, long)
+    integer_types = (int, long, np.int32, np.int64)
-                  (slice(None, None, -1), False), (slice(None, None, -2), False),
+    index_list = [(0, False), (np.int32(0), False), (np.int64(0), False),
-                  ((1, 2, 3), False), ((1, 2, 3, 4), True),
+                  (np_int((slice(None), 2, slice(1, 5), 1)), False),
-        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0])
+        mapping = '{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0] / self._gates)
-                end.append(slice_i+1)
+                end.append(slice_i+1 if slice_i != -1 else self.shape[i])
-    A2[3:8] *= 10;
+    A2[3:8] *= 10
-    scaladocs = ['index', 'index.html', 'ml', 'lib', 'index.js', 'package.html']
+    scaladocs = ['index', 'index.html', 'org', 'lib', 'index.js', 'package.html']
-                          test_exclude = True):
+    def test_reduce_inner(numpy_reduce_func, numpy_reduce_grad_func, mx_reduce_sym, nan_prob=0,
-                b = mx_reduce_sym(a, keepdims=keepdims)
+                if test_none_axis:
-                      mx.symbol.norm, test_exclude=False)
+    test_none_axis = [True, False]
-    kwargs = {'install_requires': ['numpy<=1.13.3,>=1.8.2', 'requests==2.18.4', 'graphviz==0.8.1'], 'zip_safe': False}
+    kwargs = {'install_requires': ['numpy<=1.15.0,>=1.8.2', 'requests<2.19.0,>=2.18.4', 'graphviz<0.9.0,>=0.8.1'], 'zip_safe': False}
-                    choices=["dist_async", "local"])
+                    choices=["dist_sync", "dist_async", "local"])
-                                   priority=-weight_index)
+            mod.prepare(batch, sparse_row_id_fn=batch_row_ids)
-                               priority=-weight_index)
+
-        mod.save_checkpoint("checkpoint", epoch, save_optimizer_states=save_optimizer_states)
+        mod.save_checkpoint("checkpoint", epoch)
-import os
+import os, logging
-def get_movielens_iter(filename, batch_size, dummy_iter):
+def get_movielens_iter(filename, batch_size):
-    print("Preparing data iterators for " + filename + " ... ")
+    logging.info("Preparing data iterators for " + filename + " ... ")
-    label_train = {'score':score}
+    data_train = {'user': user, 'item': item}
-def matrix_fact_net(factor_size, num_hidden, max_user, max_item, sparse_embed=True):
+def matrix_fact_net(factor_size, num_hidden, max_user, max_item, dense):
-    if sparse_embed:
+    stype = 'default' if dense else 'row_sparse'
-                                                 input_dim=max_user, output_dim=factor_size)
+        user = embed(data=user, weight=user_weight,
-                                                 input_dim=max_item, output_dim=factor_size)
+        item = embed(data=item, weight=item_weight,
-        user = mx.symbol.Embedding(data=user, input_dim=max_user, output_dim=factor_size)
+        user = mx.symbol.Embedding(data=user, weight=user_weight,
-        item = mx.symbol.Embedding(data=item, input_dim=max_item, output_dim=factor_size)
+        item = mx.symbol.Embedding(data=item, weight=item_weight,
-    user = mx.symbol.FullyConnected(data=user, num_hidden=num_hidden)
+    user_act = mx.symbol.FullyConnected(data=user, num_hidden=num_hidden)
-    item = mx.symbol.FullyConnected(data=item, num_hidden=num_hidden)
+    item_act = mx.symbol.FullyConnected(data=item, num_hidden=num_hidden)
-    pred = mx.symbol.sum(data=pred, axis = 1)
+    pred = user_act * item_act
-                    help='logging frequency')
+parser.add_argument('--log-interval', type=int, default=100,
-                    help="use the dummy data iterator for speed test")
+parser.add_argument('--gpus', type=str,
-    print_every = args.print_every
+    log_interval = args.log_interval
-    ctx = mx.gpu(0) if args.use_gpu else mx.cpu(0)
+    ctx = [mx.gpu(int(i)) for i in args.gpus.split(',')] if args.gpus else [mx.cpu()]
-    val_iter = get_movielens_iter(MOVIELENS['val'], batch_size, dummy_iter)
+    train_iter = get_movielens_iter(MOVIELENS['train'], batch_size)
-    net = matrix_fact_net(factor_size, factor_size, max_user, max_movies, sparse_embed=use_sparse)
+    net = matrix_fact_net(factor_size, factor_size, max_user, max_movies, dense=args.dense)
-    mod = mx.module.Module(symbol=net, context=ctx, data_names=['user', 'item'],
+    mod = mx.module.Module(net, context=ctx, data_names=['user', 'item'],
-    mod.init_optimizer(optimizer=optim)
+    optim = mx.optimizer.create(optimizer, learning_rate=learning_rate,
-    speedometer = mx.callback.Speedometer(batch_size, print_every)
+    speedometer = mx.callback.Speedometer(batch_size, log_interval)
-    return list(data.items())
+    return list(sorted(data.items()))
-# pylint: disable=fixme, too-many-arguments, too-many-locals, too-many-public-methods, too-many-branches
+# pylint: disable=fixme, too-many-arguments, too-many-locals
-              reset=True, epoch=0):
+              reset=True, epoch=0, sparse_row_id_fn=None):
-
+            self.prepare(eval_batch, sparse_row_id_fn=sparse_row_id_fn)
-    def iter_predict(self, eval_data, num_batch=None, reset=True):
+    def iter_predict(self, eval_data, num_batch=None, reset=True, sparse_row_id_fn=None):
-                always_output_list=False):
+                always_output_list=False, sparse_row_id_fn=None):
-            validation_metric=None, monitor=None):
+            validation_metric=None, monitor=None, sparse_row_id_fn=None):
-                    self.prepare(next_data_batch)
+                    self.prepare(next_data_batch, sparse_row_id_fn=sparse_row_id_fn)
-    def prepare(self, data_batch):
+    # pylint: disable=unused-argument
-        pass
+        if sparse_row_id_fn is not None:
-        """Prepares a data batch for forward.
+    def prepare(self, data_batch, sparse_row_id_fn=None):
-        """
+            The current batch of data for forward computation.
-    >>>> mx.nd.random.uniform(0, 1, ctx=mx.gpu(0))
+    >>> mx.nd.random.uniform(0, 1, ctx=mx.gpu(0))
-    <NDArray 2x2 @cpu(0)>
+    [ 0.71589124  0.08976638]
-    >>>> mx.nd.random.normal(0, 1, ctx=mx.gpu(0))
+    >>> mx.nd.random.normal(0, 1, ctx=mx.gpu(0))
-        f(x; \frac{1}{\beta}) = \frac{1}{\beta} \exp(-\frac{x}{\beta}),
+    .. math:: f(x; \frac{1}{\beta}) = \frac{1}{\beta} \exp(-\frac{x}{\beta}),
-        f(x; \frac{1}{\beta}) = \frac{1}{\beta} \exp(-\frac{x}{\beta}),
+    .. math:: f(x; \frac{1}{\beta}) = \frac{1}{\beta} \exp(-\frac{x}{\beta}),
-           "BaseSparseNDArray", "CSRNDArray", "RowSparseNDArray"]
+           "BaseSparseNDArray", "CSRNDArray", "RowSparseNDArray",
-            out[neg_indices] = slope * (np.exp(out[neg_indices]) - 1.)
+            out[neg_indices] = slope * np.expm1(out[neg_indices])
-        rtol = 1e-4
+        rtol = 1e-2
-            rtol = 1e-3
+            rtol = 1e-2
-        preds : list of NDArray
+        preds : OrderedDict of str -> NDArray
-        mx.test_utils.assert_almost_equal(real_dx, dx.asnumpy()[i], rtol=1e-4)
+    for x in [mx.nd.array([[0,1,2,3,4],[4,3,2,1,0]])/10.0, mx.nd.array([0,1,2,3,4])/10.0]:
-            "Interval {} must be smaller than length {}".format(interval, length)
+        assert interval <= length, \
-    # check_name(conv_sym, ['conv_output'])
+    conv_sym = mx.sym.Convolution(data, kernel=(2, 2), num_filter=1, name='conv')
-def get_docker_tag(platform: str) -> None:
+def get_docker_tag(platform: str) -> str:
-def container_run(platform: str, docker_binary: str, command: List[str]) -> None:
+def container_run(platform: str, docker_binary: str, command: List[str], dry_run: bool = False, into_container: bool = False) -> str:
-    local_build_folder = '{}/build'.format(mx_root)
+    local_build_folder = buildir()
-        logging.error("You can try to get into the container by using the following command: %s", ' '.join(into_cmd))
+    if not dry_run and not into_container:
-    parser = argparse.ArgumentParser()
+    parser = argparse.ArgumentParser(description="""Utility for building and testing MXNet on docker
-                        help="Build the container",
+    parser.add_argument("--build-only",
-        print(platforms)
+        list_platforms()
-            build_docker(platform, docker_binary)
+        build_docker(platform, docker_binary)
-    else:
+    elif args.all:
-            logging.info("No command specified, trying default build: %s", ' '.join(cmd))
+            shutil.rmtree(buildir(), ignore_errors=True)
-            from the length of the array and remaining dimensions.
+            - ``0``  copy this dimension from the input to the output shape.
-            if x.shape == (3, 4, 5), x.reshape((0, 20)).shape will be (3, 20).
+              Example::
-        >>> x = mx.nd.arange(0,6).reshape((2,3))
+        >>> x = mx.nd.arange(0,6).reshape(2,3)
-        >>> y = x.reshape((3,2))
+        >>> y = x.reshape(3,2)
-        >>> y = x.reshape((3,-1))
+        >>> y = x.reshape(3,-1)
-                                         ctypes.byref(handle)))
+        check_call(_LIB.MXNDArrayReshape64(self.handle,
-    a_nd[1] = v 
+    a_nd[1] = v
-    a_nd[2] = v 
+    a_nd[2] = v
-    # assign by slice 
+    # assign by slice
-    for func in ['sum', 'nansum', 'prod', 'nanprod', 'mean', 'max', 'min']:
+    for func in ['sum', 'nansum', 'prod', 'nanprod', 'mean', 'max', 'min', 'norm']:
-    def test_reduce_inner(numpy_reduce_func, numpy_reduce_grad_func, mx_reduce_sym, nan_prob = 0):
+    def test_reduce_inner(numpy_reduce_func, numpy_reduce_grad_func, mx_reduce_sym, nan_prob = 0,
-            exclude = np.random.randint(0, 2)
+            if test_exclude:
-    for func in ['sum', 'mean']:
+    for func in ['sum', 'mean', 'norm']:
-    for func in ['sum', 'nansum', 'prod', 'nanprod', 'mean', 'max', 'min']:
+    for func in ['sum', 'nansum', 'prod', 'nanprod', 'mean', 'max', 'min', 'norm']:
-    assert_almost_equal(out1.asnumpy(), out3.asnumpy())
+    assert_almost_equal(out1.asnumpy(), out2.asnumpy(), rtol=1e-3)
-    backward function will be used instead of the default chain-rule.
+    """Customize differentiation in autograd.
-            the input ndarray is returned instead of a copy.
+            `False`, and the dtype requested is the same as the ndarray's
-            the input ndarray is returned instead of a copy.
+            `False`, and the dtype requested is the same as the ndarray's
-"""Module for importing and exporting ONNX models."""
+"""Module for ONNX model format support for Apache MXNet."""
-    """Imports the ONNX model file passed as a parameter into MXNet symbol and parameters.
+    """Imports the ONNX model file, passed as a parameter, into MXNet symbol and parameters.
-    Mxnet symbol and parameter objects.
+    sym : :class:`~mxnet.symbol.Symbol`
-        Dict of converted parameters stored in mxnet.ndarray format
+    arg_params : dict of ``str`` to :class:`~mxnet.ndarray.NDArray`
-        raise ImportError("Onnx and protobuf need to be installed")
+        raise ImportError("Onnx and protobuf need to be installed. "
-            raise ImportError("Unable to import onnx which is required {}".format(e))
+        except ImportError:
-    def astype(self, dtype):
+    def astype(self, dtype, copy=True):
-            The copied array after casting to the specified type.
+            The copied array after casting to the specified type, or
-    def astype(self, dtype):
+    def astype(self, dtype, copy=True):
-                self.rnn = rnn.RNN(num_hidden, 'relu', num_layers, dropout=dropout,
+                self.rnn = rnn.RNN(num_hidden, num_layers, dropout=dropout,
-                self.rnn = rnn.RNN(num_hidden, num_layers, dropout=dropout,
+                self.rnn = rnn.RNN(num_hidden, num_layers, 'tanh', dropout=dropout,
-parser.add_argument('--emsize', type=int, default=200,
+parser.add_argument('--emsize', type=int, default=650,
-parser.add_argument('--nhid', type=int, default=200,
+parser.add_argument('--nhid', type=int, default=650,
-parser.add_argument('--lr', type=float, default=1.0,
+parser.add_argument('--lr', type=float, default=20,
-parser.add_argument('--clip', type=float, default=0.2,
+parser.add_argument('--clip', type=float, default=0.25,
-parser.add_argument('--batch_size', type=int, default=32, metavar='N',
+parser.add_argument('--batch_size', type=int, default=20, metavar='N',
-parser.add_argument('--dropout', type=float, default=0.2,
+parser.add_argument('--dropout', type=float, default=0.5,
-model.collect_params().initialize(mx.init.Xavier(), ctx=context)
+model.initialize(mx.init.Xavier(), ctx=context)
-            gluon.utils.clip_global_norm(grads, args.clip * args.bptt * args.batch_size)
+            gluon.utils.clip_global_norm(grads, args.clip)
-            trainer.step(args.batch_size)
+            trainer.step(1)
-                cur_L = total_L / args.bptt / args.batch_size / args.log_interval
+                cur_L = total_L / args.log_interval
-            model.collect_params().save(args.save)
+            model.save_params(args.save)
-            model.collect_params().load(args.save, context)
+            trainer.set_learning_rate(args.lr)
-    model.collect_params().load(args.save, context)
+    model.load_params(args.save, context)
-        assert os.path.exists(roi_rec['image']), '%s does not exist'.format(roi_rec['image'])
+        assert os.path.exists(roi_rec['image']), '{} does not exist'.format(roi_rec['image'])
-                                  .format(name=self.__class__.__name__ + "." + k))
+                                  .format(name=self.__class__.__name__ + "." + k), stacklevel=3)
-                              "computation. Is this intended?"%i)
+                              "computation. Is this intended?"%i, stacklevel=4)
-                              "Is this intended?"%name)
+                              "Is this intended?"%name, stacklevel=4)
-        self.hybridize()
+    assert num_layers in resnet_spec, \
-                          'using HybridSequential for the best performance.')
+                          'using HybridSequential for the best performance.', stacklevel=2)
-            "grad_req must be one of write, add, or null, but got %s"%req
+            "grad_req must be one of 'write', 'add', or 'null', but got '%s'"%req
-                        return arr_list[idx]
+            ctx_list = self._ctx_map[ctx.device_typeid&1]
-                "Parameter %s was not initialized on context %s. "
+                "Parameter '%s' was not initialized on context %s. "
-                "Parameter %s has not been initialized yet because initialization was " \
+                "Parameter '%s' has not been initialized yet because initialization was " \
-            "Parameter %s has not been initialized. Note that " \
+            "Parameter '%s' has not been initialized. Note that " \
-                    "Failed loading Parameter %s from saved params: " \
+                    "Failed loading Parameter '%s' from saved params: " \
-                "Failed loading Parameter %s from saved params: " \
+                "Failed loading Parameter '%s' from saved params: " \
-                    "Failed to load Parameter %s on %s because it was " \
+                    "Failed to load Parameter '%s' on %s because it was " \
-                "Failed to load Parameter %s on %s because it was " \
+                "Failed to load Parameter '%s' on %s because it was " \
-            "Cannot initialize Parameter %s because it has " \
+            "Cannot initialize Parameter '%s' because it has " \
-        self._ctx_map = []
+        self._ctx_map = [[], []]
-            dev_list = self._ctx_map[ctx.device_typeid]
+            dev_list = self._ctx_map[ctx.device_typeid&1]
-                          "Set force_reinit=True to re-initialize."%self.name)
+            warnings.warn("Parameter '%s' is already initialized, ignoring. " \
-            raise ValueError("Cannot initialize Parameter %s because it has " \
+            raise ValueError("Cannot initialize Parameter '%s' because it has " \
-            raise ValueError("Cannot reset context for Parameter %s because it "
+            raise ValueError("Cannot reset context for Parameter '%s' because it "
-                "Parameter %s has not been initialized"%self.name
+                "Parameter '%s' has not been initialized"%self.name
-                "Cannot get gradient array for Parameter %s " \
+                "Cannot get gradient array for Parameter '%s' " \
-                "Cannot get gradient array for Parameter %s " \
+                "Cannot get gradient array for Parameter '%s' " \
-            raise RuntimeError("Parameter %s has not been initialized"%self.name)
+            raise RuntimeError("Parameter '%s' has not been initialized"%self.name)
-                        "desired %s vs stored %s."%(
+                        "Cannot retrieve Parameter '%s' because desired attribute " \
-                raise KeyError("No constant named {}. Please specify value " \
+                raise KeyError("No constant named '{}'. Please specify value " \
-                "Parameter {} already exists but it is not a constant.".format(
+                "Parameter '{}' already exists but it is not a constant.".format(
-                "Constant {} already exists but it's value doesn't match new value"
+                "Constant '{}' already exists but it's value doesn't match new " \
-                    "Parameters with the same name %s"%k
+                    "Parameters with the same name '%s'"%k
-                    "%s does not start with %s. If you are using Block.save_params, " \
+                    "Prefix '%s' is to be striped before saving, but Parameter " \
-                    "with %s"%(restore_prefix, name, restore_prefix)
+                    "restore_prefix is '%s' but Parameters name '%s' does not start " \
-                    "Parameter %s is missing in file %s"%(name[lprefix:], filename)
+                    "Parameter '%s' is missing in file '%s', which contains parameters: %s. " \
-                        name[lprefix:], filename)
+                    "Parameter '%s' loaded from file '%s' is not present in ParameterDict, " \
-            assert abs(1. * count[str(mx.nd.array(p))] / repeat2 - 1. / math.factorial(data.shape[0])) < 0.01
+            err = abs(1. * count[str(mx.nd.array(p))] / repeat2 - 1. / math.factorial(data.shape[0]))
-    testSmall(mx.nd.arange(0, 18).reshape((3, 2, 3)), 100, 20000)
+    testSmall(mx.nd.arange(0, 3), 100, 40000)
-    state of the previous layer at time `t` or :math:`input_t` for the first layer.
+    where :math:`h_t` is the hidden state at time `t`, and :math:`x_t` is the output
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    test_tvm_bridge()
+    import nose
-    check_name(conv_sym, ['conv_output'])
+    # Temporarily disabling convolutional test as it is exposing a hang.
-@with_seed(1234)
+# NOTE(haojin2): Skipping the numeric check tests for float16 data type due to precision issues,
-    slp = 0.0625
+    slp = 0.25
-        xa = np.random.uniform(low=-1.0,high=-0.2,size=shape).astype(dtype)
+        xa = np.random.uniform(low=-1.0,high=1.0,size=shape).astype(dtype)
-        for act_type in ['leaky']:
+        for act_type in ['elu', 'leaky']:
-            check_symbolic_backward(y, [xa], [np.ones(shape)], [ga], rtol=eps, atol=1e-5, dtype=dtype)
+            # Skip numeric check for float16 type to get rid of flaky behavior
-@with_seed(1234)
+# NOTE(haojin2): Skipping the numeric check tests for float16 data type due to precision issues,
-        for gam in [np.array([0.1], dtype=dtype), np.array([0.1, 0.2, 0.3, 0.4], dtype=dtype)]:
+        for gam in [np.array([0.1, 0.2, 0.3, 0.4], dtype=dtype)]:
-
+            # Skip numeric check for float16 type to get rid of flaky behavior
-        logger.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))
+        logger.info('[Epoch %d] time cost: %f'%(epoch, epoch_time))
-            tmp = np.random.uniform(-1.0, 1.0, (nout, nin))
+            tmp = random.uniform(-1.0, 1.0, shape=(nout, nin)).asnumpy()
-            tmp = np.random.normal(0.0, 1.0, (nout, nin))
+            tmp = random.normal(0.0, 1.0, shape=(nout, nin)).asnumpy()
-            np.random.shuffle(self.idx)
+            tmp_idx = arange(self.data[0][1].shape[0], dtype=np.int32)
-        if param is None:
+        if param is None: # pylint: disable=too-many-nested-blocks
-                    assert v is None or v == getattr(param, k), \
+                    existing = getattr(param, k)
-        def __init__(self, **kwargs):
+        def __init__(self, in_units=0, **kwargs):
-                self.dense1 = nn.Dense(5, in_units=5)
+                self.dense0 = nn.Dense(5, in_units=in_units)
-    net1 = Net(prefix='net1_')
+    net1 = Net(prefix='net1_', in_units=5)
-    sym, params = onnx_mxnet.import_model('super_resolution.onnx')
+    sym, arg_params, aux_params = onnx_mxnet.import_model('super_resolution.onnx')
-    return sym, params
+    return sym, arg_params, aux_params
-def perform_inference(sym, params, input_img, img_cb, img_cr):
+def perform_inference(sym, arg_params, aux_params, input_img, img_cb, img_cr):
-    mod.set_params(arg_params=params, aux_params=None)
+    mod.set_params(arg_params=arg_params, aux_params=aux_params)
-    MX_SYM, MX_PARAM = import_onnx()
+    MX_SYM, MX_ARG_PARAM, MX_AUX_PARAM = import_onnx()
-    perform_inference(MX_SYM, MX_PARAM, INPUT_IMG, IMG_CB, IMG_CR)
+    perform_inference(MX_SYM, MX_ARG_PARAM, MX_AUX_PARAM, INPUT_IMG, IMG_CB, IMG_CR)
-from .op_translations import reshape, cast, split, _slice, transpose, squeeze
+from .op_translations import reshape, cast, split, _slice, transpose, squeeze, flatten
-    return sym, params
+    sym, arg_params, aux_params = graph.from_onnx(model_proto.graph)
-                raise RuntimeError("Unable to map op_name {} to sym".format(op_name))
+        # For storing arg  and aux params for the graph.
-            for k, i in zip(list(node.output), range(len(node.output))):
+            for k, i in zip(list(node.output), range(len(mxnet_sym.list_outputs()))):
-        return out, self._params
+        return out, argDict, auxDict
-    new_attrs = translation_utils._fix_attribute_names(attrs, {'epsilon' : 'eps'})
+    new_attrs = translation_utils._fix_attribute_names(attrs, {'epsilon' : 'eps',
-                                                     ['spatial', 'is_test', 'consumed_inputs'])
+                                                     ['spatial', 'consumed_inputs'])
-    return 'pooling', new_attrs, inputs
+    return 'Pooling', new_attrs, inputs
-    return 'pooling', new_attrs, inputs
+    return 'Pooling', new_attrs, inputs
-    return translation_utils._fix_gemm('FullyConnected', inputs, new_attrs, cls)
+    return gemm_op, new_attrs, inputs
-def local_response_norm(op_name, attrs, inputs):
+def local_response_norm(attrs, inputs, cls):
-def dropout(op_name, attrs, inputs):
+def dropout(attrs, inputs, cls):
-                                                         'pooling_convention': 'valid'
+                                                        {'pooling_convention': 'valid'
-                                                         'pooling_convention': 'valid'
+                                                        {'pooling_convention': 'valid'
-                                    stride=stride, kernel=kernel)
+
-        data_names = [i for i in sym.get_internals().list_inputs()]
+        sym, arg_params, aux_params = graph.from_onnx(MXNetBackend.make_graph(node, inputs))
-                                   'Squeeze', 'Upsample', 'Reshape', 'Conv'])
+                                   'Squeeze', 'Upsample', 'Reshape', 'Conv',
-        mod.init_params()
+        if arg_params is None and aux_params is None:
-        return MXNetBackendRep(sym, params, device)
+        sym, arg_params, aux_params = graph.from_onnx(model.graph)
-    def __init__(self, symbol, params, device):
+    def __init__(self, symbol, arg_params, aux_params, device):
-        self.params = params
+        self.arg_params = arg_params
-        mod.set_params(arg_params=self.params, aux_params=None)
+        mod.set_params(arg_params=self.arg_params, aux_params=self.aux_params)
-IMPLEMENTED_OPERATORS = [
+IMPLEMENTED_OPERATORS_TEST = [
-    #'test_concat.*',  #---Failing test
+    'test_concat',
-    #'test_reflect_pad',
+    'test_constant_pad',
-    #'test_softmax*',
+    'test_softmax_example',
-    #'test_batch_norm',
+    'test_transpose',
-    #'test_cast',
+    'test_cast',
-    #'test_transpose*',
+    #'test_transpose',
-    'test_min'
+    'test_min',
-for op_test in IMPLEMENTED_OPERATORS:
+BASIC_MODEL_TESTS = [
-import backend as mxnet_backend
+from onnx import numpy_helper
-    sym, params = super_resolution.import_onnx()
+    sym, arg_params, aux_params = super_resolution.import_onnx()
-    assert params is not None
+    assert arg_params is not None
-    param_keys = params.keys()
+    param_keys = arg_params.keys()
-                                                    img_cb, img_cr)
+    result_img = super_resolution.perform_inference(sym, arg_params, aux_params,
-                 decay_factor=(1 - 1e-8), sparse_update=False, **kwargs):
+                 decay_factor=(1 - 1e-8), lazy_update=False, **kwargs):
-        self.sparse_update = sparse_update
+        self.lazy_update = lazy_update
-            if all_zeros and self.sparse_update:
+            if all_zeros and self.lazy_update:
-                        compare_optimizer(opt1(sparse_update=True, **kwarg), opt2(**kwarg), shape,
+                        compare_optimizer(opt1(lazy_update=True, **kwarg), opt2(**kwarg), shape,
-    def __init__(self, lamda1=0.01, learning_rate=0.1, beta=1, sparse_update=False, **kwargs):
+    def __init__(self, lamda1=0.01, learning_rate=0.1, beta=1, lazy_update=False, **kwargs):
-        self.sparse_update = sparse_update
+        self.lazy_update = lazy_update
-            if all_zeros and self.sparse_update:
+            if all_zeros and self.lazy_update:
-        compare_optimizer(opt1(sparse_update=True, **kwarg), opt2(**kwarg), shape,
+        compare_optimizer(opt1(lazy_update=True, **kwarg), opt2(**kwarg), shape,
-            assert_almost_equal(data_train.getdata().asnumpy(), expected)
+            data = data_train.getdata()
-            'feature_dim': 62060,
+            'feature_dim': 62060 + 1,
-                assert(np.sum(batch.label[0].asnumpy() <= 0) == 0)
+                data = batch.data[0]
-def _get_jupyter_notebook(lang, lines):
+def _get_jupyter_notebook(lang, all_lines):
-    for in_code, blk_lang, lines in _get_blocks(lines):
+    # Exclude lines containing <!--notebook-skip-line-->
-from . import nd
+from . import ndarray as nd
-            arg_shapes, _, _ = self.symbol.infer_shape(**dict(input_shapes))
+            arg_shapes, _, _ = self.symbol.infer_shape(**shapes)
-            self.ctx[0], grad_req='null', type_dict=type_dict, **dict(input_shapes))
+            self.ctx[0], grad_req='null', type_dict=type_dict, **shapes)
-                               numeric_eps=1e-2, rtol=1e-2, atol=1e-3)
+                               numeric_eps=1e-2, rtol=1e-2, atol=1e-2)
-        if x.split('/')[1] == start.split('/')[1]: return x
+        if '3rdparty' in x:
-            h = m.groups()[0].strip('./')
+            path = m.groups()[0]
-expand(sys.argv[3], [], "nnvm")
+expand(sys.argv[2], [], "3rdparty/dmlc-core")
-sys.path.append(os.path.join('/scratch', "mxnet/dmlc-core/tracker"))
+sys.path.append(os.path.join(os.environ['HOME'], "mxnet/3rdparty/dmlc-core/tracker"))
-                include_dirs=["../include/", "../nnvm/include"],
+                include_dirs=["../include/", "../3rdparty/nnvm/include"],
-sys.path.append(os.path.join(curr_path, "../dmlc-core/tracker"))
+sys.path.append(os.path.join(curr_path, "../3rdparty/dmlc-core/tracker"))
-    if not os.path.isdir(os.path.join(os.path.expanduser(root, 'val', 'n01440764'))):
+    if not os.path.isdir(os.path.expanduser(os.path.join(root, 'val', 'n01440764'))):
-train_dataset = contrib.data.text.WikiText2('./data', 'train', seq_len=args.bptt)
+dirname = './data'
-val_dataset, test_dataset = [contrib.data.text.WikiText2('./data', segment,
+val_dataset, test_dataset = [contrib.data.text.WikiText2(dirname, segment,
-                    new_h, new_w = new_size * h / w, new_size
+                    new_h, new_w = new_size * h // w, new_size
-                    new_h, new_w = new_size, new_size * w / h
+                    new_h, new_w = new_size, new_size * w // h
-
+@unittest.skip("JIRA issue: https://issues.apache.org/jira/projects/MXNET/issues/MXNET-130")
-               'ps-lite',
+    cosine = cosine / 2
-            self.summary_writer.add_scalar(name, value)
+            self.summary_writer.add_scalar(name, value, global_step=param.epoch)
-        mx.profiler.profiler_set_state('run')
+        mx.profiler.set_config(profile_all=True, filename=name)
-        mx.profiler.profiler_set_state('stop')
+        mx.profiler.set_state('stop')
-    profiler.profiler_set_state('run')
+    profiler.set_state('run')
-    profiler.profiler_set_state('stop')
+    profiler.set_state('stop')
-    mx.profiler.profiler_set_config(mode='symbolic', filename=args.profile_filename)
+    mx.profiler.set_config(profile_symbolic=True, filename=args.profile_filename)
-    mx.profiler.profiler_set_state('run')
+    mx.profiler.set_config(profile_all=True, filename='profile_imageiter.json')
-    mx.profiler.profiler_set_state('stop')
+    mx.profiler.set_state('stop')
-    mx.profiler.profiler_set_config(mode='symbolic', filename=args.profile_filename)
+    mx.profiler.set_config(profile_symbolic=True, filename=args.profile_filename)
-            mx.profiler.profiler_set_state('run')
+            mx.profiler.set_state('run')
-            mx.profiler.profiler_set_state('stop')
+            mx.profiler.set_state('stop')
-    mx.profiler.profiler_set_state('run')
+    mx.profiler.set_config(profile_all=True, filename='profile_ndarray.json')
-    mx.profiler.profiler_set_state('stop')
+    mx.profiler.set_state('stop')
-    profiler.profiler_set_config(mode='symbolic', filename='test_profile_create_domain_dept.json')
+    profiler.set_config(profile_symbolic=True, filename='test_profile_create_domain_dept.json')
-__all__ = ['VariationalDropoutCell']
+__all__ = ['VariationalDropoutCell', 'LSTMPCell']
-from ...rnn import BidirectionalCell, SequentialRNNCell, ModifierCell
+from ...rnn import BidirectionalCell, SequentialRNNCell, ModifierCell, HybridRecurrentCell
-
+
-    scala_path = app.builder.srcdir + '/../scala-package/core/src/main/scala/ml/dmlc/mxnet'
+    scala_path = app.builder.srcdir + '/../scala-package'
-    _run_cmd('cd ' + scala_path + '; scaladoc `find . | grep .*scala`; exit 0')
+    _run_cmd('cd ' + scala_path + '; scaladoc `find . -type f -name "*.scala" | egrep \"\/core|\/infer\" | egrep -v \"Suite\"`; exit 0')
-        if ('device' in self.type) or ('dist' in self.type):
+        if ('device' in self.type) or ('dist' in self.type): # pylint: disable=unsupported-membership-test
-        if 'dist' in self.type and is_worker.value:
+        if 'dist' in self.type and is_worker.value: # pylint: disable=unsupported-membership-test
-            if kvstore and 'dist' in kvstore.type and not '_async' in kvstore.type:
+            if kvstore and 'dist' in kvstore.type and '_async' not in kvstore.type:
-        return tuple(pdata[:ndim.value])
+        return tuple(pdata[:ndim.value]) # pylint: disable=invalid-slice-index
-            if key_var_num_args:
+            if key_var_num_args: # pylint: disable=using-constant-test
-                   c_str_array(kwargs.values())))
+        check_call(_LIB.MXFuncInvokeEx(
-            return ndargs[:n_mutate_vars]
+            return ndargs[:n_mutate_vars] # pylint: disable=invalid-slice-index
-           'negative_binomial', 'generalized_negative_binomial']
+           'negative_binomial', 'generalized_negative_binomial', 'shuffle']
-           'negative_binomial', 'generalized_negative_binomial']
+           'negative_binomial', 'generalized_negative_binomial', 'shuffle']
-def check_l2_normalization(in_shape, mode, norm_eps=1e-10):
+def check_l2_normalization(in_shape, mode, dtype, norm_eps=1e-10):
-    in_data = np.random.uniform(-1, 1, in_shape)
+    in_data = np.random.uniform(-1, 1, in_shape).astype(dtype)
-    assert_almost_equal(exe.outputs[0].asnumpy(), np_out, rtol=1e-5)
+    assert_almost_equal(exe.outputs[0].asnumpy(), np_out, rtol=1e-2 if dtype is 'float16' else 1e-5, atol=1e-5)
-                        check_l2_normalization((nbatch, nchannel, height, width), mode)
+    for dtype in ['float16', 'float32', 'float64']:
-        symbol, data_names, label_names = sym_gen(default_bucket_key)
+        symbol, data_names, label_names = self._call_sym_gen(default_bucket_key)
-            _, data_names, _ = self._sym_gen(self._default_bucket_key)
+            _, data_names, _ = self._call_sym_gen(self._default_bucket_key)
-            symbol, _, _ = self._sym_gen(self._default_bucket_key)
+            symbol, _, _ = self._call_sym_gen(self._default_bucket_key)
-        symbol, data_names, label_names = self._sym_gen(self._default_bucket_key)
+        symbol, data_names, label_names = self._call_sym_gen(self._default_bucket_key)
-            symbol, data_names, label_names = self._sym_gen(bucket_key)
+            symbol, data_names, label_names = self._call_sym_gen(bucket_key)
-                                 output_dim=num_embedding, name='embed')
+                                 output_dim=num_embedding)
-    def load_params(self, filename, ctx, allow_missing=False,
+    def load_params(self, filename, ctx=cpu(), allow_missing=False,
-        ctx : Context or list of Context
+        ctx : Context or list of Context, default cpu()
-    """Set up the configure of profiler.
+    """Set up the configure of profiler (Deprecated).
-    warnings.warn('profiler.profiler_set_config() is deprecated. ' \
+    warnings.warn('profiler.profiler_set_config() is deprecated. '
-    warnings.warn('profiler.dump_profile() is deprecated. ' \
+    warnings.warn('profiler.dump_profile() is deprecated. '
-    activation: {'relu' or 'tanh'}, default 'tanh'
+    activation: {'relu' or 'tanh'}, default 'relu'
-    of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]
+    of fixed size. eg. [4, 20] -> [[0.25, 0.1], [0.6, -0.2]]
-        - **data**: 2D tensor with shape: `(x1, x2)`.
+        - **data**: (N-1)-D tensor with shape: `(x1, x2, ..., xN-1)`.
-        - **out**: 3D tensor with shape: `(x1, x2, output_dim)`.
+        - **out**: N-D tensor with shape: `(x1, x2, ..., xN-1, output_dim)`.
-def check_layer_normalization(in_shape, axis, eps, dtype=np.float32):
+def check_layer_normalization(in_shape, axis, eps, dtype=np.float32, forward_check_eps=1E-3):
-        std = np.sqrt(var + eps)
+        mean = data.mean(axis=axis, keepdims=True).astype(dtype)
-    assert_allclose(out, out_nd.asnumpy(), 1E-4, 1E-4)
+    assert_almost_equal(out, out_nd.asnumpy(), forward_check_eps, forward_check_eps)
-        for in_shape in [(10, 6, 5), (5, 5)]:
+    for dtype, forward_check_eps in zip([np.float16, np.float32, np.float64],
-                    check_layer_normalization(in_shape, axis, eps)
+                for eps in [1E-2, 1E-3]:
-def unittest_correlation(data_shape,kernel_size,max_displacement,stride1,stride2,pad_size,is_multiply):
+def unittest_correlation(data_shape,kernel_size,max_displacement,stride1,stride2,pad_size,is_multiply,dtype):
-    img1 = img1.astype(np.float32)
+    img1 = img1.astype(dtype)
-    img2 = img2.astype(np.float32)
+    img2 = img2.astype(dtype)
-    unittest_correlation((5,1,11,11), kernel_size = 5,max_displacement = 1,stride1 = 1,stride2 = 1,pad_size = 2,is_multiply = False)
+    for dtype in ['float16', 'float32', 'float64']:
-        self._num_workers = num_workers
+        self._num_workers = num_workers if num_workers >= 0 else 0
-            key_queue.put((None, None))
+            generator = lambda: [(yield self._batchify_fn([self._dataset[idx] for idx in batch]))
-            worker.join()
+        # multi-worker
-
+from . import onnx
-    if shape == 0:
+def check_label_shapes(labels, preds, wrap=False, shape=False):
-        check_label_shapes(labels, preds)
+        labels, preds = check_label_shapes(labels, preds, True)
-            check_label_shapes(label, pred_label)
+            labels, preds = check_label_shapes(label, pred_label)
-        check_label_shapes(labels, preds)
+        labels, preds = check_label_shapes(labels, preds, True)
-        check_label_shapes(labels, preds)
+        labels, preds = check_label_shapes(labels, preds, True)
-        check_label_shapes(labels, preds)
+        labels, preds = check_label_shapes(labels, preds, True)
-        check_label_shapes(labels, preds)
+        labels, preds = check_label_shapes(labels, preds, True)
-        check_label_shapes(labels, preds)
+        labels, preds = check_label_shapes(labels, preds, True)
-        check_label_shapes(labels, preds)
+        labels, preds = check_label_shapes(labels, preds, True)
-        check_label_shapes(labels, preds)
+        labels, preds = check_label_shapes(labels, preds, True)
-        check_label_shapes(labels, preds)
+        labels, preds = check_label_shapes(labels, preds, True)
-            check_label_shapes(label, pred, 1)
+            check_label_shapes(label, pred, False, True)
-            check_label_shapes(labels, preds)
+            labels, preds = check_label_shapes(labels, preds, True)
-    def check(target):
+    def check(target, dtype):
-        y = tvm.placeholder(shape)
+        x = tvm.placeholder(shape, dtype=dtype)
-        zz = tvm.compute(shape, lambda *i: z(*i) * scale)
+        zz = tvm.compute(shape, lambda *i: z(*i) * scale.astype(dtype))
-        zz = mx.nd.empty(shape=shape, ctx=ctx)
+        xx = mx.nd.uniform(shape=shape, ctx=ctx).astype(dtype)
-
+    for tgt in ["llvm", "cuda"]:
-            out = self._forward_gpu(inputs, states)
+        if inputs.context.device_type == 'gpu' or \
-            out = self._forward_cpu(inputs, states)
+            out = self._forward(inputs, states)
-    def _forward_cpu(self, inputs, states):
+    def _forward(self, inputs, states):
-    def _forward_gpu(self, inputs, states):
+    def _forward_kernel(self, inputs, states):
-
+    check_rnn_layer_w_rand_inputs(gluon.rnn.LSTM(100, num_layers=3, bidirectional=True))
-        "--shuffle", str(int(args.shuffle)), "--pack-label", "1"])
+    cmd_arguments = ["python",
-            assert_almost_equal(exe_test.outputs[0].asnumpy(), np.dot(np_onehot, weight.asnumpy()))
+            assert_almost_equal(exe_test.outputs[0].asnumpy(), np.dot(np_onehot, weight.asnumpy()), atol=1e-4)
-            assert_almost_equal(grad_map["embed_weight"].asnumpy(), np.dot(np_onehot.T, grad.asnumpy()))
+            assert_almost_equal(grad_map["embed_weight"].asnumpy(), np.dot(np_onehot.T, grad.asnumpy()), atol=1e-4)
-           'BatchNorm', 'InstanceNorm', 'Flatten', 'Lambda', 'HybridLambda']
+           'BatchNorm', 'InstanceNorm', 'LayerNorm', 'Flatten', 'Lambda', 'HybridLambda']
-      out = \frac{x - mean[data]}{ \sqrt{Var[data]} + \epsilon} * gamma + beta
+      \bar{C} = \{i \mid i \neq 0, i \neq axis\}
-        The axis that should be normalized. This is typically the channels
+        The axis that will be excluded in the normalization process. This is typically the channels
-        set `axis=1` in `InstanceNorm`. If `layout='NHWC'`, then set `axis=3`.
+        set `axis=1` in `InstanceNorm`. If `layout='NHWC'`, then set `axis=3`. Data will be
-        self._kwargs = {'eps': epsilon, 'axis': axis}
+        self._kwargs = {'eps': epsilon, 'axis': axis, 'center': center, 'scale': scale}
-
+        idx = 0
-            raise StopIteration
+            return
-                raise StopIteration
+                return
-    test_mkldnn_install()
+import numpy as np
-import mxnet as mx
+import mxnet as mx
-        super().__init__()
+        super(Softmax,self).__init__()
-        kernel.launch((x, y, x.shape[1], self._reqCode(req[0])), mx.gpu(0), (1, 1, 1), (x.shape[0], 1, 1))
+
-        kernel.launch((l, y, dx, self._reqCode(req[0])), mx.gpu(0), (y.shape[0], 1, 1), (y.shape[1], 1, 1))
+
-            inputs = F.broadcast_mul(inputs, self.drop_inputs_mask.expand_dims(axis=axis))
+            inputs = F.Dropout(inputs, p=self.drop_inputs, axes=(axis,))
-            outputs = F.broadcast_mul(outputs, self.drop_outputs_mask.expand_dims(axis=axis))
+            outputs = F.Dropout(outputs, p=self.drop_outputs, axes=(axis,))
-    def __init__(self, rate, **kwargs):
+    def __init__(self, rate, axes=(), **kwargs):
-        return F.Dropout(x, p=self._rate, name='fwd')
+        return F.Dropout(x, p=self._rate, axes=self._axes, name='fwd')
-        s = '{name}(p = {_rate})'
+        s = '{name}(p = {_rate}, axes={_axes})'
-    def __init__(self, rate, prefix=None, params=None):
+    def __init__(self, rate, axes=(), prefix=None, params=None):
-        self.rate = rate
+        self._rate = rate
-        s = '{name}(rate = {rate})'
+        s = '{name}(rate={_rate}, axes={_axes})'
-            inputs = F.Dropout(data=inputs, p=self.rate, name='t%d_fwd'%self._counter)
+        if self._rate > 0:
-   
+
-    check_dropout_axes(0.25, nshape, axes = (1, 2, 3))
+    with mx.autograd.train_mode():
-from ...rnn.rnn_cell import _format_sequence, _get_begin_state
+from ...rnn.rnn_cell import _format_sequence, _get_begin_state, _mask_sequence_variable_length
-    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None,
-                                                              layout, merge_outputs)
+                                                              layout, merge_outputs,
-        outputs, states = self.base_cell.unroll(length, inputs, states, layout, merge_outputs=True)
+        outputs, states = self.base_cell.unroll(length, inputs, states, layout, merge_outputs=True,
-
+        merge_outputs = isinstance(outputs, tensor_types) if merge_outputs is None else \
-
+        if valid_length is not None:
-            inputs = F.concat(*inputs, dim=axis)
+            inputs = F.stack(*inputs, axis=axis)
-    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None,
-        inputs, _, F, batch_size = _format_sequence(length, inputs, layout, False)
+        inputs, axis, F, batch_size = _format_sequence(length, inputs, layout, False)
-
+            if valid_length is not None:
-    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None,
-                                         merge_outputs=None if i < num_cells-1 else merge_outputs)
+            inputs, states = cell.unroll(length, inputs=inputs, begin_state=states,
-    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None,
-                merge_outputs=merge_outputs)
+                merge_outputs=merge_outputs, valid_length=None)
-    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None,
-                                                layout=layout, merge_outputs=merge_outputs)
+                                                layout=layout, merge_outputs=merge_outputs,
-        inputs, _, F, _ = _format_sequence(length, inputs, layout, merge_outputs)
+        inputs, axis, F, _ = _format_sequence(length, inputs, layout, merge_outputs)
-    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None,
-                                            layout=layout, merge_outputs=merge_outputs)
+                                            layout=layout, merge_outputs=merge_outputs,
-                                            inputs=list(reversed(inputs)),
+                                            inputs=reversed_inputs,
-
+                                            layout=layout, merge_outputs=False,
-                             and isinstance(r_outputs, tensor_types))
+            merge_outputs = isinstance(l_outputs, tensor_types)
-            r_outputs, _, _, _ = _format_sequence(None, r_outputs, layout, merge_outputs)
+            reversed_r_outputs, _, _, _ = _format_sequence(None, reversed_r_outputs, layout,
-            outputs = F.concat(l_outputs, r_outputs, dim=2, name='%sout'%self._output_prefix)
+            reversed_r_outputs = F.stack(*reversed_r_outputs, axis=axis)
-
+                       for i, (l_o, r_o) in enumerate(zip(l_outputs, reversed_r_outputs))]
-.. _ResNet V2: https://arxiv.org/abs/1512.03385
+.. _ResNet V2: https://arxiv.org/abs/1603.05027
-            downloaded_file_path = download(_get_repo_file_url(self._namespace, archive_file_name),
+            namespace = 'gluon/dataset/'+self._namespace
-        data_file = download(_get_repo_file_url(self._namespace, data[0]),
+        namespace = 'gluon/dataset/'+self._namespace
-        label_file = download(_get_repo_file_url(self._namespace, label[0]),
+        label_file = download(_get_repo_file_url(namespace, label[0]),
-            filename = download(_get_repo_file_url(self._namespace, self._archive_file[0]),
+            namespace = 'gluon/dataset/'+self._namespace
-    """Return the URL for hoste file in Gluon repository.
+    """Return the URL for hosted file in Gluon repository.
-                                                                   filename=filename)
+    return '{base_url}{namespace}/{filename}'.format(base_url=_get_repo_url(),
-        Returns a sliced view of this array.
+        Returns a newly created NDArray based on the indexing key.
-from ...utils import download, check_sha1
+from ...utils import download, check_sha1, _get_repo_file_url
-    def __init__(self, repo_dir, root, vocabulary):
+    def __init__(self, root, namespace, vocabulary):
-        super(_LanguageModelDataset, self).__init__(repo_dir, root, None)
+        self._namespace = namespace
-            downloaded_file_path = download(self._get_url(archive_file_name),
+            downloaded_file_path = download(_get_repo_file_url(self._namespace, archive_file_name),
-        data, label = self._read_batch(os.path.join(self._root, data_file_name))
+        data, label = self._read_batch(path)
-        super(WikiText2, self).__init__('wikitext-2', root, vocab)
+        super(WikiText2, self).__init__(root, 'wikitext-2', vocab)
-        super(WikiText103, self).__init__('wikitext-103', root, vocab)
+        super(WikiText103, self).__init__(root, 'wikitext-103', vocab)
-        self._repo_dir = repo_dir
+    def __init__(self, root, transform):
-            os.makedirs(self._root)
+        root = os.path.expanduser(root)
-from ...utils import download, check_sha1
+from ...utils import download, check_sha1, _get_repo_file_url
-        super(MNIST, self).__init__('mnist', root, transform)
+        self._namespace = 'mnist'
-        data_file = download(self._get_url(data[0]),
+        data_file = download(_get_repo_file_url(self._namespace, data[0]),
-        label_file = download(self._get_url(label[0]),
+        label_file = download(_get_repo_file_url(self._namespace, label[0]),
-        super(MNIST, self).__init__('fashion-mnist', root, transform) # pylint: disable=bad-super-call
+        self._namespace = 'fashion-mnist'
-        super(CIFAR10, self).__init__('cifar10', root, transform)
+        self._namespace = 'cifar10'
-            filename = download(self._get_url(self._archive_file[0]),
+            filename = download(_get_repo_file_url(self._namespace, self._archive_file[0]),
-        super(CIFAR10, self).__init__('cifar100', root, transform) # pylint: disable=bad-super-call
+        self._namespace = 'cifar100'
-        fname = path
+        path = os.path.expanduser(path)
-            The row_ids for which to pull for each value. Each row_id is an 1D NDArray \
+            The row_ids for which to pull for each value. Each row_id is an 1-D NDArray \
-            row_ids = mx.nd.array(row_ids_np, dtype='int64')
+            row_ids = mx.nd.array(row_ids_np).reshape((num_rows/2, 2))
-            row_ids = mx.nd.array(row_ids_np)
+            row_ids = mx.nd.array(row_ids_np).reshape((num_rows/2, 2))
-                row_ids = [mx.nd.array(row_id, dtype='int64')] * count
+                row_ids = [mx.nd.array(row_id)] * count
-                total_row_ids = mx.nd.array(np.random.randint(num_rows, size=count*num_rows), dtype='int64')
+                total_row_ids = mx.nd.array(np.random.randint(num_rows, size=count*num_rows))
-                    row_ids.append(mx.nd.array(row_id, dtype='int64'))
+                    row_ids.append(mx.nd.array(row_id))
-    test_rsp_push_pull()
+    import nose
-            row_ids.append(mx.nd.array(row_id))
+            row_ids.append(mx.nd.array(row_id).reshape((2, num_rows//2)))
-            assert_almost_equal(exe.grad_arrays[0].asnumpy(), X.astype(dsttype).astype(srctype), rtol=1e-3)
+            assert_almost_equal(exe.outputs[0].asnumpy(), X.astype(srctype).astype(dsttype), rtol=1e-3, atol=1e-5)
-    return train, val
+def get_imagenet_transforms(data_shape=224, dtype='float32'):
-import argparse, time
+import argparse, time, os
-logging.basicConfig(level=logging.INFO)
+from mxnet.metric import Accuracy, TopKAccuracy, CompositeEvalMetric
-                    help='validation record file to use, required for imagenet.')
+                    help='dataset to use. options are mnist, cifar10, imagenet and dummy.')
-parser.add_argument('--epochs', type=int, default=3,
+parser.add_argument('--num-worker', '-j', dest='num_workers', default=4, type=int,
-parser.add_argument('-momentum', type=float, default=0.9,
+parser.add_argument('--lr', type=float, default=0.1,
-parser.add_argument('--log-interval', type=int, default=50, help='Number of batches to wait before logging.')
+parser.add_argument('--log-interval', type=int, default=50,
-
+# global variables
-
+model_name = opt.model
-
+context = [mx.gpu(int(i)) for i in opt.gpus.split(',')] if opt.gpus.strip() else [mx.cpu()]
-context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]
+lr_steps = [int(x) for x in opt.lr_steps.split(',') if x.strip()]
-model_name = opt.model
+def get_model(model, ctx, opt):
-    kwargs['batch_norm'] = opt.batch_norm
+    net = models.get_model(model, **kwargs)
-net = models.get_model(opt.model, **kwargs)
+net = get_model(opt.model, context, opt)
-    # get dataset iterators
+    """get dataset iterators"""
-                                                         num_parts=num_workers, part_index=rank)
+            train_data, val_data = get_imagenet_iterator(opt.data_dir, batch_size, opt.num_workers, 299, opt.dtype)
-                                                         num_parts=num_workers, part_index=rank)
+            train_data, val_data = get_imagenet_iterator(opt.data_dir, batch_size, opt.num_workers, 224, opt.dtype)
-    metric = mx.metric.Accuracy()
+    metric.reset()
-def train(epochs, ctx):
+def train(opt, ctx):
-    net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)
+    net.collect_params().reset_ctx(ctx)
-                            {'learning_rate': opt.lr, 'wd': opt.wd, 'momentum': opt.momentum},
+                            {'learning_rate': opt.lr, 'wd': opt.wd, 'momentum': opt.momentum,
-    for epoch in range(epochs):
+    best_acc = [0]
-            label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)
+            data = gluon.utils.split_and_load(batch.data[0].astype(opt.dtype), ctx_list=ctx, batch_axis=0)
-                    L.backward()
+                ag.backward(Ls)
-                               epoch, i, batch_size/(time.time()-btic), name, acc))
+                logger.info('Epoch[%d] Batch [%d]\tSpeed: %f samples/sec\t%s=%f, %s=%f'%(
-        logging.info('[Epoch %d] time cost: %f'%(epoch, time.time()-tic))
+        logger.info('[Epoch %d] training: %s=%f, %s=%f'%(epoch, name[0], acc[0], name[1], acc[1]))
-        logging.info('[Epoch %d] validation: %s=%f'%(epoch, name, val_acc))
+        logger.info('[Epoch %d] validation: %s=%f, %s=%f'%(epoch, name[0], val_acc[0], name[1], val_acc[1]))
-    net.save_params('image-classifier-%s-%d.params'%(opt.model, epochs))
+        # save model if meet requirements
-                optimizer_params = {'learning_rate': opt.lr, 'wd': opt.wd, 'momentum': opt.momentum},
+                optimizer_params = {'learning_rate': opt.lr, 'wd': opt.wd, 'momentum': opt.momentum, 'multi_precision': True},
-        train(opt.epochs, context)
+        train(opt, context)
-
+    ''' test regression operator '''
-                     lambda x, y : x - y)
+                     lambda x, y : x - y,
-                     lambda x, y : x - y)
+                     lambda x, y : x - y,
-                     lambda x, y : np.where(x > y, np.ones(x.shape), -np.ones(x.shape)))
+                     lambda x, y : np.where(x > y, np.ones(x.shape), -np.ones(x.shape)),
-from .ndarray import op
+    See Also
-            grad = clip(grad, -self.clip_gradient, self.clip_gradient)
+        is_sparse = weight.stype == 'row_sparse' and grad.stype == 'row_sparse'
-            assert len(history_indices) == grad_indices_count
+            kwargs = {'epsilon': self.float_stable_eps,
-                            "model that maked it only use a subset of the Parameters (Blocks) "
+                            "model that made it only use a subset of the Parameters (Blocks) "
-        assert_almost_equal(grad_map["embed_weight"].asnumpy(), np.dot(data_onehot.T, grad.asnumpy()))
+    ''' test sparse embedding operator '''
-        check_sparse_embedding(exe_test, weight, np_onehot, grad, density)
+    check_sparse_embedding(in_dim, out_dim, batch, densities, True)
-        max_val = np.max(out)
+        max_val = np.max(np.abs(out))
-        assert_almost_equal(cpu_out.asnumpy(), gpu_out.asnumpy(), rtol=1e-2, atol=1e-2)
+        max_val = np.max(np.abs(cpu_out.asnumpy()))
-nbatch_train = len(train_dataset) / args.batch_size
+nbatch_train = len(train_dataset) // args.batch_size
-nbatch_val = len(val_dataset) / args.batch_size
+nbatch_val = len(val_dataset) // args.batch_size
-nbatch_test = len(test_dataset) / args.batch_size
+nbatch_test = len(test_dataset) // args.batch_size
-    profile_filename = 'test_profile.json'
+def enable_profiler(profile_filename, run=True, continuous_dump=False, aggregate_stats=False):
-    enable_profiler(False, False)
+    enable_profiler('test_profiler.json', False, False)
-    enable_profiler()
+    enable_profiler('test_profile_create_domain.json')
-    profiler.profiler_set_config(mode='symbolic', filename='temp.json')
+    profiler.profiler_set_config(mode='symbolic', filename='test_profile_create_domain_dept.json')
-    enable_profiler()
+    enable_profiler('test_profile_task.json')
-    enable_profiler()
+    enable_profiler('test_profile_frame.json')
-      enable_profiler()
+        enable_profiler('test_profile_event.json')
-    enable_profiler()
+    enable_profiler('test_profile_tune_pause_resume.json')
-      enable_profiler()
+        enable_profiler('test_profile_counter.json')
-    enable_profiler(True, True, True)
+    file_name = 'test_continuous_profile_and_instant_marker.json'
-        new_file_size = os.path.getsize("test_profile.json")
+        new_file_size = os.path.getsize(file_name)
-def verify_generator(generator, buckets, probs, nsamples=1000000, nrepeat=5, success_rate=0.25):
+def verify_generator(generator, buckets, probs, nsamples=1000000, nrepeat=5, success_rate=0.15):
-           'tensor_types']
+__all__ = ['DeferredInitializationError', 'Parameter', 'Constant',
-            name = klass.__name__.lower()
+            name = klass.__name__
-        raw_data = self.vocabulary.to_indices([x for x in line for line in raw_data if x])
+        raw_data = self.vocabulary.to_indices([x for line in raw_data for x in line if x])
-    assert len(wikitext2_test) == 15941
+    assert len(wikitext2_train) == 59305,  len(wikitext2_train)
-                print("Fixing" + os.path.join(path, name))
+                print("Fixing " + os.path.join(path, name))
-                self.true_negatives += 1.
+        self.true_positives += (pred_true * label_true).sum()
-            return self.true_positives / (self.true_positives + self.false_positives)
+            return float(self.true_positives) / (self.true_positives + self.false_positives)
-            return self.true_positives / (self.true_positives + self.false_negatives)
+            return float(self.true_positives) / (self.true_positives + self.false_negatives)
-                return y * (1-y)
+                return dy * y * (1-y)
-from .utils import load, save, zeros, empty, array
+from .utils import load, load_frombuffer, save, zeros, empty, array
-__all__ = ['zeros', 'empty', 'array', 'load', 'save']
+__all__ = ['zeros', 'empty', 'array', 'load', 'load_frombuffer', 'save']
-from common import setup_module, with_seed
+from common import setup_module, with_seed, assertRaises, TemporaryDirectory
-           "ones", "add", "arange", "divide", "equal", "full", "greater", "greater_equal",
+           "ones", "add", "arange", "eye", "divide", "equal", "full", "greater", "greater_equal",
-           "pow", "maximum", "minimum", "hypot", "zeros", "ones", "full", "arange"]
+           "pow", "maximum", "minimum", "hypot", "eye", "zeros", "ones", "full", "arange"]
-    # Enable USE_MKL2017_EXPERIMENTAL for better CPU performance
+    # Enable USE_MKLDNN for better CPU performance
-MKLML related test cases
+MKL-DNN related test cases
-def test_mklml_install():
+def test_mkldnn_install():
-    the mxnet module and see if the mklml library is mapped to this 
+    compiled with Intel MKL-DNN library. The method will try to import 
-        logging.info("Bypass mklml install test for non-Linux OS")
+        logging.info("Bypass mkldnn install test for non-Linux OS")
-                       "/maps | grep libmklml_ > /dev/null")
+                       "/maps | grep libmkldnn > /dev/null")
-        logging.info("MXNet is built/installed correctly with MKLML")
+        logging.info("MXNet is built/installed correctly with MKL-DNN")
-        assert 0, "MXNet is built/installed incorrectly with MKLML, please " \
+        assert 0, "MXNet is built/installed incorrectly with MKL-DNN, please " \
-    test_mklml_install()
+    test_mkldnn_install()
-    root : str, default '~/.mxnet/datasets/cifar10'
+    root : str, default '~/.mxnet/datasets/wikitext-2'
-    root : str, default '~/.mxnet/datasets/cifar10'
+    root : str, default '~/.mxnet/datasets/wikitext-103'
-    'relacy_shims.h'
+    'relacy_shims.h', 'ittnotify.h', 'shared_mutex'
-from .base import _LIB, check_call, c_str
+import warnings
-        c_str(filename)))
+    warnings.warn('profiler.profiler_set_config() is deprecated. ' \
-    """Set up the profiler state to record operator.
+def set_state(state='stop'):
-    check_call(_LIB.MXDumpProfile())
+    warnings.warn('profiler.dump_profile() is deprecated. ' \
-    print('profile file save to {0}'.format(profile_filename))
+    enable_profiler(False, False)
-            profiler.profiler_set_state('run')
+            profiler.set_state('run')
-            profiler.profiler_set_state('stop')
+            profiler.set_state('stop')
-    test_profiler()
+    import nose
-
+import math
-__all__ = []
+__all__ = ["rand_zipfian"]
-__all__ = []
+__all__ = ["rand_zipfian"]
-               '3rdparty',   
+               '3rdparty',
-    return any([any([p in l.decode('utf-8') for p in _LICENSE_PATTERNS]) for l in lines])
+    return any([any([p in l for p in _LICENSE_PATTERNS]) for l in lines])
-            print('skip ' + fname + ', it matches the white list')
+            logging.info('skip ' + fname + ', it matches the white list')
-            print('skip ' + fname + ', unknown file extension')
+            logging.info('skip ' + fname + ', unknown file extension')
-    with open(fname, 'rb') as f:
+    with open(fname, 'r', encoding="utf-8") as f:
-    with open(fname, 'wb') as f:
+    with open(fname, 'w', encoding="utf-8") as f:
-            f.write(lines[0].rstrip()+b'\n\n')
+        if lines[0].startswith('#!'):
-        f.write(str.encode(_get_license(_LANGS[ext])))
+        f.write(_get_license(_LANGS[ext]))
-    return False
+            f.write(l.rstrip()+'\n')
-                        'them automatically', excepts)
+        logging.warning('The following files do not contain a valid license, '+
-    process_folder(os.path.join(os.path.dirname(__file__), '..'), args.action[0])
+    files = list(chain(*args.file))
-              'mobilenet0.25': mobilenet0_25
+              'mobilenet0.25': mobilenet0_25,
-            'Model %s is not supported. Available options are\n\t%s'%(
+            'Model %s is not supported. Available options are\n\t%s' % (
-           'get_mobilenet']
+"""MobileNet and MobileNetV2, implemented in Gluon."""
-from ... import nn
+
-def _add_conv(out, channels=1, kernel=1, stride=1, pad=0, num_group=1):
+# pylint: disable= too-many-arguments
-    out.add(nn.Activation('relu'))
+    if active:
-                channels = [int(x*multiplier) for x in [64]+[128]*2+[256]*2+[512]*6+[1024]*2]
+                _add_conv(self.features, channels=int(32 * multiplier), kernel=3, pad=1, stride=2)
-        net.load_params(get_model_file('mobilenet%s'%version_suffix, root=root), ctx=ctx)
+        net.load_params(
-                  'mobilenet1.0', 'mobilenet0.75', 'mobilenet0.5', 'mobilenet0.25']
+                  'mobilenet1.0', 'mobilenet0.75', 'mobilenet0.5', 'mobilenet0.25',
-        eprint('testing forward for %s'%model_name)
+        eprint('testing forward for %s' % model_name)
-__version__ = "1.1.0"
+__version__ = "1.2.0"
-    {'wiki.ab.vec': '9d89a403a9a866d3da8dd8cfab849f59ee499343',
+    {'crawl-300d-2M.vec': '9b556504d099a6c01f3dd76b88775d02cb2f1946',
-     'wiki.as.vec': 'cad5883b5147cbe6cdbf604f65cabdb675a59258',
+     'wiki.ar.vec': 'c46e2142f799cc385bd25f0c0a8943ca565505a4',
-     'wiki.eu.vec': '5e72f4ef93666971fea5d2180b354e0a0821ba91',
+     'wiki.bat_smg.vec': 'cb3aef58da2011183b39fca64cabf3d9d7a62f4b',
-     'wiki.bn.vec': '6fc3bfd9af455719f55bee0bea31b11afc70cf06',
+     'wiki.bg.vec': '7c1cc6d0c52b038e4b7173259b0c009f242cf486',
-     'wiki.bs.vec': 'c4943a290819ceae1611dd11179b40aab0df0471',
+     'wiki.bjn.vec': '5f134cf288e8042dcd048a3ee76159aab42c7288',
-     'wiki.ch.vec': '46803f3a1734f6a7b0d8cb053bbb86a6915d02e9',
+     'wiki.cdo.vec': '95e8196bf76323dbabab1b8a49ba4d677af3ccea',
-     'wiki.kw.vec': 'f9eaa35a7e4f077f6de85c7801f74582f91b52c1',
+     'wiki.ckb.vec': 'adb2fef309f1d93f429442b9c16c1564192c58f3',
-     'wiki.hr.vec': '0c96f9af092cf8a84b03aec1426cd23921671489',
+     'wiki.cr.vec': '61dd9f044b7dfa56dcf1c3c07c7504c569420528',
-     'wiki.arz.vec': '5e904087043b91f4945dd708f4230fdf51360132',
+     'wiki.dz.vec': '24888f0b2cd156360bfb5e9e905240163ba798d8',
-     'wiki.myv.vec': '7de0927fd3d65677de7f770b3bd57c73b58df85d',
+     'wiki.es.vec': '2f41401aa0925167176bcd7a6770423d891dfef5',
-     'wiki.ee.vec': 'f2212f58ec082321bc9b93873cd22702d0a64d64',
+     'wiki.eu.vec': '5e72f4ef93666971fea5d2180b354e0a0821ba91',
-     'wiki.fj.vec': 'c70fca34a7e43143600c54d7bf199b88846ac6f2',
+     'wiki.fa.vec': '09b6cc685c895c66b853af9617787d3ab0891e2c',
-     'wiki.ff.vec': '57ea8febb24ba8ac4421ec97ed8918d44c69f42c',
+     'wiki.fy.vec': 'd4beef537b7ff142a3986513879ff51a9ec14a7b',
-     'wiki.de.vec': '2ed2696afe55f023b0040b238d9a47e5fedfe48b',
+     'wiki.gan.vec': '7e53a33b7bd5b0360ea4cb452145616c09445029',
-     'wiki.kl.vec': '390406cc33e02f86cfaf7ae273193679924f7413',
+     'wiki.gl.vec': '8888bb8f3d70b36729b9ae479fe3765e0c083862',
-     'wiki.ht.vec': '5039dfb58a074ac046813f2dae81159be8c5213f',
+     'wiki.gv.vec': '993a7ee31bdacc91763dad656aa6c2947b873473',
-     'wiki.haw.vec': 'c23a50529dc010401c99833c8f990c1b27843db3',
+     'wiki.haw.vec': '58fea5aa1b37723797d26fb3d050ce6176757240',
-     'wiki.mrj.vec': 'aa1c1ecba1ffd6b42c8d9659a8a04ab328ae1650',
+     'wiki.hif.vec': '49697cf784814d3f1a47559724028e0fc0940d36',
-     'wiki.ho.vec': 'ef6b84d508d4d0a4c4cf90facaca1eebe62b2187',
+     'wiki.ho.vec': '9c75a09e099213aa8cd1f1020b223427537cbdd8',
-     'wiki.id.vec': 'c49d5c9bec89114599427f6c12a5bda2e5523dfd',
+     'wiki.hy.vec': '21f9259d04cfd22db446a45d3622af225f00cf20',
-     'wiki.ga.vec': 'caaa5b2167a499893313ac1aa38416a6a0fe9a24',
+     'wiki.ig.vec': 'd2d1643b4fb1a18a4d002cf2969073f7f201b3b2',
-     'wiki.csb.vec': '649cb2692f08414987c875dc331022567d367497',
+     'wiki.kab.vec': 'e3b73d41267d8d4cd42f6cc5a0c05dc4e021bf74',
-     'wiki.kv.vec': '164dc44d701b9d606a45f0b0446076adc3858dca',
+     'wiki.kn.vec': '32763f4f860f0d081f3aabf3e7d17b7858e7d877',
-     'wiki.kj.vec': 'adf29c1a3aa5dd53d85e04d68aa11a26c0eaf6c8',
+     'wiki.krc.vec': '0c6ef043d51e5f337a309804f1db180fa0bb2cb8',
-     'wiki.ckb.vec': 'adb2fef309f1d93f429442b9c16c1564192c58f3',
+     'wiki.kv.vec': '164dc44d701b9d606a45f0b0446076adc3858dca',
-     'wiki.lv.vec': 'ef6b549f96e22718f513d47a611d3d6bc001a164',
+     'wiki.lbe.vec': '283619d93255571f14fd4545bb0577979171b990',
-     'wiki.mk.vec': '85a3d3f13fa88ffde023d2326c65bdded4983dff',
+     'wiki.lv.vec': 'ef6b549f96e22718f513d47a611d3d6bc001a164',
-     'wiki.zh_min_nan.vec': 'f91ccb013e200bb7ed560082ddf4bdd9c2f315bb',
+     'wiki.mh.vec': '7d2d8bff722fe0a5d869d9da11792a406aff3dc3',
-     'wiki.mo.vec': '9824ebe366bc52d84e66d1c0cc72b5f7ebb46110',
+     'wiki.mi.vec': 'e8acf9c7c2ab840a192c563aa776201a88e4ca89',
-     'wiki.mus.vec': 'bb94534fdeee4df77ae3e27c252c8874f69a307d',
+     'wiki.mo.vec': 'cc54b661aefabdf516b49d24acb51273b3acf210',
-     'wiki.ng.vec': '8577634e236133980243be0a6fb3c02ad2bb5290',
+     'wiki.na.vec': '8a592eb3dbe5693372714dff495d01cabc3ea215',
-     'wiki.no.vec': 'd52e8019d7cc48569c8c3b514d2b1bd10261b5c0',
+     'wiki.ng.vec': 'c3016cc07d40bd43bea84b7c600244ff3d2a928e',
-     'wiki.ii.vec': '755a6b8ffa664e342c2ab72847af343c47f46c70',
+     'wiki.nrm.vec': 'b4cb941b126b26fa045c5fc75a490a31a969101c',
-     'wiki.or.vec': 'a6b120fe536b6c0133b077dca0043c3bc97eef0b',
+     'wiki.olo.vec': 'cbadb4cada4dc579d0becdac93dfb479d76bf6c8',
-     'wiki.pi.vec': '07a5d05e5363e8b8b132220a71de4bdc0a623cfc',
+     'wiki.pam.vec': '8fbd31e70d0ca0c61eb1a152efaa8ecb29180967',
-     'wiki.fa.vec': '09b6cc685c895c66b853af9617787d3ab0891e2c',
+     'wiki.pa.vec': '4939d0db77a5b28d7d5aab0fab4f999d93b2053e',
-     'wiki.pms.vec': 'e30bda8d33d61db43243c157b9ac2feeaff316c8',
+     'wiki.pdc.vec': '401e24d0fb9b0ae9e06a5c700684361f58727fcf',
-     'wiki.pnt.vec': 'a9efbf962a895e1d08dde5fd56797dd03abb421e',
+     'wiki.pms.vec': 'e30bda8d33d61db43243c157b9ac2feeaff316c8',
-     'wiki.ru.vec': '7514a2c60ee4118abb451ed32a0d61cb52dec384',
+     'wiki.rmy.vec': '3d36d3485961900c23355a0f7c2ba656a8558c29',
-     'wiki.stq.vec': '1bf88af29f1d86cac16042a5bea6b1651c96a8c1',
+     'wiki.scn.vec': 'bde043a235551e1643506774c5d9b61ecf2fc424',
-     'wiki.sr.vec': '3cf09f476f55a92fdd2880f7ba336656ab232736',
+     'wiki.sc.vec': 'dba8dc7754ef04b1ba0cd702d94eea9575cde91c',
-     'wiki.sd.vec': '36852d1253496e598fbd9b9009f07f454a6bea5b',
+     'wiki.sm.vec': '88c2c57ca483626b052403418cb4372d72352bc9',
-     'wiki.es.vec': '2f41401aa0925167176bcd7a6770423d891dfef5',
+     'wiki.sq.vec': 'd07ffed553f5eb4756d0a1548a7ba9a51a52f7c6',
-     'wiki.sw.vec': '8e70d207dbbd14e60a48e260a23fbf284a8e9f06',
+     'wiki.sr.vec': '3cf09f476f55a92fdd2880f7ba336656ab232736',
-     'wiki.tg.vec': '6a5cd5bfe571ca0359b66d21bf6950553213f42d',
+     'wiki.sw.vec': '8e70d207dbbd14e60a48e260a23fbf284a8e9f06',
-     'wiki.te.vec': 'e71dcf3cc45da1bcdae5e431324025bd2026d0c8',
+     'wiki.tcy.vec': '388b1d89642fcc790b688e9643b3d19e14d66f40',
-     'wiki.tpi.vec': '407b96d235f54f3e0be9dc23a3bab89c6593a621',
+     'wiki.tk.vec': '33ae577f77d339ab7a0dff88855b8d5c974d0aef',
-     'wiki.tcy.vec': '388b1d89642fcc790b688e9643b3d19e14d66f40',
+     'wiki.tt.vec': '913bb3a11da6f8142b3bbec3ef065162d9350f1d',
-     'wiki.tyv.vec': 'e8f9a36dc58e4108c553f96e247a877a099ab5ba',
+     'wiki.ty.vec': 'b881f60b8c75a71864d9847a17961d368f3058fc',
-     'wiki.ve.vec': 'b7d2947501de1c30a9f8496d5efae20c051104e1',
+     'wiki.ve.vec': 'b7d2947501de1c30a9f8496d5efae20c051104e1',
-     'wiki.pnb.vec': '35f38862d3d83012d6db7baa8a4105e3e0a416e7',
+     'wiki.wa.vec': '18f9ca1a585e1d18c3630029141a2e19d7d34a8e',
-     'crawl-300d-2M.vec': '9b556504d099a6c01f3dd76b88775d02cb2f1946'}
+     'wiki.zea.vec': 'ee12db26aab3f2b3b2745a298ef414e7aeb5a058',
-        embedding_name='fasttext')) == 297
+        embedding_name='fasttext')) == 327
-    assert len(reg['fasttext']) == 297
+    assert len(reg['fasttext']) == 327
-                cur_param = pre_filter * int(node["attrs"]["num_filter"])
+            if "no_bias" in node["attrs"] and node["attrs"]["no_bias"] == 'True':
-                cur_param = pre_filter * int(node["attrs"]["num_filter"])
+                num_group = int(node['attrs'].get('num_group', '1'))
-                cur_param = pre_filter * (int(node["attrs"]["num_hidden"]))
+            if "no_bias" in node["attrs"] and node["attrs"]["no_bias"] == 'True':
-                cur_param = (pre_filter+1) * (int(node["attrs"]["num_hidden"]))
+                cur_param = (pre_filter+1) * int(node["attrs"]["num_hidden"])
-from util import get_data, estimate_density
+from mxnet.test_utils import get_bz2_data
-        get_data(
+        get_bz2_data(
-    def reshape(self, shape):
+    def reshape(self, *shape, **kwargs):
-        shape : tuple of int
+        shape : tuple of int, or n ints
-    def reshape(self, shape):
+    def reshape(self, *shape, **kwargs):
-    def __init__(self, epsilon=1e-5, center=True, scale=False,
+    def __init__(self, axis=1, epsilon=1e-5, center=True, scale=False,
-        self._kwargs = {'eps': epsilon}
+        self._kwargs = {'eps': epsilon, 'axis': axis}
-                              name='fwd', **self._kwargs)
+        if self._axis == 1:
-    """Pads the input tensor using the reflection of the input boundary.
+    r"""Pads the input tensor using the reflection of the input boundary.
-          :math:`W_{out} = W_{in} + 2 * padding
+
-        plain = _rng.rand(*shape) + 0.1
+        plain = np.random.rand(*shape) + 0.1
-                         + [("__random_proj", _rng.normal(0, 0.01, size=out_shape[0]))])
+    args_grad_npy = dict([(k, np.random.normal(0, 0.01, size=location[k].shape))
-    args_grad_npy = {k:_rng.normal(size=v.shape) for k, v in expected.items()}
+    args_grad_npy = {k:np.random.normal(size=v.shape) for k, v in expected.items()}
-        location = {k: _rng.normal(size=arr.shape, scale=1.0) for k, arr in
+        location = {k: np.random.normal(size=arr.shape, scale=1.0) for k, arr in
-
+# Test seed 89411477 (module seed 1829754103) resulted in a py3-gpu CI runner core dump.
-    np.random.seed(0)
+
-
+@with_seed(0)
-    np.random.seed(0)
+
-    np.random.seed(0)
+
-    np.random.seed(1234)
+
-    np.random.seed(1234)
+
-    np.random.seed(1234)
+
-    np.random.seed(1234)
+@with_seed()
-
+@with_seed()
-    np.random.seed(1234)
+
-    np.random.seed(1234)
+
-    np.random.seed(1234)
+
-    assert_almost_equal(go.asnumpy(), co.asnumpy(), rtol=1e-2, atol=1e-8)
+    # atol of 1e-6 required, as exposed by seed 2124685726
-
+        assert_almost_equal(g.asnumpy(), c.asnumpy(), rtol=1e-2, atol=1e-6)
-import sys, os
+import sys, os, logging
-
+@with_seed()
-    assert_almost_equal(y.grad.asnumpy(), dy1)
+    # Non-zero atol required, as exposed by seed 630179191
-        np.random.seed(0)
+
-    np.random.seed(0)
+@with_seed()
-
+@with_seed(1234)
-    np.random.seed(1234)
+@with_seed(1234)
-    np.random.seed(1234)
+@with_seed()
-    np.random.seed(1234)
+@with_seed(1234)
-    np.random.seed(1234)
+@with_seed(1234)
-    np.random.seed(1234)
+@with_seed()
-    np.random.seed(1234)
+@with_seed(1234)
-    np.random.seed(1234)
+@with_seed(1234)
-    np.random.seed(1234)
+@with_seed(1234)
-    np.random.seed(1234)
+@with_seed(1234)
-    np.random.seed(1234)
+@with_seed(1234)
-    np.random.seed(1234)
+@with_seed(1234)
-from common import assertRaises
+from common import setup_module, with_seed, assertRaises
-
+@with_seed()
-    mx.random.seed(11)
+@with_seed(11)
-    mx.random.seed(11)
+@with_seed()
-
+@with_seed(11)
-        rnd.seed(11)
+@with_seed()
-
+@with_seed()
-    np.random.seed(0)
+@with_seed()
-    np.random.seed(0)
+@with_seed()
-    np.random.seed(0)
+
-    assert_almost_equal(c, C.asnumpy())
+    assert_almost_equal(c, C.asnumpy(), atol=atol)
-    assert_almost_equal(c, C.asnumpy())
+    assert_almost_equal(c, C.asnumpy(), atol=atol)
-    assert_almost_equal(c, C.asnumpy())
+    assert_almost_equal(c, C.asnumpy(), atol=atol)
-
+    assert_almost_equal(c, C.asnumpy(), atol=atol)
-    test_reduce_inner(lambda data, axis, keepdims:np_reduce(data, axis, keepdims, np.argmax),
+    # argmax and argmin are sensitive to the precision of the calculation (repro seed 1985162693).
-    test_reduce_inner(lambda data, axis, keepdims:np_reduce(data, axis, keepdims, np.argmin),
+    test_reduce_inner(lambda data, axis,
-    check_broadcast_binary(lambda x, y: x == y)
+    # The following ops are sensitive to the precision of the calculation.
-def test_order(ctx=default_context()):
+@with_seed()
-            assert dat.shape == (5, 5, 5, 5)
+            assert dat.shape == (dat_size, dat_size, dat_size, dat_size)
-                        for k in range(5):
+                for i in range(dat_size):
-    a_npy = np.random.normal(size=(5, 5, 5, 5))
+
-    gt = gt_topk(a_npy, axis=1, ret_typ="value", k=5, is_ascend=True)
+    gt = gt_topk(a_npy, axis=1, ret_typ="value", k=dat_size, is_ascend=True)
-    gt = gt_topk(a_npy, axis=None, ret_typ="value", k=5*5*5*5, is_ascend=False)
+    gt = gt_topk(a_npy, axis=None, ret_typ="value",
-    gt = gt_topk(a_npy, axis=3, ret_typ="indices", k=5, is_ascend=True)
+    gt = gt_topk(a_npy, axis=3, ret_typ="indices", k=dat_size, is_ascend=True)
-    gt = gt_topk(a_npy, axis=None, ret_typ="indices", k=5*5*5*5, is_ascend=False)
+    gt = gt_topk(a_npy, axis=None, ret_typ="indices",
-    np.random.seed(0)
+@with_seed()
-    assert_almost_equal(npout, out1)
+    # Non-zero atol required by test_operator_gpu.py:test_regression with seed 651640549
-    assert_almost_equal(npout, arr_grad.asnumpy())
+    assert_almost_equal(npout, arr_grad.asnumpy(), atol=atol)
-    assert_almost_equal(out, np_softmax(x.asnumpy()), rtol=1e-4)
+    # Non-zero atol required by test_softmax with seed 781663739
-    assert_almost_equal(grad.asnumpy(), np_softmax(x.asnumpy()) - l.asnumpy(), rtol=1e-4)
+    assert_almost_equal(grad.asnumpy(), np_softmax(x.asnumpy()) - l.asnumpy(), rtol=rtol, atol=atol)
-    check_numeric_gradient(y, [xa], numeric_eps=1E-3)
+    check_numeric_gradient(y, [xa], numeric_eps=eps)
-    assert_almost_equal(exe_test.outputs[0].asnumpy(), np.dot(np_onehot, np_weight))
+    # Non-zero atol required, as exposed by seed 781663739
-    assert_almost_equal(grad_map["embed_weight"].asnumpy(), np.dot(np_onehot.T, np_grad))
+    assert_almost_equal(grad_map["embed_weight"].asnumpy(), np.dot(np_onehot.T, np_grad), rtol=rtol, atol=atol)
-    npout = np.trunc(data_tmp)
+    # 'trunc' is sensitive to the precision of the calculation.  Force numpy to match mxnet's float32.
-
+@with_seed()
-        check_binary_op_backward(c, lambda g_out, a, b: (g_out, - g_out * (a // b)), gen_binary_data)
+        # '%' is sensitive to the precision of the calculation.  Force numpy to match mxnet's float32.
-        check_binary_op_forward(c, lambda a, b: (a != b).astype(a.dtype), gen_binary_data)
+        # '!=' is sensitive to the precision of the comparison.  Force numpy to match mxnet's float32.
-    np.random.seed(1234)
+# Seed set because the test is not robust enough to operate on random data
-                            c = mx.sym.batch_dot(a, b, transpose_a=transpose_a, transpose_b=transpose_b)
+                        a = mx.sym.Variable('a', dtype=data_type)
-                                bgrad_npy + b_init_grad_npy, rtol=1e-3, atol=1e-4)
+                        exe = c.simple_bind(ctx=default_context(),
-    np.random.seed(1234)
+@with_seed()
-def check_l2_normalization(in_shape, mode, ctx=default_context(), norm_eps=1e-10):
+def check_l2_normalization(in_shape, mode, norm_eps=1e-10):
-    np.random.seed(1234)
+# TODO(szha): Seeding this masks failures. We need to do a deep dive for failures without this seed.
-    np.random.seed(1234)
+@with_seed()
-
+@with_seed()
-    data_tmp[data_tmp == 0] = 1.0
+    # Avoid possible division by 0 errors and finite difference method inaccuracies.
-    check_numeric_gradient(test, [data_tmp])
+    check_numeric_gradient(test, [data_tmp], numeric_eps = eps)
-    check_numeric_gradient(test, [data_tmp])
+    check_numeric_gradient(test, [data_tmp], numeric_eps=eps)
-    data_tmp[data_tmp == 0] = 1.0
+    # Avoid possible division by 0 errors and finite difference method inaccuracies.
-    check_numeric_gradient(test, [data_tmp])
+    check_numeric_gradient(test, [data_tmp], numeric_eps = eps)
-    np.random.seed(42)
+# Seed set because the test is not robust enough to operate on random data
-    np.random.seed(1896893923)
+# Seed set because the test is not robust enough to operate on random data
-    np.random.seed(1896893923)
+# Seed set because the test is not robust enough to operate on random data
-    np.random.seed(1896893923)
+@with_seed()
-    np.random.seed(192837465)
+# Seed set because the test is not robust enough to operate on random data
-    np.random.seed(192837465)
+@with_seed()
-def compare_optimizer(opt1, opt2, shape, dtype, w_stype='default', g_stype='default'):
+def compare_optimizer(opt1, opt2, shape, dtype, w_stype='default', g_stype='default',
-    assert_almost_equal(w1.asnumpy(), w2.asnumpy(), rtol=1e-4, atol=1e-5)
+    compare_ndarray_tuple(state1, state2, rtol=rtol, atol=atol)
-    mx.random.seed(0)
+@with_seed()
-    mx.random.seed(0)
+@with_seed(0)
-
+@with_seed(0)
-
+@with_seed(0)
-    mx.random.seed(0)
+@with_seed()
-                        compare_optimizer(opt1(**kwarg), opt2(**kwarg), shape, dtype)
+                        # atol 2e-5 needed to pass with seed 1248389097
-                                          dtype, w_stype='row_sparse', g_stype='row_sparse')
+                                          dtype, w_stype='row_sparse', g_stype='row_sparse',
-                                          dtype, w_stype='row_sparse', g_stype='row_sparse')
+                                          dtype, w_stype='row_sparse', g_stype='row_sparse',
-    mx.random.seed(0)
+@with_seed(0)
-    mx.random.seed(0)
+@with_seed()
-    mx.random.seed(0)
+@with_seed(1234)
-    np.random.seed(1234)
+
-        y, prob = mx.nd.random.multinomial(x, shape=1000, get_prob=True)
+        y, prob = mx.nd.random.multinomial(x, shape=samples, get_prob=True)
-        mx.test_utils.assert_almost_equal(freq, x[i], rtol=0.25)
+        freq = np.bincount(y[i], minlength=5)/np.float32(samples)*x[i].sum()
-        for j in range(1000):
+        for j in range(samples):
-        mx.test_utils.assert_almost_equal(real_dx, dx.asnumpy()[i])
+        mx.test_utils.assert_almost_equal(real_dx, dx.asnumpy()[i], rtol=1e-4)
-            buckets, probs = gen_buckets_probs_with_ppf(lambda x: ss.norm.ppf(x, mu, sigma), 5)
+            buckets, probs = gen_buckets_probs_with_ppf(lambda x: ss.norm.ppf(x, mu, sigma), num_buckets)
-            verify_generator(generator=generator_mx, buckets=buckets, probs=probs)
+            verify_generator(generator=generator_mx, buckets=buckets, probs=probs,
-            verify_generator(generator=generator_mx_same_seed, buckets=buckets, probs=probs)
+            verify_generator(generator=generator_mx_same_seed, buckets=buckets, probs=probs,
-        generator_mx = lambda x: mx.nd.random.multinomial(data=mx.nd.array(np.array(probs), ctx=ctx, dtype=dtype),
+        quantized_probs = quantize_probs(probs, dtype)
-        verify_generator(generator_mx, buckets, probs)
+        verify_generator(generator=generator_mx, buckets=buckets, probs=quantized_probs,
-                [mx.nd.random.multinomial(data=mx.nd.array(np.array(probs), ctx=ctx, dtype=dtype),
+                [mx.nd.random.multinomial(data=mx.nd.array(quantized_probs, ctx=ctx, dtype=dtype),
-        verify_generator(generator=generator_mx_same_seed, buckets=buckets, probs=probs)
+        verify_generator(generator=generator_mx_same_seed, buckets=buckets, probs=quantized_probs,
-    shape = (rnd.randint(2, 10), rnd.randint(2, 10))    
+    shape = (rnd.randint(2, 10), rnd.randint(2, 10))
-        
+
-    
+
-        fn(shape=shape, out=rsp_out)
+        with random_seed(0):
-    np.random.seed(0)
+@with_seed(0)
-    np.random.seed(0)
+
-    all_type = [np.float32, np.float64, np.float16, np.uint8, np.int32]
+    all_type = [np.float32, np.float64, np.float16, np.uint8, np.int8, np.int32, np.int64]
-        train = SyntheticDataIter(args.num_classes, data_shape, 500, np.float32)
+        train = SyntheticDataIter(args.num_classes, data_shape,
-    >>> print acc.get()
+    >>> f1 = mx.metric.F1()
-            name, output_names=output_names, label_names=label_names)
+                 output_names=None, label_names=None, average="macro"):
-                    false_negatives += 1.
+            self.metrics.update_binary_stats(label, pred)
-            self.sum_metric += f1_score
+        if self.average == "macro":
-    assert f1 == f1_expected
+    microF1 = mx.metric.create("f1", average="micro")
-    pool6 = mx.sym.Pooling(in5b, kernel=(7, 7), stride=(1,1), pool_type="avg")
+    pool6 = mx.sym.Pooling(in5b, kernel=(7, 7), stride=(1,1), global_pool=True, pool_type="avg")
-                Turning this on can improve convergence and accuracy when training with float16.
+       in 32-bit precision even if actual weights used in the model have lower precision.
-@register
+def test_acc():
-           'Flatten', 'Lambda', 'HybridLambda']
+__all__ = ['Sequential', 'HybridSequential', 'Dense', 'Dropout', 'Embedding',
-from .basic_layers import Activation
+from .activations import Activation
-        
+
-    except FileNotFoundError:
+    except IOError:
-            label = label.astype('int32')
+            pred_label = pred_label.asnumpy().astype('int32')
-        self.sum_metric += ndarray.add_n(*results).asscalar()
+            self.sum_metric += (pred_label.flat == label.flat).sum()
-__version__ = "1.0.1"
+__version__ = "1.1.0"
-# pylint: disable=
+# pylint: disable=line-too-long
-        `help <http://mxnet.io/api/python/optimization.html#the-mxnet-optimizer-package>`_
+        `help <http://mxnet.io/api/python/optimization/optimization.html#the-mxnet-optimizer-package>`_
-    net += scale * tower_out
+    net = net + scale * tower_out
-    net += scale * tower_out
+    net = net + scale * tower_out
-    net += scale * tower_out
+    net = net + scale * tower_out
-                          Optimizer.opt_registry[name].__name__)
+            warnings.warn('WARNING: New optimizer %s.%s is overriding '
-class NAG(SGD):
+@register
-    This optimizer accepts the same arguments as :class:`.SGD`.
+    Parameters
-    def __init__(self, **kwargs):
+    def __init__(self, momentum=0.0, **kwargs):
-           'Lambda', 'HybridLambda']
+           'Dropout', 'BatchNorm', 'InstanceNorm', 'LeakyReLU', 'Embedding',
-           'GlobalAvgPool1D', 'GlobalAvgPool2D', 'GlobalAvgPool3D']
+           'GlobalAvgPool1D', 'GlobalAvgPool2D', 'GlobalAvgPool3D',
-        label_shapes = [l.shape for l in self.label]
+        if self.label:
-    as feature extractors. For example, you may want to extract get the output
+    as feature extractors. For example, you may want to extract the output
-                        return
+                        return True
-            return
+            return self
-            return True
+        return coverages.size > 0 and np.amin(coverages) > self.min_object_covered
-                return
+                return False
-                return
+                return False
-    if function is not None:
+    if function is None:
-        return np.array(the_input)
+    return output
-    if function is not None:
+    if function is None:
-        return np.array(input1)
+    return output
-
+        return False
-
+    # pylint: disable=unexpected-keyword-arg
-
+    # pylint: enable=unexpected-keyword-arg
-                args.num_examples / args.batch_size, np.float32)
+        train = SyntheticDataIter(args.num_classes, data_shape, 500, np.float32)
-        for name, arr in self._arg_params.items():
+        for name, arr in sorted(self._arg_params.items()):
-        for name, arr in self._aux_params.items():
+        for name, arr in sorted(self._aux_params.items()):
-        train = SyntheticDataIter(args.num_classes, data_shape, 500, np.float32)
+        train = SyntheticDataIter(args.num_classes, data_shape,
-    args += opts.command;
+
-       re.compile('.*Epoch\[(\d+)\] Time.*=([.\d]+)')]
+res = [re.compile('.*Epoch\[(\d+)\] Train-'+s+'.*=([.\d]+)') for s in args.metric_names]\
-    print "| --- | --- | --- | --- |"
+    print("| epoch | " + " | ".join(['train-'+s for s in args.metric_names]) + " | " + " | ".join(['val-'+s for s in args.metric_names]) + " | time |")
-        print "| %2d | %f | %f | %.1f |" % (k+1, v[0]/v[1], v[2]/v[3], v[4]/v[5])
+        print("| %2d | " % (k+1)\
-    print "epoch\ttrain-accuracy\tvalid-accuracy\ttime"
+    print("\t".join(['epoch'] + ['train-' + s for s in args.metric_names] + ['val-' + s for s in args.metric_names] + ['time']))
-        print "%2d\t%f\t%f\t%.1f" % (k+1, v[0]/v[1], v[2]/v[3], v[4]/v[5])
+        print("\t".join(["%2d" % (k+1)] + ["%f" % (v[2*j]/v[2*j+1]) for j in range(2*len(args.metric_names))] + ["%.1f" % (v[-2]/v[-1])]))
-            self.num_inst += numpy.prod(pred_label.shape)
+            self.num_inst += pred_label.size
-                                                            weight.shape, weight.context)
+        weight[:] += - lr/2 * (grad + wd * weight) + normal(0, math.sqrt(lr), shape=weight.shape,
-@unittest.skip("test fails intermittently. temporarily disabled till it gets fixed. tracked at https://github.com/apache/incubator-mxnet/issues/9384")
+def test_sparse_nd_where():
-          this function will retrieve (a copy of) the latest parameters. Therefore, modifying
+          this function will retrieve (a copy of) the latest parameters.
-    embedding vector for <token_i>, the expected format of a custom pre-trained token embedding file
+    Denote by '[ed]' the argument `elem_delim`. Denote by [v_ij] the j-th element of the token
-    <v_2k>\\\\n...'
+    '[token_1][ed][v_11][ed][v_12][ed]...[ed][v_1k]\\\\n[token_2][ed][v_21][ed][v_22][ed]...[ed]
-    unknown_token : hashable object, default '<unk>'
+    unknown_token : hashable object, default '&lt;unk&gt;'
-        return mx.nd.array(ids, dtype='int32')
+from mxnet.gluon import contrib
-                    help='location of the data corpus')
+parser = argparse.ArgumentParser(description='MXNet Autograd RNN/LSTM Language Model on Wikitext-2.')
-test_data = batchify(corpus.test, args.batch_size).as_in_context(context)
+train_dataset = contrib.data.text.WikiText2('./data', 'train', seq_len=args.bptt)
-ntokens = len(corpus.dictionary)
+ntokens = len(vocab)
-        data, target = get_batch(data_source, i)
+    for i, (data, target) in enumerate(data_source):
-            data, target = get_batch(train_data, i)
+        for i, (data, target) in enumerate(train_data):
-            grads = [i.grad(context) for i in model.collect_params().values()]
+            grads = [p.grad(context) for p in model.collect_params().values()]
-            if ibatch % args.log_interval == 0 and ibatch > 0:
+            if i % args.log_interval == 0 and i > 0:
-                    epoch, ibatch, cur_L, math.exp(cur_L)))
+                    epoch, i, cur_L, math.exp(cur_L)))
-
+# coding: utf-8
-unzip wikitext-2-v1.zip
+from . import text
-mv ${DATA_DIR}/wikitext-2/wiki.train.tokens ${DATA_DIR}/wikitext-2/wiki.train.txt
+from .sampler import *
-class MNIST(_DownloadedDataset):
+class MNIST(dataset._DownloadedDataset):
-        super(MNIST, self).__init__('mnist', root, train, transform)
+        super(MNIST, self).__init__('mnist', root, transform)
-        super(MNIST, self).__init__('fashion-mnist', root, train, transform) # pylint: disable=bad-super-call
+        super(MNIST, self).__init__('fashion-mnist', root, transform) # pylint: disable=bad-super-call
-class CIFAR10(_DownloadedDataset):
+class CIFAR10(dataset._DownloadedDataset):
-        super(CIFAR10, self).__init__('cifar10', root, train, transform)
+        super(CIFAR10, self).__init__('cifar10', root, transform)
-        super(CIFAR10, self).__init__('cifar100', root, train, transform) # pylint: disable=bad-super-call
+        super(CIFAR10, self).__init__('cifar100', root, transform) # pylint: disable=bad-super-call
-            self.sum_metric += (pred_label.flatten() == label.flatten()).sum().asscalar()
+            self.sum_metric += (pred_label.reshape((-1,)) == label.reshape((-1,))).sum().asscalar()
-            'url': "http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/news20.t.bz2",
+            'url': "https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/news20.t.bz2",
-import mxnet as mx
+""" example train fit utility """
-        logging.info('Adjust learning rate to %e for epoch %d' %(lr, begin_epoch))
+        logging.info('Adjust learning rate to %e for epoch %d',
-    steps = [epoch_size * (x-begin_epoch) for x in step_epochs if x-begin_epoch > 0]
+    steps = [epoch_size * (x - begin_epoch)
-                       help='number of layers in the neural network, required by some networks such as resnet')
+                       help='number of layers in the neural network, \
-                    i, args.disp_batches*args.batch_size/(time.time()-tic)))
+            if (i + 1) % args.disp_batches == 0:
-    devs = mx.cpu() if args.gpus is None or args.gpus is '' else [
+    devs = mx.cpu() if args.gpus is None or args.gpus == "" else [
-        symbol        = network
+        context=devs,
-    lr_scheduler  = lr_scheduler
+    lr_scheduler = lr_scheduler
-        'wd' : args.wd,
+        'wd': args.wd,
-    monitor = mx.mon.Monitor(args.monitor, pattern=".*") if args.monitor > 0 else None
+    monitor = mx.mon.Monitor(
-            rnd_type='gaussian', factor_type="in", magnitude=2)
+    # A limited number of optimizers have a warmup period
-        eval_metrics.append(mx.metric.create('top_k_accuracy', top_k=args.top_k))
+        eval_metrics.append(mx.metric.create(
-    batch_end_callbacks = [mx.callback.Speedometer(args.batch_size, args.disp_batches)]
+    batch_end_callbacks = [mx.callback.Speedometer(
-              monitor            = monitor)
+              begin_epoch=args.load_epoch if args.load_epoch else 0,
-
+    check_regression(mx.symbol.MAERegressionOutput,
-        raise RuntimeError("File %s not downloaded completely" % ("inception-v3-dump.npz"))
+    utils.download("http://data.mxnet.io/data/test_images_%d_%d.npy" % (shape),
-            The labels of the data.
+            The labels of the data with class indices as values, one per sample.
-            Predicted values.
+            Prediction values for samples. Each prediction value can either be the class index,
-            label = label.asnumpy().astype('int32')
+            pred_label = pred_label.astype('int32')
-            self.num_inst += len(pred_label.flat)
+            if pred_label.context != label.context:
-# ref: http://mxnet.io/how_to/new_op.html
+# ref: http://mxnet.io/faq/new_op.html
-# ref: http://mxnet.io/how_to/new_op.html
+# ref: http://mxnet.io/faq/new_op.html
-    `How to run MXNet on multiple CPU/GPUs <http://mxnet.io/how_to/multi_devices.html>`
+    `How to run MXNet on multiple CPU/GPUs <http://mxnet.io/faq/multi_devices.html>`
-    scala_path = app.builder.srcdir + '/../scala-package/core/src/main/scala/org/apache/mxnet'
+    scala_path = app.builder.srcdir + '/../scala-package/core/src/main/scala/ml/dmlc/mxnet'
-    scaladocs = ['index', 'index.html', 'org', 'lib', 'index.js', 'package.html']
+    scaladocs = ['index', 'index.html', 'ml', 'lib', 'index.js', 'package.html']
-    download("http://data.mxnet.io/data/inception-v3-dump.npz", dirname="data")
+    hash_test_img = "355e15800642286e7fe607d87c38aeeab085b0cc"
-          '.bat':'rem', '.pl':'#', '.m':'%', '.R':'#', '.mk':'#', '.cfg':'#'}
+          '.bat':'rem', '.pl':'#', '.m':'%', '.R':'#', '.mk':'#', '.cfg':'#', '.t':'#'}
-        for _ in range(epoch_size/args.t_max):
+        for _ in range(int(epoch_size/args.t_max)):
-import Queue
+import sys
-        y = i/n
+        y = i//n
-            self.queue = Queue.Queue()
+            self.queue = queue.Queue()
-        channels = self.state_.shape[1]/self.input_length
+        channels = self.state_.shape[1]//self.input_length
-        data = self.state_[:4, -self.state_.shape[1]/self.input_length:, :, :]
+        data = self.state_[:4, -self.state_.shape[1]//self.input_length:, :, :]
-     'wiki.zu.vec': '4b244b9697a8280e6646842c5fc81bb3a6bc8ec7'}
+     'wiki.zu.vec': '4b244b9697a8280e6646842c5fc81bb3a6bc8ec7',
-        embedding_name='fasttext')) == 294
+        embedding_name='fasttext')) == 297
-    assert len(reg['fasttext']) == 294
+    assert len(reg['fasttext']) == 297
-from . import indexer
+from . import vocab
-from . import glossary
+# pylint: disable=super-init-not-called
-from . import indexer
+from . import vocab
-class TokenEmbedding(indexer.TokenIndexer):
+def register(embedding_cls):
-    those of GloVe and FastText, use `TokenEmbedding.create(embedding_name, pretrained_file_name)`.
+    those of GloVe and FastText, use
-    `TokenEmbedding.get_embedding_and_pretrained_file_names()`.
+    :func:`~mxnet.contrib.text.embedding.get_pretrained_file_names()`.
-    :class:`~mxnet.contrib.text.embedding.TokenEmbedding`.
+    The indexed tokens in a text token embedding may come from a vocabulary or from the loaded
-        super(TokenEmbedding, self).__init__(**kwargs)
+        super(_TokenEmbedding, self).__init__(**kwargs)
-                        # very beggining because the unknown index is 0.
+                        # Reserve a vector slot for the unknown token at the very beggining because
-            'embeddings of the glossary.'
+            'The length of new_vectors must be equal to the number of tokens and the width of' \
-class GloVe(TokenEmbedding):
+@register
-    pretrain_file : str, default 'glove.840B.300d.txt'
+    pretrained_file_name : str, default 'glove.840B.300d.txt'
-    embed_root : str, default os.path.join('~', '.mxnet', 'embeddings')
+    embedding_root : str, default os.path.join('~', '.mxnet', 'embeddings')
-    unknown_vec : callback
+    init_unknown_vec : callback
-        # Map a pretrained embedding file to its archive to download.
+        # Map a pre-trained embedding file to its archive to download.
-                 init_unknown_vec=nd.zeros, **kwargs):
+                 init_unknown_vec=nd.zeros, vocabulary=None, **kwargs):
-class FastText(TokenEmbedding):
+
-    pretrain_file : str, default 'wiki.en.vec'
+    pretrained_file_name : str, default 'wiki.en.vec'
-    embed_root : str, default os.path.join('~', '.mxnet', 'embeddings')
+    embedding_root : str, default os.path.join('~', '.mxnet', 'embeddings')
-    unknown_vec : callback
+    init_unknown_vec : callback
-                 init_unknown_vec=nd.zeros, **kwargs):
+                 init_unknown_vec=nd.zeros, vocabulary=None, **kwargs):
-class CustomEmbedding(TokenEmbedding):
+class CustomEmbedding(_TokenEmbedding):
-    pretrain_file_path : str
+    pretrained_file_path : str
-    unknown_vec : callback
+    encoding : str, default 'utf8'
-                 init_unknown_vec=nd.zeros, **kwargs):
+                 init_unknown_vec=nd.zeros, vocabulary=None, **kwargs):
-class TokenIndexer(object):
+class Vocabulary(object):
-    of :class:`~mxnet.contrib.text.glossary.Glossary`.
+    be used by token embeddings.
-        are aligned.
+        A list of indexed tokens where the list indices and the token indices are aligned.
-        """Converts tokens to indices according to the text indexer.
+        """Converts tokens to indices according to the vocabulary.
-            A token index or a list of token indices according to the text indexer.
+            A token index or a list of token indices according to the vocabulary.
-        """Converts token indices to tokens according to the text indexer.
+        """Converts token indices to tokens according to the vocabulary.
-            A token or a list of tokens according to the text indexer.
+            A token or a list of tokens according to the vocabulary.
-                                        unknown_token='<unk>', reserved_tokens=None)
+    vocab = text.vocab.Vocabulary(counter, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-    i1 = indexer.to_indices('c')
+    i1 = vocab.to_indices('c')
-    i2 = indexer.to_indices(['c'])
+    i2 = vocab.to_indices(['c'])
-    i3 = indexer.to_indices(['<unk>', 'non-exist'])
+    i3 = vocab.to_indices(['<unk>', 'non-exist'])
-    i4 = indexer.to_indices(['a', 'non-exist', 'a', 'b'])
+    i4 = vocab.to_indices(['a', 'non-exist', 'a', 'b'])
-    i1 = indexer.to_tokens(1)
+    vocab = text.vocab.Vocabulary(counter, most_freq_count=None, min_freq=1,
-    i2 = indexer.to_tokens([1])
+    i2 = vocab.to_tokens([1])
-    i3 = indexer.to_tokens([0, 0])
+    i3 = vocab.to_tokens([0, 0])
-    i4 = indexer.to_tokens([3, 0, 3, 2])
+    i4 = vocab.to_tokens([3, 0, 3, 2])
-    assertRaises(ValueError, indexer.to_tokens, 100)
+    assertRaises(ValueError, vocab.to_tokens, 100)
-        # 33 bytes
+    @text.embedding.register
-                     init_unknown_vec=nd.zeros, **kwargs):
+        def __init__(self, embedding_root='embeddings', init_unknown_vec=nd.zeros, **kwargs):
-                embedding_root, pretrained_file_name)
+            pretrained_file_path = Test._get_pretrained_file(embedding_root, pretrained_file_name)
-    test_embed = text.embedding.TokenEmbedding.create('test')
+    test_embed = text.embedding.create('test')
-        test_embed.idx_to_vec[0].asnumpy(), nd.zeros((5,)).asnumpy())
+    assert_almost_equal(test_embed.idx_to_vec[1].asnumpy(), (nd.arange(5) + 1).asnumpy())
-                          pretrain_file2)
+    _mk_my_pretrain_file3(os.path.join(embed_root, embed_name), elem_delim, pretrain_file2)
-        unknown_token='<unk>')
+    my_embed2 = text.embedding.CustomEmbedding(pretrain_file_path, elem_delim,
-        unknown_token='<unk1>')
+    my_embed3 = text.embedding.CustomEmbedding(pretrain_file_path, elem_delim,
-                                      invalid_pretrain_file2)
+    _mk_my_invalid_pretrain_file2(os.path.join(embed_root, embed_name), elem_delim,
-def test_token_indexer():
+def test_vocabulary():
-    assert i6.token_to_idx == {'<unk>': 0, 'c': 1, 'b': 2, 'a': 3,
+    v1 = text.vocab.Vocabulary(counter, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-    assertRaises(AssertionError, text.indexer.TokenIndexer, counter, most_freq_count=None,
+    assert v6.idx_to_token[1] == 'c'
-    assertRaises(AssertionError, text.indexer.TokenIndexer, counter, most_freq_count=None,
+    assertRaises(AssertionError, text.vocab.Vocabulary, counter, most_freq_count=None,
-    assertRaises(AssertionError, text.indexer.TokenIndexer, counter, most_freq_count=None,
+    assertRaises(AssertionError, text.vocab.Vocabulary, counter, most_freq_count=None,
-    assert i13.reserved_tokens == ['<pad>']
+    v8 = text.vocab.Vocabulary(counter, most_freq_count=None, min_freq=1, unknown_token='<unknown>',
-    assert i14.token_to_idx == {('<unk>', '<unk>'): 0, ('c', 'c'): 1, ('b', 'b'): 2, ('a', 'a'): 3,
+    v14 = text.vocab.Vocabulary(counter_tuple, most_freq_count=None, min_freq=1,
-    assert i14.reserved_tokens is None
+    assert v14.idx_to_token[1] == ('c', 'c')
-def test_glossary_with_one_embed():
+    assertRaises(AssertionError, e1.update_token_vectors, '<unk>',
-    g1 = text.glossary.Glossary(i1, my_embed)
+    v1 = text.vocab.Vocabulary(counter, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-    assert g1.idx_to_token == ['<unk>', '<pad>', 'c', 'b', 'a', 'some_word$']
+    assert ce1.token_to_idx == {'<unk>': 0, '<pad>': 1, 'c': 2, 'b': 3, 'a': 4, 'some_word$': 5}
-    assert_almost_equal(g1.idx_to_vec.asnumpy(),
+    assert_almost_equal(ce1.idx_to_vec.asnumpy(),
-    assert g1.reserved_tokens == ['<pad>']
+    assert ce1.vec_len == 5
-    assert_almost_equal(g1.get_vecs_by_tokens('c').asnumpy(),
+    assert_almost_equal(ce1.get_vecs_by_tokens('c').asnumpy(),
-    assert_almost_equal(g1.get_vecs_by_tokens(['c']).asnumpy(),
+    assert_almost_equal(ce1.get_vecs_by_tokens(['c']).asnumpy(),
-    assert_almost_equal(g1.get_vecs_by_tokens(['a', 'not_exist']).asnumpy(),
+    assert_almost_equal(ce1.get_vecs_by_tokens(['a', 'not_exist']).asnumpy(),
-    assert_almost_equal(g1.get_vecs_by_tokens(['a', 'b']).asnumpy(),
+    assert_almost_equal(ce1.get_vecs_by_tokens(['a', 'b']).asnumpy(),
-    assert_almost_equal(g1.get_vecs_by_tokens(['A', 'b']).asnumpy(),
+    assert_almost_equal(ce1.get_vecs_by_tokens(['A', 'b']).asnumpy(),
-    assert_almost_equal(g1.get_vecs_by_tokens(['A', 'b'], lower_case_backup=True).asnumpy(),
+    assert_almost_equal(ce1.get_vecs_by_tokens(['A', 'b'], lower_case_backup=True).asnumpy(),
-                            nd.array([[2, 2, 2, 2, 2],
+    ce1.update_token_vectors(['a', 'b'],
-                            )
+                             )
-    assert_almost_equal(g1.idx_to_vec.asnumpy(),
+    assert_almost_equal(ce1.idx_to_vec.asnumpy(),
-    assertRaises(ValueError, g1.update_token_vectors, 'unknown$$$', nd.array([0, 0, 0, 0, 0]))
+    assertRaises(ValueError, ce1.update_token_vectors, 'unknown$$$', nd.array([0, 0, 0, 0, 0]))
-    assertRaises(AssertionError, g1.update_token_vectors, '<unk>',
+    assertRaises(AssertionError, ce1.update_token_vectors, '<unk>',
-    assertRaises(AssertionError, g1.update_token_vectors, '<unk>', nd.array([0]))
+    assertRaises(AssertionError, ce1.update_token_vectors, '<unk>', nd.array([0]))
-    assert_almost_equal(g1.idx_to_vec.asnumpy(),
+    ce1.update_token_vectors(['<unk>'], nd.array([0, 0, 0, 0, 0]))
-    assert_almost_equal(g1.idx_to_vec.asnumpy(),
+    ce1.update_token_vectors(['<unk>'], nd.array([[10, 10, 10, 10, 10]]))
-    assert_almost_equal(g1.idx_to_vec.asnumpy(),
+    ce1.update_token_vectors('<unk>', nd.array([0, 0, 0, 0, 0]))
-    assert_almost_equal(g1.idx_to_vec.asnumpy(),
+    ce1.update_token_vectors('<unk>', nd.array([[10, 10, 10, 10, 10]]))
-def test_glossary_with_two_embeds():
+def test_composite_embedding_with_two_embeddings():
-    g1 = text.glossary.Glossary(i1, [my_embed1, my_embed2])
+    v1 = text.vocab.Vocabulary(counter, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-    assert g1.idx_to_token == ['<unk>', 'c', 'b', 'a', 'some_word$']
+    assert ce1.token_to_idx == {'<unk>': 0, 'c': 1, 'b': 2, 'a': 3, 'some_word$': 4}
-    assert_almost_equal(g1.idx_to_vec.asnumpy(),
+    assert_almost_equal(ce1.idx_to_vec.asnumpy(),
-    assert_almost_equal(g1.get_vecs_by_tokens('c').asnumpy(),
+    assert ce1.vec_len == 10
-    assert_almost_equal(g1.get_vecs_by_tokens(['b', 'not_exist']).asnumpy(),
+    assert_almost_equal(ce1.get_vecs_by_tokens(['b', 'not_exist']).asnumpy(),
-    g1.update_token_vectors(['a', 'b'],
+    ce1.update_token_vectors(['a', 'b'],
-    assert_almost_equal(g1.idx_to_vec.asnumpy(),
+    assert_almost_equal(ce1.idx_to_vec.asnumpy(),
-    assert_almost_equal(g2.idx_to_vec.asnumpy(),
+    v2 = text.vocab.Vocabulary(counter, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-    assert_almost_equal(g3.idx_to_vec.asnumpy(),
+    v3 = text.vocab.Vocabulary(counter, most_freq_count=None, min_freq=1, unknown_token='<unk1>',
-    assert_almost_equal(g4.idx_to_vec.asnumpy(),
+    v4 = text.vocab.Vocabulary(counter, most_freq_count=None, min_freq=1, unknown_token='<unk2>',
-    assert_almost_equal(g5.idx_to_vec.asnumpy(),
+    v5 = text.vocab.Vocabulary(counter2, most_freq_count=None, min_freq=1, unknown_token='a',
-    assert len(text.embedding.TokenEmbedding.get_embedding_and_pretrained_file_names(
+def test_get_and_pretrain_file_names():
-        embedding_name='glove')) == 10
+    assert len(text.embedding.get_pretrained_file_names(embedding_name='glove')) == 10
-        embedding_name=None)
+    reg = text.embedding.get_pretrained_file_names(embedding_name=None)
-                 'unknown$$')
+    assertRaises(KeyError, text.embedding.get_pretrained_file_names, 'unknown$$')
-        test_pretrain = True #model_name in pretrained_to_test
+        test_pretrain = model_name in pretrained_to_test
-    if not vgg_spec.has_key(num_layers):
+    if num_layers not in vgg_spec:
-                 beta_initializer='zeros', gamma_initializer='ones',
+                 use_global_stats=False, beta_initializer='zeros', gamma_initializer='ones',
-                        'fix_gamma': not scale}
+                        'fix_gamma': not scale, 'use_global_stats': use_global_stats}
-from ....base import _Null, numeric_types
+from .... import image
-__all__ = op.__all__ + ndarray.__all__ + utils.__all__ + ['contrib', 'linalg', 'random', 'sparse', 'image']
+__all__ = op.__all__ + ndarray.__all__ + utils.__all__ + \
-    pil_img = Image.fromarray(data_in).transpose(Image.FLIP_LEFT_RIGHT)
+    flip_in = data_in[:, ::-1, :]
-    assert_almost_equal(np.array(pil_img), data_trans.asnumpy())
+    assert_almost_equal(flip_in, data_trans.asnumpy())
-    pil_img = Image.fromarray(data_in).transpose(Image.FLIP_TOP_BOTTOM)
+    flip_in = data_in[::-1, :, :]
-    assert_almost_equal(np.array(pil_img), data_trans.asnumpy())
+    assert_almost_equal(flip_in, data_trans.asnumpy())
-    """Randomly flip the input image horizontally with a probability
+class RandomFlipLeftRight(HybridBlock):
-        super(RandomHorizontalFlip, self).__init__()
+        super(RandomFlipLeftRight, self).__init__()
-        return F.image.random_horizontal_flip(x)
+        return F.image.random_flip_left_right(x)
-    """Randomly flip the input image vertically with a probability
+class RandomFlipTopBottom(HybridBlock):
-        super(RandomVerticalFlip, self).__init__()
+        super(RandomFlipTopBottom, self).__init__()
-        return F.image.random_vertical_flip(x)
+        return F.image.random_flip_top_bottom(x)
-from mxnet.gluon.data.vision.transforms import AdjustLighting
+from mxnet.gluon.data.vision import transforms
-def test_adjust_lighting():
+def test_to_tensor():
-    assert_almost_equal(out_nd.asnumpy(), out_gt)
+    out_nd = transforms.ToTensor()(nd.array(data_in, dtype='uint8'))
-    """A dataset of multiple arrays.
+    """A dataset that combines multiple dataset-like objects, e.g.
-    The i-th sample is `(x1[i], x2[i], ...)`.
+    The i-th sample is defined as `(x1[i], x2[i], ...)`.
-    *args : one or more arrays
+    *args : one or more dataset-like objects
-from ....base import _Null
+from .... import ndarray, initializer, image
-                self.register_child(hybrid[0])
+                self.add(hybrid[0])
-                self.register_child(hblock)
+                self.add(hblock)
-                self.register_child(i)
+                self.add(i)
-    def __init__(self, size, area=(0.08, 1.0), ratio=(3.0/4.0, 4.0/3.0),
+class RandomResizedCrop(Block):
-    def __init__(self, size):
+        if isinstance(size, numeric_types):
-        self._size = size
+        if isinstance(size, numeric_types):
-        return F.image.center_crop(x, size)
+class Resize(Block):
-class Resize(HybridBlock):
+    Parameters
-        self._args = (size, interpolation)
+        if isinstance(size, numeric_types):
-        return F.image.resize(x, *self._args)
+        return F.image.random_horizontal_flip(x)
-        self._axis = axis
+class RandomVerticalFlip(HybridBlock):
-        return F.image.random_flip(x, self._axis)
+        return F.image.random_vertical_flip(x)
-    def __init__(self, max_brightness):
+    """Randomly jitters image brightness with a factor
-        self._max_brightness = max_brightness
+        self._args = (max(0, 1-brightness), 1+brightness)
-        return F.image.random_brightness(x, self._max_brightness)
+        return F.image.random_brightness(x, *self._args)
-    def __init__(self, max_contrast):
+    """Randomly jitters image contrast with a factor
-        self._max_contrast = max_contrast
+        self._args = (max(0, 1-contrast), 1+contrast)
-        return F.image.random_contrast(x, self._max_contrast)
+        return F.image.random_contrast(x, *self._args)
-    def __init__(self, max_saturation):
+    """Randomly jitters image saturation with a factor
-        self._max_saturation = max_saturation
+        self._args = (max(0, 1-saturation), 1+saturation)
-        return F.image.random_saturation(x, self._max_saturation)
+        return F.image.random_saturation(x, *self._args)
-    def __init__(self, max_hue):
+    """Randomly jitters image hue with a factor
-        self._max_hue = max_hue
+        self._args = (max(0, 1-hue), 1+hue)
-        return F.image.random_hue(x, self._max_hue)
+        return F.image.random_hue(x, *self._args)
-    def __init__(self, max_brightness=0, max_contrast=0, max_saturation=0, max_hue=0):
+    """Randomly jitters the brightness, contrast, saturation, and hue
-        self._args = (max_brightness, max_contrast, max_saturation, max_hue)
+        self._args = (brightness, contrast, saturation, hue)
-    def __init__(self, alpha_std=_Null, eigval=_Null, eigvec=_Null):
+    """Add AlexNet-style PCA-based noise to an image.
-        self._args = (alpha_std, eigval, eigvec)
+        self._alpha = alpha
-        return F.image.random_lighting(x, *self._args)
+        return F.image.random_lighting(x, self._alpha)
-	print(N/(time.time() - tic))
+import os
-__all__ = ['Dataset', 'SimpleDataset', 'ArrayDataset', 'LabeledDataset',
+__all__ = ['Dataset', 'SimpleDataset', 'ArrayDataset',
-from ... import nd, image, recordio
+from .. import dataset
-        return F.cast(x, 'float32').transpose((2, 0, 1))
+        return F.image.to_tensor(x)
-__all__ = ['Dataset', 'ArrayDataset', 'RecordFileDataset']
+__all__ = ['Dataset', 'SimpleDataset', 'ArrayDataset', 'LabeledDataset',
-_OP_NAME_PREFIX_LIST = ['_contrib_', '_linalg_', '_sparse_']
+_OP_NAME_PREFIX_LIST = ['_contrib_', '_linalg_', '_sparse_', '_image_']
-            module_name = "%s.%s.%s" % (root_namespace, module_name, op_name_prefix[1:-1])
+            module_name_local = "%s.%s.%s" % (root_namespace, module_name, op_name_prefix[1:-1])
-        function.__module__ = module_name
+        function.__module__ = module_name_local
-from . import _internal, contrib, linalg, op, random, sparse, utils
+from . import _internal, contrib, linalg, op, random, sparse, utils, image
-__all__ = op.__all__ + ndarray.__all__ + utils.__all__ + ['contrib', 'linalg', 'random', 'sparse']
+__all__ = op.__all__ + ndarray.__all__ + utils.__all__ + ['contrib', 'linalg', 'random', 'sparse', 'image']
-from . import _internal, contrib, linalg, op, random, sparse
+from . import _internal, contrib, linalg, op, random, sparse, image
-__all__ = op.__all__ + symbol.__all__ + ['contrib', 'linalg', 'random', 'sparse']
+__all__ = op.__all__ + symbol.__all__ + ['contrib', 'linalg', 'random', 'sparse', 'image']
-          '.bat':'rem', '.pl':'#'}
+          '.bat':'rem', '.pl':'#', '.m':'%', '.R':'#', '.mk':'#', '.cfg':'#'}
-
+               'src/operator/contrib/nn/deformable_im2col.cuh',
-    token_embeddings : instance or list of :class:`~TokenEmbedding`
+    token_embeddings : instance or list of :class:`~mxnet.contrib.text.embedding.TokenEmbedding`
-        raise RuntimeError('Cannot find the files.\n' +
+        raise RuntimeError('Cannot find the MXNet library.\n' +
-_mx_caffe_model = 'http://data.mxnet.io/models/imagenet/test/caffe/'
+apache_repo_url = 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/'
-        'mean' : 'https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/caffe/imagenet_mean.binaryproto',
+        'prototxt' : (_mx_caffe_model_root + 'bvlc_alexnet/deploy.prototxt',
-        'caffemodel' : 'http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel',
+        'prototxt' : (_mx_caffe_model_root + 'bvlc_googlenet/deploy.prototxt',
-        'caffemodel' : 'http://data.mxnet.io/models/imagenet/test/caffe/VGG_ILSVRC_16_layers.caffemodel',
+        'prototxt' : (_mx_caffe_model_root + 'vgg/VGG_ILSVRC_16_layers_deploy.prototxt',
-        'caffemodel' : 'http://data.mxnet.io/models/imagenet/test/caffe/VGG_ILSVRC_19_layers.caffemodel',
+        'prototxt' : (_mx_caffe_model_root + 'vgg/VGG_ILSVRC_19_layers_deploy.prototxt',
-        'mean' : _mx_caffe_model+'ResNet_mean.binaryproto',
+        'prototxt' : (_mx_caffe_model_root + 'resnet/ResNet-50-deploy.prototxt',
-        'mean' : _mx_caffe_model+'ResNet_mean.binaryproto',
+        'prototxt' : (_mx_caffe_model_root + 'resnet/ResNet-101-deploy.prototxt',
-        'mean' : _mx_caffe_model+'ResNet_mean.binaryproto',
+        'prototxt' : (_mx_caffe_model_root + 'resnet/ResNet-152-deploy.prototxt',
-    return dict(dict(model_meta_info)[model_name])
+    return model_meta_info[model_name].copy()
-    prototxt = mx.test_utils.download(meta_info['prototxt'], model_name+'_deploy.prototxt')
+    proto_url, proto_sha1 = meta_info['prototxt']
-    caffemodel = mx.test_utils.download(meta_info['caffemodel'], model_name+'.caffemodel')
+    caffemodel_url, caffemodel_sha1 = meta_info['caffemodel']
-        mean = mx.test_utils.download(mean, model_name+'_mean.binaryproto')
+    if isinstance(mean[0], str):
-from imdb import Imdb
+from .imdb import Imdb
-        return False
+        weight_like = ('_weight', '_bias', '_beta', '_gamma',
-    :class:`~mxnet.text.embedding.CustomEmbedding`.
+    :class:`~mxnet.contrib.text.embedding.CustomEmbedding`.
-    :class:`~mxnet.text.embedding.TokenEmbedding`.
+    :class:`~mxnet.contrib.text.embedding.TokenEmbedding`.
-        :func:`~mxnet.text.embedding.TokenEmbedding.create`.
+        :func:`~mxnet.contrib.text.embedding.TokenEmbedding.create`.
-        ... class MyTextEmbed(mxnet.text.embedding.TokenEmbedding):
+        >>> @mxnet.contrib.text.embedding.TokenEmbedding.register
-        >>> embed = mxnet.text.embedding.TokenEmbedding.create('MyTokenEmbed')
+        >>> embed = mxnet.contrib.text.embedding.TokenEmbedding.create('MyTokenEmbed')
-        """Creates an instance of :class:`~mxnet.text.embedding.TokenEmbedding`.
+        """Creates an instance of :class:`~mxnet.contrib.text.embedding.TokenEmbedding`.
-        `mxnet.text.embedding.TokenEmbedding.get_embedding_and_pretrained_file_names()`.
+        `mxnet.contrib.text.embedding.TokenEmbedding.get_embedding_and_pretrained_file_names()`.
-        :class:`~mxnet.text.glossary.TokenEmbedding`:
+        :class:`~mxnet.contrib.text.glossary.TokenEmbedding`:
-        method returns all the valid names of `pretrained_file_name` for the specified
+        `mxnet.contrib.text.embedding.TokenEmbedding.create(embedding_name, pretrained_file_name)`.
-            `mxnet.text.embedding.TokenEmbedding.create(embedding_name, pretrained_file_name)`.
+            `mxnet.contrib.text.embedding.TokenEmbedding.create(embedding_name,
-    files, such as via instances of :class:`~mxnet.text.embedding.TokenEmbedding`.
+    files, such as via instances of :class:`~mxnet.contrib.text.embedding.TokenEmbedding`.
-        Examples: str, int, and tuple.
+    token_indexer : :class:`~mxnet.contrib.text.indexer.TokenIndexer`
-                 unknown_token='<unk>', reserved_tokens=None):
+    def __init__(self, token_indexer, token_embeddings):
-                                       reserved_tokens=reserved_tokens)
+                'The argument `token_embeddings` must be an instance or a list of instances ' \
-            :class:`~mxnet.text.embedding.TokenEmbedding`
+            :class:`~mxnet.contrib.text.embedding.TokenEmbedding`
-    :class:`~mxnet.text.glossary.Glossary`.
+    be used by instances of :class:`~mxnet.contrib.text.embedding.TokenEmbedding`, such as instances
-                                unknown_token='<unk>', reserved_tokens=['<pad>'])
+    i1 = text.indexer.TokenIndexer(counter, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-                                unknown_token='<unk>', reserved_tokens=None)
+    i1 = text.indexer.TokenIndexer(counter, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-                                unknown_token='<unk>', reserved_tokens=None)
+    i2 = text.indexer.TokenIndexer(counter, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-                                unknown_token='<unk1>', reserved_tokens=None)
+    i3 = text.indexer.TokenIndexer(counter, most_freq_count=None, min_freq=1,
-                                unknown_token='<unk2>', reserved_tokens=None)
+    i4 = text.indexer.TokenIndexer(counter, most_freq_count=None, min_freq=1,
-                                unknown_token='a', reserved_tokens=None)
+    i5 = text.indexer.TokenIndexer(counter2, most_freq_count=None, min_freq=1, unknown_token='a',
-    **sparse updates** are applied by::
+    If the storage types of weight and grad are both ``row_sparse``, and ``lazy_update`` is True, \
-    The sparse update only updates the mean and var for the weights whose row_sparse
+    The lazy update only updates the mean and var for the weights whose row_sparse
-                 **kwargs):
+                 lazy_update=True, **kwargs):
-                      stype=weight.stype),  # mean
+                      stype=stype),  # mean
-                      stype=weight.stype))  # variance
+                      stype=stype))  # variance
-                            compare_optimizer(opt1(sparse_update=True, **kwarg), opt2(**kwarg), shape,
+                        compare_optimizer(opt1(sparse_update=True, **kwarg), opt2(**kwarg), shape,
-                        help='Use Caffe input-data layer (True | False)')
+    parser.add_argument('--caffe-data', action='store_true',
-                    help='use pre-trained word2vec')
+parser.add_argument('--pretrained-embedding', action='store_true',
-                    help='use pre-trained word2vec')
+parser.add_argument('--pretrained-embedding', action='store_true',
-                        help='Use Double DQN')
+    parser.add_argument('--double-q', action='store_true',
-                    help='whether to use bidirectional layers')
+parser.add_argument('--bidirectional', action='store_true',
-                        help='show detection time')
+    parser.add_argument('--no-force', dest='force_nms', action='store_false',
-                        help='force non-maximum suppression on different class')
+    parser.add_argument('--no-force', dest='force_nms', action='store_false',
-    parser.add_argument('--force', dest='force_nms', type=bool, default=False,
+    parser.add_argument('--force', dest='force_nms', action='store_true',
-    parser.add_argument('--use-difficult', dest='use_difficult', type=bool, default=False,
+    parser.add_argument('--use-difficult', dest='use_difficult', action='store_true',
-                        help='use PASCAL VOC 07 metric')
+    parser.add_argument('--no-voc07', dest='use_voc07_metric', action='store_false',
-                        type=bool, default=True)
+    parser.add_argument('--no-shuffle', dest='shuffle', help='shuffle list',
-    parser.add_argument('--force', dest='force_nms', type=bool, default=False,
+    parser.add_argument('--force', dest='force_nms', action='store_true',
-    parser.add_argument('--use-difficult', dest='use_difficult', type=bool, default=False,
+    parser.add_argument('--use-difficult', dest='use_difficult', action='store_true',
-                        help='use PASCAL VOC 07 11-point metric')
+    parser.add_argument('--no-voc07', dest='use_voc07_metric', action='store_false',
-    cgroup.add_argument('--list', type=bool, default=False,
+    cgroup.add_argument('--list', action='store_true',
-    cgroup.add_argument('--recursive', type=bool, default=False,
+    cgroup.add_argument('--recursive', action='store_true',
-        im2rec will randomize the image order in <prefix>.lst')
+    cgroup.add_argument('--no-shuffle', dest='shuffle', action='store_false',
-    rgroup.add_argument('--pass-through', type=bool, default=False,
+    rgroup.add_argument('--pass-through', action='store_true',
-    rgroup.add_argument('--center-crop', type=bool, default=False,
+    rgroup.add_argument('--center-crop', action='store_true',
-    rgroup.add_argument('--pack-label', type=bool, default=False,
+    rgroup.add_argument('--pack-label', action='store_true',
-from .indexer import TokenIndexer
+from . import _constants as C
-class TokenEmbedding(TokenIndexer):
+class TokenEmbedding(indexer.TokenIndexer):
-    the available `embedding_name` and `pretrained_file_name`, use
+    To load token embeddings from an externally hosted pre-trained token embedding file, such as
-    embedding file, use :class:`~mxnet.text.embedding.CustomEmbedding`.
+    Alternatively, to load embedding vectors from a custom pre-trained token embedding file, use
-    embedding vector initialized by `init_unknown_vec`.
+    For every unknown token, if its representation `self.unknown_token` is encountered in the
-    the rest will be skipped.
+    If a token is encountered multiple times in the pre-trained token embedding file, only the
-    instances of :class:`~mxnet.text.embedding.TokenEmbedding`.
+    For the same token, its index and embedding vector may vary across different instances of
-        are aligned.
+        A list of indexed tokens where the list indices and the token indices are aligned.
-        unknown token will be indexed as the same representation.
+        The representation for any unknown token. In other words, any unknown token will be indexed
-        unknown_token token and a padding token.
+        For all the indexed tokens in this embedding, this NDArray maps each token's index to an
-                                 cls=embedding_cls,
+        return url_format.format(repo_url=repo_url, cls=embedding_cls,
-                        init_unknown_vec, encoding='utf8'):
+    def _load_embedding(self, pretrained_file_path, elem_delim, init_unknown_vec, encoding='utf8'):
-        embedding vector initialized by `init_unknown_vec`.
+        For every unknown token, if its representation `self.unknown_token` is encountered in the
-        be loaded and the rest will be skipped.
+        If a token is encountered multiple times in the pre-trained text embedding file, only the
-                     pretrained_file_path)
+        logging.info('Loading pre-trained token embedding vectors from %s', pretrained_file_path)
-                                       % (line_num, pretrained_file_path)
+                assert len(elems) > 1, 'At line %d of the pre-trained text embedding file: the ' \
-                                  % (line_num, token))
+                    warnings.warn('At line %d of the pre-trained token embedding file: the '
-                                  'vector %s is likely a header and is '
+                    warnings.warn('At line %d of the pre-trained text embedding file: token %s '
-                            'of all the tokens must be the same.' \
+                            'At line %d of the pre-trained token embedding file: the dimension ' \
-                shape=self.vec_len)
+            self._idx_to_vec[C.UNKNOWN_IDX] = init_unknown_vec(shape=self.vec_len)
-            in the lower case will be looked up.
+            If False, each token in the original case will be looked up; if True, each token in the
-            NDArray of shape=(len(tokens), self.vec_len).
+            The embedding vector(s) of the token(s). According to numpy conventions, if `tokens` is
-                       for token in tokens]
+            indices = [self.token_to_idx.get(token, C.UNKNOWN_IDX) for token in tokens]
-                            self.idx_to_vec.shape[0], self.idx_to_vec.shape[1])
+        vecs = nd.Embedding(nd.array(indices), self.idx_to_vec, self.idx_to_vec.shape[0],
-            updated.
+            A token or a list of tokens whose embedding vector are to be updated.
-            list of multiple strings, it must be 2-D.
+            An NDArray to be assigned to the embedding vectors of `tokens`. Its length must be equal
-            'The property `idx_to_vec` has not been properly set.'
+        assert self.idx_to_vec is not None, 'The property `idx_to_vec` has not been properly set.'
-                'singleton.'
+            assert isinstance(new_vectors, nd.NDArray) and len(new_vectors.shape) in [1, 2], \
-                'of multiple strings.'
+            assert isinstance(new_vectors, nd.NDArray) and len(new_vectors.shape) == 2, \
-                                 (token, self.idx_to_token[C.UNKNOWN_IDX]))
+                raise ValueError('Token %s is unknown. To update the embedding vector for an '
-        embedding with :func:`~mxnet.text.embedding.TokenEmbedding.create`.
+        Once an embedding is registered, we can create an instance of this embedding with
-            TokenEmbedding, 'token embedding')
+        register_text_embedding = registry.get_register_func(TokenEmbedding, 'token embedding')
-        get_embedding_and_pretrained_file_names()`.
+        Creates a token embedding instance by loading embedding vectors from an externally hosted
-            externally hosted pre-trained token embedding file.
+            A token embedding instance that loads embedding vectors from an externally hosted
-            TokenEmbedding, 'token embedding')
+        create_text_embedding = registry.get_create_func(TokenEmbedding, 'token embedding')
-                            embedding_name,
+            raise KeyError('Cannot find pretrained file %s for token embedding %s. Valid '
-        of `embedding_name` with associated `pretrained_file_name`.
+        To load token embedding vectors from an externally hosted pre-trained token embedding file,
-            embedding_name, pretrained_file_name)`.
+            A list of all the valid pre-trained token embedding file names (`pretrained_file_name`)
-                               'valid embedding names.' % embedding_name)
+                               'embedding_name=None).keys()` to get all the valid embedding '
-    representations showcase interesting linear substructures of the word vector
+    GloVe is an unsupervised learning algorithm for obtaining vector representations for words.
-        token.
+        The callback used to initialize the embedding vector for the unknown token.
-        are aligned.
+        A list of indexed tokens where the list indices and the token indices are aligned.
-        unknown token will be indexed as the same representation.
+        The representation for any unknown token. In other words, any unknown token will be indexed
-        unknown_token token and a padding token.
+        For all the indexed tokens in this embedding, this NDArray maps each token's index to an
-                                                          pretrained_file_name)
+        pretrained_file_path = GloVe._get_pretrained_file(embedding_root, pretrained_file_name)
-    devices. (Source from https://fasttext.cc/)
+    FastText is an open-source, free, lightweight library that allows users to learn text
-    files, visit
+    To get the updated URLs to the externally hosted pre-trained token embedding files, visit
-        token.
+        The callback used to initialize the embedding vector for the unknown token.
-        are aligned.
+        A list of indexed tokens where the list indices and the token indices are aligned.
-        unknown token will be indexed as the same representation.
+        The representation for any unknown token. In other words, any unknown token will be indexed
-        unknown_token token and a padding token.
+        For all the indexed tokens in this embedding, this NDArray maps each token's index to an
-                                                             pretrained_file_name)
+        pretrained_file_path = FastText._get_pretrained_file(embedding_root, pretrained_file_name)
-    embedding file.
+    This is to load embedding vectors from a user-defined pre-trained text embedding file.
-    a custom pre-trained token embedding file is:
+    Denote by '<ed>' the argument `elem_delim`. Denote by <v_ij> the j-th element of the token
-    <v_22><ed>...<ed><v_2k>\\\\n...'
+    '<token_1><ed><v_11><ed><v_12><ed>...<ed><v_1k>\\\\n<token_2><ed><v_21><ed><v_22><ed>...<ed>
-        value on the same line of the custom pre-trained token embedding file.
+        The delimiter for splitting a token and every embedding vector element value on the same
-        token.
+        The callback used to initialize the embedding vector for the unknown token.
-        are aligned.
+        A list of indexed tokens where the list indices and the token indices are aligned.
-        unknown token will be indexed as the same representation.
+        The representation for any unknown token. In other words, any unknown token will be indexed
-        unknown_token token and a padding token.
+        For all the indexed tokens in this embedding, this NDArray maps each token's index to an
-                             encoding)
+        self._load_embedding(pretrained_file_path, elem_delim, init_unknown_vec, encoding)
-class Glossary(TokenEmbedding):
+class Glossary(embedding.TokenEmbedding):
-    :class:`~mxnet.text.embedding.TokenEmbedding`.
+    For each indexed token in a glossary, an embedding vector will be associated with it. Such
-        and tuple.
+        Counts text token frequencies in the text data. Its keys will be indexed according to
-        each token.
+        One or multiple pre-trained token embeddings to load. If it is a list of multiple
-        than its largest possible value restricted by `counter` and
+        The maximum possible number of the most frequent tokens in the keys of `counter` that can be
-        be indexed.
+        The minimum frequency required for a token in the keys of `counter` to be indexed.
-        hashable type. Examples: str, int, and tuple.
+        The representation for any unknown token. In other words, any unknown token will be indexed
-        and tuple.
+        A list of reserved tokens that will always be indexed, such as special symbols representing
-        are aligned.
+        A list of indexed tokens where the list indices and the token indices are aligned.
-        unknown token will be indexed as the same representation.
+        The representation for any unknown token. In other words, any unknown token will be indexed
-        unknown_token token and a padding token.
+        For all the indexed tokens in this embedding, this NDArray maps each token's index to an
-                 min_freq=1, unknown_token='<unk>', reserved_tokens=None):
+    def __init__(self, counter, token_embeddings, most_freq_count=None, min_freq=1,
-                'whose embedding vectors will be loaded or ' \
+            assert isinstance(embed, embedding.TokenEmbedding), \
-                                       unknown_token=unknown_token,
+        super(Glossary, self).__init__(counter=counter, most_freq_count=most_freq_count,
-            concatenated for each token.
+            One or multiple pre-trained token embeddings to load. If it is a list of multiple
-from collections import Counter
+import collections
-from . import constants as C
+from . import _constants as C
-    :class:`~mxnet.text.embedding.TokenEmbedding`, such as instances of
+    Build indices for the unknown token, reserved tokens, and input counter keys. Indexed tokens can
-        and tuple.
+        Counts text token frequencies in the text data. Its keys will be indexed according to
-        effect.
+        The maximum possible number of the most frequent tokens in the keys of `counter` that can be
-        be indexed.
+        The minimum frequency required for a token in the keys of `counter` to be indexed.
-        hashable type. Examples: str, int, and tuple.
+        The representation for any unknown token. In other words, any unknown token will be indexed
-        and tuple.
+        A list of reserved tokens that will always be indexed, such as special symbols representing
-        unknown token will be indexed as the same representation.
+        The representation for any unknown token. In other words, any unknown token will be indexed
-                 unknown_token='<unk>', reserved_tokens=None):
+    def __init__(self, counter=None, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-                                     most_freq_count, min_freq)
+            self._index_counter_keys(counter, unknown_token, reserved_tokens, most_freq_count,
-                                           reserved_tokens):
+    def _index_unknown_and_reserved_tokens(self, unknown_token, reserved_tokens):
-                              enumerate(self._idx_to_token)}
+        self._token_to_idx = {token: idx for idx, token in enumerate(self._idx_to_token)}
-                            most_freq_count, min_freq):
+    def _index_counter_keys(self, counter, unknown_token, reserved_tokens, most_freq_count,
-        `most_freq_count` and `min_freq`.
+        Indexes keys of `counter` according to frequency thresholds such as `most_freq_count` and
-        assert isinstance(counter, Counter), \
+        assert isinstance(counter, collections.Counter), \
-            if reserved_tokens is not None else set()
+        unknown_and_reserved_tokens = set(reserved_tokens) if reserved_tokens is not None else set()
-            indexer.
+            A token index or a list of token indices according to the text indexer.
-                                 'invalid.' % idx)
+                raise ValueError('Token index %d in the provided `indices` is invalid.' % idx)
-from collections import Counter
+import collections
-    sequences of tokens may look like::
+    For token_delim='<td>' and seq_delim='<sd>', a specified string of two sequences of tokens may
-        counting tokens from `source_str`.
+        The collections.Counter instance to be updated with the token counts of `source_str`. If
-        `source_str`.
+        The `counter_to_update` collections.Counter instance after being updated with the token
-        return Counter(source_str)
+        return collections.Counter(source_str)
-from mxnet.contrib.text.embedding import TokenEmbedding, CustomEmbedding
+from mxnet.contrib import text
-           + token_delim + seq_delim
+    seq1 = token_delim + token_delim.join(['Life', 'is', 'great', '!']) + token_delim + seq_delim
-                                       to_lower=False)
+    cnt1 = text.utils.count_tokens_from_str(
-         "isn't": 1, 'bad': 1})
+        {'is': 2, 'life': 2, '.': 2, 'Life': 1, 'great': 1, '!': 1, 'good': 1, "isn't": 1,
-                                       to_lower=True)
+    cnt2 = text.utils.count_tokens_from_str(
-         "isn't": 1, 'bad': 1})
+        {'life': 3, 'is': 2, '.': 2, 'great': 1, '!': 1, 'good': 1, "isn't": 1, 'bad': 1})
-    cnt3 = utils.count_tokens_from_str(
+    cnt3 = text.utils.count_tokens_from_str(
-         "isn't": 1, 'bad': 1})
+        {'is': 2, 'life': 4, '.': 2, 'Life': 1, 'great': 1, '!': 1, 'good': 1, "isn't": 1,
-    cnt4 = utils.count_tokens_from_str(
+    cnt4 = text.utils.count_tokens_from_str(
-         "isn't": 1, 'bad': 1})
+        {'life': 5, 'is': 2, '.': 2, 'great': 1, '!': 1, 'good': 1, "isn't": 1, 'bad': 1})
-                           unknown_token='<unk>', reserved_tokens=None)
+    indexer = text.indexer.TokenIndexer(counter, most_freq_count=None, min_freq=1,
-
+    indexer = text.indexer.TokenIndexer(counter, most_freq_count=None, min_freq=1,
-    class Test(TokenEmbedding):
+    @text.embedding.TokenEmbedding.register
-            {'embedding_test.vec': '29b9a6511cf4b5aae293c44a9ec1365b74f2a2f8'} # 33 bytes
+            {'embedding_test.vec': '29b9a6511cf4b5aae293c44a9ec1365b74f2a2f8'}
-                                                             pretrained_file_name)
+            pretrained_file_path = Test._get_pretrained_file(
-    test_embed = TokenEmbedding.create('test')
+    test_embed = text.embedding.TokenEmbedding.create('test')
-
+    assert_almost_equal(
-                             '0.05']) + '\n'
+    seq1 = token_delim.join(['a', '0.01', '0.02', '0.03', '0.04', '0.05']) + '\n'
-                             '0.15']) + '\n'
+    seq1 = token_delim.join(['a', '0.01', '0.02', '0.03', '0.04', '0.05']) + '\n'
-                         pretrain_file)
+    _mk_my_pretrain_file(os.path.join(embed_root, embed_name), elem_delim, pretrain_file)
-    my_embed = CustomEmbedding(pretrain_file_path, elem_delim)
+    my_embed = text.embedding.CustomEmbedding(pretrain_file_path, elem_delim)
-                                  [0, 0, 0, 0, 0]]))
+    assert_almost_equal(unk_vecs.asnumpy(), np.array([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]))
-                                unknown_token='<unk>')
+    my_embed2 = text.embedding.CustomEmbedding(
-                                unknown_token='<unk1>')
+    my_embed3 = text.embedding.CustomEmbedding(
-                 elem_delim)
+    _mk_my_invalid_pretrain_file(os.path.join(embed_root, embed_name), elem_delim,
-                 elem_delim)
+    assertRaises(AssertionError, text.embedding.CustomEmbedding, pretrain_file_path, elem_delim)
-    assert g6.token_to_idx == {'<unk>': 0, 'c': 1, 'b': 2, 'a': 3,
+    i1 = text.indexer.TokenIndexer(counter, most_freq_count=None, min_freq=1, unknown_token='<unk>',
-                             ('c', 'c'), ('c', 'c'), ('c', 'c'),
+    assert i6.idx_to_token[1] == 'c'
-                                ('b', 'b'): 2, ('a', 'a'): 3,
+    i14 = text.indexer.TokenIndexer(counter_tuple, most_freq_count=None, min_freq=1,
-    assert g14.reserved_tokens is None
+    assert i14.idx_to_token[1] == ('c', 'c')
-                         pretrain_file)
+    _mk_my_pretrain_file(os.path.join(embed_root, embed_name), elem_delim, pretrain_file)
-                               init_unknown_vec=nd.ones)
+    my_embed = text.embedding.CustomEmbedding(pretrain_file_path, elem_delim,
-                  unknown_token='<unk>', reserved_tokens=['<pad>'])
+    g1 = text.glossary.Glossary(counter, my_embed, most_freq_count=None, min_freq=1,
-                               'some_word$': 5}
+    assert g1.token_to_idx == {'<unk>': 0, '<pad>': 1, 'c': 2, 'b': 3, 'a': 4, 'some_word$': 5}
-                                              lower_case_backup=True).asnumpy(),
+    assert_almost_equal(g1.get_vecs_by_tokens(['A', 'b'], lower_case_backup=True).asnumpy(),
-                 nd.array([0, 0, 0, 0, 0]))
+    assertRaises(ValueError, g1.update_token_vectors, 'unknown$$$', nd.array([0, 0, 0, 0, 0]))
-                 nd.array([0]))
+    assertRaises(AssertionError, g1.update_token_vectors, '<unk>', nd.array([0]))
-                            )
+    g1.update_token_vectors(['<unk>'], nd.array([0, 0, 0, 0, 0]))
-                            )
+    g1.update_token_vectors(['<unk>'], nd.array([[10, 10, 10, 10, 10]]))
-                            )
+    g1.update_token_vectors('<unk>', nd.array([0, 0, 0, 0, 0]))
-                            )
+    g1.update_token_vectors('<unk>', nd.array([[10, 10, 10, 10, 10]]))
-                          pretrain_file2)
+    _mk_my_pretrain_file(os.path.join(embed_root, embed_name), elem_delim, pretrain_file1)
-    my_embed2 = CustomEmbedding(pretrain_file_path2, elem_delim)
+    my_embed1 = text.embedding.CustomEmbedding(pretrain_file_path1, elem_delim,
-                  min_freq=1, unknown_token='<unk>', reserved_tokens=None)
+    g1 = text.glossary.Glossary(counter, [my_embed1, my_embed2], most_freq_count=None, min_freq=1,
-                               'some_word$': 4}
+    assert g1.token_to_idx == {'<unk>': 0, 'c': 1, 'b': 2, 'a': 3, 'some_word$': 4}
-                          pretrain_file4)
+    _mk_my_pretrain_file3(os.path.join(embed_root, embed_name), elem_delim, pretrain_file3)
-                                unknown_token='<unk2>')
+    my_embed3 = text.embedding.CustomEmbedding(pretrain_file_path3, elem_delim,
-                  min_freq=1, unknown_token='<unk>', reserved_tokens=None)
+    g2 = text.glossary.Glossary(counter, [my_embed3, my_embed4], most_freq_count=None, min_freq=1,
-                  min_freq=1, unknown_token='<unk1>', reserved_tokens=None)
+    g3 = text.glossary.Glossary(counter, [my_embed3, my_embed4], most_freq_count=None, min_freq=1,
-                  min_freq=1, unknown_token='<unk2>', reserved_tokens=None)
+    g4 = text.glossary.Glossary(counter, [my_embed3, my_embed4],most_freq_count=None, min_freq=1,
-                  min_freq=1, unknown_token='a', reserved_tokens=None)
+    g5 = text.glossary.Glossary(counter2, [my_embed3, my_embed4], most_freq_count=None, min_freq=1,
-    assert len(TokenEmbedding.get_embedding_and_pretrained_file_names(
+    assert len(text.embedding.TokenEmbedding.get_embedding_and_pretrained_file_names(
-    assert len(TokenEmbedding.get_embedding_and_pretrained_file_names(
+    assert len(text.embedding.TokenEmbedding.get_embedding_and_pretrained_file_names(
-    reg = TokenEmbedding.get_embedding_and_pretrained_file_names(
+    reg = text.embedding.TokenEmbedding.get_embedding_and_pretrained_file_names(
-                 TokenEmbedding.get_embedding_and_pretrained_file_names,
+    assertRaises(KeyError, text.embedding.TokenEmbedding.get_embedding_and_pretrained_file_names,
-def sequence_mask_numpy(array, lengths, value):
+# Numpy Implementation of Sequence Ops
-def check_sequence_mask(shape, xpu, mask_value):
+    # conform to [batch, seqlen, ...]
-        numeric_eps=1e-3, rtol=1e-2)
+    shapes = [(3, 4), (1, 1), (3, 4, 3, 1, 1)]
-    check_sequence_mask((3, 4), default_context(), 0.14)
+    check_sequence_func("mask", axis = 0, mask_value=-2.3)
-    exe = y.simple_bind(ctx=default_context(), data=(10, 10))
+    def zero_count(array, ratio):
-    assert (exe.grad_arrays[0].asnumpy() == exe.outputs[0].asnumpy()).all()
+        if ratio == 1:
-    assert (exe.grad_arrays[0].asnumpy() == exe.arg_arrays[0].asnumpy()).all()
+        exe.arg_arrays[0][:] = 1
-    exe = y.simple_bind(ctx=default_context(), data=(10, 10))
+        check_correctness(exe, exe.arg_arrays[0].asnumpy(), ratio)
-    assert (exe.grad_arrays[0].asnumpy() == exe.outputs[0].asnumpy()).all()
+        if ratio == 0.5:
-    assert (exe.grad_arrays[0].asnumpy() == exe.outputs[0].asnumpy()).all()
+            exe.forward(is_train=False)
-    img = np.fromstring(s, dtype=np.uint8)
+    img = np.frombuffer(s, dtype=np.uint8)
-    for _ in range(3):
+
-gpus = range(1,1+len(mx.test_utils.list_gpus()))
+num_gpus = len(mx.test_utils.list_gpus())
-import get_data
+from mxnet.test_utils import get_mnist_ubyte
-        get_data.GetMNIST_ubyte()
+        get_mnist_ubyte()
-import get_data
+from mxnet.test_utils import get_cifar10
-    get_data.GetCifar10()
+def get_cifar10_iterator(batch_size, data_shape, resize=-1, num_parts=1, part_index=0):
-        rand_mirror = True)
+        rand_mirror = True,
-        batch_size  = batch_size)
+        batch_size  = batch_size,
-def imagenet_iterator(train_data, val_data, batch_size, data_shape, resize=-1):
+
-        min_random_scale        = 0.533)
+        min_random_scale        = 0.533,
-        resize             = resize)
+        resize             = resize,
-parser.add_argument('--dataset', type=str, default='mnist',
+parser.add_argument('--dataset', type=str, default='cifar10',
-def test(ctx):
+def get_data_iters(dataset, batch_size, num_workers=1, rank=0):
-                            kvstore = opt.kvstore)
+                            kvstore = kv)
-        name, val_acc = test(ctx)
+        name, val_acc = test(ctx, val_data)
-                kvstore=opt.kvstore,
+                kvstore=kv,
-from get_data import MNISTIterator
+from mxnet.test_utils import get_mnist_iterator
-train, val = MNISTIterator(batch_size=batch_size, input_shape = (784,))
+train, val = get_mnist_iterator(batch_size=batch_size, input_shape = (784,))
-from get_data import MNISTIterator
+from mxnet.test_utils import get_mnist_iterator
-train, val = MNISTIterator(batch_size=100, input_shape = (784,))
+train, val = get_mnist_iterator(batch_size=100, input_shape = (784,))
-from get_data import MNISTIterator
+from mxnet.test_utils import get_mnist_iterator
-train, val = MNISTIterator(batch_size=100, input_shape = (784,))
+train, val = get_mnist_iterator(batch_size=100, input_shape = (784,))
-from get_data import MNISTIterator
+from mxnet.test_utils import get_mnist_iterator
-train, val = MNISTIterator(batch_size=100, input_shape = (784,))
+train, val = get_mnist_iterator(batch_size=100, input_shape = (784,))
-from get_data import MNISTIterator
+from mxnet.test_utils import get_mnist_iterator
-train, val = MNISTIterator(batch_size=100, input_shape = (784,))
+train, val = get_mnist_iterator(batch_size=100, input_shape = (784,))
-    return (train_dataiter, val_dataiter)
+from mxnet.test_utils import get_mnist_ubyte
-get_data.GetMNIST_ubyte()
+get_mnist_ubyte()
-    get_data.GetMNIST_ubyte()
+    get_mnist_ubyte()
-from common import get_data
+from mxnet.test_utils import get_cifar10
-get_data.GetCifar10()
+get_cifar10()
-from common import get_data
+from mxnet.test_utils import get_mnist_ubyte
-get_data.GetMNIST_ubyte()
+get_mnist_ubyte()
-from common import get_data, assertRaises
+from common import assertRaises
-    get_data.GetMNIST_ubyte()
+    get_mnist_ubyte()
-    get_data.GetCifar10()
+    get_cifar10()
-import os.path, re, StringIO
+import os.path, re
-  blacklist.append('TargetConditionals.h')
+    blacklist.append('TargetConditionals.h')
-  blacklist.append('process.h')
+    blacklist.append('windows.h')
-        print item
+
-
+out = BytesIO()
-        #print 'loop found: %s in ' % x, pending
+        #print('loop found: {} in {}'.format(x, pending))
-    print >>out, "//===== EXPANDED  : %s =====\n" %x
+    whtspace = '  ' * expand.treeDepth
-f = open(sys.argv[5], 'wb')
+with open(sys.argv[5], 'wb') as f:
-    print >>f, "#define MSHADOW_USE_CBLAS 0"
+    if minimum != 0:
-print >>f, '''
+    f.write(
-'''
+\n"""
-    sysheaders.append('complex.h')
+    if minimum != 0 and android != 0 and 'complex.h' not in sysheaders:
-    print >>f, "#include <%s>" % k
+    for k in sorted(sysheaders):
-print >>f, out.getvalue()
+    f.write(b'\n')
-        print 'Not processed:', x
+for src in sources:
-        net = nn.Sequential()
+        net = nn.HybridSequential()
-        assert device.device_type == 'gpu' or same(ret1, ret2), \
+        assert same(ret1, ret2), \
-        assert device.device_type == 'gpu' or same(ret1, ret2), \
+        assert same(ret1, ret2), \
-        assert device.device_type == 'gpu' or same(un1.asnumpy(), un2.asnumpy()), \
+        assert same(un1.asnumpy(), un2.asnumpy()), \
-            buckets, probs = gen_buckets_probs_with_ppf(lambda x: ss.uniform.ppf(x, loc=low, scale=high - low), 5)
+            scale = high - low
-__all__ = ['HybridConcurrent', 'Identity']
+__all__ = ['Concurrent', 'HybridConcurrent', 'Identity']
-from ..utils import _indent
+from .... import nd
-class HybridConcurrent(HybridBlock):
+class Concurrent(Sequential):
-        # use net's name_scope to give child Blocks appropriate names.
+        # use net's name_scope to give children blocks appropriate names.
-    def __init__(self, concat_dim, prefix=None, params=None):
+    def __init__(self, axis=-1, prefix=None, params=None):
-        self.register_child(block)
+        self.axis = axis
-        out = F.concat(*out, dim=self.concat_dim)
+        out = F.concat(*out, dim=self.axis)
-    This layer is often used in conjunction with HybridConcurrent
+    This block can be used in conjunction with HybridConcurrent
-from ..custom_layers import HybridConcurrent, Identity
+from ...contrib.nn import HybridConcurrent, Identity
-    out = HybridConcurrent(concat_dim=1, prefix='')
+    out = HybridConcurrent(axis=1, prefix='')
-from ..custom_layers import HybridConcurrent
+from ...contrib.nn import HybridConcurrent
-    out = HybridConcurrent(concat_dim=1, prefix=prefix)
+    out = HybridConcurrent(axis=1, prefix=prefix)
-    out = HybridConcurrent(concat_dim=1, prefix=prefix)
+    out = HybridConcurrent(axis=1, prefix=prefix)
-    out = HybridConcurrent(concat_dim=1, prefix=prefix)
+    out = HybridConcurrent(axis=1, prefix=prefix)
-    out = HybridConcurrent(concat_dim=1, prefix=prefix)
+    out = HybridConcurrent(axis=1, prefix=prefix)
-    out = HybridConcurrent(concat_dim=1, prefix=prefix)
+    out = HybridConcurrent(axis=1, prefix=prefix)
-        branch_3x3_split = HybridConcurrent(concat_dim=1, prefix='')
+        branch_3x3_split = HybridConcurrent(axis=1, prefix='')
-        branch_3x3dbl_split = HybridConcurrent(concat_dim=1, prefix='')
+        branch_3x3dbl_split = HybridConcurrent(axis=1, prefix='')
-from ..custom_layers import HybridConcurrent
+from ...contrib.nn import HybridConcurrent
-    paths = HybridConcurrent(concat_dim=1, prefix='')
+    paths = HybridConcurrent(axis=1, prefix='')
-        test_pretrain = model_name in pretrained_to_test
+        test_pretrain = True #model_name in pretrained_to_test
-            loss = pred - pred*label + max_val + F.log(F.exp(-max_val)+F.exp(-pred-max_val))
+            # We use the stable formula: max(x, 0) - x * z + log(1 + exp(-abs(x)))
-          as pred.
+        - **label**: truth tensor with values -1/1 (label_format is 'signed')
-        loss = F.log(1.0 + F.exp(-pred * label))
+        if self._label_format == 'signed':
-        from the store with specified row_ids.
+        from the store with specified row_ids. When there is only one row_id, KVStoreRowSparsePull \
-        ckeys, cvals, use_str_keys = _ctype_key_value(key, out)
+        if isinstance(row_ids, NDArray):
-def init_kv_with_str(stype='default'):
+def init_kv_with_str(stype='default', kv_type='local'):
-    kv = mx.kv.create()
+    kv = mx.kv.create(kv_type)
-    kv.init('e', mx.nd.ones(shape).tostype('row_sparse'))
+def test_rsp_push_pull():
-        vals_to_pull = vals[0] if len(vals) == 1 else vals
+        def check_rsp_pull(kv, count, ctxs, is_same_rowid=False, use_slice=False):
-                assert_almost_equal(retained[row], expected_val)
+            kv.row_sparse_pull('e', out=vals_to_pull, row_ids=row_ids_to_pull)
-    check_row_sparse_pull(kv, 4, mx.gpu(0))
+        check_rsp_pull(kv, 1, [mx.gpu(0)])
-    test_row_sparse_pull()
+    test_rsp_push_pull()
-                      mp_sgd_update, mp_sgd_mom_update, square, ftrl_update, ftml_update)
+                      mp_sgd_update, mp_sgd_mom_update, square, ftrl_update, ftml_update,
-                                                            ctx=weight.context)
+                                                            weight.shape, weight.context)
-import sys
+from __future__ import print_function
-from mxnet.symbol_doc import SymbolDoc
+from collections import namedtuple
-import math
+
-def lstm(num_hidden, indata, prev_state, param, seqidx, layeridx):
+def _lstm(num_hidden, indata, prev_state, param, seqidx, layeridx):
-                num_hidden, num_label):
+def _lstm_unroll_base(num_lstm_layer, seq_len, num_hidden):
-    assert (len(last_states) == num_lstm_layer)
+    assert len(last_states) == num_lstm_layer
-    # embeding layer
+    # embedding layer
-                              seqidx=seqidx, layeridx=i)
+            next_state = _lstm(
-    pred_ctc = mx.sym.Reshape(data=pred_fc, shape=(-4, seq_len, -1, 0))
+
-    softmax_class = mx.symbol.SoftmaxActivation(data=pred_fc)
+    softmax_class = mx.symbol.SoftmaxActivation(data=pred)
-
+
-               )
+# Licensed to the Apache Software Foundation (ASF) under one
-
+""" An example of predicting CAPTCHA image data with a LSTM network pre-trained with a CTC loss"""
-import mxnet as mx
+import argparse
-import numpy as np
+import sys
-import os
+import numpy as np
-    CONST_CHAR='0123456789'
+    CONST_CHAR = '0123456789'
-        self.init_state_dict={}
+        self.init_state_dict = {}
-        all_shapes = [('data', (batch_size, 80 * 30))] + init_states + [('label', (batch_size, num_label))]
+        all_shapes = [('data', (batch_size, 80, 30))] + init_states + [('label', (batch_size, num_label))]
-        self.predictor.forward(data=img, **self.init_state_dict)
+        self.predictor = Predictor(open(self.path_of_json, 'rb').read(),
-    def __get_string(self, label_list):
+    @staticmethod
-    img = cv2.imread('sample.jpg', 0)
+    img = cv2.imread('sample0.png', 0)
-    model.save("ocr")
+import re
-    def collect_params(self):
+    def collect_params(self, select=None):
-        children's Parameters."""
+        children's Parameters(default), also can returns the select :py:class:`ParameterDict`
-        ret.update(self.params)
+        if not select:
-            ret.update(cld.collect_params())
+            ret.update(cld.collect_params(select=select))
-
+def test_collect_paramters():
-                 transform=None):
+    def __init__(self, root=os.path.join('~', '.mxnet', 'datasets', 'mnist'),
-                 transform=None):
+    def __init__(self, root=os.path.join('~', '.mxnet', 'datasets', 'fashion-mnist'),
-                 transform=None):
+    def __init__(self, root=os.path.join('~', '.mxnet', 'datasets', 'cifar10'),
-                 transform=None):
+    def __init__(self, root=os.path.join('~', '.mxnet', 'datasets', 'cifar100'),
-def get_model_file(name, root='~/.mxnet/models/'):
+def get_model_file(name, root=os.path.join('~', '.mxnet', 'models')):
-def purge(root='~/.mxnet/models/'):
+def purge(root=os.path.join('~', '.mxnet', 'models')):
-def alexnet(pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
+def alexnet(pretrained=False, ctx=cpu(),
-def get_densenet(num_layers, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
+def get_densenet(num_layers, pretrained=False, ctx=cpu(),
-def inception_v3(pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
+def inception_v3(pretrained=False, ctx=cpu(),
-def get_mobilenet(multiplier, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
+def get_mobilenet(multiplier, pretrained=False, ctx=cpu(),
-def get_resnet(version, num_layers, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
+def get_resnet(version, num_layers, pretrained=False, ctx=cpu(),
-def get_squeezenet(version, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
+def get_squeezenet(version, pretrained=False, ctx=cpu(),
-def get_vgg(num_layers, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
+def get_vgg(num_layers, pretrained=False, ctx=cpu(),
-                self.features.add(_conv3x3(channels[0], 1, 3))
+                self.features.add(_conv3x3(channels[0], 1, 0))
-                                            in_channels=3))
+                self.features.add(nn.Conv2D(channels[0], 7, 2, 3, use_bias=False))
-                self.features.add(_conv3x3(channels[0], 1, 3))
+                self.features.add(_conv3x3(channels[0], 1, 0))
-                                            in_channels=3))
+                self.features.add(nn.Conv2D(channels[0], 7, 2, 3, use_bias=False))
-    scala_path = app.builder.srcdir + '/../scala-package/core/src/main/scala/ml/dmlc/mxnet'
+    scala_path = app.builder.srcdir + '/../scala-package/core/src/main/scala/org/apache/mxnet'
-    scaladocs = ['index', 'index.html', 'ml', 'lib', 'index.js', 'package.html']
+    scaladocs = ['index', 'index.html', 'org', 'lib', 'index.js', 'package.html']
-    labels = labels.asnumpy()
+    labels = labels.asnumpy().astype(int)
-# pylint: skip-file
+# pylint: disable=missing-docstring, arguments-differ
-   import pickle
+
-    def setup(self, dims, sparseness_penalty=None, pt_dropout=None, ft_dropout=None, input_act=None, internal_act='relu', output_act=None):
+    def setup(self, dims, sparseness_penalty=None, pt_dropout=None,
-                                                sparseness_penalty, idropout, odropout, encoder_act, decoder_act)
+            istack, iargs, iargs_grad, iargs_mult, iauxs = self.make_stack(
-        self.decoder = self.make_decoder(self.encoder, dims, sparseness_penalty, ft_dropout, internal_act, input_act)
+        self.encoder, self.internals = self.make_encoder(
-                   odropout=None, encoder_act='relu', decoder_act='relu'):
+    def make_stack(self, istack, data, num_input, num_hidden, sparseness_penalty=None,
-                x = mx.symbol.IdentityAttachKLSparseReg(data=x, name='sparse_encoder_%d' % istack, penalty=sparseness_penalty)
+                x = mx.symbol.IdentityAttachKLSparseReg(
-                x = mx.symbol.IdentityAttachKLSparseReg(data=x, name='sparse_decoder_%d' % istack, penalty=sparseness_penalty)
+                x = mx.symbol.IdentityAttachKLSparseReg(
-            auxs['sparse_encoder_%d_moving_avg' % istack] = mx.nd.ones((num_hidden), self.xpu) * 0.5
+            auxs['sparse_encoder_%d_moving_avg' % istack] = mx.nd.ones(num_hidden, self.xpu) * 0.5
-            auxs['sparse_decoder_%d_moving_avg' % istack] = mx.nd.ones((num_input), self.xpu) * 0.5
+            auxs['sparse_decoder_%d_moving_avg' % istack] = mx.nd.ones(num_input, self.xpu) * 0.5
-            init(k,v)
+        for k, v in args.items():
-    def make_encoder(self, data, dims, sparseness_penalty=None, dropout=None, internal_act='relu', output_act=None):
+    def make_encoder(self, data, dims, sparseness_penalty=None, dropout=None, internal_act='relu',
-                    x = mx.symbol.IdentityAttachKLSparseReg(data=x, name='sparse_encoder_%d' % i, penalty=sparseness_penalty)
+                if internal_act == 'sigmoid' and sparseness_penalty:
-                    x = mx.symbol.IdentityAttachKLSparseReg(data=x, name='sparse_encoder_%d' % i, penalty=sparseness_penalty)
+                if output_act == 'sigmoid' and sparseness_penalty:
-    def make_decoder(self, feature, dims, sparseness_penalty=None, dropout=None, internal_act='relu', input_act=None):
+    def make_decoder(self, feature, dims, sparseness_penalty=None, dropout=None,
-                    x = mx.symbol.IdentityAttachKLSparseReg(data=x, name='sparse_decoder_%d' % i, penalty=sparseness_penalty)
+                if internal_act == 'sigmoid' and sparseness_penalty:
-                    x = mx.symbol.IdentityAttachKLSparseReg(data=x, name='sparse_decoder_%d' % i, penalty=sparseness_penalty)
+                if input_act == 'sigmoid' and sparseness_penalty:
-    def layerwise_pretrain(self, X, batch_size, n_iter, optimizer, l_rate, decay, lr_scheduler=None, print_every=1000):
+    def layerwise_pretrain(self, X, batch_size, n_iter, optimizer, l_rate, decay,
-        solver = Solver(optimizer, momentum=0.9, wd=decay, learning_rate=l_rate, lr_scheduler=lr_scheduler)
+        solver = Solver(optimizer, momentum=0.9, wd=decay, learning_rate=l_rate,
-                                            data_iter, X.shape[0], self.xpu).values())[0]
+                X_i = list(model.extract_feature(
-                         0, n_iter, {}, False)
+            logging.info('Pre-training layer %d...', i)
-    def finetune(self, X, batch_size, n_iter, optimizer, l_rate, decay, lr_scheduler=None, print_every=1000):
+    def finetune(self, X, batch_size, n_iter, optimizer, l_rate, decay, lr_scheduler=None,
-        solver = Solver(optimizer, momentum=0.9, wd=decay, learning_rate=l_rate, lr_scheduler=lr_scheduler)
+            return np.mean(np.square(label-pred))/2.0
-                                 X.shape[0], self.xpu).values())[0]
+        Y = list(model.extract_feature(
-parser = argparse.ArgumentParser(description='Train an auto-encoder model for mnist dataset.')
+parser = argparse.ArgumentParser(description='Train an auto-encoder model for mnist dataset.',
-                    help='the interval of printing during training.')
+                    help='interval of printing during training.')
-                    help='the batch size used for training.')
+                    help='batch size used for training.')
-                    help='the number of iterations for pretraining.')
+                    help='number of iterations for pretraining.')
-                    help='the number of iterations for fine-tuning.')
+                    help='number of iterations for fine-tuning.')
-                         'The decoder layers are created in the reverse order.')
+                    help='number of hidden units for the layers of the encoder.'
-logging.basicConfig(level=logging.DEBUG)
+logging.basicConfig(level=logging.INFO)
-        internal_act='relu', output_act='relu')
+    xpu = mx.gpu() if gpu else mx.cpu()
-                                decay=0.0, lr_scheduler=mx.misc.FactorScheduler(20000,0.1),
+                                decay=0.0, lr_scheduler=mx.lr_scheduler.FactorScheduler(20000, 0.1),
-                      lr_scheduler=mx.misc.FactorScheduler(20000,0.1), print_every=print_every)
+                      lr_scheduler=mx.lr_scheduler.FactorScheduler(20000, 0.1), print_every=print_every)
-            plt.imshow(original_image.reshape((28,28)))
+            plt.imshow(original_image.reshape((28, 28)))
-# pylint: skip-file
+# pylint: disable=missing-docstring
-   import pickle
+    import cPickle as pickle
-    outputs = [[] for i in exe.outputs]
+    outputs = [[] for _ in exe.outputs]
-# pylint: skip-file
+# pylint: disable=missing-docstring
-import logging
+
-        if i%self.interval == 0 and logging.getLogger().isEnabledFor(self.level):
+        if i % self.interval == 0 and logging.getLogger().isEnabledFor(self.level):
-                logging.log(self.level, 'Iter:%d  param:%s\t\tstat(%s):%s'%(i, key, self.stat.__name__, str(self.stat(arr.asnumpy()))))
+                logging.log(self.level, 'Iter:%d  param:%s\t\tstat(%s):%s',
-        if i%self.interval == 0 and logging.getLogger().isEnabledFor(self.level):
+        if i % self.interval == 0 and logging.getLogger().isEnabledFor(self.level):
-                metric.reset()
+                logging.log(self.level, 'Iter:%d  param:%s\t\tstat(%s):%s\t\tgrad_stat:%s',
-              data_iter, begin_iter, end_iter, args_lrmult={}, debug = False):
+              data_iter, begin_iter, end_iter, args_lrmult=None, debug=False):
-                        x = mx.symbol.BlockGrad(x, name=blob_names[i])
+            for x in sym.get_internals():
-        update_dict = {name: nd for name, nd in zip(sym.list_arguments(), exe.grad_arrays) if nd is not None}
+        update_dict = {
-            except:
+            except StopIteration:
-        dll_path.extend([p.strip() for p in os.environ['LD_LIBRARY_PATH'].split(":")])
+        dll_path[0:0] = [p.strip() for p in os.environ['LD_LIBRARY_PATH'].split(":")]
-    **sparse updates** are applied by::
+    If the storage types of weight and grad are both ``row_sparse``, and ``lazy_update`` is True, \
-    def __init__(self, momentum=0.0, **kwargs):
+    def __init__(self, momentum=0.0, lazy_update=True, **kwargs):
-            momentum = zeros(weight.shape, weight.context, dtype=weight.dtype, stype=weight.stype)
+            momentum = zeros(weight.shape, weight.context, dtype=weight.dtype, stype=stype)
-    assert (mx.nd.scatter_nd(data, idx, shape=(2, 2)).asnumpy() == [[0, 0], [2, 3]]).all()
+        assert (mx.nd._internal._backward_gather_nd(y, idx, shape=data.shape).asnumpy() == data.grad.asnumpy()).all()
-    long as they have the same number of elements.
+    containing values -1 or 1 (0 or 1 if `label_format` is binary).
-
+    label_format : str, default 'signed'
-    def __init__(self, weight=None, batch_axis=0, **kwargs):
+    def __init__(self, weight=None, batch_axis=0, label_format='signed', **kwargs):
-    def export(self, path):
+    def export(self, path, epoch=0):
-            will be created.
+            Path to save model. Two files `path-symbol.json` and `path-xxxx.params`
-        ndarray.save('%s-0000.params'%path, arg_dict)
+        ndarray.save('%s-%04d.params'%(path, epoch), arg_dict)
-    hideen = mx.symbol.Activation(data=hidden, act_type='relu')
+    hidden = mx.symbol.Activation(data=hidden, act_type='relu')
-    hideen = mx.symbol.Activation(data=hidden, act_type='relu')
+    hidden = mx.symbol.Activation(data=hidden, act_type='relu')
-      url='https://github.com/dmlc/mxnet',
+      url='https://github.com/apache/incubator-mxnet',
-        rhs_nd = rand_ndarray(rhs_shape, rhs_stype, density=rhs_den, distribution="uniform")
+        if rhs_stype == 'csr':
-            raise ValueError("Value other than csr for lhs not supported")
+
-        rhs_stype = "row_sparse" if rhs == "rsp" else "default"
+        else:
-        density_list = data_dict['density']
+            feature_dim_list = data_dict['feature_dim']
-        num_repeat = data_dict['num_repeat']
+            default_output_index = data_dict['default_index']['output_dim']
-                      lhs_stype, rhs_stype, density, rhs_density, lhs_trans, ctx,
+                      lhs_stype, rhs_stype, density, density, lhs_trans, ctx,
-from test_sparse_ndarray import test_sparse_nd_setitem
+from test_sparse_ndarray import test_sparse_nd_setitem, test_sparse_nd_binary_scalar_op
-    def check(fn, stype):
+    def check(fn, stype, out_stype=None):
-        check(lambda x: x / 2, stype)
+        check(lambda x: x / 2, stype, out_stype=stype)
-import argparse
+from mxnet.test_utils import *
-##########################################################
+DEFAULT_DATA_DIR = "datasets/caltech101/data"
-    
+    '''convert the caltech101 mat file to images
-    
+
-    
+
-    
+
-    
+
-        img = img.resize([h,w], Image.BILINEAR)
+        img = img.resize([h, w], Image.BILINEAR)
-        
+    '''Parses input args
-    parser.add_argument('--invert', help='invert the image color i.e. default shapes are black and background is white in caltech101, invert the shapes to white', action='store_true')
+    parser.add_argument('--dataset', help='caltech101 dataset mat file path',
-
+    # Note if you change the height or width you will need to change the network as well,
-      
+
-    main()
+    sys.exit(main())
-from PIL import Image
+import errno
-# of paper `Larsen, Anders Boesen Lindbo, et al. "Autoencoding beyond pixels using a 
+# of paper `Larsen, Anders Boesen Lindbo, et al. "Autoencoding beyond pixels using a
-#constant operator in mxnet, not used in this code
+    '''constant operator in mxnet, no used in the code
-#######################################################################        
+    '''The encoder is a CNN which takes 32x32 image as input
-    
+
-    
+
-    
+    '''The genrator is a CNN which takes 100 dimensional embedding as input
-    
+
-    gout = mx.sym.Activation(g5, name='genact5', act_type=activation)    
+    gout = mx.sym.Activation(g5, name='genact5', act_type=activation)
-
+    '''First part of the discriminator which takes a 32x32 image as input
-    
+
-    return dact3 
+    return dact3
-        
+    '''Second part of the discriminator which takes a 256x8x8 feature map as input
-            
+
-    
+
-    
+
-    
+    h = mx.sym.Flatten(dact4)
-    
+
-####################################################################### 
+    '''GaussianLogDensity loss calculation for layer wise loss
-        
+    '''Calculate the discriminator layer loss
-    
+
-    
+    label = mx.sym.Flatten(label)
-    
+
-    
+
-    
+
-####################################################################### 
+
-    
+    '''KLDivergenceLoss loss
-    
+
-        
+
-    #import ipdb; ipdb.set_trace()
+    '''Get the dataset
-    
+
-    
+
-    np.random.seed(1234) # set seed for deterministic ordering
+    data = data.reshape((data.shape[0], 1, data.shape[1], data.shape[2]))
-####################################################################### 
+    '''Create a random iterator for generator
-    
+    '''fill the ith grid of the buffer matrix with the values from the img
-    sy = (i/m)*shape[0]
+    sy = (i//m)*shape[0]
-####################################################################### 
+    '''create a grid of images and save it as a final image
-            
+
-    #num = 1
+def train(dataset, nef, ndf, ngf, nc, batch_size, Z, lr, beta1, epsilon, ctx, check_point, g_dl_weight, output_path, checkpoint_path, data_path, activation,num_epoch, save_after_every, visualize_after_every, show_after_every):
-    
+    symE = mx.sym.Group([z_mu, z_lv, z])
-    
+
-    symD2 = dloss    
+    symD2 = dloss
-    mods = [modE]    
+    mods = [modE]
-    #modD = mx.mod.Module(symbol=symD, data_names=('data',), label_names=('label',), context=ctx)
+    modD.add(modD1).add(modD2, take_labels=True, auto_wiring=True)
-    # =============module DL=============    
+
-    # =============module KL=============    
+
-        })    
+        })
-       
+
-    # ============calculating prediction accuracy==============
+        '''calculating prediction accuracy
-    # ============calculating binary cross-entropy loss==============
+        '''calculating binary cross-entropy loss
-    # ============calculating KL divergence loss==============
+
-        #label = label.ravel()
+        '''calculating KL divergence loss
-        return KLLoss   
+        return KLLoss
-            
+
-            sample = mx.io.DataBatch([z], label=None, provide_data = [('rand', (batch_size, Z, 1, 1))])                          
+            sample = mx.io.DataBatch([z], label=None, provide_data = [('rand', (batch_size, Z, 1, 1))])
-            label[:] = 0    
+            xz = modG.get_outputs()
-                        
+
-            
+            gradD22 = [[grad.copyto(grad.context) for grad in grads] for grads in modD2._exec_group.grad_arrays]
-            lx = [out.copyto(out.context) for out in modD1.get_outputs()]            
+            lx = [out.copyto(out.context) for out in modD1.get_outputs()]
-                                        
+                    gradr += 0.5 * (gradf + gradd)
-            outG = modG.get_outputs()            
+            outG = modG.get_outputs()
-            gradG1 = [[grad.copyto(grad.context) for grad in grads] for grads in modG._exec_group.grad_arrays]                        
+            gradG1 = [[grad.copyto(grad.context) for grad in grads] for grads in modG._exec_group.grad_arrays]
-            label[:] = 1    
+            xz = modG.get_outputs()
-            modD.backward()            
+            modD.backward()
-            
+
-            xz = modG.get_outputs()            
+            xz = modG.get_outputs()
-            outD1 = modD1.get_outputs()            
+            outD1 = modD1.get_outputs()
-            dlGrad = modDL.get_input_grads()                        
+            dlGrad = modDL.get_input_grads()
-           
+            modG.backward(diffD)
-            modG.update()            
+                    grad = g_dl_weight * grad + 0.5 * (gradg1 + gradg2)
-            outG = modG.get_outputs()            
+            outG = modG.get_outputs()
-            gradG1 = [[grad.copyto(grad.context) for grad in grads] for grads in modG._exec_group.grad_arrays]                        
+            gradG1 = [[grad.copyto(grad.context) for grad in grads] for grads in modG._exec_group.grad_arrays]
-            label[:] = 1    
+            xz = modG.get_outputs()
-            modD.backward()            
+            modD.backward()
-            
+
-            xz = modG.get_outputs()            
+            xz = modG.get_outputs()
-            outD1 = modD1.get_outputs()            
+            outD1 = modD1.get_outputs()
-            dlGrad = modDL.get_input_grads()                        
+            dlGrad = modDL.get_input_grads()
-           
+            modG.backward(diffD)
-            modG.update()            
+                    grad = g_dl_weight * grad + 0.5 * (gradg1 + gradg2)
-            #sample = mx.io.DataBatch([z], label=None, provide_data = [('rand', (batch_size, Z, 1, 1))])                          
+
-            
+
-            outD1 = modD1.get_outputs()            
+            outD1 = modD1.get_outputs()
-            dlGrad = modDL.get_input_grads()                        
+            dlGrad = modDL.get_input_grads()
-            
+            modG.backward(diffD)
-
+            mE.update([pred], [pred])
-                
+
-                
+
-    
+    '''Test the VAE with a pretrained encoder and generator.
-    
+    symE = mx.sym.Group([z_mu, z_lv, z])
-    #mx.viz.plot_network(symD, shape={'data': (batch_size, nc, 64, 64)}).view()
+    symG = generator(ngf, nc, no_bias=True, fix_gamma=True, eps=1e-5 + 1e-12, z_dim = Z, activation=activation )
-    # =============module G============= 
+    # =============module G=============
-        sample = mx.io.DataBatch([mu], label=None, provide_data = [('rand', (batch_size, Z, 1, 1))])         
+        sample = mx.io.DataBatch([mu], label=None, provide_data = [('rand', (batch_size, Z, 1, 1))])
-            
+        outG = modG.get_outputs()
-            savemat(embedding_path+'/'+image_name+'.mat', {'embedding':mu.asnumpy()})    
+
-    parser.add_argument('--save_embedding', help='saves the shape embedding of each input image', action='store_true')                   
+    parser.add_argument('--save_embedding', help='saves the shape embedding of each input image', action='store_true')
-    parser.add_argument('--testing_data_path', help='testing data path', default='/home/ubuntu/datasets/MPEG7dataset/images/', type=str)
+    parser.add_argument('--activation', help='activation i.e. sigmoid or tanh', default='sigmoid', type=str)
-    parser.add_argument('--checkpoint_path', help='checkpoint saving path ', default='checkpoints32x32_sigmoid/', type=str)     
+    parser.add_argument('--pretrained_generator_path', help='pretrained generator model path', default='checkpoints32x32_sigmoid/caltech_G-0045.params', type=str)
-    parser.add_argument('--batch_size', help='batch size, keep it 1 during testing', default=64, type=int)    
+    parser.add_argument('--ngf', help='generator filter count in the second last layer', default=64, type=int)
-    parser.add_argument('--beta1', help='beta1 for adam optimizer', default=0.5, type=float)    
+    parser.add_argument('--lr', help='learning rate', default=0.0002, type=float)
-    parser.add_argument('--g_dl_weight', help='discriminator layer loss weight', default=1e-1, type=float)                     
+    parser.add_argument('--g_dl_weight', help='discriminator layer loss weight', default=1e-1, type=float)
-    
+    parser.add_argument('--show_after_every', help='show metrics after this number of iterations', default=10, type=int)
-    
+
-    if args.train:        
+
-    
+
-        test(args.nef, args.ngf, args.nc, 1, args.Z, ctx, args.pretrained_encoder_path, args.pretrained_generator_path, args.output_path, args.testing_data_path, args.activation, args.save_embedding, args.embedding_path)    
+        test(args.nef, args.ngf, args.nc, 1, args.Z, ctx, args.pretrained_encoder_path, args.pretrained_generator_path, args.output_path, args.testing_data_path, args.activation, args.save_embedding, args.embedding_path)
-    logging.basicConfig(level=logging.DEBUG)    
+    logging.basicConfig(level=logging.DEBUG)
-    with file(filename) as f:
+    with open(filename) as f:
-    for line in file(fname):
+    for line in open(fname):
-            idx = range(s*slice_size, (s+1)*slice_size)
+            idx = range(int(s*slice_size), int((s+1)*slice_size))
-                      mp_sgd_update, mp_sgd_mom_update, square, ftrl_update)
+                      mp_sgd_update, mp_sgd_mom_update, square, ftrl_update, ftml_update)
-These can constructed by passing ``pretrained=True``:
+We provide pre-trained models for all the listed models.
-    sym = mx.sym.Convolution(num_filter=3, kernel=(3,), pad=(1,), name='conv')
+    sym = mx.sym.Convolution(layout='NCW', num_filter=3, kernel=(3,), pad=(1,), name='conv')
-    sym = mx.sym.Convolution(num_filter=3, kernel=(3,), stride=(2,), name='conv')
+    sym = mx.sym.Convolution(layout='NCW', num_filter=3, kernel=(3,), stride=(2,), name='conv')
-    sym = mx.sym.Convolution(num_filter=3, kernel=(3,), dilate=(2,), name='conv')
+    sym = mx.sym.Convolution(layout='NCW', num_filter=3, kernel=(3,), dilate=(2,), name='conv')
-    sym = mx.sym.Convolution(num_filter=3, kernel=(1,), pad=(0,), name='conv')
+    sym = mx.sym.Convolution(layout='NCW', num_filter=3, kernel=(1,), pad=(0,), name='conv')
-#    check_consistency_NxM([sym, sym_no_cudnn], ctx_list)
+    # 1D deconvolution
-#    # 3D convolution (not yet enabled)
+#    # 3D deconvolution (not yet enabled)
-    kernel = (2*pad[0]+1, 2*pad[1]+1)
+    ndim = len(pad)
-    assert out_shapes[0] == (input_shape[0], 5, 8, 8)
+    default_target_size = 8
-    shape = (1, 4, 9, 9)
+    for dim in [1, 2, 3]:
-        np.testing.assert_allclose(arr1.asnumpy(), arr2.asnumpy(), rtol=1e-3, atol=1e-4)
+        x = mx.sym.Variable('x')
-                            np.testing.assert_allclose(arr1.asnumpy(), arr2.asnumpy(), rtol=1e-3, atol=1e-4)
+    for dim in [1,2]:
-    spike_imgs[0,0,16,16] = 1.0
+    data_size = 33
-    out_grads[0,0, 16,16] = 1.0
+    out_grads[center] = 1.0
-    assert(np.sum(vgrad)==np.prod(kernel_shape))
+    out = out_o.reshape(out_o.shape[2:])
-    assert(out_o[0,0,16,16]==np.prod(kernel_shape))
+    assert_allclose(out_o[center],np.prod(kernel_shape),atol=1e-5)
-    white_in2 = mx.nd.ones(shape=(1,1,33,33))
+    white_in = mx.nd.ones(shape=data_shape)
-    assert(out[0,0,16,16] - np.sum(kernel_gradient) - out_orig[0,0,16,16] < 0.001)
+    assert(out[center] - np.sum(kernel_gradient) - out_orig[center] < 0.001)
-       mx.test_utils.set_env_var("MXNET_EXEC_BULK_EXEC_TRAIN", prev_bulk_train_val)
+def test_bind():
-    test_reshape()
+    import nose
-        assert layout == 'NCHW', "Only supports NCW layout for now"
+        assert layout == 'NCHW', "Only supports NCHW layout for now"
-        assert layout == 'NCDHW', "Only supports NCW layout for now"
+        assert layout == 'NCDHW', "Only supports NCDHW layout for now"
-        assert layout == 'NCHW', "Only supports NCW layout for now"
+        assert layout == 'NCHW', "Only supports NCHW layout for now"
-        assert layout == 'NCDHW', "Only supports NCW layout for now"
+        assert layout == 'NCDHW', "Only supports NCDHW layout for now"
-    ('6562166cd597a6328a32a0ce47bb651df80b3bbb', 'resnet152_v1'),
+    ('9f83e440996887baf91a6aff1cccc1c903a64274', 'mobilenet0.25'),
-    ('2a903ab21260c85673a78fe65037819a843a1f43', 'resnet50_v1'),
+    ('c940b1a062b32e3a5762f397c9d1e178b5abd007', 'resnet50_v1'),
-    ('eb7a368774aa34a12ed155126b641ae7556dad9d', 'resnet50_v2'),
+    ('81a4e66af7859a5aa904e2b4051aa0d3bc472b2f', 'resnet50_v2'),
-    out.add(nn.BatchNorm(scale=False))
+    out.add(nn.BatchNorm(scale=True))
-        self.body.add(nn.Conv2D(channels//4, kernel_size=1, strides=1))
+        self.body.add(nn.Conv2D(channels//4, kernel_size=1, strides=stride))
-        self.body.add(_conv3x3(channels//4, stride, channels//4))
+        self.body.add(_conv3x3(channels//4, 1, channels//4))
-    learning_rate: float
+    learning_rate : float
-    learning_rate: float
+    learning_rate : float
-                return True
+    """Return True if ``data`` has instance of ``dtype``.
-    or ``mx.nd.sparse.CSRNDArray``.
+    ``mx.nd.sparse.CSRNDArray`` or ``scipy.sparse.csr_matrix``.
-        if ((_has_instance(data, CSRNDArray) or _has_instance(label, CSRNDArray)) and
+        self.data = _init_data(data, allow_empty=False, default_name=data_name)
-                 last_batch_handle='pad')
+    # CSRNDArray or scipy.sparse.csr_matrix with last_batch_handle not equal to 'discard' will throw NotImplementedError
-                                    arg_names=params.keys())
+                                    wd=prior_precision)
-        urllib.request.urlretrieve(origin, data_path, context=context)
+        ctx = ssl._create_unverified_context()
-from mxnet.gluon import nn, autograd, Block, HybridBlock, Parameter, ParameterDict
+from mxnet import autograd, gluon
-from mxnet.gluon import nn, autograd, Block, HybridBlock, Parameter
+from mxnet import autograd, gluon
-    'cuda_runtime.h', 'cudnn.h', 'cudnn_lrn-inl.h', 'curand.h',
+    'cuda_runtime.h', 'cudnn.h', 'cudnn_lrn-inl.h', 'curand.h', 'curand_kernel.h',
-                                                            weight.shape, weight.context)
+                                                            shape=weight.shape,
-            adam = mx.optimizer.Adam(clip_gradient=5.0, learning_rate=0.001,
+            adam = mx.optimizer.Adam(clip_gradient=5.0, learning_rate=0.0005,
-    l = mx.random.uniform(-1, 1, shape, ctx=mx.cpu()).copyto(xpu)
+    x = mx.random.uniform(-1, 1, shape, ctx=xpu)
-    mod.fit(data_iter, num_epoch=30, optimizer_params={'learning_rate': 0.005, 'wd': 0.0005},
+    mod.fit(data_iter, num_epoch=60, optimizer_params={'learning_rate': 0.0005, 'wd': 0.0005},
-import os
+import sys, os
-logging.basicConfig(format=fmt, filemode='a+', filename='./cnn_text_classification.log', level=logging.DEBUG)
+logging.basicConfig(format=fmt, stream=sys.stdout, level=logging.DEBUG)
-    train_without_pretrained_embedding()
+
-        bucket_plan = np.hstack([np.zeros(n, int)+i for i, n in enumerate(bucket_n_batches)])
+        bucket_plan = np.hstack([np.zeros(int(n), int)+i for i, n in enumerate(bucket_n_batches)])
-
+    """ Calculates prediction perplexity
-    contexts = [mx.context.gpu(i) for i in range(1)]
+    # Update count per available GPUs
-        return (sm, data_names, label_names)
+        return sm, data_names, label_names
-            sym_kwargs[k] = v
+    _keys = []
-            vals.append(v)""")
+            _keys.append(_k)
-        vals.append(%s)"""%(name, name, name))
+        _keys.append('%s')
-        vals.append(np.dtype(%s).name)"""%(dtype_name, dtype_name, dtype_name))
+        _keys.append('%s')
-    return _symbol_creator(%d, None, sym_kwargs, keys, vals, name)"""%(
+    return _symbol_creator(%d, None, sym_kwargs, _keys, _vals, name)"""%(
-
+            # Test negative dim
-    kwargs = {'install_requires': ['numpy', 'requests', 'graphviz'], 'zip_safe': False}
+    kwargs = {'install_requires': ['numpy<=1.13.3,>=1.8.2', 'requests==2.18.4', 'graphviz==0.8.1'], 'zip_safe': False}
-__version__ = "1.0.0"
+__version__ = "1.0.1"
-# pylint:skip-file
+# pylint: disable=missing-docstring, deprecated-module
-                yield SimpleBatch(data_names, data_all, label_names, label_all)
+import mxnet as mx
-                      help = "use gpu")
+    parser.add_option("-g", "--gpu", action="store_true", dest="gpu", default=False,
-                          init_states)
+    network = get_lstm_net(data_train.vocab_size, seq_len, num_lstm_layer, num_hidden)
-    if options.gpu == True:
+    if options.gpu:
-              batch_end_callback = mx.callback.Speedometer(batch_size, 50),)
+    model = mx.mod.Module(
-sys.path.insert(0, "../../python")
+# pylint: disable=missing-docstring
-    data = mx.sym.Reshape(data = data, shape = (-1, 1, num_hidden))
+
-                                           label = label_weight)
+    pred = mx.sym.sum(data=pred, axis=2)
-def nce_loss_subwords(data, label, label_mask, label_weight, embed_weight, vocab_size, num_hidden, num_label):
+def nce_loss_subwords(
-                                         output_dim = num_hidden)
+    label_units_embed = mx.sym.Embedding(data=label,
-                                             name = 'label_units_embed')
+    # it's achieved by multiplying zeros to useless units in order to handle variable-length input.
-    label_embed = mx.sym.sum(label_units_embed, axis=2, name = 'label_embed')
+    label_embed = mx.sym.sum(label_units_embed, axis=2, name='label_embed')
-    # by boardcast_mul and sum you can get prediction scores in all num_label inputs,
+    # by boardcast_mul and sum you can get prediction scores in all label_embed inputs,
-    data = mx.sym.Reshape(data = data, shape = (-1, 1, num_hidden))
+    data = mx.sym.Reshape(data=data, shape=(-1, 1, num_hidden))
-    pred = mx.sym.sum(data = pred, axis = 2)
+    pred = mx.sym.sum(data=pred, axis=2)
-                                           label = label_weight)
+    return mx.sym.LogisticRegressionOutput(data=pred,
-        tmp = sorted(tmp, key = itemgetter(1), reverse = True)
+        tmp = sorted(tmp, key=itemgetter(1), reverse=True)
-        for a, b in tmp:
+        for a, _ in tmp:
-        tmp = sorted(tmp, key = itemgetter(1), reverse = True)
+        tmp = sorted(tmp, key=itemgetter(1), reverse=True)
-        for a, b in tmp:
+        for a, _ in tmp:
-# pylint:skip-file
+# pylint: disable=missing-docstring
-sys.path.insert(0, "../../python")
+
-from nce import *
+from nce import nce_loss, NceAccuracy
-def get_net(vocab_size, num_label):
+def get_net(num_vocab):
-                    num_label = num_label)
+    pred = mx.sym.FullyConnected(data=data, num_hidden=100)
-    data_test = DataIter(1000, batch_size, vocab_size, num_label, feature_size)
+    data_train = DataIterNce(100000, batch_size, vocab_size, num_label, feature_size)
-                                 initializer=mx.init.Xavier(factor_type="in", magnitude=2.34))
+    network = get_net(vocab_size)
-
+    model.fit(
-# pylint:skip-file
+# pylint: disable=missing-docstring
-sys.path.insert(0, "../../python")
+
-from collections import namedtuple
+from random_data import DataIterSoftmax
-def get_net(vocab_size):
+def get_net(num_labels):
-    sm = mx.sym.SoftmaxOutput(data = pred, label = label)
+    pred = mx.sym.FullyConnected(data=data, num_hidden=100)
-    data_test = DataIter(1000, batch_size, vocab_size, num_label, feature_size)
+    data_train = DataIterSoftmax(100000, batch_size, vocab_size, num_label, feature_size)
-              batch_end_callback = mx.callback.Speedometer(batch_size, 50),)
+    model = mx.mod.Module(
-# pylint:skip-file
+# pylint: disable=missing-docstring, deprecated-module
-                yield SimpleBatch(data_names, data_all, label_names, label_all)
+import mxnet as mx
-                      help = "use gpu")
+    parser.add_option("-g", "--gpu", action="store_true", dest="gpu", default=False,
-    data_train = DataIter("./data/text8", batch_size, num_label)
+    data_train = DataIterWords("./data/text8", batch_size, num_label)
-    network = get_net(data_train.vocab_size, num_label - 1, num_label)
+    network = get_word_net(data_train.vocab_size, num_label - 1)
-    if options.gpu == True:
+    if options.gpu:
-                                 initializer=mx.init.Xavier(factor_type="in", magnitude=2.34))
+    model = mx.mod.Module(
-
+    model.fit(
-# pylint:skip-file
+# pylint: disable=missing-docstring, deprecated-module
-logging.basicConfig(level=logging.INFO, format=head)
+import mxnet as mx
-
+    options, args = parser.parse_args()
-    data_train = DataIter("./data/text8", batch_size, num_label)
+    data_train = DataIterSubWords(
-    network = get_net(data_train.vocab_size, num_label - 1, num_label)
+    network = get_subword_net(data_train.vocab_size, num_label - 1, embedding_size)
-    if options.gpu == True:
+    devs = mx.cpu()
-                                 initializer=mx.init.Xavier(factor_type="in", magnitude=2.34))
+    model = mx.mod.Module(
-              batch_end_callback=mx.callback.Speedometer(batch_size, 50), )
+    model.fit(
-parser.add_argument('--data', type=str, default='./data/ptb.',
+parser.add_argument('--data', type=str, default='./data/wikitext-2/wiki.',
-sys.path.insert(0, "../../python")
+import os
-if __name__ == '__main__':
+def main():
-    contexts = [mx.context.gpu(i) for i in range(1)]
+    contexts = [mx.context.cpu(i) for i in range(1)]
-    vocab = default_build_vocab("./data/sort.train.txt")
+    vocab = default_build_vocab(os.path.join(DATA_DIR, TRAIN_FILE))
-    model = BiLSTMInferenceModel(5, len(vocab),
+    model = BiLSTMInferenceModel(SEQ_LEN, len(vocab),
-    tks = sys.argv[1:]
+
-sys.path.insert(0, "../../python")
+import os
-sys.path.insert(0, "../../python")
+import random
-if __name__ == '__main__':
+def main():
-    contexts = [mx.context.gpu(i) for i in range(1)]
+    if args.cpu:
-    vocab = default_build_vocab("./data/sort.train.txt")
+    vocab = default_build_vocab(os.path.join(DATA_DIR, TRAIN_FILE))
-    data_train = BucketSentenceIter("./data/sort.train.txt", vocab,
+    data_train = BucketSentenceIter(os.path.join(DATA_DIR, TRAIN_FILE), vocab,
-    data_val = BucketSentenceIter("./data/sort.valid.txt", vocab,
+    data_val = BucketSentenceIter(os.path.join(DATA_DIR, VALID_FILE), vocab,
-
+
-                    'softmax', 'log_softmax'])
+                    'softmax', 'log_softmax', 'rint', 'ceil', 'floor', 'trunc', 'fix'])
-        grad *= self.rescale_grad + wd * weight
+        grad = grad * self.rescale_grad + wd * weight
-        for _ in range(len(self._batch_sampler)):
+        for _ in range(num_batches):
-    ale.setInt('random_seed', rng.randint(1000))
+    ale.setInt(b'random_seed', rng.randint(1000))
-        ale.setBool('display_screen', True)
+            ale.setBool(b'sound', False) # Sound doesn't work on OSX
-    ale.loadROM(rom_path)
+        ale.setBool(b'display_screen', False)
-                   set(data_shapes.items() + [(k, v.shape) for k, v in self.params.items()])
+                   set(list(data_shapes.items()) + list([(k, v.shape) for k, v in self.params.items()]))
-        # embeding layer
+        # embedding layer
-                    sm = mx.sym.softmax_cross_entropy(fc, label, name="t%d_sm" % seqidx)
+                    # Currently softmax_cross_entropy fails https://github.com/apache/incubator-mxnet/issues/6874
-                sm = mx.sym.softmax_cross_entropy(fc, label, name="sm")
+                # Currently softmax_cross_entropy fails https://github.com/apache/incubator-mxnet/issues/6874
-        #bind with shared executor
+        # bind with shared executor
-                # updare parameters
+                # update parameters
-                train_nll += sum([x.asscalar() for x in seq_loss]) / batch_size
+                train_nll += sum([x.sum().asscalar() for x in seq_loss]) / batch_size
-                val_nll += sum([x.asscalar() for x in seq_loss]) / batch_size
+                val_nll += sum([x.sum().asscalar() for x in seq_loss]) / batch_size
-sys.path.insert(0, "../rnn")
+sys.path.insert(0, "../../rnn/old")
-        # predict by the inner product, which is elementwise product and then sum
+        # predict by the inner product, which is element-wise product and then sum
-from matrix_fact_parallel_model import matrix_fact_model_parallel_net
+from model import matrix_fact_model_parallel_net
-    # the initializer uesd to initialize the parameters
+    # the initializer used to initialize the parameters
-    '''This class implements the Variational Auto Encoder'''
+    """This class implements the Variational Auto Encoder"""
-        #save model parameters (i.e. weights and biases)
+        return(-mx.symbol.sum(mx.symbol.broadcast_mul(loss_label,mx.symbol.log(x_hat))
-        #if saved parameters, can access them at specific iteration e.g. last epoch using
+        # save loss(ELBO) for the training set
-        [N,features] = np.shape(x_train)          #number of examples and features
+        #   self.arg_params = arg_params
-            nd_iter_val = mx.io.NDArrayIter(data={'data':x_valid},label={'loss_label':x_valid},batch_size = batch_size)
+            nd_iter_val = mx.io.NDArrayIter(data={'data':x_valid}, label={'loss_label':x_valid}, batch_size=batch_size)
-        act_h = mx.sym.Activation(data=encoder_h, act_type="tanh",name="activation_h")
+        mu = mx.sym.FullyConnected(data=act_h, name="mu", num_hidden=n_latent)
-        act_z = mx.sym.Activation(data=decoder_z, act_type="tanh",name="actication_z")
+        # latent manifold
-        act_x = mx.sym.Activation(data=decoder_x, act_type="sigmoid",name='activation_x')
+        decoder_x = mx.sym.FullyConnected(data=act_z, name="decoder_x", num_hidden=features)
-        KL = -0.5*mx.symbol.sum(1+logvar-pow( mu,2)-mx.symbol.exp(logvar),axis=1)
+        KL = -0.5 * mx.symbol.sum(1+logvar-pow(mu,2)-mx.symbol.exp(logvar), axis=1)
-        output = mx.symbol.MakeLoss(sum(loss),name='loss')
+        # compute minus ELBO to minimize
-        #train the model
+        # train the model
-            symbol = output ,
+            symbol=output ,
-
+            label_names=['loss_label'])
-                    eval_metric = 'Loss')
+                  initializer=initializer, # initialize the weights and bias
-    def encoder(model,x):
+    @staticmethod
-        encoder_h = np.dot(params['encoder_h_weight'].asnumpy(),np.transpose(x)) + np.reshape(params['encoder_h_bias'].asnumpy(),(encoder_n,1))
+        encoder_h = np.dot(params['encoder_h_weight'].asnumpy(), np.transpose(x)) \
-        z = mu + np.multiply(np.exp(0.5*logvar),np.random.normal(loc=0, scale=1,size=np.shape(logvar))) 
+    @staticmethod
-    def decoder(model,z):
+    @staticmethod
-        decoder_z = np.dot(params['decoder_z_weight'].asnumpy(),np.transpose(z)) + np.reshape(params['decoder_z_bias'].asnumpy(),(decoder_n,1))
+        decoder_z = np.dot(params['decoder_z_weight'].asnumpy(),np.transpose(z)) \
-import argparse, cPickle, math, os, random
+import argparse, math, os, random
-        train_iter, dev_iter, test_iter, vocab = cPickle.load(f)
+if os.path.exists('dataset.pickle'):
-        cPickle.dump([train_iter, dev_iter, test_iter, vocab], f)
+    with open('dataset.pickle', 'wb') as f:
-import cPickle
+try:
-from module import MutableModule
+from .module import MutableModule
-        cPickle.dump(imdb_boxes, f, cPickle.HIGHEST_PROTOCOL)
+        pickle.dump(imdb_boxes, f, pickle.HIGHEST_PROTOCOL)
-            cPickle.dump(original_boxes, f, cPickle.HIGHEST_PROTOCOL)
+            pickle.dump(original_boxes, f, pickle.HIGHEST_PROTOCOL)
-                 for _ in xrange(imdb.num_classes)]
+    all_boxes = [[[] for _ in range(num_images)]
-        cPickle.dump(all_boxes, f, protocol=cPickle.HIGHEST_PROTOCOL)
+        pickle.dump(all_boxes, f, protocol=pickle.HIGHEST_PROTOCOL)
-    for k, v in cudaconfig.iteritems():
+    for k, v in cudaconfig.items():
-from coco import coco
+from .imdb import IMDB
-import cPickle
+try:
-from imdb import IMDB
+from .imdb import IMDB
-        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))
+        self._class_to_ind = dict(zip(self.classes, range(self.num_classes)))
-                roidb = cPickle.load(fid)
+                roidb = pickle.load(fid)
-            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)
+            pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)
-                       'score': scores[k]} for k in xrange(dets.shape[0])]
+                       'score': scores[k]} for k in range(dets.shape[0])]
-            cPickle.dump(coco_eval, f, cPickle.HIGHEST_PROTOCOL)
+            pickle.dump(coco_eval, f, pickle.HIGHEST_PROTOCOL)
-    hashes = np.round(boxes * scale).dot(v)
+    hashes = np.round(boxes * scale).dot(v).astype(np.int)
-import cPickle
+try:
-            box_list = cPickle.load(f)
+            box_list = pickle.load(f)
-import cPickle
+try:
-from ds_utils import unique_boxes, filter_small_boxes
+from .imdb import IMDB
-                roidb = cPickle.load(fid)
+                roidb = pickle.load(fid)
-            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)
+            pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)
-                roidb = cPickle.load(fid)
+                roidb = pickle.load(fid)
-            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)
+            pickle.dump(roidb, fid, pickle.HIGHEST_PROTOCOL)
-
+try:
-            cPickle.dump(recs, f, protocol=cPickle.HIGHEST_PROTOCOL)
+            pickle.dump(recs, f, protocol=pickle.HIGHEST_PROTOCOL)
-            recs = cPickle.load(f)
+            recs = pickle.load(f)
-    fg_rois_per_image = np.round(config.TRAIN.FG_FRACTION * rois_per_image).astype(int)
+    fg_rois_per_image = np.round(config.TRAIN.FG_FRACTION * rois_per_image).astype(np.int)
-    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_indexes.size)
+    fg_rois_per_this_image = int(np.minimum(fg_rois_per_image, fg_indexes.size))
-    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image, bg_indexes.size)
+    bg_rois_per_this_image = int(np.minimum(bg_rois_per_this_image, bg_indexes.size))
-from bbox_transform import bbox_overlaps, bbox_transform
+from .bbox_transform import bbox_overlaps, bbox_transform
-
+from builtins import range
-                         for i in xrange(ratio_anchors.shape[0])])
+                         for i in range(ratio_anchors.shape[0])])
-import mask as maskUtils
+from .mask import *
-        ious = maskUtils.iou(d,g,iscrowd)
+        ious = iou(d,g,iscrowd)
-import _mask
+from rcnn.pycocotools import _mask
-from symbol_resnet import *
+from .symbol_vgg import *
-        fg_rois_per_image = np.round(self._fg_fraction * rois_per_image).astype(int)
+        fg_rois_per_image = np.round(self._fg_fraction * rois_per_image).astype(np.int)
-import proposal_target
+from . import proposal
-
+from . import proposal
-import cPickle
+try:
-        detections = cPickle.load(f)
+        detections = pickle.load(f)
-        assert(positive_cls_weight > 0)
+        assert(float(positive_cls_weight) > 0)
-import sys
+import os
-        os.system("cd ml-10M100K; sh split_ratings.sh; cd -;")
+def get_movielens_data(data_dir, prefix):
-import time
+import os
-    'val': './ml-10M100K/r1.test',
+    'train': './data/ml-10M100K/r1.train',
-    get_movielens_data(MOVIELENS['dataset'])
+    data_dir = os.path.join(os.getcwd(), 'data')
-        download(url, dirname=data_dir, overwrite=False)
+        download(url, fname=data_origin_name, dirname=data_dir, overwrite=False)
-                bz_file.close()
+            for line in bz_file:
-    has_bias = True
+    param = node['attrs']
-from util import download_file
+from common.util import download_file
-# pylint: skip-file
+"""
-    return pallete
+def make_file_extension_assertion(extension):
-ctx = mx.gpu(0)
+            Returns:
-    """get the (1, 3, h, w) np.array data for the img_path"""
+    """get the (1, 3, h, w) np.array data for the supplied image
-    fcnxs_args["data"] = mx.nd.array(get_data(img), ctx)
+    """Module main execution"""
-    exector = fcnxs.bind(ctx, fcnxs_args ,args_grad=None, grad_req="null", aux_states=fcnxs_args)
+    exector = fcnxs.bind(ctx, fcnxs_args, args_grad=None, grad_req="null", aux_states=fcnxs_args)
-    out_img.save(seg)
+    out_img.putpalette(get_palette())
-        self.conn = conn
+        self._conn = conn
-        return getattr(self.conn, name)
+        attr = self.__dict__.get('_conn', None)
-    learning_rate = 0.1, momentum = 0.9, wd = 0.00001)
+context=mx.cpu()
-          batch_end_callback=mx.callback.Speedometer(100,100))
+mod = mx.mod.Module(mlp, context=context)
-    learning_rate = 0.1, momentum = 0.9, wd = 0.00001)
+# MXNET_CPU_WORKER_NTHREADS must be greater than 1 for custom op to work on CPU
-model.fit(X=train, eval_data=val)
+mod = mx.mod.Module(mlp, context=context)
-        in_grad[0][:] = ((out_data[0] - 1) * in_data[1] * self.pos_grad_scale + out_data[0] * (1 - in_data[1]) * self.neg_grad_scale) / out_data[0].shape[1]
+        in_grad[0][:] = ((out_data[0] - 1) * in_data[1] * self.pos_grad_scale
-    exe2 = lr.simple_bind(ctx = mx.gpu(1), data = (2 * m, n))
+
-    print('wlr output:')
+
-    print('lr output:')
+
-    print('wlr grad:')
+
-    print('lr grad:')
+
-    act = mx.sym.Activation(data=bn, act_type='relu', name='%s%s_relu' %(name, suffix))
+alpha_values = [0.25, 0.50, 0.75, 1.0]
-    pool = mx.sym.Pooling(data=conv_14, kernel=(7, 7), stride=(1, 1), pool_type="avg", name="global_pool")
+
-        **model_args)
+
-       epoch_end_callback = checkpoint)
+    mod.fit(train_data=train, eval_metric=eval_metrics, eval_data=val, optimizer='sgd',
-                "while the %d-th has %d."%(length, i+1, len(data))
+                "All arrays must have the same length; array[0] has length %d " \
-                                           atol=1e-2, rtol=0.1)
+    dim0 = 30
-    model.fit(X=train, eval_data=val)
+# Licensed to the Apache Software Foundation (ASF) under one
-import os, sys
+import os
-def run_imageiter(path_rec, n, batch_size = 32):
+def run_imageiter(path_rec, n, batch_size=32):
-import numpy as np
+
-    executor = C.simple_bind(mx.gpu(1), 'write', A=(4096, 4096), B=(4096, 4096))
+    executor = C.simple_bind(mx.gpu(0), 'write', A=(4096, 4096), B=(4096, 4096))
-    arr._copy_slice_to(1, 1, 3, sub_arr)
+    sub_arr = arr.slice(begin=(None, 1), end=(None, 3))
-content_np = data_processing.PreprocessContentImage("../IMG_4343.jpg", min(dshape[2:]), dshape)
+content_np = data_processing.PreprocessContentImage("../input/IMG_4343.jpg", min(dshape[2:]), dshape)
-vgg_params = mx.nd.load("./vgg19.params")
+vgg_params = mx.nd.load("../model/vgg19.params")
-style_np = data_processing.PreprocessStyleImage("../starry_night.jpg", shape=dshape)
+style_np = data_processing.PreprocessStyleImage("../input/starry_night.jpg", shape=dshape)
-        content_np = data_processing.PreprocessContentImage(path, min(dshape[2:]), dshape)
+        try:
-
+# Licensed to the Apache Software Foundation (ASF) under one
-from builtins import range
+@unittest.skip("test fails intermittently. temporarily disabled till it gets fixed. tracked at https://github.com/apache/incubator-mxnet/issues/8288")
-    networks = ['alexnet', 'vgg', 'inception-bn', 'inception-v3', 'resnet-50', 'resnet-152']
+    networks = ['alexnet', 'vgg-16', 'inception-bn', 'inception-v3', 'resnet-50', 'resnet-152']
-        class sigmoid(Function):
+        class sigmoid(mx.autograd.Function):
-    cgroup.add_argument('--exts', nargs='+', default=['.jpeg', '.jpg'],
+    cgroup.add_argument('--exts', nargs='+', default=['.jpeg', '.jpg', '.png'],
-import os
+import os
-from mxnet.test_utils import *
+from mxnet.test_utils import almost_equal
-import unittest
+# Licensed to the Apache Software Foundation (ASF) under one
-        output_names = self.list_outputs()
+        output_count = len(self)
-            stop = len(output_names) if index.stop is None else index.stop
+            stop = output_count if index.stop is None else index.stop
-        if index >= len(output_names):
+        if index >= output_count:
-# pylint: disable=invalid-name, no-member
+# pylint: disable=invalid-name, no-member, trailing-comma-tuple
-import logging
+import logging
-            logging.warn('Skip DetRandomCropAug due to invalid area_range: %s', area_range)
+            warnings.warn('Skip DetRandomCropAug due to invalid area_range: %s', area_range)
-                         aspect_ratio_range)
+            warnings.warn('Skip DetRandomCropAug due to invalid aspect_ratio_range: %s',
-            logging.warn('Skip DetRandomPadAug due to invalid parameters: %s', area_range)
+            warnings.warn('Skip DetRandomPadAug due to invalid parameters: %s', area_range)
-                         aspect_ratio_range)
+            warnings.warn('Skip DetRandomPadAug due to invalid aspect_ratio_range: %s',
-            logging.warn('Unable to import cv2, skip drawing: %s', str(e))
+            warnings.warn('Unable to import cv2, skip drawing: %s', str(e))
-                            Optimizer.opt_registry[name].__name__)
+            warnings.warn('WARNING: New optimizer %s.%s is overriding existing '
-import mxnet as mx
+import os
-                         layer.type)
+            warnings.warn('No handling for layer %s of type %s, should we ignore it?', layer.name,
-        sep = X.shape[0]*9/10
+        sep = X.shape[0]*9//10
-
+# monitor
-          batch_end_callback = mx.callback.Speedometer(100, 100))
+# train with monitor
-        'caffemodel' : 'http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel',
+        'prototxt' : 'https://raw.githubusercontent.com/BVLC/caffe/master/models/bvlc_alexnet/deploy.prototxt',
-        return s.format(**self.__dict__)
+        return s.format(name=self.name, shape=self.shape, dtype=self.dtype)
-from get_data import get_libsvm_data
+from data import get_avazu_data
-    get_libsvm_data(data_dir, AVAZU['test'], AVAZU['url'])
+    get_avazu_data(data_dir, AVAZU['train'], AVAZU['url'])
-        for batch in data_iter:
+        for batch in train_data:
-        data_iter.reset()
+        train_data.reset()
-    os.chdir("..")
+from mxnet.test_utils import DummyIter
-
+from data import get_movielens_iter, get_movielens_data
-        from_layers = ['_plus12', '_plus15', '', '', '', '']
+        from_layers = ['_plus29', '_plus32', '', '', '', '']
-    ctx                = device,
+model = mx.mod.Module(
-    initializer        = mx.init.Xavier(factor_type="in", magnitude=2.34))
+    label_names        = ('softmax1_label', 'softmax2_label'))
-    X                  = train,
+    train_data         = train,
-        for i in xrange(len(env_vs)):
+        for i in range(len(env_vs)):
-    parser.add_argument('--env-type', default='PongDeterministic-v3')
+    parser.add_argument('--env-type', default='PongDeterministic-v4')
-    parser.add_argument('--lr-factor', dest='lr_refactor_ratio', type=str, default=0.1,
+    parser.add_argument('--lr-factor', dest='lr_refactor_ratio', type=float, default=0.1,
-
+@unittest.skip("flaky test. https://github.com/apache/incubator-mxnet/issues/8892")
-    'cusolverDn.h'
+    'cusolverDn.h', 'internal/concurrentqueue_internal_debug.h', 'relacy/relacy_std.hpp',
-
+    iter_num = 5
-    flag = False
+        print("Iteration {}/{}".format(i + 1, iter_num))
-                         'wd': 0})
+                         'wd': 0},
-                                                        ckeys, cvals))
+        if ('device' in self.type) or ('dist' in self.type):
-__version__ = "0.12.1"
+__version__ = "1.0.0"
-                                 "is not implemented yet.")
+                _internal._set_value(float(value), out=self)
-        assert same(x.asnumpy(), dst_nd.asnumpy())
+        assert np.all(x.asnumpy() == dst_nd.asnumpy() if isinstance(dst_nd, NDArray) else dst)
-
+    # scalar assigned to row_sparse NDArray
-    group2ctxs={'dev1':mx.cpu(), 'dev2':[mx.gpu(i) for i in range(num_gpus)]}
+    group2ctxs={'dev1':[mx.cpu()]*num_gpus, 'dev2':[mx.gpu(i) for i in range(num_gpus)]}
-        ret = [{}] * ctx_len
+        ret = [{} for i in range(ctx_len)]
-from test_sparse_ndarray import test_sparse_nd_check_format
+from test_sparse_ndarray import test_sparse_nd_check_format, test_sparse_nd_copy
-    def check_module_ctx_group(ctxs, group2ctxs):
+    def check_module_ctx_group(ctxs, group2ctxs, grad_ctxs=None):
-        assert np.all(mod1_input_grads[1].asnumpy() == mod2_input_grads[1].asnumpy())
+        if grad_ctxs is not None:
-    check_module_ctx_group([mx.cpu(0)], {'dev1': mx.cpu(1), 'dev2': mx.cpu(2)})
+    check_module_ctx_group([mx.cpu(0)], {'dev1': mx.cpu(1), 'dev2': mx.cpu(2)}, grad_ctxs=[mx.cpu(1), mx.cpu(2)])
-    def __init__(self, sym):
+    def __init__(self, sym, flags=()):
-        check_call(_LIB.MXCreateCachedOp(
+        check_call(_LIB.MXCreateCachedOpEx(
-    def hybridize(self, active=True):
+    def hybridize(self, active=True, **kwargs):
-            cld.hybridize(active)
+            cld.hybridize(active, **kwargs)
-        self._cached_op = ndarray.CachedOp(out)
+        self._cached_op = ndarray.CachedOp(out, self._flags)
-    def hybridize(self, active=True):
+    def hybridize(self, active=True, **kwargs):
-        super(HybridBlock, self).hybridize(active)
+        self._flags = kwargs.items()
-    def hybridize(self, active=True):
+    def hybridize(self, active=True, **kwargs):
-        super(Sequential, self).hybridize(active)
+        super(Sequential, self).hybridize(active, **kwargs)
-
+def test_inline():
-    x2 = mx.nd.ones((4,4), ctx=mx.gpu(1))
+    x2 = mx.nd.ones((4,4), ctx=mx.cpu(0))
-                optim_str = pickle.dumps(optimizer, 0)
+                optim_str = py_str(pickle.dumps(optimizer, 0))
-    total_norm = ndarray.add_n(*[ndarray.dot(x, x)
+    ctx = arrays[0].context
-            to_add = sparse.elemwise_add(div, _internal._mul_scalar(retained_weight, wd))
+            to_add = sparse.elemwise_add(div, _internal._mul_scalar(retained_weight, float(wd)))
-            weight[:] = sparse.elemwise_add(weight, _internal._mul_scalar(to_add, -lr))
+            weight[:] = sparse.elemwise_add(weight, _internal._mul_scalar(to_add, float(-lr)))
-    return _internal._full(shape=shape, dtype=dtype, value=val, **kwargs)
+    return _internal._full(shape=shape, dtype=dtype, value=float(val), **kwargs)
-    cmake_build_path = os.path.join(curr_path, '../../build/Release/')
+    cmake_build_path = os.path.join(curr_path, '../../build/')
-                                    dtype=self.dtype, value=value, out=self)
+                                    dtype=self.dtype, value=float(value), out=self)
-    group2ctxs : list of dict of str to context
+    group2ctxs : dict of str to context or list of context,
-    group2ctxs : list of dict of str to context
+    group2ctxs : dict of str to context or list of context,
-        self.group2ctxs = group2ctxs
+        self.group2ctxs = _prepare_group2ctxs(group2ctxs, len(contexts))
-    group2ctxs : list of dict of str to context
+    group2ctxs : dict of str to context or list of context,
-
+    def check_module_ctx_group(ctxs, group2ctxs):
-                                 group2ctxs=[{'dev1':mx.cpu(1), 'dev2':mx.cpu(2)}])
+                                 group2ctxs=[{'dev1': mx.cpu(1), 'dev2': mx.cpu(2)}])
-        self.infer_type(*args)
+    def cast(self, dtype):
-                                          dtype=None, init=weight_initializer,
+                                          init=weight_initializer,
-                                            dtype=None, init=bias_initializer,
+                                            init=bias_initializer,
-                                     init=gamma_initializer, allow_deferred_init=True,
+                                     shape=(in_channels,), init=gamma_initializer,
-                                    init=beta_initializer, allow_deferred_init=True,
+                                    shape=(in_channels,), init=beta_initializer,
-                                            shape=(in_channels,), dtype=None,
+                                            shape=(in_channels,),
-                                           shape=(in_channels,), dtype=None,
+                                           shape=(in_channels,),
-                                      dtype=None, init=weight_initializer,
+                                      init=weight_initializer,
-                                          dtype=None, init=weight_initializer,
+                                          init=weight_initializer,
-                                            dtype=None, init=bias_initializer,
+                                            init=bias_initializer,
-        self.shape = shape
+    @property
-                return arr_list[idx]
+            if ctx.device_typeid < len(self._ctx_map):
-        init, ctx, default_init = self._deferred_init
+        init, ctx, default_init, data = self._deferred_init
-                initializer.InitDesc(self.name, {'__init__': init}), data)
+            if data is None:
-        if self.dtype is None or not self.shape or np.prod(self.shape) <= 0:
+        if not self.shape or np.prod(self.shape) <= 0:
-                self._deferred_init = (init, ctx, default_init)
+                self._deferred_init = (init, ctx, default_init, None)
-        self._deferred_init = (init, ctx, default_init)
+        self._deferred_init = (init, ctx, default_init, None)
-            self._deferred_init = (init, ctx, default_init)
+            init, _, default_init, data = self._deferred_init
-            "Parameter %s has not been initialized"%self.name
+        """Sets this parameter's value on all contexts."""
-            self.shape = data.shape
+    def cast(self, dtype):
-                                          dtype=None, init=i2h_weight_initializer,
+                                          init=i2h_weight_initializer,
-                                          dtype=None, init=h2h_weight_initializer,
+                                          init=h2h_weight_initializer,
-                                        dtype=None, init=i2h_bias_initializer,
+                                        init=i2h_bias_initializer,
-                                        dtype=None, init=h2h_bias_initializer,
+                                        init=h2h_bias_initializer,
-                                          dtype=None, init=i2h_weight_initializer,
+                                          init=i2h_weight_initializer,
-                                          dtype=None, init=h2h_weight_initializer,
+                                          init=h2h_weight_initializer,
-                                        dtype=None, init=i2h_bias_initializer,
+                                        init=i2h_bias_initializer,
-                                        dtype=None, init=h2h_bias_initializer,
+                                        init=h2h_bias_initializer,
-                                          dtype=None, init=i2h_weight_initializer,
+                                          init=i2h_weight_initializer,
-                                          dtype=None, init=h2h_weight_initializer,
+                                          init=h2h_weight_initializer,
-                                        dtype=None, init=i2h_bias_initializer,
+                                        init=i2h_bias_initializer,
-                                        dtype=None, init=h2h_bias_initializer,
+                                        init=h2h_bias_initializer,
-    net(mx.nd.ones((16, 3, 32, 32), dtype='float64')).wait_to_read()
+    net.cast('float64')
-    net(mx.nd.ones((16, 3, 32, 32), dtype='float64')).wait_to_read()
+    net(mx.nd.ones((16, 3, 32, 32), dtype='float32'))
-    name : {'local', 'device', 'dist_sync', 'dist_device_sync', 'dist_async'}
+    name : {'local', 'device', 'nccl', 'dist_sync', 'dist_device_sync', 'dist_async'}
-                                              kvstore, executor_manager.param_names)
+                    if 'nccl' in kvstore.type:
-            'multi_precision': True}
+        'learning_rate': lr,
-        monitor            = monitor)
+              begin_epoch        = args.load_epoch if args.load_epoch else 0,
-
+    compression_params : dict
-    def __init__(self, params, optimizer, optimizer_params=None, kvstore='device'):
+    def __init__(self, params, optimizer, optimizer_params=None, kvstore='device',
-
+        self._compression_params = compression_params
-                 fixed_param_names=None, state_names=None, group2ctxs=None):
+                 fixed_param_names=None, state_names=None, group2ctxs=None,
-                        state_names=self._state_names, group2ctxs=self._group2ctxs)
+                        state_names=self._state_names,
-                            state_names=self._state_names, group2ctxs=self._group2ctxs)
+                            state_names=self._state_names,
-                 fixed_param_names=None, state_names=None, group2ctxs=None):
+                 fixed_param_names=None, state_names=None, group2ctxs=None,
-import time
+from mxnet.test_utils import assert_almost_equal
-    print('worker ' + str(my_rank) + ' is done')
+    print('worker ' + str(my_rank) + ' is done with non compression tests')
-shapes = [(4, 4), (100, 100), (2000, 2000)];
+from mxnet.test_utils import assert_almost_equal
-nrepeat = 10
+def check_diff_to_scalar(A, x, rank=None):
-data = [[[np.random.random(s)*2-1 for i in range(nworker)] for s in shapes] for j in range(nrepeat)]
+def compute_expected_2bit_quantization(arr, curr_residual, threshold):
-test_kvstore('local_allreduce_device')
+def test_compress_kvstore(kv_type, compression='2bit', threshold=0.5):
-test_group_kvstore('local_allreduce_device')
+if __name__ == "__main__":
-        num_batches, test_results, **kwargs):
+        num_batches, test_results, gc_type, **kwargs):
-# pylint: disable=invalid-name, protected-access, too-many-arguments, no-self-use, too-many-locals, broad-except, too-many-lines
+# pylint: disable=invalid-name, protected-access, too-many-arguments, no-self-use, too-many-locals, broad-except
-                        "types, got %d."%(n_out, len(otype))
+                        "shapes, got %d."%(n_out, len(otype))
-                        "types, got %d."%(n_in, len(itype))
+                        "shapes, got %d."%(n_in, len(itype))
-                        "types, got %d."%(n_aux, len(atype))
+                        "shapes, got %d."%(n_aux, len(atype))
-                                                                         writable=True))
+                                    tensors[tags[i]].append(NDArray(cast(ndarraies[i],
-                                                                         writable=False))
+                                    tensors[tags[i]].append(NDArray(cast(ndarraies[i],
-                                                                         writable=True))
+                                    tensors[tags[i]].append(NDArray(cast(ndarraies[i],
-                                                                         writable=False))
+                                    tensors[tags[i]].append(NDArray(cast(ndarraies[i],
-                         inferstorage_backward_functype(infer_storage_type_backward_entry)]
+                         infertype_functype(infer_type_entry)]
-                    assert(isinstance(in_data[0], mx.nd.sparse.CSRNDArray))
+            self.assign(out_data[0], req[0], in_data[0]*in_data[0])
-                assert (aux[0].asnumpy() == 1).all()
+            assert (aux[0].asnumpy() == 1).all()
-
+    x = mx.nd.array(np.random.uniform(-1, 1, size=(4, 10)))
-    assert (aux.stype == 'csr')
+
-        Set self[key] to value.
+        Sets value to self[key]. This functions supports advanced indexing defined in the following reference with
-        key : int, slice or tuple
+        key : int, slice, list, np.ndarray, NDArray, or tuple of all previous types
-        value : scalar, NDArray or numpy.ndarray
+        value : scalar or array-like object that can be broadcast to the shape of self[key]
-        some limitations.
+        some restrictions.
-        3. Boolean array indexing.
+
-        key : int or slice, or array like
+        key : int, slice, list, np.ndarray, NDArray, or tuple of all previous types
-            app.builder.srcdir)
+    if not os.path.exists(os.path.join(app.builder.srcdir, '..', 'config.mk')):
-``pretrained=True``:
+These can constructed by passing ``pretrained=True``:
-                            data=(1,200))
+    # generate reqs with null
-            assert arr.context == group2ctx['stage2']
+    grad_reqs = ['write', grad_req_with_null]
-        'multi_precision': True}
+            'learning_rate': lr,
-              monitor            = monitor)
+        begin_epoch        = args.load_epoch if args.load_epoch else 0,
-                 compression_params=None):
+    def __init__(self, params, optimizer, optimizer_params=None, kvstore='device'):
-        self._compression_params = compression_params
+
-                 compression_params=None):
+                 fixed_param_names=None, state_names=None, group2ctxs=None):
-                        compression_params=self._compression_params)
+                        state_names=self._state_names, group2ctxs=self._group2ctxs)
-                            compression_params=self._compression_params)
+                            state_names=self._state_names, group2ctxs=self._group2ctxs)
-                 compression_params=None):
+                 fixed_param_names=None, state_names=None, group2ctxs=None):
-from test_kvstore import compute_expected_2bit_quantization
+import time
-    print('worker ' + str(my_rank) + ' is done with compression tests')
+    print('worker ' + str(my_rank) + ' is done')
-from mxnet.test_utils import assert_almost_equal
+keys = [3, 5, 7]
-    assert(np.sum(np.abs((A - x).asnumpy())) == 0), (rank, A.asnumpy(), x)
+lr = .1
-    return np.array(compr), np.array(new_residual).reshape(arr.shape), np.array(decompr).reshape(arr.shape)
+## generate data
-    check_compr_random(kv, threshold)
+test_kvstore('local_update_cpu')
-    test_group_kvstore('local_allreduce_device')
+test_group_kvstore('local_update_cpu')
-        num_batches, test_results, gc_type, **kwargs):
+        num_batches, test_results, **kwargs):
-            if not self._data[df_name].has_key(key):
+            if key not in self._data[df_name]:
-        if spell_check_ret.has_key(error.word):
+        if error.word in spell_check_ret:
-        if dp[nxt].has_key(nxt_c):
+        if nxt_c in dp[nxt]:
-    if node.has_key('inputs'):
+    if 'inputs' in node:
-    if node.has_key('inputs'):
+    if 'inputs' in node:
-    param = node['attr']
+    param = _get_attrs(node)
-    mx_non_linearity = node['attr']['act_type']
+    mx_non_linearity = _get_attrs(node)['act_type']
-    param = node['attr']
+    param = _get_attrs(node)
-    param = node['attr']
+    param = _get_attrs(node)
-            eps = literal_eval(node['attr']['eps'])
+    attrs = _get_attrs(node)
-    param = node['attr']
+    param = _get_attrs(node)
-                internel_layer = mx.symbol.BatchNorm(data=internel_layer, name="bn%s_%s" %(i + 1, j + 1)) 
+                internel_layer = mx.symbol.BatchNorm(data=internel_layer, name="bn%s_%s" %(i + 1, j + 1))
-    return internel_layer    
+    return internel_layer
-    return fc8  
+    return fc8
-        Use batch normalization. 
+        Use batch normalization.
-        Data precision.   
+        Data precision.
-    if not vgg_spec.has_key(num_layers):        
+    if not vgg_spec.has_key(num_layers):
-    layers, filters = vgg_spec[num_layers] 
+    layers, filters = vgg_spec[num_layers]
-        classifier = mx.sym.Cast(data=classifier, dtype=np.float32)  
+        classifier = mx.sym.Cast(data=classifier, dtype=np.float32)
-    # CSRNDArray with last_batch_handle not equal to 'discard' will throw NotImplementedError 
+    # CSRNDArray with last_batch_handle not equal to 'discard' will throw NotImplementedError
-    
+
-    
+
-    
+
-        
+
-    
+
-                break
+    train.add_argument('--gc-type', type=str, default='none',
-            'multi_precision': True}
+        'learning_rate': lr,
-        monitor            = monitor)
+              begin_epoch        = args.load_epoch if args.load_epoch else 0,
-
+    compression_params : dict
-    def __init__(self, params, optimizer, optimizer_params=None, kvstore='device'):
+    def __init__(self, params, optimizer, optimizer_params=None, kvstore='device',
-
+        self._compression_params = compression_params
-                 fixed_param_names=None, state_names=None, group2ctxs=None):
+                 fixed_param_names=None, state_names=None, group2ctxs=None,
-                        state_names=self._state_names, group2ctxs=self._group2ctxs)
+                        state_names=self._state_names,
-                            state_names=self._state_names, group2ctxs=self._group2ctxs)
+                            state_names=self._state_names,
-                 fixed_param_names=None, state_names=None, group2ctxs=None):
+                 fixed_param_names=None, state_names=None, group2ctxs=None,
-import time
+from mxnet.test_utils import assert_almost_equal
-    print('worker ' + str(my_rank) + ' is done')
+    print('worker ' + str(my_rank) + ' is done with non compression tests')
-shapes = [(4, 4), (100, 100), (2000, 2000)];
+from mxnet.test_utils import assert_almost_equal
-nrepeat = 10
+def check_diff_to_scalar(A, x, rank=None):
-data = [[[np.random.random(s)*2-1 for i in range(nworker)] for s in shapes] for j in range(nrepeat)]
+def compute_expected_2bit_quantization(arr, curr_residual, threshold):
-test_kvstore('local_allreduce_device')
+def test_compress_kvstore(kv_type, compression='2bit', threshold=0.5):
-test_group_kvstore('local_allreduce_device')
+if __name__ == "__main__":
-        num_batches, test_results, **kwargs):
+        num_batches, test_results, gc_type, **kwargs):
-    >>> input = mx.nd.random_uniform(shape=(5, 3, 10))
+    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
-    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
+    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
-    >>> input = mx.nd.random_uniform(shape=(5, 3, 10))
+    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
-    >>> c0 = mx.nd.random_uniform(shape=(3, 3, 100))
+    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
-    >>> input = mx.nd.random_uniform(shape=(5, 3, 10))
+    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
-    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
+    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
-        mod.save_checkpoint("checkpoint", epoch, save_optimizer_states=False)
+        save_optimizer_states = 'dist' not in kv.type if kv else True
-    sparse updates are applied by::
+    **sparse updates** are applied by::
-    sparse updates are applied by::
+    **sparse updates** are applied by::
-    sparse updates are applied by::
+    **sparse updates** are applied by::
-from mxnet.ndarray.sparse import RowSparseNDArray, CSRNDArray
+from mxnet.ndarray.sparse import RowSparseNDArray, CSRNDArray
-    
+    assertRaises(mx.base.MXNetError, mx.nd.sparse.retain, a, invalid_arg="garbage_value")
-        https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#combining-advanced-and-basic-indexing  # pylint: disable=line-too-long
+        https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#combining-advanced-and-basic-indexing
-        check_numeric_gradient(c, gen_broadcast_data(idx=200), rtol=1e-2, atol=1e-3)
+        data = gen_broadcast_data(idx=200)
-        check_numeric_gradient(c, gen_broadcast_data(idx=200), rtol=1e-2, atol=1e-3)
+        data = gen_broadcast_data(idx=200)
-        test_dot_csr(lhs_shape, (lhs_shape[0], rnd.randint(5, 10)), 'default', True , lhs_d, rhs_d) # (scalar kernel)
+        test_dot_csr(lhs_shape, (lhs_shape[1], 1), 'default', False, lhs_d, rhs_d)  # test gpu SpMV
-        densities = [0, 0.01, 0.1, 0.2, 0.5]
+        densities = [0, 0.01, 0.2, 0.5, 1.0]
-                    # check symbolic backward since ograd can be a rsp
+                    # check symbolic backward since ograd can be an rsp
-                    if axis == 1 and keepdims:
+                    if axis == 1 and keepdim:
-                        check_symbolic_backward(test, [rsp], [ret], [igrad_expected.asnumpy()],
+                        # check backward when ograd is row sparse
-                    
+
-from ..base import c_array, c_str
+from ..base import c_str_array, c_handle_array
-        output_vars = c_array(NDArrayHandle, [i.handle for i in out])
+        output_vars = c_handle_array(out)
-        c_array(NDArrayHandle, [arr.handle for arr in ndargs]),
+        c_handle_array(ndargs),
-        c_array(ctypes.c_char_p, [c_str(str(val)) for val in vals]),
+        c_str_array(keys),
-            output_vars = c_array(NDArrayHandle, [i.handle for i in out])
+            output_vars = c_handle_array(out)
-            c_array(NDArrayHandle, [arr.handle for arr in args]),
+            c_handle_array(args),
-from ..base import c_array, c_str, mx_uint
+from ..base import c_str_array, c_handle_array, c_str, mx_uint
-            args = c_array(SymbolHandle, [s.handle for s in kwargs.values()])
+            keys = c_str_array(kwargs.keys())
-            args = c_array(SymbolHandle, [s.handle for s in args])
+            args = c_handle_array(kwargs.values())
-                       [c_str(str(val)) for val in kwargs.values()])
+        keys = c_str_array(kwargs.keys())
-        c_array(ctypes.c_char_p, [c_str(str(i)) for i in vals]),
+        c_str_array(keys),
-from .base import mx_uint, NDArrayHandle, c_array, MXCallbackList, SymbolHandle
+from .base import _LIB, check_call, string_types, mx_uint
-        c_array(NDArrayHandle, gradient_handles)))
+        len(variables),
-    head_handles = c_array(NDArrayHandle, [i.handle for i in heads])
+    head_handles = c_handle_array(heads)
-        var_handles = [variables.handle]
+        variables = [variables]
-    var_handles = c_array(NDArrayHandle, var_handles)
+    var_handles = c_handle_array(variables)
-            c_array(NDArrayHandle, input_handles),
+            c_handle_array(inputs),
-            c_array(NDArrayHandle, output_handles),
+            c_handle_array(outputs),
-    return (ctype * len(values))(*values)
+    out = (ctype * len(values))()
-from ..base import mx_uint, NDArrayHandle, c_array
+from ..base import mx_uint, NDArrayHandle, c_array, c_array_buf, c_handle_array
-        c_array(NDArrayHandle, gradient_handles)))
+        len(variables),
-            c_array(NDArrayHandle, output_handles),
+            len(outputs),
-    assert len(ograd_handles) == len(output_handles), \
+    assert len(ograd_handles) == len(outputs), \
-        c_array(NDArrayHandle, output_handles),
+        len(outputs),
-from .base import check_call, c_array, py_str
+from .base import check_call, c_handle_array, py_str
-        ndarray = c_array(NDArrayHandle, [item.handle for item in out_grads])
+        ndarray = c_handle_array(out_grads)
-from .base import c_array, c_str, mx_uint, py_str
+from .base import c_str_array, mx_uint, py_str
-            param_vals.append(c_str(str(val)))
+            param_keys.append(k)
-        param_vals = c_array(ctypes.c_char_p, param_vals)
+        param_keys = c_str_array(param_keys)
-from .base import check_call, c_array, c_str, string_types, mx_uint, py_str
+from .base import _LIB, c_str_array, c_handle_array, c_array, c_array_buf, c_str
-        c_vals_arr = c_array(NDArrayHandle, c_vals)
+        c_vals_arr = c_array(ctypes.c_void_p, c_vals)
-        return (c_keys, c_array(NDArrayHandle, [vals.handle]), use_str_keys)
+        c_keys = c_str_array([keys]) if use_str_keys \
-        return (c_keys, c_array(NDArrayHandle, [value.handle for value in vals]), use_str_keys)
+        c_keys = c_str_array([keys] * len(vals)) if use_str_keys \
-from ..base import c_array, mx_real_t
+from ..base import c_array, c_array_buf, c_handle_array, mx_real_t
-        c_array(mx_uint, shape),
+        c_array_buf(mx_uint, native_array('I', shape)),
-                                         c_array(ctypes.c_int, shape),
+                                         c_array_buf(ctypes.c_int, native_array('i', shape)),
-            1, c_array(NDArrayHandle, [self.handle]),
+            1, c_handle_array([self]),
-from ..base import c_array, mx_real_t, integer_types
+from ..base import c_array_buf, mx_real_t, integer_types
-        c_array(mx_uint, shape),
+        c_array_buf(mx_uint, native_array('I', shape)),
-        c_array(mx_uint, aux_shapes),
+        c_array_buf(ctypes.c_int, native_array('i', aux_type_ids)),
-from ..base import _LIB, check_call, py_str, c_str, string_types, mx_uint, NDArrayHandle, c_array
+from ..base import _LIB, check_call, py_str, c_str, string_types, mx_uint, NDArrayHandle
-    handles = []
+        handles = c_array(NDArrayHandle, [])
-        keys = c_array(ctypes.c_char_p, keys)
+        str_keys = data.keys()
-            handles.append(val.handle)
+        if any(not isinstance(v, NDArray) for v in data):
-                                  c_array(NDArrayHandle, handles),
+                                  handles,
-from .base import c_array, c_str, mx_uint, mx_float, ctypes2numpy_shared, NDArrayHandle, py_str
+from .base import _LIB, check_call, MXCallbackList, c_array, c_array_buf
-                tensor_shapes[i] = cast(c_array(mx_uint, rshape[i]), POINTER(mx_uint))
+                tensor_shapes[i] = cast(c_array_buf(mx_uint,
-                    tensor_shapes[i] = cast(c_array(mx_uint, rshape[i]), POINTER(mx_uint))
+                    tensor_shapes[i] = cast(c_array_buf(mx_uint,
-                rdeps = cast(c_array(c_int, rdeps), c_int_p)
+                rdeps = cast(c_array_buf(c_int, array('i', rdeps)), c_int_p)
-                        tensor_shapes[i] = cast(c_array(mx_uint, rshape[i]), POINTER(mx_uint))
+                        tensor_shapes[i] = cast(c_array_buf(mx_uint,
-                    rdeps = cast(c_array(c_int, rdeps), c_int_p)
+                    rdeps = cast(c_array_buf(c_int, array('i', rdeps)), c_int_p)
-from .base import _LIB, mx_uint, c_array, check_call
+from .base import _LIB, mx_uint, c_array, c_array_buf, c_str_array, check_call
-            c_array(ctypes.c_char_p, [c_str(opt) for opt in options]),
+            c_str_array(options),
-            c_array(ctypes.c_char_p, [c_str(name) for name in exports]),
+            c_str_array(exports),
-            c_array(ctypes.c_int, [ctypes.c_int(i) for i in dtypes]),
+            c_array_buf(ctypes.c_int, array('i', is_ndarray)),
-from ..base import c_array, c_str, mx_uint, py_str, string_types
+from ..base import _LIB, numeric_types, c_array, c_array_buf, c_str, c_str_array, c_handle_array
-            args = c_array(SymbolHandle, [s.handle for s in kwargs.values()])
+            keys = c_str_array(kwargs.keys())
-            args = c_array(SymbolHandle, [s.handle for s in args])
+            args = c_handle_array(args)
-            keys = None
+            keys = c_array(ctypes.c_char_p, [])
-            keys = []
+            str_keys = []
-                    keys.append(c_str(k))
+                    str_keys.append(k)
-            c_array(ctypes.c_int, sdata),
+            keys,
-            keys = None
+            keys = c_array(ctypes.c_char_p, [])
-            keys = []
+            str_keys = []
-                keys.append(c_str(k))
+                str_keys.append(k)
-            c_array(mx_uint, sdata),
+            keys,
-                    provided_arg_type_data.append(ctypes.c_int(_DTYPE_NP_TO_MX[v]))
+                    provided_arg_type_names.append(k)
-            provided_arg_type_data = c_array(ctypes.c_int, provided_arg_type_data)
+            provided_arg_type_names = c_str_array(provided_arg_type_names)
-                    provided_arg_stype_data.append(ctypes.c_int(_STORAGE_TYPE_STR_TO_ID[v]))
+                    provided_arg_stype_names.append(k)
-            provided_arg_stype_data = c_array(ctypes.c_int, provided_arg_stype_data)
+            provided_arg_stype_names = c_str_array(provided_arg_stype_names)
-                provided_arg_shape_names.append(c_str(k))
+                provided_arg_shape_names.append(k)
-                provided_grad_req_types = [c_str(grad_req)]
+                provided_grad_req_types = [grad_req]
-                provided_grad_req_types = [c_str(item) for item in grad_req]
+                provided_grad_req_types = grad_req
-                provided_grad_req_names = c_array(ctypes.c_char_p, provided_grad_req_names)
+                    provided_grad_req_names.append(k)
-            provided_grad_req_types = c_array(ctypes.c_char_p, provided_grad_req_types)
+            provided_grad_req_types = c_str_array(provided_grad_req_types)
-                ctx_map_dev_ids.append(ctypes.c_int(val.device_id))
+                ctx_map_keys.append(key)
-            ctx_map_dev_ids = c_array(ctypes.c_int, ctx_map_dev_ids)
+            ctx_map_keys = c_str_array(ctx_map_keys)
-            shared_arg_name_list = [c_str(name) for name in shared_arg_names]
+            shared_arg_name_list = shared_arg_names
-            for k, v in shared_buffer.items():
+            buffer_names = shared_buffer.keys()
-            shared_buffer_handles = c_array(NDArrayHandle, shared_buffer_handles)
+            shared_buffer_names = c_str_array(buffer_names)
-                                                 c_array(mx_uint, provided_arg_shape_idx),
+                                                 c_str_array(provided_arg_shape_names),
-                                                 c_array(ctypes.c_char_p, shared_arg_name_list),
+                                                 c_str_array(shared_arg_name_list),
-                [mx_uint(_GRAD_REQ_MAP[grad_req])] * len(listed_arguments))
+            reqs_array = c_array_buf(mx_uint,
-            reqs_array = c_array(mx_uint, [mx_uint(_GRAD_REQ_MAP[item]) for item in grad_req])
+            reqs_array = c_array_buf(mx_uint,
-                    req_array.append(mx_uint(_GRAD_REQ_MAP[grad_req[name]]))
+                    req_array.append(_GRAD_REQ_MAP[grad_req[name]])
-            reqs_array = c_array(mx_uint, req_array)
+                    req_array.append(0)
-                ctx_map_dev_ids.append(ctypes.c_int(val.device_id))
+                ctx_map_keys.append(key)
-                                         c_array(ctypes.c_int, ctx_map_dev_ids),
+                                         c_str_array(ctx_map_keys),
-        c_wrt = c_array(ctypes.c_char_p, [c_str(key) for key in wrt])
+        c_wrt = c_str_array(wrt)
-        ihandles.append(sym.handle)
+    if any(not isinstance(sym, Symbol) for sym in symbols):
-        c_array(SymbolHandle, ihandles), ctypes.byref(handle)))
+        mx_uint(len(symbols)),
-from .base import mx_uint, mx_float, NDArrayHandle, FunctionHandle
+from .base import c_array, c_str_array, c_handle_array, py_str, build_param_doc as _build_param_doc
-                   c_array(NDArrayHandle, [x.handle for x in ndargs[n_mutate_vars:]]), \
+                   c_handle_array(ndargs[n_mutate_vars:]), \
-                   c_array(NDArrayHandle, [x.handle for x in ndargs[:n_mutate_vars]]),
+                   c_handle_array(ndargs[:n_mutate_vars]),
-                   c_array(ctypes.c_char_p, kwargs.values()),))
+                   c_str_array(kwargs.keys()),
-    devstr2type = {'cpu': 1, 'gpu': 2, 'cpu_pinned': 3}
+    devtype2str = {1: 'cpu', 2: 'gpu', 3: 'cpu_pinned', 5: 'cpu_shared'}
-    >>> with mx.Context('cpu', 1):
+    >>> with mx.cpu():
-    ...    cpu_array = mx.nd.ones((2, 3))
+    cpu(0)
-    cpu(1)
+    cpu(0)
-    >>> with mx.Context('gpu', 1):
+    >>> cpu_array = mx.nd.ones((2, 3))
-    ...    gpu_array = mx.nd.ones((2, 3))
+    >>> gpu_array = mx.nd.ones((2, 3), ctx=mx.gpu(1))
-from ... import nd
+from ... import nd, context
-def _batchify(data):
+def rebuild_ndarray(*args):
-        return [_batchify(i) for i in data]
+        return [default_batchify_fn(i) for i in data]
-                 last_batch=None, batch_sampler=None):
+                 last_batch=None, batch_sampler=None, batchify_fn=None,
-            yield _batchify([self._dataset[idx] for idx in batch])
+        if self._num_workers == 0:
-    ctx = mx.cpu(0)
+    ctx = mx.gpu(0) if args.use_gpu else mx.cpu(0)
-    a = time.time()
+                    
-        assert_almost_equal(exe_test.outputs[0].asnumpy(), np.dot(data_onehot, weight.asnumpy()))
+        executor.forward(is_train=True)
-            check_sparse_embedding(exe_test, weight, np_onehot, grad, density)
+    densities = [0, 0.5, 1]
-# pylint: disable=invalid-name, protected-access, too-many-arguments, no-self-use, too-many-locals, broad-except
+# pylint: disable=invalid-name, protected-access, too-many-arguments, no-self-use, too-many-locals, broad-except, too-many-lines
-                        "shapes, got %d."%(n_out, len(otype))
+                        "types, got %d."%(n_out, len(otype))
-                        "shapes, got %d."%(n_in, len(itype))
+                        "types, got %d."%(n_in, len(itype))
-                        "shapes, got %d."%(n_aux, len(atype))
+                        "types, got %d."%(n_aux, len(atype))
-                                                                    writable=True))
+                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],
-                                                                    writable=False))
+                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],
-                                                                    writable=True))
+                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],
-                                                                    writable=False))
+                                    tensors[tags[i]].append(_ndarray_cls(cast(ndarraies[i],
-                         infertype_functype(infer_type_entry)]
+                         infertype_functype(infer_type_entry),
-            aux[0][:] = 1
+            if in_data[0].stype == 'default':
-            assert (aux[0].asnumpy() == 1).all()
+            if in_data[0].stype == 'default':
-    aux = mx.nd.zeros_like(x)
+    x = x.tostype('csr')
-
+    mx.nd.waitall()
-    def __init__(self, batch_size, data_shape, batches = 5):
+    def __init__(self, batch_size, data_shape, batches = 100):
-        self.sym = sym
+        self.sym_info = (sym.attr_dict(), sym.list_arguments()) if sym is not None else ()
-            for name in self.sym.list_arguments():
+        if self.sym_info:
-            for name in self.sym.list_arguments():
+        if self.sym_info:
-            return in_shape, [in_shape[0]], []
+            return in_shape, [in_shape[0]], [in_shape[0]]
-            return in_type, [in_type[0]], []
+            return in_type, [in_type[0]], [in_type[0]]
-    op = mx.symbol.Custom(data=data, name='sqr', op_type='sqr')
+    aux = mx.symbol.Variable('aux')
-    check_numeric_gradient(op, [x])
+    aux = mx.nd.zeros_like(x)
-    check_numeric_gradient(op, [x])
+    aux = mx.nd.zeros_like(x)
-    mx.contrib.autograd.mark_variables([x], [dx])
+    x.attach_grad()
-        y = mx.nd.Custom(x, op_type='sqr')
+        y = mx.nd.Custom(x, aux, op_type='sqr')
-        super(Lambda, self).__init__()
+    def __init__(self, function, prefix=None):
-        super(HybridLambda, self).__init__()
+    def __init__(self, function, prefix=None):
-            self._func = lambda F, *args: getattr(F, function)(*args)
+            func_dict = {sym: getattr(sym, function), nd: getattr(nd, function)}
-                                           function=self._func_impl.__name__)
+                                           function=self._func_name)
-                        str(value), str(type(value))))
+        >>> x[[0], [1, 2]] = 5
-                               begin=begin, end=end)
+            raise ValueError('Indexing NDArray with index=%s and type=%s is not supported'
-        Returns a sliced view of this array.
+        Returns a sliced view of this array if the elements fetched are contiguous in memory;
-        # multi-dimensional slicing is not supported yet
+        >>> x = mx.nd.arange(0, 8, dtype='int32').reshape((2, 2, 2))
-            if key > self.shape[0] - 1:
+            if key > shape[0] - 1:
-                        key, self.shape[0]))
+                        key, shape[0]))
-            if key.start is not None or key.stop is not None:
+            if key.step is not None and key.step != 1:
-                    str(key), str(type(key))))
+                return self
-            stop = mx_uint(stop)
+        start, stop, _ = _get_index_range(start, stop, self.shape[0])
-            self.handle, start, stop, ctypes.byref(handle)))
+            self.handle, mx_uint(start), mx_uint(stop), ctypes.byref(handle)))
-        idx = mx_uint(idx)
+        if idx < 0:
-            self.handle, idx, ctypes.byref(handle)))
+            self.handle, mx_uint(idx), ctypes.byref(handle)))
-    out = _internal._full(shape=shape, ctx=ctx, dtype=dtype, value=val, out=out)
+    out = empty(shape, ctx, dtype) if out is None else out
-
+
-        Hello, World
+        >>> print(x.value)
-    """A dataset with a data array and a label array.
+    """A dataset of multiple arrays.
-    The i-th sample is `(data[i], lable[i])`.
+    The i-th sample is `(x1[i], x2[i], ...)`.
-        The label array. Can be mxnet or numpy array.
+    *args : one or more arrays
-            self._label = label
+    def __init__(self, *args):
-        return self._data[idx], self._label[idx]
+        if len(self._data) == 1:
-        return len(self._data)
+        return self._length
-
+    dataset = gluon.data.ArrayDataset(X)
-        mod.save_params('image-classifier-%s-%d-final.params'%(opt.model, epochs))
+        mod.save_params('image-classifier-%s-%d-final.params'%(opt.model, opt.epochs))
-                 fixed_param_names=None, state_names=None):
+                 fixed_param_names=None, state_names=None, group2ctxs=None):
-                        state_names=self._state_names)
+                        state_names=self._state_names, group2ctxs=self._group2ctxs)
-                            state_names=self._state_names)
+                            state_names=self._state_names, group2ctxs=self._group2ctxs)
-                 fixed_param_names=None, grad_req='write', state_names=None):
+                 fixed_param_names=None, grad_req='write', state_names=None, group2ctxs=None):
-                                           shared_exec=shared_exec,
+                                           shared_exec=shared_exec, group2ctx=group2ctx,
-                 fixed_param_names=None, state_names=None):
+                 fixed_param_names=None, state_names=None, group2ctxs=None):
-                                                     grad_req=grad_req,
+                                                     grad_req=grad_req, group2ctxs=self._group2ctxs,
-    with file(filename) as f:
+    with open(filename, 'r') as f:
-            self.assign(out_data[0], req[0], mx.nd.array(out))
+            p = in_data[0]  # shape=(b,d)
-        y = in_data[1].asnumpy()
+        p = in_data[0]  # shape=(b,d)
-        self.assign(in_grad[0], req[0], mx.nd.array(grad))
+        self.assign(in_grad[0], req[0], grad)
-        y = in_data[1].asnumpy()  # seems right
+        p = in_data[0]  # shape=(b,d)
-        self.assign(in_grad[0], req[0], mx.nd.array(grad))
+        self.assign(in_grad[0], req[0], grad)
-    print("Done with test")
+    print("performance test")
-        self._cached_params = None
+        self._cached_op_args = None
-                        if name not in params]
+        # verify graph inputs
-            cargs[i] = args[j]
+        cargs = [args[i] if is_arg else i.data()
-        """Infers shape of Parameters from inputs."""
+    def _infer_attrs(self, infer_fn, attr, *args):
-                      zip(out.list_auxiliary_states(), aux_shapes)})
+        arg_attrs, _, aux_attrs = getattr(out, infer_fn)(
-            i.shape = sdict[i.name]
+            setattr(i, attr, sdict[i.name])
-                    return self._call_cached_op(x, *args)
+                    if self._active:
-                    params = {i: j.data(ctx) for i, j in self._reg_params.items()}
+                    self._finish_deferred_init(self._active, x, *args)
-                                          init=weight_initializer,
+                                          dtype=None, init=weight_initializer,
-                                            init=bias_initializer,
+                                            dtype=None, init=bias_initializer,
-                                     allow_deferred_init=True,
+                                     shape=(in_channels,), dtype=None,
-                                    allow_deferred_init=True,
+                                    shape=(in_channels,), dtype=None,
-                                            shape=(in_channels,),
+                                            shape=(in_channels,), dtype=None,
-                                           shape=(in_channels,),
+                                           shape=(in_channels,), dtype=None,
-                                      init=weight_initializer,
+                                      dtype=None, init=weight_initializer,
-                                          init=weight_initializer,
+                                          dtype=None, init=weight_initializer,
-                                            init=bias_initializer,
+                                            dtype=None, init=bias_initializer,
-        if not self.shape or np.prod(self.shape) <= 0:
+        if self.dtype is None or not self.shape or np.prod(self.shape) <= 0:
-                                          init=i2h_weight_initializer,
+                                          dtype=None, init=i2h_weight_initializer,
-                                          init=h2h_weight_initializer,
+                                          dtype=None, init=h2h_weight_initializer,
-                                        init=i2h_bias_initializer,
+                                        dtype=None, init=i2h_bias_initializer,
-                                        init=h2h_bias_initializer,
+                                        dtype=None, init=h2h_bias_initializer,
-                                          init=i2h_weight_initializer,
+                                          dtype=None, init=i2h_weight_initializer,
-                                          init=h2h_weight_initializer,
+                                          dtype=None, init=h2h_weight_initializer,
-                                        init=i2h_bias_initializer,
+                                        dtype=None, init=i2h_bias_initializer,
-                                        init=h2h_bias_initializer,
+                                        dtype=None, init=h2h_bias_initializer,
-                                          init=i2h_weight_initializer,
+                                          dtype=None, init=i2h_weight_initializer,
-                                          init=h2h_weight_initializer,
+                                          dtype=None, init=h2h_weight_initializer,
-                                        init=i2h_bias_initializer,
+                                        dtype=None, init=i2h_bias_initializer,
-                                        init=h2h_bias_initializer,
+                                        dtype=None, init=h2h_bias_initializer,
-        assert same(A[ind].asnumpy(), A2[ind][np.newaxis, :])
+    shape = (rnd.randint(2, 10), rnd.randint(2, 10))    
-    check_slice_nd_csr_fallback(shape)
+    A = mx.nd.sparse.zeros('csr', shape)
-    def test_variations():
+def test_sparse_axis_operations():
-                ret = mx.nd.sum(csr_array, axis=axis)
+                ret = func_name(csr_array, axis=axis)
-                ret_expected = mx.nd.sum(dns, axis=axis)
+                ret_expected = func_name(dns, axis=axis)
-    def test_fallback(axis=0, keepdims=True, exclude=True):
+    def test_fallback(func_name, axis=0, keepdims=True, exclude=True):
-                        exclude=exclude)
+        ret= func_name(csr_array, axis=axis, keepdims=keepdims,
-    test_fallback(axis=0, keepdims=True, exclude=True)
+    test_variations(mx.nd.sum)
-# pylint: skip-file
+import mxnet as mx
-    def get_rnn_sym(num_layers, num_words, num_hidden, num_embed, seq_len):
+    def get_rnn_sym(num_layers, num_words, num_hidden, num_embed, seq_len, sparse_embedding):
-                                 output_dim=num_embed, name='embed')
+        if sparse_embedding:
-    def test_shared_exec_group(exec_grp_shared, exec_grp_created, shared_arg_names=None, extra_args=None):
+    def test_shared_exec_group(exec_grp_shared, exec_grp_created, shared_arg_names=None,
-                        "Shared argument gradient '%s' does not sharing memory." % (arg_name)
+                if check_shared_grad:
-                           shared_arg_names=shared_arg_names, extra_args=extra_args)
+    sparse_embedding_opt = [True, False]
-def test_sparse_nd_binary_rop():
+def test_sparse_nd_binary_scalar_op():
-    and `last_batch_handle` set to `discard`.
+    `NDArrayIter` also supports ``mx.nd.sparse.CSRNDArray``
-                (shuffle or last_batch_handle != 'discard')):
+                (last_batch_handle != 'discard')):
-                                      " and `last_batch_handle` set to `discard`.")
+                                      " with `last_batch_handle` set to `discard`.")
-                          for k, v in self.label]
+            self.data = _shuffle(self.data, self.idx)
-from common import get_data
+from common import get_data, assertRaises
-        pass
+
-
+    def asscipy(self):
-    of array_like
+    arg1: tuple of int, tuple of array_like, array_like, CSRNDArray, scipy.sparse.csr_matrix, \
-            return empty('csr', arg1, ctx=ctx, dtype=dtype)
+            # construct a sparse csr matrix from
-        assert csr_created.context == Context.default_ctx
+        assert csr_created.dtype == dtype, (csr_created, dtype)
-            canonical_csr_nd = f(non_canonical_csr)
+            non_canonical_csr = spsp.csr_matrix((data, indices, indptr), shape=(3, 3), dtype=csr_nd.dtype)
-
+            self.features.add(nn.BatchNorm())
-            self.classifier.add(nn.Dense(classes, in_units=in_channels))
+            self.output = nn.Dense(classes, in_units=in_channels)
-        x = self.classifier(x)
+        x = self.output(x)
-                                expected_result_storage_type=least_sparse(lhs_stype, rhs_stype),
+                                expected_result_storage_type=most_dense(lhs_stype, rhs_stype),
-                                    .format(force_lr_overlap, force_grad_overlap, shape))
+                                print("  force_lr_overlap={}, force_grad_overlap={}, shape={}".
-    """Convert a transpose layer from mxnet to coreml.
+    """Convert a batchnorm layer from mxnet to coreml.
-    .. math:: L = \frac{1}{2} \sum_i \vert {pred}_i - {label}_i \vert.
+    .. math:: L = \sum_i \vert {pred}_i - {label}_i \vert.
-                   else self._hidden_channels)
+        shape = self.i2h_weight.shape
-                        else self._units)
+                        layout='{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0]))
-            s += ', in_channels={0}'.format(self.in_channels)
+        in_channels = self.gamma.shape[0]
-                                                 self._channels),
+                        mapping='{0} -> {1}'.format(shape[1] if shape[1] else None, shape[0]),
-                assert i == 0 or i == j, \
+            for self_dim, data_dim in zip(self.shape, data.shape):
-
+    def __repr__(self):
-                   else self._hidden_size)
+        shape = self.i2h_weight[0].shape
-            assert_allclose(np.expand_dims(x.asnumpy(), axis=axis), y.asnumpy())
+        for axis in range(-ndim + 1, ndim):
-__version__ = "0.12.0"
+__version__ = "0.12.1"
-                 -4.0, 4.0]
+                 -4.0, 4.0],
-                                                    " with `last_batch_handle` set to `discard`."
+    #test CSRNDArray with shuffle=True will throw NotImplementedError 
-           'Dropout', 'BatchNorm', 'LeakyReLU', 'Embedding', 'Flatten']
+           'Dropout', 'BatchNorm', 'LeakyReLU', 'Embedding', 'Flatten',
-    # Operators implemented for CPU only currently
+    # Currently disabled on GPU as syevd needs cuda8
-    if not(default_context() == mx.cpu()):
+    # Currently disabled on GPU as syevd needs cuda8
-    #check_fw(test_syevd, [a_np], [u_np, l_np], np.float32)
+    check_fw(test_syevd, [a_np], [u_np, l_np], np.float32)
-    out[:] = val
+    if ctx is None:
-    return _internal._MulScalar(ones(shape=shape, dtype=dtype, **kwargs), scalar=val)
+    return _internal._full(shape=shape, dtype=dtype, value=val, **kwargs)
-                self.classifier.add(nn.Dense(classes))
+            self.output = nn.Dense(classes)
-        x = self.classifier(x)
+        x = self.output(x)
-            self.classifier = nn.Dense(classes)
+            self.output = nn.Dense(classes)
-        x = self.classifier(x)
+        x = self.output(x)
-            self.classifier.add(nn.Dense(classes))
+            self.output = nn.Dense(classes)
-        x = self.classifier(x)
+        x = self.output(x)
-                self.classifier.add(nn.Dense(classes))
+            self.output = nn.Dense(classes)
-        x = self.classifier(x)
+        x = self.output(x)
-            self.classifier.add(nn.Dense(classes, in_units=channels[-1]))
+            self.output = nn.Dense(classes, in_units=channels[-1])
-        x = self.classifier(x)
+        x = self.output(x)
-            self.classifier.add(nn.Flatten())
+            self.output = nn.HybridSequential(prefix='')
-        x = self.classifier(x)
+        x = self.output(x)
-                                         bias_initializer='zeros'))
+            self.features.add(nn.Dense(4096, activation='relu',
-        x = self.classifier(x)
+        x = self.output(x)
-                self.defaultString = 'true'
+            elif self.type == "bool":
-                for k in _str2tuple(node["attr"]["kernel"]):
+            if ("no_bias" in node["attrs"]) and int(node["attrs"]["no_bias"]):
-                for k in _str2tuple(node["attr"]["kernel"]):
+                cur_param = pre_filter * int(node["attrs"]["num_filter"])
-                cur_param += int(node["attr"]["num_filter"])
+                cur_param += int(node["attrs"]["num_filter"])
-                cur_param = pre_filter * (int(node["attr"]["num_hidden"]))
+            if ("no_bias" in node["attrs"]) and int(node["attrs"]["no_bias"]):
-                cur_param = (pre_filter+1) * (int(node["attr"]["num_hidden"]))
+                cur_param = (pre_filter+1) * (int(node["attrs"]["num_hidden"]))
-                                                 node["attr"]["num_filter"])
+            label = r"Convolution\n%s/%s, %s" % ("x".join(_str2tuple(node["attrs"]["kernel"])),
-            label = r"FullyConnected\n%s" % node["attr"]["num_hidden"]
+            label = r"FullyConnected\n%s" % node["attrs"]["num_hidden"]
-            label = r"%s\n%s" % (op, node["attr"]["act_type"])
+            label = r"%s\n%s" % (op, node["attrs"]["act_type"])
-                                             if "stride" in node["attr"] else "1")
+            label = r"Pooling\n%s, %s/%s" % (node["attrs"]["pool_type"],
-                label = node["attr"]["op_type"]
+                label = node["attrs"]["op_type"]
-                                params = input_node["attr"]
+                            if "attrs" in input_node:
-    print ('Saved model successfully to {}'.format(args.save_model_name))
+    print('Saved model successfully to {}'.format(args.save_model_name))
-        return self._record.read_idx(idx)
+        return self._record.read_idx(self._record.keys[idx])
-    eval_iter = DetRecordIter(path_imgrec, batch_size, data_shape,
+    eval_iter = DetRecordIter(path_imgrec, batch_size, data_shape, mean_pixels=mean_pixels,
-            -  **S** (*CSRNDArray or scipy.sparse.csr_matrix*) - A sparse matrix.
+            -  **S** (*CSRNDArray or scipy.sparse.csr.csr_matrix*) - A sparse matrix.
-            float32 otherwise.
+            The default dtype is ``S.dtype``.
-            The default dtype is float32.
+            The default dtype is ``data.dtype`` if ``data`` is an NDArray or numpy.ndarray, \
-    arg1: tuple of int, tuple of array_like, array_like, CSRNDArray or scipy.sparse.csr_matrix
+    arg1: NDArray, CSRNDArray, numpy.ndarray, scipy.sparse.csr.csr_matrix, tuple of int or tuple \
-    shape : tuple of int
+    shape : tuple of int, optional
-        if `values` is an `NDArray`, `float32` otherwise.
+        The data type of the output array.
-            dns = _array(arg1, ctx=ctx, dtype=dtype)
+            # prepare default ctx and dtype since mx.nd.array doesn't use default values
-        ctx = Context.default_ctx
+    ctx = Context.default_ctx if ctx is None else ctx
-    dtype = mx_real_t if dtype is None else dtype
+    dtype = _prepare_default_dtype(data, dtype)
-            float32 otherwise.
+            The default dtype is ``S.dtype``.
-    arg1: tuple of int, tuple of array_like, array_like or RowSparseNDArray
+    arg1: NDArray, numpy.ndarray, RowSparseNDArray, tuple of int or tuple of array_like
-    shape : tuple of int
+    shape : tuple of int, optional
-        if `data` is an `NDArray`, `float32` otherwise.
+        The data type of the output array.
-            dns = _array(arg1, ctx=ctx, dtype=dtype)
+            # prepare default dtype since mx.nd.array doesn't use default values
-        ctx = Context.default_ctx
+    ctx = Context.default_ctx if ctx is None else ctx
-    dtype = mx_real_t if dtype is None else dtype
+    dtype = _prepare_default_dtype(data, dtype)
-        dim0 = 0 if num_indices == 0 else indices[num_indices - 1].asscalar() + 1
+        if num_indices == 0:
-        Device context (default is the current default context).
+        The default context is ``source_array.context`` if ``source_array`` is an NDArray. \
-        if `source_array` is an `NDArray`, `float32` otherwise.
+        if `source_array` is an `NDArray`, `numpy.ndarray` or `scipy.sparse.csr.csr_matrix`, \
-        arr[:] = source_array
+        # prepare dtype and ctx based on source_array, if not provided
-        return csr_matrix((csr.data, csr.indices, csr.indptr), shape=csr.shape, dtype=dtype)
+        dtype = _prepare_default_dtype(source_array, dtype)
-    def check_create_csr_from_nd(shape, density):
+    def check_create_csr_from_nd(shape, density, dtype):
-        data = matrix.data
+        # create data array with provided dtype and ctx
-    dim1 = 50
+    dim0 = 20
-        check_create_csr_from_nd(shape, density)
+        check_create_csr_from_nd(shape, density, dtype)
-        arr = f(dense_arr, dtype=dtype)
+    def check_create_from_dns(shape, f, dense_arr, dtype, default_dtype, ctx):
-    dense_arrs = [mx.nd.ones(shape), np.ones(shape), np.ones(shape).tolist()]
+    src_dtype = np.float64
-            check_create_from_dns(shape, f, dense_arr, dtype)
+            default_dtype = dense_arr.dtype if isinstance(dense_arr, (NDArray, np.ndarray)) \
-        arr = mx.nd.sparse.csr_matrix(shape)
+    def check_csr_empty(shape, dtype, ctx):
-        arr = mx.nd.sparse.row_sparse_array(shape)
+    def check_rsp_empty(shape, dtype, ctx):
-    check_rsp_empty(shape_3d)
+    check_csr_empty(shape, dtype, ctx)
-def test_factorization_machine_module():
+def test_factorization_machine_module(verbose=False):
-    check_factorization_machine_module('adam')
+    if verbose is True:
-if __name__ == '__main__':
+def main():
-    optim_args = {'learning_rate': 0.05, 'wd': 0.00001, 'momentum': 0.9}
+    optim_args = {'learning_rate': 0.001, 'wd': 0.00001, 'momentum': 0.9}
-big_shape = (1200, 1200)        # bigger than BIGARRAY_BOUND
+big_shape = (1200, 1200)        # bigger than MXNET_KVSTORE_BIGARRAY_BOUND
-        big_v = mx.nd.zeros(big_shape)
+        v = mx.nd.sparse.zeros('row_sparse', shape)
-
+            kv.push('11', v)
-            big_num_rows = shape[0]
+            val = mx.nd.sparse.zeros('row_sparse', shape)
-            kv.row_sparse_pull('100', out=big_val, row_ids=mx.nd.array(big_all_row_ids, dtype='int64'))
+            kv.row_sparse_pull('100', out=big_val, row_ids=mx.nd.array(big_all_row_ids))
-            check_diff_to_scalar(big_val, mx.nd.ones(big_shape))
+            check_diff_to_scalar(val, 1)
-            row_ids = mx.nd.array(row_ids_np, dtype='int64')
+            row_ids = mx.nd.array(row_ids_np)
-                    'exp', 'expm1', 'log', 'log10', 'log2', 'log1p', 'sqrt', 'rsqrt', 'square'])
+                    'exp', 'expm1', 'log', 'log10', 'log2', 'log1p', 'sqrt', 'rsqrt', 'square',
-                 'exp', 'expm1', 'square']:
+                 'exp', 'expm1', 'square', 'reciprocal', 'argmax_channel']:
-                 'arcsinh', 'arctanh', 'log', 'log10', 'log2', 'log1p', 'sqrt', 'rsqrt']:
+                 'arcsinh', 'arctanh', 'log', 'log10', 'log2', 'log1p', 'sqrt', 'rsqrt',
-                    'square'])
+                    'square', 'reciprocal' 'reshape_like', 'cbrt', 'rcbrt', 'relu', 'sigmoid',
-                 'exp', 'expm1',  'square']:
+                 'exp', 'expm1',  'square', 'reciprocal', 'argmax_channel']:
-                 'arcsinh', 'arctanh', 'log', 'log10', 'log2', 'log1p', 'sqrt', 'rsqrt']:
+                 'arcsinh', 'arctanh', 'log', 'log10', 'log2', 'log1p', 'sqrt', 'rsqrt',
-                    self.defaultString = "false"
+            elif self.defaultString == 'False':
-                for k in _str2tuple(node["attrs"]["kernel"]):
+            if ("no_bias" in node["attr"]) and (node["attr"]["no_bias"] == 'True'):
-                for k in _str2tuple(node["attrs"]["kernel"]):
+                cur_param = pre_filter * int(node["attr"]["num_filter"])
-                cur_param += int(node["attrs"]["num_filter"])
+                cur_param += int(node["attr"]["num_filter"])
-                cur_param = pre_filter * (int(node["attrs"]["num_hidden"]))
+            if ("no_bias" in node["attr"]) and (node["attr"]["no_bias"] == 'True'):
-                cur_param = (pre_filter+1) * (int(node["attrs"]["num_hidden"]))
+                cur_param = (pre_filter+1) * (int(node["attr"]["num_hidden"]))
-                                                 node["attrs"]["num_filter"])
+            label = r"Convolution\n%s/%s, %s" % ("x".join(_str2tuple(node["attr"]["kernel"])),
-            label = r"FullyConnected\n%s" % node["attrs"]["num_hidden"]
+            label = r"FullyConnected\n%s" % node["attr"]["num_hidden"]
-            label = r"%s\n%s" % (op, node["attrs"]["act_type"])
+            label = r"%s\n%s" % (op, node["attr"]["act_type"])
-                                             if "stride" in node["attrs"] else "1")
+            label = r"Pooling\n%s, %s/%s" % (node["attr"]["pool_type"],
-                label = node["attrs"]["op_type"]
+                label = node["attr"]["op_type"]
-                                params = input_node["attrs"]
+                            if "attr" in input_node:
-            if isinstance(value, Block):
+            elif isinstance(value, Block):
-
+def test_softmax():
-        key : slice
+        key : int or slice
-               [4, 5, 6]])
+        array([[ 1.,  0.,  2.],
-        array([[0, 0, 3]], dtype=float32)
+        array([[ 0.,  0.,  3.]], dtype=float32)
-            raise ValueError("__getitem__ with int key is not implemented for CSRNDArray")
+            if key == -1:
-    from setuptools.command.install import install
+    kwargs = {'install_requires': ['numpy', 'requests', 'graphviz'], 'zip_safe': False}
-
+sys.path.insert(0, CURRENT_DIR)
-                self.defaultString = 'true'
+            elif self.type == "bool":
-                for k in _str2tuple(node["attr"]["kernel"]):
+            if ("no_bias" in node["attrs"]) and int(node["attrs"]["no_bias"]):
-                for k in _str2tuple(node["attr"]["kernel"]):
+                cur_param = pre_filter * int(node["attrs"]["num_filter"])
-                cur_param += int(node["attr"]["num_filter"])
+                cur_param += int(node["attrs"]["num_filter"])
-                cur_param = pre_filter * (int(node["attr"]["num_hidden"]))
+            if ("no_bias" in node["attrs"]) and int(node["attrs"]["no_bias"]):
-                cur_param = (pre_filter+1) * (int(node["attr"]["num_hidden"]))
+                cur_param = (pre_filter+1) * (int(node["attrs"]["num_hidden"]))
-                                                 node["attr"]["num_filter"])
+            label = r"Convolution\n%s/%s, %s" % ("x".join(_str2tuple(node["attrs"]["kernel"])),
-            label = r"FullyConnected\n%s" % node["attr"]["num_hidden"]
+            label = r"FullyConnected\n%s" % node["attrs"]["num_hidden"]
-            label = r"%s\n%s" % (op, node["attr"]["act_type"])
+            label = r"%s\n%s" % (op, node["attrs"]["act_type"])
-                                             if "stride" in node["attr"] else "1")
+            label = r"Pooling\n%s, %s/%s" % (node["attrs"]["pool_type"],
-                label = node["attr"]["op_type"]
+                label = node["attrs"]["op_type"]
-                                params = input_node["attr"]
+                            if "attrs" in input_node:
-    make_op_func : str
+    make_op_func : function
-        op_name_prefix = _get_op_name_prefix(function.__name__)
+        op_name_prefix = _get_op_name_prefix(name)
-            function.__module__ = "%s.%s.%s" % (root_namespace, module_name, op_name_prefix[1:-1])
+            func_name = name[len(op_name_prefix):]
-            module_internal.__all__.append(function.__name__)
+            module_name = "%s.%s.%s" % (root_namespace, module_name, op_name_prefix[1:-1])
-            module_op.__all__.append(function.__name__)
+            func_name = name
-from . import _internal, contrib, linalg, sparse, random, utils
+from . import _internal, contrib, linalg, op, random, sparse, utils
-__all__ = []
+import os as _os
-from .op import NDArrayBase
+from ._internal import NDArrayBase
-# pylint: disable=unused-import
+# coding: utf-8
-        from .._cy2.ndarray import CachedOp, _imperative_invoke
+    from .gen_op import * # pylint: disable=unused-wildcard-import
-    ret_type = ctypes.c_char_p()
+    pass
-_init_op_module('mxnet', 'ndarray', _make_ndarray_function)
+__all__ = ['CachedOp']
-from .ndarray import _STORAGE_TYPE_STR_TO_ID
+from . import op
-from . import _internal, contrib, linalg, sparse, random
+from . import _internal, contrib, linalg, op, random, sparse
-__all__ = []
+# Use different version of SymbolBase
-# pylint: disable=unused-import
+# coding: utf-8
-        from .._cy2.symbol import _symbol_creator
+    from .gen_op import *
-                         ret_type)
+    pass
-_init_op_module('mxnet', 'symbol', _make_atomic_symbol_function)
+__all__ = []
-from .op import SymbolBase, _set_symbol_class, AttrScope, _Null  # pylint: disable=unused-import
+from ._internal import SymbolBase, _set_symbol_class
-    kwargs = {'install_requires': ['numpy', 'requests', 'graphviz'], 'zip_safe': False}
+    kwargs = {'install_requires': required_packages,
-
+      cmdclass={'install': PostInstallCommand},
-    dtype: np.float32 or np.float64
+    dtype: np.float16 or np.float32 or np.float64
-    assert dtype == np.float32 or dtype == np.float64
+    assert dtype == np.float16 or dtype == np.float32 or dtype == np.float64
-    dtype: np.float32 or np.float64
+    dtype: np.float16 or np.float32 or np.float64
-    assert dtype == np.float32 or dtype == np.float64
+    assert dtype == np.float16 or dtype == np.float32 or dtype == np.float64
-    dtype: np.float32 or np.float64
+    dtype: np.float16 or np.float32 or np.float64
-    assert dtype == np.float32 or dtype == np.float64
+    assert dtype == np.float16 or dtype == np.float32 or dtype == np.float64
-    dtype: np.float32 or np.float64
+    dtype: np.float16 or np.float32 or np.float64
-    assert dtype == np.float32 or dtype == np.float64
+    assert dtype == np.float16 or dtype == np.float32 or dtype == np.float64
-    dtype: np.float32 or np.float64
+    dtype: np.float16 or np.float32 or np.float64
-    assert dtype == np.float32 or dtype == np.float64
+    assert dtype == np.float16 or dtype == np.float32 or dtype == np.float64
-    dtype: np.float32 or np.float64
+    dtype: np.float16 or np.float32 or np.float64
-    assert dtype == np.float32 or dtype == np.float64
+    assert dtype == np.float16 or dtype == np.float32 or dtype == np.float64
-                      lambda x: (1 / x))
+                      lambda x: 1. / (x * np.log(10.)))
-                      lambda x: (1 / x))
+                      lambda x: 1. / (x * np.log(2.)))
-                                           lambda x: (1 / x),
+                                           lambda x: 1. / (x * np.log(10.)),
-                                           lambda x: (1 / x),
+                                           lambda x: 1. / (x * np.log(2.)),
-                      mp_sgd_update, mp_sgd_mom_update, ftrl_update)
+                      mp_sgd_update, mp_sgd_mom_update, square, ftrl_update)
-        return zeros(weight.shape, weight.context)  # history
+        return zeros(weight.shape, weight.context, stype=weight.stype)  # history
-        weight[:] += -lr * (grad / sqrt(history + self.float_stable_eps) + wd * weight)
+        save_history_stype = history.stype
-    out = mx.sym.MakeLoss(out)
+    out = mx.sym.make_loss(out)
-    assert(metric.get()[1] < 0.05), metric.get()[1]
+    def check_factorization_machine_module(optimizer=None, num_epochs=None):
-
+        if verbose is True:
-                                                          ograd_density=ograd_density)
+    with warnings.catch_warnings():
-#    x.backward()
+    with mx.autograd.record():
-#
+@unittest.skip("Test fails intermittently. Temporarily disabled until fixed. Tracked at https://github.com/apache/incubator-mxnet/issues/8230")
-    weight = arg_params['weight']
+    weight_index = mod._exec_group.param_names.index('weight')
-                kv.row_sparse_pull('weight', weight, row_ids=[row_ids])
+                kv.row_sparse_pull('weight', weight_param, row_ids=[row_ids],
-from ..base import c_array, mx_real_t
+from ..base import c_array, mx_real_t, integer_types
-    `indptr` and `indices`. It uses the standard CSR representation where the column indices for
+    `indptr` and `indices`. It uses the CSR representation where the column indices for
-    array([ 1.,  2.,  3.], dtype=float32)
+
-        >>> a = mx.nd.sparse.csr_matrix(data, indptr, indices, (3, 3))
+        >>> a = mx.nd.sparse.csr_matrix((data, indices, indptr), shape=(3, 3))
-    `indices`.
+    `indices`. The number of dimensions has to be at least 2.
-    `src` is converted to a `np.ndarray` if it's neither an `NDArray` nor an `np.ndarray`.
+def _prepare_src_array(source_array, dtype):
-    """Creates a 2D array with compressed sparse row (CSR) format.
+    if not isinstance(source_array, NDArray) and not isinstance(source_array, np.ndarray):
-        An object exposing the array interface, with shape [nnz].
+    arg1: tuple of int, tuple of array_like, array_like, CSRNDArray or scipy.sparse.csr_matrix
-    >>> a = mx.nd.sparse.csr_matrix([1, 2, 3], [0, 1, 2, 2, 3], [1, 0, 2], (4, 3))
+    >>> a = mx.nd.sparse.csr_matrix(([1, 2, 3], [1, 0, 2], [0, 1, 2, 2, 3]), shape=(4, 3))
-                                          [indptr_type, indices_type], aux_shapes))
+    data = _prepare_src_array(data, dtype)
-    """Creates a multidimensional row sparse array with a set of tensor slices at given indices.
+def row_sparse_array(arg1, shape=None, ctx=None, dtype=None):
-        An object exposing the array interface, with shape [D0].
+    arg1: tuple of int, tuple of array_like, array_like or RowSparseNDArray
-    >>> a = mx.nd.sparse.row_sparse_array([[1, 2], [3, 4]], [1, 4], (6, 2))
+    >>> a = mx.nd.sparse.row_sparse_array(([[1, 2], [3, 4]], [1, 4]), shape=(6, 2))
-                                                [indices_type], [indices.shape]))
+    data = _prepare_src_array(data, dtype)
-def zeros(stype, shape, ctx=None, dtype=None, aux_types=None, **kwargs):
+def zeros(stype, shape, ctx=None, dtype=None, **kwargs):
-    assert(len(aux_types) == len(_STORAGE_AUX_TYPES[stype]))
+    if stype == 'row_sparse' or stype == 'csr':
-def empty(stype, shape, ctx=None, dtype=None, aux_types=None):
+def empty(stype, shape, ctx=None, dtype=None):
-        return zeros(stype, shape, ctx=ctx, dtype=dtype, aux_types=aux_types)
+        return zeros(stype, shape, ctx=ctx, dtype=dtype)
-def array(source_array, ctx=None, dtype=None, aux_types=None):
+def array(source_array, ctx=None, dtype=None):
-    >>> csr = sp.csr_matrix((2, 100))
+    >>> import scipy.sparse as spsp
-               "Please use `cast_storage` to create RowSparseNDArray or CSRNDArray from an NDArray"
+               "Please use `tostype` to create RowSparseNDArray or CSRNDArray from an NDArray"
-        arr = empty(source_array.stype, source_array.shape, ctx, dtype, aux_types)
+        arr = empty(source_array.stype, source_array.shape, ctx=ctx, dtype=dtype)
-    if spsp is not None and isinstance(source_array, spsp.csr.csr_matrix):
+    elif spsp and isinstance(source_array, spsp.csr.csr_matrix):
-        return arr
+        return csr_matrix((csr.data, csr.indices, csr.indptr), shape=csr.shape, dtype=dtype)
-def zeros(shape, ctx=None, dtype=None, stype=None, aux_types=None, **kwargs):
+def zeros(shape, ctx=None, dtype=None, stype=None, **kwargs):
-        return _zeros_sparse_ndarray(stype, shape, ctx, dtype, aux_types, **kwargs)
+        return _zeros_sparse_ndarray(stype, shape, ctx, dtype, **kwargs)
-def empty(shape, ctx=None, dtype=None, stype=None, aux_types=None):
+def empty(shape, ctx=None, dtype=None, stype=None):
-        return _empty_sparse_ndarray(stype, shape, ctx, dtype, aux_types)
+        return _empty_sparse_ndarray(stype, shape, ctx, dtype)
-def array(source_array, ctx=None, dtype=None, aux_types=None):
+def array(source_array, ctx=None, dtype=None):
-        return _sparse_array(source_array, ctx=ctx, dtype=dtype, aux_types=aux_types)
+        return _sparse_array(source_array, ctx=ctx, dtype=dtype)
-        return _sparse_array(source_array, ctx=ctx, dtype=dtype, aux_types=aux_types)
+        return _sparse_array(source_array, ctx=ctx, dtype=dtype)
-                                         (num_rows, num_cols), dtype=dtype)
+        result = mx.nd.sparse.csr_matrix((csr.data, csr.indices, csr.indptr),
-            return result, (np.array([], dtype=dtype), np.array([], dtype='int64'))
+            return result, (np.array([], dtype=dtype), np.array([]))
-        arr = mx.nd.sparse.row_sparse_array(val, indices, shape, indices_type=np.int64, dtype=dtype)
+        arr = mx.nd.sparse.row_sparse_array((val, indices), shape=shape, dtype=dtype)
-    repeat = 10
+    repeat = 1
-    densities = [0, 0.01, 0.1, 0.2, 0.5]
+    densities = [0, 0.5]
-    densities = [0, 0.01, 0.1, 0.2, 0.5]
+    densities = [0, 0.5]
-                                              indices=indices, shape=shape)
+        csr_created = mx.nd.sparse.csr_matrix((data, indices, indptr), shape=shape)
-            import scipy.sparse as sp
+            import scipy.sparse as spsp
-            csr_sp = sp.rand(shape[0], shape[1], density, format="csr")
+            csr_sp = spsp.rand(shape[0], shape[1], density, format="csr")
-            non_canonical_csr = sp.csr_matrix((data, indices, indptr), shape=(3, 3))
+            non_canonical_csr = spsp.csr_matrix((data, indices, indptr), shape=(3, 3))
-    densities = [0, 0.01, 0.1, 0.2, 0.5]
+    densities = [0, 0.5]
-    densities = [0, 0.01, 0.1, 0.2, 0.5]
+    densities = [0, 0.5, 1]
-        rsp_created = mx.nd.sparse.row_sparse_array(data=data, indices=indices, shape=shape)
+        rsp_created = mx.nd.sparse.row_sparse_array((data, indices), shape=shape)
-        assert(nd.stype == stype)
+def test_create_sparse_nd_infer_shape():
-    a = mx.nd.zeros((2,2))
+    a = mx.nd.ones((2,2))
-                     'csr', (2,2), aux_types=[np.int32, np.int32])
+    assert_exception(mx.nd.sparse.csr_matrix, ValueError,
-__version__ = "0.11.1"
+__version__ = "0.12.0"
-    start : float, optional
+    start : number, optional
-    stop : float
+    stop : number
-    step : float, optional
+    step : number, optional
-            assert_almost_equal(pred, gt)
+        # General Random Tests
-
+    from __builtin__ import sum as py_sum
-    aux_shapes = sum(aux_shapes, ())
+    aux_shapes = py_sum(aux_shapes, ())
-        super(CastAug, self).__init__(type='float32')
+    def __init__(self, typ='float32'):
-        src = src.astype(np.float32)
+        src = src.astype(self.typ)
-            inputs = [symbol.var('input_%d'%i) for i in range(len(args))]
+            if len(args) > 1:
-
+    def _clear_cached_op(self):
-        arg_dict = {restore_prefix+k: v for k, v in ndarray.load(filename).items()}
+        loaded = [(k[4:] if k.startswith('arg:') or k.startswith('aux:') else k, v) \
-    x.backward()
+#def test_autograd_save_memory():
-                                    compare_optimizer(opt1(**kwarg), opt2(**kwarg), shape, dtype, g_stype='row_sparse')
+#def test_rms():
-from utils import DotDict, namedtuple_with_defaults, zip_namedtuple, config_as_dict
+from config.utils import DotDict, namedtuple_with_defaults, zip_namedtuple, config_as_dict
-from imdb import Imdb
+from dataset.imdb import Imdb
-from pycocotools.coco import COCO
+from dataset.imdb import Imdb
-from imdb import Imdb
+from dataset.imdb import Imdb
-from common import multi_layer_feature, multibox_layer
+from symbol.common import multi_layer_feature, multibox_layer
-import symbol_builder
+from symbol import symbol_builder
-    kv = mx.kv.create('dist_sync')
+def test_sync_init():
-    os.chdir(data_dir)
+    """Download and extract bz2 data.
-    os.chdir("..")
+# Licensed to the Apache Software Foundation (ASF) under one
-        to sync all workers.
+        The actual operation is executed asynchronously. If there are consecutive
-        for the same input key(s) are finished.
+        `pull` is executed asynchronously after all previous `pull` calls and only
-        The returned values are gauranteed to be the latest values in the store.
+        The returned values are guaranteed to be the latest values in the store.
-        For `RowSparseNDArray` values, please use ``row_sparse_pull`` instead.
+        For `RowSparseNDArray` values, this call is ignored,
-        `push`/`pull`/`row_sparse_pull` calls for the same input key(s) are finished.
+        `pull`/`row_sparse_pull` calls and the last `push` call for the
-        check_diff_to_scalar(val2, num)
+            num = (nworker + 1) * nworker * rate / 2 * (i + 1) + 1
-        check_diff_to_scalar(val, expected)
+            # select a random subset of rows this worker is interested in
-        check_diff_to_scalar(big_val, mx.nd.ones(big_shape))
+            # pull a subset of rows this worker is interested in
-        check_diff_to_scalar(val, expected, rank=my_rank)
+            # select a random subset of rows this worker is interested in
-            {
+        },
-                'inputs': [ ('lam', [ [ 1.0, 8.5 ], [ 2.7 , 0.5 ] ]) ],
+                'inputs': [ ('lam', [ [ 25.0, 8.5 ], [ 2.7 , 0.5 ] ]) ],
-        ])
+        ]
-    shape = (100, 100)
+    # Create enough samples such that we get a meaningful distribution.
-
+        # check multi-distribution sampling
-                        assert np.abs(check_func(samples, params)) < tol, "symbolic test: %s check for `%s` did not pass" % (check_name, name)
+        # check multi-distribution sampling
-        print('Arch         :', platform.architecture())
+        check_python()
-            print('No corresponding pip install for current python.')
+        check_pip()
-            pass
+        check_mxnet()
-        print('version      :', platform.version())
+        check_os()
-            subprocess.call(['wmic', 'cpu', 'get', 'name'])
+        check_hardware()
-            test_connection(name, url, args.timeout)
+        check_network(args)
-            assert len(shared_group.execs) == len(self._context)
+            assert len(shared_group.execs) >= len(self._context)
-        optimizer_params['multi_precision'] = True
+            'lr_scheduler': lr_scheduler,
-from .ndarray import (NDArray, zeros, clip, sqrt, array, maximum, abs as NDabs)
+from .ndarray import (NDArray, zeros, clip, sqrt, cast, maximum, abs as NDabs)
-                 param_dict=None):
+                 multi_precision=False, param_dict=None):
-    def __init__(self, momentum=0.0, multi_precision=False, **kwargs):
+    def __init__(self, momentum=0.0, **kwargs):
-        momentum = None
+    def create_state_multi_precision(self, index, weight):
-            return (momentum, weight_master_copy)
+            weight_master_copy = weight.astype(numpy.float32)
-    def update(self, index, weight, grad, state):
+    def _update_impl(self, index, weight, grad, state, multi_precision=False):
-        if not use_multi_precision:
+        if not multi_precision:
-            self.states[index] = self.optimizer.create_state(index, weight)
+            self.states[index] = self.optimizer.create_state_multi_precision(index, weight)
-        self.optimizer.update(index, weight, grad, self.states[index])
+        self.optimizer.update_multi_precision(index, weight, grad, self.states[index])
-            assert_almost_equal(state1.asnumpy(), state2.asnumpy())
+    state1 = opt1.create_state_multi_precision(0, w1)
-    mp_options = [{}]
+    mp_options = [{}, {'multi_precision': False}, {'multi_precision': True}]
-                          np.float32, w_stype='row_sparse', g_stype='row_sparse')
+    cg_options = [{}, {'clip_gradient': 0.4}, {'clip_gradient': 0.5}]
-        compare_optimizer(opt1(**kwarg), opt2(**kwarg), shape, np.float32, g_stype='row_sparse')
+    cg_options = [{}, {'clip_gradient': 0.4}, {'clip_gradient': 0.5}]
-    If ``flatten`` is set to be True, then the shapes are:
+    Inputs:
-        The output would have shape `(x1, x2, ..., xn, units)`.
+    Outputs:
-    """Applies an activation function to input.
+    r"""Applies an activation function to input.
-        Arbitrary.
+    Inputs:
-        Same shape as input.
+    Outputs:
-        Arbitrary.
+    Inputs:
-        Same shape as input.
+    Outputs:
-        Arbitrary.
+    Inputs:
-        Same shape as input.
+    Outputs:
-        Arbitrary.
+    Inputs:
-        Same shape as input.
+    Outputs:
-    """Turns non-negative integers (indexes/tokens) into dense vectors
+    r"""Turns non-negative integers (indexes/tokens) into dense vectors
-        2D tensor with shape: `(N, M)`.
+    Inputs:
-        3D tensor with shape: `(N, M, output_dim)`.
+    Output:
-    """Flattens the input to two dimensional.
+    r"""Flattens the input to two dimensional.
-        Arbitrary shape `(N, a, b, c, ...)`
+    Inputs:
-        2D tensor with shape: `(N, a*b*c...)`
+    Output:
-    """1D convolution layer (e.g. temporal convolution).
+    r"""1D convolution layer (e.g. temporal convolution).
-        (batch_size, in_channels, width) if `layout` is `NCW`.
+    Inputs:
-        out_width is calculated as::
+    Outputs:
-            out_width = floor((width+2*padding-dilation*(kernel_size-1)-1)/stride)+1
+              out_width = floor((width+2*padding-dilation*(kernel_size-1)-1)/stride)+1
-    """2D convolution layer (e.g. spatial convolution over images).
+    r"""2D convolution layer (e.g. spatial convolution over images).
-        (batch_size, in_channels, height, width) if `layout` is `NCHW`.
+    Inputs:
-        (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.
+    Outputs:
-            out_width = floor((width+2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)/stride[1])+1
+              out_height = floor((height+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1
-        `NCDHW`.
+    Inputs:
-        out_depth, out_height and out_width are calculated as::
+    Outputs:
-            out_width = floor((width+2*padding[2]-dilation[2]*(kernel_size[2]-1)-1)/stride[2])+1
+              out_depth = floor((depth+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1
-        (batch_size, channels, out_width) if `layout` is `NCW`.
+    Inputs:
-        out_width is calculated as::
+    Outputs:
-            out_width = (width-1)*strides-2*padding+kernel_size+output_padding
+              out_width = (width-1)*strides-2*padding+kernel_size+output_padding
-        (batch_size, in_channels, height, width) if `layout` is `NCHW`.
+    Inputs:
-        (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.
+    Outputs:
-            out_width = (width-1)*strides[1]-2*padding[1]+kernel_size[1]+output_padding[1]
+              out_height = (height-1)*strides[0]-2*padding[0]+kernel_size[0]+output_padding[0]
-        (batch_size, in_channels, depth, height, width) if `layout` is `NCDHW`.
+    Inputs:
-        out_depth, out_height and out_width are calculated as::
+    Outputs:
-        (batch_size, channels, out_width) if `layout` is `NCW`.
+    Inputs:
-        out_width is calculated as::
+    Outputs:
-            out_width = floor((width+2*padding-pool_size)/strides)+1
+              out_width = floor((width+2*padding-pool_size)/strides)+1
-        equation.
+          When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        (batch_size, channels, height, width) if `layout` is `NCHW`.
+    Inputs:
-        (batch_size, channels, out_height, out_width)  if `layout` is `NCHW`.
+    Outputs:
-        out_height and out_width are calculated as::
+              out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1
-        equation.
+          When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        is `NCDHW`.
+    Inputs:
-        out_depth, out_height and out_width are calculated as ::
+    Outputs:
-            out_width = floor((width+2*padding[2]-pool_size[2])/strides[2])+1
+              out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1
-        equation.
+          When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        (batch_size, channels, out_width) if `layout` is `NCW`.
+    Inputs:
-        out_width is calculated as::
+    Outputs:
-            out_width = floor((width+2*padding-pool_size)/strides)+1
+              out_width = floor((width+2*padding-pool_size)/strides)+1
-        equation.
+          When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        (batch_size, channels, height, width) if `layout` is `NCHW`.
+    Inputs:
-        (batch_size, channels, out_height, out_width)  if `layout` is `NCHW`.
+    Outputs:
-        out_height and out_width are calculated as::
+              out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1
-        equation.
+          When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        is `NCDHW`.
+    Inputs:
-        out_depth, out_height and out_width are calculated as ::
+    Outputs:
-            out_width = floor((width+2*padding[2]-pool_size[2])/strides[2])+1
+              out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1
-        equation.
+          When `ceil_mode` is `True,` ceil will be used instead of floor in this
-        cell : rnn cell
+        cell : RecurrentCell
-        and the output recurrent state is omitted.
+    Inputs:
-        input has shape `(sequence_length, batch_size, input_size)`
+    Inputs:
-        and the output recurrent state is omitted.
+    Outputs:
-        and the output recurrent state is omitted.
+    Inputs:
-    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
+    >>> input = mx.nd.random_uniform(shape=(5, 3, 10))
-    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
+    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
-    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
+    >>> input = mx.nd.random_uniform(shape=(5, 3, 10))
-    >>> c0 = mx.nd.random.uniform(shape=(3, 3, 100))
+    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
-    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
+    >>> input = mx.nd.random_uniform(shape=(5, 3, 10))
-    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
+    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
-    def layerwise_pretrain(self, X, batch_size, n_iter, optimizer, l_rate, decay, lr_scheduler=None):
+    def layerwise_pretrain(self, X, batch_size, n_iter, optimizer, l_rate, decay, lr_scheduler=None, print_every=1000):
-        solver.set_monitor(Monitor(1000))
+        solver.set_monitor(Monitor(print_every))
-    def finetune(self, X, batch_size, n_iter, optimizer, l_rate, decay, lr_scheduler=None):
+    def finetune(self, X, batch_size, n_iter, optimizer, l_rate, decay, lr_scheduler=None, print_every=1000):
-        solver.set_monitor(Monitor(1000))
+        solver.set_monitor(Monitor(print_every))
-# pylint: skip-file
+import argparse
-    ae_model = AutoEncoderModel(mx.gpu(0), [784,500,500,2000,10], pt_dropout=0.2,
+    ae_model = AutoEncoderModel(mx.cpu(0), layers, pt_dropout=0.2,
-                   lr_scheduler=mx.misc.FactorScheduler(20000,0.1))
+    ae_model.layerwise_pretrain(train_X, batch_size, pretrain_num_iter, 'sgd', l_rate=0.1,
-from test_sparse_ndarray import test_create_csr, test_create_row_sparse
+from test_sparse_ndarray import test_create_csr, test_create_row_sparse, test_sparse_nd_slice
-                    assert same(ret.asnumpy(), ret_expected.asnumpy())
+                    assert_almost_equal(ret.asnumpy(), ret_expected.asnumpy())
-           'KLDivLoss', 'CTCLoss']
+           'KLDivLoss', 'CTCLoss', 'HuberLoss', 'HingeLoss',
-    return label.reshape(output.shape) if F is ndarray else label.reshape(())
+def _reshape_like(F, x, y):
-    """Calculates the mean squared error between output and label.
+    r"""Calculates the mean squared error between `pred` and `label`.
-        L = \\frac{1}{2}\\sum_i \\Vert {output}_i - {label}_i \\Vert^2.
+    .. math:: L = \frac{1}{2} \sum_i \vert {pred}_i - {label}_i \vert^2.
-    Output and label can have arbitrary shape as long as they have the same
+    `pred` and `label` can have arbitrary shape as long as they have the same
-        to weigh each sample in the batch separately, `sample_weight` should have shape (64, 1).
+    Inputs:
-        The loss output has the shape (batch_size,).
+    Outputs:
-        loss = F.square(output - label)
+    def hybrid_forward(self, F, pred, label, sample_weight=None):
-    """Calculates the mean absolute error between output and label.
+    r"""Calculates the mean absolute error between `pred` and `label`.
-        L = \\frac{1}{2}\\sum_i \\vert {output}_i - {label}_i \\vert.
+    .. math:: L = \frac{1}{2} \sum_i \vert {pred}_i - {label}_i \vert.
-    Output and label must have the same shape.
+    `pred` and `label` can have arbitrary shape as long as they have the same
-        to weigh each sample in the batch separately, `sample_weight` should have shape (64, 1).
+    Inputs:
-        The loss output has the shape (batch_size,).
+    Outputs:
-        loss = F.abs(output - label)
+    def hybrid_forward(self, F, pred, label, sample_weight=None):
-    BCE loss is useful when training logistic regression.
+    BCE loss is useful when training logistic regression. If `from_sigmoid`
-        loss(o, t) = - 1/n \sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
+        L = - \sum_i {label}_i * \log({pred}_i) +
-        log-sum-exp trick.
+        the loss calculate sigmoid and BCE together, which is more numerically
-        to weigh each sample in the batch separately, `sample_weight` should have shape (64, 1).
+    Inputs:
-        The loss output has the shape (batch_size,).
+    Outputs:
-        label = _reshape_label_as_output(F, output, label)
+    def hybrid_forward(self, F, pred, label, sample_weight=None):
-            loss = output - output*label + max_val + F.log(F.exp(-max_val)+F.exp(-output-max_val))
+            max_val = F.relu(-pred)
-            loss = -(F.log(output+1e-12)*label + F.log(1.-output+1e-12)*(1.-label))
+            loss = -(F.log(pred+1e-12)*label + F.log(1.-pred+1e-12)*(1.-label))
-    """Computes the softmax cross entropy loss. (alias: SoftmaxCELoss)
+    r"""Computes the softmax cross entropy loss. (alias: SoftmaxCELoss)
-    If `sparse_label` is `True`, label should contain integer category indicators:
+    If `sparse_label` is `True` (default), label should contain integer
-        L = -\\sum_i {log}(p_{i,{label}_i})
+        \DeclareMathOperator{softmax}{softmax}
-    `output.shape` = (1,2,3,4) and axis = 2, `label.shape` should be (1,2,4).
+        L = -\sum_i \log p_{i,{label}_i}
-    with the same shape as output:
+    `label`'s shape should be `pred`'s shape with the `axis` dimension removed.
-        L = -\\sum_i \\sum_j {label}_j {log}(p_{ij})
+        p = \softmax({pred})
-        The loss output has the shape (batch_size,).
+    Inputs:
-    def hybrid_forward(self, F, output, label, sample_weight=None):
+    def hybrid_forward(self, F, pred, label, sample_weight=None):
-            output = F.log_softmax(output)
+            pred = F.log_softmax(pred, self._axis)
-            loss = -F.pick(output, label, axis=self._axis, keepdims=True)
+            loss = -F.pick(pred, label, axis=self._axis, keepdims=True)
-            loss = -F.sum(output*label, axis=self._axis, keepdims=True)
+            label = _reshape_like(F, label, pred)
-    """The Kullback-Leibler divergence loss.
+    r"""The Kullback-Leibler divergence loss.
-    (discretely sampled) continuous output distributions.
+    KL divergence measures the distance between contiguous distributions. It
-    Label's shape should be the same as output's.
+        L = \sum_i {label}_i * \big[\log({label}_i) - {pred}_i\big]
-        to weigh each sample in the batch separately, `sample_weight` should have shape (64, 1).
+    Inputs:
-        The loss output has the shape (batch_size,).
+    Outputs:
-    def __init__(self, from_logits=True, weight=None, batch_axis=0, **kwargs):
+    def __init__(self, from_logits=True, axis=-1, weight=None, batch_axis=0,
-    def hybrid_forward(self, F, output, label, sample_weight=None):
+    def hybrid_forward(self, F, pred, label, sample_weight=None):
-        loss = label * (F.log(label+1e-12) - output)
+            pred = F.log_softmax(pred, self._axis)
-        Layout of the output sequence activation vector.
+        Layout of prediction tensor. 'N', 'T', 'C' stands for batch size,
-        Layout of the labels.
+        Layout of the labels. 'N', 'T' stands for batch size, and sequence
-        The CTC loss output has the shape (batch_size,).
+    Inputs:
-               "Only 'NTC' and 'TNC' layouts for output are supported. Got: %s"%layout
+               "Only 'NTC' and 'TNC' layouts for pred are supported. Got: %s"%layout
-                       data_lengths=None, label_lengths=None, sample_weight=None):
+    def hybrid_forward(self, F, pred, label,
-            data = F.swapaxes(data, 0, 1)
+            pred = F.swapaxes(pred, 0, 1)
-                                 use_data_lengths=data_lengths is not None,
+        loss = F.contrib.CTCLoss(pred, label, pred_lengths, label_lengths,
-                                 data_lengths=data_lengths, label_lengths=label_lengths,
+
-    out1 = mx.random.uniform(0, 1, shape=(N, 1))
+    out1 = mx.random.uniform(0.1, 0.9, shape=(N, 1))
-    Loss(label, label)
+def test_huber_loss():
-parser.add_argument('--gpus', type=int, default=0,
+parser.add_argument('--num-gpus', type=int, default=0,
-gpus = opt.gpus
+num_gpus = opt.num_gpus
-context = [mx.gpu(i) for i in range(gpus)] if gpus > 0 else [mx.cpu()]
+batch_size *= max(1, num_gpus)
-        mod = mx.mod.Module(softmax, context=[mx.gpu(i) for i in range(gpus)] if gpus > 0 else [mx.cpu()])
+        mod = mx.mod.Module(softmax, context=[mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()])
-                    help='what kvstore to use [local, dist_async, etc]')
+                    help='what kvstore to use',
-
+    logging.info(args)
-    model = linear_model(num_features)
+    # The positive class weight, says how much more we should upweight the importance of
-    mod.init_optimizer(optimizer=sgd, kvstore=kv)
+    optim = mx.optimizer.create(optimizer, learning_rate=0.01, rescale_grad=1.0/batch_size/num_worker)
-    metric = mx.metric.create('log_loss')
+    metric = mx.metric.create(['nll_loss'])
-            # for distributed training, we need to explicitly pull sparse weights from kvstore
+            # for distributed training, we need to manually pull sparse weights from kvstore
-                                   priority=-index, row_ids=[row_ids])
+                kv.row_sparse_pull('weight', weight, row_ids=[row_ids])
-            # update parameters
+            # update all parameters (including the weight parameter)
-                logging.info('epoch %d batch %d, train log loss = %s' % (epoch, nbatch, metric.get()[1]))
+            speedometer_param = mx.model.BatchEndParam(epoch=epoch, nbatch=nbatch,
-        logging.info('epoch %d, eval log loss = %s' % (epoch, score[0][1]))
+        score = mod.score(eval_data, ['nll_loss'])
-            self.classifier.add(nn.Flatten())
+def test_cross_device_autograd():
-                         "Please convert to number with asscalar() first.")
+        num_elements = reduce(operator.mul, self.shape, 1)
-import unittest 
+import unittest
-        data_iter.reset()
+        data_iter.reset()
-        batch_size = 128
+        batch_size = 33
-                 news_metadata['origin_name'])
+                     news_metadata['origin_name'])
-        assert(num_batches == int(expected_num_batches)), (num_batches, expected_num_batches)
+                                      batch_size=batch_size)
-    'feature_dim': 1000000,
+    'feature_dim': 1000001,
-    'feature_dim': 20216830,
+    'feature_dim': 20216831,
-def get_libsvm_data(data_dir, data_name, url, data_origin_name):
+def get_libsvm_data(data_dir, data_name, url):
-        os.system("bzip2 -d %r" % data_origin_name)
+        zippath = os.path.join(data_dir, data_name + ".bz2")
-parser.add_argument('--num-epoch', type=int, default=1,
+parser.add_argument('--num-epoch', type=int, default=5,
-    'feature_dim': 1000000,
+parser.add_argument('--kvstore', type=str, default=None,
-     return model
+def linear_model(num_features):
-    logging.basicConfig(level=log_level, format=head)
+    kv = mx.kvstore.create(kvstore) if kvstore else None
-        logging.debug('preparing data ... ')
+    num_features = AVAZU['num_features']
-        assert os.path.exists(path)
+    train_data = os.path.join(data_dir, AVAZU['train'])
-    train_data = mx.io.LibSVMIter(data_libsvm=path, data_shape=(feature_dim,),
+    train_data = mx.io.LibSVMIter(data_libsvm=train_data, data_shape=(num_features,),
-        train_data = DummyIter(train_data)
+    eval_data = mx.io.LibSVMIter(data_libsvm=val_data, data_shape=(num_features,),
-    model = linear_model(feature_dim)
+    model = linear_model(num_features)
-    mod.init_params(initializer=mx.init.Uniform(scale=.1))
+    mod.init_params()
-                           learning_rate=0.1, rescale_grad=1.0/batch_size/num_worker)
+                           learning_rate=0.001, rescale_grad=1.0/batch_size/num_worker)
-        mx.profiler.profiler_set_state('run')
+    metric = mx.metric.create('log_loss')
-    start = time.time()
+    logging.info('Training started ...')
-                               priority=-index, row_ids=[row_ids])
+            # for distributed training, we need to explicitly pull sparse weights from kvstore
-            # accumulate prediction accuracy
+            # update training metric
-    logging.info('num_worker = ' + str(num_worker) + ', time cost = ' + str(time_cost))
+            if nbatch % 100 == 0:
-        and `pull` calls for the same input key(s) are finished.
+        for the same input key(s) are finished.
-    """Scope for collecting child `Block`s."""
+    """Scope for collecting child `Block` s."""
-    assign child `Block` as regular attributes::
+    :py:class:`Block` can be nested recursively in a tree structure. You can create and
-    Child `Block` assigned this way will be registered and `collect_params`
+    Child :py:class:`Block` assigned this way will be registered and :py:meth:`collect_params`
-        should be unique within one model to prevent name collisions.
+        Parameters and child :py:class:`Block` s in this :py:class:`Block` 's
-        if you want `dense1` to share `dense0`'s weights, you can do::
+        :py:class:`ParameterDict` for sharing weights with the new :py:class:`Block`. For example,
-        """Prefix of this `Block`."""
+        """Prefix of this :py:class:`Block`."""
-        """Name of this `Block`, without '_' in the end."""
+        """Name of this :py:class:`Block`, without '_' in the end."""
-        names. Should be used within a `with` statement::
+        """Returns a name space object managing a child :py:class:`Block` and parameter
-        """Returns this `Block`'s parameter dictionary (does not include its
+        """Returns this :py:class:`Block`'s parameter dictionary (does not include its
-        """Returns a `ParameterDict` containing this `Block` and all of its
+        """Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its
-        """Registers block as a child of self. `Block`s assigned to self as
+        """Registers block as a child of self. :py:class:`Block` s assigned to self as
-        """Initializes `Parameter`s of this `Block` and its children.
+        """Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.
-        Equivalent to `block.collect_params().initialize(...)`
+        Equivalent to ``block.collect_params().initialize(...)``
-        """Activates or deactivates `HybridBlock`s recursively. Has no effect on
+        """Activates or deactivates :py:class:`HybridBlock` s recursively. Has no effect on
-        """Overrides to implement forward computation using `NDArray`. Only
+        """Overrides to implement forward computation using :py:class:`NDArray`. Only
-    i.e. you cannot call `.asnumpy()`, `.shape`, `.dtype`, etc on tensors.
+    Forward computation in :py:class:`HybridBlock` must be static to work with :py:class:`Symbol` s,
-    `Block`. After activation, `HybridBlock` will create a symbolic graph
+    Before activating with :py:meth:`hybridize()`, :py:class:`HybridBlock` works just like normal
-    the cached graph will be used instead of `hybrid_forward`.
+    the cached graph will be used instead of :py:meth:`hybrid_forward`.
-        `NDArray` or `Symbol`."""
+        :py:class:`NDArray` or :py:class:`Symbol`."""
-        h_t = o_t \circ tanh(c_t)
+        \begin{array}{ll}
-        h_t = o_t \circ tanh(c_t)
+        \begin{array}{ll}
-        h_t = o_t \circ tanh(c_t)
+        \begin{array}{ll}
-        h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h
+        \begin{array}{ll}
-        h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h
+        \begin{array}{ll}
-        h^\prime_t = (1 - z_t) \circ n_t + z_t \circ h
+        \begin{array}{ll}
-    """MNIST handwritten digits dataset from `http://yann.lecun.com/exdb/mnist`_.
+    """MNIST handwritten digits dataset from http://yann.lecun.com/exdb/mnist
-        A user defined callback that transforms each instance. For example::
+        A user defined callback that transforms each sample. For example:
-    `https://github.com/zalandoresearch/fashion-mnist`_.
+    https://github.com/zalandoresearch/fashion-mnist
-        A user defined callback that transforms each instance. For example::
+        A user defined callback that transforms each sample. For example:
-    """CIFAR10 image classification dataset from `https://www.cs.toronto.edu/~kriz/cifar.html`_.
+    """CIFAR10 image classification dataset from https://www.cs.toronto.edu/~kriz/cifar.html
-        A user defined callback that transforms each instance. For example::
+        A user defined callback that transforms each sample. For example:
-    """CIFAR100 image classification dataset from `https://www.cs.toronto.edu/~kriz/cifar.html`_.
+    """CIFAR100 image classification dataset from https://www.cs.toronto.edu/~kriz/cifar.html
-        A user defined callback that transforms each instance. For example::
+        A user defined callback that transforms each sample. For example:
-        A user defined callback that transforms each instance. For example::
+        A user defined callback that transforms each sample. For example:
-        A function that takes data and label and transforms them::
+        A function that takes data and label and transforms them:
-            transform = lambda data, label: (data.astype(np.float32)/255, label)
+        transform = lambda data, label: (data.astype(np.float32)/255, label)
-    """Calculates the mean squared error between output and label:
+    """Calculates the mean squared error between output and label.
-        in the batch, `sample_weight` should have shape (64, 1).
+
-    """Calculates the mean absolute error between output and label:
+    """Calculates the mean absolute error between output and label.
-        in the batch, `sample_weight` should have shape (64, 1).
+
-        in the batch, `sample_weight` should have shape (64, 1).
+
-        in the batch, `sample_weight` should have shape (64, 1).
+
-        in the batch, `sample_weight` should have shape (64, 1).
+
-    Input shapes:
+
-    densenet = models.densenet_161()
+    from mxnet.gluon.model_zoo import vision
-    alexnet = models.alexnet(pretrained=True)
+    from mxnet.gluon.model_zoo import vision
-
+__all__ = ['Sequential', 'HybridSequential', 'Dense', 'Activation',
-    """Stacks `Block`s sequentially.
+    """Stacks Blocks sequentially.
-    """Stacks `HybridBlock`s sequentially.
+    """Stacks HybridBlocks sequentially.
-    """Leaky version of a Rectified Linear Unit.
+    r"""Leaky version of a Rectified Linear Unit.
-    It allows a small gradient when the unit is not active::
+    .. math::
-        `f(x) = x for x >= 0`.
+        f\left(x\right) = \left\{
-    """A Container holding parameters (weights) of `Block`s.
+    """A Container holding parameters (weights) of Blocks.
-    not `null`, it will also hold a gradient array on each `Context`::
+    :py:class:`Parameter` holds a copy of the parameter on each :py:class:`Context` after
-          to manually call `zero_grad()` to clear the gradient buffer before each
+        - ``'write'`` means everytime gradient is written to grad :py:class:`NDArray`.
-        when using `NDArray` API.
+        unknown shape can be used for :py:class:`Symbol` API, but ``init`` will throw an error
-        Data type of this parameter. For example, numpy.float32 or 'float32'.
+        Data type of this parameter. For example, ``numpy.float32`` or ``'float32'``.
-        with `x.grad_req = 'null'` saves memory and computation when you don't
+        This can be set before or after initialization. Setting ``grad_req`` to ``'null'``
-        `param.lr_mult = 2.0`
+        is calculated with ``learning_rate * lr_mult``. You can set it with
-        """Initializes parameter and gradient arrays. Only used for `NDArray` API.
+        """Initializes parameter and gradient arrays. Only used for :py:class:`NDArray` API.
-        ctx : Context or list of Context, defaults to `context.current_context()`.
+            The initializer to use. Overrides :py:meth:`Parameter.init` and default_init.
-            their values consistent when updating. Normally `gluon.Trainer` does this for you.
+            .. note::
-            Default initializer is used when both `init` and `Parameter.init` are `None`.
+            Default initializer is used when both :py:func:`init`
-        ctx : Context or list of Context, default `context.current_context()`.
+        ctx : Context or list of Context, default ``context.current_context()``.
-        as `values`."""
+        as :py:meth:`values`."""
-    prefix : str, default ''
+    prefix : str, default ``''``
-        parameters with another `Block`.
+        If not ``None``, when this dict's :py:meth:`get` method creates a new parameter, will
-        with `get`."""
+        """Prefix of this dict. It will be prepended to :py:class:`Parameter`s' name created
-        found, `get` will create a new `Parameter` with key-word arguments and
+        """Retrieves a :py:class:`Parameter` with name ``self.prefix+name``. If not found,
-            The rest of key-word arguments for the created `Parameter`.
+            The rest of key-word arguments for the created :py:class:`Parameter`.
-            The created or retrieved `Parameter`.
+            The created or retrieved :py:class:`Parameter`.
-        """Copies all Parameters in `other` to self."""
+        """Copies all Parameters in ``other`` to self."""
-        API. It has no effect when using `Symbol` API.
+        """Initializes all Parameters managed by this dictionary to be used for :py:class:`NDArray`
-            Otherwise, `Parameter.init` takes precedence.
+            Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.
-        ctx : Context or list of Context, default `context.current_context()`.
+        ctx : Context or list of Context, default :py:meth:`context.current_context()`.
-                    "Blocks or you forgot to use `with name_scope()`` during init. " \
+                    "Blocks or you forgot to use ``with name_scope()`` during init. " \
-    """Simple recurrent neural network cell.
+    r"""Elman RNN recurrent neural network cell.
-    """Long-Short Term Memory (LSTM) network cell.
+    r"""Long-Short Term Memory (LSTM) network cell.
-    """Gated Rectified Unit (GRU) network cell.
+    r"""Gated Rectified Unit (GRU) network cell.
-from ..nn import Block
+from .. import Block
-from . import _internal, contrib, linalg, sparse, random
+from . import _internal, contrib, linalg, sparse, random, utils
-__all__ = op.__all__ + ndarray.__all__ + ['contrib', 'linalg', 'random', 'sparse']
+__all__ = op.__all__ + ndarray.__all__ + utils.__all__ + ['contrib', 'linalg', 'random', 'sparse']
-    stypes = ['default']  #,'row_sparse', 'csr']
+    stypes = ['default', 'row_sparse', 'csr']
-    stypes = ['default']  #,'row_sparse', 'csr']
+    stypes = ['default', 'row_sparse', 'csr']
-            y.backward()
+            y.backward(out_grad=mx.nd.ones_like(y).tostype(x.stype))
-    stypes = ['default']  #, 'row_sparse', 'csr']
+    stypes = ['default', 'row_sparse', 'csr']
-        x = mx.nd.zeros((1,), stype=array_stype)
+        x = mx.nd.zeros((1, 1), stype=array_stype)
-    stypes = ['default']  #, 'csr', 'row_sparse']
+    stypes = ['default', 'csr', 'row_sparse']
-
+import unittest
-    num_repeats = 10
+    def check_sparse_nd_elemwise_binary(shapes, stypes, f, g):
-                       shape, ['row_sparse', 'row_sparse'], op, g)
+        check_sparse_nd_elemwise_binary(shape, ['default'] * 2, op, g)
-    N = 10
+    N = 3
-    N = 10
+    N = 3
-    N = 10
+    N = 3
-    assert(np.sum(out.asnumpy()) != 0)
+def test_sparse_nd_storage_fallback():
-        return output
+        return check_function(l, r)
-
+def do_normalize(arr):
-                                  lhs_grad_stype='row_sparse', rhs_grad_stype='row_sparse')
+    def check_elemwise_add_ex(lhs_stype, rhs_stype, shape, lhs_grad_stype=None, rhs_grad_stype=None):
-    density = [1.00, 0.50, 0.05, 0.01]
+    density = [1.00, 0.50, 0.01]
-    density = [1.00, 0.50, 0.10, 0.05, 0.01]
+    density = [1.00, 0.50, 0.01]
-    densities = [0.01, 0.1, 0.2, 0.5, 0.8, 1.0]
+    densities = [0.01, 0.5, 1.0]
-        grad_stypes.append(expected_grad_result_type)
+        grad_stypes = {'data' : expected_grad_result_type}
-                          lambda input, outg: outg * assign_each(input, lambda x: x > 0.0))
+                          lambda output, outg: outg * assign_each(output, lambda x: x > 0.0), backward_is_use_output=True)
-                                rtol=1e-3, atol=1e-4)
+    def check_broadcast_add(shape, lhs_stype, rhs_stype):
-            check_numeric_gradient(test, location)
+        out_dns = (lhs_dns + rhs_dns).asnumpy()
-                check_softmax_with_shape(rhs, rhs, shape, preserve_shape=True)
+    shape = rand_shape_2d()
-            check_sparse_elementwise_sum_with_shape('row_sparse', shape, np.random.randint(1, 9))
+    for dim in range(2, 4):
-                    image = aug(image)[0]
+                    image = aug(image)
-                    target = aug(target)[0]
+                    target = aug(target)
-    net.conv4.collect_params().initialize(mx.init.Orthogonal(scale=1), ctx=ctx)
+    net.conv4.collect_params().initialize(mx.init.Orthogonal(scale=1), force_reinit=True, ctx=ctx)
-            if isinstance(key[0], (NDArray, np.ndarray, list)):
+            if isinstance(key[0], (NDArray, np.ndarray, list, tuple)):
-def get_model_file(name, root=os.path.expanduser('~/.mxnet/models/')):
+def get_model_file(name, root='~/.mxnet/models/'):
-def purge(root=os.path.expanduser('~/.mxnet/models/')):
+def purge(root='~/.mxnet/models/'):
-def test_simple_bind_special_case():
+def test_simple_bind_incomplete_shape_inference_in_one_forward_pass():
-                                   |--> add_op
+          \                        |--> add_op
-from get_data import mnist_iterator
+from get_data import MNISTIterator
-train, val = mnist_iterator(batch_size=batch_size, input_shape = (784,))
+train, val = MNISTIterator(batch_size=batch_size, input_shape = (784,))
-from get_data import mnist_iterator
+from get_data import MNISTIterator
-train, val = mnist_iterator(batch_size=100, input_shape = (784,))
+train, val = MNISTIterator(batch_size=100, input_shape = (784,))
-from get_data import mnist_iterator
+from get_data import MNISTIterator
-train, val = mnist_iterator(batch_size=100, input_shape = (784,))
+train, val = MNISTIterator(batch_size=100, input_shape = (784,))
-from get_data import mnist_iterator
+from get_data import MNISTIterator
-train, val = mnist_iterator(batch_size=100, input_shape = (784,))
+train, val = MNISTIterator(batch_size=100, input_shape = (784,))
-from data import mnist_iterator
+import sys
-train, val = mnist_iterator(batch_size=100, input_shape = (784,))
+train, val = MNISTIterator(batch_size=100, input_shape = (784,))
-from data import mnist_iterator
+import sys
-train, val = mnist_iterator(batch_size=100, input_shape = (784,))
+train, val = MNISTIterator(batch_size=100, input_shape = (784,))
-def mnist_iterator(batch_size, input_shape):
+def MNISTIterator(batch_size, input_shape):
-        indices = range(self.num_images)
+        indices = list(range(self.num_images))
-                                   label=self._label.values(),
+            data_batch = mx.io.DataBatch(data=list(self._data.values()),
-    print("Saved model: {}-{:04d}.param".format(save_prefix, args.epoch))
+    print("Saved model: {}-{:04d}.params".format(save_prefix, args.epoch))
-    """A sparse representation of 2D NDArray in the standard CSR format.
+    """A sparse representation of 2D NDArray in the Compressed Sparse Row format.
-    in values[indptr[i]:indptr[i+1]].
+    row i are stored in ``indices[indptr[i]:indptr[i+1]]`` and their corresponding values are stored
-    - indices: a 1-D int64 NDArray with shape [D0].
+    - indices: a 1-D int64 NDArray with shape [D0] with values sorted in ascending order.
-    A RowSparseNDArray is typically used to represent non-zero row-slices of a large NDArray
+    A RowSparseNDArray is typically used to represent non-zero row slices of a large NDArray
-                in 32-bit precision even if actual weights used in the model have lower precision.
+       ``True`` makes internal 32-bit copy of the weights and applies gradients \
-    following way: row_index = (random_number_generated / num_rows)
+    following way:
-                                     (num_rows, num_cols), dtype=dtype)
+    try:
-
+
-    density = rnd.rand() if density is None else density
+
-from mxnet.base import py_str
+from mxnet.test_utils import rand_ndarray, assert_almost_equal, assert_exception
-        assert_exception(kv.row_sparse_pull, key, out=dns_val, row_ids=mx.nd.array([1]))
+        assert_exception(kv.row_sparse_pull, MXNetError,
-        assert_exception(kv.row_sparse_pull, key, out=dns_val,
+        assert_exception(kv.row_sparse_pull, MXNetError, key, out=dns_val,
-        assert_exception(kv.row_sparse_pull, key, rsp_val,
+        assert_exception(kv.init, MXNetError, key, dns_val)
-        assert_exception(kv.row_sparse_pull, key, rsp_val,
+        assert_exception(kv.init, MXNetError, key, dns_val)
-    # initialize parameters by uniform random numbers
+    # initialize parameters by random numbers
-    # use Sparse SGD with learning rate 0.1 to train
+    # use sparse Adam with learning rate 0.1 to train
-    # use accuracy as the metric
+    # use MSE as the metric
-    # train 10 epoch
+    # train 10 epochs
-        return
+    assert_exception(mx.nd.sparse.retain, mx.base.MXNetError,
-
+@unittest.skip("test fails intermittently. temporarily disabled till it gets fixed. tracked at https://github.com/apache/incubator-mxnet/issues/8049")
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha=4., beta=7.)
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha=4., beta=7.)
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha=4., beta=7.,
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha=4., beta=7.,
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha=4., beta=7.,
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha=4., beta=7.,
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha=4., beta=7.,
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha=4., beta=7.,
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha=4., beta=7.)
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha=4., beta=7.)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha=4.)
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha=4.)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha=4., transpose_a=True,
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha=4., transpose_a=True,
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha=4., transpose_a=True)
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha=4., transpose_a=True)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha=4., transpose_b=True)
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha=4., transpose_b=True)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha=4.)
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha=4.)
-    test_potrf = mx.sym.linalg_potrf(data1)
+    test_potrf = mx.sym.linalg.potrf(data1)
-    test_potri = mx.sym.linalg_potri(data1)
+    test_potri = mx.sym.linalg.potri(data1)
-    test_trsm = mx.sym.linalg_trsm(data1, data2, alpha=7.)
+    test_trsm = mx.sym.linalg.trsm(data1, data2, alpha=7.)
-    test_trmm = mx.sym.linalg_trmm(data1, data2, alpha=7., transpose=True,
+    test_trmm = mx.sym.linalg.trmm(data1, data2, alpha=7., transpose=True,
-    test_sumlogdiag = mx.sym.linalg_sumlogdiag(data1)
+    test_sumlogdiag = mx.sym.linalg.sumlogdiag(data1)
-    test_potrf = mx.sym.linalg_potrf(_make_symm_symbol(data1, ndims=4))
+    test_potrf = mx.sym.linalg.potrf(_make_symm_symbol(data1, ndims=4))
-    test_potri = mx.sym.linalg_potri(data1_ltri)
+    test_potri = mx.sym.linalg.potri(data1_ltri)
-    test_trsm = mx.sym.linalg_trsm(data1_ltri, data2, alpha=7.)
+    test_trsm = mx.sym.linalg.trsm(data1_ltri, data2, alpha=7.)
-    test_trsm2 = mx.sym.linalg_trsm(
+    test_trsm2 = mx.sym.linalg.trsm(
-    test_trsm3 = mx.sym.linalg_trsm(
+    test_trsm3 = mx.sym.linalg.trsm(
-    test_trsm4 = mx.sym.linalg_trsm(
+    test_trsm4 = mx.sym.linalg.trsm(
-    test_trmm = mx.sym.linalg_trmm(
+    test_trmm = mx.sym.linalg.trmm(
-    test_trmm2 = mx.sym.linalg_trmm(data1_ltri, data2, alpha=-2.)
+    test_trmm2 = mx.sym.linalg.trmm(data1_ltri, data2, alpha=-2.)
-    test_trmm3 = mx.sym.linalg_trmm(data1_ltri, data2, rightside=True)
+    test_trmm3 = mx.sym.linalg.trmm(data1_ltri, data2, rightside=True)
-    test_trmm4 = mx.sym.linalg_trmm(
+    test_trmm4 = mx.sym.linalg.trmm(
-# Tests for new operators linalg_syrk, linalg_gelqf
+# Tests for operators linalg.syrk, linalg.gelqf
-    l_q = mx.sym.linalg_trmm(l, q, alpha=1., name='L_times_Q')
+    q, l = mx.sym.linalg.gelqf(a)
-    q, l = mx.sym.linalg_gelqf(a)
+    q, l = mx.sym.linalg.gelqf(a)
-    q, l = mx.sym.linalg_gelqf(a)
+    q, l = mx.sym.linalg.gelqf(a)
-    # Tests for linalg_syrk
+    # Tests for linalg.syrk
-        #print('m={}, n={}, alpha={}'.format(m, n, alpha))
+        #print('syrk: m={}, n={}, alpha={}'.format(m, n, alpha))
-        test_syrk1 = mx.sym.linalg_syrk(data1, transpose=False, alpha=alpha)
+        test_syrk1 = mx.sym.linalg.syrk(data1, transpose=False, alpha=alpha)
-        test_syrk2 = mx.sym.linalg_syrk(data1, transpose=True, alpha=alpha)
+        test_syrk2 = mx.sym.linalg.syrk(data1, transpose=True, alpha=alpha)
-    # Tests for linalg_gelqf
+    # Tests for linalg.gelqf
-    if default_context() != mx.cpu():
+    if not (default_context() == mx.cpu()):
- 
+
-        #print('m={}, n={}'.format(m, n))
+        #print('gelqf: m={}, n={}'.format(m, n))
-            outputs = [outputs]
+        if isinstance(outputs, (list, tuple)) and len(outputs) == 1:
-        return _regroup(ret, self._out_format)[0]
+        return _regroup(list(ret), self._out_format)[0]
-    assert len(out.get_internals().list_outputs()) == len(outputs.list_outputs())
+    assert len(out) == len(outputs.list_outputs())
-            pred_label = pred_label.astype('int32').as_in_context(label.context)
+            pred_label = pred_label.asnumpy().astype('int32')
-            self.num_inst += label.size
+            self.sum_metric += (pred_label.flat == label.flat).sum()
-    return (train_dataiter, val_dataiter)
+import os
-from data import mnist_iterator
+curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))
-        super(Multi_Accuracy, self).__init__('multi-accuracy', num)
+        self.num = num
-            if i is None:
+            if self.num is None:
-from data import mnist_iterator
+curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))
-from data import mnist_iterator
+import os
-from data import mnist_iterator
+import sys
-        os.system("mkdir data/")
+    if not os.path.isdir("data"):
-        os.system("wget -q http://deeplearning.net/data/mnist/mnist.pkl.gz -P data/")
+        download('http://deeplearning.net/data/mnist/mnist.pkl.gz',
-        os.system("mkdir data/")
+    if not os.path.isdir("data"):
-        os.chdir("..")
+        zip_file_path = download('http://data.mxnet.io/mxnet/data/mnist.zip',
-        os.system("mkdir data/")
+    if not os.path.isdir("data"):
-        os.chdir("..")
+        zip_file_path = download('http://data.mxnet.io/mxnet/data/cifar10.zip',
-        prefix = args.prefix + args.network + '_' + str(args.data_shape[0])
+        prefix = args.prefix + args.network + '_' + str(data_shape[0])
-_OP_NAME_PREFIX_LIST = ['_contrib_', '_linalg_', '_random_', '_sparse_']
+_OP_NAME_PREFIX_LIST = ['_contrib_', '_linalg_', '_sparse_']
-from . import _internal, contrib, linalg, random, sparse
+from . import _internal, contrib, linalg, sparse, random
-__all__ = []
+
-from .ndarray._internal import _sample_gennegbinomial as generalized_negative_binomial
+from .ndarray.random import *
-from . import _internal, contrib, linalg, random, sparse
+from . import _internal, contrib, linalg, sparse, random
-__all__ = []
+"""Random distribution generator Symbol API of MXNet."""
-            'ndop': mx.random.normal,
+            'ndop': mx.nd.random.normal,
-            'ndop': mx.random.uniform,
+            'ndop': mx.nd.random.uniform,
-                'ndop': mx.random.gamma,
+                'ndop': mx.nd.random.gamma,
-                'inputs': [ ('lam', [ [ 1.0, 8.5 ], [ 2.7 , 0.5 ] ]) ],
+                'ndop': mx.nd.random.exponential,
-                    ('std', lambda x, params: np.std(x.astype(np.float64)) - 1.0 / params['lam'], tol)
+                    ('mean', lambda x, params: np.mean(x.astype(np.float64)) - params['scale'], tol),
-                'multisymbol': mx.sym.sample_poisson,
+                'ndop': mx.nd.random.poisson,
-                'ndop': mx.random.negative_binomial,
+                'ndop': mx.nd.random.negative_binomial,
-                'inputs': [ ('k', [ [ 20, 49 ], [ 15 , 16 ] ]) , ('p', [ [ 0.4 , 0.77 ], [ 0.5, 0.84 ] ]) ],
+                'inputs': [ ('k', [ [ 3, 4 ], [ 5 , 6 ] ]) , ('p', [ [ 0.4 , 0.77 ], [ 0.5, 0.84 ] ]) ],
-                'ndop': mx.random.generalized_negative_binomial,
+                'ndop': mx.nd.random.generalized_negative_binomial,
-            symbol = symbdic['multisymbol']
+            symbol = symbdic['symbol']
-        y, prob = mx.nd.sample_multinomial(x, shape=1000, get_prob=True)
+        y, prob = mx.nd.random.multinomial(x, shape=1000, get_prob=True)
-    test_sample_multinomial()
+    import nose
-from .rtc import Rtc as rtc
+from . import rtc
-from .base import _LIB, NDArrayHandle, RtcHandle, mx_uint, c_array, check_call
+import numpy as np
-        }
+    source : str
-                                    ctypes.byref(self.handle)))
+    def __init__(self, source, options=(), exports=()):
-        check_call(_LIB.MXRtcFree(self.handle))
+        check_call(_LIB.MXRtcCudaModuleFree(self.handle))
-        """Run the kernel.
+    def get_kernel(self, name, signature):
-            Block dimension for kernel launch.
+        name : str
-                                  mx_uint(block_dims[2])))
+        assert ctx.device_type == 'gpu', "Cuda kernel can only be launched on GPU"
-
+        begin : int
-                outputs_slice.append(_slice_axis(output, oaxis, oslice))
+            for label, axis in zip(labels, self.label_layouts):
-            preds = OrderedDict(zip(self.output_names, outputs_slice))
+            preds = OrderedDict(zip(self.output_names, texec.outputs))
-        mx.test_utils.assert_almost_equal(np_dx, inputs.grad.asnumpy(), rtol=1e-3)
+        mx.test_utils.assert_almost_equal(np_out, out.asnumpy(), rtol=1e-3, atol=1e-5)
-    mx.test_utils.assert_almost_equal(np_dx, inputs.grad.asnumpy(), rtol=1e-3)
+    mx.test_utils.assert_almost_equal(np_out, out.asnumpy(), rtol=1e-3, atol=1e-5)
-from mxnet.test_utils import rand_ndarray, set_default_context, assert_almost_equal
+from mxnet.test_utils import rand_ndarray, set_default_context, assert_almost_equal, get_bz2_data
-from util import get_data, estimate_density
+from util import estimate_density
-        get_data(
+        get_bz2_data(
-from mxnet.test_utils import *
+from mxnet.test_utils import *
-
+parser.add_argument('--omit-row-sparse-push', action='store_true',
-     return model
+    inputs = mx.symbol.Variable("data", stype='csr')
-        get_libsvm_data(data_dir, metadata['data_name'], metadata['url'],
+        get_bz2_data(data_dir, metadata['data_name'], metadata['url'],
-                else:
+                elif not omit_row_sparse_push:
-
+import bz2
-        get_data(data_dir, news_metadata['name'], news_metadata['url'],
+        get_bz2_data(data_dir, news_metadata['name'], news_metadata['url'],
-        L = \\frac{1}{2}\\sum_i \\vert {output}_i - {label}_i \\vert^2.
+        L = \\frac{1}{2}\\sum_i \\Vert {output}_i - {label}_i \\Vert^2.
-        loss(o, t) = - 1/n \\sum_i (t[i] * \\log(o[i]) + (1 - t[i]) * \\log(1 - o[i]))
+        loss(o, t) = - 1/n \sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
-        return imdecode(s)
+        def locate():
-        densities = [0, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1.0]
+        arr_grad = [mx.nd.empty(shape, stype=stype) for _ in range(n)]
-                 i2h_kernel=(3, 3), h2h_kernel=(3, 3),
+                 i2h_kernel, h2h_kernel,
-                    zip(labels, self.label_layouts, texec.outputs, self.output_layouts):
+            for label, laxis in zip(labels, self.label_layouts):
-    def __init__(self, root, train, transform):
+    def __init__(self, repo_dir, root, train, transform):
-    root : str
+    root : str, default '~/.mxnet/datasets/mnist'
-    train : bool
+    train : bool, default True
-    transform : function
+    transform : function, default None
-        super(MNIST, self).__init__(root, train, transform)
+        super(MNIST, self).__init__('mnist', root, train, transform)
-        data_file = download(self._base_url + data[0], self._root,
+        data_file = download(self._get_url(data[0]),
-        label_file = download(self._base_url + label[0], self._root,
+        label_file = download(self._get_url(label[0]),
-    root : str
+    root : str, default '~/.mxnet/datasets/fashion-mnist'
-    train : bool
+    train : bool, default True
-    transform : function
+    transform : function, default None
-        super(MNIST, self).__init__(root, train, transform) # pylint: disable=bad-super-call
+        super(MNIST, self).__init__('fashion-mnist', root, train, transform) # pylint: disable=bad-super-call
-    root : str
+    root : str, default '~/.mxnet/datasets/cifar10'
-    train : bool
+    train : bool, default True
-    transform : function
+    transform : function, default None
-        super(CIFAR10, self).__init__(root, train, transform)
+        self._archive_file = ('cifar-10-binary.tar.gz', 'fab780a1e191a7eda0f345501ccd62d20f7ed891')
-                                sha1_hash='e8aa088b9774a44ad217101d2e2569f823d2d491')
+        if any(not os.path.exists(path) or not check_sha1(path, sha1)
-            label = np.concatenate(label)
+            data_files = self._train_data
-            data, label = self._read_batch(filename)
+            data_files = self._test_data
-    transform : function
+    transform : function, default None
-    transform : callable
+    transform : callable, default None
-              '/gluon/models/{file_name}.zip'
+apache_repo_url = 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/'
-def get_model_file(name, local_dir=os.path.expanduser('~/.mxnet/models/')):
+def get_model_file(name, root=os.path.expanduser('~/.mxnet/models/')):
-    The local_dir directory will be created if it doesn't exist.
+    The root directory will be created if it doesn't exist.
-    local_dir : str, default '~/.mxnet/models'
+    root : str, default '~/.mxnet/models'
-    file_path = os.path.join(local_dir, file_name+'.params')
+    file_path = os.path.join(root, file_name+'.params')
-        os.makedirs(local_dir)
+    if not os.path.exists(root):
-    download(_url_format.format(file_name=file_name),
+    zip_file_path = os.path.join(root, file_name+'.zip')
-        zf.extractall(local_dir)
+        zf.extractall(root)
-def purge(local_dir=os.path.expanduser('~/.mxnet/models/')):
+def purge(root=os.path.expanduser('~/.mxnet/models/')):
-    local_dir : str, default '~/.mxnet/models'
+    root : str, default '~/.mxnet/models'
-    files = os.listdir(local_dir)
+    files = os.listdir(root)
-            os.remove(os.path.join(local_dir, f))
+            os.remove(os.path.join(root, f))
-def alexnet(pretrained=False, ctx=cpu(), **kwargs):
+def alexnet(pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
-        net.load_params(get_model_file('alexnet'), ctx=ctx)
+        net.load_params(get_model_file('alexnet', root=root), ctx=ctx)
-def get_densenet(num_layers, pretrained=False, ctx=cpu(), **kwargs):
+def get_densenet(num_layers, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
-        net.load_params(get_model_file('densenet%d'%(num_layers)), ctx=ctx)
+        net.load_params(get_model_file('densenet%d'%(num_layers), root=root), ctx=ctx)
-def inception_v3(pretrained=False, ctx=cpu(), **kwargs):
+def inception_v3(pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
-        net.load_params(get_model_file('inceptionv3'), ctx=ctx)
+        net.load_params(get_model_file('inceptionv3', root=root), ctx=ctx)
-def get_mobilenet(multiplier, pretrained=False, ctx=cpu(), **kwargs):
+def get_mobilenet(multiplier, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
-        net.load_params(get_model_file('mobilenet%s'%version_suffix), ctx=ctx)
+        net.load_params(get_model_file('mobilenet%s'%version_suffix, root=root), ctx=ctx)
-def get_resnet(version, num_layers, pretrained=False, ctx=cpu(), **kwargs):
+def get_resnet(version, num_layers, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
-        net.load_params(get_model_file('resnet%d_v%d'%(num_layers, version)), ctx=ctx)
+        net.load_params(get_model_file('resnet%d_v%d'%(num_layers, version),
-def get_squeezenet(version, pretrained=False, ctx=cpu(), **kwargs):
+def get_squeezenet(version, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
-        net.load_params(get_model_file('squeezenet%s'%version), ctx=ctx)
+        net.load_params(get_model_file('squeezenet%s'%version, root=root), ctx=ctx)
-def get_vgg(num_layers, pretrained=False, ctx=cpu(), **kwargs):
+def get_vgg(num_layers, pretrained=False, ctx=cpu(), root='~/.mxnet/models', **kwargs):
-        net.load_params(get_model_file('vgg%d%s'%(num_layers, batch_norm_suffix)), ctx=ctx)
+        net.load_params(get_model_file('vgg%d%s'%(num_layers, batch_norm_suffix),
-    assert len(gluon.data.vision.CIFAR10(root='data', train=False)) == 10000
+    assert len(gluon.data.vision.MNIST(root='data/mnist')) == 60000
-        model = get_model(model_name, pretrained=test_pretrain)
+        model = get_model(model_name, pretrained=test_pretrain, root='model/')
-from __future__ import print_function
+from ..nn import LeakyReLU
-                      raise_on_err=True, ground_truth=None):
+                      raise_on_err=True, ground_truth=None, equal_nan=False):
-                assert_almost_equal(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
+                assert_almost_equal(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]],
-                    assert_almost_equal(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
+                    assert_almost_equal(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]],
-    def check_fluent_regular(func, kwargs, shape=(5, 17, 1)):
+                    'clip', 'abs', 'sign', 'sin', 'cos', 'tan', 'arcsin', 'arccos', 'arctan',
-                    assert almost_equal(r.asnumpy(), f.asnumpy())
+                    assert almost_equal(r.asnumpy(), f.asnumpy(), equal_nan=equal_nan)
-                assert almost_equal(regular.asnumpy(), fluent.asnumpy())
+                assert almost_equal(regular.asnumpy(), fluent.asnumpy(), equal_nan=equal_nan)
-                 'ones_like', 'abs', 'sign']:
+                 'ones_like', 'abs', 'sign', 'sin', 'cos', 'degrees', 'radians',
-    def check_fluent_regular(func, kwargs, shape=(5, 17, 1)):
+                    'clip', 'abs', 'sign', 'sin', 'cos', 'tan', 'arcsin', 'arccos', 'arctan',
-                                     skip_grad=func not in has_grad)
+                                     skip_grad=func not in has_grad,
-                 'ones_like', 'abs', 'sign']:
+                 'ones_like', 'abs', 'sign', 'sin', 'cos', 'degrees', 'radians',
-def check_symbol_consistency(sym1, sym2, ctx, skip_grad=False):
+def check_symbol_consistency(sym1, sym2, ctx, skip_grad=False, equal_nan=False):
-                                    grad_req='null' if skip_grad else 'write')
+                                    grad_req='null' if skip_grad else 'write',
-    def add(self, block):
+    def add(self, *blocks):
-        self.register_child(block)
+        for block in blocks:
-        return self._children[i]
+    def __getitem__(self, key):
-    def add(self, block):
+    def add(self, *blocks):
-        self.register_child(block)
+        for block in blocks:
-        return self._children[i]
+    def __getitem__(self, key):
-    model.add(nn.Dense(32, in_units=64))
+    model.add(nn.Dense(64, activation='tanh', in_units=256),
-    model.add(nn.Dense(32, in_units=64))
+    model.add(nn.Dense(64, activation='tanh'),
-from .ndarray import NDArray
+from .ndarray import NDArray, _ndarray_cls
-        head_grads = [head_grads] if head_grads is not None else None
+    head_handles, hgrad_handles = _parse_head(heads, head_grads)
-        output_handles.append(arr.handle)
+    check_call(_LIB.MXAutogradBackwardEx(
-        "heads and head_grads must have the same length"
+
-        c_array(NDArrayHandle, ograd_handles),
+        len(head_handles),
-        ctypes.c_int(train_mode)))
+        ctypes.c_int(create_graph),
-                self._data[ctx] = self._data[ctx].detach()
+            self._data = [i.detach() for i in self._data]
-        if arr_dict is not None:
+    def _check_and_get(self, arr_list, ctx):
-                return list(arr_dict.values())
+                return arr_list
-                    ctx = self._ctx_list[0]
+                if len(arr_list) == 1:
-                return ret
+            idx = self._ctx_map[ctx.device_typeid][ctx.device_id]
-    def _init_impl(self, data, ctx):
+    def _init_impl(self, data, ctx_list):
-            self._data[i] = data.copyto(i)
+        self._ctx_list = list(ctx_list)
-
+        self._grad = [ndarray.zeros_like(i) for i in self._data]
-            if self.allow_deferred_init:
+            if self._allow_deferred_init:
-        for i in self._grad.values():
+        for i in self._grad:
-
+            0,
-            ctypes.c_int(train_mode)))
+            ctypes.c_int(0),
-        raise Exception("unknown storage type")
+        raise Exception("unknown storage type: %s"%stype)
-    stypes = ['row_sparse', 'csr', 'default']
+    stypes = ['default']  #,'row_sparse', 'csr']
-    stypes = ['row_sparse', 'csr', 'default']
+    stypes = ['default']  #,'row_sparse', 'csr']
-    stypes = ['default', 'row_sparse', 'csr']
+    stypes = ['default']  #, 'row_sparse', 'csr']
-    stypes = ['csr', 'default', 'row_sparse']
+    stypes = ['default']  #, 'csr', 'row_sparse']
-def check_rnn_forward(layer, inputs):
+def check_rnn_forward(layer, inputs, deterministic=True):
-    mx.nd.waitall()
+        out = layer.unroll(3, inputs, merge_outputs=False)[0]
-    check_rnn_forward(gluon.rnn.DropoutCell(0.5), mx.nd.ones((8, 3, 200)))
+    check_rnn_forward(gluon.rnn.DropoutCell(0.5), mx.nd.ones((8, 3, 200)), False)
-                      mx.nd.ones((8, 3, 200)))
+                      mx.nd.ones((8, 3, 200)), False)
-    mx.nd.waitall()
+
-        self._list_iamges(self._root)
+        self._list_images(self._root)
-    def _list_iamges(self, root):
+    def _list_images(self, root):
-            Ending index of used outputs.
+        pad = self.cur_batch_pad or 0
-
+            outputs_slice = []
-            preds = OrderedDict(zip(self.output_names, texec.outputs))
+            preds = OrderedDict(zip(self.output_names, outputs_slice))
-                                                      ograd_density=ograd_density)
+                            with warnings.catch_warnings():
-                                                force_overlap=force_overlap)
+                    with warnings.catch_warnings():
-            raise DeferredInitializationError
+            raise DeferredInitializationError(
-            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16)
+            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
-            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16)
+            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
-            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16)
+            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
-            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16)
+            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
-            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16)
+            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
-            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16)
+            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
-            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16)
+            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
-            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16)
+            check_numeric_gradient(test, in_location, mean_std, numeric_eps=1e-2, rtol=0.16, atol=1e-4)
-        loss = F_contrib.CTCLoss(data, label,
+        loss = F.contrib.CTCLoss(data, label,
-        key : int or slice
+        key : int or slice, or array like
-                return self
+            return self
-            end = []
+            assert len(key) > 0, "Cannot slice with empty indices"
-            return op.slice(self, begin, end).reshape(oshape)
+            if isinstance(key[0], (NDArray, np.ndarray, list)):
-        return
+    # Currently disabled on GPU as they need cuda8
-                  'squeezenet1.0', 'squeezenet1.1']
+                  'squeezenet1.0', 'squeezenet1.1',
-import math
+import numpy as np
-    total_norm = math.sqrt(total_norm.asscalar())
+    total_norm = ndarray.add_n(*[ndarray.dot(x, x)
-    exec1.forward(is_train=True)
+    exec1 = Y.bind(xpu, args = [x, l], grad_req={'X':'null', 'L':'null'})
-
+    check_sequence_mask((3, 4), default_context(), 0.14)
-
+    # test for matrix case
-
+    assert_array_equal(test_wrapper(arr_4, xpu, sequence_length=seq_len_1, use_sequence_length=True), arr_5)
-from .ndarray import (NDArray, zeros, clip, sqrt, sign, array, maximum, abs as NDabs)
+from .ndarray import (NDArray, zeros, clip, sqrt, array, maximum, abs as NDabs)
-                      mp_sgd_update, mp_sgd_mom_update)
+                      mp_sgd_update, mp_sgd_mom_update, ftrl_update)
-                zeros(weight.shape, weight.context))  # n
+        return (zeros(weight.shape, weight.context, stype=weight.stype),  # z
-            grad = clip(grad, -self.clip_gradient, self.clip_gradient)
+        kwargs = {'lamda1': self.lamda1, 'beta': self.beta, 'rescale_grad': self.rescale_grad}
-                    ((self.beta + sqrt(n)) / lr + wd) * (NDabs(dn) > self.lamda1)
+        z, n = state
-        for dim in range(2, maxdim):
+    def check_sparse_elementwise_sum_with_shape(stype, shape, n):
-        L = \\frac{1}{2}\\sum_i \\Vert {output}_i - {label}_i \\Vert^2.
+        L = \\frac{1}{2}\\sum_i \\vert {output}_i - {label}_i \\vert^2.
-        loss(o, t) = - 1/n \sum_i (t[i] * log(o[i]) + (1 - t[i]) * log(1 - o[i]))
+        loss(o, t) = - 1/n \\sum_i (t[i] * \\log(o[i]) + (1 - t[i]) * \\log(1 - o[i]))
-
+
-                print ("server %d, unknown command (%d, %s)" % (
+                print("server %d, unknown command (%d, %s)" % (
-def _get_uniform_dataset_csr(num_rows, num_cols, density=0.1, dtype=None):
+def shuffle_csr_column_indices(csr):
-def rand_sparse_ndarray(shape, stype, density=None, distribution=None, dtype=None):
+def assign_each(the_input, function):
-    >>> assert(row4nnz == 2*row3nnz)
+    density = rnd.rand() if density is None else density
-        indices = np.argwhere(idx_sample < density).flatten()
+        if rsp_indices is not None:
-            csr = _get_uniform_dataset_csr(shape[0], shape[1], density, dtype=dtype)
+            csr = _get_uniform_dataset_csr(shape[0], shape[1], density,
-            csr = _get_powerlaw_dataset_csr(shape[0], shape[1], density, dtype=dtype)
+            csr = _get_powerlaw_dataset_csr(shape[0], shape[1], density=density, dtype=dtype)
-def rand_ndarray(shape, stype, density=None, dtype=None, distribution=None):
+def rand_ndarray(shape, stype, density=None, dtype=None,
-        arr, _ = rand_sparse_ndarray(shape, stype, density=density, dtype=dtype,
+        arr, _ = rand_sparse_ndarray(shape, stype, density=density,
-def almost_equal(a, b, rtol=None, atol=None):
+def almost_equal(a, b, rtol=None, atol=None, equal_nan=False):
-    return np.allclose(a, b, rtol=get_rtol(rtol), atol=get_atol(atol))
+    return np.allclose(a, b, rtol=get_rtol(rtol), atol=get_atol(atol), equal_nan=equal_nan)
-def assert_almost_equal(a, b, rtol=None, atol=None, names=('a', 'b')):
+def assert_almost_equal(a, b, rtol=None, atol=None, names=('a', 'b'), equal_nan=False):
-    if almost_equal(a, b, rtol, atol):
+    if almost_equal(a, b, rtol, atol, equal_nan=equal_nan):
-        executor.arg_dict[k][:] = v
+        stype = executor.arg_dict[k].stype
-            executor.arg_dict[k][:] = v
+            executor.arg_dict[k][:] = as_stype(v, stype, dtype=dtype)
-            executor.arg_dict[k][:] = v
+            executor.arg_dict[k][:] = as_stype(v, stype, dtype=dtype)
-                    executor.aux_dict[key][:] = val
+                    adstype = executor.aux_dict[key].stype
-            approx_grads[k].ravel()[i] = (f_peps - f_neps).sum() / eps
+            approx_grad = (f_peps - f_neps).sum() / eps
-        executor.arg_dict[k][:] = old_value
+        executor.arg_dict[k][:] = as_stype(old_value, stype, dtype=dtype)
-        use_forward_train=use_forward_train, dtype=dtype)
+        executor, location_npy, aux_states_npy,
-                           aux_states=None, ctx=None, dtype=default_dtype()):
+                           aux_states=None, ctx=None, equal_nan=False,
-
+                            ("EXPECTED_%s"%output_name, "FORWARD_%s"%output_name),
-                            dtype=default_dtype()):
+                            equal_nan=False, dtype=default_dtype()):
-            args_grad_data[k] = nd.tostype(grad_stypes[k])
+            stype = grad_stypes[k]
-                     for k, v in out_grads.items()}
+        outg = list()
-                                ("EXPECTED_%s"%name, "BACKWARD_%s"%name))
+                                ("EXPECTED_%s"%name, "BACKWARD_%s"%name),
-                                rtol, atol, ("EXPECTED_%s"%name, "BACKWARD_%s"%name))
+                                rtol, atol, ("EXPECTED_%s"%name, "BACKWARD_%s"%name),
-                                rtol, atol, ("EXPECTED_%s"%name, "BACKWARD_%s"%name))
+                                rtol, atol, ("EXPECTED_%s"%name, "BACKWARD_%s"%name),
-
+    return args_grad_data
-        lhs_nd = rand_ndarray(lhs_shape, 'csr', density=lhs_density)
+        lhs_nd = rand_ndarray(lhs_shape, 'csr', density=lhs_density, shuffle_csr_indices=False)
-
+import unittest
-
+    
-        print count, ': ', warning
+    if total_count>0:
-        `data` is an activation tensor without softmax.
+        `data` is an activation tensor (i.e. before softmax).
-        `label` is the label index matrix.
+        `label` is the label index matrix with zero-indexed labels.
-        When `label_lengths` is not specified, the first occurrence of `padding_mask`
+        input has shape `(label_sequence_length, batch_size)`. Padding mask of value ``-1``
-        are smaller than 4. Thus, given *padding_mask* = 0, the resulting ```label```
+
-          [[2, 1, 0, 0], [3, 2, 2, 0]]
+          [[1, 0, -1, -1], [2, 1, 1, -1], [0, 1, 0, 2]]
-                 weight=None, **kwargs):
+    def __init__(self, layout='NTC', label_layout='NT', weight=None, **kwargs):
-                                 padding_mask=self._padding_mask)
+                                 blank_label='last')
-    loss = mx.gluon.loss.CTCLoss(padding_mask=0)
+    loss = mx.gluon.loss.CTCLoss()
-    gpu_label = mx.nd.array([[2,1,0,0],[3,2,2,0]], ctx=mx.gpu(0))
+    cpu_label = mx.nd.array([[2,1,-1,-1],[3,2,2,-1]], ctx=mx.cpu(0))
-from mxnet.test_utils import assert_almost_equal
+from mxnet.test_utils import assert_almost_equal, default_context
-def get_net(num_hidden):
+def get_net(num_hidden, flatten=True):
-    fc1 = mx.symbol.FullyConnected(data, name='fc1', num_hidden=128)
+    fc1 = mx.symbol.FullyConnected(data, name='fc1', num_hidden=128, flatten=flatten)
-    fc2 = mx.symbol.FullyConnected(act1, name = 'fc2', num_hidden = 64)
+    fc2 = mx.symbol.FullyConnected(act1, name = 'fc2', num_hidden = 64, flatten=flatten)
-    fc3 = mx.symbol.FullyConnected(act2, name='fc3', num_hidden=num_hidden)
+    fc3 = mx.symbol.FullyConnected(act2, name='fc3', num_hidden=num_hidden, flatten=flatten)
-    l = loss(mx.nd.ones((20,2,4)), mx.nd.array([[2,1,0,0],[3,2,2,0]]))
+    loss = gluon.loss.CTCLoss()
-    l = loss(mx.nd.ones((20,2,4)), mx.nd.array([[2,1,0,0],[3,2,2,0]]).T)
+    loss = gluon.loss.CTCLoss(layout='TNC')
-    l = loss(mx.nd.ones((2,20,4)), mx.nd.array([[2,1,-1,-1],[3,2,2,-1]]))
+    loss = gluon.loss.CTCLoss(layout='TNC', label_layout='TN')
-        coverages = self._calculate_areas(intersects) / object_areas
+        coverages = self._calculate_areas(intersects) / object_areas[valid_objects]
-# pylint: disable=too-many-lines
+
-# Common
+def test_learning_rate():
-    test_sparse_sgd()
+    import nose
-    return softmax
+def get_symbol(num_classes, num_layers=11, batch_norm=False, dtype='float32', **kwargs):
-        self._update_count(index)
+        self._update_count(index)
-        self._update_count(index)
+        self._update_count(index)
-        self._update_count(index)
+        self._update_count(index)
-        self._update_count(index)
+        self._update_count(index)
-        self._update_count(index)
+        self._update_count(index)
-        self._update_count(index)
+        self._update_count(index)
-        self._update_count(index)
+        self._update_count(index)
-        self._update_count(index)
+        self._update_count(index)
-        self._update_count(index)
+def imagenet_iterator(train_data, val_data, batch_size, data_shape, resize=-1):
-print(opt)
+logging.info(opt)
-    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': opt.lr, 'wd': opt.wd},
+    net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)
-                batch_end_callback = mx.callback.Speedometer(batch_size, 1))
+        mod.fit(train_data,
-        self._base_url = 'https://apache-mxnet.s3.amazonaws.com/gluon/dataset/mnist/'
+        self._base_url = 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com' \
-        self._base_url = 'https://apache-mxnet.s3.amazonaws.com/gluon/dataset/fashion-mnist/'
+        self._base_url = 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com' \
-bucket = 'apache-mxnet'
+_url_format = 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com' \
-                                file_name=file_name),
+    download(_url_format.format(file_name=file_name),
-                logging.info('[Epoch %d Batch %d] speed: %f samples/s, training: %s=%f'%(
+                logging.info('Epoch[%d] Batch [%d]\tSpeed: %f samples/sec\t%s=%f'%(
-    def __init__(self, name, img_size, batch_size):
+    def __init__(self, mode, name, img_size, batch_size):
-                \nThe network_name is a valid model defined as network_name.py in the image-classification/symbol folder.')
+            if len(args) != 4 or isinstance(args[0], str) == False or isinstance(args[1], str) == False:
-                return Network(name=args[0], batch_size=batch_size, img_size=img_size)
+                # check if the network exists
-                \nThe network_name is a valid model defined as network_name.py in the image-classification/symbol folder.')
+                print('expected network attributes in format mode:network_name:batch_size:image_size \
-    parser.add_argument('--worker_file', type=str, help='file that contains a list of worker hostnames or list of worker ip addresses that can be sshed without a password.',required=True)
+    parser.add_argument('--networks', dest='networks', nargs='+', type=str, help='one or more networks in the format mode:network_name:batch_size:image_size \
-    s=[]
+    i = 1
-        i=i*2
+        i = i * 2
-    img_per_sec = re.findall("(?:Batch\s+\[30\]\\\\tSpeed:\s+)(\d+\.\d+)(?:\s+)", str(f.readlines()))
+def images_processed(log_loc, mode):
-    stop_args = ['python', '../../tools/kill-mxnet.py', hosts_file]
+def stop_old_processes(hosts_file, prog_name):
-    hosts = log_loc + '/' + network + '_' + str(num_nodes*num_gpus) + '_workers'
+def run_benchmark(kv_store, data_shape, batch_size, num_gpus, num_nodes, network, args_workers_file, mode):
-    launch_args = ['../../tools/launch.py', '-n', str(num_nodes), '-s', str(num_nodes*2), '-H', hosts, ' '.join(imagenet_args) ]
+    if mode == 'native':
-    #use train_imagenet when running on a single node
+    # use train_imagenet/image_classification when running on a single node
-        imagenet.startCmd(timeout = 60 * 10)
+        imagenet = RunCmd(benchmark_args, log)
-        launch.startCmd(timeout = 60 * 10)
+        launch.startCmd(timeout=60 * 10)
-    LOGGER.info('network: %s, num_gpus: %d, image/sec: %f', network, num_gpus*num_nodes, img_per_sec)
+    if mode == 'native':
-    speedup_chart = pygal.Line(x_title ='gpus',y_title ='speedup', logarithmic=True)
+    speedup_chart = pygal.Line(x_title='gpus', y_title='speedup', logarithmic=True)
-        LOGGER.info('%s: image_single_gpu:%.2f' %(net.name, image_single_gpu))
+        y_values = [each / image_single_gpu for each in net.gpu_speedup.values()]
-            , formatter= lambda y_val, img = copy.deepcopy(image_single_gpu), batch_size = copy.deepcopy(net.batch_size): 'speedup:%.2f, img/sec:%.2f, batch/gpu:%d' % \
+        speedup_chart.add(net.name, y_values \
-        #use kv_store='device' when running on 1 node
+        # use kv_store='device' when running on 1 node
-                                        num_gpus=num_gpus, num_nodes=1, network=net.name, args_workers_file=args.worker_file)
+            imgs_per_sec = run_benchmark(kv_store='device', data_shape=net.img_size, batch_size=net.batch_size, \
-                         num_gpus=args.gpu_count, num_nodes=num_nodes, network=net.name, args_workers_file=args.worker_file)
+            imgs_per_sec = run_benchmark(kv_store='dist_sync_device', data_shape=net.img_size,
-        vsw = np.sin(alpha * np.pi)
+        u = np.cos(alpha * np.pi)
-        t = np.dot(np.dot(self.tyiq, bt), self.ityiq).T
+                       [0.0, u, -w],
-        batch_size = 32
+        default_batch_size = 32
-        batch_size = 32 * len(gpus)
+        default_batch_size = 32 * len(gpus)
-        test_imagenet_model_performance(m, val, gpus, batch_size)
+        # Build/testing machines tend to be short on GPU memory
-        NDArray, CSRNDArray, RowSparseNDArray
+        NDArray, CSRNDArray or RowSparseNDArray
-        NDArray
+        NDArray, CSRNDArray or RowSparseNDArray
-        NDArray
+        NDArray, CSRNDArray or RowSparseNDArray
-# pylint: disable=unused-import
+# pylint: disable=unused-import, too-many-lines
-        >>> x = mx.nd.zeros('row_sparse', (2,3), dtype='float32')
+        >>> x = mx.nd.sparse.zeros('row_sparse', (2,3), dtype='float32')
-        >>> src = mx.nd.zeros((3,3), stype='csr')
+        >>> src = mx.nd.sparse.zeros('csr', (3,3))
-        >>> x = mx.nd.zeros((2, 3), stype='row_sparse')
+        >>> x = mx.nd.sparse.zeros('row_sparse', (2, 3))
-        >>> x = mx.nd.zeros('row_sparse', (3,3))
+        >>> x = mx.nd.sparse.zeros('row_sparse', (3,3))
-    """Creates a 2D array with compressed sparse row(CSR) format.
+    """Creates a 2D array with compressed sparse row (CSR) format.
-    >>> mx.nd.zeros((1,2), mx.cpu(), stype='csr')
+    >>> mx.nd.sparse.zeros('csr', (1,2))
-    >>> mx.nd.zeros((1,2), mx.cpu(), 'float16', stype='row_sparse').asnumpy()
+    >>> mx.nd.sparse.zeros('row_sparse', (1,2), ctx=mx.cpu(), dtype='float16').asnumpy()
-    >>> mx.nd.sparse.array(mx.nd.zeros((3, 2), stype='csr'))
+    >>> mx.nd.sparse.array(mx.nd.sparse.zeros('csr', (3, 2)))
-    >>> mx.nd.sparse.array(mx.nd.zeros((3, 2), stype='row_sparse'))
+    >>> mx.nd.sparse.array(mx.nd.sparse.zeros('row_sparse', (3, 2)))
-"""Contrib NDArray API of MXNet."""
+"""Contrib Symbol API of MXNet."""
-"""Linear Algebra NDArray API of MXNet."""
+"""Linear Algebra Symbol API of MXNet."""
-"""Random distribution generator NDArray API of MXNet."""
+"""Random Distribution Generator Symbol API of MXNet."""
-        The storage type of the variable.
+        The storage type of the variable, such as 'row_sparse', 'csr', 'default', etc
-parser.add_argument('--file_path', type=str, default='docs/_build/html/get_started/install.html',
+parser.add_argument('--file_path', type=str, default='docs/_build/html/install/index.html',
-                    'https://github.com/apache/incubator-mxnet/tree/%s/example' % (args.current_version)
+            # Add tag for example link
-        btn += '<div class="download_btn"><a href="%s" download="%s">' \
+        btn += '<div class="download-btn"><a href="%s" download="%s">' \
-def _parse_location(sym, location, ctx):
+def _parse_location(sym, location, ctx, dtype=default_dtype()):
-    location = {k: mx.nd.array(v, ctx=ctx) if isinstance(v, np.ndarray) \
+    location = {k: mx.nd.array(v, ctx=ctx, dtype=dtype) if isinstance(v, np.ndarray) \
-def _parse_aux_states(sym, aux_states, ctx):
+def _parse_aux_states(sym, aux_states, ctx, dtype=default_dtype()):
-        aux_states = {k: mx.nd.array(v, ctx=ctx) for k, v in aux_states.items()}
+        aux_states = {k: mx.nd.array(v, ctx=ctx, dtype=dtype) for k, v in aux_states.items()}
-def numeric_grad(executor, location, aux_states=None, eps=1e-4, use_forward_train=True):
+def numeric_grad(executor, location, aux_states=None, eps=1e-4,
-    approx_grads = {k: np.zeros(v.shape, dtype=np.float32)
+    assert dtype == np.float32 or dtype == np.float64
-                           grad_stype_dict=None):
+                           grad_stype_dict=None, dtype=default_dtype()):
-    location = _parse_location(sym=sym, location=location, ctx=ctx)
+    location = _parse_location(sym=sym, location=location, ctx=ctx, dtype=dtype)
-    aux_states = _parse_aux_states(sym=sym, aux_states=aux_states, ctx=ctx)
+    aux_states = _parse_aux_states(sym=sym, aux_states=aux_states, ctx=ctx,
-                    [("__random_proj", mx.nd.array(random_projection(out_shape[0]), ctx=ctx))])
+                    [("__random_proj", mx.nd.array(random_projection(out_shape[0]),
-    args_grad = {k: mx.nd.array(v, ctx=ctx) for k, v in args_grad_npy.items()}
+    args_grad = {k: mx.nd.array(v, ctx=ctx, dtype=dtype) for k, v in args_grad_npy.items()}
-                                     eps=numeric_eps, use_forward_train=use_forward_train)
+    numeric_gradients = numeric_grad(
-                           aux_states=None, ctx=None):
+                           aux_states=None, ctx=None, dtype=default_dtype()):
-    aux_states = _parse_aux_states(sym=sym, aux_states=aux_states, ctx=ctx)
+    location = _parse_location(sym=sym, location=location, ctx=ctx, dtype=dtype)
-    args_grad_data = {k:mx.nd.empty(v.shape, ctx=ctx) for k, v in location.items()}
+    args_grad_data = {k:mx.nd.empty(v.shape, ctx=ctx, dtype=dtype) for k, v in location.items()}
-                            aux_states=None, grad_req='write', ctx=None, grad_stypes=None):
+                            aux_states=None, grad_req='write', ctx=None, grad_stypes=None,
-    aux_states = _parse_aux_states(sym=sym, aux_states=aux_states, ctx=ctx)
+    location = _parse_location(sym=sym, location=location, ctx=ctx, dtype=dtype)
-        nd = mx.nd.array(v, ctx=ctx)
+        nd = mx.nd.array(v, ctx=ctx, dtype=dtype)
-        out_grads = [mx.nd.array(v, ctx=ctx) for v in out_grads]
+        out_grads = [mx.nd.array(v, ctx=ctx, dtype=dtype) for v in out_grads]
-        out_grads = {k:mx.nd.array(v, ctx=ctx) for k, v in out_grads.items()}
+        out_grads = {k:mx.nd.array(v, ctx=ctx, dtype=dtype)
-def test_laop():
+# Helper functions for test_laop
-    data4 = mx.symbol.Variable('data4')
+
-    check_symbolic_forward(test_gemm, [data_in1, data_in2, data_in4], [res_gemm])
+    res_gemm = 4. * np.dot(data_in1, data_in2) + 7. * data_in4
-    check_symbolic_forward(test_gemm, [data_in1, data_in2, data_in3], [res_gemm])
+        check_grad(test_gemm, [data_in1, data_in2, data_in4])
-    check_symbolic_forward(test_gemm, [data_in1, data_in1, data_in3], [res_gemm])
+        check_grad(test_gemm, [data_in1, data_in2, data_in3])
-    check_symbolic_forward(test_gemm, [data_in1, data_in1, data_in4], [res_gemm])
+        check_grad(test_gemm, [data_in1, data_in1, data_in3])
-      check_numeric_gradient(test_gemm, [data_in1, data_in1, data_in4], numeric_eps=1e-3, rtol=1e-1, atol=1e-1)
+        check_grad(test_gemm, [data_in1, data_in1, data_in4])
-    check_symbolic_forward(test_gemm, [a, b, c], [r])
+    a = rep_3x(data_in1, 2, 3)
-      check_numeric_gradient(test_gemm, [a, b, c], numeric_eps=1e-3, rtol=1e-1, atol=1e-1)
+        check_grad(test_gemm, [a, b, c])
-    check_symbolic_forward(test_gemm, [data_in1, data_in2], [res_gemm])
+    res_gemm = 4. * np.dot(data_in1, data_in2)
-    check_symbolic_forward(test_gemm, [data_in1, data_in2], [res_gemm])
+        check_grad(test_gemm, [data_in1, data_in2])
-    check_symbolic_forward(test_gemm, [data_in1, data_in1], [res_gemm])
+        check_grad(test_gemm, [data_in1, data_in2])
-    check_symbolic_forward(test_gemm, [data_in1, data_in1], [res_gemm])
+        check_grad(test_gemm, [data_in1, data_in1])
-      check_numeric_gradient(test_gemm, [data_in1, data_in1], numeric_eps=1e-3, rtol=1e-1, atol=1e-1)
+        check_grad(test_gemm, [data_in1, data_in1])
-    check_symbolic_forward(test_gemm, [a, b], [r])
+    a = rep_3x(data_in1, 2, 3)
-      check_numeric_gradient(test_gemm, [a, b], numeric_eps=1e-3, rtol=1e-1, atol=1e-1)
+        check_grad(test_gemm, [a, b])
-    shape = (4, 4, 1, 1 )
+    shape = (4, 4, 1, 1)
-    check_symbolic_forward(test_potrf, [data_in], [res_potrf])
+    test_potrf = mx.sym.linalg_potrf(data1)
-      check_numeric_gradient(test_potrf, [data_in])
+        check_grad(test_potrf, [data_in])
-    check_symbolic_forward(test_potri, [data_in], [res_potri])
+    res_potri = np.divide(ones, data_in * data_in)
-      check_numeric_gradient(test_potri, [data_in], atol = 0.01, rtol = 1.5)
+        check_grad(test_potri, [data_in])
-    check_symbolic_forward(test_trsm, [trian_in,data_in], [ones])
+    trian_in = data_in * 7.
-      check_numeric_gradient(test_trsm, [trian_in,data_in], atol = 0.02, rtol = 2.0)
+        check_grad(test_trsm, [trian_in,data_in])
-    check_symbolic_forward(test_trmm, [trian_in,data_in], [ones])
+    trian_in = np.divide(ones, trian_in)
-      check_numeric_gradient(test_trmm, [trian_in,data_in], atol = 0.02, rtol = 2.0)
+        check_grad(test_trmm, [trian_in, data_in])
-    check_symbolic_forward(test_sumlogdiag, [data_in], [res_sumlogdiag])
+    res_sumlogdiag = np.reshape(np.log(data_in), (4, 4))
-    grad_check = 0
+        check_grad(test_sumlogdiag, [data_in])
-    check_symbolic_forward(test_potrf, [a], [r])
+    test_potrf = mx.sym.linalg_potrf(_make_symm_symbol(data1, ndims=4))
-      check_numeric_gradient(test_potrf, [a], numeric_eps=1e-3, rtol=1e-2, atol=1e-1)
+        check_grad(test_potrf, [a])
-    check_symbolic_forward(test_potri, [a], [r], atol=0.01)
+    data1_ltri = _make_lower_triangle_symm(
-    check_symbolic_forward(test_trsm, [a,b], [r])
+        check_grad(test_potri, [a])
-      check_numeric_gradient(test_trsm, [a,b], numeric_eps=1e-3, rtol=1e-2, atol=1e-1)
+        check_grad(test_trsm, [a, b])
-    check_symbolic_forward(test_trsm2, [a,b], [r])
+    test_trsm2 = mx.sym.linalg_trsm(
-    check_symbolic_forward(test_trsm3, [a,b], [r])
+        check_grad(test_trsm2, [a, b])
-    check_symbolic_forward(test_trsm4, [a,b], [r])
+        check_grad(test_trsm3, [a, b])
-    check_symbolic_forward(test_trmm, [a,b], [r])
+        check_grad(test_trsm4, [a, b])
-      check_numeric_gradient(test_trmm, [a,b], numeric_eps=1e-3, rtol=1e-2, atol=1e-1)
+        check_grad(test_trmm, [a, b])
-    check_symbolic_forward(test_trmm2, [a,b], [r])
+    test_trmm2 = mx.sym.linalg_trmm(data1_ltri, data2, alpha=-2.)
-      check_numeric_gradient(test_trmm2, [a,b], numeric_eps=1e-3, rtol=1e-2, atol=1e-1)
+        check_grad(test_trmm2, [a, b])
-    check_symbolic_forward(test_trmm3, [a,b], [r])
+    test_trmm3 = mx.sym.linalg_trmm(data1_ltri, data2, rightside=True)
-      check_numeric_gradient(test_trmm3, [a,b], numeric_eps=1e-3, rtol=1e-2, atol=1e-1)
+        check_grad(test_trmm3, [a, b])
-    check_symbolic_forward(test_trmm4, [a,b], [r])
+    test_trmm4 = mx.sym.linalg_trmm(
-      check_numeric_gradient(test_trmm4, [a,b], numeric_eps=1e-3, rtol=1e-2, atol=1e-1)
+        check_grad(test_trmm4, [a, b])
-    check_symbolic_forward(test_sumlogdiag, [a], [r])
+    a = rep_3x(pow, 4, 4)
-      check_numeric_gradient(test_sumlogdiag, [a])
+        check_grad(test_sumlogdiag, [a])
-import StringIO
+# import StringIO from io for python3 compatibility
-    """Returns an iterator for ``mx.nd.NDArray``, ``numpy.ndarray`` or ``h5py.Dataset``.
+    """Returns an iterator for ``mx.nd.NDArray``, ``numpy.ndarray``, ``h5py.Dataset``
-        value : NDArray or sequence of NDArray
+        value : NDArray, RowSparseNDArray or sequence of NDArray or RowSparseNDArray
-        >>> keys = [5, 7, 9]
+        >>> keys = ['5', '7', '9']
-        value : NDArray or list of NDArray or list of list of NDArray
+        value : NDArray, RowSparseNDArray, list of NDArray or RowSparseNDArray,
-        >>> keys = [4, 5, 6]
+        >>> keys = ['4', '5', '6']
-        For row_sparse values, please use `row_sparse_pull` instead.
+        For `RowSparseNDArray` values, please use ``row_sparse_pull`` instead.
-        >>> keys = [5, 7, 9]
+        >>> keys = ['5', '7', '9']
-         with specified row_ids.
+        """ Pulls a single RowSparseNDArray value or a sequence of RowSparseNDArray values \
-        out: NDArray or list of NDArray or list of list of NDArray
+        out: RowSparseNDArray or list of RowSparseNDArray or list of list of RowSparseNDArray
-            The row_ids for which to pull for each value. Each row_id is an 1D-NDArray \
+            The row_ids for which to pull for each value. Each row_id is an 1D NDArray \
-        >>> a = mx.nd.zeros(shape, stype='row_sparse')
+        >>> a = mx.nd.sparse.zeros('row_sparse', shape)
-        state = momentum * state + lr * rescale_grad * clip(grad, clip_gradient) + wd * weight
+        rescaled_grad = lr * rescale_grad * clip(grad, clip_gradient) + wd * weight
-    Sparse updating is supported. For details of the update algorithm see
+    If the storage types of weight, state and grad are all ``row_sparse``, \
-    For details of the update algorithm, see :class:`ndarray.adam_update`.
+    For details of the update algorithm, see :class:`~mxnet.ndarray.adam_update`.
-    csr_iter = iter(mx.io.NDArrayIter(csr, csr, batch_size))
+    csr_iter = iter(mx.io.NDArrayIter(csr, csr, batch_size, last_batch_handle='discard'))
-            row_ids.append(mx.nd.array(row_id, dtype='int64'))
+            row_ids.append(mx.nd.array(row_id))
-    all_rows = mx.nd.array(np.arange(shape[0]), dtype='int64')
+    all_rows = mx.nd.array(np.arange(shape[0]))
-                                   batch_size=batch_size)
+    train_iter = mx.io.NDArrayIter(data=csr_nd, label={'label':label},
-    iterator = mx.io.NDArrayIter(data=data, label={'label':label}, batch_size=n)
+    iterator = mx.io.NDArrayIter(data=data, label={'label':label},
-    if literal_eval(param['pad']) != (0, 0):
+    if 'pad' in param.keys() and literal_eval(param['pad']) != (0, 0):
-    stride_height, stride_width = literal_eval(param['stride'])
+
-    if literal_eval(param['pad']) != (0, 0):
+    if 'pad' in param.keys() and literal_eval(param['pad']) != (0, 0):
-    stride_height, stride_width = literal_eval(param['stride'])
+    stride_height = 1
-    """Loads and returns a given MXNet model.
+    """Returns a module loaded with the provided model.
-        # We prepend 1 because the coreml model only accept 1 input data at a time.
+        # We prepend 1 because the coreml model only accept 1 input data at a time (=batch-size).
-
+from converter import utils
-def _get_mxnet_module(net, input_shape, mode, label_names, input_names=None):
+def _get_mxnet_module(net, data_shapes, mode, label_names, input_names=None):
-    )
+    mod = utils.create_module(sym=net, data_shapes=data_shapes, label_shapes=input_names, label_names=label_names)
-                          pre_processing_args=None):
+                          pre_processing_args=None, input_name='data'):
-        mod = _get_mxnet_module(net, input_shape, mode, label_names)
+
-        input_data = {'data': np.random.uniform(-10., 10., input_shape)}
+        input_data = {input_name: np.random.uniform(-10., 10., input_shape)}
-        mod.forward(Batch([mx.nd.array(input_data['data'])]))
+        mod.forward(Batch([mx.nd.array(input_data[input_name])]))
-            input_shape={'data': input_shape},
+            input_shape={input_name: input_shape},
-                                input_shape=input_shape,
+                                data_shapes=[('data', input_shape)],
-    batch_end_callback = mx.callback.Speedometer(train_data.batch_size, frequent=frequent, auto_reset=false)
+    batch_end_callback = mx.callback.Speedometer(train_data.batch_size, frequent=frequent, auto_reset=False)
-    batch_end_callback = mx.callback.Speedometer(train_data.batch_size, frequent=frequent, auto_reset=false)
+    batch_end_callback = mx.callback.Speedometer(train_data.batch_size, frequent=frequent, auto_reset=False)
-    batch_end_callback = mx.callback.Speedometer(train_data.batch_size, frequent=args.frequent, auto_reset=false)
+    batch_end_callback = mx.callback.Speedometer(train_data.batch_size, frequent=args.frequent, auto_reset=False)
-        if logger.level == logging.INFO:
+        if logger.level == logging.DEBUG:
-        if logger.level == logging.INFO:
+        if logger.level == logging.DEBUG:
-parser.add_argument('--num-batch', type=int, default=99999999,
+parser.add_argument('--num-batch', type=int, default=MAX_NUM_BATCH,
-parser.add_argument('--kvstore', type=str, default='local',
+parser.add_argument('--kvstore', type=str, default=None,
-parser.add_argument('--sparse-log-level', type=str, default='INFO',
+parser.add_argument('--sparse-log-level', type=str, default='DEBUG',
-datasets = { 'kdda' : kdda, 'avazu' : avazu }
+datasets = { 'kdda' : kdda, 'avazu' : avazu , 'criteo': criteo }
-    kv = mx.kvstore.create(kvstore) if args.num_gpu >= 1 else None
+    kv = mx.kvstore.create(kvstore) if kvstore else None
-    if dummy_iter:
+    if dummy_iter or measure_only == COMP or measure_only  == COMM:
-            mod.update()
+            if measure_only != IO and measure_only != COMM:
-        logging.info('epoch %d, %s' % (epoch, metric.get()))
+        logging.info('epoch {}, {}'.format(epoch, metric.get()))
-            print "num_batches = ", nbatch
+            logging.debug("num_batches = {}".format(nbatch))
-    logging.info('num_worker = ' + str(num_worker) + ', time cost = ' + str(time_cost))
+    logging.info('num_worker = {}, rank = {}, time cost = {}'.format(str(num_worker), str(rank), str(time_cost)))
-    for num_base in [32, 64]:
+    for num_base in [1, 4, 16, 32, 64]:
-                        np.testing.assert_allclose(arr1.asnumpy(), arr2.asnumpy(), rtol=1e-3, atol=1e-4)
+                    for in_size in [7, 32]:
-    parser.add_argument('--data-shape', dest='data_shape', type=int, default=512,
+    parser.add_argument('--data-shape', dest='data_shape', type=str, default='512',
-        prefix = args.prefix + args.network + '_' + str(args.data_shape)
+        prefix = args.prefix + args.network + '_' + str(args.data_shape[0])
-                            args.data_shape,
+                            data_shape,
-        self.mod.bind(data_shapes=[('data', (batch_size, 3, data_shape, data_shape))])
+        self.mod.bind(data_shapes=[('data', (batch_size, 3, data_shape[0], data_shape[1]))])
-        self.data_shape = data_shape
+# Licensed to the Apache Software Foundation (ASF) under one
-import find_mxnet
+import caffe_parser
-    arg_names = prob.list_arguments()
+from convert_symbol import convert_symbol
-        iter = get_iter(layers)
+    aux_params = {}
-                or layer_type == 'PReLU' or layer_type == 'Normalize':
+    layers, names = caffe_parser.read_caffemodel(prototxt_fname, caffemodel_fname)
-                                layer_blobs[0].width]
+                    wmat_dim = [layer_blobs[0].num, layer_blobs[0].channels,
-            bias = np.array(layer_blobs[1].data)
+
-                    print('Swapping BGR of caffe into RGB in mxnet')
+                    # Swapping BGR of caffe into RGB in mxnet
-            print('converting layer {0}, wmat shape = {1}, bias shape = {2}'.format(layer_name, wmat.shape, bias.shape))
+            assert(wmat.flags['C_CONTIGUOUS'] is True)
-    model.init_params(arg_params=arg_params, aux_params={})
+        elif layer_type == 'Scale':
-    model.save_checkpoint(args.save_model_name, 1)
+def main():
-import math
+import caffe_parser
-        solver_config = caffe.proto.caffe_pb2.NetParameter()
+def _get_input(proto):
-    return read_proto_file(file_path, solver_config)
+        raise ValueError('Cannot find input size')
-
+def _convert_conv_param(param):
-    pad = 0
+    pad_w = 0
-        pad = 0 if len(param.pad) == 0 else param.pad[0]
+        if len(param.pad) > 0:
-        kernel_size = param.kernel_size[0]
+
-                   (param.num_output, pad, pad, kernel_size, kernel_size, stride, stride, not param.bias_term)
+    if hasattr(param, 'dilation'):
-def find_layer(layers, name):
+def _find_layer(layers, name):
-    output_name = ""
+def _parse_proto(prototxt_fname):
-    for i in range(len(layer)):
+    symbol_string = "import mxnet as mx\ndata = mx.symbol.Variable(name='data')\n"
-        from_name = 'data='
+        skip_layer = False
-        if layer[i].type == 'Convolution' or layer[i].type == 4:
+        name = re.sub('[-/]', '_', layer.name)
-            param_string = conv_param_to_string(layer[i].convolution_param)
+            param_string = _convert_conv_param(layer.convolution_param)
-        if layer[i].type == 'Deconvolution' or layer[i].type == 39:
+        if layer.type == 'Deconvolution' or layer.type == 39:
-            param_string = conv_param_to_string(layer[i].convolution_param)
+            param_string = _convert_conv_param(layer.convolution_param)
-        if layer[i].type == 'Pooling' or layer[i].type == 17:
+        if layer.type == 'Pooling' or layer.type == 17:
-                raise Exception("Unknown Pooling Method!")
+            param_string = _convert_pooling_param(layer.pooling_param)
-        if layer[i].type == 'ReLU' or layer[i].type == 18:
+        if layer.type == 'ReLU' or layer.type == 18:
-        if layer[i].type == 'TanH' or layer[i].type == 23:
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-        if layer[i].type == 'Sigmoid' or layer[i].type == 19:
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-        if layer[i].type == 'LRN' or layer[i].type == 15:
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-                           (param.alpha, param.beta, param.k, param.local_size)
+            param = layer.lrn_param
-        if layer[i].type == 'InnerProduct' or layer[i].type == 14:
+        if layer.type == 'InnerProduct' or layer.type == 14:
-            param_string = "num_hidden=%d, no_bias=%s" % (param.num_output, not param.bias_term)
+            param = layer.inner_product_param
-        if layer[i].type == 'Dropout' or layer[i].type == 6:
+        if layer.type == 'Dropout' or layer.type == 6:
-            param = layer[i].dropout_param
+            param = layer.dropout_param
-            if layer[i].softmax_param.axis == 2:
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-                    (mapping[layer[i].bottom[0]], mapping[layer[i].bottom[0]])
+                    (mapping[layer.bottom[0]], mapping[layer.bottom[0]])
-                type_string = 'identical'
+        if layer.type == 'Flatten' or layer.type == 8:
-        if layer[i].type == 'Concat' or layer[i].type == 3:
+        if layer.type == 'Split' or layer.type == 22:
-        if layer[i].type == 'Crop':
+        if layer.type == 'Crop':
-        if layer[i].type == 'BatchNorm':
+        if layer.type == 'BatchNorm':
-        if layer[i].type == 'PReLU':
+            param = layer.batch_norm_param
-            param = layer[i].prelu_param
+            param = layer.prelu_param
-            conv_layer = find_layer(layer, bottom)
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-            param = layer[i].norm_param
+            param = layer.norm_param
-                    (name, name, mapping[layer[i].bottom[0]])
+                    (name, name, mapping[layer.bottom[0]])
-        if layer[i].type == 'Permute':
+        if layer.type == 'Permute':
-            param_string = "axes=(%s)" % (','.join([str(x) for x in layer[i].permute_param.order]))
+            param_string = "axes=(%s)" % (','.join([str(x) for x in layer.permute_param.order]))
-            if layer[i].bottom[0] == 'data':
+        if layer.type == 'PriorBox':
-            step = '(%f, %f)' % (step_h / finput_dim, step_w / finput_dim)
+            finput_dimh = float(input_dim[2])
-                (name, mapping[layer[i].bottom[0]], sizes, ratios_string, clip, step, name)
+            symbol_string += '%s = mx.contrib.symbol.MultiBoxPrior(%s, sizes=%s, ratios=%s, clip=%s, steps=%s, name="%s")\n' % \
-        if layer[i].type == 'DetectionOutput':
+        if layer.type == 'DetectionOutput':
-            param = layer[i].detection_output_param
+            param = layer.detection_output_param
-            param_string = "nms_threshold=%f, nms_topk=%d" % \
+            type_string = 'mx.contrib.symbol.MultiBoxDetection'
-            symbol_string += "%s = %s\n" % (name, mapping[bottom[0]])
+        if skip_layer:
-            bottom = layer[i].bottom
+            bottom = layer.bottom
-                                     (flatten_name, flatten_name, mapping[bottom[0]])
+                    symbol_string += "%s=mx.symbol.Flatten(name='%s', data=%s)\n" % (
-                                 (name, type_string, from_name, mapping[bottom[0]], param_string, name)
+                symbol_string += "%s = %s(name='%s', data=%s %s)\n" % (
-                if layer[i].type == 'Concat' and layer[i].concat_param.axis == 2:
+                if layer.type == 'Concat' and layer.concat_param.axis == 2:
-            mapping[layer[i].top[j]] = name
+        for j in range(len(layer.top)):
-    exec(sym)
+    Parameters
-    exec("ret = " + output_name, globals(), _locals)
+    exec("ret = " + output_name, globals(), _locals)  # pylint: disable=exec-used
-        print(symbol_string)
+    parser = argparse.ArgumentParser(
-    def save_optimizer_states(self, fname):
+    def save_optimizer_states(self, fname, dump_optimizer=False):
-            fout.write(self._updater.get_states())
+            fout.write(self._updater.get_states(dump_optimizer))
-        assert self._updater is not None, "Cannot save states for distributed training"
+        assert self._updater is not None, "Cannot load states for distributed training"
-        self.states = pickle.loads(states)
+        states = pickle.loads(states)
-        return pickle.dumps(self.states)
+    def get_states(self, dump_optimizer=False):
-    trainer = gluon.Trainer([x], 'sgd', {'learning_rate': 1.0})
+    trainer = gluon.Trainer([x], 'sgd', {'learning_rate': 1.0, 'momentum': 0.5})
-    assert (x.data(mx.cpu(1)).asnumpy() == -3).all()
+    assert (x.data(mx.cpu(1)).asnumpy() == -4).all()
-        self.provide_data = [('data', (batch_size, 2400))] + init_states
+        self.provide_data = [('data', (batch_size, 80, 30))] + init_states
-                img = img.reshape((80 * 30))
+                img = img.reshape((80, 30))
-        self._data = nd.array(data)
+        self._data = nd.array(data, dtype=data.dtype)
-        self._data = [nd.array(x, dtype=x.dtype) for x in data]
+        self._data = nd.array(data)
-        self._data = [nd.array(x, dtype=x.dtype) for x in data]
+        self._data = nd.array(data, dtype=data.dtype)
-        self.prev_output = None
+        self._prev_output = None
-        self.prev_output = None
+        self._prev_output = None
-        prev_output = self.prev_output
+        prev_output = self._prev_output
-        self.prev_output = output
+        self._prev_output = output
-    def attach_grad(self, grad_req='write'):
+    def attach_grad(self, grad_req='write', stype=None):
-        grad = op.zeros_like(self)  # pylint: disable=undefined-variable
+        from . import zeros as _zeros
-        return NDArray(hdl)
+        return _ndarray_cls(hdl)
-        return NDArray(hdl)
+        return _ndarray_cls(hdl)
-    assert mod.score(data_iter, eval_metric=mx.metric.Loss())[0][1] < 0.01
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 0.01},
-            eval_metric=mx.metric.Loss())
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 0.01},
-            eval_metric=mx.metric.Loss())
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 0.01},
-            initializer=mx.init.Xavier(magnitude=2), eval_metric=mx.metric.Loss())
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 0.01},
-            initializer=mx.init.Xavier(magnitude=3), eval_metric=mx.metric.Loss())
+            initializer=mx.init.Xavier(magnitude=2), eval_metric=mx.metric.Loss(),
-            eval_metric=mx.metric.Loss())
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 0.01},
-                                        'git clone --recursive https://github.com/apache/incubator-mxnet.git')
+                                        'git clone --recursive https://github.com/apache/incubator-mxnet.git mxnet')
-                                        'git clone --recursive https://github.com/apache/incubator-mxnet.git '
+                                        'git clone --recursive https://github.com/apache/incubator-mxnet.git mxnet '
-            loss = -(F.log(output+1e-8)*label + F.log(1.-output+1e-8)*(1.-label))
+            loss = -(F.log(output+1e-12)*label + F.log(1.-output+1e-12)*(1.-label))
-        loss = label * (F.log(label+1e-8) - output)
+        loss = label * (F.log(label+1e-12) - output)
-    def __init__(self, eps=1e-8, name='cross-entropy',
+    def __init__(self, eps=1e-12, name='cross-entropy',
-from test_sparse_operator import test_sparse_nd_zeros, test_sparse_retain
+from test_sparse_operator import *
-            data = mx.nd.random_uniform(shape=shape)
+            data = mx.nd.random_uniform(shape=shape, ctx=default_context())
-    check_fluent_regular('pick', {'axis': 1, 'begin': 5, 'end': 7})
+    check_fluent_regular('pick', {'axis': 1, 'index': mx.nd.array([[2], [3], [5], [6], [11]])})
-                              lhs_grad_stype='row_sparse', rhs_grad_stype='row_sparse')
+    if default_context().device_type == 'cpu':
-    assert_almost_equal(arr_grads[0].asnumpy(), arr_grads[1].asnumpy())
+    if default_context().device_type == 'cpu':
-    density = [1.00, 0.50, 0.10, 0.05, 0.01]
+    density = [1.00, 0.50, 0.05, 0.01]
-                                       atol=1e-2, rtol=0.1)
+    if default_context().device_type == 'cpu':
-        rhs_dns = mx.nd.cast_storage(rhs_nd, stype='default')
+    if default_context().device_type == 'cpu':
-        check_numeric_gradient(test, location)
+        def check_concat(shape, lhs_stype, rhs_stype):
-            check_softmax_with_shape(rhs, rhs, shape, preserve_shape=True)
+        shape = rand_shape_2d()
-        check_sparse_elementwise_sum_with_shape('row_sparse', shape, np.random.randint(1, 9))
+    if default_context().device_type == 'cpu':
-        stop = mx_uint(stop) if stop else mx_uint(self.shape[0])
+        if start is None:
-        return reconstructed_x
+        return reconstructed_x
-    batch_end_callback = callback.Speedometer(train_data.batch_size, frequent=frequent)
+    batch_end_callback = mx.callback.Speedometer(train_data.batch_size, frequent=frequent, auto_reset=false)
-    batch_end_callback = callback.Speedometer(train_data.batch_size, frequent=frequent)
+    batch_end_callback = mx.callback.Speedometer(train_data.batch_size, frequent=frequent, auto_reset=false)
-    batch_end_callback = callback.Speedometer(train_data.batch_size, frequent=args.frequent)
+    batch_end_callback = mx.callback.Speedometer(train_data.batch_size, frequent=args.frequent, auto_reset=false)
-            c_key_i, c_val_i = _ctype_key_value(key, val)
+            c_key_i, c_val_i, str_keys_i = _ctype_key_value(key, val)
-    keys = str(keys)
+            use_str_keys = str_keys_i if use_str_keys is None else use_str_keys
-                c_array(NDArrayHandle, [vals.handle]))
+        c_keys = c_array(ctypes.c_char_p, [c_str(keys)]) if use_str_keys \
-                c_array(NDArrayHandle, [value.handle for value in vals]))
+        c_keys = c_array(ctypes.c_char_p, [c_str(keys)] * len(vals)) if use_str_keys \
-        key : str or sequence of str
+        key : str, int, or sequence of str or int
-        >>> keys = ['5', '7', '9']
+        >>> keys = [5, 7, 9]
-        check_call(_LIB.MXKVStoreInitEx(self.handle, mx_uint(len(ckeys)), ckeys, cvals))
+        ckeys, cvals, use_str_keys = _ctype_key_value(key, value)
-        key : str or list of str
+        key : str, int, or sequence of str or int
-            ctypes.c_int(priority)))
+        ckeys, cvals, use_str_keys = _ctype_key_value(key, value)
-        key : int or list of int
+        key : str, int, or sequence of str or int
-        >>> keys = ['5', '7', '9']
+        >>> keys = [5, 7, 9]
-            ctypes.c_int(priority)))
+        ckeys, cvals, use_str_keys = _ctype_key_value(key, out)
-        key : str or list of str
+        key : str, int, or sequence of str or int
-            self.handle, mx_uint(len(ckeys)), ckeys, cvals, crow_ids, ctypes.c_int(priority)))
+        ckeys, cvals, use_str_keys = _ctype_key_value(key, out)
-        check_call(_LIB.MXKVStoreSetUpdater(self.handle, self._updater_func, None))
+        # set updater with str keys
-    kv.set_optimizer(mx.optimizer.create('test', lr))
+    kv.set_optimizer(mx.optimizer.create('test', rescale_grad=lr))
-    kv.set_optimizer(mx.optimizer.create('test', lr))
+    kv.set_optimizer(mx.optimizer.create('test', rescale_grad=lr))
-    """use updater: +="""
+    """use updater: += with int keys"""
-    str_kv._set_updater(updater)
+    str_kv._set_updater(str_updater)
-        dns_val = mx.nd.ones(shape) * 2
+    def check_ignored_pull_single(kv, key):
-    def check_invalid_list_kv_pair(kv, key):
+
-            pass
+
-    check_invalid_list_kv_pair(str_kv, str_keys)
+    kvs = [int_kv, str_kv]
-    test_row_sparse_pull()
+    import nose
-# Use different verison of SymbolBase
+try:
-
+    @indices.setter
-            "Please use `cast_storage` to create BaseSparseNDArray from an NDArray"
+        assert(source_array.stype != 'default'), \
-    return arr
+        raise ValueError("Unexpected source_array type: ", type(source_array))
-        (default values depend on the storage type)
+        An optional list of types of the aux data for RowSparseNDArray or CSRNDArray.
-        (default values depend on the storage type)
+        An optional list of types of the aux data for RowSparseNDArray or CSRNDArray.
-        (default values depend on the storage type)
+        An optional list of types of the aux data for RowSparseNDArray or CSRNDArray.
-    if isinstance(source_array, NDArray) and source_array.stype != 'default':
+    if spsp is not None and isinstance(source_array, spsp.csr.csr_matrix):
-    csr = sp.rand(num_rows, num_cols, density, dtype=dtype, format="csr")
+    from scipy import sparse as spsp
-    return rnd.randint(1, dim+1, size=n)
+def rand_shape_nd(num_dim, dim=10):
-                              (num_samples, feature_dim))
+    # generate some random csr data
-        shape = rand_shape_2d(dim0, dim1)
+    def check_create_csr_from_nd(shape, density):
-           "true_divide", "waitall", "_new_empty_handle"]
+           "multiply", "not_equal", "onehot_encode", "power", "subtract", "true_divide",
-            cost_sparse = measure_cost(num_repeat, False, False, mx.nd.dot, csr_data, weight, transpose_a=transpose)
+            cost_sparse = measure_cost(num_repeat, False, False, mx.nd.sparse.dot, csr_data, weight, transpose_a=transpose)
-        dot_func_sparse = mx.nd.dot if fw == "mxnet" else sp.spmatrix.dot
+        dot_func_sparse = mx.nd.sparse.dot if fw == "mxnet" else sp.spmatrix.dot
-                    help='logging level [debug, info, error]')
+parser.add_argument('--sparse-log-level', type=str, default='INFO',
-     embed = mx.symbol.dot(x, w)
+     embed = mx.symbol.sparse.dot(x, w)
-    log_level = args.log_level
+    log_level = args.sparse_log_level
-    if rank != 0:
+    if log_level == 'ERROR':
-        super(FashionMNIST, self).__init__(root, train, transform)
+        super(MNIST, self).__init__(root, train, transform) # pylint: disable=bad-super-call
-            weight = mx.nd.random_uniform(low=0, high=1, shape=weight_shape)
+            weight = mx.nd.random.uniform(low=0, high=1, shape=weight_shape)
-        weight = mx.nd.random_uniform(low=0, high=1, shape=(k, m))
+        weight = mx.nd.random.uniform(low=0, high=1, shape=(k, m))
-        dns = mx.nd.random_uniform(shape=(k, n)).copyto(ctx)
+        dns = mx.nd.random.uniform(shape=(k, n)).copyto(ctx)
-        dns = mx.nd.random_uniform(shape=(m, n)).copyto(ctx)
+        dns = mx.nd.random.uniform(shape=(m, n)).copyto(ctx)
-    loss = mx.contrib.sym.ctc_loss(data=pred_ctc, label=label)
+    loss = mx.sym.contrib.ctc_loss(data=pred_ctc, label=label)
-        noise = mx.nd.random_normal(0, 1, shape=(opt.batch_size, nz, 1, 1), ctx=ctx)
+        noise = mx.nd.random.normal(0, 1, shape=(opt.batch_size, nz, 1, 1), ctx=ctx)
-            self.transitions = nd.random_normal(shape=(self.tagset_size, self.tagset_size))
+            self.transitions = nd.random.normal(shape=(self.tagset_size, self.tagset_size))
-                nd.random_normal(shape=(2, 1, self.hidden_dim // 2))]
+        return [nd.random.normal(shape=(2, 1, self.hidden_dim // 2)),
-        rois = mx.contrib.symbol.Proposal(
+        rois = mx.symbol.contrib.Proposal(
-        rois = mx.contrib.symbol.Proposal(
+        rois = mx.symbol.contrib.Proposal(
-        group = mx.contrib.symbol.Proposal(
+        group = mx.symbol.contrib.Proposal(
-        rois = mx.contrib.symbol.Proposal(
+        rois = mx.symbol.contrib.Proposal(
-        rois = mx.contrib.symbol.Proposal(
+        rois = mx.symbol.contrib.Proposal(
-            clip=clip, name="{}_anchors".format(from_name), steps=step)
+        anchors = mx.symbol.contrib.MultiBoxPrior(from_layer, sizes=size_str, ratios=ratio_str,
-    tmp = mx.contrib.symbol.MultiBoxTarget(
+    tmp = mx.symbol.contrib.MultiBoxTarget(
-    det = mx.contrib.symbol.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
+    det = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
-    out = mx.contrib.symbol.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
+    out = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
-    tmp = mx.contrib.symbol.MultiBoxTarget(
+    tmp = mx.symbol.contrib.MultiBoxTarget(
-    det = mx.contrib.symbol.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
+    det = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
-    out = mx.contrib.symbol.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
+    out = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
-    tmp = mx.contrib.symbol.MultiBoxTarget(
+    tmp = mx.symbol.contrib.MultiBoxTarget(
-    det = mx.contrib.symbol.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
+    det = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
-    out = mx.contrib.symbol.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
+    out = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
-            symbol_string += '%s = mx.contrib.symbol.MultiBoxPrior(%s, sizes=%s, ratios=%s, clip=%s, steps=%s, name="%s")\n' % \
+            symbol_string += '%s = mx.symbol.contrib.MultiBoxPrior(%s, sizes=%s, ratios=%s, clip=%s, steps=%s, name="%s")\n' % \
-            type_string = 'mx.contrib.symbol.MultiBoxDetection'
+            type_string = 'mx.symbol.contrib.MultiBoxDetection'
-from ..base import NDArrayHandle, OpHandle, CachedOpHandle
+from ..base import c_array, c_str
-                            stype=_STORAGE_TYPE_ID_TO_STR[out_stypes[0]])
+                            stype=out_stypes[0])
-                             stype=_STORAGE_TYPE_ID_TO_STR[out_stypes[i]])
+                             stype=out_stypes[i])
-                                stype=_STORAGE_TYPE_ID_TO_STR[out_stypes[0]])
+                                stype=out_stypes[0])
-                                 stype=_STORAGE_TYPE_ID_TO_STR[out_stypes[i]])
+                                 stype=out_stypes[i])
-from .symbol import _GRAD_REQ_MAP, Symbol
+from .ndarray import _GRAD_REQ_MAP
-from ..symbol import _GRAD_REQ_MAP
+from ..ndarray import NDArray, zeros_like, _GRAD_REQ_MAP
-    >>> x = mx.nd.random_normal(shape=(16, 3, 224, 224))
+    >>> x = mx.nd.random.normal(shape=(16, 3, 224, 224))
-    >>> input = mx.nd.random_uniform(shape=(5, 3, 10))
+    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
-    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
+    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
-    >>> input = mx.nd.random_uniform(shape=(5, 3, 10))
+    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
-    >>> c0 = mx.nd.random_uniform(shape=(3, 3, 100))
+    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
-    >>> input = mx.nd.random_uniform(shape=(5, 3, 10))
+    >>> input = mx.nd.random.uniform(shape=(5, 3, 10))
-    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
+    >>> h0 = mx.nd.random.uniform(shape=(3, 3, 100))
-from .op import CachedOp
+from . import _internal, contrib, linalg, random, sparse
-from . import sum, round, max, min, slice, abs # pylint: disable=redefined-builtin
+from . import op
-    np.int64   : 6,
+    None: -1,
-    6 : np.int64,
+    -1: None,
-    'csr'        : 2,
+    'undefined': _STORAGE_TYPE_UNDEFINED,
-    return _STORAGE_TYPE_ID_TO_STR[storage_type.value]
+    return storage_type.value
-            return broadcast_add(self, other, out=self)
+            return op.broadcast_add(self, other, out=self)
-            return broadcast_sub(self, other, out=self)
+            return op.broadcast_sub(self, other, out=self)
-            return broadcast_mul(self, other, out=self)
+            return op.broadcast_mul(self, other, out=self)
-            return broadcast_div(self, other, out=self)
+            return op.broadcast_div(self, other, out=self)
-            return broadcast_mod(self, other, out=self)
+            return op.broadcast_mod(self, other, out=self)
-            return slice(self, begin, end).reshape(oshape)
+            return op.slice(self, begin, end).reshape(oshape)
-        return zeros_like(self, *args, **kwargs)
+        return op.zeros_like(self, *args, **kwargs)
-        return ones_like(self, *args, **kwargs)
+        return op.ones_like(self, *args, **kwargs)
-        return broadcast_axes(self, *args, **kwargs)
+        return op.broadcast_axes(self, *args, **kwargs)
-        return repeat(self, *args, **kwargs)
+        return op.repeat(self, *args, **kwargs)
-        return pad(self, *args, **kwargs)
+        return op.pad(self, *args, **kwargs)
-        return swapaxes(self, *args, **kwargs)
+        return op.swapaxes(self, *args, **kwargs)
-        return split(self, *args, **kwargs)
+        return op.split(self, *args, **kwargs)
-        return slice(self, *args, **kwargs)
+        return op.slice(self, *args, **kwargs)
-        return slice_axis(self, *args, **kwargs)
+        return op.slice_axis(self, *args, **kwargs)
-        return take(self, *args, **kwargs)
+        return op.take(self, *args, **kwargs)
-        return one_hot(self, *args, **kwargs)
+        return op.one_hot(self, *args, **kwargs)
-        return pick(self, *args, **kwargs)
+        return op.pick(self, *args, **kwargs)
-        return sort(self, *args, **kwargs)
+        return op.sort(self, *args, **kwargs)
-        return topk(self, *args, **kwargs)
+        return op.topk(self, *args, **kwargs)
-        return argsort(self, *args, **kwargs)
+        return op.argsort(self, *args, **kwargs)
-        return argmax(self, *args, **kwargs)
+        return op.argmax(self, *args, **kwargs)
-        return argmin(self, *args, **kwargs)
+        return op.argmin(self, *args, **kwargs)
-        return clip(self, *args, **kwargs)
+        return op.clip(self, *args, **kwargs)
-        return abs(self, *args, **kwargs)
+        return op.abs(self, *args, **kwargs)
-        return sign(self, *args, **kwargs)
+        return op.sign(self, *args, **kwargs)
-        return flatten(self, *args, **kwargs)
+        return op.flatten(self, *args, **kwargs)
-        return expand_dims(self, *args, **kwargs)
+        return op.expand_dims(self, *args, **kwargs)
-        return tile(self, *args, **kwargs)
+        return op.tile(self, *args, **kwargs)
-        return transpose(self, *args, **kwargs)
+        return op.transpose(self, *args, **kwargs)
-        return flip(self, *args, **kwargs)
+        return op.flip(self, *args, **kwargs)
-        return sum(self, *args, **kwargs)
+        return op.sum(self, *args, **kwargs)
-        return nansum(self, *args, **kwargs)
+        return op.nansum(self, *args, **kwargs)
-        return prod(self, *args, **kwargs)
+        return op.prod(self, *args, **kwargs)
-        return nanprod(self, *args, **kwargs)
+        return op.nanprod(self, *args, **kwargs)
-        return mean(self, *args, **kwargs)
+        return op.mean(self, *args, **kwargs)
-        return max(self, *args, **kwargs)
+        return op.max(self, *args, **kwargs)
-        return min(self, *args, **kwargs)
+        return op.min(self, *args, **kwargs)
-        return norm(self, *args, **kwargs)
+        return op.norm(self, *args, **kwargs)
-        return round(self, *args, **kwargs)
+        return op.round(self, *args, **kwargs)
-        return rint(self, *args, **kwargs)
+        return op.rint(self, *args, **kwargs)
-        return fix(self, *args, **kwargs)
+        return op.fix(self, *args, **kwargs)
-        return floor(self, *args, **kwargs)
+        return op.floor(self, *args, **kwargs)
-        return ceil(self, *args, **kwargs)
+        return op.ceil(self, *args, **kwargs)
-        return trunc(self, *args, **kwargs)
+        return op.trunc(self, *args, **kwargs)
-            return broadcast_to(self.reshape(cur_shape), shape=shape)
+            return op.broadcast_to(self.reshape(cur_shape), shape=shape)
-            return broadcast_to(self, shape=tuple(shape))
+            return op.broadcast_to(self, shape=tuple(shape))
-        return _storage_type(self.handle)
+        return _STORAGE_TYPE_ID_TO_STR[_storage_type(self.handle)]
-        return transpose(self)
+        return op.transpose(self)
-        grad = zeros_like(self)  # pylint: disable=undefined-variable
+        grad = op.zeros_like(self)  # pylint: disable=undefined-variable
-        return cast_storage(self, stype=stype)
+        return op.cast_storage(self, stype=stype)
-    return transpose(tensor, axes)
+    return op.transpose(tensor, axes)
-        broadcast_add,
+        op.broadcast_add,
-        broadcast_sub,
+        op.broadcast_sub,
-        broadcast_mul,
+        op.broadcast_mul,
-        broadcast_div,
+        op.broadcast_div,
-        broadcast_mod,
+        op.broadcast_mod,
-        broadcast_power,
+        op.broadcast_power,
-        broadcast_maximum,
+        op.broadcast_maximum,
-        broadcast_minimum,
+        op.broadcast_minimum,
-        broadcast_equal,
+        op.broadcast_equal,
-        broadcast_not_equal,
+        op.broadcast_not_equal,
-        broadcast_greater,
+        op.broadcast_greater,
-        broadcast_greater_equal,
+        op.broadcast_greater_equal,
-        broadcast_lesser,
+        op.broadcast_lesser,
-        broadcast_lesser_equal,
+        op.broadcast_lesser_equal,
-        from .._ctypes.ndarray import NDArrayBase, _STORAGE_TYPE_ID_TO_STR
+        from .._ctypes.ndarray import NDArrayBase
-        from .._cy3.ndarray import NDArrayBase, _imperative_invoke, _STORAGE_TYPE_ID_TO_STR
+        from .._cy3.ndarray import NDArrayBase, _imperative_invoke
-        from .._cy2.ndarray import NDArrayBase, _imperative_invoke, _STORAGE_TYPE_ID_TO_STR
+        from .._cy2.ndarray import NDArrayBase, _imperative_invoke
-    from .._ctypes.ndarray import NDArrayBase, _imperative_invoke, _STORAGE_TYPE_ID_TO_STR
+    from .._ctypes.ndarray import NDArrayBase, _imperative_invoke
-from ..base import mx_uint, check_call, _LIB, py_str, OpHandle, c_str, _Null
+from ..base import mx_uint, check_call, _LIB, py_str, _init_op_module, _Null
-_init_ndarray_module("mxnet")
+_init_op_module('mxnet', 'ndarray', _make_ndarray_function)
-from . import slice as nd_slice
+from . import op
-        >>> a = mx.nd.csr_matrix(data, indptr, indices, (3, 3))
+        >>> a = mx.nd.sparse.csr_matrix(data, indptr, indices, (3, 3))
-                return nd_slice(self, begin=begin, end=end)
+                return op.slice(self, begin=begin, end=end)
-        return cast_storage(self, stype=stype)
+        return op.cast_storage(self, stype=stype)
-        return cast_storage(self, stype=stype)
+        return op.cast_storage(self, stype=stype)
-    >>> a = mx.nd.csr_matrix([1, 2, 3], [0, 1, 2, 2, 3], [1, 0, 2], (4, 3))
+    >>> a = mx.nd.sparse.csr_matrix([1, 2, 3], [0, 1, 2, 2, 3], [1, 0, 2], (4, 3))
-    >>> a = mx.nd.row_sparse_array([[1, 2], [3, 4]], [1, 4], (6, 2))
+    >>> a = mx.nd.sparse.row_sparse_array([[1, 2], [3, 4]], [1, 4], (6, 2))
-    if stype is None:
+def _ndarray_cls(handle, writable=True, stype=_STORAGE_TYPE_UNDEFINED):
-    if stype == 'default':
+    if stype == _STORAGE_TYPE_DEFAULT:
-    elif stype == 'csr':
+    elif stype == _STORAGE_TYPE_CSR:
-    elif stype == 'row_sparse':
+    elif stype == _STORAGE_TYPE_ROW_SPARSE:
-               "Please use `cast_storage` to create BaseSparseNDArray from an NDArray"
+        assert(source_array.stype != 'default'),\
-        raise NotImplementedError('creating BaseSparseNDArray from ' \
+        raise NotImplementedError('creating BaseSparseNDArray from '
-    >>> print(mx.nd.random_normal(shape=(2,2)).asnumpy())
+    >>> print(mx.nd.random.normal(shape=(2,2)).asnumpy())
-    >>> print(mx.nd.random_normal(shape=(2,2)).asnumpy())
+    >>> print(mx.nd.random.normal(shape=(2,2)).asnumpy())
-    >>> print(mx.nd.random_normal(shape=(2,2)).asnumpy())
+    >>> print(mx.nd.random.normal(shape=(2,2)).asnumpy())
-    >>> print(mx.nd.random_normal(shape=(2,2)).asnumpy())
+    >>> print(mx.nd.random.normal(shape=(2,2)).asnumpy())
-from . import _internal, sparse, op
+from . import _internal, contrib, linalg, random, sparse
-from ..ndarray import _GRAD_REQ_MAP
+# pylint: enable=wildcard-import
-from mxnet.symbol_doc import _build_doc
+from ..base import mx_uint, check_call, _LIB, py_str
-from ..base import _Null
+from ..base import _Null, _init_op_module
-_init_symbol_module("mxnet")
+_init_op_module('mxnet', 'symbol', _make_atomic_symbol_function)
-from . import sum, round, max, min, slice, abs # pylint: disable=redefined-builtin
+from . import _internal
-        arg_arrays = [_ndarray_cls(NDArrayHandle(in_arg_handles[i])) \
+        arg_arrays = [_ndarray_cls(NDArrayHandle(in_arg_handles[i]))
-        return reshape(self, *args, **kwargs)
+        return op.reshape(self, *args, **kwargs)
-        return cast(self, *args, **kwargs)
+        return op.cast(self, *args, **kwargs)
-        return zeros_like(self, *args, **kwargs)
+        return op.zeros_like(self, *args, **kwargs)
-        return ones_like(self, *args, **kwargs)
+        return op.ones_like(self, *args, **kwargs)
-        return broadcast_axes(self, *args, **kwargs)
+        return op.broadcast_axes(self, *args, **kwargs)
-        return repeat(self, *args, **kwargs)
+        return op.repeat(self, *args, **kwargs)
-        return pad(self, *args, **kwargs)
+        return op.pad(self, *args, **kwargs)
-        return swapaxes(self, *args, **kwargs)
+        return op.swapaxes(self, *args, **kwargs)
-        return split(self, *args, **kwargs)
+        return op.split(self, *args, **kwargs)
-        return slice(self, *args, **kwargs)
+        return op.slice(self, *args, **kwargs)
-        return slice_axis(self, *args, **kwargs)
+        return op.slice_axis(self, *args, **kwargs)
-        return take(self, *args, **kwargs)
+        return op.take(self, *args, **kwargs)
-        return one_hot(self, *args, **kwargs)
+        return op.one_hot(self, *args, **kwargs)
-        return pick(self, *args, **kwargs)
+        return op.pick(self, *args, **kwargs)
-        return sort(self, *args, **kwargs)
+        return op.sort(self, *args, **kwargs)
-        return topk(self, *args, **kwargs)
+        return op.topk(self, *args, **kwargs)
-        return argsort(self, *args, **kwargs)
+        return op.argsort(self, *args, **kwargs)
-        return argmax(self, *args, **kwargs)
+        return op.argmax(self, *args, **kwargs)
-        return argmin(self, *args, **kwargs)
+        return op.argmin(self, *args, **kwargs)
-        return clip(self, *args, **kwargs)
+        return op.clip(self, *args, **kwargs)
-        return abs(self, *args, **kwargs)
+        return op.abs(self, *args, **kwargs)
-        return sign(self, *args, **kwargs)
+        return op.sign(self, *args, **kwargs)
-        return flatten(self, *args, **kwargs)
+        return op.flatten(self, *args, **kwargs)
-        return expand_dims(self, *args, **kwargs)
+        return op.expand_dims(self, *args, **kwargs)
-        return broadcast_to(self, *args, **kwargs)
+        return op.broadcast_to(self, *args, **kwargs)
-        return tile(self, *args, **kwargs)
+        return op.tile(self, *args, **kwargs)
-        return transpose(self, *args, **kwargs)
+        return op.transpose(self, *args, **kwargs)
-        return flip(self, *args, **kwargs)
+        return op.flip(self, *args, **kwargs)
-        return sum(self, *args, **kwargs)
+        return op.sum(self, *args, **kwargs)
-        return nansum(self, *args, **kwargs)
+        return op.nansum(self, *args, **kwargs)
-        return prod(self, *args, **kwargs)
+        return op.prod(self, *args, **kwargs)
-        return nanprod(self, *args, **kwargs)
+        return op.nanprod(self, *args, **kwargs)
-        return mean(self, *args, **kwargs)
+        return op.mean(self, *args, **kwargs)
-        return max(self, *args, **kwargs)
+        return op.max(self, *args, **kwargs)
-        return min(self, *args, **kwargs)
+        return op.min(self, *args, **kwargs)
-        return norm(self, *args, **kwargs)
+        return op.norm(self, *args, **kwargs)
-        return round(self, *args, **kwargs)
+        return op.round(self, *args, **kwargs)
-        return rint(self, *args, **kwargs)
+        return op.rint(self, *args, **kwargs)
-        return fix(self, *args, **kwargs)
+        return op.fix(self, *args, **kwargs)
-        return floor(self, *args, **kwargs)
+        return op.floor(self, *args, **kwargs)
-        return ceil(self, *args, **kwargs)
+        return op.ceil(self, *args, **kwargs)
-        return trunc(self, *args, **kwargs)
+        return op.trunc(self, *args, **kwargs)
-    sym = mx.contrib.sym.count_sketch(name='countsketch',out_dim = out_dim)
+    sym = mx.sym.contrib.count_sketch(name='countsketch',out_dim = out_dim)
-    sym = mx.contrib.sym.ifft(name='ifft', compute_size = 128)
+    sym = mx.sym.contrib.ifft(name='ifft', compute_size = 128)
-    sym = mx.contrib.sym.fft(name='fft', compute_size = 128)
+    sym = mx.sym.contrib.fft(name='fft', compute_size = 128)
-    sym = mx.contrib.sym.PSROIPooling(spatial_scale=0.0625, output_dim=2, pooled_size=3, name='psroipool')
+    sym = mx.sym.contrib.PSROIPooling(spatial_scale=0.0625, output_dim=2, pooled_size=3, name='psroipool')
-    sym = mx.contrib.sym.DeformablePSROIPooling(spatial_scale=0.0625, sample_per_part=4, group_size=3, pooled_size=3,
+    sym = mx.sym.contrib.DeformablePSROIPooling(spatial_scale=0.0625, sample_per_part=4, group_size=3, pooled_size=3,
-    sym = mx.contrib.sym.DeformableConvolution(num_filter=3, kernel=(3,3), name='deformable_conv')
+    sym = mx.sym.contrib.DeformableConvolution(num_filter=3, kernel=(3,3), name='deformable_conv')
-    sym = mx.contrib.sym.DeformableConvolution(num_filter=3, kernel=(3,3), pad=(1,1), name='deformable_conv')
+    sym = mx.sym.contrib.DeformableConvolution(num_filter=3, kernel=(3,3), pad=(1,1), name='deformable_conv')
-    sym = mx.contrib.sym.DeformableConvolution(num_filter=3, kernel=(3,3), stride=(2,2), name='deformable_conv')
+    sym = mx.sym.contrib.DeformableConvolution(num_filter=3, kernel=(3,3), stride=(2,2), name='deformable_conv')
-    sym = mx.contrib.sym.DeformableConvolution(num_filter=3, kernel=(3,3), dilate=(2,2), name='deformable_conv')
+    sym = mx.sym.contrib.DeformableConvolution(num_filter=3, kernel=(3,3), dilate=(2,2), name='deformable_conv')
-    sym = mx.contrib.sym.DeformableConvolution(num_filter=4, kernel=(3,3), num_deformable_group=2,
+    sym = mx.sym.contrib.DeformableConvolution(num_filter=4, kernel=(3,3), num_deformable_group=2,
-    x = mx.nd.random_uniform(shape=(10,))
+    x = mx.nd.random.uniform(shape=(10,))
-    y = mx.nd.random_uniform(shape=(10,))
+    y = mx.nd.random.uniform(shape=(10,))
-    x = mx.nd.random_uniform(shape=(128, 33, 64))
+    x = mx.nd.random.uniform(shape=(128, 33, 64))
-    x = mx.nd.random_uniform(shape=(128, 33, 64))
+    x = mx.nd.random.uniform(shape=(128, 33, 64))
-        model(mx.nd.random_uniform(shape=data_shape)).wait_to_read()
+        model(mx.nd.random.uniform(shape=data_shape)).wait_to_read()
-                                       mx.nd.random_uniform(5, 15, dshape2)],
+    data_batch = mx.io.DataBatch(data=[mx.nd.random.uniform(0, 9, dshape1),
-                                       mx.nd.random_uniform(5, 15, dshape2)],
+    data_batch = mx.io.DataBatch(data=[mx.nd.random.uniform(0, 9, dshape1),
-                                       mx.nd.random_uniform(10, 25, dshape2)],
+    data_batch = mx.io.DataBatch(data=[mx.nd.random.uniform(3, 5, dshape1),
-                                       mx.nd.random_uniform(5, 15, dshape2)],
+    data_batch = mx.io.DataBatch(data=[mx.nd.random.uniform(0, 9, dshape1),
-                                       mx.nd.random_uniform(15, 25, dshape2)],
+    data_batch = mx.io.DataBatch(data=[mx.nd.random.uniform(0, 9, dshape1),
-                                            mx.nd.random_uniform(15, 25, dataset_shape2)],
+    eval_dataiter = mx.io.NDArrayIter(data=[mx.nd.random.uniform(0, 9, dataset_shape1),
-                                            mx.nd.random_uniform(15, 25, dataset_shape2)])
+    pred_dataiter = mx.io.NDArrayIter(data=[mx.nd.random.uniform(0, 9, dataset_shape1),
-    A = mx.nd.random_uniform(shape=shape)
+    A = mx.nd.random.uniform(shape=shape)
-                                        g_out * a ** b * np.log(a)), gen_broadcast_data)
+                                                         g_out * a ** b * np.log(a)), gen_broadcast_data)
-    ctc = mx.contrib.sym.ctc_loss(in_var, labels_var)
+    ctc = mx.sym.contrib.ctc_loss(in_var, labels_var)
-    a_ = mx.contrib.nd.dequantize(qa, min1, max1, out_type='float32')
+    qa, min1, max1 = mx.nd.contrib.quantize(a, min0, max0, out_type='uint8')
-                    op = mx.contrib.sym.PSROIPooling(data=im_data_var, rois=rois_data_var, spatial_scale=spatial_scale,
+                    op = mx.sym.contrib.PSROIPooling(data=im_data_var, rois=rois_data_var, spatial_scale=spatial_scale,
-                        op = mx.contrib.sym.DeformableConvolution(name='test_op', data=im_data_var,
+                        op = mx.sym.contrib.DeformableConvolution(name='test_op', data=im_data_var,
-                    op = mx.contrib.sym.DeformablePSROIPooling(data=im_data_var, rois=rois_data_var,
+                    op = mx.sym.contrib.DeformablePSROIPooling(data=im_data_var, rois=rois_data_var,
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7)
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha = 4, beta = 7)
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_a = 1, transpose_b = 1)
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_a = 1, transpose_b = 1)
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_a = 1)
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_a = 1)
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_b = 1)
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_b = 1)
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7)
+    test_gemm = mx.sym.linalg.gemm(data1, data2, data3, alpha = 4, beta = 7)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4)
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha = 4)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4, transpose_a = 1, transpose_b = 1)
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha = 4, transpose_a = 1, transpose_b = 1)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4, transpose_a = 1)
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha = 4, transpose_a = 1)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4, transpose_b = 1)
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha = 4, transpose_b = 1)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4)
+    test_gemm = mx.sym.linalg.gemm2(data1, data2, alpha = 4)
-    test_potrf = mx.sym.linalg_potrf(data1)
+    test_potrf = mx.sym.linalg.potrf(data1)
-    test_potri = mx.sym.linalg_potri(data1)
+    test_potri = mx.sym.linalg.potri(data1)
-    test_trsm = mx.sym.linalg_trsm(data1,data2,alpha = 7)
+    test_trsm = mx.sym.linalg.trsm(data1,data2,alpha = 7)
-    test_trmm = mx.sym.linalg_trmm(data1,data2,alpha = 7, transpose = 1, rightside = 1)
+    test_trmm = mx.sym.linalg.trmm(data1,data2,alpha = 7, transpose = 1, rightside = 1)
-    test_sumlogdiag = mx.sym.linalg_sumlogdiag(data1)
+    test_sumlogdiag = mx.sym.linalg.sumlogdiag(data1)
-    test_trsm2 = mx.sym.linalg_trsm(data1,data2,alpha = -2, rightside = 1, transpose = 1)
+    test_trsm2 = mx.sym.linalg.trsm(data1,data2,alpha = -2, rightside = 1, transpose = 1)
-    test_trsm3 = mx.sym.linalg_trsm(data1,data2,alpha = 0.50, transpose = 1)
+    test_trsm3 = mx.sym.linalg.trsm(data1,data2,alpha = 0.50, transpose = 1)
-    test_trsm4 = mx.sym.linalg_trsm(data1,data2,alpha = -0.5, rightside = 1)
+    test_trsm4 = mx.sym.linalg.trsm(data1,data2,alpha = -0.5, rightside = 1)
-    test_trmm2 = mx.sym.linalg_trmm(data1,data2,alpha = -2)
+    test_trmm2 = mx.sym.linalg.trmm(data1,data2,alpha = -2)
-    test_trmm3 = mx.sym.linalg_trmm(data1,data2,rightside = 1)
+    test_trmm3 = mx.sym.linalg.trmm(data1,data2,rightside = 1)
-    test_trmm4 = mx.sym.linalg_trmm(data1,data2,alpha = 1.2,transpose = 1)
+    test_trmm4 = mx.sym.linalg.trmm(data1,data2,alpha = 1.2,transpose = 1)
-            'symbol': mx.sym.random_normal,
+            'symbol': mx.sym.random.normal,
-            'symbol': mx.sym.random_uniform,
+            'symbol': mx.sym.random.uniform,
-                'symbol': mx.sym.random_gamma,
+                'symbol': mx.sym.random.gamma,
-                'symbol': mx.sym.random_exponential,
+                'symbol': mx.sym.random.exponential,
-                'symbol': mx.sym.random_poisson,
+                'symbol': mx.sym.random.poisson,
-                'symbol': mx.sym.random_negative_binomial,
+                'symbol': mx.sym.random.negative_binomial,
-                'symbol': mx.sym.random_generalized_negative_binomial,
+                'symbol': mx.sym.random.generalized_negative_binomial,
-    mx.nd.random_normal(shape=shape, out=out)
+    mx.nd.random.normal(shape=shape, out=out)
-    fns = [mx.nd.random_uniform, mx.nd.random_normal, mx.nd.random_gamma]
+    fns = [mx.nd.random.uniform, mx.nd.random.normal, mx.nd.random.gamma]
-# pylint: disable= too-many-lines, redefined-builtin, protected-access
+# pylint: disable=too-many-lines, protected-access
-from . import broadcast_sub, broadcast_div, broadcast_to, broadcast_equal, cast_storage
+from . import broadcast_sub, broadcast_div, broadcast_to, broadcast_axes, broadcast_equal
-from . import zeros_like, slice, broadcast_minimum, broadcast_maximum, broadcast_mod
+from . import zeros_like, ones_like, broadcast_minimum, broadcast_maximum, broadcast_mod
-# pylint: disable= no-member, protected-access, too-many-arguments
+# pylint: disable= no-member, protected-access, too-many-arguments, redefined-outer-name
-from . import _internal, reshape
+from . import _internal, reshape, transpose, zeros_like, ones_like, broadcast_axes, broadcast_to
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        ----------
+        Example
-        """Shorthand for mxnet.sym.reshape.
+    def reshape(self, *args, **kwargs):
-            from the length of the array and remaining dimensions.
+        The arguments are the same as for :py:func:`reshape`, with
-            A reshaped symbol.
+        The arguments are the same as for :py:func:`cast`, with
-        return reshape(self, shape=shape)
+        return slice_axis(self, *args, **kwargs)
-    ----------
+    Example
-    ----------
+    Example
-
+# pylint: disable=redefined-outer-name
-def check_symbol_consistency(sym1, sym2, ctx):
+def test_symbol_fluent():
-    mx.test_utils.check_consistency([sym1, sym2], ctx_list=[ctx, ctx])
+    mx.test_utils.check_consistency([sym1, sym2], ctx_list=[ctx, ctx],
-from utils import load_model
+from converter.utils import load_model
-from utils import load_model
+from converter.utils import load_model
-    src_url = "http://www.apache.org/dyn/closer.cgi/incubator/" \
+    src_url = "https://www.apache.org/dyn/closer.cgi/incubator/" \
-    pgp_url = "http://www.apache.org/dyn/closer.cgi/incubator/" \
+    pgp_url = "https://www.apache.org/dyn/closer.cgi/incubator/" \
-    sha_url = "http://www.apache.org/dyn/closer.cgi/incubator/" \
+    sha_url = "https://www.apache.org/dyn/closer.cgi/incubator/" \
-    md5_url = "http://www.apache.org/dyn/closer.cgi/incubator/" \
+    md5_url = "https://www.apache.org/dyn/closer.cgi/incubator/" \
-    def __init__(self, root='~/.mxnet/datasets/', train=True,
+    def __init__(self, root='~/.mxnet/datasets/mnist', train=True,
-                                  sha1_hash='2a80914081dc54586dbdf242f9805a6b8d2a15fc')
+            data, label = self._train_data, self._train_label
-                                  sha1_hash='763e7fa3757d93b0cdec073cef058b2004252c17')
+            data, label = self._test_data, self._test_label
-    def __init__(self, root='~/.mxnet/datasets/', train=True,
+    def __init__(self, root='~/.mxnet/datasets/cifar10', train=True,
-
+    assert len(gluon.data.vision.FashionMNIST(root='data')) == 60000
-    x = mx.nd.zeros((128, 1024, 1024), ctx=mx.gpu(0))
+    x = mx.nd.zeros((128, 512, 512), ctx=mx.gpu(0))
-        for i in range(50):
+        for i in range(200):
-    data_iter = mx.io.NDArrayIter(data, label, batch_size=10, label_name='label')
+    data_iter = mx.io.NDArrayIter(data, label, batch_size=10, label_name='label', shuffle=True)
-            eval_metric=mx.metric.Loss())
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 0.1, 'wd': 0.00045},
-    data_iter = mx.io.NDArrayIter(data, label, batch_size=10, label_name='label')
+    data_iter = mx.io.NDArrayIter(data, label, batch_size=10, label_name='label', shuffle=True)
-            initializer=mx.init.Uniform(0.5), eval_metric=mx.metric.Loss())
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 0.01},
-            return
+    def _check_and_get(self, arr_dict, ctx):
-        return self._data[ctx]
+        return self._check_and_get(self._data, ctx)
-        return list(self._data.values())
+        return self._check_and_get(self._data, list)
-        if self._grad is None:
+        if self._data is not None and self._grad is None:
-        return self._grad[ctx]
+        return self._check_and_get(self._grad, ctx)
-        return list(self._grad.values())
+        if self._data is not None and self._grad is None:
-        return list(self._data.keys())
+        return self._ctx_list
-            label = label.asnumpy().astype('int32')
+            label = label.astype('int32')
-            self.num_inst += len(pred_label.flat)
+            self.sum_metric += ndarray.sum(label == pred_label).asscalar()
-    as `CSVIter, `ImageRecordIter`, `MNISTIter`, etc. When initializing
+    as `CSVIter`, `ImageRecordIter`, `MNISTIter`, etc. When initializing
-    has_momentum = {'sgd', 'dcasgd'}
+    has_momentum = {'sgd', 'dcasgd', 'nag'}
-        return np_index.copy()
+        if index_size.value:
-                inds[:-extra] = np.reshape(inds_[row_perm, :], (-1,))
+                inds = np.reshape(inds[:], (-1, self.batch_size))
-    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': opt.lr, 'wd': opt.wd})
+    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': opt.lr, 'wd': opt.wd},
-        mod.fit(train_data, num_epoch=opt.epochs, batch_end_callback = mx.callback.Speedometer(batch_size, 1))
+        mod.fit(train_data, num_epoch=opt.epochs, kvstore=opt.kvstore,
-        func : callable, default `symbol.zeros`
+        func : callable, default `ndarray.zeros`
-            states = self.begin_state(batch_size)
+            states = self.begin_state(batch_size, ctx=inputs.context)
-from . import zeros_like, slice
+from . import zeros_like, slice, broadcast_minimum, broadcast_maximum, broadcast_mod
-           "imdecode", "lesser", "lesser_equal", "maximum", "minimum", "moveaxis",
+           "imdecode", "lesser", "lesser_equal", "maximum", "minimum", "moveaxis", "modulo",
-
+
-def check_binary_op_forward(symbol, baseline, gen_data, rtol=1e-3, atol=1e-5):
+
-        y = symbol.bind(default_context(), args={'a': mx.nd.array(d[0]), 'b' : mx.nd.array(d[1])})
+        y = symbol.bind(default_context(), args={'a': mx.nd.array(d[0]), 'b': mx.nd.array(d[1])})
-        y = symbol.bind(default_context(), args={'a': mx.nd.array(d[0]), 'b' : mx.nd.array(d[1])},
+        y = symbol.bind(default_context(), args={'a': mx.nd.array(d[0]), 'b': mx.nd.array(d[1])},
-        check_binary_op_forward(c, lambda a, b: a + b, gen_broadcast_data)
+        check_binary_op_forward(c, lambda a, b: a + b, gen_broadcast_data, mx_nd_func=mx.nd.add)
-        check_binary_op_forward(c, lambda a, b: a - b, gen_broadcast_data)
+        check_binary_op_forward(c, lambda a, b: a - b, gen_broadcast_data, mx_nd_func=mx.nd.subtract)
-        check_binary_op_forward(c, lambda a, b: a * b, gen_broadcast_data)
+        check_binary_op_forward(c, lambda a, b: a * b, gen_broadcast_data, mx_nd_func=mx.nd.multiply)
-        check_binary_op_forward(c, lambda a, b: a / b, gen_broadcast_data)
+        check_binary_op_forward(c, lambda a, b: a / b, gen_broadcast_data, mx_nd_func=mx.nd.divide)
-        check_binary_op_forward(c, lambda a, b: a % b, gen_broadcast_data, atol=1)
+        check_binary_op_forward(c, lambda a, b: a % b, gen_broadcast_data, atol=1, mx_nd_func=mx.nd.modulo)
-        check_binary_op_forward(c, lambda a, b: a % b, gen_broadcast_data_int)
+        check_binary_op_forward(c, lambda a, b: a % b, gen_broadcast_data_int, mx_nd_func=mx.nd.modulo)
-        check_binary_op_forward(c, lambda a, b: a ** b, gen_broadcast_data)
+        check_binary_op_forward(c, lambda a, b: a ** b, gen_broadcast_data, mx_nd_func=mx.nd.power)
-        check_binary_op_forward(c, lambda a, b: (a == b).astype(a.dtype), gen_broadcast_data_int)
+        check_binary_op_forward(c, lambda a, b: (a == b).astype(a.dtype), gen_broadcast_data_int,
-
+
-def test_correlation():
+def test_correlation():
-
+
-
+# blacklist linear algebra headers when building without blas.
-            cur_param += int(node["attr"]["num_filter"])
+            if ("no_bias" in node["attr"]) and (node["attr"]["no_bias"] == 'True'):
-            cur_param = pre_filter * (int(node["attr"]["num_hidden"]) + 1)
+            if ("no_bias" in node["attr"]) and (node["attr"]["no_bias"] == 'True'):
-    """Just your regular densely-connected NN layer.
+    r"""Just your regular densely-connected NN layer.
-        A 2D input with shape `(batch_size, in_units)`.
+        An N-D input with shape
-    def __init__(self, units, activation=None, use_bias=True,
+    def __init__(self, units, activation=None, use_bias=True, flatten=True,
-                                   name='fwd')
+        act = F.FullyConnected(x, weight, bias, no_bias=bias is None, num_hidden=self._units,
-    model.add(nn.Dense(128, activation='tanh', in_units=10))
+    model.add(nn.Dense(128, activation='tanh', in_units=10, flatten=False))
-    model.add(nn.Dense(64, activation='tanh', in_units=128))
+    model.add(nn.Dense(64, activation='tanh', in_units=256))
-    x = model(mx.nd.zeros((32, 10)))
+    x = model(mx.nd.zeros((32, 2, 10)))
-from . import ndarray as nd
+_STORAGE_TYPE_ID_TO_STR = {
-    check_call(_LIB.MXImperativeInvoke(
+    # return output stypes to avoid the c_api call for checking
-        c_array(ctypes.c_char_p, [c_str(str(val)) for val in vals])))
+        c_array(ctypes.c_char_p, [c_str(str(val)) for val in vals]),
-        return _ndarray_cls(ctypes.cast(output_vars[0], NDArrayHandle))
+        return _ndarray_cls(ctypes.cast(output_vars[0], NDArrayHandle),
-        return [_ndarray_cls(ctypes.cast(output_vars[i], NDArrayHandle))
+        return [_ndarray_cls(ctypes.cast(output_vars[i], NDArrayHandle),
-        check_call(_LIB.MXInvokeCachedOp(
+        # return output stypes to avoid the c_api call for checking
-            ctypes.byref(output_vars)))
+            ctypes.byref(output_vars),
-            return _ndarray_cls(ctypes.cast(output_vars[0], NDArrayHandle))
+            return _ndarray_cls(ctypes.cast(output_vars[0], NDArrayHandle),
-            return [_ndarray_cls(ctypes.cast(output_vars[i], NDArrayHandle))
+            return [_ndarray_cls(ctypes.cast(output_vars[i], NDArrayHandle),
-        return [NDArray(NDArrayHandle(handles[i])) for i in range(out_size.value)]
+        num_output = out_size.value
-from .._ndarray_internal import _cvcopyMakeBorder as copyMakeBorder
+from ..ndarray._internal import _cvcopyMakeBorder as copyMakeBorder
-from .._ndarray_internal import _cvcopyMakeBorder as copyMakeBorder
+from ..ndarray import _internal
-        return NDArray(hdl, False)
+        return _ndarray_cls(hdl, False)
-        return NDArray(hdl, False)
+        return _ndarray_cls(hdl, False)
-        rhs = NDArray(NDArrayHandle(rhs_handle))
+        lhs = _ndarray_cls(NDArrayHandle(lhs_handle))
-                        update_on_kvstore):
+def _initialize_kvstore(kvstore, param_arrays, arg_params, param_names, update_on_kvstore):
-    for index, pair in enumerate(zip(param_arrays, grad_arrays)):
+    for i, pair in enumerate(zip(param_arrays, grad_arrays)):
-            # use a better solution latter
+            # use a better solution later
-        """Installs and initializes optimizers.
+        """Installs and initializes optimizers, as well as initialize kvstore for
-from .. import ndarray as nd
+from ..ndarray import zeros
-                nd.zeros(x[0].shape, dtype=x[0].dtype)
+                zeros(shape=x[0].shape, dtype=x[0].dtype, stype=x[0].stype)
-                nd.zeros(x[0].shape, dtype=x[0].dtype)
+                zeros(x[0].shape, dtype=x[0].dtype)
-
+
-# pylint: enable=unused-import
+from ..base import _LIB, numeric_types, integer_types
-
+_STORAGE_TYPE_STR_TO_ID = {
-
+
-        NDArray
+        NDArray, CSRNDArray, RowSparseNDArray
-def ones(shape, ctx=None, dtype=mx_real_t, **kwargs):
+def ones(shape, ctx=None, dtype=None, **kwargs):
-    <NDArray 3x2 @gpu(0)>
+
-
+
-            setattr(module_obj, function.__name__, function)
+def zeros(shape, ctx=None, dtype=None, **kwargs):
-_init_ndarray_module(NDArray, "mxnet")
+    Returns
-# add_fileline_to_docstring(__name__)
+    """
-    :class:`~mxnet.ndarray.sgd_mom_update`.
+    Sparse updating is supported. For details of the update algorithm see
-                momentum = zeros(weight.shape, weight.context, dtype=numpy.float32)
+                momentum = zeros(weight.shape, weight.context, dtype=numpy.float32,
-            momentum = zeros(weight.shape, weight.context, dtype=weight.dtype)
+            momentum = zeros(weight.shape, weight.context, dtype=weight.dtype, stype=weight.stype)
-                zeros(weight.shape, weight.context, dtype=weight.dtype))  # variance
+        return (zeros(weight.shape, weight.context, dtype=weight.dtype,
-                zeros(weight.shape, weight.context))  # delta
+                zeros(weight.shape, weight.context, stype=weight.stype),  # n
-            return (zeros(weight.shape, weight.context), )  # n
+            return (zeros(weight.shape, weight.context, stype=weight.stype),)  # n
-from ._ndarray_internal import _sample_gennegbinomial as generalized_negative_binomial
+from .ndarray._internal import _sample_uniform as uniform
-    from ._ctypes.symbol import _symbol_creator  # pylint: disable=unused-import
+from ..base import _LIB, numeric_types
-                    shared_arg_names=None, shared_exec=None, shared_buffer=None, **kwargs):
+    def simple_bind(self, ctx, grad_req='write', type_dict=None, stype_dict=None,
-            of the current executor is not found in `shared_arg_names`.
+            of the current executor is not found in `shared_arg_names`. The `NDArray`s are
-        grad_arrays = [NDArray(NDArrayHandle(arg_grad_handles[i]))
+        arg_arrays = [_ndarray_cls(NDArrayHandle(in_arg_handles[i])) \
-        aux_arrays = [NDArray(NDArrayHandle(aux_state_handles[i]))
+        aux_arrays = [_ndarray_cls(NDArrayHandle(aux_state_handles[i]))
-def var(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None, init=None, **kwargs):
+def var(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None,
-_init_symbol_module(Symbol, "mxnet")
+_set_symbol_class(Symbol)
-from .symbol import Symbol
+import numpy.random as rnd
-    location = {k: mx.nd.array(v, ctx=ctx) for k, v in location.items()}
+    location = {k: mx.nd.array(v, ctx=ctx) if isinstance(v, np.ndarray) \
-                           atol=None, grad_nodes=None, use_forward_train=True, ctx=None):
+                           atol=None, grad_nodes=None, use_forward_train=True, ctx=None,
-    aux_states : ist or tuple or dict, optional
+    aux_states : list or tuple or dict, optional
-        aux_states_npy = {k:v.asnumpy() for k, v in aux_states.items()}
+        aux_states_npy = {k: v.asnumpy() for k, v in aux_states.items()}
-    outputs = [x.asnumpy() for x in executor.outputs]
+    outputs = [x.asnumpy() for x in executor.outputs]
-                            aux_states=None, grad_req='write', ctx=None):
+                            aux_states=None, grad_req='write', ctx=None, grad_stypes=None):
-    args_grad_data = {k: mx.nd.array(v, ctx=ctx) for k, v in args_grad_npy.items()}
+    args_grad_data = {}
-    executor = sym.bind(ctx=ctx, args=location, args_grad=args_grad_data, aux_states=aux_states)
+    executor = sym.bind(ctx=ctx, args=location, args_grad=args_grad_data,
-def check_diff_to_scalar(A, x):
+def check_diff_to_scalar(A, x, rank=None):
-    assert(np.sum(np.abs((A - x).asnumpy())) == 0), A.asnumpy()
+    assert(np.sum(np.abs((A - x).asnumpy())) == 0), (rank, A.asnumpy(), x)
-keys = [3, 5, 7]
+keys = ['3', '5', '7']
-big_shape = (1200, 1200)        # big than BIGARRAY_BOUND
+shape = (2, 3)
-kv = mx.kv.create('dist_sync')
+def init_kv():
-kv.set_optimizer(mx.optimizer.create('test', rate))
+def test_sync_push_pull():
-nworker = kv.num_workers
+        num = (nworker + 1) * nworker * rate / 2 * nrepeat + 1
-    check_diff_to_scalar(val2, num)
+        val2 = mx.nd.zeros(big_shape)
-    autograd_assert(x, func=f_square, grad_func=f_square_grad)
+    def check_unary_func(x):
-    autograd_assert(x, y, func=f_compose, grad_func=f_compose_grad)
+    def check_binary_func(x, y):
-    assert (x.grad.asnumpy() == 2).all()
+    def check_attach_grad(x):
-    wt = mx._symbol_internal._identity_with_attr_like_rhs(wt, w)
+    wt = mx.symbol._internal._identity_with_attr_like_rhs(wt, w)
-def init_kv():
+def init_kv(stype='default'):
-    kv.init(3, mx.nd.zeros(shape))
+    kv.init(3, mx.nd.zeros(shape=shape, stype=stype))
-    kv.init(keys, [mx.nd.zeros(shape)] * len(keys))
+    kv.init(keys, [mx.nd.zeros(shape=shape, stype=stype)] * len(keys))
-def init_kv_with_str():
+def init_kv_with_str(stype='default'):
-    kv.init('a', mx.nd.zeros(shape))
+    kv.init('a', mx.nd.zeros(shape, stype=stype))
-    kv.init(str_keys, [mx.nd.zeros(shape)] * len(keys))
+    kv.init(str_keys, [mx.nd.zeros(shape=shape, stype=stype)] * len(keys))
-        kv.pull(key, out = val)
+        kv.pull(key, out=val)
-        kv.pull(key, out = val)
+        kv.pull(key, out=val)
-        kv.pull(key, out = vals)
+        kv.pull(key, out=vals)
-        kv.pull(key_list, out = vals)
+        kv.pull(key_list, out=vals)
-        kv.pull(key, out = vals)
+        kv.pull(key, out=vals)
-        kv.pull(key_list, out = vals)
+        kv.pull(key_list, out=vals)
-
+def test_invalid_pull():
-
+def test_factorization_machine_module():
-        beta[0] = 3
+    def check_batchnorm_training(stype):
-        rolling_std = np.random.uniform(size=s)
+            rolling_mean = np.random.uniform(size=s)
-        data = mx.symbol.Variable('data')
+            data = mx.symbol.Variable('data', stype=stype)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
+            test = mx.symbol.BatchNorm_v1(data, fix_gamma=True)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
+            test = mx.symbol.BatchNorm(data, fix_gamma=True)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
+            test = mx.symbol.BatchNorm_v1(data, fix_gamma=True, use_global_stats=True)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
+            test = mx.symbol.BatchNorm(data, fix_gamma=True, use_global_stats=True)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
+            test = mx.symbol.BatchNorm_v1(data, fix_gamma=False)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
+            test = mx.symbol.BatchNorm(data, fix_gamma=False)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
+            test = mx.symbol.BatchNorm_v1(data, fix_gamma=False, use_global_stats=True)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
+            test = mx.symbol.BatchNorm(data, fix_gamma=False, use_global_stats=True)
-                chaxis_true = dim + chaxis
+            # Test varying channel axis
-            shapex = shape
+                shapex = shape
-            data_tmp = np.random.normal(-0.1, 0.1, size=shapex)
+                channel_count = shapex[chaxis_true]
-            beta[0] = 3
+                gamma = np.ones(channel_count)
-            xrolling_std = np.random.uniform(size=channel_count)
+                test = mx.symbol.BatchNorm(data, fix_gamma=True, axis=chaxis)
-            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2, atol=0.01)
+                test = mx.symbol.BatchNorm(data, fix_gamma=True, use_global_stats=True, axis=chaxis)
-            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2, atol=0.01)
+                test = mx.symbol.BatchNorm(data, fix_gamma=False, axis=chaxis)
-            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2, atol=0.01)
+                test = mx.symbol.BatchNorm(data, fix_gamma=False, use_global_stats=True, axis=chaxis)
-            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2, atol=0.01)
+    stypes = ['row_sparse', 'default']
-    g2 = g1.copyto(default_context())
+def compare_optimizer(opt1, opt2, shape, dtype, w_stype='default', g_stype='default'):
-                assert(same(s1.asnumpy(), s2.asnumpy()))
+        if isinstance(state1, tuple):
-                assert_almost_equal(s1.asnumpy(), s2.asnumpy(), rtol=1e-4, atol=1e-5)
+        if isinstance(state1, tuple):
-                 decay_factor=(1 - 1e-8), **kwargs):
+                 decay_factor=(1 - 1e-8), sparse_update=False, **kwargs):
-
+        num_rows = weight.shape[0]
-        weight -= lr*mean/(mx.nd.sqrt(variance) + self.epsilon)
+        for row in range(num_rows):
-            'multi_precision': True}
+            'lr_scheduler': lr_scheduler}
-__version__ = "0.11.0"
+__version__ = "0.11.1"
-            # Fix git clone to specific tag
+            # Fix git clone and pip installation to specific tag
-    parser = argparse.ArgumentParser(description="train cifar10",
+    parser = argparse.ArgumentParser(description="train imagenet-1k",
-            batch_sampler = _sampler.BatchSampler(sampler, batch_size, last_batch)
+            batch_sampler = _sampler.BatchSampler(
-from ... import recordio
+from ... import recordio, ndarray
-        self._label = label
+        if isinstance(label, ndarray.NDArray) and len(label.shape) == 1:
-                                     allow_deferred_init=True)
+                                     allow_deferred_init=True,
-                                    allow_deferred_init=True)
+                                    allow_deferred_init=True,
-                                            allow_deferred_init=True)
+                                            allow_deferred_init=True,
-                                           allow_deferred_init=True)
+                                           allow_deferred_init=True,
-                 lr_mult=1.0, wd_mult=1.0, init=None, allow_deferred_init=False):
+                 lr_mult=1.0, wd_mult=1.0, init=None, allow_deferred_init=False,
-        self._deferred_init = ()
+    @property
-        for i in ctx:
+        for i in self._data:
-
+    def __getitem__(self, key):
-        out = layer(mx.nd.ones(shape=dshape))
+        out = layer(x)
-        out = layer(mx.nd.ones(shape=dshape))
+        out = layer(x)
-                    outf.write(outstr)
+            # Fix link
-from .. import symbol, ndarray
+from .. import ndarray
-            loss = symbol.square(output - label.reshape(()))
+        label = _reshape_label_as_output(F, output, label)
-            loss = ndarray.abs(output - label.reshape(output.shape))
+        label = _reshape_label_as_output(F, output, label)
-            loss = symbol.abs(output - label.reshape(()))
+            loss = -(F.log(output+1e-8)*label + F.log(1.-output+1e-8)*(1.-label))
-    """Computes the softmax cross entropy loss.
+    """Computes the softmax cross entropy loss. (alias: SoftmaxCELoss)
-       return
+    # enable numerical checking of gradients
-parser.add_argument('--root_url', type=str, default='https://mxnet.io',
+parser.add_argument('--root_url', type=str, default='https://mxnet.incubator.apache.org/',
-from _mxnet_converter import *
+# Licensed to the Apache Software Foundation (ASF) under one
-    builder.output_layers_is1d = [False for i in output_names]
+    builder.input_layers_is1d = [False for _ in input_names]
-    """Convert a keras model to the protobuf spec.
+
-        Provide keyword arguments of known shapes.
+        Provide keyword arguments for:
-        Protobuf representation of the model
+    model: A coreml model.
-        raise TypeError("Must provide input shape to be able to perform conversion")
+    if not isinstance(input_shape, dict):
-        input_dims  = map(remove_batch, kwargs.values())
+        input_names = input_shape.keys()
-        shapes = map(remove_batch, kwargs.values())
+        names = input_shape.keys()
-    shapes = net.infer_shape(**kwargs)
+    shapes = net.infer_shape(**input_shape)
-
+    builder = _neural_network.NeuralNetworkBuilder(input_features, output_features, mode)
-    for iter, node in enumerate(nodes):
+    for node in nodes:
-    for iter, node in enumerate(nodes):
+    for idx, node in enumerate(nodes):
-        print("%d : %s, %s" % (iter, name, op))
+        print("%d : %s, %s" % (idx, name, op))
-
+    if preprocessor_args is not None:
-    return spec
+    # Return the model
-        self._test_mxnet_model(net, engine, data = input_shape)
+# Licensed to the Apache Software Foundation (ASF) under one
-__version__ = "0.10.1"
+__version__ = "0.11.0"
-
+def test_spatial_transformer_with_type():
-          '.pm':'#', '.scala':'*', '.cc':'*', '.sh':'#', '.cmake':'#'}
+          '.pm':'#', '.scala':'*', '.cc':'*', '.sh':'#', '.cmake':'#',
-def process_file(fname, action, verbose=False):
+def process_file(fname, action, verbose=True):
-    if ext == '.h' or ext == '.cc' or ext == '.cu':
+    if ext == '.h' or ext == '.cc' or ext == '.cu' or ext == '.cpp' \
-from .base import mx_uint, NDArrayHandle, c_array
+from .base import mx_uint, NDArrayHandle, c_array, MXCallbackList, SymbolHandle
-from .symbol import _GRAD_REQ_MAP
+from .symbol import _GRAD_REQ_MAP, Symbol
-from ... import recordio, image
+from ... import recordio
-        return image.imdecode(img, self._flag), header.label
+import warnings
-from ... import nd
+from ... import nd, image, recordio
-        return self._transform(self._data[idx], self._label[idx])
+        if self._transform is not None:
-                 transform=lambda data, label: (data, label)):
+                 transform=None):
-                 transform=lambda data, label: (data, label)):
+                 transform=None):
-        src = self.augmenter(src)[0]
+        src = self.augmenter(src)
-            return None
+            return ()
-        return None
+        return ()
-            return None
+            return ()
-        return None
+        return ()
-                    batch_data[i][:] = self.postprocess_data(datum)
+                    batch_data[i] = self.postprocess_data(datum)
-    MXNet must have been built with OpenCV for `imdecode` to work.
+    MXNet must have been built with USE_OPENCV=1 for `imdecode` to work.
-def _get_interp_method(interp, sizes=None):
+def _get_interp_method(interp, sizes=()):
-        return [resize_short(src, self.size, self.interp)]
+        return resize_short(src, self.size, self.interp)
-        return [imresize(src, *self.size, interp=_get_interp_method(self.interp, sizes))]
+        return imresize(src, *self.size, interp=_get_interp_method(self.interp, sizes))
-        return [random_crop(src, self.size, self.interp)[0]]
+        return random_crop(src, self.size, self.interp)[0]
-        return [random_size_crop(src, self.size, self.min_area, self.ratio, self.interp)[0]]
+        return random_size_crop(src, self.size, self.min_area, self.ratio, self.interp)[0]
-        return [center_crop(src, self.size, self.interp)[0]]
+        return center_crop(src, self.size, self.interp)[0]
-            src = [j for i in src for j in t(i)]
+            src = t(src)
-        return [src]
+        return src
-        return [src]
+        return src
-        return [src]
+        return src
-        return [src]
+        return src
-        return [src]
+        return src
-        return [color_normalize(src, self.mean, self.std)]
+        return color_normalize(src, self.mean, self.std)
-        return [src]
+        return src
-        return [src]
+        return src
-        return [src]
+        return src
-                data = [self.imdecode(s)]
+                data = self.imdecode(s)
-                    i += 1
+                assert i < batch_size, 'Batch size must be multiples of augmenter output length'
-            data = [ret for src in data for ret in aug(src)]
+            data = aug(data)
-from .base import _LIB, check_call
+from .base import _LIB, check_call, MXCallbackList
-
+def test_function():
-        record.write_idx(i, s)
+        os.makedirs('data/test_images')
-    dataset = gluon.data.ImageRecordDataset(recfile)
+    dataset = gluon.data.vision.ImageRecordDataset(recfile)
-    def forward(self, inputs, states):
+    def forward(self, inputs, states=None):
-        return self._forward_cpu(inputs, states)
+            out = self._forward_gpu(inputs, states)
-        If `bidirectional` is True, state shape will instead be
+    Recurrent state:
-    Recurrent state shape:
+    Recurrent state:
-        If `bidirectional` is True, state shape will instead be
+        If `bidirectional` is True, each recurrent state will instead have shape
-        If `bidirectional` is True, state shape will instead be
+    Recurrent state:
-from ..utils import download
+from ..utils import download, check_sha1
-    def __init__(self, root, train=True, transform=lambda data, label: (data, label)):
+    def __init__(self, root='~/.mxnet/datasets/', train=True,
-            label_file = download(url+'train-labels-idx1-ubyte.gz', self._root)
+            data_file = download(url+'train-images-idx3-ubyte.gz', self._root,
-            label_file = download(url+'t10k-labels-idx1-ubyte.gz', self._root)
+            data_file = download(url+'t10k-images-idx3-ubyte.gz', self._root,
-    def __init__(self, root, train=True, transform=lambda data, label: (data, label)):
+    def __init__(self, root='~/.mxnet/datasets/', train=True,
-            tar.extractall(self._root)
+        file_paths = [(name, os.path.join(self._root, 'cifar-10-batches-bin/', name))
-from ...test_utils import download
+from ..utils import download, check_sha1
-
+    The local_dir directory will be created if it doesn't exist.
-        if verified(file_path, name):
+        if check_sha1(file_path, sha1_hash):
-             dirname=local_dir,
+             path=zip_file_path,
-    if verified(file_path, name):
+    if check_sha1(file_path, sha1_hash):
-.. code:: python
+.. code::
-.. code:: python
+.. code::
-Pretrained model is converted from torchvision.
+
-def download(url, path=None, overwrite=False):
+def check_sha1(filename, sha1_hash):
-        The filename of the downloaded file.
+        The file path of the downloaded file.
-    if overwrite or not os.path.exists(fname):
+    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):
-                       min_eject_coverage=0.3, max_attempts=50, pad_val=(128, 128, 128)):
+                       min_eject_coverage=0.3, max_attempts=50, pad_val=(127, 127, 127)):
-        shape for output data
+        Shape for output data
-        resize shorter edge if larger than 0 at the begining
+        Resize shorter edge if larger than 0 at the begining
-        whether apply horizontal flip to image with probability 0.5
+        Whether to apply horizontal flip to image with probability 0.5
-        mean pixel values for [r, g, b]
+        Mean pixel values for [r, g, b]
-        standard deviations for [r, g, b]
+        Standard deviations for [r, g, b]
-        brightness jittering range (percent)
+        Brightness jittering range (percent)
-        contrast jittering range
+        Contrast jittering range (percent)
-        saturation jittering range
+        Saturation jittering range (percent)
-        hue jittering range
+        Hue jittering range (percent)
-        pca noise level
+        Pca noise level (percent)
-        interpolation method for all resizing operations
+        Interpolation method for all resizing operations
-        pixel value to be filled when padding is enabled. pad_val will automatically
+        Pixel value to be filled when padding is enabled. pad_val will automatically
-        augmenter list for generating distorted images
+        Augmenter list for generating distorted images
-        name for detection labels
+        Name for detection labels
-                %(str(raw.shape, obj_width))
+                %(str(raw.shape), obj_width)
-            reshape the data_shape to the new shape if not None
+            Reshape the data_shape to the new shape if not None
-            reshape label shape to new shape if not None
+            Reshape label shape to new shape if not None
-        shape for output data
+        Shape for output data
-        [0, 1], probability to apply random resizing
+        Resize shorter edge if larger than 0 at the begining
-        [0, 1], probability to convert to grayscale for all channels
+        [0, 1], probability to convert to grayscale for all channels, the number
-        whether apply horizontal flip to image with probability 0.5
+        Whether to apply horizontal flip to image with probability 0.5
-        mean pixel values for [r, g, b]
+        Mean pixel values for [r, g, b]
-        standard deviations for [r, g, b]
+        Standard deviations for [r, g, b]
-        brightness jittering range (percent)
+        Brightness jittering range (percent)
-        contrast jittering range
+        Contrast jittering range (percent)
-        saturation jittering range
+        Saturation jittering range (percent)
-        hue jittering range
+        Hue jittering range (percent)
-        pca noise level
+        Pca noise level (percent)
-        interpolation method for all resizing operations
+        Interpolation method for all resizing operations
-        model = mx.mod.Module(symbol=sym, label_names=[arg_names[-1], ])
+        model = mx.mod.Module(symbol=sym, label_names=[prob_label(arg_names), ])
-    models = ['bvlc_googlenet']
+    models = ['bvlc_googlenet', 'vgg-16', 'resnet-50']
-    The cross entropy is given by
+    The cross entropy over a batch of sample size :math:`N` is given by
-        -y\\log \\hat{y} + (1-y)\\log (1-\\hat{y})
+       -\\sum_{n=1}^{N}\\sum_{k=1}^{K}t_{nk}\\log (y_{nk}),
-    'omp.h', 'execinfo.h', 'packet/sse-inl.h', 'emmintrin.h', 'thrust/device_vector.h', 
+    'omp.h', 'execinfo.h', 'packet/sse-inl.h', 'emmintrin.h', 'thrust/device_vector.h',
-ï»¿# -*- coding: utf-8 -*-
+# Licensed to the Apache Software Foundation (ASF) under one
-                    outf.write(outstr)
+                    outf.write(outstr)
-    print("Validation error:", ae_model.eval(val_X))
+# Licensed to the Apache Software Foundation (ASF) under one
-    return mse, ret
+    return mse, ret
-    
+
-    
+
-    for k in range(len(tks)):        
+    for k in range(len(tks)):
-    
+
-    
+
-        
+
-    last_states = [LSTMState(c = mx.sym.Variable("l0_init_c"), h = mx.sym.Variable("l0_init_h")), 
+    last_states = [LSTMState(c = mx.sym.Variable("l0_init_c"), h = mx.sym.Variable("l0_init_h")),
-        
+
-        
+
-from data import get_iterator 
+from data import get_iterator
-   
+
-    
+
-    return sm, ('data',), ('softmax_label',)  
+    return sm, ('data',), ('softmax_label',)
-    
+
-    main()
+# Licensed to the Apache Software Foundation (ASF) under one
-    return fcnxs_args, fcnxs_auxs
+# Licensed to the Apache Software Foundation (ASF) under one
-            logger.info('batch[%d] Validation-%s=%f', nbatch, name, value)
+# Licensed to the Apache Software Foundation (ASF) under one
-    return softmax
+# Licensed to the Apache Software Foundation (ASF) under one
-Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi		
+Contains the definition of the Inception Resnet V2 architecture.
-        
+
-        
+
-        
+
-        
+
-        
+
-                             name='stage%d_unit%d' % (i + 1, 1), bottle_neck=bottle_neck, num_group=num_group, 
+                             name='stage%d_unit%d' % (i + 1, 1), bottle_neck=bottle_neck, num_group=num_group,
-            
+
-                  num_group   = num_group, 
+                  num_group   = num_group,
-    
+
-    
+
-    
+
-        
+
-        
+
-    img_lst = pd.read_csv(test_lst_path,sep="/",header=None, nrows=1) 
+    img_lst = pd.read_csv(test_lst_path,sep="/",header=None, nrows=1)
-    
+
-     
+
-    
+
-   
+
-parser.add_argument('--load-epoch', type=int, 
+parser.add_argument('--load-epoch', type=int,
-parser.add_argument('--log-file', type=str, 
+parser.add_argument('--log-file', type=str,
-    
+
-        # bind max_len first 
+        # bind max_len first
-        
+
-        # we don't need to store the last state 
+        # we don't need to store the last state
-    buckets.reverse()    
+    buckets.reverse()
-    idx = seqidx 
+    idx = seqidx
-        for data_batch in X_train_batch:  
+        for data_batch in X_train_batch:
-              
+
-            set_rnn_inputs_from_batch(m, data_batch, batch_seq_length, batch_size)  
+            set_rnn_inputs_from_batch(m, data_batch, batch_seq_length, batch_size)
-            
+
-# is this function being used? 
+# is this function being used?
-# reuse the bucket_io library 
+# reuse the bucket_io library
-        devs = mx.cpu() 
+        devs = mx.cpu()
-        
+
-        
+
-            
+
-        
+
-    
+
-    
+
-                    num_label = num_label)    
+                    num_label = num_label)
-    
+
-    
+
-    
+
-    
+
-    
+
-                    num_label = num_label)    
+                    num_label = num_label)
-        
+
-    
+
-    
+
-    
+
-    
+
-    lr = mx.lr_scheduler.FactorScheduler(step=args.lr_sched_delay, 
+    lr = mx.lr_scheduler.FactorScheduler(step=args.lr_sched_delay,
-    
+
-    
+
-    
+
-    
+
-    mx.profiler.profiler_set_state('stop')
+    mx.profiler.profiler_set_state('stop')
-print(res)
+print(res)
-model.fit(X=train, eval_data=val, monitor=mon, 
+model.fit(X=train, eval_data=val, monitor=mon,
-    return keep
+    return keep
-        return m
+        return m
-        self.useSegm = None
+        self.useSegm = None
-        return _mask.toBbox([rleObjs])[0]
+        return _mask.toBbox([rleObjs])[0]
-    
+
-    However, the output of this layer is the original prediction -- same as 
+    However, the output of this layer is the original prediction -- same as
-    Cross-entropy loss provides a very large penalty for guessing 
+    Cross-entropy loss provides a very large penalty for guessing
-    net = mx.symbol.Custom(data=data, label=labs, name='ce', 
+    net = mx.symbol.Custom(data=data, label=labs, name='ce',
-    return mx.io.NDArrayIter(data={'user':user,'item':item},label={'score':score}, 
+    return mx.io.NDArrayIter(data={'user':user,'item':item},label={'score':score},
-        os.system("wget http://files.grouplens.org/datasets/movielens/%s.zip" % prefix) 
+        os.system("wget http://files.grouplens.org/datasets/movielens/%s.zip" % prefix)
-    return (load_mldata_iter('./%s/u1.base' % prefix, batch_size), 
+    return (load_mldata_iter('./%s/u1.base' % prefix, batch_size),
-    It only shuffles one of the input data columns, specified in the 
+    It only shuffles one of the input data columns, specified in the
-        """Takes a list of numpy arrays for data, 
+        """Takes a list of numpy arrays for data,
-        In this version, we have no weights to apply.  
+        In this version, we have no weights to apply.
-        
+
-            op_type='SparseRandomProjection', 
+    net = mx.symbol.Custom(indexes=data, values=vals, name='rproj',
-import randomproj 
+import randomproj
-    
+
-                final_score *= (1-D[i]) 
+                final_score *= (1-D[i])
-            self.obs, 
+            self.obs,
-    def define_exe(self, ctx, init, updater, input_shapes=None, args=None, 
+    def define_exe(self, ctx, init, updater, input_shapes=None, args=None,
-                
+
-        
+
-    def define_exe(self, ctx, init, updater, input_shapes=None, args=None, 
+    def define_exe(self, ctx, init, updater, input_shapes=None, args=None,
-        
+
-                
+
-        self, 
+        self,
-algo.train()
+algo.train()
-        
+
-                       self.action_space.low, 
+
-        raise NotImplementedError
+        raise NotImplementedError
-    for l, n in len_dict.items(): # TODO: There are better heuristic ways to do this    
+    for l, n in len_dict.items(): # TODO: There are better heuristic ways to do this
-            
+
-                         name='LSTM', 
+                         name='LSTM',
-        mod = mx.mod.BucketingModule(sym_gen, 
+        mod = mx.mod.BucketingModule(sym_gen,
-    for l, n in len_dict.items(): # TODO: There are better heuristic ways to do this    
+    for l, n in len_dict.items(): # TODO: There are better heuristic ways to do this
-            # Transpose data if model parallel 
+            # Transpose data if model parallel
-            # Model parallelism 
+
-            
+
-                
+
-        self.bucket_curr_idx = [0 for x in self.data]
+        self.bucket_curr_idx = [0 for x in self.data]
-                         name='LSTM', 
+                         name='LSTM',
-                         state_cell=rnn_c_init, 
+                         state_cell=rnn_c_init,
-    
+
-            sym = lstm_unroll(num_lstm_layer, seq_len, feat_dim, num_hidden=num_hidden, 
+            sym = lstm_unroll(num_lstm_layer, seq_len, feat_dim, num_hidden=num_hidden,
-            sym = lstm_unroll(num_lstm_layer, seq_len, feat_dim, num_hidden=num_hidden, 
+            sym = lstm_unroll(num_lstm_layer, seq_len, feat_dim, num_hidden=num_hidden,
-    
+
-# Copyright 2013    Yajie Miao    Carnegie Mellon University 
+# Licensed to the Apache Software Foundation (ASF) under one
-def _nnet2kaldi_maxout(nnet_spec, pool_size = 1, set_layer_num = -1, 
+def _nnet2kaldi_maxout(nnet_spec, pool_size = 1, set_layer_num = -1,
-    fout.close();
+    fout.close();
-		raise Exception(msg)
+		raise Exception(msg)
-            
+
-        
+
-        return a, ReadLabel(self.labelFile)
+        return a, ReadLabel(self.labelFile)
-            dt = numpy.dtype([('sample',(numpy.float32,dim))]) 
+            dt = numpy.dtype([('sample',(numpy.float32,dim))])
-            dt = numpy.dtype([('sample',(numpy.float32,sampSize/4))]) 
+            dt = numpy.dtype([('sample',(numpy.float32,sampSize/4))])
-            
+
-        
+
-                
+
-        
+
-"""
+"""
-    
+
-    if withfinal: 
+    if withfinal:
-        
+
-    utils.pickle_save(nnet_dict, filename)   
+    utils.pickle_save(nnet_dict, filename)
-        dict_key = str(i) + ' ' + activation + ' b' 
+        dict_key = str(i) + ' ' + activation + ' b'
-            layers[i].delta_params[1].set_value(np.asarray(string_2_array(nnet_dict[dict_key]), dtype=theano.config.floatX))            
+            dict_key = str(i) + ' ' + activation + ' db'
-            layers[i].params_carry[1].set_value(np.asarray(string_2_array(nnet_dict[dict_key]), dtype=theano.config.floatX))            
+            dict_key = str(i) + ' ' + activation + ' b_carry'
-            #layers[i].delta_params_carry[1].set_value(np.asarray(string_2_array(nnet_dict[dict_key]), dtype=theano.config.floatX))            
+            #dict_key = str(i) + ' ' + activation + ' db_carry'
-            layers[-1].params_carry[1].set_value(np.asarray(string_2_array(nnet_dict[dict_key]), dtype=theano.config.floatX))            
+            dict_key = 'logreg b_carry'
-            #layers[-1].delta_params_carry[1].set_value(np.asarray(string_2_array(nnet_dict[dict_key]), dtype=theano.config.floatX))            
+            #dict_key = 'logreg db_carry'
-       
+
-               dict_a = 'W ' + str(i) + ' ' + str(next_X) + ' ' + str(this_X) 
+               dict_a = 'W ' + str(i) + ' ' + str(next_X) + ' ' + str(this_X)
-    
+
-        conv_layer.W.set_value(W_array) 
+        conv_layer.W.set_value(W_array)
-        conv_layer.b.set_value(np.asarray(string_2_array(nnet_dict[dict_a]), dtype=theano.config.floatX)) 
+        conv_layer.b.set_value(np.asarray(string_2_array(nnet_dict[dict_a]), dtype=theano.config.floatX))
-        
+
-        b = self.output.load_next_block()        
+        b = self.output.load_next_block()
-        b = self.output.get_state()   
+        b = self.output.get_state()
-        self.output.set_state(state)        
+        self.output.set_state(state)
- 
+
-        
+
- 
+
- 
+
- 
+
-            
+
-            data_batch = SimpleBatch(data_names, 
+
-    
+
-        sm = mx.sym.SoftmaxOutput(data=pred, label=label, ignore_label=0, 
+        sm = mx.sym.SoftmaxOutput(data=pred, label=label, ignore_label=0,
-    
+
-    
+
-    
+
-	pass
+	pass
-    run_RBM(mnist_conf)    
+    run_RBM(mnist_conf)
-    run_SDA(mnist_conf)    
+    run_SDA(mnist_conf)
-    eval_DNN(mnist_conf)    
+    eval_DNN(mnist_conf)
-    batch_end_callbacks = [mx.callback.Speedometer(batch_size, 
+    batch_end_callbacks = [mx.callback.Speedometer(batch_size,
-            'train or predict or load can be the candidate for the mode')
+# Licensed to the Apache Software Foundation (ASF) under one
-            self._curr_module.save_optimizer_states(state_name)
+# Licensed to the Apache Software Foundation (ASF) under one
-            max_freq=self.max_freq, overwrite=overwrite, 
+            max_freq=self.max_freq, overwrite=overwrite,
-                               provide_label=self.provide_label)
+# Licensed to the Apache Software Foundation (ASF) under one
-    if (os.path.isfile(csvfilename) is False) or overwrite: 
+    if (os.path.isfile(csvfilename) is False) or overwrite:
-        return prob
+        return prob
-            
+
-    
+
-              # eval_metric = mx.metric.np(Accuracy_LCS), 
+              # eval_metric = mx.metric.np(Accuracy_LCS),
-                
+
-            
+
-        
+
-    
+
-# pylint: disable= arguments-differ
+# pylint: disable= arguments-differ, too-many-lines
-        Equivalent to the product of the arrayâs dimensions.
+        Equivalent to the product of the array's dimensions.
-        """Data-type of the arrayâs elements.
+        """Data-type of the array's elements.
-    assert_allclose(y.asnumpy(), np.exp(x.asnumpy()*5.0))
+    assert_allclose(y.asnumpy(), np.exp(x.asnumpy()*5.0))
-    
+
-    test_indexed_recordio()
+    test_indexed_recordio()
-  
+# Licensed to the Apache Software Foundation (ASF) under one
-  main()
+# Licensed to the Apache Software Foundation (ASF) under one
-new_model.save(args.save_model, 1)
+# Licensed to the Apache Software Foundation (ASF) under one
-  return dict(zip(conv_names, res))
+# Licensed to the Apache Software Foundation (ASF) under one
-  return new_model
+# Licensed to the Apache Software Foundation (ASF) under one
-            print('Did not find and list file with prefix %s'%args.prefix)
+# Licensed to the Apache Software Foundation (ASF) under one
-    
+
-                 op_name='Convolution', prefix=None, params=None, **kwargs):
+                 op_name='Convolution', adj=None, prefix=None, params=None):
-            self._kwargs.update(kwargs)
+            if adj is not None:
-def set_recording(is_recording):
+def set_recording(is_recording): #pylint: disable=redefined-outer-name
-    """Set status to training/not training. This affects ctx.is_train in operator
+def set_training(train_mode): #pylint: disable=redefined-outer-name
-    is_train=True while simply passing through if is_train=False.
+    train_mode=True while simply passing through if train_mode=False.
-    is_train: bool
+    train_mode: bool
-        ctypes.c_int(is_train), ctypes.byref(prev)))
+        ctypes.c_int(train_mode), ctypes.byref(prev)))
-class RecordingStateScope(object):
+    Returns
-        with RecordingStateScope(True, True):
+
-        self._prev_is_train = None
+    def __init__(self, is_record, train_mode): #pylint: disable=redefined-outer-name
-        self._prev_is_train = set_training(self._enter_is_train)
+        if self._enter_is_record is not None:
-            set_training(self._prev_is_train)
+        if self._enter_is_record is not None and self._prev_is_record != self._enter_is_record:
-    and captures training code.
+def record(train_mode=True): #pylint: disable=redefined-outer-name
-              should also use is_train=False, otherwise gradient is undefined.
+    .. note:: When forwarding with train_mode=False, the corresponding backward
-        Whether to do forward for training or inference.
+    train_mode: bool, default True
-    return RecordingStateScope(True, is_train)
+    return _RecordingStateScope(True, train_mode)
-    and captures testing code.
+def pause(train_mode=False): #pylint: disable=redefined-outer-name
-        Whether to do forward for training or inference.
+    train_mode: bool, default False
-    return RecordingStateScope(False, is_train)
+    return _RecordingStateScope(None, False)
-def backward(heads, head_grads=None, retain_graph=False, is_train=True):
+def backward(heads, head_grads=None, retain_graph=False, train_mode=True): #pylint: disable=redefined-outer-name
-        Whether to do backward for training or inference.
+    train_mode: bool, optional
-            ctypes.c_int(is_train)))
+            ctypes.c_int(train_mode)))
-        ctypes.c_int(is_train)))
+        ctypes.c_int(train_mode)))
-    def backward(self, out_grad=None, retain_graph=False, is_train=True):
+    def backward(self, out_grad=None, retain_graph=False, train_mode=True):
-        is_train : bool, optional
+        train_mode : bool, optional
-            ctypes.c_int(is_train)))
+            ctypes.c_int(train_mode)))
-    with record(True):
+    with record(train_mode=True):
-    with record(False):
+        with predict_mode():
-        y.backward(is_train=False)
+        y.backward(train_mode=False)
-        batch_size = states[0].shape[self._layout.find('N')]
+        batch_size = inputs.shape[self._layout.find('N')]
-    np.random.seed()
+    # TODO(szha): Seeding this masks failures. We need to do a deep dive for failures without this seed.
-    models = ['bvlc_googlenet', 'vgg-16', 'resnet-50']
+    models = ['bvlc_googlenet']
-from test_nn import *
+from test_gluon import *
-        self._params = {}
+        self._params = OrderedDict()
-        The optimizer to use.
+        The optimizer to use. See
-        `{'learning_rate': 0.1}`
+        `{'learning_rate': 0.1}`. All optimizers accept learning_rate, wd (weight decay),
-        kvstore type for multi-gpu and distributed training.
+        kvstore type for multi-gpu and distributed training. See help on
-    def __init__(self, params, optimizer, optimizer_params, kvstore='device'):
+    def __init__(self, params, optimizer, optimizer_params=None, kvstore='device'):
-                self._params.append(param)
+            self._params.append(param)
-        self._optimizer.set_wd_mult(wd_mult)
+        param_dict = {i: param for i, param in enumerate(self._params)}
-        self._update_on_kvstore = update_on_kvstore
+        kvstore, update_on_kvstore = _create_kvstore(self._kvstore, len(self._contexts),
-            assert 'dist' not in self._kvstore.type, "distributed training not supported yet"
+            if 'dist' in kvstore.type:
-                "was %s, now %s"%(param.name, self._contexts, param.list_ctx())
+            if param.grad_req == 'null':
-                if arr._fresh_grad:
+                if not ignore_stale_grad or arr._fresh_grad:
-                 lr_scheduler=None, sym=None, begin_num_update=0):
+                 lr_scheduler=None, sym=None, begin_num_update=0,
-        if index in self.lr_mult:
+        if index in self.param_dict:
-        if index in self.wd_mult:
+        if index in self.param_dict:
-        self.conv1 = _conv3x3(channels//4, 1, in_channels)
+        self.conv1 = nn.Conv2D(channels//4, kernel_size=1, strides=1, use_bias=False)
-        self.conv3 = _conv3x3(channels, 1, channels//4)
+        self.conv3 = nn.Conv2D(channels, kernel_size=1, strides=1, use_bias=False)
-    assert_almost_equal(out, args_grad[0].asnumpy(), rtol=1E-3, atol=1e-4)
+    assert_almost_equal(out, args_grad[0].asnumpy(), rtol=1E-3, atol=1e-3)
-    assert_almost_equal(out + args_grad_addto_npy[0], args_grad_addto[0].asnumpy(), rtol=1e-4, atol=1e-4)
+    assert_almost_equal(out + args_grad_addto_npy[0], args_grad_addto[0].asnumpy(), rtol=1e-4, atol=1e-3)
-    conv_4 = Conv(conv_3_dw, num_filter=128, kernel=(1, 1), pad=(0, 0), stride=(1, 1), name="conv_4") # 56/56
+    conv_4 = Conv(conv_4_dw, num_filter=128, kernel=(1, 1), pad=(0, 0), stride=(1, 1), name="conv_4") # 56/56
-        optimizer_params    = opt_params, 
+        optimizer_params    = opt_params,
-        batch_end_callback  = mx.callback.Speedometer(args.batch_size, args.disp_batches),
+        batch_end_callback  = mx.callback.Speedometer(args.batch_size, args.disp_batches, auto_reset=False),
-        batch_end_callback  = mx.callback.Speedometer(args.batch_size, args.disp_batches))
+        batch_end_callback  = mx.callback.Speedometer(args.batch_size, args.disp_batches, auto_reset=False))
-                                    params["num_outputs"] = int(params["num_outputs"]) - 1
+import re
-                wmat = layer_blobs[0].data
+                wmat = np.array(layer_blobs[0].data).reshape(arg_shape_dic[weight_name])
-                if layer.name == bn_name:
+                if layer.name == bn_name or re.sub('[-/]', '_', layer.name) == bn_name:
-        model = mx.mod.Module(symbol=sym, label_names=['prob_label', ])
+        model = mx.mod.Module(symbol=sym, label_names=[arg_names[-1], ])
-    ret = _locals['ret']
+    ret = []
-                    name, type_string, name, ','.join([mapping[x] for x in bottom]), param_string)
+                if layer.type == 'Eltwise' and param.operation == 1 and len(param.coeff) > 0:
-    xrange = range
+from builtins import range
-    for i in xrange(args.chunks):
+    chunk_size = (N + args.chunks - 1) // args.chunks
-            margin = (img.shape[0] - img.shape[1]) / 2;
+            margin = (img.shape[0] - img.shape[1]) // 2;
-            margin = (img.shape[1] - img.shape[0]) / 2;
+            margin = (img.shape[1] - img.shape[0]) // 2;
-            newsize = (args.resize, img.shape[0] * args.resize / img.shape[1])
+            newsize = (args.resize, img.shape[0] * args.resize // img.shape[1])
-            newsize = (img.shape[1] * args.resize / img.shape[0], args.resize)
+            newsize = (img.shape[1] * args.resize // img.shape[0], args.resize)
-    """
+    """Abstract base class for Convolutional RNN cells"""
-                 prefix='', params=None, conv_layout='NCHW'):
+                 i2h_weight_initializer, h2h_weight_initializer,
-                                 bias=self._hB)
+        i2h, h2h = self._conv_forward(inputs, states, name)
-        Jozefowicz et al. 2015 recommends setting this to 1.0
+                 i2h_weight_initializer=None, h2h_weight_initializer=None,
-                 prefix='ConvLSTM_', params=None, forget_bias=1.0,
+                 prefix='ConvLSTM_', params=None,
-
+        i2h, h2h = self._conv_forward(inputs, states, name)
-                                 bias=self._hB)
+        i2h, h2h = self._conv_forward(inputs, states, name)
-                               prefix='rnn_', forget_bias=1.0)
+                               prefix='rnn_')
-    if is_train=False.
+    for gradient computation.
-    check_call(_LIB.MXAutogradSetIsTraining(
+    check_call(_LIB.MXAutogradSetIsRecording(
-class TrainingStateScope(object):
+class RecordingStateScope(object):
-        with TrainingStateScope(True):
+        with RecordingStateScope(True, True):
-    def __init__(self, enter_state):
+    def __init__(self, enter_state, is_train):
-def record():
+def record(is_train=True):
-    return TrainingStateScope(True)
+    return RecordingStateScope(True, is_train)
-def pause():
+def pause(is_train=False):
-    return TrainingStateScope(False)
+    return RecordingStateScope(False, is_train)
-def backward(heads, head_grads=None, retain_graph=False):
+def backward(heads, head_grads=None, retain_graph=False, is_train=True):
-        check_call(_LIB.MXAutogradBackward(
+        check_call(_LIB.MXAutogradBackwardEx(
-            ctypes.c_int(retain_graph)))
+            ctypes.c_int(retain_graph),
-    check_call(_LIB.MXAutogradBackward(
+    check_call(_LIB.MXAutogradBackwardEx(
-        ctypes.c_int(retain_graph)))
+        ctypes.c_int(retain_graph),
-    numeric_types = (float, int, np.float32, np.int32)
+    numeric_types = (float, int, np.generic)
-    numeric_types = (float, int, long, np.float32, np.int32)
+    numeric_types = (float, int, long, np.generic)
-            backward is invalid.
+            a backward call is expected to follow.
-    def backward(self, out_grads=None):
+    def backward(self, out_grads=None, is_train=True):
-        check_call(_LIB.MXExecutorBackward(
+        check_call(_LIB.MXExecutorBackwardEx(
-        self._output_dirty = False
+            ndarray,
-    assert name in models, 'Model %s is not supported'%name
+    if name not in models:
-                    batch_label[i][num_object:][:] = -1
+                    batch_label[i][0:num_object] = nd.array(label)
-    def backward(self, out_grad=None, retain_graph=False):
+    def backward(self, out_grad=None, retain_graph=False, is_train=True):
-        out_grad: list of NDArray or None
+        out_grad : NDArray, optional
-        check_call(_LIB.MXAutogradBackward(
+        check_call(_LIB.MXAutogradBackwardEx(
-            ctypes.c_int(retain_graph)))
+            ctypes.c_int(retain_graph),
-        data_shape = (7, 3, 224, 224) if 'inception' not in model_name else (7, 3, 299, 299)
+        data_shape = (2, 3, 224, 224) if 'inception' not in model_name else (2, 3, 299, 299)
-        model(mx.nd.random_uniform(shape=data_shape))
+        model(mx.nd.random_uniform(shape=data_shape)).wait_to_read()
-        self._defered_init = ()
+        self._deferred_init = ()
-        if self._defered_init:
+        if self._deferred_init:
-                assert set(ctx) == set(self._defered_init[1]), \
+            if self._deferred_init:
-        self._defered_init = ()
+        self._deferred_init = ()
-        if not self._defered_init:
+        if not self._deferred_init:
-        self._defered_init = ()
+        init, ctx, default_init = self._deferred_init
-                self._defered_init = (init, ctx, default_init)
+                self._deferred_init = (init, ctx, default_init)
-        self._defered_init = (init, ctx, default_init)
+        self._deferred_init = (init, ctx, default_init)
-            self._defered_init = (init, ctx, default_init)
+        elif self._deferred_init:
-                return self._defered_init[1]
+            if self._deferred_init:
-def test_defered_init():
+def test_deferred_init():
-    `Parameter` holds a copy of the the parameter on each `Context` after
+    `Parameter` holds a copy of the parameter on each `Context` after
-        The prefix to be prepended to all Parameters' name created by this dict.
+        The prefix to be prepended to all Parameters' names created by this dict.
-    data : list of ``NDArray` or dict of str to ``NDArray``
+    data : ``NDArray``, list of ``NDArray` or dict of str to ``NDArray``
-    else:
+    elif isinstance(data, list):
-    maxdim = 5
+        # test save/load as list
-        return self._batch_sampler
+        return len(self._batch_sampler)
-        auglist.append(RandomSizedCropAug(crop_size, 0.3, (3.0 / 4.0, 4.0 / 3.0), inter_method))
+        auglist.append(RandomSizedCropAug(crop_size, 0.08, (3.0 / 4.0, 4.0 / 3.0), inter_method))
-    m = buf.shape[1]/shape[0]
+    n = buf.shape[0]//shape[1]
-    sy = (i/m)*shape[1]
+    sy = (i//m)*shape[1]
-        max_area = h * int(h * new_ratio)
+    area = h * w
-    new_h = int(np.sqrt(new_area / new_ratio))
+        new_w = int(round(np.sqrt(target_area * new_ratio)))
-    y0 = random.randint(0, h - new_h)
+        if random.random() < 0.5:
-    return out, (x0, y0, new_w, new_h)
+        if new_w <= w and new_h <= h:
-from ...base import string_types, numeric_types
+from ...base import string_types, numeric_types, _as_list
-                                   squeeze_axis=1)
+            inputs = _as_list(ndarray.split(inputs, axis=in_axis,
-        return [obj]
+from ..base import _as_list
-        for i in self._grad:
+        for i in self._grad.values():
-    """Create an atomic symbol function by handle and funciton name."""
+    """Create an atomic symbol function by handle and function name."""
-        `(num_layers, batch_size, 2*num_hidden)`
+        `(2*num_layers, batch_size, num_hidden)`
-        `(num_layers, batch_size, 2*num_hidden)`.
+        `(2*num_layers, batch_size, num_hidden)`.
-        `(num_layers, batch_size, 2*num_hidden)`
+        `(2*num_layers, batch_size, num_hidden)`
-        return self.prefix
+        return self._name
-            return self._cached_graph
+        if not self._cached_graph:
-        sym_args = _regroup(syms, self._in_format)[0]
+            params = {i: j.var() for i, j in self._reg_params.items()}
-        out, self._out_format = _flatten(out)
+            self._cached_graph = inputs, symbol.Group(out)
-        args, _, = _flatten(args)
+        inputs, out = self._get_graph(*args)
-            **{i.name: j.shape for i, j in zip(syms, args)})
+            **{i.name: j.shape for i, j in zip(inputs, args)})
-        _, out = self._get_graph(*args)
+        inputs, out = self._get_graph(*args)
-        self._in_idx = [(i, int(name)) for i, name in enumerate(out.list_inputs())
+        assert len(params) + len(self._cached_graph[0]) == len(out.list_inputs()), \
-            params = {i: j.var() for i, j in self._reg_params.items()}
+
-                self.features.add(nn.Activation('relu'))
+                self.features.add(nn.Conv2D(64, kernel_size=11, strides=4,
-                self.features.add(nn.Activation('relu'))
+                self.features.add(nn.Conv2D(192, kernel_size=5, padding=2,
-                self.features.add(nn.Activation('relu'))
+                self.features.add(nn.Conv2D(384, kernel_size=3, padding=1,
-                self.classifier.add(nn.Dropout(0.5))
+                self.classifier.add(nn.Dropout(0.5))
-                self.act = Activation(activation)
+                self.act = Activation(activation, prefix=activation+'_')
-            act = F.FullyConnected(x, weight, no_bias=True, num_hidden=self._units)
+            act = F.FullyConnected(x, weight, no_bias=True, num_hidden=self._units,
-            act = F.FullyConnected(x, weight, bias, num_hidden=self._units)
+            act = F.FullyConnected(x, weight, bias, num_hidden=self._units,
-        return F.Activation(x, act_type=self._act_type)
+        return F.Activation(x, act_type=self._act_type, name='fwd')
-        return F.Dropout(x, p=self._rate)
+        return F.Dropout(x, p=self._rate, name='fwd')
-                        'fix_gamma': not center}
+                        'fix_gamma': not scale}
-        return F.BatchNorm(x, gamma, beta, running_mean, running_var, **self._kwargs)
+        return F.BatchNorm(x, gamma, beta, running_mean, running_var,
-        return F.LeakyReLU(x, act_type='leaky', slope=self._alpha)
+        return F.LeakyReLU(x, act_type='leaky', slope=self._alpha, name='fwd')
-        return F.Embedding(x, weight, **self._kwargs)
+        return F.Embedding(x, weight, name='fwd', **self._kwargs)
-        return s.format(name=self.__class__.__name__,
+        s = '{block_name}({input_dim} -> {output_dim}, {dtype})'
-                self.act = Activation(activation)
+                self.act = Activation(activation, prefix=activation+'_')
-            act = getattr(F, self._op_name)(x, weight, **self._kwargs)
+            act = getattr(F, self._op_name)(x, weight, name='fwd', **self._kwargs)
-            act = getattr(F, self._op_name)(x, weight, bias, **self._kwargs)
+            act = getattr(F, self._op_name)(x, weight, bias, name='fwd', **self._kwargs)
-        return F.Pooling(x, **self._kwargs)
+        return F.Pooling(x, name='fwd', **self._kwargs)
-        self._finish_deferred_init()
+    def _check_initialized(self, ctx=None):
-    def initialize(self, init=initializer.Uniform(), ctx=None, verbose=False):
+    def initialize(self, init=initializer.Uniform(), ctx=None, verbose=False,
-            v.initialize(None, ctx, init)
+            v.initialize(None, ctx, init, force_reinit=force_reinit)
-            weight = sum(w.copyto(context.cpu()) for w in block) / len(block)
+            weight = param._reduce()
-        name = self._curr_prefix
+        prefix = 't%d_'%self._counter
-                               name='%si2h'%name)
+                               name=prefix+'i2h')
-                               name='%sh2h'%name)
+                               name=prefix+'h2h')
-                                      name='%sout'%name)
+                                      name=prefix+'out')
-        name = self._curr_prefix
+        prefix = 't%d_'%self._counter
-                               name='%si2h'%name)
+                               num_hidden=self._hidden_size*4, name=prefix+'i2h')
-                               name='%sh2h'%name)
+                               num_hidden=self._hidden_size*4, name=prefix+'h2h')
-                                name='%so'%name)
+        slice_gates = F.SliceChannel(gates, num_outputs=4, name=prefix+'slice')
-                                   name='%sstate'%name)
+                                   name=prefix+'state')
-                                  name='%sout'%name)
+                                  name=prefix+'out')
-        name = self._curr_prefix
+        prefix = 't%d_'%self._counter
-                               name="%si2h" % name)
+                               name=prefix+'i2h')
-                               name="%sh2h" % name)
+                               name=prefix+'h2h')
-        h2h_r, h2h_z, h2h = F.SliceChannel(h2h, num_outputs=3, name="%sh2h_slice" % name)
+        i2h_r, i2h_z, i2h = F.SliceChannel(i2h, num_outputs=3,
-                                  name="%sr_act" % name)
+                                  name=prefix+'r_act')
-                                   name="%sz_act" % name)
+                                   name=prefix+'z_act')
-                                  name="%sh_act" % name)
+                                  name=prefix+'h_act')
-                                   name='%sout' % name)
+                                   name=prefix+'out')
-    dropout : float
+    rate : float
-    def __init__(self, dropout, prefix=None, params=None):
+    def __init__(self, rate, prefix=None, params=None):
-        self.dropout = dropout
+        assert isinstance(rate, numeric_types), "rate must be a number"
-        s = '{name}(p = {dropout})'
+        s = '{name}(rate = {rate})'
-            inputs = F.Dropout(data=inputs, p=self.dropout)
+        if self.rate > 0:
-        super(ModifierCell, self).__init__(prefix=None, params=None)
+        assert not base_cell._modified, \
-        output = F.elemwise_add(output, inputs, name="%s_plus_residual" % output.name)
+        output = F.elemwise_add(output, inputs, name='t%d_fwd'%self._counter)
-            for i, name in enumerate(self.list_outputs()):
+            for i, name in enumerate(output_names):
-        if index >= (len(self.list_outputs())):
+        if index >= len(output_names):
-        train(opt.epochs, [mx.gpu(i) for i in range(gpus)] if gpus > 0 else [mx.cpu()])
+
-    epsilon: float, default 1e-3
+    epsilon: float, default 1e-5
-    def __init__(self, axis=1, momentum=0.9, epsilon=1e-3, center=True, scale=True,
+    def __init__(self, axis=1, momentum=0.9, epsilon=1e-5, center=True, scale=True,
-    >>> eval_end_callbacks = [mx.tensorboard.LogMetricsCallback(evaluation_log)]
+    >>> batch_end_callbacks = [mx.contrib.tensorboard.LogMetricsCallback(training_log)]
-        return
+    # The following sequence should throw an exception. We discard the expected
-        return
+    # The following sequence should throw an exception. We discard the expected
-        return
+    # The following bind() should throw an exception. We discard the expected stderr
-from mxnet.test_utils import check_consistency, set_default_context
+from mxnet.test_utils import check_consistency, set_default_context, assert_almost_equal
-    assert_allclose(go.asnumpy(), co.asnumpy(), rtol=1e-2)
+    assert_almost_equal(go.asnumpy(), co.asnumpy(), rtol=1e-2, atol=1e-8)
-        assert_allclose(g.asnumpy(), c.asnumpy(), rtol=1e-2)
+        assert_almost_equal(g.asnumpy(), c.asnumpy(), rtol=1e-2, atol=1e-8)
-    'omp.h', 'execinfo.h', 'packet/sse-inl.h', 'emmintrin.h', 'thrust/device_vector.h'
+    'omp.h', 'execinfo.h', 'packet/sse-inl.h', 'emmintrin.h', 'thrust/device_vector.h', 
-    print total_count
+    print total_count, 'warnings'
-    all_layers = sym.get_internals()
+    all_layers = symbol.get_internals()
-        indices = range(self._length)
+        indices = list(range(self._length))
-                                       begin=begin, end=end)
+                self._slice_assign(value, begin, end, expand)
-                                       begin=begin, end=end)
+                value = array(value, ctx=self.context, dtype=self.dtype)
-        return np.prod(self.shape)
+        size = 1
-def get_symbol(num_classes, dtype, **kwargs):
+def get_symbol(num_classes, dtype='float32', **kwargs):
-        initializier : initializer function, optional
+        initializer : initializer function, optional
-           Defaults to 'local', often no need to change for single machiine.
+           Defaults to 'local', often no need to change for single machine.
-        Assume the optimizer has udpated *i*-th weight by *k_i* times, namely
+        Assume the optimizer has updated *i*-th weight by *k_i* times, namely
-net.collect_params().initialize(mx.init.Uniform(0.02))
+net.initialize(mx.init.Uniform(0.02))
-    buff = cv2.cvtColor(buff, cv2.COLOR_BGR2RGB)
+    buff = buff[:,:,::-1]
-    nc = 3
+
-            btic = time.time()
+else:
-        logging.info('time: %f' % (time.time() - tic))
+        # logging.info('speed: {} samples/s'.format(opt.batch_size / (time.time() - btic)))
-            netD.collect_params().save(os.path.join(outf,'discriminator_epoch_%d.params' % epoch))
+    name, acc = metric.get()
-    netD.collect_params().save(os.path.join(outf, 'discriminator.params'))
+    if check_point:
-train_data, val_data = mnist_iterator(batch_size=opt.batch_size, input_shape=(28*28,))
+def transformer(data, label):
-        label = batch.label[0].as_in_context(ctx)
+    for data, label in val_data:
-    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)
+    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)
-                          {'learning_rate': opt.lr, 'momentum': opt.momentum})
+                            {'learning_rate': opt.lr, 'momentum': opt.momentum})
-        for i, batch in enumerate(train_data):
+        for i, (data, label) in enumerate(train_data):
-            label = batch.label[0].as_in_context(ctx)
+            data = data.as_in_context(ctx)
-    net.collect_params().save('mnist.params')
+    net.save_params('mnist.params')
-    net.collect_params().initialize(mx.init.Orthogonal(), ctx=ctx)
+    net.initialize(mx.init.Orthogonal(), ctx=ctx)
-    net.collect_params().save('superres.params')
+    net.save_params('superres.params')
-    net.collect_params().load('superres.params', ctx=ctx)
+    net.load_params('superres.params', ctx=ctx)
-def imdecode(buf, **kwargs):
+def imdecode(buf, *args, **kwargs):
-    return _internal._cvimdecode(buf, **kwargs)
+    return _internal._cvimdecode(buf, *args, **kwargs)
-    np.int32   : 4
+    np.int32   : 4,
-    4 : np.int32
+    4 : np.int32,
-      packages=find_packages(where=CURRENT_DIR),
+      packages=find_packages(),
-                gts = label[np.where(label[:, 0].astype(int) == cid)[0], :]
+                label_indices = np.where(label[:, 0].astype(int) == cid)[0]
-from . import recordio
+from ..base import numeric_types
-        Default method is bicubic interpolation.
+        Possible values:
-        new_h, new_w = size * h / w, size
+        new_h, new_w = size * h // w, size
-    return imresize(src, new_w, new_h, interp=interp)
+        new_h, new_w = size, size * w // h
-    """Crop src at fixed location, and (optionally) resize it to size."""
+    """Crop src at fixed location, and (optionally) resize it to size.
-        out = imresize(out, *size, interp=interp)
+        sizes = (h, w, size[1], size[0])
-            Area - 3. See OpenCV imresize function for more details.
+    interp: int, optional, default=2
-        with Bicubic (slow) or Bilinear (faster but still looks OK).
+    interp : int, optional, default=2
-    src -= mean
+    """Normalize src with mean and std.
-    """Randomly crop src with size. Randomize area and aspect ratio."""
+    """Randomly crop src with size. Randomize area and aspect ratio.
-    """Make resize shorter edge to size augmenter."""
+class Augmenter(object):
-    def aug(src):
+class ResizeAug(Augmenter):
-        return [resize_short(src, size, interp)]
+        return [resize_short(src, self.size, self.interp)]
-    return aug
+class ForceResizeAug(Augmenter):
-    """Make random crop augmenter"""
+    Parameters
-    def aug(src):
+    def __call__(self, src):
-        return [random_crop(src, size, interp)[0]]
+        sizes = (src.shape[0], src.shape[1], self.size[1], self.size[0])
-    return aug
+class RandomCropAug(Augmenter):
-    """Make random crop with random resizing and random aspect ratio jitter augmenter."""
+    def __call__(self, src):
-    def aug(src):
+
-        return [random_size_crop(src, size, min_area, ratio, interp)[0]]
+        return [random_size_crop(src, self.size, self.min_area, self.ratio, self.interp)[0]]
-    return aug
+class CenterCropAug(Augmenter):
-    """Make center crop augmenter."""
+    Parameters
-    def aug(src):
+    def __call__(self, src):
-        return [center_crop(src, size, interp)[0]]
+        return [center_crop(src, self.size, self.interp)[0]]
-    return aug
+class RandomOrderAug(Augmenter):
-    """Apply list of augmenters in random order"""
+    Parameters
-    def aug(src):
+    def __call__(self, src):
-        for t in ts:
+        random.shuffle(self.ts)
-    def aug(src):
+
-        rgb = np.dot(eigvec * alpha, eigval)
+        alpha = np.random.normal(0, self.alphastd, size=(3,))
-    return aug
+class ColorNormalizeAug(Augmenter):
-    std = nd.array(std)
+    Parameters
-    def aug(src):
+    def __call__(self, src):
-        return [color_normalize(src, mean, std)]
+        if random.random() < self.p:
-    return aug
+class HorizontalFlipAug(Augmenter):
-    """Random horizontal flipping."""
+    Parameters
-    def aug(src):
+    def __call__(self, src):
-        if random.random() < p:
+        if random.random() < self.p:
-def CastAug():
+class CastAug(Augmenter):
-    def aug(src):
+    def __call__(self, src):
-    """Creates an augmenter list."""
+                    mean=None, std=None, brightness=0, contrast=0, saturation=0, hue=0,
-    if mean is not None and std is not None:
+    if mean is not None or std is not None:
-            print('loading recordio...')
+            logging.info('%s: loading recordio %s...',
-            print('loading image list...')
+            logging.info('%s: loading image list %s...', class_name, path_imglist)
-            print('loading image list...')
+            logging.info('%s: loading image list...', class_name)
-                if isinstance(img[0], numeric_types):
+                if len(img) > 2:
-                result[key] = (label, img[1])
+                result[key] = (label, img[-1])
-            C = N / num_parts
+            C = N // num_parts
-        return fname
+    if not overwrite and os.path.exists(fname):
-from google.protobuf import text_format
+from google.protobuf import text_format # pylint: disable=relative-import
-    from google.protobuf import text_format
+    from google.protobuf import text_format # pylint: disable=relative-import
-        Prefix acts like a name space. It will be prepended to the name of all
+        Prefix acts like a name space. It will be prepended to the names of all
-        train = SyntheticDataIter(args.num_classes, data_shape, 50, dtype)
+        train = SyntheticDataIter(args.num_classes, data_shape, 500, np.float32)
-    input_data = mx.symbol.Variable(name="data")
+def get_symbol(num_classes, dtype, **kwargs):
-    conv1 = mx.symbol.Convolution(name='conv1',
+    conv1 = mx.sym.Convolution(name='conv1',
-    pool1 = mx.symbol.Pooling(
+    relu1 = mx.sym.Activation(data=conv1, act_type="relu")
-    conv2 = mx.symbol.Convolution(name='conv2',
+    conv2 = mx.sym.Convolution(name='conv2',
-    pool2 = mx.symbol.Pooling(data=lrn2, kernel=(3, 3), stride=(2, 2), pool_type="max")
+    relu2 = mx.sym.Activation(data=conv2, act_type="relu")
-    conv3 = mx.symbol.Convolution(name='conv3',
+    conv3 = mx.sym.Convolution(name='conv3',
-    conv4 = mx.symbol.Convolution(name='conv4',
+    relu3 = mx.sym.Activation(data=conv3, act_type="relu")
-    conv5 = mx.symbol.Convolution(name='conv5',
+    relu4 = mx.sym.Activation(data=conv4, act_type="relu")
-    pool3 = mx.symbol.Pooling(data=relu5, kernel=(3, 3), stride=(2, 2), pool_type="max")
+    relu5 = mx.sym.Activation(data=conv5, act_type="relu")
-    dropout1 = mx.symbol.Dropout(data=relu6, p=0.5)
+    flatten = mx.sym.Flatten(data=pool3)
-    dropout2 = mx.symbol.Dropout(data=relu7, p=0.5)
+    fc2 = mx.sym.FullyConnected(name='fc2', data=dropout1, num_hidden=4096)
-    softmax = mx.symbol.SoftmaxOutput(data=fc3, name='softmax')
+    fc3 = mx.sym.FullyConnected(name='fc3', data=dropout2, num_hidden=num_classes)
-    return softmax
+import numpy as np
-    pooling = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pad=(0,0), pool_type="max", name=('max_pool_%s_pool' % name))
+    pooling = mx.sym.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pad=(0,0), pool_type="max", name=('max_pool_%s_pool' % name))
-    data = mx.symbol.Variable(name="data")
+def get_symbol(num_classes=1000, dtype='float32', **kwargs):
-    softmax = mx.symbol.SoftmaxOutput(data=fc1, name='softmax')
+    fc1 = mx.sym.FullyConnected(data=flatten, num_hidden=num_classes, name='fc1')
-import find_mxnet
+import numpy as np
-    act = mx.symbol.Activation(data=bn, act_type='relu', name='%s%s_relu' %(name, suffix))
+    conv = mx.sym.Convolution(data=data, num_filter=num_filter, kernel=kernel, stride=stride, pad=pad, no_bias=True, name='%s%s_conv2d' %(name, suffix))
-    p1 = mx.symbol.Pooling(c, kernel=(3, 3), stride=(2, 2), pool_type='max', name='%s_maxpool_1' %name)
+    p1 = mx.sym.Pooling(c, kernel=(3, 3), stride=(2, 2), pool_type='max', name='%s_maxpool_1' %name)
-    concat = mx.symbol.Concat(*[p1, c2], name='%s_concat_1' %name)
+    concat = mx.sym.Concat(*[p1, c2], name='%s_concat_1' %name)
-    concat = mx.symbol.Concat(*[c1, c2], name='%s_concat_2' %name)
+    concat = mx.sym.Concat(*[c1, c2], name='%s_concat_2' %name)
-    p1 = mx.symbol.Pooling(concat, kernel=(3, 3), stride=(2, 2), pool_type='max', name='%s_maxpool_2' %name)
+    p1 = mx.sym.Pooling(concat, kernel=(3, 3), stride=(2, 2), pool_type='max', name='%s_maxpool_2' %name)
-    concat = mx.symbol.Concat(*[c1, p1], name='%s_concat_3' %name)
+    concat = mx.sym.Concat(*[c1, p1], name='%s_concat_3' %name)
-    p1 = mx.symbol.Pooling(input, kernel=(3, 3), pad=(1, 1), pool_type='avg', name='%s_avgpool_1' %name)
+    p1 = mx.sym.Pooling(input, kernel=(3, 3), pad=(1, 1), pool_type='avg', name='%s_avgpool_1' %name)
-    concat = mx.symbol.Concat(*[c1, c2, c3, c4], name='%s_concat_1' %name)
+    concat = mx.sym.Concat(*[c1, c2, c3, c4], name='%s_concat_1' %name)
-    p1 = mx.symbol.Pooling(input, kernel=(3, 3), stride=(2, 2), pool_type='max', name='%s_maxpool_1' %name)
+    p1 = mx.sym.Pooling(input, kernel=(3, 3), stride=(2, 2), pool_type='max', name='%s_maxpool_1' %name)
-    concat = mx.symbol.Concat(*[p1, c2, c3], name='%s_concat_1' %name)
+    concat = mx.sym.Concat(*[p1, c2, c3], name='%s_concat_1' %name)
-    p1 = mx.symbol.Pooling(input, kernel=(3, 3), pad=(1, 1), pool_type='avg', name='%s_avgpool_1' %name)
+    p1 = mx.sym.Pooling(input, kernel=(3, 3), pad=(1, 1), pool_type='avg', name='%s_avgpool_1' %name)
-    p1 = mx.symbol.Pooling(input, kernel=(3, 3), stride=(2, 2), pool_type='max', name='%s_maxpool_1' %name)
+    p1 = mx.sym.Pooling(input, kernel=(3, 3), stride=(2, 2), pool_type='max', name='%s_maxpool_1' %name)
-    concat = mx.symbol.Concat(*[p1, c2, c3], name='%s_concat_1' %name)
+    concat = mx.sym.Concat(*[p1, c2, c3], name='%s_concat_1' %name)
-    p1 = mx.symbol.Pooling(input, kernel=(3, 3), pad=(1, 1), pool_type='avg', name='%s_avgpool_1' %name)
+    p1 = mx.sym.Pooling(input, kernel=(3, 3), pad=(1, 1), pool_type='avg', name='%s_avgpool_1' %name)
-    concat = mx.symbol.Concat(*[c1, c2, c3_1, c3_2, c4_1, c4_2], name='%s_concat' %name)
+    concat = mx.sym.Concat(*[c1, c2, c3_1, c3_2, c4_1, c4_2], name='%s_concat' %name)
-    data = mx.symbol.Variable(name="data")
+def get_symbol(num_classes=1000, dtype='float32', **kwargs):
-    x = mx.symbol.Pooling(x, kernel=(8, 8), pad=(1, 1), pool_type='avg', name='global_avgpool')
+    x = mx.sym.Pooling(x, kernel=(8, 8), pad=(1, 1), pool_type='avg', name='global_avgpool')
-    x = mx.symbol.Dropout(x, p=0.2)
+    x = mx.sym.Dropout(x, p=0.2)
-    softmax = mx.symbol.SoftmaxOutput(fc1, name='softmax')
+    flatten = mx.sym.Flatten(x, name='flatten')
-                  workspace   = conv_workspace)
+import numpy as np
-def resnet(units, num_stages, filter_list, num_classes, image_shape, bottle_neck=True, bn_mom=0.9, workspace=256, memonger=False):
+def resnet(units, num_stages, filter_list, num_classes, image_shape, bottle_neck=True, bn_mom=0.9, workspace=256, dtype='float32', memonger=False):
-    data = mx.sym.identity(data=data, name='id')
+    if dtype == 'float32':
-        body = mx.symbol.Pooling(data=body, kernel=(3, 3), stride=(2,2), pad=(1,1), pool_type='max')
+        body = mx.sym.Pooling(data=body, kernel=(3, 3), stride=(2,2), pad=(1,1), pool_type='max')
-    return mx.symbol.SoftmaxOutput(data=fc1, name='softmax')
+    pool1 = mx.sym.Pooling(data=body, global_pool=True, kernel=(7, 7), pool_type='avg', name='pool1')
-def get_symbol(num_classes, num_layers, image_shape, conv_workspace=256, **kwargs):
+def get_symbol(num_classes, num_layers, image_shape, conv_workspace=256, dtype='float32', **kwargs):
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-                  workspace   = conv_workspace)
+                  workspace   = conv_workspace,
-def resnet(units, num_stages, filter_list, num_classes, image_shape, bottle_neck=True, bn_mom=0.9, workspace=256, memonger=False):
+def resnet(units, num_stages, filter_list, num_classes, image_shape, bottle_neck=True, bn_mom=0.9, workspace=256, dtype='float32', memonger=False):
-    data = mx.sym.identity(data=data, name='id')
+    if dtype == 'float32':
-        body = mx.symbol.Pooling(data=body, kernel=(3, 3), stride=(2,2), pad=(1,1), pool_type='max')
+        body = mx.sym.Pooling(data=body, kernel=(3, 3), stride=(2,2), pad=(1,1), pool_type='max')
-    return mx.symbol.SoftmaxOutput(data=fc1, name='softmax')
+    pool1 = mx.sym.Pooling(data=relu1, global_pool=True, kernel=(7, 7), pool_type='avg', name='pool1')
-def get_symbol(num_classes, num_layers, image_shape, conv_workspace=256, **kwargs):
+def get_symbol(num_classes, num_layers, image_shape, conv_workspace=256, dtype='float32', **kwargs):
-                  workspace   = conv_workspace)
+                  workspace   = conv_workspace,
-                  workspace   = conv_workspace)
+import numpy as np
-def resnext(units, num_stages, filter_list, num_classes, num_group, image_shape, bottle_neck=True, bn_mom=0.9, workspace=256, memonger=False):
+def resnext(units, num_stages, filter_list, num_classes, num_group, image_shape, bottle_neck=True, bn_mom=0.9, workspace=256, dtype='float32', memonger=False):
-        body = mx.symbol.Pooling(data=body, kernel=(3, 3), stride=(2,2), pad=(1,1), pool_type='max')
+        body = mx.sym.Pooling(data=body, kernel=(3, 3), stride=(2,2), pad=(1,1), pool_type='max')
-    return mx.symbol.SoftmaxOutput(data=fc1, name='softmax')
+    pool1 = mx.sym.Pooling(data=body, global_pool=True, kernel=(7, 7), pool_type='avg', name='pool1')
-def get_symbol(num_classes, num_layers, image_shape, num_group=32, conv_workspace=256, **kwargs):
+def get_symbol(num_classes, num_layers, image_shape, num_group=32, conv_workspace=256, dtype='float32', **kwargs):
-                  workspace   = conv_workspace)
+                  workspace   = conv_workspace,
-        return self._params
+    def __setattr__(self, name, value):
-        return ret
+    def _alias(self):
-    def save(self, filename):
+    def save(self, filename, strip_prefix=''):
-            arg_dict[param.name] = weight
+            if not param.name.startswith(strip_prefix):
-        arg_dict = ndarray.load(filename)
+    def load(self, filename, ctx, allow_missing=False,
-                    "Parameter %s is missing in file %s"%(name, filename)
+                    "Parameter %s is missing in file %s"%(name[lprefix:], filename)
-                        name, filename)
+                        name[lprefix:], filename)
-          ],
+      packages=find_packages(where=CURRENT_DIR),
-            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2)
+            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2, atol=0.01)
-            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2)
+            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2, atol=0.01)
-            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2)
+            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2, atol=0.01)
-            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2)
+            check_numeric_gradient(test, [data_tmp, gamma, beta], [xrolling_mean, xrolling_std], numeric_eps=1e-2, rtol=0.2, atol=0.01)
-    assert_almost_equal(out, np_out, rtol=1e-4)
+    assert_almost_equal(out, np_out, rtol=1e-4, atol=1e-4)
-          'mxnet._cy2', 'mxnet._cy3', 'mxnet.notebook', 'mxnet.contrib'
+          'mxnet._cy2', 'mxnet._cy3', 'mxnet.notebook', 'mxnet.contrib',
-# and separate train set and test set
+# This is just to normalize the input and separate train set and test set
-test_iter.label =  mx.io._init_data(Y_test, allow_empty=True, default_name='svm_label')
+train_iter = mx.io.NDArrayIter(X_train, Y_train, batch_size=batch_size, label_name='svm_label')
-            'train or predict or load can be the candidate for the mode')
+import json
-    """Load libary by searching possible path."""
+    """Load library by searching possible path."""
-    if isinstance(data, (np.ndarray, NDArray)):
+    if isinstance(data, (np.ndarray, NDArray, h5py.Dataset)
-        raise TypeError("Input must be NDArray, numpy.ndarray, " + \
+        raise TypeError("Input must be NDArray, numpy.ndarray, h5py.Dataset " + \
-        if not isinstance(v, NDArray):
+        if not isinstance(v, (NDArray, h5py.Dataset) if h5py else NDArray):
-                    "should be NDArray or numpy.ndarray")
+                                "should be NDArray, numpy.ndarray or h5py.Dataset")
-    """Returns an iterator for ``mx.nd.NDArray`` or ``numpy.ndarray``.
+    """Returns an iterator for ``mx.nd.NDArray``, ``numpy.ndarray`` or ``h5py.Dataset``.
-            self.label = [(k, array(v.asnumpy()[idx], v.context)) for k, v in self.label]
+            np.random.shuffle(self.idx)
-            self.label = label_dict.items()
+            self.idx = self.idx[:new_n]
-        self.num_data = self.data_list[0].shape[0]
+        self.num_data = self.idx.shape[0]
-            "batch_size need to be smaller than data size."
+            "batch_size needs to be smaller than data size."
-            return [x[1][self.cursor:self.cursor+self.batch_size] for x in data_source]
+            return [
-            return [concatenate([x[1][self.cursor:], x[1][:pad]]) for x in data_source]
+            return [
-    labels = np.ones([1000, 1])
+    data = np.ones([1000, 2, 2])
-    dataiter = mx.io.NDArrayIter(datas, labels, 128, True, last_batch_handle='pad')
+        data[i] = i / 100
-    dataiter = mx.io.NDArrayIter(datas, labels, 128, False, last_batch_handle='pad')
+    dataiter = mx.io.NDArrayIter(data, label, 128, False, last_batch_handle='pad')
-                    assert len(ashape) == n_aux
+                    assert len(oshape) == n_out, \
-                    assert len(atype) == n_aux
+                    assert len(otype) == n_out, \
-# Use different verison of SymbolBase
+# Use different version of SymbolBase
-
+def test_sequence_reverse():
-from numpy.testing import assert_allclose
+from numpy.testing import assert_allclose, assert_array_equal
-            axis_flags = np.random.randint(-5, 6, size=ndim)
+            axis_flags = np.random.randint(0, 2, size=ndim)
-    assert_almost_equal(out, np_out, rtol=1e-4, atol=1e-5)
+    assert_almost_equal(out, np_out, rtol=1e-4)
-    assert_almost_equal(exe.outputs[0].asnumpy(), np_out, rtol=1e-4, atol=1e-5)
+    assert_almost_equal(exe.outputs[0].asnumpy(), np_out, rtol=1e-5)
-class L2Loss(HybridBlock):
+class Loss(HybridBlock):
-        self._batch_axis = batch_axis
+        super(L2Loss, self).__init__(weight, batch_axis, **kwargs)
-class L1Loss(HybridBlock):
+class L1Loss(Loss):
-        self._batch_axis = batch_axis
+        super(L1Loss, self).__init__(weight, batch_axis, **kwargs)
-class SoftmaxCrossEntropyLoss(HybridBlock):
+class SoftmaxCrossEntropyLoss(Loss):
-        super(SoftmaxCrossEntropyLoss, self).__init__(**kwargs)
+        super(SoftmaxCrossEntropyLoss, self).__init__(weight, batch_axis, **kwargs)
-class KLDivLoss(HybridBlock):
+class KLDivLoss(Loss):
-        super(KLDivLoss, self).__init__(**kwargs)
+        super(KLDivLoss, self).__init__(weight, batch_axis, **kwargs)
-        self._batch_axis = batch_axis
+from ..utils import _indent
-import mxnet.ndarray as F
+    """A model with an encoder, recurrent layer, and a decoder."""
-            self.encoder = nn.Embedding(vocab_size, num_embed)
+            self.encoder = nn.Embedding(vocab_size, num_embed,
-parser.add_argument('--lr', type=float, default=20,
+parser.add_argument('--lr', type=float, default=1.0,
-parser.add_argument('--clip', type=float, default=0.25,
+parser.add_argument('--clip', type=float, default=0.2,
-parser.add_argument('--batch_size', type=int, default=20, metavar='N',
+parser.add_argument('--batch_size', type=int, default=32, metavar='N',
-parser.add_argument('--save', type=str,  default='model.params',
+parser.add_argument('--save', type=str, default='model.params',
-model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied)
+model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid,
-                       'wd': 0})
+                        {'learning_rate': args.lr,
-    for ibatch, i in enumerate(range(0, data_source.shape[0] - 1, args.bptt)):
+    for i in range(0, data_source.shape[0] - 1, args.bptt):
-    best_val = None
+    best_val = float("Inf")
-            gluon.utils.clip_global_norm(grads, args.clip * args.batch_size)
+            # Here gradient is for the whole batch.
-                cur_L = total_L / args.batch_size / args.bptt / args.log_interval
+                cur_L = total_L / args.bptt / args.batch_size / args.log_interval
-    print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))
+    print('Best test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))
-                    posteriors = posteriors[:batch.utt_len,1:] - np.log(data_test.label_mean[1:]).T
+                    posteriors = posteriors[:batch.utt_len[0],1:] - np.log(data_test.label_mean[1:]).T
-        net = net + data1
+        net = net + data3
-        arr_grad = [mx.nd.empty(shape) for i in range(n)]
+    arr = []
-    exec1 = net.bind(mx.cpu(),
+    exec1 = net.bind(ctx1,
-                     group2ctx={'dev1': mx.cpu(0), 'dev2': mx.cpu(1)})
+                     group2ctx={'dev1': ctx1, 'dev2': ctx2})
-    exec2 = net.bind(mx.cpu(),
+    arr[2][:] = 3.0
-    exec2.forward()
+    exec1.forward(is_train=True)
-    out_grad = mx.nd.empty(shape, mx.cpu(1))
+    out_grad = mx.nd.empty(shape, ctx1)
-    exec2.backward([out_grad.copyto(mx.cpu())])
+    exec2.backward([out_grad.copyto(ctx1)])
-    def score(self, eval_data, eval_metric=None, num_batch=None, batch_end_callback=None,
+    def score(self, eval_data, eval_metric, num_batch=None, batch_end_callback=None,
-        eval_metric = _parse_metric(self.symbol, eval_metric)
+        if not isinstance(eval_metric, metric.EvalMetric):
-    def fit(self, train_data, eval_data=None, eval_metric=None,
+    def fit(self, train_data, eval_data=None, eval_metric='acc',
-            validation_metric = 'acc'
+        if not isinstance(eval_metric, metric.EvalMetric):
-        if label_shapes:
+        if label_shapes is not None:
-        if self.label_shapes:
+        if self.label_shapes is not None:
-        if label_shapes:
+        if label_shapes is not None:
-            if label_shapes:
+            if label_shapes is not None:
-        if label_shapes:
+        if label_shapes is not None:
-        if label_shapes:
+        if label_shapes is not None:
-        if label_shapes:
+        if label_shapes is not None:
-                self.data_names, [], data_shapes, [])
+        if not for_training:
-                self.data_names, self.label_names, data_shapes, label_shapes)
+            pass
-        self._exec_group = DataParallelExecutorGroup(symbol, self._context,
+        self._exec_group = DataParallelExecutorGroup(self._symbol, self._context,
-    notebook = nbformat.read(file_path + '_python.ipynb', as_version=4)
+    notebook = nbformat.read(file_path + '.ipynb', as_version=4)
-    label_shape, pred_shape = labels.shape, preds.shape
+def check_label_shapes(labels, preds, shape=0):
-        _check_lengths_equal(labels, preds)
+        check_label_shapes(labels, preds)
-            _check_lengths_equal(label, pred_label)
+            check_label_shapes(label, pred_label)
-        _check_lengths_equal(labels, preds)
+        check_label_shapes(labels, preds)
-            _check_lengths_equal(label, pred_label)
+            check_label_shapes(label, pred_label)
-        _check_lengths_equal(labels, preds)
+        check_label_shapes(labels, preds)
-            _check_lengths_equal(label, pred)
+            check_label_shapes(label, pred)
-        _check_lengths_equal(labels, preds)
+        assert len(labels) == len(preds)
-        _check_lengths_equal(labels, preds)
+        check_label_shapes(labels, preds)
-        _check_lengths_equal(labels, preds)
+        check_label_shapes(labels, preds)
-        _check_lengths_equal(labels, preds)
+        check_label_shapes(labels, preds)
-        _check_lengths_equal(labels, preds)
+        check_label_shapes(labels, preds)
-            _check_lengths_equal(label, pred)
+
-        _check_lengths_equal(labels, preds)
+        check_label_shapes(labels, preds)
-            _check_shapes_equal(label, pred)
+            check_label_shapes(label, pred, 1)
-            _check_lengths_equal(labels, preds)
+            check_label_shapes(labels, preds)
-    def _init_weight(self, _, arr):
+    def _init_weight(self, name, arr):
-                "but %s has type %s."%(str(block), str(type(block))))
+                "but %s has type %s. If you are using Sequential, " \
-from .base import _LIB, string_types, numeric_types
+from .base import _LIB, string_types, numeric_types, integer_types
-        if isinstance(key, int):
+            raise ValueError('Cannot assign to readonly NDArray')
-        if isinstance(key, py_slice):
+        elif isinstance(key, py_slice):
-                raise ValueError('NDArray only supports continuous slicing on axis 0')
+                raise ValueError('NDArray only supports slicing with step size 1')
-        if isinstance(key, tuple):
+                raise TypeError(
-                assert isinstance(slice_i, (py_slice, int))
+            assert len(key) <= len(my_shape), \
-                if isinstance(slice_i, int):
+                if isinstance(slice_i, integer_types):
-                        "NDArray only supports continuous slicing."
+                        "NDArray only supports slicing with step size 1."
-            end = tuple(end)
+                        "NDArray does not support slicing with key %s of type %s."%(
-                raise TypeError('type %s not supported' % str(type(value)))
+                raise TypeError(
-        if isinstance(key, int):
+        if isinstance(key, integer_types):
-        if isinstance(key, py_slice):
+        elif isinstance(key, py_slice):
-                raise ValueError('NDArray only supports continuous slicing on axis 0')
+                raise ValueError("NDArray only supports slicing with step size 1.")
-        if isinstance(key, tuple):
+        elif isinstance(key, tuple):
-                if isinstance(slice_i, int):
+                if isinstance(slice_i, integer_types):
-                        raise ValueError("NDArray only supports continuous slicing.")
+                        raise ValueError("NDArray only supports slicing with step size 1.")
-                            str(slice_i)))
+                        "NDArray does not support slicing with key %s of type %s."%(
-    if isinstance(shape, int):
+    if isinstance(shape, integer_types):
-        """Create prefix and params for new `Block`."""
+        """Creates prefix and params for new `Block`."""
-    assign child `Block`s as regular attributes::
+    `Block` can be nested recursively in a tree structure. You can create and
-    Child `Block`s assigned this way will be registered and `collect_params`
+    Child `Block` assigned this way will be registered and `collect_params`
-        ParameterDict for sharing weights with the new `Block`. For example,
+        `ParameterDict` for sharing weights with the new `Block`. For example,
-        """Returns a ParameterDict containing this `Block` and all of its
+        """Returns a `ParameterDict` containing this `Block` and all of its
-        """Prefix of this Block."""
+        """Prefix of this `Block`."""
-        """Name of this Block, without '_' in the end."""
+        """Name of this `Block`, without '_' in the end."""
-        names. Should be used by a `with` statement::
+        """Returns a name space object managing a child `Block` and parameter
-        """Register block as a child of self. `Block`s assigned to self as
+        """Registers block as a child of self. `Block`s assigned to self as
-        """Initialize `Parameter`s of this Block and its children.
+        """Initializes `Parameter`s of this `Block` and its children.
-        """Override to implement forward computation using NDArray. Only
+        """Overrides to implement forward computation using `NDArray`. Only
-    Before activated with `hybridize()`, `HybridBlock` works just like normal
+    Before activating with `hybridize()`, `HybridBlock` works just like normal
-    representing the forward computation and cache it. On subsequent forwards
+    representing the forward computation and cache it. On subsequent forwards,
-        """Infer shape of Parameters from inputs."""
+        """Infers shape of Parameters from inputs."""
-        NDArray or Symbol."""
+        `NDArray` or `Symbol`."""
-        """Override to construct symbolic graph for this `Block`.
+        """Overrides to construct symbolic graph for this `Block`.
-        the loss to be weighted.
+        The loss to be weighted.
-        global scalar weight for loss
+        Global scalar weight for loss.
-        per sample weighting. Must be broadcastable to
+        Per sample weighting. Must be broadcastable to
-        shape (64, 1)
+        in the batch separately, `sample_weight` should have
-        weighted loss
+        Weighted loss
-    """Calculate the mean squared error between output and label:
+    """Calculates the mean squared error between output and label:
-    output and label can have arbitrary shape as long as they have the same
+    Output and label can have arbitrary shape as long as they have the same
-        global scalar weight for loss
+        Global scalar weight for loss.
-        per sample weighting. Must be broadcastable to
+        Per sample weighting. Must be broadcastable to
-        in the batch, sample_weight should have shape (64, 1)
+        in the batch, `sample_weight` should have shape (64, 1).
-    """Calculate the mean absolute error between output and label:
+    """Calculates the mean absolute error between output and label:
-    output and label must have the same shape.
+    Output and label must have the same shape.
-        global scalar weight for loss
+        Global scalar weight for loss.
-        per sample weighting. Must be broadcastable to
+        Per sample weighting. Must be broadcastable to
-        in the batch, sample_weight should have shape (64, 1)
+        in the batch, `sample_weight` should have shape (64, 1).
-    """Compute the softmax cross entropy loss.
+    """Computes the softmax cross entropy loss.
-    If sparse_label is True, label should contain integer category indicators:
+    If `sparse_label` is `True`, label should contain integer category indicators:
-    output.shape = (1,2,3,4) and axis = 2, label.shape should be (1,2,4)
+    Label's shape should be output's shape without the `axis` dimension. i.e. for
-    If sparse_label is False, label should cantain probability distribution
+    If `sparse_label` is `False`, label should contain probability distribution
-        The axis to sum over when computing softmax and entropy
+        The axis to sum over when computing softmax and entropy.
-        whether label is a integer array instead of probability distribution
+        Whether label is an integer array instead of probability distribution.
-        whether input is log probability (usually from log_softmax) instead
+        Whether input is a log probability (usually from log_softmax) instead
-        global scalar weight for loss
+        Global scalar weight for loss.
-        per sample weighting. Must be broadcastable to
+        Per sample weighting. Must be broadcastable to
-        in the batch, sample_weight should have shape (64, 1)
+        in the batch, `sample_weight` should have shape (64, 1).
-    label's shape should be the same as output's.
+
-        whether input is log probability (usually from log_softmax) instead
+    from_logits : bool, default is `True`
-        global scalar weight for loss
+        Global scalar weight for loss.
-        per sample weighting. Must be broadcastable to
+        Per sample weighting. Must be broadcastable to
-        in the batch, sample_weight should have shape (64, 1)
+        in the batch, `sample_weight` should have shape (64, 1).
-            net.add(Dense(20))
+            net.add(nn.Dense(10, activation='relu'))
-        """Add block on top of the stack."""
+        """Adds block on top of the stack."""
-    """Stack `HybridBlock`s sequentially.
+    """Stacks `HybridBlock`s sequentially.
-            net.add(Dense(20))
+            net.add(nn.Dense(10, activation='relu'))
-        """Add block on top of the stack."""
+        """Adds block on top of the stack."""
-    Note: the input must be a tensor with rank 2. Use flatten to convert it
+    Note: the input must be a tensor with rank 2. Use `flatten` to convert it
-        defered to the first time `forward` is called and `in_units`
+        Size of the input data. If not specified, initialization will be
-        a 2D input with shape `(batch_size, in_units)`.
+        A 2D input with shape `(batch_size, in_units)`.
-        the output would have shape `(batch_size, units)`.
+        The output would have shape `(batch_size, units)`.
-        name of activation function to use.
+        Name of activation function to use.
-    Normalize the input at each batch, i.e. applies a transformation
+    Normalizes the input at each batch, i.e. applies a transformation
-        The axis that should be normalized. This is ypically the channels
+        The axis that should be normalized. This is typically the channels
-        initialization will be defered to the first time `forward` is called
+        initialization will be deferred to the first time `forward` is called
-        Initializer for the `embeddings` matrix
+        Initializer for the `embeddings` matrix.
-    If `use_bias` is True, a bias vector is created and added to the outputs.
+    If `use_bias` is `True`, a bias vector is created and added to the outputs.
-        Specifys the dimensions of the convolution window.
+        Specifies the dimensions of the convolution window.
-        Specifys the strides of the convolution.
+        Specifies the strides of the convolution.
-        Specifys the dilation rate to use for dilated convolution.
+        Specifies the dilation rate to use for dilated convolution.
-        controls the connections between inputs and outputs.
+        Controls the connections between inputs and outputs.
-        At groups=2, the operation becomes equivalent to having two conv
+        At groups=2, the operation becomes equivalent to having two convolution
-        Convolution is perform over 'D', 'H', and 'W' dimensions.
+        Convolution is performed over 'D', 'H', and 'W' dimensions.
-        initialization will be defered to the first time `forward` is called
+        initialization will be deferred to the first time `forward` is called
-        Activation function to use. See :func:`~mxnet.nd.Activation`.
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
-    defered to the first time `forward` is called and `in_channels` will be
+    deferred to the first time `forward` is called and `in_channels` will be
-        Specifys the dimensions of the convolution window.
+        Specifies the dimensions of the convolution window.
-        Specifys the dilation rate to use for dilated convolution.
+        Specifies the dilation rate to use for dilated convolution.
-        controls the connections between inputs and outputs.
+        Controls the connections between inputs and outputs.
-        initialization will be defered to the first time `forward` is called
+        initialization will be deferred to the first time `forward` is called
-        Activation function to use. See :func:`mx.nd.Activation`.
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
-    defered to the first time `forward` is called and `in_channels` will be
+    deferred to the first time `forward` is called and `in_channels` will be
-        Specifys the dimensions of the convolution window.
+        Specifies the dimensions of the convolution window.
-        Specifys the dilation rate to use for dilated convolution.
+        Specifies the dilation rate to use for dilated convolution.
-        controls the connections between inputs and outputs.
+        Controls the connections between inputs and outputs.
-        initialization will be defered to the first time `forward` is called
+        initialization will be deferred to the first time `forward` is called
-        Activation function to use. See :func:`mx.nd.Activation`.
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
-    outputs. If `use_bias` is True,
+    outputs. If `use_bias` is `True`,
-    defered to the first time `forward` is called and `in_channels` will be
+    deferred to the first time `forward` is called and `in_channels` will be
-        Specifys the dimensions of the convolution window.
+        Specifies the dimensions of the convolution window.
-        Specifys the dilation rate to use for dilated convolution.
+        Specifies the dilation rate to use for dilated convolution.
-        controls the connections between inputs and outputs.
+        Controls the connections between inputs and outputs.
-        initialization will be defered to the first time `forward` is called
+        initialization will be deferred to the first time `forward` is called
-        Activation function to use. See :func:`mx.nd.Activation`.
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
-    defered to the first time `forward` is called and `in_channels` will be
+    deferred to the first time `forward` is called and `in_channels` will be
-        Specifys the dimensions of the convolution window.
+        Specifies the dimensions of the convolution window.
-        Specifys the dilation rate to use for dilated convolution.
+        Specifies the dilation rate to use for dilated convolution.
-        controls the connections between inputs and outputs.
+        Controls the connections between inputs and outputs.
-        initialization will be defered to the first time `forward` is called
+        initialization will be deferred to the first time `forward` is called
-        Activation function to use. See :func:`mx.nd.Activation`.
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
-    defered to the first time `forward` is called and `in_channels` will be
+    deferred to the first time `forward` is called and `in_channels` will be
-        Specifys the dimensions of the convolution window.
+        Specifies the dimensions of the convolution window.
-        Specifys the dilation rate to use for dilated convolution.
+        Specifies the dilation rate to use for dilated convolution.
-        controls the connections between inputs and outputs.
+        Controls the connections between inputs and outputs.
-        initialization will be defered to the first time `forward` is called
+        initialization will be deferred to the first time `forward` is called
-        Activation function to use. See :func:`mx.nd.Activation`.
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
-    defered to the first time `forward` is called and `in_channels` will be
+    deferred to the first time `forward` is called and `in_channels` will be
-        Specifys the dimensions of the convolution window.
+        Specifies the dimensions of the convolution window.
-        Specifys the dilation rate to use for dilated convolution.
+        Specifies the dilation rate to use for dilated convolution.
-        controls the connections between inputs and outputs.
+        Controls the connections between inputs and outputs.
-        initialization will be defered to the first time `forward` is called
+        initialization will be deferred to the first time `forward` is called
-        Activation function to use. See :func:`mx.nd.Activation`.
+        Activation function to use. See :func:`~mxnet.ndarray.Activation`.
-        If None, it will default to `pool_size`.
+        If `None`, it will default to `pool_size`.
-        When True, will use ceil instead of floor to compute the output shape.
+        When `True`, will use ceil instead of floor to compute the output shape.
-        When ceil_mode is True, ceil will be used instead of floor in this
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        If None, it will default to `pool_size`.
+        If `None`, it will default to `pool_size`.
-        When True, will use ceil instead of floor to compute the output shape.
+        When `True`, will use ceil instead of floor to compute the output shape.
-        When ceil_mode is True, ceil will be used instead of floor in this
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        If None, it will default to `pool_size`.
+        If `None`, it will default to `pool_size`.
-        When True, will use ceil instead of floor to compute the output shape.
+        When `True`, will use ceil instead of floor to compute the output shape.
-        When ceil_mode is True, ceil will be used instead of floor in this
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        If None, it will default to `pool_size`.
+        If `None`, it will default to `pool_size`.
-        When True, will use ceil instead of floor to compute the output shape.
+        When `True`, will use ceil instead of floor to compute the output shape.
-        When ceil_mode is True, ceil will be used instead of floor in this
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        If None, it will default to `pool_size`.
+        If `None`, it will default to `pool_size`.
-        When ceil_mode is True, ceil will be used instead of floor in this
+        When `ceil_mode` is `True`, ceil will be used instead of floor in this
-        If None, it will default to `pool_size`.
+        If `None`, it will default to `pool_size`.
-        When ceil_mode is True, ceil will be used instead of floor in this
+        When `ceil_mode` is `True,` ceil will be used instead of floor in this
-        - 'null' means gradient is not reqested for this parameter. gradient arrays
+        - 'null' means gradient is not requested for this parameter. gradient arrays
-        unknown shaped can be used for `Symbol` API, but `init` will throw an error
+        unknown shape can be used for `Symbol` API, but `init` will throw an error
-        Weight decay multiplier (L2 regulerizer coefficient). Works similarly to lr_mult.
+        Weight decay multiplier (L2 regularizer coefficient). Works similar to lr_mult.
-        """Intialize parameter and gradient arrays. Only used for `NDArray` API.
+        """Initializes parameter and gradient arrays. Only used for `NDArray` API.
-            their values consistent when updating. Normally nn.Trainer does this for you.
+            their values consistent when updating. Normally `gluon.Trainer` does this for you.
-            Default initializer is used when both `init` and `Parameter.init` are None.
+            Default initializer is used when both `init` and `Parameter.init` are `None`.
-        """(Re)init by loading from data."""
+        """(Re)initializes by loading from data."""
-        """Finish deferred initialization."""
+        """Finishes deferred initialization."""
-        """Set data and grad."""
+        """Sets data and grad."""
-        """Set this parameter's value on all contexts to data."""
+        """Sets this parameter's value on all contexts to data."""
-        intialized on this context before.
+        initialized on this context before.
-        """Returns a list of contexts this parameter is initialized on"""
+        """Returns a list of contexts this parameter is initialized on."""
-        """Set gradient buffer on all contexts to 0. No action is taken if
+        """Sets gradient buffer on all contexts to 0. No action is taken if
-        If not None, when this dict's get method creates a new parameter, will
+        If not `None`, when this dict's `get` method creates a new parameter, will
-        parameters with another Block.
+        parameters with another `Block`.
-        with `get`"""
+        with `get`."""
-        found, `get` will create a new Parameter with key-word arguments and
+        """Retrieves a `Parameter` with name `self.prefix+name`. If not found,
-            name of the desired Parameter. It will be prepended with this dictionary's
+            Name of the desired Parameter. It will be prepended with this dictionary's
-            The rest of key-word arguments for the created Parameter.
+            The rest of key-word arguments for the created `Parameter`.
-            The created or retrieved Parameter.
+            The created or retrieved `Parameter`.
-        """Copy all Parameters in `other` to self."""
+        """Copies all Parameters in `other` to self."""
-        API. Has no effect when using `Symbol` API.
+        """Initializes all Parameters managed by this dictionary to be used for `NDArray`
-            Otherwise `Parameter.init` takes precedence.
+            Global default Initializer to be used when `Parameter.init` is `None`.
-            Keep a copy of Parameters on one or many context(s).
+            Keeps a copy of Parameters on one or many context(s).
-        """Set all Parameters' gradient buffer to 0."""
+        """Sets all Parameters' gradient buffer to 0."""
-        (this prefix is also used for names of weights if `params` is None
+        (this prefix is also used for names of weights if `params` is `None`
-        A new Parameter container is created if `params` is None.
+        A new Parameter container is created if `params` is `None`.
-            symbol.var etc. Use symbol.var if you want to directly
+            For Symbol API, func can be `symbol.zeros`, `symbol.uniform`,
-            For NDArray API, func can be ndarray.zeros, ndarray.ones, etc.
+            For NDArray API, func can be `ndarray.zeros`, `ndarray.ones`, etc.
-            mean, std, dtype, etc.
+            Additional keyword arguments passed to func. For example
-        """Unroll an RNN cell across time steps.
+        """Unrolls an RNN cell across time steps.
-            number of steps to unroll
+            Number of steps to unroll.
-            or (length, batch_size, ...) if layout == 'TNC'.
+            (batch_size, length, ...) if `layout` is 'NTC',
-            Created from `begin_state()` if None.
+            Created from `begin_state()` if `None`.
-            If None, output whatever is faster
+            If `False`, returns outputs as a list of Symbols.
-            The type of this symbol is same as the output of begin_state().
+            The type of this symbol is same as the output of `begin_state()`.
-        """Unroll the recurrent cell for one time step.
+        """Unrolls the recurrent cell for one time step.
-            input symbol, 2D, batch_size * num_units
+            Input symbol, 2D, of shape (batch_size * num_units).
-            This can be used as input state to the next time step
+            The type of this symbol is same as the output of `begin_state()`.
-        number of units in output symbol
+        Number of units in output symbol
-        type of activation function.
+        Type of activation function.
-        (and name of weight if params is None)
+        Prefix for name of `Block`s
-        created if None.
+        Container for weight sharing between cells.
-        number of units in output symbol.
+        Number of units in output symbol.
-        Initializer for the bias vector. By default bias for the forget
+        Initializer for the bias vector. By default, bias for the forget
-        (and name of weight if params is None)
+        Prefix for name of `Block`s
-        created if None.
+        Container for weight sharing between cells.
-        number of units in output symbol.
+        Number of units in output symbol.
-        (and name of weight if params is None)
+        (and name of weight if params is `None`).
-        created if None.
+        Container for weight sharing between cells.
-    """Sequantially stacking multiple RNN cells."""
+    """Sequentially stacking multiple RNN cells."""
-        """Append a cell into the stack.
+        """Appends a cell into the stack.
-    """Apply dropout on input.
+    """Applies dropout on input.
-        percentage of elements to drop out, which
+        Percentage of elements to drop out, which
-    no longer be called directly. The modifer cell
+    no longer be called directly. The modifier cell
-    """Apply Zoneout on base cell."""
+    """Applies Zoneout on base cell."""
-        cell for forward unrolling
+        Cell for forward unrolling
-        cell for backward unrolling
+        Cell for backward unrolling
-    """implementation of recurrent layers."""
+    """Implementation of recurrent layers."""
-        """Unfuse the fused RNN in to a stack of rnn cells."""
+        """Unfuses the fused RNN in to a stack of rnn cells."""
-        func : callable, default symbol.zeros
+            Only required for `NDArray` API. Size of the batch ('N' in layout).
-            symbol.var etc. Use symbol.var if you want to directly
+            For Symbol API, func can be `symbol.zeros`, `symbol.uniform`,
-            For NDArray API, func can be ndarray.zeros, ndarray.ones, etc.
+            For NDArray API, func can be `ndarray.zeros`, `ndarray.ones`, etc.
-            mean, std, dtype, etc.
+            Additional keyword arguments passed to func. For example
-    r"""Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence.
+    r"""Applies a multi-layer Elman RNN with `tanh` or `ReLU` non-linearity to an input sequence.
-        The number of features in the hidden state h
+        The number of features in the hidden state h.
-        RNN layer except the last layer
+        RNN layer except the last layer.
-        If True, becomes a bidirectional RNN.
+        If `True`, becomes a bidirectional RNN.
-        The number of features in the hidden state h
+        The number of features in the hidden state h.
-        RNN layer except the last layer
+        RNN layer except the last layer.
-        If True, becomes a bidirectional RNN.
+        If `True`, becomes a bidirectional RNN.
-        Initializer for the bias vector. By default bias for the forget
+        Initializer for the bias vector. By default, bias for the forget
-    params : ParameterDict or None
+    params : `ParameterDict` or `None`
-        `(num_layers, batch_size, 2*num_hidden)`
+        `(num_layers, batch_size, 2*num_hidden)`.
-    be used together with autograd.
+    """Applies an `Optimizer` on a set of Parameters. Trainer should
-        key-word arguments to be passed to optimizer constructor. For example,
+        Key-word arguments to be passed to optimizer constructor. For example,
-        autograd.compute_gradient and outside of record() scope.
+        """Makes one step of parameter update. Should be called after
-            Batch size of data processed. Gradient will be normalized by 1/batch_size.
+            Batch size of data processed. Gradient will be normalized by `1/batch_size`.
-    """Split a NDArray into num_slice slices along batch_axis.
+    """Splits an NDArray into `num_slice` slices along `batch_axis`.
-        If True, An error will be raised when `num_slice` does not evenly
+        If `True`, an error will be raised when `num_slice` does not evenly
-        Return value is a list even if num_slice is 1.
+        Return value is a list even if `num_slice` is 1.
-    each slice to one context in ctx_list.
+    """Splits an NDArray into `len(ctx_list)` slices along `batch_axis` and loads
-        A list of Contexts
+        A list of Contexts.
-    list of NDArray, each corresponds to a context in ctx_list.
+    list of NDArray
-    """Rescales NDArrays so that the sum of their 2-norm is smaller than max_norm.
+    """Rescales NDArrays so that the sum of their 2-norm is smaller than `max_norm`.
-            _check_shapes_equal(label, pred_label)
+            _check_lengths_equal(label, pred_label)
-            _check_shapes_equal(label, pred_label)
+            _check_lengths_equal(label, pred_label)
-            _check_shapes_equal(label, pred)
+            _check_lengths_equal(label, pred)
-        assert len(labels) == len(preds)
+        _check_lengths_equal(labels, preds)
-            _check_shapes_equal(label, pred)
+            _check_lengths_equal(label, pred)
-             self.body = nn.HSequential()
+             self.body = nn.HybridSequential()
-        layer = nn.HSequential()
+        layer = nn.HybridSequential()
-            self.body = nn.HSequential()
+            self.body = nn.HybridSequential()
-        layer = nn.HSequential()
+        layer = nn.HybridSequential()
-from .. import symbol, ndarray
+from .. import symbol, ndarray, initializer
-                super(Net, self).__init__(**kwargs)
+                super(Model, self).__init__(**kwargs)
-            dense1 = nn.Dense(20, params=dense1.collect_params())
+            dense1 = nn.Dense(20, params=dense0.collect_params())
-                    "Please use HSequential instead of Sequential.")
+                    "Please use HybridSequential instead of Sequential.")
-class HSequential(HybridBlock):
+class HybridSequential(HybridBlock):
-        super(HSequential, self).__init__(prefix=prefix, params=params)
+        super(HybridSequential, self).__init__(prefix=prefix, params=params)
-                 weight_initializer=None, bias_initializer=None,
+                 weight_initializer=None, bias_initializer='zeros',
-                                          init=weight_initializer)
+                                          init=weight_initializer,
-                                            init=bias_initializer)
+                                            init=bias_initializer,
-                                     shape=(in_channels,), init=gamma_initializer)
+                                     shape=(in_channels,), init=gamma_initializer,
-                                    shape=(in_channels,), init=beta_initializer)
+                                    shape=(in_channels,), init=beta_initializer,
-                                            init=running_mean_initializer)
+                                            init=running_mean_initializer,
-                                           init=running_variance_initializer)
+                                           init=running_variance_initializer,
-                                      init=weight_initializer)
+                                      init=weight_initializer,
-                 weight_initializer=None, bias_initializer=None,
+                 weight_initializer=None, bias_initializer='zeros',
-                                          init=weight_initializer)
+                                          init=weight_initializer,
-                                            init=bias_initializer)
+                                            init=bias_initializer,
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-                 weight_initializer=None, bias_initializer=None,
+                 weight_initializer=None, bias_initializer='zeros',
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-                 bias_initializer=None, in_channels=0, **kwargs):
+                 bias_initializer='zeros', in_channels=0, **kwargs):
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-                 use_bias=True, weight_initializer=None, bias_initializer=None,
+                 use_bias=True, weight_initializer=None, bias_initializer='zeros',
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-                 weight_initializer=None, bias_initializer=None,
+                 weight_initializer=None, bias_initializer='zeros',
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-                 bias_initializer=None, in_channels=0, **kwargs):
+                 bias_initializer='zeros', in_channels=0, **kwargs):
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-                 bias_initializer=None, in_channels=0, **kwargs):
+                 bias_initializer='zeros', in_channels=0, **kwargs):
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-    Input Shape:
+    Input shape:
-    Output Shape:
+    Output shape:
-    will then hold a copy of the the parameter on each `Context`. If `grad_req` is
+    `Parameter` holds a copy of the the parameter on each `Context` after
-        b = mx.nn.Parameter('fc_bias', shape(64,), init=mx.init.Zero())
+        w = mx.gluon.Parameter('fc_weight', shape=(64, 100), init=mx.init.Xavier())
-                 lr_mult=1.0, wd_mult=1.0, init=None):
+                 lr_mult=1.0, wd_mult=1.0, init=None, allow_deferred_init=False):
-                   allow_deferring=True):
+    def initialize(self, init=None, ctx=None, default_init=initializer.Uniform()):
-            if allow_deferring:
+        if init is None:
-                             "to first forward pass."%(self.name, str(self.shape)))
+                             "invalid shape: %s."%(self.name, str(self.shape)))
-            ctx = context.current_context()
+            list_ctx = self.list_ctx()
-            ctx = context.current_context()
+            list_ctx = self.list_ctx()
-    def initialize(self, init=initializer.Xavier(), ctx=None, verbose=False):
+    def initialize(self, init=initializer.Uniform(), ctx=None, verbose=False):
-    """HRecurrentCell supports both Symbol and NDArray forwarding."""
+class HybridRecurrentCell(RecurrentCell, HybridBlock):
-        super(HRecurrentCell, self).__init__(prefix=prefix, params=params)
+        super(HybridRecurrentCell, self).__init__(prefix=prefix, params=params)
-class RNNCell(HRecurrentCell):
+class RNNCell(HybridRecurrentCell):
-                 i2h_bias_initializer=None, h2h_bias_initializer=None,
+                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',
-                                          init=i2h_weight_initializer)
+                                          init=i2h_weight_initializer,
-                                          init=h2h_weight_initializer)
+                                          init=h2h_weight_initializer,
-                                        init=i2h_bias_initializer)
+                                        init=i2h_bias_initializer,
-                                        init=h2h_bias_initializer)
+                                        init=h2h_bias_initializer,
-class LSTMCell(HRecurrentCell):
+class LSTMCell(HybridRecurrentCell):
-                 i2h_bias_initializer='lstmbias', h2h_bias_initializer=None,
+                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',
-                                          init=i2h_weight_initializer)
+                                          init=i2h_weight_initializer,
-                                          init=h2h_weight_initializer)
+                                          init=h2h_weight_initializer,
-                                        init=i2h_bias_initializer)
+                                        init=i2h_bias_initializer,
-                                        init=h2h_bias_initializer)
+                                        init=h2h_bias_initializer,
-class GRUCell(HRecurrentCell):
+class GRUCell(HybridRecurrentCell):
-                 i2h_bias_initializer=None, h2h_bias_initializer=None,
+                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',
-                                          init=i2h_weight_initializer)
+                                          init=i2h_weight_initializer,
-                                          init=h2h_weight_initializer)
+                                          init=h2h_weight_initializer,
-                                        init=i2h_bias_initializer)
+                                        init=i2h_bias_initializer,
-                                        init=h2h_bias_initializer)
+                                        init=h2h_bias_initializer,
-class DropoutCell(HRecurrentCell):
+class DropoutCell(HybridRecurrentCell):
-class ModifierCell(HRecurrentCell):
+class ModifierCell(HybridRecurrentCell):
-class BidirectionalCell(HRecurrentCell):
+class BidirectionalCell(HybridRecurrentCell):
-                                    init=i2h_weight_initializer))
+                                    init=i2h_weight_initializer,
-                                    init=h2h_weight_initializer))
+                                    init=h2h_weight_initializer,
-                                    init=i2h_bias_initializer))
+                                    init=i2h_bias_initializer,
-                                    init=h2h_bias_initializer))
+                                    init=h2h_bias_initializer,
-    >>> rnn = nn.RNN(100, 3)
+    >>> layer = mx.gluon.rnn.RNN(100, 3)
-    >>> output, hn = rnn(input, h0)
+    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
-                 i2h_bias_initializer=None, h2h_bias_initializer=None,
+                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',
-    >>> rnn = nn.LSTM(100, 3)
+    >>> layer = mx.gluon.rnn.LSTM(100, 3)
-    >>> output, hn = rnn(input, (h0, c0))
+    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
-                 i2h_bias_initializer='lstmbias', h2h_bias_initializer=None,
+                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',
-    >>> rnn = nn.GRU(100, 2)
+    >>> layer = mx.gluon.rnn.GRU(100, 3)
-    >>> output, hn = rnn(input, h0)
+    >>> h0 = mx.nd.random_uniform(shape=(3, 3, 100))
-                 i2h_bias_initializer=None, h2h_bias_initializer=None,
+                 i2h_bias_initializer='zeros', h2h_bias_initializer='zeros',
-        {'learning_rate': 0.1}
+        key-word arguments to be passed to optimizer constructor. For example,
-        self._scale = optimizer_params.get('rescale_grad', 1.0)
+        if isinstance(params, (dict, ParameterDict)):
-                                    shape_info, self.context)
+        return '\n%s\n<%s %s @%s>' % (str(self.asnumpy()),
-                if isinstance(slice_i, py_slice):
+                elif isinstance(slice_i, py_slice):
-                    assert slice_i.step is None
+                    assert slice_i.step is None, \
-
+            shape = self.shape
-from . import symbol
+from . import symbol, context
-    def __init__(self, need_top_grad=False):
+    def __init__(self, need_top_grad=True):
-                                       aux=tensors[4])
+                            with ctx:
-                                        aux=tensors[4])
+                            with ctx:
-    model.collect_params().initialize()
+    model.collect_params().initialize(mx.init.Xavier(magnitude=2.24))
-        nn.Conv3D(16, (1, 8, 4), in_channels=4),
+        nn.Conv3D(16, (1, 8, 4), in_channels=4, activation='relu'),
-    assert_almost_equal(out, np_out, rtol=1e-4)
+    assert_almost_equal(out, np_out, rtol=1e-4, atol=1e-5)
-    assert_almost_equal(exe.outputs[0].asnumpy(), np_out, rtol=1e-5)
+    assert_almost_equal(exe.outputs[0].asnumpy(), np_out, rtol=1e-4, atol=1e-5)
-        label_shape, pred_shape = labels.shape, preds.shape
+def _check_shapes_equal(labels, preds):
-        check_label_shapes(labels, preds)
+        _check_lengths_equal(labels, preds)
-            check_label_shapes(label, pred_label)
+            _check_shapes_equal(label, pred_label)
-        check_label_shapes(labels, preds)
+        _check_lengths_equal(labels, preds)
-            check_label_shapes(label, pred_label)
+            _check_shapes_equal(label, pred_label)
-        check_label_shapes(labels, preds)
+        _check_lengths_equal(labels, preds)
-            check_label_shapes(label, pred)
+            _check_shapes_equal(label, pred)
-        check_label_shapes(labels, preds)
+        _check_lengths_equal(labels, preds)
-        check_label_shapes(labels, preds)
+        _check_lengths_equal(labels, preds)
-        check_label_shapes(labels, preds)
+        _check_lengths_equal(labels, preds)
-        check_label_shapes(labels, preds)
+        _check_lengths_equal(labels, preds)
-            check_label_shapes(labels, preds)
+            _check_lengths_equal(labels, preds)
-        if self.count + self.batch_size < len(self.filenames):
+        from PIL import Image
-                image = mx.img.imdecode(binary_image, flag=self.flag)
+                image = Image.open(fn).convert('YCbCr').split()[0]
-            label = [mx.nd.transpose(label, axes=(0, 3, 1, 2)).astype('float32')]
+            data = [mx.nd.transpose(data, axes=(0, 3, 1, 2)).astype('float32')/255]
-parser.add_argument('--gpus', type=int, default=0, help='number of GPUs to use')
+parser.add_argument('--upscale_factor', type=int, default=3, help="super resolution upscale factor. default is 3.")
-batch_size = opt.batch_size
+batch_size, test_batch_size = opt.batch_size, opt.test_batch_size
-                           batch_size, color_flag,
+                           test_batch_size, color_flag,
-        return iters
+
-ctx = [mx.gpu(i) for i in range(opt.gpus)] if opt.gpus > 0 else [mx.cpu()]
+ctx = [mx.gpu(0)] if opt.use_gpu else [mx.cpu()]
-            self.conv4 = nn.Conv2D(upscale_factor ** 2, (3, 3), strides=(1, 1), padding=(1, 1), in_channels=32)
+            self.conv1 = nn.Conv2D(64, (5, 5), strides=(1, 1), padding=(2, 2))
-        metric.reset()
+        metric.reset()
-        print('training mae at epoch %d: %s=%f'%(i, name, acc))
+        print('training mse at epoch %d: %s=%f'%(i, name, acc))
-    net.collect_params().load('superres.params')
+    net.collect_params().load('superres.params', ctx=ctx)
-    out_img_y *= 255.0
+    data = mx.nd.expand_dims(mx.nd.expand_dims(mx.nd.array(y), axis=0), axis=0)
-    out_img.save('resolved.jpg')
+    out_img.save('resolved.png')
-        self.label = mx.nd.array(label, dtype=self.dtype)
+        self.data = mx.nd.array(data, dtype=self.dtype, ctx=mx.Context('cpu_pinned', 0))
-MOCK_MODULES = ['numpy', 'numpy.testing', 'scipy', 'scipy.sparse', 'sklearn', 'matplotlib']
+MOCK_MODULES = ['scipy', 'scipy.sparse', 'sklearn']
-def _get_blocks(lang, lines):
+def _get_blocks(lines):
-                yield (pre_in_code, cur_block)
+                yield (pre_in_code, pre_lang, cur_block)
-        yield (pre_in_code, cur_block)
+        yield (pre_in_code, pre_lang, cur_block)
-    for in_code, lines in _get_blocks(lang, lines):
+    for in_code, blk_lang, lines in _get_blocks(lines):
-            "source":  '\n'.join(lines)
+            "source":  src
-        ipynb = out_prefix + '_' + lang + '.ipynb'
+        ipynb = out_prefix
-        source[i] = '\n'.join(lines)
+        # # then add lang buttons
-            axis_flags = np.random.randint(0, 2, size=ndim)
+            axis_flags = np.random.randint(-5, 6, size=ndim)
-from mxnet.foo import nn
+from mxnet import gluon
-class Policy(foo.Block):
+class Policy(gluon.Block):
-loss = foo.loss.L1Loss()
+trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': 3e-2})
-from mxnet.foo import nn
+from mxnet import gluon
-trainerD = foo.Trainer(netD.collect_params(), 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})
+trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})
-loss = foo.loss.SoftmaxCrossEntropyLoss()
+loss = gluon.loss.SoftmaxCrossEntropyLoss()
-from mxnet.foo import nn
+from mxnet import gluon, autograd
-parser = argparse.ArgumentParser(description='MXNet Foo MNIST Example')
+parser = argparse.ArgumentParser(description='MXNet Gluon MNIST Example')
-    trainer = foo.Trainer(net.collect_params(), 'sgd',
+    trainer = gluon.Trainer(net.collect_params(), 'sgd',
-    loss = foo.loss.SoftmaxCrossEntropyLoss()
+    loss = gluon.loss.SoftmaxCrossEntropyLoss()
-from mxnet.foo import nn
+from mxnet import gluon
-class BasicBlockV1(foo.HybridBlock):
+class BasicBlockV1(gluon.HybridBlock):
-class BottleneckV1(foo.HybridBlock):
+class BottleneckV1(gluon.HybridBlock):
-class ResnetV1(foo.HybridBlock):
+class ResnetV1(gluon.HybridBlock):
-class BasicBlockV2(foo.HybridBlock):
+class BasicBlockV2(gluon.HybridBlock):
-class BottleneckV2(foo.HybridBlock):
+class BottleneckV2(gluon.HybridBlock):
-class ResnetV2(foo.HybridBlock):
+class ResnetV2(gluon.HybridBlock):
-        label = foo.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)
+        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
-    trainer = foo.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})
+    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})
-    loss = foo.loss.SoftmaxCrossEntropyLoss()
+    loss = gluon.loss.SoftmaxCrossEntropyLoss()
-            label = foo.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)
+            data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
-from mxnet.foo import nn
+from mxnet import gluon
-class SuperResolutionNet(foo.Block):
+class SuperResolutionNet(gluon.Block):
-        label = foo.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)
+        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
-    trainer = foo.Trainer(net.collect_params(), 'adam', {'learning_rate': opt.lr})
+    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': opt.lr})
-    loss = foo.loss.L2Loss()
+    loss = gluon.loss.L2Loss()
-            label = foo.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)
+            data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
-from mxnet.foo import nn, rnn
+from mxnet import gluon
-class RNNModel(foo.Block):
+class RNNModel(gluon.Block):
-from mxnet.foo import nn, rnn
+from mxnet import gluon, autograd
-trainer = foo.Trainer(model.collect_params(), 'sgd',
+trainer = gluon.Trainer(model.collect_params(), 'sgd',
-loss = foo.loss.SoftmaxCrossEntropyLoss()
+loss = gluon.loss.SoftmaxCrossEntropyLoss()
-            foo.utils.clip_global_norm(grads, args.clip * args.batch_size)
+            gluon.utils.clip_global_norm(grads, args.clip * args.batch_size)
-from . import foo
+from . import gluon
-        from mxnet.foo import Block, nn
+        from mxnet.gluon import Block, nn
-from test_foo_rnn import *
+from test_gluon_rnn import *
-    check_rnn_layer(foo.rnn.GRU(100, num_layers=3))
+    check_rnn_layer(gluon.rnn.RNN(100, num_layers=3))
-    check_rnn_layer(foo.rnn.LSTM(100, num_layers=3, bidirectional=True))
+    check_rnn_layer(gluon.rnn.LSTM(100, num_layers=3, bidirectional=True))
-from mxnet.foo import nn
+from mxnet import gluon
-        labels = foo.utils.split_and_load(batch.label[0], ctx_list, batch_axis=0)
+        datas = gluon.utils.split_and_load(batch.data[0], ctx_list, batch_axis=0)
-    trainer = foo.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})
+    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.5})
-    loss = foo.loss.SoftmaxCrossEntropyLoss()
+    loss = gluon.loss.SoftmaxCrossEntropyLoss()
-            labels = foo.utils.split_and_load(batch.label[0], ctx_list, batch_axis=0)
+            datas = gluon.utils.split_and_load(batch.data[0], ctx_list, batch_axis=0)
-from mxnet import foo
+from mxnet import gluon
-    cell = foo.rnn.RNNCell(100, prefix='rnn_')
+    cell = gluon.rnn.RNNCell(100, prefix='rnn_')
-    cell = foo.rnn.LSTMCell(100, prefix='rnn_')
+    cell = gluon.rnn.LSTMCell(100, prefix='rnn_')
-    stack.add(foo.rnn.LSTMCell(100, i2h_bias_initializer=mx.init.LSTMBias(forget_bias), prefix='l1_'))
+    stack = gluon.rnn.SequentialRNNCell()
-    cell = foo.rnn.GRUCell(100, prefix='rnn_')
+    cell = gluon.rnn.GRUCell(100, prefix='rnn_')
-    cell = foo.rnn.ResidualCell(foo.rnn.GRUCell(50, prefix='rnn_'))
+    cell = gluon.rnn.ResidualCell(gluon.rnn.GRUCell(50, prefix='rnn_'))
-                foo.rnn.GRUCell(25, prefix='rnn_r_')))
+    cell = gluon.rnn.ResidualCell(
-    cell = foo.rnn.SequentialRNNCell()
+    cell = gluon.rnn.SequentialRNNCell()
-            cell.add(foo.rnn.ResidualCell(foo.rnn.LSTMCell(100, prefix='rnn_stack%d_' % i)))
+            cell.add(gluon.rnn.ResidualCell(gluon.rnn.LSTMCell(100, prefix='rnn_stack%d_' % i)))
-            cell.add(foo.rnn.LSTMCell(100, prefix='rnn_stack%d_'%i))
+            cell.add(gluon.rnn.LSTMCell(100, prefix='rnn_stack%d_'%i))
-            foo.rnn.LSTMCell(100, prefix='rnn_r0_'),
+    cell = gluon.rnn.BidirectionalCell(
-    cell = foo.rnn.ZoneoutCell(foo.rnn.RNNCell(100, prefix='rnn_'), zoneout_outputs=0.5,
+    cell = gluon.rnn.ZoneoutCell(gluon.rnn.RNNCell(100, prefix='rnn_'), zoneout_outputs=0.5,
-    check_rnn_forward(foo.rnn.GRUCell(100, input_size=200), mx.nd.ones((8, 3, 200)))
+    check_rnn_forward(gluon.rnn.LSTMCell(100, input_size=200), mx.nd.ones((8, 3, 200)))
-                                       foo.rnn.LSTMCell(100, input_size=200))
+    bilayer = gluon.rnn.BidirectionalCell(gluon.rnn.LSTMCell(100, input_size=200),
-    check_rnn_forward(foo.rnn.DropoutCell(0.5), mx.nd.ones((8, 3, 200)))
+    check_rnn_forward(gluon.rnn.DropoutCell(0.5), mx.nd.ones((8, 3, 200)))
-    check_rnn_forward(foo.rnn.ZoneoutCell(foo.rnn.LSTMCell(100, input_size=200),
+    check_rnn_forward(gluon.rnn.ZoneoutCell(gluon.rnn.LSTMCell(100, input_size=200),
-    net.add(foo.rnn.GRUCell(100, input_size=100))
+    net = gluon.rnn.SequentialRNNCell()
-from mxnet import foo
+from mxnet import gluon
-    loss = foo.loss.L1Loss()
+    loss = gluon.loss.L1Loss()
-    loss = foo.loss.L1Loss(weight=0.5)
+    loss = gluon.loss.L1Loss(weight=0.5)
-    loss = foo.loss.L1Loss()
+    loss = gluon.loss.L1Loss()
-    loss = foo.loss.L2Loss()
+    loss = gluon.loss.L2Loss()
-    loss = foo.loss.L2Loss(weight=0.25)
+    loss = gluon.loss.L2Loss(weight=0.25)
-    loss = foo.loss.L2Loss()
+    loss = gluon.loss.L2Loss()
-    loss = foo.loss.SoftmaxCrossEntropyLoss()
+    loss = gluon.loss.SoftmaxCrossEntropyLoss()
-    Loss = foo.loss.SoftmaxCrossEntropyLoss()
+    Loss = gluon.loss.SoftmaxCrossEntropyLoss()
-    Loss = foo.loss.L2Loss()
+    Loss = gluon.loss.L2Loss()
-    Loss = foo.loss.L1Loss()
+    Loss = gluon.loss.L1Loss()
-    Loss = foo.loss.SoftmaxCrossEntropyLoss()
+    Loss = gluon.loss.SoftmaxCrossEntropyLoss()
-    Loss = foo.loss.SoftmaxCrossEntropyLoss()
+    Loss = gluon.loss.SoftmaxCrossEntropyLoss()
-from mxnet.foo import nn
+from mxnet import gluon
-    p = foo.Parameter('weight', shape=(10, 10))
+    p = gluon.Parameter('weight', shape=(10, 10))
-    params = foo.ParameterDict('net_')
+    params = gluon.ParameterDict('net_')
-    class Net(foo.Block):
+    class Net(gluon.Block):
-    res = foo.utils.split_data(x, num_slice, batch_axis, **kwargs)
+    res = gluon.utils.split_data(x, num_slice, batch_axis, **kwargs)
-class Policy(nn.Layer):
+class Policy(foo.Block):
-
+net.collect_params().initialize(mx.init.Uniform(0.02))
-        final_nodes = [loss]
+        L = sum([loss(value, mx.nd.array([r])) for r, value in zip(rewards, values)])
-netD.all_params().initialize(mx.init.Normal(0.02), ctx=ctx)
+netG.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)
-trainerD = foo.Trainer(netD.all_params(), 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})
+trainerG = foo.Trainer(netG.collect_params(), 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})
-            errD_real = foo.loss.softmax_cross_entropy_loss(output, real_label)
+            errD_real = loss(output, real_label)
-            errD_fake = foo.loss.softmax_cross_entropy_loss(output, fake_label)
+            errD_fake = loss(output, fake_label)
-            errG = foo.loss.softmax_cross_entropy_loss(output, real_label)
+            errG = loss(output, real_label)
-import numpy as np
+import argparse
-from mxnet import autograd as ag
+import numpy as np
-train_data, val_data = mnist_iterator(batch_size=100, input_shape = (784,))
+train_data, val_data = mnist_iterator(batch_size=opt.batch_size, input_shape=(28*28,))
-def test(ctxs):
+def test(ctx):
-    trainer = foo.Trainer(net.all_params(), 'sgd', {'learning_rate': 0.1})
+        data = batch.data[0].as_in_context(ctx)
-    for i in range(epoch):
+    for epoch in range(epochs):
-        test(ctxs)
+        for i, batch in enumerate(train_data):
-    net.all_params().save('mnist.params')
+    net.collect_params().save('mnist.params')
-    train(10, [mx.cpu(0), mx.cpu(1)])
+    if opt.cuda:
-from __future__ import division
+from __future__ import division, print_function
-parser.add_argument('--symbolic', action='store_true', default=False, help='whether to train in symbolic way with module.')
+parser.add_argument('--dataset', type=str, default='cifar10',
-def conv3x3(filters, stride, in_filters):
+
-                     use_bias=False, in_filters=in_filters)
+                     use_bias=False, in_channels=in_channels)
-    def __init__(self, filters, stride, downsample=False, in_filters=0, **kwargs):
+class BasicBlockV1(foo.HybridBlock):
-            self.bn1 = nn.BatchNorm(num_features=in_filters)
+            self.conv1 = conv3x3(filters, stride, in_channels)
-            self.bn2 = nn.BatchNorm(num_features=filters)
+            self.bn2 = nn.BatchNorm(in_channels=filters)
-                self.bn_ds = nn.BatchNorm(num_features=filters)
+                self.conv_ds = nn.Conv2D(filters, kernel_size=1, strides=stride, use_bias=False, in_channels=in_channels)
-    def __init__(self, filters, stride, downsample=False, in_filters=0, **kwargs):
+class BottleneckV1(foo.HybridBlock):
-            self.bn1 = nn.BatchNorm(num_features=filters//4)
+            self.conv1 = nn.Conv2D(filters//4, kernel_size=1, strides=1, in_channels=in_channels)
-            self.bn3 = nn.BatchNorm(num_features=filters)
+            self.bn2 = nn.BatchNorm(in_channels=filters//4)
-                self.bn_ds = nn.BatchNorm(num_features=filters)
+                self.conv_ds = nn.Conv2D(filters, kernel_size=1, strides=stride, use_bias=False, in_channels=in_channels)
-class ResnetV1(nn.HybridLayer):
+class ResnetV1(foo.HybridBlock):
-                 self.bn0 = nn.BatchNorm(num_features=filters[0])
+                                        in_channels=3)
-             in_filters = filters[0]
+             in_channels = filters[0]
-                 in_filters = filters[i+1]
+                                                stride, in_channels=filters[i]))
-    def _make_layer(self, block, layers, filters, stride, in_filters=0):
+    def _make_layer(self, block, layers, filters, stride, in_channels=0):
-        layer.add(block(filters, stride, True, in_filters=in_filters))
+        layer.add(block(filters, stride, True, in_channels=in_channels))
-            layer.add(block(filters, 1, False, in_filters=filters))
+            layer.add(block(filters, 1, False, in_channels=filters))
-    def __init__(self, filters, stride, downsample=False, in_filters=0, **kwargs):
+class BasicBlockV2(foo.HybridBlock):
-            self.bn2 = nn.BatchNorm(num_features=filters)
+            self.bn1 = nn.BatchNorm(in_channels=in_channels)
-                                            in_filters=in_filters)
+                                            in_channels=in_channels)
-    def __init__(self, filters, stride, downsample=False, in_filters=0, **kwargs):
+class BottleneckV2(foo.HybridBlock):
-            self.bn2 = nn.BatchNorm(num_features=filters//4)
+            self.bn1 = nn.BatchNorm(in_channels=in_channels)
-            self.bn3 = nn.BatchNorm(num_features=filters//4)
+            self.bn3 = nn.BatchNorm(in_channels=filters//4)
-                                            in_filters=in_filters)
+                                            in_channels=in_channels)
-class ResnetV2(nn.HybridLayer):
+class ResnetV2(foo.HybridBlock):
-            self.bn_data = nn.BatchNorm(num_features=3, scale=False, center=False)
+            self.bn_data = nn.BatchNorm(in_channels=3, scale=False, center=False)
-                self.bn0 = nn.BatchNorm(num_features=filters[0])
+                                       in_channels=3)
-            in_filters = filters[0]
+            in_channels = filters[0]
-                in_filters = filters[i+1]
+                                               stride, in_channels=in_channels))
-            self.bn1 = nn.BatchNorm(num_features=in_filters)
+            self.bn1 = nn.BatchNorm(in_channels=in_channels)
-            self.dense1 = nn.Dense(classes, in_units=in_filters)
+            self.dense1 = nn.Dense(classes, in_units=in_channels)
-    def _make_layer(self, block, layers, filters, stride, in_filters=0):
+    def _make_layer(self, block, layers, filters, stride, in_channels=0):
-        layer.add(block(filters, stride, True, in_filters=in_filters))
+        layer.add(block(filters, stride, True, in_channels=in_channels))
-            layer.add(block(filters, 1, False, in_filters=filters))
+            layer.add(block(filters, 1, False, in_channels=filters))
-    logging.info('validation acc: %s=%f'%metric.get())
+    return metric.get()
-    trainer = foo.Trainer(net.all_params(), 'sgd', {'learning_rate': 0.1})
+    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)
-    for i in range(epoch):
+    for epoch in range(epoch):
-        for batch in train_data:
+        for i, batch in enumerate(train_data):
-            losses = []
+            Ls = []
-                    losses.append(loss)
+                    L = loss(z, y)
-                    loss.backward()
+                for L in Ls:
-            logging.info('speed: {} samples/s'.format(batch_size/(time.time()-btic)))
+            if opt.log_interval:
-        test(ctx)
+        print('[Epoch %d] training: %s=%f'%(epoch, name, acc))
-class SuperResolutionNet(nn.Layer):
+class SuperResolutionNet(foo.Block):
-            self.conv4 = nn.Conv2D(upscale_factor ** 2, (3, 3), strides=(1, 1), padding=(1, 1), in_filters=32)
+            self.conv1 = nn.Conv2D(64, (5, 5), strides=(1, 1), padding=(2, 2), in_channels=1)
-    trainer = foo.Trainer(net.all_params(), 'adam', {'learning_rate': opt.lr})
+    net.collect_params().initialize(mx.init.Orthogonal(), ctx=ctx)
-                    ag.compute_gradient([loss])
+                    L = loss(z, y)
-    net.all_params().save('superres.params')
+    net.collect_params().save('superres.params')
-    net.all_params().load('superres.params')
+    net.collect_params().load('superres.params')
-class RNNModel(nn.Layer):
+class RNNModel(foo.Block):
-trainer = foo.Trainer(model.all_params(), 'sgd',
+model.collect_params().initialize(mx.init.Xavier(), ctx=context)
-
+loss = foo.loss.SoftmaxCrossEntropyLoss()
-    total = 0.0
+    total_L = 0.0
-    return total / ntotal
+        L = loss(output, target)
-        total = 0.0
+        total_L = 0.0
-                loss.backward()
+                L = loss(output, target)
-            grads = [i.grad(context) for i in model.all_params().values()]
+            grads = [i.grad(context) for i in model.collect_params().values()]
-            total += mx.nd.sum(loss).asscalar()
+            total_L += mx.nd.sum(L).asscalar()
-                cur_loss = total / args.batch_size / args.bptt / args.log_interval
+                cur_L = total_L / args.batch_size / args.bptt / args.log_interval
-                total = 0.0
+                    epoch, ibatch, cur_L, math.exp(cur_L)))
-        val_loss = eval(val_data)
+        val_L = eval(val_data)
-            epoch, time.time()-start_time, val_loss, math.exp(val_loss)))
+            epoch, time.time()-start_time, val_L, math.exp(val_L)))
-    print('test loss %.2f, test ppl %.2f'%(test_loss, math.exp(test_loss)))
+    test_L = eval(test_data)
-def features(input_data, num_features):
+def features(input_data, in_channels):
-    fc3 = mx.symbol.FullyConnected(data=dropout2, num_hidden=num_features)
+    fc3 = mx.symbol.FullyConnected(data=dropout2, num_hidden=in_channels)
-    """Set status to training/not training. When training, graph will be constructed
+def set_recording(is_recording):
-    is_train: bool
+    is_recording: bool
-        ctypes.c_int(is_train), ctypes.byref(prev)))
+        ctypes.c_int(is_recording), ctypes.byref(prev)))
-        self._prev = set_is_training(self._enter_state)
+        self._prev = set_recording(self._enter_state)
-            set_is_training(self._prev)
+            set_recording(self._prev)
-# pylint: disable=unused-argument
+# pylint: disable=arguments-differ
-from .. import symbol, ndarray, metric
+from .. import symbol, ndarray
-
+from .block import HybridBlock
-            extra_outputs=(), metrics=None, name='l2'):
+class L2Loss(HybridBlock):
-            extra_outputs=(), metrics=None, name='l1'):
+    def __init__(self, weight=1., batch_axis=0, **kwargs):
-                               extra_outputs=(), metrics='acc', name='ce'):
+    def __init__(self, weight=None, batch_axis=0, **kwargs):
-        where label is sparse integer or probability distribution
+    sparse_label : bool, default True
-                       extra_outputs, metrics, name)
+    def __init__(self, axis=-1, sparse_label=True, from_logits=False, weight=None,
-from .layer import *
+from .basic_layers import *
-from .layer import HybridLayer
+from ..block import HybridBlock
-class _Conv(HybridLayer):
+class _Conv(HybridBlock):
-    padding: An integer or a tuple/list of n integers,
+    channels : int
-    groups: int
+    dilation: int or tuple/list of n ints,
-        see mx.sym.Activation.
+    layout : str,
-        see Initializer.
+    use_bias: bool
-                 groups, layout, in_filters=0, activation=None, use_bias=True,
+    def __init__(self, channels, kernel_size, strides, padding, dilation,
-            self._in_filters = in_filters
+            self._channels = channels
-                'pad': padding, 'num_filter': filters, 'num_group': groups,
+                'pad': padding, 'num_filter': channels, 'num_group': groups,
-            dshape[layout.find('C')] = in_filters
+            dshape[layout.find('C')] = in_channels
-    (integers, the number of input channels).
+    If `in_channels` is not specified, `Parameter` initialization will be
-    padding: An integer or a tuple/list of 1 integers,
+    channels : int
-    groups: int
+    dilation : int or tuple/list of 1 int
-        Can be 'NCW', 'NWC', etc.
+    layout: str, default 'NCW'
-        see mx.sym.Activation.
+        respectively. Convolution is applied on the 'W' dimension.
-        see Initializer.
+    use_bias : bool
-        (batch_size, in_channel(in_filters), width) if `layout` is `NCW`.
+        (batch_size, in_channels, width) if `layout` is `NCW`.
-        where,
+        (batch_size, channels, out_width) if `layout` is `NCW`.
-        w = width, p = padding, d = dilation, k = kernel_size, s = stride
+            out_width = floor((width+2*padding-dilation*(kernel_size-1)-1)/stride)+1
-    def __init__(self, filters, kernel_size, strides=1, padding=0, dilation=1,
+    def __init__(self, channels, kernel_size, strides=1, padding=0, dilation=1,
-                 in_filters=0, **kwargs):
+                 in_channels=0, **kwargs):
-            in_filters, activation, use_bias, weight_initializer, bias_initializer, **kwargs)
+            channels, kernel_size, strides, padding, dilation, groups, layout,
-
+    If `in_channels` is not specified, `Parameter` initialization will be
-    padding: An integer or a tuple/list of 2 integers,
+    channels : int
-    groups: int
+    dilation : int or tuple/list of 2 int
-        Can be 'NCHW', 'NHWC', etc.
+    layout : str, default 'NCHW'
-        see mx.sym.Activation.
+        dimensions respectively. Convolution is applied on the 'H' and
-        see Initializer.
+    use_bias : bool
-        (batch_size, in_channel(in_filters), height, width) if `layout` is `NCHW`.
+        (batch_size, in_channels, height, width) if `layout` is `NCHW`.
-            out_height = floor((h+2*p-d*(k-1)-1)/s)+1
+        (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.
-        where,
+        out_height and out_width are calculated as::
-        w = width, h = height, p = padding, d = dilation, k = kernel_size, s = stride
+            out_height = floor((height+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1
-    def __init__(self, filters, kernel_size, strides=(1, 1), padding=(0, 0),
+    def __init__(self, channels, kernel_size, strides=(1, 1), padding=(0, 0),
-                 bias_initializer=None, in_filters=0, **kwargs):
+                 bias_initializer=None, in_channels=0, **kwargs):
-            in_filters, activation, use_bias, weight_initializer, bias_initializer, **kwargs)
+            channels, kernel_size, strides, padding, dilation, groups, layout,
-
+    If `in_channels` is not specified, `Parameter` initialization will be
-    padding: An integer or a tuple/list of 3 integers,
+    channels : int
-    groups: int
+    dilation : int or tuple/list of 3 int
-        Can be 'NCDHW', 'NDHWC', etc.
+    layout : str, default 'NCDHW'
-        see mx.sym.Activation.
+        depth dimensions respectively. Convolution is applied on the 'D',
-        see Initializer.
+    use_bias : bool
-        (batch_size, in_channel(in_filters), depth, height, width) if `layout` is `NCDHW`.
+        (batch_size, in_channels, depth, height, width) if `layout` is `NCDHW`.
-        They are calculated as follows::
+        (batch_size, channels, out_depth, out_height, out_width) if `layout` is
-            out_width = floor((w+2*p-d*(k-1)-1)/s)+1
+        out_depth, out_height and out_width are calculated as::
-        d = depth, h = height, w = width, p = padding, d = dilation, k = kernel_size, s = stride
+            out_depth = floor((depth+2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)/stride[0])+1
-    def __init__(self, filters, kernel_size, strides=(1, 1, 1), padding=(0, 0, 0),
+    def __init__(self, channels, kernel_size, strides=(1, 1, 1), padding=(0, 0, 0),
-                 in_filters=0, **kwargs):
+                 in_channels=0, **kwargs):
-            in_filters, activation, use_bias, weight_initializer, bias_initializer, **kwargs)
+            channels, kernel_size, strides, padding, dilation, groups, layout,
-    (integers, the number of input channels).
+    If `in_channels` is not specified, `Parameter` initialization will be
-    padding: An integer or a tuple/list of 1 integers,
+    channels : int
-    groups: int
+    dilation : int or tuple/list of 3 int
-        Can be 'NCW', 'NWC', etc.
+    layout : str, default 'NCW'
-        see mx.sym.Activation.
+        respectively. Convolution is applied on the 'W' dimension.
-        see Initializer.
+    use_bias : bool
-        (batch_size, in_channel(in_filters), width) if `layout` is `NCW`.
+        (batch_size, in_channels, width) if `layout` is `NCW`.
-            out_width = (w-1)*s-2*p+k+op
+        (batch_size, channels, out_width) if `layout` is `NCW`.
-        where,
+        out_width is calculated as::
-        w = width, p = padding, k = kernel_size, s = stride, op = output_padding
+            out_width = (width-1)*strides-2*padding+kernel_size+output_padding
-    def __init__(self, filters, kernel_size, strides=1, padding=0, output_padding=0,
+    def __init__(self, channels, kernel_size, strides=1, padding=0, output_padding=0,
-                 in_filters=0, **kwargs):
+                 in_channels=0, **kwargs):
-            in_filters, activation, use_bias, weight_initializer,
+            channels, kernel_size, strides, padding, dilation, groups, layout,
-    (integers, the number of input channels).
+    If `in_channels` is not specified, `Parameter` initialization will be
-    padding: An integer or a tuple/list of 2 integers,
+    channels : int
-    groups: int
+    dilation : int or tuple/list of 3 int
-        Can be 'NCHW', 'NHWC', etc.
+    layout : str, default 'NCHW'
-        see mx.sym.Activation.
+        dimensions respectively. Convolution is applied on the 'H' and
-        see Initializer.
+    use_bias : bool
-        (batch_size, in_channel(in_filters), height, width) if `layout` is `NCHW`.
+        (batch_size, in_channels, height, width) if `layout` is `NCHW`.
-            out_width = (w-1)*s-2*p+k+op
+        (batch_size, channels, out_height, out_width) if `layout` is `NCHW`.
-        where,
+        out_height and out_width are calculated as::
-        h = height, w = width, p = padding, k = kernel_size, s = stride, op = output_padding
+            out_height = (height-1)*strides[0]-2*padding[0]+kernel_size[0]+output_padding[0]
-    def __init__(self, filters, kernel_size, strides=(1, 1), padding=(0, 0),
+    def __init__(self, channels, kernel_size, strides=(1, 1), padding=(0, 0),
-                 bias_initializer=None, in_filters=0, **kwargs):
+                 bias_initializer=None, in_channels=0, **kwargs):
-            in_filters, activation, use_bias, weight_initializer,
+            channels, kernel_size, strides, padding, dilation, groups, layout,
-    (integers, the number of input channels).
+    If `in_channels` is not specified, `Parameter` initialization will be
-    padding: An integer or a tuple/list of 3 integers,
+    channels : int
-    groups: int
+    dilation : int or tuple/list of 3 int
-        Can be 'NCDHW', 'NDHWC', etc.
+    layout : str, default 'NCDHW'
-        see mx.sym.Activation.
+        depth dimensions respectively. Convolution is applied on the 'D',
-        see Initializer.
+    use_bias : bool
-        (batch_size, in_channel(in_filters), depth, height, width) if `layout` is `NCDHW`.
+        (batch_size, in_channels, depth, height, width) if `layout` is `NCDHW`.
-        where,
+        (batch_size, channels, out_depth, out_height, out_width) if `layout` is `NCDHW`.
-        op = output_padding
+            out_depth = (depth-1)*strides[0]-2*padding[0]+kernel_size[0]+output_padding[0]
-    def __init__(self, filters, kernel_size, strides=(1, 1, 1), padding=(0, 0, 0),
+    def __init__(self, channels, kernel_size, strides=(1, 1, 1), padding=(0, 0, 0),
-                 bias_initializer=None, in_filters=0, **kwargs):
+                 bias_initializer=None, in_channels=0, **kwargs):
-            in_filters, activation, use_bias, weight_initializer, bias_initializer,
+            channels, kernel_size, strides, padding, dilation, groups, layout,
-    def __init__(self, pool_size, strides, padding, global_pool, pool_type, **kwargs):
+class _Pooling(HybridBlock):
-            'pool_type': pool_type}
+            'global_pool': global_pool, 'pool_type': pool_type,
-    """Max pooling operation for temporal data.
+    """Max pooling operation for one dimensional data.
-        E.g. 2 will halve the input.
+    pool_size: int
-    padding: Integer,
+    padding: int
-        Can be 'NCW', 'NWC', etc.
+        zero-padded on both sides for padding number of points.
-        respectively. padding is applied on W dimension.
+        respectively. Pooling is applied on the W dimension.
-        (batch_size, channel, width) if `layout` is `NCW`.
+        (batch_size, channels, width) if `layout` is `NCW`.
-        out_width depends on other input parameters as well. It is calculated as follows::
+        (batch_size, channels, out_width) if `layout` is `NCW`.
-            out_width = ceil((w+2*p-ps)/s+1)
+        out_width is calculated as::
-        where,
+            out_width = floor((width+2*padding-pool_size)/strides)+1
-        w = width, p = padding, ps = pool_size, s = stride
+        When ceil_mode is True, ceil will be used instead of floor in this
-    def __init__(self, pool_size=2, strides=None, padding=0, layout='NCW', **kwargs):
+    def __init__(self, pool_size=2, strides=None, padding=0, layout='NCW',
-            pool_size, strides, padding, False, 'max', **kwargs)
+            pool_size, strides, padding, ceil_mode, False, 'max', **kwargs)
-    """Max pooling operation for spatial data.
+    """Max pooling operation for two dimensional (spatial) data.
-        E.g. 2 will halve the input.
+    pool_size: int or list/tuple of 2 ints,
-    padding: Integer or list/tuple of 2 Integers,
+    padding: int or list/tuple of 2 ints,
-        Can be 'NCHW', 'NHWC', etc.
+        zero-padded on both sides for padding number of points.
-        (batch_size, channel, height, width) if `layout` is `NCHW`.
+        (batch_size, channels, height, width) if `layout` is `NCHW`.
-        They are calculated as follows::
+        (batch_size, channels, out_height, out_width)  if `layout` is `NCHW`.
-            out_width = ceil((w+2*p-ps)/s+1)
+        out_height and out_width are calculated as::
-        where,
+            out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1
-        h = height, w = width, p = padding, ps = pool_size, s = stride
+        When ceil_mode is True, ceil will be used instead of floor in this
-    def __init__(self, pool_size=(2, 2), strides=None, padding=0, layout='NCHW', **kwargs):
+    def __init__(self, pool_size=(2, 2), strides=None, padding=0, layout='NCHW',
-            pool_size, strides, padding, False, 'max', **kwargs)
+            pool_size, strides, padding, ceil_mode, False, 'max', **kwargs)
-        E.g. 2 will halve the input.
+    pool_size: int or list/tuple of 3 ints,
-    padding: Integer or list/tuple of 3 Integers,
+    padding: int or list/tuple of 3 ints,
-        Can be 'NCDHW', 'NDHWC', etc.
+        zero-padded on both sides for padding number of points.
-        (batch_size, channel, depth, height, width) if `layout` is `NCDHW`.
+        (batch_size, channels, depth, height, width) if `layout` is `NCDHW`.
-        They are calculated as follows::
+        (batch_size, channels, out_depth, out_height, out_width) if `layout`
-            out_width = ceil((w+2*p-ps)/s+1)
+        out_depth, out_height and out_width are calculated as ::
-        where,
+            out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1
-        d = depth, h = height, w = width, p = padding, ps = pool_size, s = stride
+        When ceil_mode is True, ceil will be used instead of floor in this
-    def __init__(self, pool_size=(2, 2, 2), strides=None, padding=0, layout='NCDHW', **kwargs):
+    def __init__(self, pool_size=(2, 2, 2), strides=None, padding=0,
-            pool_size, strides, padding, False, 'max', **kwargs)
+            pool_size, strides, padding, ceil_mode, False, 'max', **kwargs)
-        E.g. 2 will halve the input.
+    pool_size: int
-    padding: Integer,
+    padding: int
-        Can be 'NCW', 'NWC', etc.
+        zero-padded on both sides for padding number of points.
-        respectively. padding is applied on W dimension.
+        respectively. padding is applied on 'W' dimension.
-        (batch_size, channel, width) if `layout` is `NCW`.
+        (batch_size, channels, width) if `layout` is `NCW`.
-        out_width depends on other input parameters as well. It is calculated as follows::
+        (batch_size, channels, out_width) if `layout` is `NCW`.
-            out_width = ceil((w+2*p-ps)/s+1)
+        out_width is calculated as::
-        where,
+            out_width = floor((width+2*padding-pool_size)/strides)+1
-        w = width, p = padding, ps = pool_size, s = stride
+        When ceil_mode is True, ceil will be used instead of floor in this
-    def __init__(self, pool_size=2, strides=None, padding=0, layout='NCW', **kwargs):
+    def __init__(self, pool_size=2, strides=None, padding=0, layout='NCW',
-            pool_size, strides, padding, False, 'avg', **kwargs)
+            pool_size, strides, padding, ceil_mode, False, 'avg', **kwargs)
-        E.g. 2 will halve the input.
+    pool_size: int or list/tuple of 2 ints,
-    padding: Integer or list/tuple of 2 Integers,
+    padding: int or list/tuple of 2 ints,
-        Can be 'NCHW', 'NHWC', etc.
+        zero-padded on both sides for padding number of points.
-        (batch_size, channel, height, width) if `layout` is `NCHW`.
+        (batch_size, channels, height, width) if `layout` is `NCHW`.
-        They are calculated as follows::
+        (batch_size, channels, out_height, out_width)  if `layout` is `NCHW`.
-            out_width = ceil((w+2*p-ps)/s+1)
+        out_height and out_width are calculated as::
-        where,
+            out_height = floor((height+2*padding[0]-pool_size[0])/strides[0])+1
-        h = height, w = width, p = padding, ps = pool_size, s = stride
+        When ceil_mode is True, ceil will be used instead of floor in this
-    def __init__(self, pool_size=(2, 2), strides=None, padding=0, layout='NCHW', **kwargs):
+    def __init__(self, pool_size=(2, 2), strides=None, padding=0,
-            pool_size, strides, padding, False, 'avg', **kwargs)
+            pool_size, strides, padding, ceil_mode, False, 'avg', **kwargs)
-        E.g. 2 will halve the input.
+    pool_size: int or list/tuple of 3 ints,
-    padding: Integer or list/tuple of 3 Integers,
+    padding: int or list/tuple of 3 ints,
-        Can be 'NCDHW', 'NDHWC', etc.
+        zero-padded on both sides for padding number of points.
-        (batch_size, channel, depth, height, width) if `layout` is `NCDHW`.
+        (batch_size, channels, depth, height, width) if `layout` is `NCDHW`.
-        They are calculated as follows::
+        (batch_size, channels, out_depth, out_height, out_width) if `layout`
-            out_width = ceil((w+2*p-ps)/s+1)
+        out_depth, out_height and out_width are calculated as ::
-        where,
+            out_depth = floor((depth+2*padding[0]-pool_size[0])/strides[0])+1
-        d = depth, h = height, w = width, p = padding, ps = pool_size, s = stride
+        When ceil_mode is True, ceil will be used instead of floor in this
-    def __init__(self, pool_size=(2, 2, 2), strides=None, padding=0, layout='NCDHW', **kwargs):
+    def __init__(self, pool_size=(2, 2, 2), strides=None, padding=0,
-            pool_size, strides, padding, False, 'avg', **kwargs)
+            pool_size, strides, padding, ceil_mode, False, 'avg', **kwargs)
-    """
+    """Global max pooling operation for temporal data."""
-            (1,), None, 0, True, 'max', **kwargs)
+            (1,), None, 0, True, True, 'max', **kwargs)
-    """
+    """Global max pooling operation for spatial data."""
-            (1, 1), None, 0, True, 'max', **kwargs)
+            (1, 1), None, 0, True, True, 'max', **kwargs)
-    """
+    """Global max pooling operation for 3D data."""
-            (1, 1, 1), None, 0, True, 'max', **kwargs)
+            (1, 1, 1), None, 0, True, True, 'max', **kwargs)
-    """
+    """Global average pooling operation for temporal data."""
-            (1,), None, 0, True, 'avg', **kwargs)
+            (1,), None, 0, True, True, 'avg', **kwargs)
-    """
+    """Global average pooling operation for spatial data."""
-            (1, 1), None, 0, True, 'avg', **kwargs)
+            (1, 1), None, 0, True, True, 'avg', **kwargs)
-    """
+    """Global max pooling operation for 3D data."""
-            (1, 1, 1), None, 0, True, 'avg', **kwargs)
+            (1, 1, 1), None, 0, True, True, 'avg', **kwargs)
-    """A Container holding parameters (weights) of layers.
+    """A Container holding parameters (weights) of `Block`s.
-                             "in_filters, num_features etc for Layers or " \
+                             "in_channels, etc for `Block`s or " \
-            "in_filters, num_features etc for Layers."%(
+            "in_channels, etc for `Block`s."%(
-            "nested child layers "%(self.name))
+            "with Block.collect_params() instead of Block.params " \
-        parameters with another layer.
+        parameters with another Block.
-from ..nn import Layer, HybridLayer
+from ..block import Block, HybridBlock
-class RecurrentCell(Layer):
+class RecurrentCell(Block):
-        Prefix for names of layers
+        Prefix for names of `Block`s
-class HRecurrentCell(RecurrentCell, HybridLayer):
+class HRecurrentCell(RecurrentCell, HybridBlock):
-        prefix for name of layers
+        prefix for name of `Block`s
-        prefix for name of layers
+        prefix for name of `Block`s
-        prefix for name of layers
+        prefix for name of `Block`s
-from ..nn import Layer
+from ..nn import Block
-class _RNNLayer(Layer):
+class _RNNLayer(Block):
-        Prefix of this layer.
+        Prefix of this `Block`.
-        Shared Parameters for this Layer.
+        Shared Parameters for this `Block`.
-        Prefix of this layer.
+        Prefix of this `Block`.
-        Shared Parameters for this Layer.
+        Shared Parameters for this `Block`.
-        Prefix of this layer.
+        Prefix of this `Block`.
-        Shared Parameters for this Layer.
+        Shared Parameters for this `Block`.
-                            "model that maked it only use a subset of the Parameters (Layers) "
+                            "model that maked it only use a subset of the Parameters (Blocks) "
-                            "warning and skip updating of Parameters with state gradient" \
+                            "warning and skip updating of Parameters with stale gradient" \
-    layer.all_params().initialize(ctx=[mx.cpu(0), mx.gpu(0)])
+    layer.collect_params().initialize(ctx=[mx.cpu(0), mx.gpu(0)])
-    trainer = foo.Trainer(net.all_params(), 'sgd', {'learning_rate': 0.5})
+    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx_list)
-                    loss.backward()
+                    L = loss(z, y)
-    net1.all_params().save('mnist.params')
+    net1.collect_params().save('mnist.params')
-    net2.all_params().load('mnist.params', ctx=[mx.cpu(0)])
+    net2.collect_params().load('mnist.params', ctx=[mx.cpu(0)])
-    assert sorted(cell.all_params().keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
+    assert sorted(cell.collect_params().keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
-    assert sorted(cell.all_params().keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
+    assert sorted(cell.collect_params().keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
-    assert sorted(cell.all_params().keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
+    assert sorted(cell.collect_params().keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
-    assert sorted(cell.all_params().keys()) == \
+    assert sorted(cell.collect_params().keys()) == \
-    assert sorted(cell.all_params().keys()) == \
+    assert sorted(cell.collect_params().keys()) == \
-    keys = sorted(cell.all_params().keys())
+    keys = sorted(cell.collect_params().keys())
-    layer.all_params().initialize()
+    layer.collect_params().initialize()
-    assert mx.nd.sum(foo.loss.l2_loss(output, label, sample_weight=weighting)).asscalar() == 6
+    loss = foo.loss.L1Loss()
-    assert sym.list_outputs()[1:] == ['data1_out_output', 'data2_out_output', 'loss2_loss']
+    loss = foo.loss.SoftmaxCrossEntropyLoss()
-    check_loss(foo.loss.softmax_cross_entropy_loss)
+    L = loss(output, label, weighting).asnumpy()
-    loss = foo.loss.softmax_cross_entropy_loss(output, l, extra_outputs=(fc2,))
+    Loss = foo.loss.SoftmaxCrossEntropyLoss()
-    assert mod.score(data_iter)[0][1] == 1.0
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 1.},
-    loss = foo.loss.l2_loss(output, l)
+    Loss = foo.loss.L2Loss()
-
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 1.},
-    loss = foo.loss.l1_loss(output, l)
+    Loss = foo.loss.L1Loss()
-    assert mod.score(data_iter)[0][1] < 0.05
+            initializer=mx.init.Uniform(0.5), eval_metric=mx.metric.Loss())
-    loss = foo.loss.softmax_cross_entropy_loss(output, l, sample_weight=w)
+    Loss = foo.loss.SoftmaxCrossEntropyLoss()
-    assert [i.shape for i in mod.get_outputs()] == [(10, 10), (10, 5)]
+    mod.fit(data_iter, num_epoch=200, optimizer_params={'learning_rate': 1.},
-    loss = foo.loss.softmax_cross_entropy_loss(output, l)
+    Loss = foo.loss.SoftmaxCrossEntropyLoss()
-    mod.fit(data_iter, num_epoch=100, optimizer_params={'learning_rate': 1.})
+    mod.fit(data_iter, num_epoch=100, optimizer_params={'learning_rate': 1.},
-    assert mod.score(data_iter)[0][1] == 1.0
+    mod.fit(data_iter, num_epoch=100, optimizer_params={'learning_rate': 1.},
-    class Net(nn.Layer):
+    class Net(foo.Block):
-    net1.all_params().initialize()
+    net2 = Net(prefix='net2_', params=net1.collect_params())
-    model.all_params().initialize()
+    model.collect_params().initialize()
-    layer.all_params().initialize()
+    layer.collect_params().initialize()
-        nn.Conv1D(16, 3, strides=3, groups=2, in_filters=4),
+        nn.Conv1D(16, 3, in_channels=4),
-        nn.Conv2D(16, (3, 4), padding=4, in_filters=4),
+        nn.Conv2D(16, (3, 4), in_channels=4),
-        nn.Conv3D(16, (3, 3, 3), padding=4, in_filters=4),
+        nn.Conv3D(16, (1, 8, 4), in_channels=4),
-    layer = nn.Conv2D(16, (3, 3), layout='NHWC', in_filters=4)
+    layer = nn.Conv2D(16, (3, 3), layout='NHWC', in_channels=4)
-    layer = nn.Conv3D(16, (3, 3, 3), layout='NDHWC', in_filters=4)
+    layer = nn.Conv3D(16, (3, 3, 3), layout='NDHWC', in_channels=4)
-    #     nn.Conv1DTranspose(16, 3, strides=3, groups=2, in_filters=4),
+    #     nn.Conv1DTranspose(16, 3, in_channels=4),
-        nn.Conv2DTranspose(16, (3, 4), strides=4, output_padding=3, in_filters=4),
+        nn.Conv2DTranspose(16, (3, 4), in_channels=4),
-    #     nn.Conv3DTranspose(16, (3, 3, 3), padding=4, in_filters=4),
+    #     nn.Conv3DTranspose(16, (1, 8, 4), in_channels=4),
-    # layer = nn.Conv2DTranspose(16, (3, 3), layout='NHWC', in_filters=4)
+    # layer = nn.Conv2DTranspose(16, (3, 3), layout='NHWC', in_channels=4)
-    # layer = nn.Conv3DTranspose(16, (3, 3, 3), layout='NDHWC', in_filters=4)
+    # layer = nn.Conv3DTranspose(16, (3, 3, 3), layout='NDHWC', in_channels=4)
-    layer = nn.BatchNorm(num_features=10)
+    layer = nn.BatchNorm(in_channels=10)
-    layer.all_params().initialize()
+    layer = nn.Conv2D(10, 2, in_channels=4)
-    layer.all_params().initialize()
+    layer = nn.Conv2D(10, 2, in_channels=4)
-    layer.all_params().initialize()
+    layer = nn.Conv2D(10, 2, in_channels=4)
-    layer.all_params().initialize()
+    layer.collect_params().initialize()
-def test(ctx):
+def test(ctxs):
-        label = foo.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+        data = foo.utils.split_and_load(batch.data[0], ctxs, batch_axis=0)
-    net.all_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)
+def train(epoch, ctxs):
-            label = foo.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+            datas = foo.utils.split_and_load(batch.data[0], ctxs, batch_axis=0)
-                for x, y in zip(data, label):
+                for x, y in zip(datas, labels):
-            metric.update(label, outputs)
+            metric.update(labels, outputs)
-        test(ctx)
+        test(ctxs)
-        label = foo.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+        data = foo.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
-            label = foo.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+            data = foo.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
-        label = foo.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+        data = foo.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
-            label = foo.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+            data = foo.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)
-    size = data.shape[batch_axis] // num_slice
+    size = data.shape[batch_axis]
-        slices = [data[i*size:(i+1)*size] for i in range(num_slice)]
+        slices = [data[i*step:(i+1)*step] if i < num_slice - 1 else data[i*step:size]
-        slices = [ndarray.slice_axis(data, i*size, (i+1)*size)
+        slices = [ndarray.slice_axis(data, batch_axis, i*step, (i+1)*step)
-    each slice into a context.
+
-        A list of Context
+        A list of Contexts
-        return [i.as_in_context(ctx) for i, ctx in zip(slices, ctx_list)]
+
-def score(net, ctx):
+def score(net, ctx_list):
-        label = foo.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+        datas = foo.utils.split_and_load(batch.data[0], ctx_list, batch_axis=0)
-        for x in data:
+        for x in datas:
-        metric.update(label, outputs)
+        metric.update(labels, outputs)
-    net.all_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)
+def train(net, epoch, ctx_list):
-            label = foo.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+            datas = foo.utils.split_and_load(batch.data[0], ctx_list, batch_axis=0)
-                for x, y in zip(data, label):
+                for x, y in zip(datas, labels):
-            metric.update(label, outputs)
+            metric.update(labels, outputs)
-                    ag.compute_gradient([loss])
+                    ag.backward([loss])
-            compute_gradient([y])
+            backward([y])
-            compute_gradient([y])
+            backward([y])
-            compute_gradient([y])
+            backward([y])
-                                        num_input=num_embed)
+            if mode == 'rnn_relu':
-        output, hidden = self.rnn.unroll(None, emb, layout='TNC', merge_outputs=True)
+        output, hidden = self.rnn(emb, hidden)
-    kernel_initializer: Initializer for the `kernel` weights matrix
+    weight_initializer: Initializer for the `kernel` weights matrix
-                 kernel_initializer=None, bias_initializer=None,
+                 weight_initializer=None, bias_initializer=None,
-                                          init=kernel_initializer)
+                                          init=weight_initializer)
-    kernel_initializer: Initializer for the `kernel` weights matrix
+    weight_initializer: Initializer for the `kernel` weights matrix
-                 kernel_initializer=None, bias_initializer=None, **kwargs):
+                 groups=1, layout='NCW', activation=None, use_bias=True,
-            in_filters, activation, use_bias, kernel_initializer, bias_initializer, **kwargs)
+            in_filters, activation, use_bias, weight_initializer, bias_initializer, **kwargs)
-    kernel_initializer: Initializer for the `kernel` weights matrix
+    weight_initializer: Initializer for the `kernel` weights matrix
-                 kernel_initializer=None, bias_initializer=None, **kwargs):
+                 dilation=(1, 1), groups=1, layout='NCHW',
-            in_filters, activation, use_bias, kernel_initializer, bias_initializer, **kwargs)
+            in_filters, activation, use_bias, weight_initializer, bias_initializer, **kwargs)
-    kernel_initializer: Initializer for the `kernel` weights matrix
+    weight_initializer: Initializer for the `kernel` weights matrix
-                 kernel_initializer=None, bias_initializer=None, **kwargs):
+                 dilation=(1, 1, 1), groups=1, layout='NCDHW', activation=None,
-            in_filters, activation, use_bias, kernel_initializer, bias_initializer, **kwargs)
+            in_filters, activation, use_bias, weight_initializer, bias_initializer, **kwargs)
-    kernel_initializer: Initializer for the `kernel` weights matrix
+    weight_initializer: Initializer for the `kernel` weights matrix
-                 **kwargs):
+                 dilation=1, groups=1, layout='NCW', activation=None, use_bias=True,
-            in_filters, activation, use_bias, kernel_initializer,
+            in_filters, activation, use_bias, weight_initializer,
-    kernel_initializer: Initializer for the `kernel` weights matrix
+    weight_initializer: Initializer for the `kernel` weights matrix
-                 kernel_initializer=None, bias_initializer=None, **kwargs):
+                 activation=None, use_bias=True, weight_initializer=None,
-            in_filters, activation, use_bias, kernel_initializer,
+            in_filters, activation, use_bias, weight_initializer,
-    kernel_initializer: Initializer for the `kernel` weights matrix
+    weight_initializer: Initializer for the `kernel` weights matrix
-                 kernel_initializer=None, bias_initializer=None, **kwargs):
+                 activation=None, use_bias=True, weight_initializer=None,
-            in_filters, activation, use_bias, kernel_initializer, bias_initializer,
+            in_filters, activation, use_bias, weight_initializer, bias_initializer,
-        if _LayerScope._current is None:
+    def create(prefix, params, hint):
-            return prefix
+                prefix = _name.NameManager.current.get(None, hint) + '_'
-        return ParameterDict(prefix)
+            params = ParameterDict(params.prefix, params)
-        self._params = _LayerScope.create_params(self._prefix, params)
+        self._prefix, self._params = _LayerScope.create(prefix, params, self._alias())
-            {name : shape for name, shape in zip(out.list_auxiliary_states(), aux_shapes)})
+        sdict.update({name : shape for name, shape in \
-    kernel_initializer: Initializer for the `kernel` weights matrix
+    weight_initializer: Initializer for the `kernel` weights matrix
-                 kernel_initializer=None, bias_initializer=None,
+                 weight_initializer=None, bias_initializer=None,
-                                          init=kernel_initializer)
+                                          init=weight_initializer)
-                 num_features=0, beta_initializer='zeros', gamma_initializer='ones',
+                 beta_initializer='zeros', gamma_initializer='ones',
-                 **kwargs):
+                 num_features=0, **kwargs):
-    embeddings_initializer : Initializer
+    weight_initializer : Initializer
-                 embeddings_initializer=None, **kwargs):
+                 weight_initializer=None, **kwargs):
-                                      init=embeddings_initializer)
+                                      init=weight_initializer)
-    def initialize(self, init=initializer.Xavier(), ctx=None):
+    def initialize(self, init=initializer.Xavier(), ctx=None, verbose=False):
-from ... import symbol, init, ndarray
+from ... import symbol, ndarray
-    def begin_state(self, func=symbol.zeros, batch_size=0, **kwargs):
+    def begin_state(self, batch_size=0, func=ndarray.zeros, **kwargs):
-    num_hidden : int
+    hidden_size : int
-        type of activation function
+        type of activation function.
-                 prefix=None, params=None):
+    def __init__(self, hidden_size, activation='tanh',
-        self._num_hidden = num_hidden
+        self._hidden_size = hidden_size
-        self.h2h_bias = self.params.get('h2h_bias', shape=(num_hidden,))
+        self._input_size = input_size
-        return ('',)
+        return [{'shape': (batch_size, self._hidden_size), '__layout__': 'NC'}]
-                       h2h_weight, h2h_bias):
+    def hybrid_forward(self, F, inputs, states, i2h_weight,
-                               num_hidden=self._num_hidden,
+                               num_hidden=self._hidden_size,
-                               num_hidden=self._num_hidden,
+                               num_hidden=self._hidden_size,
-        number of units in output symbol
+    hidden_size : int
-                 prefix=None, params=None):
+    def __init__(self, hidden_size,
-        self.h2h_bias = self.params.get('h2h_bias', shape=(4*num_hidden,))
+        self._hidden_size = hidden_size
-        return ['_i', '_f', '_c', '_o']
+        return [{'shape': (batch_size, self._hidden_size), '__layout__': 'NC'},
-                       h2h_weight, h2h_bias):
+    def hybrid_forward(self, F, inputs, states, i2h_weight,
-                               num_hidden=self._num_hidden*4,
+                               num_hidden=self._hidden_size*4,
-                               num_hidden=self._num_hidden*4,
+                               num_hidden=self._hidden_size*4,
-        number of units in output symbol
+    hidden_size : int
-    def __init__(self, num_hidden, num_input=0, prefix=None, params=None):
+    def __init__(self, hidden_size,
-        self.h2h_bias = self.params.get('h2h_bias', shape=(3*num_hidden))
+        self._hidden_size = hidden_size
-        return ['_r', '_z', '_o']
+        return [{'shape': (batch_size, self._hidden_size), '__layout__': 'NC'}]
-                       h2h_weight, h2h_bias):
+    def hybrid_forward(self, F, inputs, states, i2h_weight,
-                               num_hidden=self._num_hidden * 3,
+                               num_hidden=self._hidden_size * 3,
-                               num_hidden=self._num_hidden * 3,
+                               num_hidden=self._hidden_size * 3,
-class HSequentialRNNCell(HRecurrentCell):
+class SequentialRNNCell(RecurrentCell):
-        super(HSequentialRNNCell, self).__init__(prefix='', params=None)
+    def __init__(self, prefix=None, params=None):
-            "Bidirectional HSequentialRNNCell doesn't support zoneout. " \
+        assert not isinstance(base_cell, SequentialRNNCell) or not base_cell._bidirectional, \
-        states = [l_states, r_states]
+        states = l_states + r_states
-        Jozefowicz et al. 2015 recommends setting this to 1.0.
+    forget_bias: float, default 1.0
-    def __init__(self, forget_bias):
+    def __init__(self, forget_bias=1.0):
-        return '%s\n<%s %s @%s>' % ('',#str(self.asnumpy()),
+        return '%s\n<%s %s @%s>' % (str(self.asnumpy()),
-from test_rnn import *
+#from test_rnn import *
-
+def check_rnn_layer(layer):
-                               prefix='rnn_', dropout=0.5))
+    with mx.cpu(0):
-           ['rnn_parameters']
+    assert_allclose(go.asnumpy(), co.asnumpy(), rtol=1e-2)
-    assert np.array_equal(outputs[0].asnumpy(), expected_outputs)
+def test_rnn_layer():
-                      mx.nd.ones((8, 3, 200)))
+    check_rnn_layer(foo.rnn.LSTM(100, num_layers=3, bidirectional=True))
-from mxnet.contrib import autograd as ag
+from mxnet import autograd
-            with ag.record():
+            with autograd.record():
-                    ag.compute_gradient([loss])
+                    loss.backward()
-    cell = foo.rnn.LSTMCell(100, prefix='rnn_', forget_bias=1.0)
+    cell = foo.rnn.LSTMCell(100, prefix='rnn_')
-    stack.add(foo.rnn.LSTMCell(100, forget_bias=forget_bias, prefix='l1_'))
+    stack = foo.rnn.SequentialRNNCell()
-    cell = foo.rnn.HSequentialRNNCell()
+    cell = foo.rnn.SequentialRNNCell()
-    check_rnn_forward(foo.rnn.GRUCell(100, num_input=200), mx.nd.ones((8, 3, 200)))
+    check_rnn_forward(foo.rnn.LSTMCell(100, input_size=200), mx.nd.ones((8, 3, 200)))
-                                       foo.rnn.LSTMCell(100, num_input=200))
+    bilayer = foo.rnn.BidirectionalCell(foo.rnn.LSTMCell(100, input_size=200),
-    check_rnn_forward(foo.rnn.ZoneoutCell(foo.rnn.LSTMCell(100, num_input=200),
+    check_rnn_forward(foo.rnn.ZoneoutCell(foo.rnn.LSTMCell(100, input_size=200),
-    net.add(foo.rnn.GRUCell(100, num_input=100))
+    net = foo.rnn.SequentialRNNCell()
-    net2 = Net(prefix='net1_', params=net1.all_params())
+    net2 = Net(prefix='net2_', params=net1.all_params())
-    L = \\frac{1}{2}\\sum_i \\Vert {output}_i - {label}_i \\Vert^2.
+        L = \\frac{1}{2}\\sum_i \\Vert {output}_i - {label}_i \\Vert^2.
-    L = \\frac{1}{2}\\sum_i \\vert {output}_i - {label}_i \\vert.
+        L = \\frac{1}{2}\\sum_i \\vert {output}_i - {label}_i \\vert.
-    L = -\\sum_i {log}(p_{i,{label}_i})
+        p = {softmax}({output})
-    L = -\\sum_i \\sum_j {label}_j {log}(p_{ij})
+        p = {softmax}({output})
-        'N', 'C', 'H', 'W' stands for batch, channel, and width (time) dimensions
+        Can be 'NCW', 'NWC', etc.
-        'N', 'C', 'H', 'W' stands for batch, channel, and width (time) dimensions
+        Can be 'NCW', 'NWC', etc.
-            if isinstance(layer, Sequantial):
+            if isinstance(layer, Sequential):
-            net.add(Dense(20))
+    Example
-            net.add(Dense(20))
+    Example
-    the output would have shape `(batch_size, units)`.
+    Input shape:
-    """Applies an activation function to input.
+    """Applies an activation function to input. Refer
-    Same shape as input.
+    Input shape:
-    which helps prevent overfitting.
+    which helps prevent overfitting. Refer
-        http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)
+        `Dropout: A Simple Way to Prevent Neural Networks from Overfitting
-    close to 0 and the activation standard deviation close to 1.
+    close to 0 and the activation standard deviation close to 1. Refer
-    `f(x) = x for x >= 0`.
+    It allows a small gradient when the unit is not active::
-    3D tensor with shape: `(batch_size, sequence_length, output_dim)`.
+    Input shape:
-from mxnet.contrib import autograd
+from mxnet import autograd
-from mxnet.contrib import autograd
+from mxnet import autograd
-from mxnet.contrib import autograd as ag
+from mxnet import autograd as ag
-import time
+import argparse, time
-from mxnet.contrib import autograd as ag
+from mxnet import autograd as ag
-    return ResnetV2(BottleneckV2, classes, [3, 4, 6, 3], [64, 256, 512, 1024, 2048], False)
+batch_size *= max(1, gpus)
-train_data, val_data = cifar10_iterator(batch_size, (3, 32, 32))
+# get dataset iterators
-    print('validation acc: %s=%f'%metric.get())
+    logging.info('validation acc: %s=%f'%metric.get())
-            print('speed: {} samples/s'.format(batch.data[0].shape[0]/(time.time()-btic)))
+            logging.info('speed: {} samples/s'.format(batch_size/(time.time()-btic)))
-        print('time: %f'%(time.time()-tic))
+        logging.info('training acc at epoch %d: %s=%f'%(i, name, acc))
-    mod.fit(train_data, num_epoch=100, batch_end_callback = mx.callback.Speedometer(batch_size, 10))
+    if opt.symbolic:
-from mxnet.contrib import autograd as ag
+from mxnet import autograd as ag
-        self._active = True
+        self._active = active
-    def forward(self, F, x):
+    def forward(self, x):
-    with autograd.train_section():
+    with autograd.record():
-# state size. (nc) x 64 x 64
+with netG.name_scope():
-# netD.add(nn.Activation('sigmoid'))
+with netD.name_scope():
-        with autograd.train_section():
+        with autograd.record():
-        with autograd.train_section():
+        with autograd.record():
-net.add(nn.Dense(10, in_units=64))
+with net.name_scope():
-            with ag.train_section():
+            with ag.record():
-class BasicBlockV1(nn.Layer):
+class BasicBlockV1(nn.HybridLayer):
-    def forward(self, domain, x):
+    def hybrid_forward(self, F, x):
-        out = domain.Activation(x, act_type='relu')
+        out = F.Activation(x, act_type='relu')
-        out = domain.Activation(out, act_type='relu')
+        out = F.Activation(out, act_type='relu')
-class BottleneckV1(nn.Layer):
+class BottleneckV1(nn.HybridLayer):
-    def forward(self, domain, x):
+    def hybrid_forward(self, F, x):
-        out = domain.Activation(out, act_type='relu')
+        out = F.Activation(out, act_type='relu')
-        out = domain.Activation(out, act_type='relu')
+        out = F.Activation(out, act_type='relu')
-        out = domain.Activation(out, act_type='relu')
+        out = F.Activation(out, act_type='relu')
-class ResnetV1(nn.Layer):
+class ResnetV1(nn.HybridLayer):
-             self.body = nn.Sequential()
+             self.body = nn.HSequential()
-        layer = nn.Sequential()
+        layer = nn.HSequential()
-    def forward(self, domain, x):
+    def hybrid_forward(self, F, x):
-            x = domain.Activation(x, act_type='relu')
+            x = F.Activation(x, act_type='relu')
-class BasicBlockV2(nn.Layer):
+class BasicBlockV2(nn.HybridLayer):
-    def forward(self, domain, x):
+    def hybrid_forward(self, F, x):
-        x = domain.Activation(x, act_type='relu')
+        x = F.Activation(x, act_type='relu')
-        x = domain.Activation(x, act_type='relu')
+        x = F.Activation(x, act_type='relu')
-class BottleneckV2(nn.Layer):
+class BottleneckV2(nn.HybridLayer):
-    def forward(self, domain, x):
+    def hybrid_forward(self, F, x):
-        x = domain.Activation(x, act_type='relu')
+        x = F.Activation(x, act_type='relu')
-        x = domain.Activation(x, act_type='relu')
+        x = F.Activation(x, act_type='relu')
-        x = domain.Activation(x, act_type='relu')
+        x = F.Activation(x, act_type='relu')
-class ResnetV2(nn.Layer):
+class ResnetV2(nn.HybridLayer):
-            self.body = nn.Sequential()
+            self.body = nn.HSequential()
-        layer = nn.Sequential()
+        layer = nn.HSequential()
-    def forward(self, domain, x):
+    def hybrid_forward(self, F, x):
-            x = domain.Activation(x, act_type='relu')
+            x = F.Activation(x, act_type='relu')
-        x = domain.Activation(x, act_type='relu')
+        x = F.Activation(x, act_type='relu')
-            with ag.train_section():
+            losses = []
-                    ag.compute_gradient([loss])
+                    losses.append(loss)
-    def forward(self, F, x):
+    def forward(self, x):
-            with ag.train_section():
+            with ag.record():
-    def forward(self, F, inputs, hidden):
+    def forward(self, inputs, hidden):
-            with autograd.train_section():
+            with autograd.record():
-def train_section():
+def record():
-        with autograd.train_section():
+        with autograd.record():
-def test_section():
+def pause():
-        with autograd.train_section():
+        with autograd.record():
-            with autograd.test_section():
+            with autograd.pause():
-from .layer import Layer
+from .layer import HybridLayer
-    sym = symbol.invoke(op, [symbol.var('data', shape=data_shape)])
+def _infer_weight_shape(op_name, data_shape, kwargs):
-class _Conv(Layer):
+class _Conv(HybridLayer):
-            attrs = {
+            self._op_name = op_name
-            self._op = symbol.CachedOp(op_name, 3 if use_bias else 2, **attrs)
+            self._kwargs.update(kwargs)
-            wshapes = _infer_weight_shape(self._op, dshape)
+            wshapes = _infer_weight_shape(op_name, dshape, self._kwargs)
-    def forward(self, F, x, weight, bias=None):
+    def hybrid_forward(self, F, x, weight, bias=None):
-            act = F.invoke(self._op, [x, weight])
+            act = getattr(F, self._op_name)(x, weight, **self._kwargs)
-            act = F.invoke(self._op, [x, weight, bias])
+            act = getattr(F, self._op_name)(x, weight, bias, **self._kwargs)
-class _Pooling(Layer):
+class _Pooling(HybridLayer):
-        attrs = {
+        self._kwargs = {
-        return F.invoke(self._op, [x])
+    def hybrid_forward(self, F, x):
-                    self.dense1 = nn.Dense(20, in_units=20)
+                    self.dense0 = nn.Dense(20)
-            def forward(self, F, x):
+            def forward(self, x):
-        """Infer parameter shape given input shapes.
+    def hybridize(self, active=True):
-            A list of input argument shapes.
+    def forward(self, *args):
-        sdict = {name: shape for name, shape in zip(sym.list_arguments(), arg_shapes)}
+        # pylint: disable= invalid-name
-        for i in self.params.values():
+            {name : shape for name, shape in zip(out.list_auxiliary_states(), aux_shapes)})
-        """Defines the forward computation. Arguments can be either NDArray or Symbol."""
+    def _build_cache(self, *args):
-                    params = {k: v.data(ctx) for k, v in self._reg_params.items()}
+                    params = {i: j.data(ctx) for i, j in self._reg_params.items()}
-                    for i in self.params.values():
+                    self.infer_shape(x, *args)
-                return self.forward(ndarray, x, *args, **params)
+                    params = {i: j.data(ctx) for i, j in self._reg_params.items()}
-            return self.forward(symbol, x, *args, **params)
+            params = {i: j.var() for i, j in self._reg_params.items()}
-        """Simple forward supports both `Symbol` and `NDArray` API.
+    def hybrid_forward(self, F, x, *args, **kwargs):
-            `Symbol` or `NDArray` value of registered Parameters.
+        x : Symbol
-        net.add(Dense(20))
+        with net.name_scope():
-        super(Sequential, self).__init__(prefix='', params=None)
+    def __init__(self, prefix=None, params=None):
-        #pylint: disable=arguments-differ
+    def forward(self, x):
-        raise NotImplementedError
+class HSequential(HybridLayer):
-class Dense(Layer):
+    def add(self, layer):
-                                       num_hidden=units, no_bias=not use_bias)
+            self._units = units
-    def forward(self, F, x, weight, bias=None):
+    def hybrid_forward(self, F, x, weight, bias=None):
-            act = F.invoke(self._op, [x, weight])
+            act = F.FullyConnected(x, weight, no_bias=True, num_hidden=self._units)
-            act = F.invoke(self._op, [x, weight, bias])
+            act = F.FullyConnected(x, weight, bias, num_hidden=self._units)
-class Activation(Layer):
+class Activation(HybridLayer):
-        return F.invoke(self._op, [x])
+    def hybrid_forward(self, F, x):
-class Dropout(Layer):
+class Dropout(HybridLayer):
-        self._op = symbol.CachedOp('Dropout', 1, p=rate)
+        self._rate = rate
-        return F.invoke(self._op, [x])
+    def hybrid_forward(self, F, x):
-class BatchNorm(Layer):
+class BatchNorm(HybridLayer):
-        self._op = symbol.CachedOp('BatchNorm', 5, **attrs)
+        self._kwargs = {'axis': axis, 'eps': epsilon, 'momentum': momentum,
-        return F.invoke(self._op, [x, gamma, beta, running_mean, running_var])
+    def hybrid_forward(self, F, x, gamma, beta, running_mean, running_var):
-class LeakyReLU(Layer):
+class LeakyReLU(HybridLayer):
-        self._op = symbol.CachedOp('LeakyReLU', 1, act_type='leaky', slope=alpha)
+        self._alpha = alpha
-        return F.invoke(self._op, [x])
+    def hybrid_forward(self, F, x):
-class Embedding(Layer):
+class Embedding(HybridLayer):
-                                   output_dim=output_dim, dtype=dtype)
+        self._kwargs = {'input_dim': input_dim, 'output_dim': output_dim,
-        return F.invoke(self._op, [x, weight])
+    def hybrid_forward(self, F, x, weight):
-        out = mx.nd.FullyConnected(x, w.value(ctx), b.value(ctx), num_hidden=64)
+        out = mx.nd.FullyConnected(x, w.data(ctx), b.data(ctx), num_hidden=64)
-        with autograd.test_section():
+        with autograd.pause():
-from ..nn import Layer
+from ..nn import Layer, HybridLayer
-    def call(self, inputs, states):
+    def forward(self, inputs, states):
-        return super(RecurrentCell, self).call(inputs, states)
+        return super(RecurrentCell, self).forward(inputs, states)
-class RNNCell(RecurrentCell):
+class RNNCell(HRecurrentCell):
-                h2h_weight, h2h_bias):
+    def hybrid_forward(self, F, inputs, states, i2h_weight, i2h_bias,
-class LSTMCell(RecurrentCell):
+class LSTMCell(HRecurrentCell):
-                h2h_weight, h2h_bias):
+    def hybrid_forward(self, F, inputs, states, i2h_weight, i2h_bias,
-class GRUCell(RecurrentCell):
+class GRUCell(HRecurrentCell):
-                h2h_weight, h2h_bias):
+    def hybrid_forward(self, F, inputs, states, i2h_weight, i2h_bias,
-class FusedRNNCell(RecurrentCell):
+class FusedRNNCell(HRecurrentCell):
-        cell : SequentialRNNCell
+        cell : HSequentialRNNCell
-        stack = SequentialRNNCell()
+        stack = HSequentialRNNCell()
-class SequentialRNNCell(RecurrentCell):
+class HSequentialRNNCell(HRecurrentCell):
-        super(SequentialRNNCell, self).__init__(prefix='', params=None)
+        super(HSequentialRNNCell, self).__init__(prefix='', params=None)
-    def forward(self, *args, **kwargs):
+    def hybrid_forward(self, *args, **kwargs):
-class DropoutCell(RecurrentCell):
+class DropoutCell(HRecurrentCell):
-    def forward(self, F, inputs, states):
+    def hybrid_forward(self, F, inputs, states):
-            return self.forward(F, inputs, begin_state if begin_state else [])
+            return self.hybrid_forward(F, inputs, begin_state if begin_state else [])
-class ModifierCell(RecurrentCell):
+class ModifierCell(HRecurrentCell):
-    def forward(self, F, inputs, states):
+    def hybrid_forward(self, F, inputs, states):
-            "Bidirectional SequentialRNNCell doesn't support zoneout. " \
+        assert not isinstance(base_cell, HSequentialRNNCell) or not base_cell._bidirectional, \
-    def forward(self, F, inputs, states):
+    def hybrid_forward(self, F, inputs, states):
-    def forward(self, F, inputs, states):
+    def hybrid_forward(self, F, inputs, states):
-class BidirectionalCell(RecurrentCell):
+class BidirectionalCell(HRecurrentCell):
-        autograd.compute_gradient and outside of train_section() scope.
+        autograd.compute_gradient and outside of record() scope.
-        return '%s\n<%s %s @%s>' % (str(self.asnumpy()),
+        return '%s\n<%s %s @%s>' % ('',#str(self.asnumpy()),
-    stack = foo.rnn.SequentialRNNCell()
+    stack = foo.rnn.HSequentialRNNCell()
-    stack = foo.rnn.SequentialRNNCell()
+    stack = foo.rnn.HSequentialRNNCell()
-    stack = foo.rnn.SequentialRNNCell()
+    stack = foo.rnn.HSequentialRNNCell()
-    stack = foo.rnn.SequentialRNNCell()
+    stack = foo.rnn.HSequentialRNNCell()
-            with ag.train_section():
+            with ag.record():
-        with train_section():
+        with record():
-    with train_section():
+    with record():
-        with test_section():
+        with pause():
-    with train_section():
+    with record():
-    with train_section():
+    with record():
-    with train_section():
+    with record():
-    with train_section():
+    with record():
-    with train_section():
+    with record():
-        with train_section():
+        with record():
-    with train_section():
+    with record():
-    stack = foo.rnn.SequentialRNNCell()
+    stack = foo.rnn.HSequentialRNNCell()
-    cell = foo.rnn.SequentialRNNCell()
+    cell = foo.rnn.HSequentialRNNCell()
-            layer.unroll(3, inputs, merge_outputs=False)[0])
+    with mx.autograd.record():
-    net = foo.rnn.SequentialRNNCell()
+    net = foo.rnn.HSequentialRNNCell()
-        def forward(self, F, x):
+        def forward(self, x):
-    with mx.contrib.autograd.train_section():
+    with mx.autograd.record():
-    with mx.contrib.autograd.train_section():
+    with mx.autograd.record():
-    mx.contrib.autograd.compute_gradient([x])
+    x.backward()
-    with mx.contrib.autograd.train_section():
+    with mx.autograd.record():
-    mx.contrib.autograd.compute_gradient([x])
+    x.backward()
-    with mx.contrib.autograd.train_section():
+    with mx.autograd.record():
-    mx.contrib.autograd.compute_gradient([x])
+    x.backward()
-    with mx.contrib.autograd.train_section():
+    with mx.autograd.record():
-            data = ndarray.zeros(shape=self.shape, dtype=self.dtype, ctx=ctx[0])
+            data = ndarray.zeros(shape=self.shape, dtype=self.dtype,
-                data)
+                initializer.InitDesc(self.name, {'__init__': init}), data)
-                self._data[i] = data.copyto(i)
+            self._init_impl(data, ctx)
-                return
+    def _init_impl(self, data, ctx):
-                self._grad[i] = ndarray.zeros_like(self._data[i])
+        self._grad = OrderedDict()
-            autograd.mark_variables(self.list_data(), self.list_grad(), self.grad_req)
+        autograd.mark_variables(self.list_data(), self.list_grad(), self.grad_req)
-    def load(self, filename, allow_missing=False, ignore_extra=False):
+    def load(self, filename, ctx, allow_missing=False, ignore_extra=False):
-            self[name].set_data(arg_dict[name])
+            self[name]._load_init(arg_dict[name], ctx)
-    def set_grad(self, grad_req='write'):
+    def attach_grad(self, grad_req='write'):
-def test_set_grad():
+def test_attach_grad():
-    x.set_grad()
+    x.attach_grad()
-    params.initialize()
+    params.initialize(ctx=mx.cpu())
-    params.load('test.params')
+    params.load('test.params', mx.cpu())
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, F, x):
+    def forward(self, F, x):
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, domain, x):
+    def forward(self, domain, x):
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, domain, x):
+    def forward(self, domain, x):
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, domain, x):
+    def forward(self, domain, x):
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, domain, x):
+    def forward(self, domain, x):
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, domain, x):
+    def forward(self, domain, x):
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, domain, x):
+    def forward(self, domain, x):
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, F, x):
+    def forward(self, F, x):
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, F, inputs, hidden):
+    def forward(self, F, inputs, hidden):
-        loss = nn.custom_loss(loss, output, label, name='l2')
+    Examples
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, F, x, weight, bias=None):
+    def forward(self, F, x, weight, bias=None):
-    def generic_forward(self, F, x):
+    def forward(self, F, x):
-                with self.scope:
+                with self.name_scope():
-            def forward(self, x):
+            def forward(self, F, x):
-    def scope(self):
+    def name_scope(self):
-        sym = self.symbol_forward(*inputs, **params)
+        sym = self.forward(symbol, *inputs, **params)
-    def forward(self, x, *args):
+        return self.call(*args)  # pylint: disable=no-value-for-parameter
-                return self.ndarray_forward(x, *args, **params)
+                try:
-                "Layer requires the first argument to forward be either Symbol or NDArray"
+                "Layer requires the first argument to forward be either " \
-        return self.generic_forward(symbol, x, *args, **kwargs)
+            return self.forward(symbol, x, *args, **params)
-    def generic_forward(self, F, x, *args, **kwargs):
+    def forward(self, F, x, *args, **kwargs):
-    def forward(self, x):
+    def call(self, x):
-    def generic_forward(self, F, x, *args, **kwargs):
+    def forward(self, F, x, *args, **kwargs):
-        with self.scope:
+        with self.name_scope():
-    def generic_forward(self, F, x, weight, bias=None):
+    def forward(self, F, x, weight, bias=None):
-    def generic_forward(self, F, x):
+    def forward(self, F, x):
-    - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](
+    [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](
-    def generic_forward(self, F, x):
+    def forward(self, F, x):
-    def generic_forward(self, F, x, gamma, beta, running_mean, running_var):
+    def forward(self, F, x, gamma, beta, running_mean, running_var):
-    def generic_forward(self, F, x):
+    def forward(self, F, x):
-    def generic_forward(self, F, x, weight):
+    def forward(self, F, x, weight):
-        ---------
+        Parameters
-    def forward(self, inputs, states):
+    def call(self, inputs, states):
-        return super(RecurrentCell, self).forward(inputs, states)
+        return super(RecurrentCell, self).call(inputs, states)
-                        h2h_weight, h2h_bias):
+    def forward(self, F, inputs, states, i2h_weight, i2h_bias,
-                        h2h_weight, h2h_bias):
+    def forward(self, F, inputs, states, i2h_weight, i2h_bias,
-                        h2h_weight, h2h_bias):
+    def forward(self, F, inputs, states, i2h_weight, i2h_bias,
-    def generic_forward(self, *args, **kwargs):
+    def forward(self, *args, **kwargs):
-    def generic_forward(self, F, inputs, states):
+    def forward(self, F, inputs, states):
-            return self.generic_forward(F, inputs, begin_state if begin_state else [])
+            return self.forward(F, inputs, begin_state if begin_state else [])
-    def generic_forward(self, F, inputs, states):
+    def forward(self, F, inputs, states):
-    def generic_forward(self, F, inputs, states):
+    def forward(self, F, inputs, states):
-    def generic_forward(self, F, inputs, states):
+    def forward(self, F, inputs, states):
-                                shape_info, self.context)
+        return '%s\n<%s %s @%s>' % (str(self.asnumpy()),
-from .ndarray import NDArray, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
+from .ndarray import NDArray, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP, _GRAD_REQ_MAP
-    def grad(self, wrt):
+    def gradient(self, wrt):
-            with self.scope:
+            with self.name_scope():
-        def generic_forward(self, F, x):
+        def forward(self, F, x):
-    print 'validation acc: %s=%f'%metric.get()
+    print('validation acc: %s=%f'%metric.get())
-            print 'speed: {} samples/s'.format(batch.data[0].shape[0]/(time.time()-btic))
+            print('speed: {} samples/s'.format(batch.data[0].shape[0]/(time.time()-btic)))
-        print 'time: %f'%(time.time()-tic)
+        print('training acc at epoch %d: %s=%f'%(i, name, acc))
-netG.add(nn.BatchNorm(num_features=ngf * 8))
+netG.add(nn.Conv2DTranspose(ngf * 8, 4, 1, 0, use_bias=False))
-netG.add(nn.BatchNorm(num_features=ngf * 4))
+netG.add(nn.Conv2DTranspose(ngf * 4, 4, 2, 1, use_bias=False))
-netG.add(nn.BatchNorm(num_features=ngf * 2))
+netG.add(nn.Conv2DTranspose(ngf * 2, 4, 2, 1, use_bias=False))
-netG.add(nn.BatchNorm(num_features=ngf))
+netG.add(nn.Conv2DTranspose(ngf, 4, 2, 1, use_bias=False))
-netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, in_filters=ngf, use_bias=False))
+netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, use_bias=False))
-netD.add(nn.Conv2D(ndf, 4, 2, 1, in_filters=nc, use_bias=False))
+netD.add(nn.Conv2D(ndf, 4, 2, 1, use_bias=False))
-netD.add(nn.BatchNorm(num_features=ndf * 2))
+netD.add(nn.Conv2D(ndf * 2, 4, 2, 1, use_bias=False))
-netD.add(nn.BatchNorm(num_features=ndf * 4))
+netD.add(nn.Conv2D(ndf * 4, 4, 2, 1, use_bias=False))
-netD.add(nn.BatchNorm(num_features=ndf * 8))
+netD.add(nn.Conv2D(ndf * 8, 4, 2, 1, use_bias=False))
-netD.add(nn.Conv2D(2, 4, 1, 0, in_filters=ndf * 8, use_bias=False))
+netD.add(nn.Conv2D(2, 4, 1, 0, use_bias=False))
-from ..parameter import Parameter, ParameterDict
+from ..parameter import Parameter, ParameterDict, DeferredInitializationError
-        will be registered automatically."""
+        """Register layer as sublayer of self. Layers assigned to
-    def __call__(self, *args, **kwargs):
+    def infer_shape(self, *args):
-        return self.forward(*args, **kwargs)
+        try:
-from ..base import mx_real_t
+from ..base import mx_real_t, MXNetError
-    def initialize(self, init=None, ctx=None, default_init=initializer.Xavier()):
+    def initialize(self, init=None, ctx=None, default_init=initializer.Xavier(),
-            their values consistent when updating. Normally nn.Optim does this for you.
+            their values consistent when updating. Normally nn.Trainer does this for you.
-            self._grad = None
+        if self.shape is None or np.prod(self.shape) <= 0:
-            self._grad[i] = ndarray.zeros_like(self._data[i])
+            self._data = OrderedDict()
-        autograd.mark_variables(self.list_data(), self.list_grad(), self.grad_req)
+            if self.grad_req == 'null':
-            "because it was not initialized on %s"%(self.name, str(ctx), str(ctx))
+        self._check_initialized(ctx)
-            "Parameter %s has not been initialized"%self.name
+        self._check_initialized()
-            "because it was not initialized on %s"%(self.name, str(ctx), str(ctx))
+        self._check_initialized(ctx)
-        assert self._data is not None, \
+        self._check_initialized()
-            "Parameter %s has not been initialized"%self.name
+        if self._data is None:
-        self._init_kvstore(kvstore)
+        self._kv_initialized = False
-    def _init_kvstore(self, kvstore):
+        self._updaters = [opt.get_updater(self._optimizer) \
-        kvstore, update_on_kvstore = _create_kvstore(kvstore, len(self._contexts), arg_arrays)
+        kvstore, update_on_kvstore = _create_kvstore(self._kvstore, len(self._contexts), arg_arrays)
-from .base import check_call, MXNetError, _Null  # pylint: disable=unused-import
+from .base import check_call, MXNetError, NotImplementedForSymbol, _Null  # pylint: disable=unused-import
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.__iadd__, '+=', other, 1)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.__isub__, '-=', other)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.__imul__, '*=', other)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.__idiv__, '/=', other)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.__itruediv__, '/=', other)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.__rpow__, 'y**x', other)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.wait_to_read, None)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.asnumpy, None)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.asscalar, None)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.astype, None)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.copy, None)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.as_in_context, None)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.detach, None)
-        raise NotImplementedError('Not supported. This is available in NDArray only')
+        raise NotImplementedForSymbol(self.backward, None)
-import sys
+import random
-    for ndim in range(1, 6):
+    for ndim in range(1, 7):
-            "because it hasn't been initialized!"%(self.name)
+            "because it hasn't been initialized. Note that " \
-                ignore = label == self.ignore_label
+                ignore = (label == self.ignore_label).astype(pred.dtype)
-trainer = foo.Trainer(net.params, 'adam', {'learning_rate': 3e-2})
+net.all_params().initialize(mx.init.Uniform(0.02))
-netD.params.initialize(mx.init.Normal(0.02), ctx=ctx)
+netG.all_params().initialize(mx.init.Normal(0.02), ctx=ctx)
-trainerD = foo.Trainer(netD.params, 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})
+trainerG = foo.Trainer(netG.all_params(), 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})
-    print 'validation acc: %s=%f'%metric.get()
+    print('validation acc: %s=%f'%metric.get())
-    trainer = foo.Trainer(net.params, 'sgd', {'learning_rate': 0.1})
+    net.all_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)
-        print 'training acc at epoch %d: %s=%f'%(i, name, acc)
+        print('training acc at epoch %d: %s=%f'%(i, name, acc))
-    net.params.save('mnist.params')
+    net.all_params().save('mnist.params')
-    trainer = foo.Trainer(net.params, 'sgd', {'learning_rate': 0.1})
+    net.all_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)
-    net.params.save('mnist.params')
+    net.all_params().save('mnist.params')
-    def get_prefix(prefix, hint):
+    def create_prefix(prefix, hint):
-    def get_params(prefix, params):
+    def create_params(prefix, params):
-        params = ParameterDict(prefix)
+            return ParameterDict(params.prefix, params)
-            _LayerScope._current._layer.register_sublayer(layer)
+            return ParameterDict(prefix, _LayerScope._current._layer._params._shared)
-                    self.dense2 = nn.Dense(20, in_units=20, prefix='dense2_')
+                    self.dense0 = nn.Dense(20, in_units=10)
-                return self.dense2(x)
+                x = self.dense0(x)
-            dense2 = nn.Dense(20, in_units=10, prefix='dense2_', params=params)
+        ParameterDict for sharing weights with the new Layer. For example,
-    Layer instead."""
+    Layer supports forwarding with both `Symbol` and `NDArray`."""
-        self._params = _LayerScope.get_params(self._prefix, params)
+        self._prefix = _LayerScope.create_prefix(prefix, self._alias())
-            _LayerScope.register_sublayer(self)
+            self.register_child(value)
-        """A ParameterDict managing this Layer's Parameters."""
+        """Returns this Layer's parameter dictionary (does not include its
-    def register_sublayer(self, layer):
+    def register_child(self, layer):
-        self.register_sublayer(layer)
+        self.register_child(layer)
-        before.
+        """Returns a copy of this parameter on one context. Must have been
-        return self._data.values()
+        return list(self._data.values())
-        return self._grad.values()
+        return list(self._grad.values())
-        return self._data.keys()
+        return list(self._data.keys())
-    def __init__(self, prefix=''):
+    def __init__(self, prefix='', shared=None):
-        arguments will be passed to Parameter's contructor.
+        """Retrieve a Parameter with name `self.prefix+name`. If not found,
-            self._params[name] = Parameter(name, **kwargs)
+        param = self._get_impl(name)
-                if hasattr(param, k):
+                if hasattr(param, k) and getattr(param, k) is not None:
-                            k, str(getattr(param, k)), str(v))
+                        "Cannot retrieve Parameter %s because desired attribute " \
-            Suffix of the created child dictionary
+        return param
-            return
+    def update(self, other):
-        other._params = params
+            if k in self._params:
-        self.register_sublayer(cell)
+        self.register_child(cell)
-        self.register_sublayer(r_cell)
+        self.register_child(l_cell)
-    size = data.shape[batch_axis] / num_slice
+    size = data.shape[batch_axis] // num_slice
-    assert sorted(cell.params._params.keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
+    assert sorted(cell.all_params().keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
-    assert sorted(cell.params._params.keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
+    assert sorted(cell.all_params().keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
-    assert sorted(cell.params._params.keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
+    assert sorted(cell.all_params().keys()) == ['rnn_h2h_bias', 'rnn_h2h_weight', 'rnn_i2h_bias', 'rnn_i2h_weight']
-    assert sorted(cell.params._params.keys()) == \
+    assert sorted(cell.all_params().keys()) == \
-    assert sorted(cell.params._params.keys()) == \
+    assert sorted(cell.all_params().keys()) == \
-    keys = sorted(cell.params._params.keys())
+    keys = sorted(cell.all_params().keys())
-    layer.params.initialize()
+    layer.all_params().initialize()
-    model.params.initialize()
+    model.all_params().initialize()
-    layer.params.initialize()
+    layer.all_params().initialize()
-    layer.params.initialize()
+    layer.all_params().initialize()
-    layer.params.initialize()
+    layer.all_params().initialize()
-    layer.params.initialize()
+    layer.all_params().initialize()
-        prev_output = self.prev_output if self.prev_output else symbol.zeros((0, 0))
+        prev_output = self.prev_output if self.prev_output is not None else symbol.zeros((0, 0))
-    params : RNNParams or None, optional
+    params : Parameter or None, optional
-        A new RNNParams container is created if `params` is None.
+        A new Parameter container is created if `params` is None.
-    params : RNNParams or None
+    params : Parameter or None
-    params : RNNParams or None
+    params : Parameter or None
-    params : RNNParams or None
+    params : Parameter or None
-from mxnet import nn
+from mxnet import foo
-trainer = nn.Optim(net.params, 'adam', {'learning_rate': 3e-2})
+trainer = foo.Trainer(net.params, 'adam', {'learning_rate': 3e-2})
-        loss = sum([nn.loss.l1_loss(value, mx.nd.array([r])) for r, value in zip(rewards, values)])
+        loss = sum([foo.loss.l1_loss(value, mx.nd.array([r])) for r, value in zip(rewards, values)])
-from mxnet import nn
+from mxnet import foo
-optimizerD = nn.Optim(netD.params, 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})
+trainerG = foo.Trainer(netG.params, 'adam', {'learning_rate': opt.lr, 'beta1': opt.beta1})
-            errD_real = nn.loss.softmax_cross_entropy_loss(output, real_label)
+            errD_real = foo.loss.softmax_cross_entropy_loss(output, real_label)
-            errD_fake = nn.loss.softmax_cross_entropy_loss(output, fake_label)
+            errD_fake = foo.loss.softmax_cross_entropy_loss(output, fake_label)
-        optimizerD.step(opt.batchSize)
+        trainerD.step(opt.batchSize)
-            errG = nn.loss.softmax_cross_entropy_loss(output, real_label)
+            errG = foo.loss.softmax_cross_entropy_loss(output, real_label)
-        optimizerG.step(opt.batchSize)
+        trainerG.step(opt.batchSize)
-from mxnet import nn
+from mxnet import foo
-        label = nn.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+        data = foo.utils.load_data(batch.data[0], ctx_list=ctx, batch_axis=0)
-    optim = nn.Optim(net.params, 'sgd', {'learning_rate': 0.1})
+    trainer = foo.Trainer(net.params, 'sgd', {'learning_rate': 0.1})
-            label = nn.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+            data = foo.utils.load_data(batch.data[0], ctx_list=ctx, batch_axis=0)
-                    loss = nn.loss.softmax_cross_entropy_loss(z, y)
+                    loss = foo.loss.softmax_cross_entropy_loss(z, y)
-            optim.step(batch.data[0].shape[0])
+            trainer.step(batch.data[0].shape[0])
-from mxnet import nn
+from mxnet import foo
-        label = nn.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+        data = foo.utils.load_data(batch.data[0], ctx_list=ctx, batch_axis=0)
-    optim = nn.Optim(net.params, 'sgd', {'learning_rate': 0.1})
+    trainer = foo.Trainer(net.params, 'sgd', {'learning_rate': 0.1})
-            label = nn.utils.load_data(batch.label[0], ctx_list=ctx, batch_axis=0)
+            data = foo.utils.load_data(batch.data[0], ctx_list=ctx, batch_axis=0)
-                    loss = nn.loss.softmax_cross_entropy_loss(z, y)
+                    loss = foo.loss.softmax_cross_entropy_loss(z, y)
-            optim.step(batch.data[0].shape[0])
+            trainer.step(batch.data[0].shape[0])
-            print 'speed: {} samples/s'.format(train_data.label_shape[0]/(time.time()-btic))
+            print 'speed: {} samples/s'.format(batch.data[0].shape[0]/(time.time()-btic))
-from . import nn
+from . import foo
-from .layer import *
+from . import nn
-from .optim import *
+from . import rnn
-from .conv_layers import *
+from .trainer import *
-from ..base import numeric_types
+from ... import symbol
-from .parameter import Parameter, ParameterDict
+from ... import symbol, ndarray
-from ..contrib import autograd
+from .. import autograd
-            ctx = Context.current_context()
+            ctx = context.current_context()
-    """Optimizes a set of Parameters. Optim should be used together with autograd.
+class Trainer(object):
-    param_dict : ParameterDict
+    params : ParameterDict
-        self._params = [param for param in param_dict.values() if param.grad_req != 'null']
+    def __init__(self, params, optimizer, optimizer_params, kvstore='device'):
-                    grad._fresh_grad = False
+                    arr._fresh_grad = False
-    check_rnn_forward(mx.rnn.FusedRNNCell(100, num_layers=2, num_input=200),
+def test_foo_rnn():
-from mxnet.contrib.autograd import *
+from mxnet.ndarray import zeros_like
-from mxnet import nn
+from mxnet import foo
-    assert mx.nd.sum(nn.loss.l1_loss(output, label, sample_weight=weighting)).asscalar() == 5.
+    assert mx.nd.sum(foo.loss.l1_loss(output, label)).asscalar() == 6.
-    assert mx.nd.sum(nn.loss.l2_loss(output, label, sample_weight=weighting)).asscalar() == 6
+    assert mx.nd.sum(foo.loss.l2_loss(output, label)).asscalar() == 7.
-    loss = nn.loss.softmax_cross_entropy_loss(output, label).asnumpy()
+    loss = foo.loss.softmax_cross_entropy_loss(output, label).asnumpy()
-    loss = nn.loss.softmax_cross_entropy_loss(output, label, sample_weight=weighting).asnumpy()
+    loss = foo.loss.softmax_cross_entropy_loss(output, label, sample_weight=weighting).asnumpy()
-    check_loss(nn.loss.softmax_cross_entropy_loss)
+    check_loss(foo.loss.l1_loss)
-    loss = nn.loss.softmax_cross_entropy_loss(output, l, extra_outputs=(fc2,))
+    loss = foo.loss.softmax_cross_entropy_loss(output, l, extra_outputs=(fc2,))
-    loss = nn.loss.l2_loss(output, l)
+    loss = foo.loss.l2_loss(output, l)
-    loss = nn.loss.l1_loss(output, l)
+    loss = foo.loss.l1_loss(output, l)
-    loss = nn.loss.custom_loss(loss, output, l, weight=0.5, metrics='mse')
+    loss = foo.loss.custom_loss(loss, output, l, weight=0.5, metrics='mse')
-    loss = nn.loss.softmax_cross_entropy_loss(output, l, sample_weight=w)
+    loss = foo.loss.softmax_cross_entropy_loss(output, l, sample_weight=w)
-    loss = nn.loss.multitask_loss([loss1, loss2])
+    loss1 = foo.loss.softmax_cross_entropy_loss(output1, l1)
-    loss = nn.loss.softmax_cross_entropy_loss(output, l)
+    loss = foo.loss.softmax_cross_entropy_loss(output, l)
-from mxnet import nn
+from mxnet import foo
-    p = nn.Parameter('weight', shape=(10, 10))
+    p = foo.Parameter('weight', shape=(10, 10))
-    params = nn.ParameterDict('net_')
+    params = foo.ParameterDict('net_')
-
+from __future__ import print_function
-        eval_metric = _parse_metric(self._symbol, eval_metric)
+        eval_metric = _parse_metric(self.symbol, eval_metric)
-        eval_metric = _parse_metric(self._symbol, eval_metric)
+        eval_metric = _parse_metric(self.symbol, eval_metric)
-            ctx = Context.current_context()
+            ctx = context.current_context()
-from .ndarray import NDArray, zeros as _nd_zeros, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
+from .context import Context
-    def __init__(self, batch_size, data_shape):
+    def __init__(self, batch_size, data_shape, batches = 5):
-
+        if self._batches < self.batches:
-class BasicBlock(nn.Layer):
+class BasicBlockV1(nn.Layer):
-        super(BasicBlock, self).__init__(**kwargs)
+        super(BasicBlockV1, self).__init__(**kwargs)
-class Bottleneck(nn.Layer):
+class BottleneckV2(nn.Layer):
-        super(Bottleneck, self).__init__(**kwargs)
+        super(BottleneckV2, self).__init__(**kwargs)
-class Resnet(nn.Layer):
+class ResnetV2(nn.Layer):
-        super(Resnet, self).__init__(**kwargs)
+        super(ResnetV2, self).__init__(**kwargs)
-    return Resnet(Bottleneck, classes, [3, 4, 6, 3], [64, 256, 512, 1024, 2048], False)
+def resnet18v2_cifar(classes):
-net = resnet18_cifar(10)
+net = resnet18v2_cifar(10)
-            print batch_size/(time.time()-btic)
+            print 'speed: {} samples/s'.format(train_data.label_shape[0]/(time.time()-btic))
-    def score(self, eval_data, eval_metric, num_batch=None, batch_end_callback=None,
+    def score(self, eval_data, eval_metric=None, num_batch=None, batch_end_callback=None,
-            eval_metric = metric.create(eval_metric)
+        eval_metric = _parse_metric(self._symbol, eval_metric)
-    def fit(self, train_data, eval_data=None, eval_metric='acc',
+    def fit(self, train_data, eval_data=None, eval_metric=None,
-            eval_metric = metric.create(eval_metric)
+        eval_metric = _parse_metric(self._symbol, eval_metric)
-        if label_shapes is not None:
+        if label_shapes:
-        if self.label_shapes is not None:
+        if self.label_shapes:
-        if label_shapes is not None:
+        if label_shapes:
-            if label_shapes is not None:
+            if label_shapes:
-        if label_shapes is not None:
+        if label_shapes:
-        if label_shapes is not None:
+        if label_shapes:
-        if label_shapes is not None:
+        if label_shapes:
-            assert not inputs_need_grad
+        if not for_training and self._label_names and not label_shapes:
-            self.data_names, self.label_names, data_shapes, label_shapes)
+            symbol = self._symbol
-        self._exec_group = DataParallelExecutorGroup(self._symbol, self._context,
+        self._exec_group = DataParallelExecutorGroup(symbol, self._context,
-from .base import check_call, MXNetError, _Null # pylint: disable=unused-import
+from .base import check_call, MXNetError, _Null  # pylint: disable=unused-import
-from .ndarray import NDArray, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
+from .ndarray import NDArray, zeros as _nd_zeros, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
-    def eval(self, ctx=cpu(), **kwargs):
+    def eval(self, ctx=None, **kwargs):
-    test_deformable_convolution_with_type()
+def test_fused():
-           ['rnn_t0_out_plus_residual_output', 'rnn_t1_out_plus_residual_output']
+    # assert outputs.list_outputs() == \
-           ['bi_t0_plus_residual_output', 'bi_t1_plus_residual_output']
+    # assert outputs.list_outputs() == \
-    test_convgru()
+    import nose
-                      "#define CPP_PACKAGE_INCLUDE_MXNET_CPP_OP_H_\n"
+                      "#ifndef MXNET_CPP_OP_H_\n"
-                      "#endif  // CPP_PACKAGE_INCLUDE_MXNET_CPP_OP_H_\n")
+                      "#endif  // MXNET_CPP_OP_H_\n")
-        raise ValueError("The truth value of an NDArray with more than one element is ambiguous.")
+        raise ValueError("The truth value of an NDArray is ambiguous. " \
-        return in_shape, [in_shape[0]], []
+        return in_shape, (in_shape[0],)*len(self.list_outputs()), ()
-    
+
-                                                               pooled_size=num_group, output_dim=num_classes, 
+                    op = mx.contrib.sym.DeformablePSROIPooling(data=im_data_var, rois=rois_data_var,
-  a_ = mx.contrib.nd.dequantize(qa, min1, max1, out_type='float32')
+    min0 = mx.nd.array([0.0])
-  a_real  = mx.nd.array([[0.13725491, 0.59215689], [0.60392159, 0.8588236]])
+    qa_real = mx.nd.array([[35, 151], [154, 219]])
-  assert same(a_.asnumpy(),  a_real.asnumpy())
+    assert same(qa.asnumpy(), qa_real.asnumpy())
-                  % (str(err))
+        err_msg = str(err)
-            test_tutorial(file_dir)
+            test_tutorial_nb(file_dir)
-
+import argparse
-        init_h = [('l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l])) for l in range(num_rnn_layer)]
+        init_c = [('l%d_init_c' % l, (batch_size, num_hidden_rnn_list[l]))
-                           range(num_rnn_layer)]
+        forward_init_c = [('forward_l%d_init_c' % l, (batch_size, num_hidden_rnn_list[l]))
-                           range(num_rnn_layer)]
+        forward_init_h = [('forward_l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l]))
-        init_h = [('l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l])) for l in range(num_rnn_layer)]
+        init_h = [('l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l]))
-                           range(num_rnn_layer)]
+        forward_init_h = [('forward_l%d_init_h' % l, (batch_size, num_hidden_rnn_list[l]))
-            net = bi_lstm_unroll(net=net,
+def arch(args, seq_len=None):
-                                 num_lstm_layer=num_rnn_layer,
+                                 num_hidden_gru_list=num_hidden_rnn_list,
-            net = bi_gru_unroll(net=net,
+                                 is_batchnorm=is_batchnorm,
-                                is_batchnorm=is_batchnorm)
+                                label=label,
-        args.config.set('arch', 'max_t_count', str(seq_len_after_conv_layer2))
+            raise Exception('mode must be the one of the followings - train,predict,load')
-sys.path.insert(0, "../../python")
+from collections import namedtuple
-import os
+from stt_bucketing_module import STTBucketingModule
-
+def load_labelutil(labelUtil, is_bi_graphemes, language="en"):
-    is_bi_graphemes = args.config.getboolean('common', 'is_bi_graphemes')
+    log = LogUtil().getlogger()
-    elif mode =="train" or mode == "load":
+    if mode == "train" or mode == "load":
-
+        datagen.load_train_data(data_json, max_duration=max_duration)
-            datagen.load_validation_data(val_json)
+                log.info("Read mean and std from meta files")
-            'Define mode in the cfg file first. train or predict or load can be the candidate for the mode.')
+            datagen.get_meta_from_file(
-    if batch_size == 1 and is_batchnorm:
+    if batch_size == 1 and is_batchnorm and (mode == 'train' or mode == 'load'):
-        max_label_length = datagen.get_max_label_length(partition="train",is_bi_graphemes=is_bi_graphemes)
+        max_label_length = \
-            'Define mode in the cfg file first. train or predict or load can be the candidate for the mode.')
+        max_label_length = \
-                                    count=datagen.val_count,
+    sort_by_duration = (mode == "train")
-                                    is_bi_graphemes=is_bi_graphemes)
+                                    sort_by_duration=sort_by_duration,
-    is_start_from_batch = args.config.getboolean('load','is_start_from_batch')
+    is_start_from_batch = args.config.getboolean('load', 'is_start_from_batch')
-    model_loaded = symbol_template.arch(args)
+    is_bucketing = args.config.getboolean('arch', 'is_bucketing')
-    else:
+    elif mode == 'load' or mode == 'predict':
-
+        if is_bucketing:
-        label_names = [x[0] for x in data_train.provide_label]
+            data_names = [x[0] for x in data_train.provide_data]
-                                             load_optimizer_states=load_optimizer_states)
+            model_loaded = mx.module.Module.load(
-        raise Exception('cfg file path must be provided. ex)python main.py --configfile examplecfg.cfg')
+        raise Exception('cfg file path must be provided. ' +
-        random.seed(random_seed)
+        np.random.seed(random_seed)
-    if mx_random_seed !=-1:
+    if mx_random_seed != -1:
-            'Define mode in the cfg file first. train or predict or load can be the candidate for the mode.')
+            'Define mode in the cfg file first. ' +
-    elif mode == "train" or mode == "load":
+    if mode == "train" or mode == "load":
-        module = mx.mod.Module(model_loaded, context=contexts, data_names=data_names, label_names=label_names)
+        if is_bucketing:
-        do_training(args=args, module=model_loaded, data_train=data_train, data_val=data_val, begin_epoch=model_num_epoch+1)
+        do_training(args=args, module=model_loaded, data_train=data_train, data_val=data_val,
-                          label_shapes=data_train.provide_label)
+        if is_bucketing:
-        if is_batchnorm :
+        eval_metric = STTMetric(batch_size=batch_size, num_gpu=num_gpu)
-                model_loaded.forward(data_batch, is_train=True)
+                model_loaded.forward(data_batch, is_train=False)
-            model_loaded.score(eval_data=data_train, num_batch=None, eval_metric=eval_metric, reset=True)
+        else:
-
+from multiprocessing import cpu_count, Process, Manager
-        self.max_length_list_in_batch =[]
+        self.max_length_list_in_batch = []
-    def featurize(self, audio_clip, overwrite=False):
+    def featurize(self, audio_clip, overwrite=False, save_feature_as_csvfile=False):
-            max_freq=self.max_freq, overwrite=overwrite)
+            max_freq=self.max_freq, overwrite=overwrite, 
-        self.load_metadata_from_desc_file(desc_file, 'train')
+    def load_train_data(self, desc_file, max_duration):
-        self.load_metadata_from_desc_file(desc_file, 'validation')
+    def load_validation_data(self, desc_file, max_duration):
-        self.max_seq_length=max_seq_length
+        self.max_seq_length = max_seq_length
-    def prepare_minibatch(self, audio_paths, texts, overwrite=False, is_bi_graphemes=False):
+    def prepare_minibatch(self, audio_paths, texts, overwrite=False,
-        features = [self.featurize(a, overwrite=overwrite) for a in audio_paths]
+        features = [self.featurize(a, overwrite=overwrite, save_feature_as_csvfile=save_feature_as_csvfile) for a in audio_paths]
-        x = np.zeros((mb_size, self.max_seq_length, feature_dim))
+        if seq_length == -1:
-            audio_paths_iter = iter(self.audio_paths)
+            audio_paths = self.audio_paths
-            count = count + float(next_feat.shape[0])
+            audio_paths = samples
-        np.savetxt(generate_file_path(self.save_dir, self.model_name, 'feats_std'), self.feats_std)
+        np.savetxt(
-                 is_bi_graphemes=False, partition="train",):
+                 is_bi_graphemes=False, partition="train",
-                data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=True, is_bi_graphemes=self.is_bi_graphemes)
+                data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=True, is_bi_graphemes=self.is_bi_graphemes, save_feature_as_csvfile=self.save_feature_as_csvfile)
-                data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=False, is_bi_graphemes=self.is_bi_graphemes)
+                data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=False, is_bi_graphemes=self.is_bi_graphemes, save_feature_as_csvfile=self.save_feature_as_csvfile)
-              fix_gamma=True,
+              fix_gamma=False,
-                               output_mean_var=output_mean_var
+                               output_mean_var=output_mean_var,
-                               output_mean_var=output_mean_var
+                               output_mean_var=output_mean_var,
-         no_bias=False
+         no_bias=False,
-        net = mx.sym.Convolution(data=net, num_filter=channels, kernel=filter_dimension, stride=stride, no_bias=no_bias)
+        net = mx.sym.Convolution(data=net, num_filter=channels, kernel=filter_dimension, stride=stride, no_bias=no_bias,
-                                 no_bias=no_bias)
+                                 no_bias=no_bias, name=name)
-                                 no_bias=no_bias)
+                                 no_bias=no_bias, name=name)
-                                 bias=bias, no_bias=no_bias)
+                                 bias=bias, no_bias=no_bias, name=name)
-       no_bias=False
+       no_bias=False,
-        net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, no_bias=no_bias)
+        net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, no_bias=no_bias, name=name)
-            net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, no_bias=no_bias)
+            net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, no_bias=no_bias, name=name)
-            net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, bias=bias, no_bias=no_bias)
+            net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, bias=bias, no_bias=no_bias, name=name)
-        net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, weight=weight, no_bias=no_bias)
+        net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, weight=weight, no_bias=no_bias, name=name)
-            net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, weight=weight, no_bias=no_bias)
+            net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, weight=weight, no_bias=no_bias, name=name)
-            net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, weight=weight, bias=bias, no_bias=no_bias)
+            net = mx.sym.FullyConnected(data=net, num_hidden=num_hidden, weight=weight, bias=bias, no_bias=no_bias, name=name)
-        net = mx.sym.Activation(data=net, act_type=act_type)
+        net = mx.sym.Activation(data=net, act_type=act_type, name="%s_activation" % name)
-                dropout_rate=0
+                dropout_rate=0,
-                                    no_bias=is_batchnorm
+                                    no_bias=is_batchnorm,
-                        hidden = mx.sym.Activation(data=hidden, act_type=act_type_list[layer_index])
+                                           beta=beta_list[layer_index],
-def gru(num_hidden, indata, prev_state, param, seqidx, layeridx, dropout=0., is_batchnorm=False, gamma=None, beta=None):
+def gru(num_hidden, indata, prev_state, param, seqidx, layeridx, dropout=0., is_batchnorm=False, gamma=None, beta=None, name=None):
-        i2h = batchnorm(net=i2h, gamma=gamma, beta=beta)
+        if name is not None:
-                                       name="t%d_l%d_trans_i2h" % (seqidx, layeridx))
+                                       name="t%d_l%d_trans_h2h" % (seqidx, layeridx))
-               direction="forward"):
+def gru_unroll(net, num_gru_layer, seq_len,  num_hidden_gru_list, dropout=0., is_batchnorm=False, prefix="",
-                batchnorm_beta.append(mx.sym.Variable(prefix + "t%d_i2h_beta" % seqidx))
+            if is_bucketing:
-                                     beta=batchnorm_beta[k])
+                    if is_bucketing:
-                                     is_batchnorm=is_batchnorm)
+                                     is_batchnorm=is_batchnorm,
-def bi_gru_unroll(net, num_gru_layer, seq_len, num_hidden_gru_list, dropout=0., is_batchnorm=False):
+def bi_gru_unroll(net, num_gru_layer, seq_len, num_hidden_gru_list, dropout=0., is_batchnorm=False, is_bucketing=False):
-                                 direction="forward")
+                                 direction="forward",
-                                  direction="backward")
+                                  direction="backward",
-                                       is_batchnorm=False):
+                                       is_batchnorm=False, is_bucketing=False):
-                                 direction="forward")
+                                 direction="forward",
-                                  direction="backward")
+                                  direction="backward",
-def vanilla_lstm(num_hidden, indata, prev_state, param, seqidx, layeridx, is_batchnorm=False, gamma=None, beta=None):
+def vanilla_lstm(num_hidden, indata, prev_state, param, seqidx, layeridx, is_batchnorm=False, gamma=None, beta=None, name=None):
-        i2h = batchnorm(net=i2h, gamma=gamma, beta=beta)
+        if name is not None:
-         gamma=None, beta=None):
+         gamma=None, beta=None, name=None):
-        i2h = batchnorm(net=i2h, gamma=gamma, beta=beta)
+        if name is not None:
-                lstm_type='fc_lstm', is_batchnorm=False, prefix="", direction="forward"):
+                lstm_type='fc_lstm', is_batchnorm=False, prefix="", direction="forward", is_bucketing=False):
-                batchnorm_beta.append(mx.sym.Variable(prefix + "t%d_i2h_beta" % seqidx))
+            if is_bucketing:
-                                          )
+                        if is_bucketing:
-                                          is_batchnorm=is_batchnorm
+                                          is_batchnorm=is_batchnorm,
-                                                  beta=batchnorm_beta[k]
+                                                  gamma=batchnorm_gamma[i],
-                                                  is_batchnorm=is_batchnorm
+                                                  is_batchnorm=is_batchnorm,
-                   lstm_type='fc_lstm', is_batchnorm=False):
+                   lstm_type='fc_lstm', is_batchnorm=False, is_bucketing=False):
-                                  direction="forward")
+                                  direction="forward",
-                                   direction="backward")
+                                   direction="backward",
-                                        lstm_type='fc_lstm', is_batchnorm=False):
+                                        lstm_type='fc_lstm',
-                                  direction="forward")
+                                  direction="forward",
-                                   direction="backward")
+                                   direction="backward",
-    def __init__(self, batch_size, num_gpu, seq_length, is_epoch_end=False, is_logging=True):
+    def __init__(self, batch_size, num_gpu, is_epoch_end=False, is_logging=True):
-        self.seq_length = seq_length
+
-            for i in range(int(int(self.batch_size) / int(self.num_gpu))):
+            seq_length = len(pred) / int(int(self.batch_size) / int(self.num_gpu))
-                for k in range(int(self.seq_length)):
+                for k in range(int(seq_length)):
-                    loss = ctc_loss(l, pred, i, int(self.seq_length), int(self.batch_size), int(self.num_gpu))
+                    loss = ctc_loss(l, pred, i, int(seq_length), int(self.batch_size), int(self.num_gpu))
-                          eps=1e-14, overwrite=False):
+                          eps=1e-14, overwrite=False, save_feature_as_csvfile=False):
-    if (os.path.isfile(csvfilename) is False) or overwrite:
+    if (os.path.isfile(csvfilename) is False) or overwrite: 
-            np.savetxt(csvfilename, res)
+            if save_feature_as_csvfile:
-import numpy as np
+import json
-    seq_len = args.config.get('arch', 'max_t_count')
+    #seq_len = args.config.get('arch', 'max_t_count')
-    eval_metric = STTMetric(batch_size=batch_size, num_gpu=num_gpu, seq_length=seq_len,is_logging=enable_logging_validation_metric,is_epoch_end=True)
+    eval_metric = STTMetric(batch_size=batch_size, num_gpu=num_gpu, is_logging=enable_logging_validation_metric,is_epoch_end=True)
-    loss_metric = STTMetric(batch_size=batch_size, num_gpu=num_gpu, seq_length=seq_len,is_logging=enable_logging_train_metric,is_epoch_end=False)
+    loss_metric = STTMetric(batch_size=batch_size, num_gpu=num_gpu, is_logging=enable_logging_train_metric,is_epoch_end=False)
-    momentum = args.config.getfloat('train', 'momentum')
+    optimizer = args.config.get('optimizer', 'optimizer')
-    weight_decay = args.config.getfloat('train', 'weight_decay')
+    clip_gradient = args.config.getfloat('optimizer', 'clip_gradient')
-    module.bind(data_shapes=data_train.provide_data,
+        model.bind(data_shapes=data_train.provide_data,
-            raise Exception('Supported optimizers are sgd and adam. If you want to implement others define them in train.py')
+        optimizer_params = {'lr_scheduler': lr_scheduler,
-
+        data_train.is_first_epoch = False
-            pred = pred.values()
+            pred = list(pred.values())
-            label = label.values()
+            label = list(label.values())
-            for s in args:
+            for i, s in enumerate(args):
-                        raise TypeError('Arguments must be shapes (tuple)')
+                        raise TypeError("Arguments need to be shapes (tuple), "
-                    indptr.append(len(sdata))
+                if not isinstance(v, tuple):
-        new_data_shapes = (i.shape for i in data_batch.data)
+        curr_data_shapes = tuple(i.shape for i in self._data_shapes)
-
+    neg_idx = np.where(overlaps < config.TRAIN.FG_THRESH)[0]
-        keep_indexes = np.append(keep_indexes, gap_indexes)
+        gap = np.minimum(len(neg_rois), rois_per_image - keep_indexes.shape[0])
-            except Exception, e:
+            except Exception as e:
-        except Exception, e:
+        except Exception as e:
-    except Exception, e:
+    except Exception as e:
-                    q_out = Queue.Queue()
+                    try:
-                    for grad_nodes in [['im_data'], ['offset_data']]:
+                    for grad_nodes in [['im_data'], ['offset_data'], ['weight']]:
-                    allow_missing=False, force_init=False):
+                    allow_missing=False, force_init=False, allow_extra=False):
-                                      force_init=force_init)
+                                      force_init=force_init, allow_extra=allow_extra)
-            self.filter = list(layer_def.convolution_param.kernel_size)
+            if LayerRecord._is_iterable(layer_def.convolution_param.kernel_size):
-            self.pad = list(layer_def.convolution_param.pad)
+            if LayerRecord._is_iterable(layer_def.convolution_param.pad):
-            self.stride = list(layer_def.convolution_param.stride)
+            if LayerRecord._is_iterable(layer_def.convolution_param.stride):
-    for child_layer_name in layer_name_to_record.keys():
+    for child_layer_name in layer_name_to_record.keys():  # pylint: disable=too-many-nested-blocks
-                        parent_layer_def.children.append(child_layer_def)
+            if bottom in top_to_layers:
-        if layer.name in caffe_net.params and layer.type in ['Convolution', 'InnerProduct']:
+        if layer.name in caffe_net.params and layer.type in ['Convolution', 'InnerProduct',
-                mx_beta = mx_beta[:, ::-1, :, :]
+                # if RGB or RGBA
-            bn_name = normalized_layer_name.replace('scale', 'bn')
+            if 'scale' in normalized_layer_name:
-            caf_mean = caffe_net.params[layer.name][0].data
+            caf_mean = caffe_net.params[layer.name][0].data / caf_rescale_factor
-            caf_var = caffe_net.params[layer.name][1].data
+            caf_var = caffe_net.params[layer.name][1].data / caf_rescale_factor
-                            'Dropout']:
+                            'Dropout', 'Crop']:
-            caf_blob = caf_blob[:, ::-1, :, :]
+
-            mx_name = mx_name.replace('scale', 'bn')
+            if 'scale' in mx_name:
-           or layer_type == 4 or layer_type == 14 or layer_type == 'PReLU':
+        if layer_type == 'Convolution' or layer_type == 'InnerProduct'  \
-            bn_name = layer_name.replace('scale', 'bn')
+            if 'scale' in layer_name:
-            assert len(layer_blobs) == 0
+            assert len(layer_blobs) == 0
-        dilate = 1 if len(param.dilation) == 0 else param.dilation[0]
+    if hasattr(param, 'dilation'):
-                param.use_global_stats, epsilon)
+            # if next layer is scale, don't fix gamma
-from .ndarray import NDArray, zeros, clip, sqrt, sign, array
+from .ndarray import (NDArray, zeros, clip, sqrt, sign, array, maximum, abs as NDabs)
-                    ((self.beta + sqrt(n)) / lr + wd) * (NDArray.abs(dn) > self.lamda1)
+                    ((self.beta + sqrt(n)) / lr + wd) * (NDabs(dn) > self.lamda1)
-    if(add_stn):
+    if add_stn:
-        lr_step_epochs = '10',
+        lr_step_epochs = '10'
-        """Forward computation.
+        """Forward computation. It supports data batches with different shapes, such as
-        >>> # An example of forward computation.
+        >>> import mxnet as mx
-        >>> mod.bind(data_shapes=[('data', (1, 10, 10))])
+        >>> data = mx.sym.Variable('data')
-        >>> data1 = [mx.nd.ones([1, 10, 10])]
+        >>> data1 = [mx.nd.ones((1, 10))]
-           0.10000272  0.10000113  0.09999088  0.09999888]]
+        [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]
-        """Forward computation.
+        """Forward computation. It supports data batches with different shapes, such as
-    test_executor_group()
+    import nose
-cfg.train.preprocess_threads = 6
+cfg.train.preprocess_threads = 48
-        self.image_set_index = []
+        self.image_set_index = None
-    def __init__(self, image_set, year, devkit_path, shuffle=False, is_train=False):
+    def __init__(self, image_set, year, devkit_path, shuffle=False, is_train=False,
-                        'sheep', 'sofa', 'train', 'tvmonitor']
+        self.classes = self._load_class_names(names,
-import importlib
+from symbol.symbol_factory import get_symbol
-                 nms_thresh=0.5, force_nms=True):
+def get_detector(net, prefix, epoch, data_shape, mean_pixels, ctx, num_class,
-        data_shape, mean_pixels, ctx=ctx)
+        net = get_symbol(net, data_shape, num_classes=num_class, nms_thresh=nms_thresh,
-                        choices=['vgg16_ssd_300', 'vgg16_ssd_512'], help='which network to use')
+    parser.add_argument('--network', dest='network', type=str, default='resnet50',
-                        help='run demo with images, use comma(without space) to seperate multiple images')
+                        help='run demo with images, use comma to seperate multiple images')
-                        default=os.path.join(os.getcwd(), 'model', 'ssd'), type=str)
+                        default=os.path.join(os.getcwd(), 'model', 'ssd_'),
-    parser.add_argument('--data-shape', dest='data_shape', type=int, default=300,
+    parser.add_argument('--data-shape', dest='data_shape', type=int, default=512,
-    detector = get_detector(network, args.prefix, args.epoch,
+    class_names = parse_class_names(args.class_names)
-                            ctx, args.nms_thresh, args.force_nms)
+                            ctx, len(class_names), args.nms_thresh, args.force_nms)
-                                  CLASSES, args.thresh, args.show_timer)
+                                  class_names, args.thresh, args.show_timer)
-                        choices=['vgg16_ssd_300', 'vgg16_ssd_512'], help='which network to use')
+    parser.add_argument('--network', dest='network', type=str, default='vgg16_reduced',
-                        default=os.path.join(os.getcwd(), 'model', 'ssd_300'), type=str)
+                        default=os.path.join(os.getcwd(), 'model', 'ssd_'), type=str)
-    _, arg_params, aux_params = mx.model.load_checkpoint(args.prefix, args.epoch)
+    net = get_symbol(args.network, args.data_shape,
-    tmp = args.prefix.rsplit('/', 1)
+    tmp = prefix.rsplit('/', 1)
-                        choices=['vgg16_ssd_300', 'vgg16_ssd_512'], help='which network to use')
+    parser.add_argument('--network', dest='network', type=str, default='vgg16_reduced',
-    parser.add_argument('--class-names', dest='class_names', type=str, default=",".join(CLASSES),
+    parser.add_argument('--class-names', dest='class_names', type=str,
-                        default=os.path.join(os.getcwd(), 'model', 'ssd'), type=str)
+                        default=os.path.join(os.getcwd(), 'model', 'ssd_'), type=str)
-                 args.prefix, args.epoch, ctx, batch_size=args.batch_size,
+                 prefix, args.epoch, ctx, batch_size=args.batch_size,
-            .get_symbol(num_classes, nms_thresh, force_nms)
+        net = get_symbol(net, data_shape[1], num_classes=num_classes,
-                    clip=True, interm_layer=0, steps=[]):
+                    clip=False, interm_layer=0, steps=[]):
-    assert sum(x > 0 for x in normalization) == len(num_channels), \
+    assert sum(x > 0 for x in normalization) <= len(num_channels), \
-                init=mx.init.Constant(normalization[k]))
+                init=mx.init.Constant(normalization[k]),
-from common import conv_act_layer
+from common import legacy_conv_act_layer
-def get_symbol_train(num_classes=20, nms_thresh=0.5, force_suppress=False, nms_topk=400):
+def get_symbol_train(num_classes=20, nms_thresh=0.5, force_suppress=False,
-    conv8_1, relu8_1 = conv_act_layer(relu7, "8_1", 256, kernel=(1,1), pad=(0,0), \
+    conv8_1, relu8_1 = legacy_conv_act_layer(relu7, "8_1", 256, kernel=(1,1), pad=(0,0), \
-    conv8_2, relu8_2 = conv_act_layer(relu8_1, "8_2", 512, kernel=(3,3), pad=(1,1), \
+    conv8_2, relu8_2 = legacy_conv_act_layer(relu8_1, "8_2", 512, kernel=(3,3), pad=(1,1), \
-    conv9_1, relu9_1 = conv_act_layer(relu8_2, "9_1", 128, kernel=(1,1), pad=(0,0), \
+    conv9_1, relu9_1 = legacy_conv_act_layer(relu8_2, "9_1", 128, kernel=(1,1), pad=(0,0), \
-    conv9_2, relu9_2 = conv_act_layer(relu9_1, "9_2", 256, kernel=(3,3), pad=(1,1), \
+    conv9_2, relu9_2 = legacy_conv_act_layer(relu9_1, "9_2", 256, kernel=(3,3), pad=(1,1), \
-    conv10_1, relu10_1 = conv_act_layer(relu9_2, "10_1", 128, kernel=(1,1), pad=(0,0), \
+    conv10_1, relu10_1 = legacy_conv_act_layer(relu9_2, "10_1", 128, kernel=(1,1), pad=(0,0), \
-    conv10_2, relu10_2 = conv_act_layer(relu10_1, "10_2", 256, kernel=(3,3), pad=(0,0), \
+    conv10_2, relu10_2 = legacy_conv_act_layer(relu10_1, "10_2", 256, kernel=(3,3), pad=(0,0), \
-    conv11_1, relu11_1 = conv_act_layer(relu10_2, "11_1", 128, kernel=(1,1), pad=(0,0), \
+    conv11_1, relu11_1 = legacy_conv_act_layer(relu10_2, "11_1", 128, kernel=(1,1), pad=(0,0), \
-    conv11_2, relu11_2 = conv_act_layer(relu11_1, "11_2", 256, kernel=(3,3), pad=(0,0), \
+    conv11_2, relu11_2 = legacy_conv_act_layer(relu11_1, "11_2", 256, kernel=(3,3), pad=(0,0), \
-def get_symbol(num_classes=20, nms_thresh=0.5, force_suppress=False, nms_topk=400):
+def get_symbol(num_classes=20, nms_thresh=0.5, force_suppress=False,
-from common import conv_act_layer
+from common import legacy_conv_act_layer
-    conv8_1, relu8_1 = conv_act_layer(relu7, "8_1", 256, kernel=(1,1), pad=(0,0), \
+    conv8_1, relu8_1 = legacy_conv_act_layer(relu7, "8_1", 256, kernel=(1,1), pad=(0,0), \
-    conv8_2, relu8_2 = conv_act_layer(relu8_1, "8_2", 512, kernel=(3,3), pad=(1,1), \
+    conv8_2, relu8_2 = legacy_conv_act_layer(relu8_1, "8_2", 512, kernel=(3,3), pad=(1,1), \
-    conv9_1, relu9_1 = conv_act_layer(relu8_2, "9_1", 128, kernel=(1,1), pad=(0,0), \
+    conv9_1, relu9_1 = legacy_conv_act_layer(relu8_2, "9_1", 128, kernel=(1,1), pad=(0,0), \
-    conv9_2, relu9_2 = conv_act_layer(relu9_1, "9_2", 256, kernel=(3,3), pad=(1,1), \
+    conv9_2, relu9_2 = legacy_conv_act_layer(relu9_1, "9_2", 256, kernel=(3,3), pad=(1,1), \
-    conv10_1, relu10_1 = conv_act_layer(relu9_2, "10_1", 128, kernel=(1,1), pad=(0,0), \
+    conv10_1, relu10_1 = legacy_conv_act_layer(relu9_2, "10_1", 128, kernel=(1,1), pad=(0,0), \
-    conv10_2, relu10_2 = conv_act_layer(relu10_1, "10_2", 256, kernel=(3,3), pad=(1,1), \
+    conv10_2, relu10_2 = legacy_conv_act_layer(relu10_1, "10_2", 256, kernel=(3,3), pad=(1,1), \
-    conv11_1, relu11_1 = conv_act_layer(relu10_2, "11_1", 128, kernel=(1,1), pad=(0,0), \
+    conv11_1, relu11_1 = legacy_conv_act_layer(relu10_2, "11_1", 128, kernel=(1,1), pad=(0,0), \
-    conv11_2, relu11_2 = conv_act_layer(relu11_1, "11_2", 256, kernel=(3,3), pad=(1,1), \
+    conv11_2, relu11_2 = legacy_conv_act_layer(relu11_1, "11_2", 256, kernel=(3,3), pad=(1,1), \
-    conv12_1, relu12_1 = conv_act_layer(relu11_2, "12_1", 128, kernel=(1,1), pad=(0,0), \
+    conv12_1, relu12_1 = legacy_conv_act_layer(relu11_2, "12_1", 128, kernel=(1,1), pad=(0,0), \
-    conv12_2, relu12_2 = conv_act_layer(relu12_1, "12_2", 256, kernel=(4,4), pad=(1,1), \
+    conv12_2, relu12_2 = legacy_conv_act_layer(relu12_1, "12_2", 256, kernel=(4,4), pad=(1,1), \
-import sys
+import sys, os
-                    choices = ['vgg16_ssd_300', 'vgg16_ssd_512'],
+parser.add_argument('--network', type=str, default='vgg16_reduced',
-    net = importlib.import_module("symbol_" + args.network).get_symbol(args.num_classes)
+    net = symbol_factory.get_symbol(args.network, args.data_shape, num_classes=args.num_classes)
-    a.render("ssd_" + args.network)
+    a.render("ssd_" + args.network + '_' + str(args.data_shape))
-    net = importlib.import_module("symbol_" + args.network).get_symbol_train(args.num_classes)
+    net = symbol_factory.get_symbol_train(args.network, args.data_shape, num_classes=args.num_classes)
-                        choices=['vgg16_ssd_300', 'vgg16_ssd_512'], help='which network to use')
+    parser.add_argument('--network', dest='network', type=str, default='vgg16_reduced',
-    parser.add_argument('--lr', dest='learning_rate', type=float, default=0.004,
+    parser.add_argument('--lr', dest='learning_rate', type=float, default=0.002,
-    parser.add_argument('--lr-steps', dest='lr_refactor_step', type=str, default='150, 200',
+    parser.add_argument('--lr-steps', dest='lr_refactor_step', type=str, default='80, 160',
-                    class_names = [l.strip() for l in f.readlines()]
+            # try to open it to read class names
-    stride : tupe
+    stride : tuple
-    dim_match : Boolen
+    dim_match : Boolean
-    stride : tupe
+    stride : tuple
-    dim_match : Boolen
+    dim_match : Boolean
-    stride : tupe
+    stride : tuple
-    dim_match : Boolen
+    dim_match : Boolean
-    stride : tupe
+    stride : tuple
-    dim_match : Boolen
+    dim_match : Boolean
-    stride : tupe
+    stride : tuple
-    dim_match : Boolen
+    dim_match : Boolean
-                    allow_missing=False, force_init=False):
+                    allow_missing=False, force_init=False, allow_extra=False):
-    def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True):
+    def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,
-                         allow_missing=allow_missing, force_init=force_init)
+                         allow_missing=allow_missing, force_init=force_init,
-    def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True):
+    def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,
-                                     force_init=force_init)
+                                     force_init=force_init, allow_extra=allow_extra)
-                    allow_missing=False, force_init=False):
+                    allow_missing=False, force_init=False, allow_extra=False):
-                                      force_init=force_init)
+                                      force_init=force_init, allow_extra=allow_extra)
-    def set_params(self, arg_params, aux_params):
+    def set_params(self, arg_params, aux_params, allow_extra=False):
-            exec_.copy_params_from(arg_params, aux_params)
+            exec_.copy_params_from(arg_params, aux_params, allow_extra_params=allow_extra)
-                    allow_missing=False, force_init=False):
+                    allow_missing=False, force_init=False, allow_extra=False):
-        self._exec_group.set_params(self._arg_params, self._aux_params)
+        self._exec_group.set_params(self._arg_params, self._aux_params,
-    def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True):
+    def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True,
-
+        allow_extra : boolean, optional
-                             allow_missing=allow_missing, force_init=force_init)
+                             allow_missing=allow_missing, force_init=force_init,
-        self._exec_group.set_params(arg_params, aux_params)
+        self._exec_group.set_params(arg_params, aux_params, allow_extra=allow_extra)
-                    allow_missing=False, force_init=False):
+                    allow_missing=False, force_init=False, allow_extra=False):
-                    allow_missing=False, force_init=False):
+                    allow_missing=False, force_init=False, allow_extra=False):
-                               force_init=force_init)
+                               force_init=force_init, allow_extra=allow_extra)
-    else:
+    if isinstance(keys, (tuple, list)):
-
+        return (c_array(ctypes.c_char_p, c_keys), c_array(NDArrayHandle, c_vals))
-        key : int or sequence of int
+        key : str or sequence of str
-        >>> kv.init(3, mx.nd.ones(shape)*2)
+        >>> kv.init('3', mx.nd.ones(shape)*2)
-        >>> kv.pull(3, out=a)
+        >>> kv.pull('3', out=a)
-        >>> keys = [5, 7, 9]
+        >>> keys = ['5', '7', '9']
-            self.handle, mx_uint(len(ckeys)), ckeys, cvals))
+        check_call(_LIB.MXKVStoreInitEx(self.handle, mx_uint(len(ckeys)), ckeys, cvals))
-        key : int or list of int
+        key : str or list of str
-        >>> kv.pull(3, out=a) # pull out the value
+        >>> kv.push('3', mx.nd.ones(shape)*8)
-        >>> kv.pull(3, out=a)
+        >>> kv.push('3', b)
-        check_call(_LIB.MXKVStorePush(
+        check_call(_LIB.MXKVStorePushEx(
-        >>> kv.pull(3, out=a)
+        >>> kv.pull('3', out=a)
-        >>> kv.pull(3, out=b)
+        >>> kv.pull('3', out=b)
-        >>> keys = [5, 7, 9]
+        >>> keys = ['5', '7', '9']
-        check_call(_LIB.MXKVStorePull(
+        check_call(_LIB.MXKVStorePullEx(
-        >>> kv.pull(3, out=a)
+        >>> kv.pull('3', out=a)
-        >>> kv.push(3, mx.nd.ones(shape))
+        >>> kv.push('3', mx.nd.ones(shape))
-        >>> kv.pull(3, out=a)
+        >>> kv.pull('3', out=a)
-        kvstore.init(idx, arg_params[param_names[idx]])
+        name = param_names[idx]
-            kvstore.pull(idx, param_on_devs, priority=-idx)
+            kvstore.pull(name, param_on_devs, priority=-idx)
-def _update_params_on_kvstore(param_arrays, grad_arrays, kvstore):
+def _update_params_on_kvstore(param_arrays, grad_arrays, kvstore, param_names):
-        kvstore.push(index, grad_list, priority=-index)
+        kvstore.push(name, grad_list, priority=-index)
-        kvstore.pull(index, arg_list, priority=-index)
+        kvstore.pull(name, arg_list, priority=-index)
-                   kvstore=None):
+                   kvstore=None, param_names=None):
-            kvstore.push(index, grad_list, priority=-index)
+            kvstore.push(name, grad_list, priority=-index)
-            kvstore.pull(index, grad_list, priority=-index)
+            kvstore.pull(name, grad_list, priority=-index)
-                                              kvstore)
+                                              kvstore, executor_manager.param_names)
-                                   kvstore=kvstore)
+                                   kvstore=kvstore,
-                                      self._kvstore)
+                                      self._kvstore, self._exec_group.param_names)
-                           kvstore=self._kvstore)
+                           kvstore=self._kvstore,
-    check_diff_to_scalar(val, 1)
+    check_single_kv_pair(init_kv(), 3)
-    check_diff_to_scalar(a, 4)
+    def check_init(kv, key):
-        check_diff_to_scalar(v, 4)
+    check_list_kv_pair(init_kv(), keys)
-    kv = init_kv()
+    def check_aggregator(kv, key, key_list):
-    devs = [mx.Context('cpu', i) for i in range(num_devs)]
+        # single
-    vals = [mx.nd.ones(shape, d) for d in devs]
+        kv.push(key, vals)
-    kv.pull(3, out = vals)
+        for v in vals:
-        check_diff_to_scalar(v, num_devs)
+        # list
-    kv.pull(keys, out = vals)
+        for vv in vals:
-            check_diff_to_scalar(v, num_devs * 2.0)
+    check_aggregator(init_kv(), 3, keys)
-    kv._set_updater(updater)
+    def check_updater(kv, key, key_list):
-    devs = [mx.Context(dev, i) for i in range(num_devs)]
+        # single
-    vals = [mx.nd.ones(shape, d) for d in devs]
+        kv.push(key, vals)
-    kv.pull(3, out = vals)
+        for v in vals:
-        check_diff_to_scalar(v, num_devs)
+        # list
-    vals = [[mx.nd.ones(shape, d) for d in devs]] * len(keys)
+        num_push = 4
-        kv.push(keys, vals)
+    kv = init_kv()
-    kv.pull(keys, out = vals)
+    str_kv = init_kv_with_str()
-    lib = ctypes.CDLL(lib_path[0], ctypes.RTLD_GLOBAL)
+    lib = ctypes.CDLL(lib_path[0], ctypes.RTLD_LOCAL)
-        logging.info("%s exists, skip to downloada", fname)
+        logging.info("%s exists, skipping download", fname)
-def _download_caffe_model(model_name, meta_info, dst_dir='./model'):
+def download_caffe_model(model_name, meta_info, dst_dir='./model'):
-    (prototxt, caffemodel, mean) = _download_caffe_model(model_name, meta_info, dst_dir)
+    (prototxt, caffemodel, mean) = download_caffe_model(model_name, meta_info, dst_dir)
-from convert_caffe_modelzoo import convert_caffe_model, get_model_meta_info
+from convert_caffe_modelzoo import convert_caffe_model, get_model_meta_info, download_caffe_model
-    logging.info('test %s', model_name)
+def test_imagenet_model_performance(model_name, val_data, gpus, batch_size):
-                     gpus=gpus,
+                     gpus=gpus_string,
-    assert acc[1].get()[1] > meta_info['top-5-acc'] - 0.03
+    max_performance_diff_allowed = 0.03
-        gpus = ''
+        gpus = [-1]
-        test_imagenet_model(m, val, ','.join([str(i) for i in gpus]), batch_size)
+        test_model_weights_and_outputs(m, args.image_url, gpus[0])
-            'lr_scheduler': lr_scheduler}
+            'lr_scheduler': lr_scheduler,
-    bias = mx.symbol.Cast(data=bias, dtype=np.float16)
+    weight = mx.symbol.Variable(name='conv1_weight', dtype=np.float16)
-    bias = mx.symbol.Cast(data=bias, dtype=np.float16)
+    weight = mx.symbol.Variable(name='conv2_weight', dtype=np.float16)
-    bias = mx.symbol.Cast(data=bias, dtype=np.float16)
+    weight = mx.symbol.Variable(name='conv3_weight', dtype=np.float16)
-    bias = mx.symbol.Cast(data=bias, dtype=np.float16)
+    weight = mx.symbol.Variable(name='conv4_weight', dtype=np.float16)
-    bias = mx.symbol.Cast(data=bias, dtype=np.float16)
+    weight = mx.symbol.Variable(name='conv5_weight', dtype=np.float16)
-    bias = mx.symbol.Cast(data=bias, dtype=np.float16)
+    weight = mx.symbol.Variable(name='fc1_weight', dtype=np.float16)
-    bias = mx.symbol.Cast(data=bias, dtype=np.float16)
+    weight = mx.symbol.Variable(name='fc2_weight', dtype=np.float16)
-    bias = mx.symbol.Cast(data=bias, dtype=np.float16)
+    weight = mx.symbol.Variable(name='fc3_weight', dtype=np.float16)
-        weight = mx.symbol.Cast(data=weight, dtype=np.float16)
+        weight = mx.symbol.Variable(name=name + '_conv1_weight', dtype=np.float16)
-        weight = mx.symbol.Cast(data=weight, dtype=np.float16)
+        weight = mx.symbol.Variable(name=name + '_conv2_weight', dtype=np.float16)
-        weight = mx.symbol.Cast(data=weight, dtype=np.float16)
+        weight = mx.symbol.Variable(name=name + '_conv3_weight', dtype=np.float16)
-            weight = mx.symbol.Cast(data=weight, dtype=np.float16)
+            weight = mx.symbol.Variable(name=name + '_conv1sc_weight', dtype=np.float16)
-        weight = mx.symbol.Cast(data=weight, dtype=np.float16)
+        weight = mx.symbol.Variable(name=name + '_conv1_weight', dtype=np.float16)
-        weight = mx.symbol.Cast(data=weight, dtype=np.float16)
+        weight = mx.symbol.Variable(name=name + '_conv2_weight', dtype=np.float16)
-            weight = mx.symbol.Cast(data=weight, dtype=np.float16)
+            weight = mx.symbol.Variable(name=name + '_conv1sc_weight', dtype=np.float16)
-    weight = mx.symbol.Cast(data=weight, dtype=np.float16)
+    weight = mx.symbol.Variable(name='conv0_weight', dtype=np.float16)
-    bias = mx.symbol.Cast(data=bias, dtype=np.float16)
+    weight = mx.symbol.Variable(name='fc1_weight', dtype=np.float16)
-from .ndarray import sgd_update, sgd_mom_update, adam_update, rmsprop_update, rmspropalex_update
+import warnings
-    def __init__(self, momentum=0.0, **kwargs):
+    def __init__(self, momentum=0.0, multi_precision=False, **kwargs):
-            return zeros(weight.shape, weight.context, dtype=weight.dtype)
+        momentum = None
-            sgd_mom_update(weight, grad, state, out=weight,
+        if not use_multi_precision:
-                       lr=lr, wd=wd, **kwargs)
+            if state[0] is not None:
-    g1 = mx.random.uniform(shape=shape, ctx=default_context())
+def compare_optimizer(opt1, opt2, shape, dtype):
-            assert(same(s1.asnumpy(), s2.asnumpy()))
+            if s1 is not None or s2 is not None:
-            assert_almost_equal(s1.asnumpy(), s2.asnumpy(), rtol=1e-4, atol=1e-5)
+            if s1 is not None or s2 is not None:
-    def __init__(self, learning_rate=0.01, momentum=0.0, **kwargs):
+    def __init__(self, learning_rate=0.01, momentum=0.0, multi_precision=False, **kwargs):
-            return None
+        momentum = None
-            return mx.nd.zeros(weight.shape, weight.context, dtype=weight.dtype)
+            if self.momentum != 0.0:
-                    lr*mx.nd.clip(grad*self.rescale_grad, -self.clip_gradient, self.clip_gradient))
+        use_multi_precision = isinstance(state, list) or isinstance(state, tuple)
-                weight[:] = (1 - lr*wd)*weight - lr*self.rescale_grad*grad
+                mom = state
-                weight += mom
+            grad32 = array(grad, ctx=grad.context, dtype=np.float32)
-                weight += mom
+                if self.clip_gradient is not None:
-        compare_optimizer(opt1(**kwarg), opt2(**kwarg), shape)
+    mom_options = [{}, {'momentum': 0.9}]
-        compare_optimizer(opt1(**kwarg), opt2(**kwarg), shape)
+        compare_optimizer(opt1(**kwarg), opt2(**kwarg), shape, np.float32)
-        compare_optimizer(opt1(**kwarg), opt2(**kwarg), shape)
+        compare_optimizer(opt1(**kwarg), opt2(**kwarg), shape, np.float32)
-from ..base import NDArrayHandle, OpHandle
+from ..base import NDArrayHandle, OpHandle, CachedOpHandle
-        ctypes.byref(output_vars)))
+class CachedOp(object):
-                for i in range(num_output.value)]
+    def __del__(self):
-        from ._ctypes.ndarray import invoke, CachedOp, _imperative_invoke
+        from ._ctypes.ndarray import CachedOp, _imperative_invoke
-        from ._cy3.ndarray import invoke, CachedOp, _imperative_invoke
+        from ._cy3.ndarray import CachedOp, _imperative_invoke
-        from ._cy2.ndarray import invoke, CachedOp, _imperative_invoke
+        from ._cy2.ndarray import CachedOp, _imperative_invoke
-    from ._ctypes.ndarray import invoke, CachedOp, _imperative_invoke
+    from ._ctypes.ndarray import CachedOp, _imperative_invoke
-        from ._ctypes.symbol import CachedOp, invoke, _symbol_creator  # pylint: disable=unused-import
+        from ._ctypes.symbol import _symbol_creator  # pylint: disable=unused-import
-        from ._cy3.symbol import CachedOp, invoke, _symbol_creator  # pylint: disable=unused-import
+        from ._cy3.symbol import _symbol_creator  # pylint: disable=unused-import
-        from ._cy2.symbol import CachedOp, invoke, _symbol_creator  # pylint: disable=unused-import
+        from ._cy2.symbol import _symbol_creator  # pylint: disable=unused-import
-    from ._ctypes.symbol import CachedOp, invoke, _symbol_creator  # pylint: disable=unused-import
+    from ._ctypes.symbol import _symbol_creator  # pylint: disable=unused-import
-        aux_states : list of string
+        aux_states : list of str
-    op = mx.nd.CachedOp('Convolution', 3, kernel=(3, 3), num_filter=10)
+    sym = mx.sym.Convolution(kernel=(3, 3), num_filter=10) + 2
-    o1 = mx.nd.invoke(op, [data, weight, bias])
+    o1 = op(data, weight, bias)
-    o2 = mx.nd.invoke(op, [data, weight, bias])
+    o2 = op(data, weight, bias)
-                    rtol, atol = 1e-2, 1e-4
+                    rtol, atol = 1e-2, 1e-3
-                            rtol, atol = 0.05, 1e-4
+                            rtol, atol = 0.05, 1e-3
-                        rtol, atol = 1e-2, 1e-4
+                        rtol, atol = 1e-2, 1e-3
-        If there are two many splits such that some slice can be empty.
+        In case of too many splits, leading to some empty slices.
-            raise ValueError('Too many slices such that some splits are empty')
+            raise ValueError('Too many slices. Some splits are empty.')
-        In this case, many memory will be shared.
+        symbol with the same set of parameters (e.g. unrolled RNNs with different lengths).
-    return [np.round(d[0]*100), np.round(d[1]*100)]
+    return [np.round(d[0]*100).astype(int), np.round(d[1]*100).astype(int)]
-def check_binary_op_forward(symbol, baseline, gen_data):
+def gen_binary_data_int(dummy):
-def check_binary_op_backward(symbol, baseline, gen_data):
+        y = y.outputs[0].asnumpy()
-        assert_allclose(x_2, y_2.asnumpy(), rtol=1e-3, atol=1e-5)
+        assert_allclose(y_1.asnumpy(), x_1, rtol=rtol, atol=atol)
-    # gpu-testing when it is ready. 
+    # gpu-testing when it is ready.
-    #Ensure that ithis tests don't get changed by other calls to random. 
+    #Ensure that ithis tests don't get changed by other calls to random.
-    data_in4 = np.random.uniform(1, 10, shape4) 
+    data_in1 = np.random.uniform(1, 10, shape1)
-    data_in2_t = np.transpose(data_in2) 
+    data_in1_t = np.transpose(data_in1)
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7) 
+    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7)
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_a = 1, transpose_b = 1) 
+    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_a = 1, transpose_b = 1)
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_a = 1) 
+    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_a = 1)
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_b = 1) 
+    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7, transpose_b = 1)
-    # Check batch of gemm. 
+    # Check batch of gemm.
-    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7) 
+    test_gemm = mx.sym.linalg_gemm(data1, data2, data3, alpha = 4, beta = 7)
-    # Check gemm2 operator same way as gemm. 
+    # Check gemm2 operator same way as gemm.
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4) 
+    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4, transpose_a = 1, transpose_b = 1) 
+    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4, transpose_a = 1, transpose_b = 1)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4, transpose_a = 1) 
+    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4, transpose_a = 1)
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4, transpose_b = 1) 
+    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4, transpose_b = 1)
-    # Check batch of gemm2. 
+    # Check batch of gemm2.
-    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4) 
+    test_gemm = mx.sym.linalg_gemm2(data1, data2, alpha = 4)
-    # Now test all the other operators. 
+    # Now test all the other operators.
-    res_sumlogdiag = np.reshape(np.log(data_in),(4,4))  
+    res_sumlogdiag = np.reshape(np.log(data_in),(4,4))
-    # have to be excluded by manual inspection. 
+    # Tests for numeric gradients for potrf/potri/trmm/trsm are suppressed by default
-    
+
-     
+
-                assert_almost_equal(exe.grad_dict['b'].asnumpy(), bgrad_npy, rtol=1e-3)
+    for data_type in dtypes:
-        y = mx.sym.Variable('y')
+    def dot_sym(data_type):
-        y = mx.sym.Variable('y')
+    def dot_sym_xT(data_type):
-        y = mx.sym.Variable('y')
+    def dot_sym_yT(data_type):
-        y = mx.sym.Variable('y')
+    def dot_sym_xT_yT(data_type):
-        check_numeric_gradient(dot_sym_xT_yT(), [m1_npy.T, m2_npy.T], numeric_eps=1e-1, rtol=2e-2, atol=1e-3)
+    for data_type in dtypes:
-                                   bgrad_npy + b_init_grad_npy, rtol=1e-3, atol=1e-4)
+    dtypes = ['float32', 'float64']
-    ## define alexnet
+    ## define VGG11
-    relu5_2 = mx.symbol.Activation(data=conv5_2, act_type="relu", name="conv1_2")
+    relu5_2 = mx.symbol.Activation(data=conv5_2, act_type="relu", name="relu5_2")
-    ctx = mx.cpu(0),      # Run on CPU 0
+mod = mx.mod.Module(
-    X=train_iter,  # Training data set
+    label_names = ['svm_label'],
-    batch_end_callback = mx.callback.Speedometer(batch_size, 200))  # Logging module to print out progress
+    batch_end_callback = mx.callback.Speedometer(batch_size, 200),  # Logging module to print out progress
-print('Accuracy:', model.score(test_iter)*100, '%')
+print('Accuracy:', mod.score(test_iter, mx.metric.Accuracy())[0][1]*100, '%')
-    return
+    """The Test optimizer"""
-    
+
-import sys,os
+
-
+import argparse
-logger = logging.getLogger(__name__) # get a logger to accuracies are printed
+import data_helpers
-logs = sys.stderr
+logging.basicConfig(level=logging.DEBUG)
-CNNModel = namedtuple("CNNModel", ['cnn_exec', 'symbol', 'data', 'label', 'param_blocks'])
+def data_iter(batch_size, num_embed, pre_trained_word2vec=False):
-        dropout=0., with_embedding=True):
+    # randomly shuffle data
-    input_y = mx.sym.Variable('softmax_label') # placeholder for output
+    # split train/valid set
-    if not with_embedding:
+    if not pre_trained_word2vec:
-
+    return sm, ('data',), ('softmax_label',)  
-    train_without_pretrained_embedding()
+    # parse args
-from ..io import DataIter, DataBatch
+from ..io import DataIter, DataBatch, DataDesc
-                 layout='NTC'):
+                 layout='NT'):
-            self.provide_label = [(label_name, (batch_size, self.default_bucket_key))]
+            self.provide_data = [DataDesc(
-            self.provide_label = [(label_name, (self.default_bucket_key, batch_size))]
+            self.provide_data = [DataDesc(
-                         provide_label=[(self.label_name, label.shape)])
+                         provide_data=[DataDesc(
-    'resnt-101' : {
+    'resnet-101' : {
-# gradients to be processed ealier. This reduces communication overhead, especially with
+# When training a deep, complex model *on multiple GPUs* it's recommended to
-            super(MApMetric, self).__init__("mAP")
+            self.num = None
-            assert isinstance(class_names, list)
+            assert isinstance(class_names, (list, tuple))
-        self.counts = dict()
+            self.name = class_names + ['mAP']
-        super(MApMetric, self).reset()
+        if getattr(self, 'num', None) is None:
-        super(MultiBoxMetric, self).__init__(['CrossEntropy', 'SmoothL1'], 2)
+        super(MultiBoxMetric, self).__init__('MultiBox')
-        btn += 'onclick="window.location=\'%s\'"><span class="glyphicon glyphicon-download-alt"></span> %s </button>\n' % (f, f)
+        btn += '<div class="download_btn"><a href="%s" download="%s">' \
-            btn += 'onclick="window.location=\'%s\'"><span class="glyphicon glyphicon-download-alt"></span> %s </button>\n' % (f, f)
+        f = ipynb.split('/')[-1]
-            c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()]),
+            c_array(ctypes.c_char_p, [c_str(key) for key in kwargs]),
-            keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()])
+            keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs])
-                       [c_str(key) for key in kwargs.keys()])
+                       [c_str(key) for key in kwargs])
-    def _init_weight(self, desc, arr):
+    def _init_weight(self, desc, arr): # pylint: disable=arguments-differ
-    def __new__(cls, name, shape, dtype=mx_real_t, layout='NCHW'):
+    def __new__(cls, name, shape, dtype=mx_real_t, layout='NCHW'): # pylint: disable=super-on-old-class
-    def update_dict(self, labels, preds):
+    def update_dict(self, labels, preds): # pylint: disable=arguments-differ
-            if kvstore is 'local':
+            if kvstore == 'local':
-        for name in ndarg_names:
+        for name in ndarg_names: # pylint: disable=redefined-argument-from-local
-        for name in kwarg_names:
+        for name in kwarg_names: # pylint: disable=redefined-argument-from-local
-    def begin_state(self, **kwargs):
+    def begin_state(self, **kwargs): # pylint: disable=arguments-differ
-    def begin_state(self, init_sym=symbol.zeros, **kwargs):
+    def begin_state(self, init_sym=symbol.zeros, **kwargs): # pylint: disable=arguments-differ
-    def begin_state(self, **kwargs):
+    def begin_state(self, **kwargs): # pylint: disable=arguments-differ
-            keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()])
+            keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs])
-        for name in ndarg_names:
+        for name in ndarg_names: # pylint: disable=redefined-argument-from-local
-        for name in kwarg_names:
+        for name in kwarg_names: # pylint: disable=redefined-argument-from-local
-    if data_names != actual:
+    if sorted(data_names) != sorted(actual):
-            print("simple_bind error. Arguments:")
+        except MXNetError as e:
-            raise RuntimeError('simple_bind failed')
+                error_msg += "%s: %s\n" % (k, v)
-    params : RNNParams or None, optional
+    params : RNNParams, default None.
-            number of steps to unroll
+            Number of steps to unroll.
-        begin_state : nested list of Symbol, optional
+        begin_state : nested list of Symbol, default None
-            If None, output whatever is faster
+            If None, output whatever is faster.
-        number of units in output symbol
+        Number of units in output symbol.
-        type of activation function
+        Type of activation function. Options are 'relu' and 'tanh'.
-        created if None.
+        Prefix for name of layers (and name of weight if params is None).
-        number of units in output symbol
+        Number of units in output symbol.
-        created if None.
+        Prefix for name of layers (and name of weight if params is None).
-        number of units in output symbol
+        Number of units in output symbol.
-        created if None.
+        Prefix for name of layers (and name of weight if params is None).
-        created if None.
+    params : RNNParams, default None
-        cell : rnn cell
+        cell : BaseRNNCell
-        percentage of elements to drop out, which
+        Percentage of elements to drop out, which
-    """Apply Zoneout on base cell."""
+    """Apply Zoneout on base cell.
-    Adds residual connection as described in Wu et al, 2016
+    """Adds residual connection as described in Wu et al, 2016
-            beta = layer_blobs[1].data
+            gamma = np.array(layer_blobs[0].data)
-            rescale_factor = layer_blobs[2].data
+            mean = np.array(layer_blobs[0].data)
-    assert acc[1].get()[1] > meta_info['top-5-acc'] - 0.3
+    assert acc[0].get()[1] > meta_info['top-1-acc'] - 0.03
-    batch_size = 32 * len(gpus)
+    """Entrypoint for test_converter"""
-    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype)
+    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, **kwargs)
-    return _internal._ones(shape=shape, ctx=ctx, dtype=dtype)
+    return _internal._ones(shape=shape, ctx=ctx, dtype=dtype, **kwargs)
-def full(shape, val, ctx=None, dtype=mx_real_t):
+def full(shape, val, ctx=None, dtype=mx_real_t, out=None):
-    return arr
+    out = empty(shape, ctx, dtype) if out is None else out
-
+        This function utilizes simple_bind python interface.
-        # Get the total bytes allocated for this executor
+        executor = self.symbol.simple_bind(ctx=context, grad_req=self.grad_req,
-from .base import c_array, c_str, mx_uint, py_str, string_types, mx_real_t
+from .base import c_array, c_str, mx_uint, py_str, string_types
-from .base import check_call, MXNetError, _Null  # pylint: disable=unused-import
+from .base import check_call, MXNetError, _Null # pylint: disable=unused-import
-from .ndarray import NDArray, zeros as _nd_zeros, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
+from .ndarray import NDArray, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
-        return {py_str(pairs[i*2]): py_str(pairs[i*2+1]) for i in range(size.value)}
+        return {py_str(pairs[i * 2]): py_str(pairs[i * 2 + 1]) for i in range(size.value)}
-            val = py_str(pairs[i*2+1])
+            name, key = py_str(pairs[i * 2]).split('$')
-                        raise TypeError('Argument need to be one of '+str(_DTYPE_NP_TO_MX))
+                        raise TypeError('Argument need to be one of ' + str(_DTYPE_NP_TO_MX))
-                        unknowns.append('%s: %s'%(name, str(shape)))
+                        unknowns.append('%s: %s' % (name, str(shape)))
-            # pylint: enable=too-many-locals
+        # pylint: enable=too-many-locals
-        """Binds current symbol to get an executor, allocate all the arguments needed.
+    def simple_bind(self, ctx, grad_req='write', type_dict=None, group2ctx=None,
-        >>> exe = y.simple_bind(mx.cpu(), x=(5,4), grad_req=[])
+        >>> exe = y.simple_bind(mx.cpu(), x=(5,4), grad_req='null')
-
+        num_provided_arg_types = 0
-                    grad_ndarrays[name] = _nd_zeros(shape, dev, dtype=dtype)
+            ctx_map_keys = []
-            grad_ndarrays = None
+            if not isinstance(shared_buffer, dict):
-                             group2ctx=group2ctx)
+        try:
-
+
-    exe = y.simple_bind(mx.cpu(), x=(5,4), grad_req=[])
+    exe = y.simple_bind(mx.cpu(), x=(5,4), grad_req='null')
-    eprocessor = ExecutePreprocessor(timeout=900)
+    eprocessor = ExecutePreprocessor(timeout=1800)
-    kwargs = {'install_requires': ['numpy'], 'zip_safe': False}
+    kwargs = {'install_requires': ['numpy', 'requests', 'graphviz'], 'zip_safe': False}
-"""Symbolic configuration API."""
+"""NDArray configuration API."""
-from ..base import SymbolHandle, OpHandle
+from ..base import c_array, c_str, mx_uint
-from ..attribute import AttrScope
+from .common import CachedOp  # pylint: disable=unused-import
-            setattr(module_obj, function.__name__, function)
+def invoke(cached_op, args, name=None):
-def compute_gradient(outputs):
+
-    gradients: list of NDArray
+    out_grads: list of NDArray or None
-    check_call(_LIB.MXAutogradComputeGradient(
+    if out_grads is None:
-        c_array(NDArrayHandle, output_handles)))
+        c_array(NDArrayHandle, output_handles),
-        from ._ctypes.ndarray import NDArrayBase, _set_ndarray_class, _imperative_invoke
+        from ._ctypes.ndarray import NDArrayBase, _set_ndarray_class
-    def backward(self, out_grad=None):
+    def backward(self, out_grad=None, retain_graph=False):
-            c_array(NDArrayHandle, ograd_handles)))
+            c_array(NDArrayHandle, ograd_handles),
-            "but got %s"%str(type(i))
+            "but got %s"%str(i)
-        pass
+    _ = kwargs.pop('name', None)
-            "Argument {name} must have NDArray type, but got %s"%str(type({name}))
+            "Argument {name} must have NDArray type, but got %s"%str({name})
-from .base import check_call, MXNetError
+from .base import NDArrayHandle, ExecutorHandle, SymbolHandle, OpHandle
-        from ._ctypes.symbol import SymbolBase, _init_symbol_module
+        from ._ctypes.symbol import SymbolBase, _set_symbol_class
-        from ._cy3.symbol import SymbolBase, _init_symbol_module
+        from ._cy3.symbol import SymbolBase, _set_symbol_class
-        from ._cy2.symbol import SymbolBase, _init_symbol_module
+        from ._cy2.symbol import SymbolBase, _set_symbol_class
-    from ._ctypes.symbol import SymbolBase, _init_symbol_module
+    from ._ctypes.symbol import SymbolBase, _set_symbol_class
-
+
-    test_argnum()
+    import nose
-    test_default_init()
+    test_default_init()
-    sample_num = 200
+    sample_num = 500
-    
+
-    
+
-    test_sigmoid()
+    import nose
-    test_symbol_pickle()
+    import nose
-from __future__ import print_function
+from rcnn.logger import logger
-    print('class ---- [[x1, x2, y1, y2, confidence]]')
+    logger.info('---class---')
-            print(boxes)
+            logger.info('---%s---' % CLASSES[ind])
-        print('results saved to %s' % result_file)
+        logger.info('results saved to %s' % result_file)
-from __future__ import print_function
+from rcnn.logger import logger
-              'data %.4fs net %.4fs' % (t1, t2))
+        logger.info('generating %d/%d ' % (i + 1, imdb.num_images) +
-    print('wrote rpn proposals to {}'.format(rpn_file))
+    logger.info('wrote rpn proposals to %s' % rpn_file)
-        print('testing {}/{} data {:.4f}s net {:.4f}s post {:.4f}s'.format(i, imdb.num_images, t1, t2, t3))
+        logger.info('testing %d/%d data %.4fs net %.4fs post %.4fs' % (i, imdb.num_images, t1, t2, t3))
-CUDA = locate_cuda()
+
-    ),
+if CUDA is not None:
-from __future__ import print_function
+from ..logger import logger
-        print('num_images', self.num_images)
+        logger.info('%s num_images %d' % (self.name, self.num_images))
-            print('{} gt roidb loaded from {}'.format(self.name, cache_file))
+            logger.info('%s gt roidb loaded from %s' % (self.name, cache_file))
-        print('wrote gt roidb to {}'.format(cache_file))
+        logger.info('%s wrote gt roidb to %s' % (self.name, cache_file))
-            print('Collecting %s results (%d/%d)' % (cls, cls_ind, self.num_classes - 1))
+            logger.info('collecting %s results (%d/%d)' % (cls, cls_ind, self.num_classes - 1))
-        print('Writing results json to %s' % res_file)
+        logger.info('writing results json to %s' % res_file)
-        print('coco eval results saved to %s' % eval_file)
+        logger.info('eval results saved to %s' % eval_file)
-        print('%-15s %5.1f' % ('all', 100 * ap_default))
+        logger.info('~~~~ Mean and per-category AP @ IoU=%.2f,%.2f] ~~~~' % (IoU_lo_thresh, IoU_hi_thresh))
-            print('%-15s %5.1f' % (cls, 100 * ap))
+            logger.info('%-15s %5.1f' % (cls, 100 * ap))
-        print('~~~~ Summary metrics ~~~~')
+        logger.info('~~~~ Summary metrics ~~~~')
-from __future__ import print_function
+from ..logger import logger
-        assert os.path.exists(rpn_file), 'rpn data not found at {}'.format(rpn_file)
+        assert os.path.exists(rpn_file), '%s rpn data not found at %s' % (self.name, rpn_file)
-            print('appending ground truth annotations')
+            logger.info('%s appending ground truth annotations' % self.name)
-        print('append flipped images to roidb')
+        logger.info('%s append flipped images to roidb' % self.name)
-        print('average number of proposal', total_counts / self.num_images)
+            logger.info('percentage of %s is %f' % (area_name, area_count / total_counts))
-from __future__ import print_function
+from ..logger import logger
-        print('num_images', self.num_images)
+        logger.info('%s num_images %d' % (self.name, self.num_images))
-            print('{} gt roidb loaded from {}'.format(self.name, cache_file))
+            logger.info('%s gt roidb loaded from %s' % (self.name, cache_file))
-        print('wrote gt roidb to {}'.format(cache_file))
+        logger.info('%s wrote gt roidb to %s' % (self.name, cache_file))
-            print('{} ss roidb loaded from {}'.format(self.name, cache_file))
+            logger.info('%s ss roidb loaded from %s' % (self.name, cache_file))
-            print('appending ground truth annotations')
+            logger.info('%s appending ground truth annotations' % self.name)
-        print('wrote ss roidb to {}'.format(cache_file))
+        logger.info('%s wrote ss roidb to %s' % (self.name, cache_file))
-            print('Writing {} VOC results file'.format(cls))
+            logger.info('Writing %s VOC results file' % cls)
-        print('VOC07 metric? ' + ('Y' if use_07_metric else 'No'))
+        logger.info('VOC07 metric? ' + ('Y' if use_07_metric else 'No'))
-        print('Mean AP = {:.4f}'.format(np.mean(aps)))
+            logger.info('AP for {} = {:.4f}'.format(cls, ap))
-from __future__ import print_function
+from ..logger import logger
-        print('saving annotations cache to {:s}'.format(annocache))
+                logger.info('reading annotations for %d/%d' % (ind + 1, len(image_filenames)))
-from __future__ import print_function
+import logging
-        print('gt_boxes', gt_boxes)
+    logger.debug('anchors: %s' % base_anchors)
-        print('inds_inside', len(inds_inside))
+    logger.debug('total_anchors %d' % total_anchors)
-        print('anchors shape', anchors.shape)
+    logger.debug('anchors shape %s' % np.array(anchors.shape))
-        if DEBUG:
+        if logger.level == logging.INFO:
-        if DEBUG:
+        if logger.level == logging.INFO:
-    if DEBUG:
+    if logger.level == logging.DEBUG:
-        print('stdevs', stds)
+        logger.debug('means %s' % means)
-        print('rpn: num_negatives', np.sum(labels == 0))
+    if logger.level == logging.DEBUG:
-        print('rpn: num_negative avg', _bg_sum / _count)
+        logger.debug('rpn: num_positive avg %f' % (_fg_sum / _count))
-from __future__ import print_function
+from ..logger import logger
-        print('bbox regression: this should not happen')
+        logger.warning('bbox regression: len(rois) != len(overlaps)')
-        print('something wrong : zero ground truth rois')
+        logger.warning('bbox regression: len(gt_inds) == 0')
-    print('add bounding box regression targets')
+    logger.info('bbox regression: add bounding box regression targets')
-from ..cython.gpu_nms import gpu_nms
+try:
-    return _nms
+    if gpu_nms is not None:
-__version__ = '1.0.1'
+__version__ = '2.0'
-#  segToMask  - Convert polygon segmentation to binary mask.
+#  annToMask  - Convert segmentation in an annotation to binary mask.
-# COCO>loadImgs, COCO>segToMask, COCO>showAnns
+# COCO>loadImgs, COCO>annToMask, COCO>showAnns
-import mask
+from . import mask as maskUtils
-        if annotation_file is not None:
+        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()
-            print('Done (t=%0.2fs)'%(time.time()- tic))
+            assert type(dataset)==dict, 'annotation file format {} not supported'.format(type(dataset))
-        imgs = {}
+        anns, cats, imgs = {}, {}, {}
-                imgToAnns[ann['image_id']] += [ann]
+                imgToAnns[ann['image_id']].append(ann)
-            catToImgs = {cat['id']: [] for cat in self.dataset['categories']}
+
-                catToImgs[ann['category_id']] += [ann['image_id']]
+                catToImgs[ann['category_id']].append(ann['image_id'])
-            print('%s: %s'%(key, value))
+            print('{}: {}'.format(key, value))
-        if iscrowd is not None:
+        if not iscrowd == None:
-        if 'segmentation' in anns[0]:
+        if 'segmentation' in anns[0] or 'keypoints' in anns[0]:
-                        rle = mask.frPyObjects([ann['segmentation']], t['height'], t['width'])
+                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]
-            p = PatchCollection(polygons, facecolors=color, edgecolors=(0,0,0,1), linewidths=3, alpha=0.4)
+                        # mask
-        print('Loading and preparing results...     ')
+        print('Loading and preparing results...')
-        anns    = json.load(open(resFile))
+        if type(resFile) == str or type(resFile) == unicode:
-                ann['area'] = mask.area([ann['segmentation']])[0]
+                ann['area'] = maskUtils.area(ann['segmentation'])
-                    ann['bbox'] = mask.toBbox([ann['segmentation']])[0]
+                    ann['bbox'] = maskUtils.toBbox(ann['segmentation'])
-        print('DONE (t=%0.2fs)'%(time.time()- tic))
+        elif 'keypoints' in anns[0]:
-    def download(self, tarDir=None, imgIds=[]):
+    def download(self, tarDir = None, imgIds = [] ):
-            print('downloaded %d/%d images (t=%.1fs)'%(i, N, time.time()- tic))
+                urlretrieve(img['coco_url'], fname)
-import mask
+import mask as maskUtils
-    #  useCats    - [1] if true use category labels for evaluation    # Note: if useSegm=0 the evaluation is run on bounding boxes.
+    #  iouType    - ['segm'] set iouType to 'segm', 'bbox' or 'keypoints'
-    def __init__(self, cocoGt=None, cocoDt=None):
+    def __init__(self, cocoGt=None, cocoDt=None, iouType='segm'):
-        self.params = Params()              # parameters
+        self.params = Params(iouType=iouType) # parameters
-                    raise Exception('segmentation format not supported.')
+        def _toMask(anns, coco):
-        if p.useSegm:
+        # convert ground truth to mask if iouType == 'segm'
-        print('Running per image evaluation...      ')
+        print('Running per image evaluation...')
-        computeIoU = self.computeIoU
+        if p.iouType == 'segm' or p.iouType == 'bbox':
-        print('DONE (t=%0.2fs).'%(toc-tic))
+        print('DONE (t={:0.2f}s).'.format(toc-tic))
-        dt = sorted(dt, key=lambda x: -x['score'])
+        inds = np.argsort([-d['score'] for d in dt], kind='mergesort')
-        if p.useSegm:
+        if p.iouType == 'segm':
-        else:
+        elif p.iouType == 'bbox':
-        ious = mask.iou(d,g,iscrowd)
+        ious = maskUtils.iou(d,g,iscrowd)
-            if g['iscrowd'] == 1 or g['ignore'] or (g['area']<aRng[0] or g['area']>aRng[1]):
+            if g['ignore'] or (g['area']<aRng[0] or g['area']>aRng[1]):
-        dt = sorted(dt, key=lambda x: -x['score'])[0:maxDet]
+        gtind = np.argsort([g['_ignore'] for g in gt], kind='mergesort')
-        ious = self.ious[imgId, catId][0:maxDet, np.array(gtind)] if N_iou >0 else self.ious[imgId, catId]
+        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]
-                        # match successful and best so far, store appropriately
+                        # if match successful and best so far, store appropriately
-        print('Accumulating evaluation results...   ')
+        print('Accumulating evaluation results...')
-                    E = filter(None, E)
+                    E = [self.evalImgs[Nk + Na + i] for i in i_list]
-                    npig = len([ig for ig in gtIg if ig == 0])
+                    gtIg = np.concatenate([e['gtIgnore'] for e in E])
-                        inds = np.searchsorted(rc, p.recThrs)
+                        inds = np.searchsorted(rc, p.recThrs, side='left')
-            'date': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
+            'date': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
-        print('DONE (t=%0.2fs).'%( toc-tic ))
+        print('DONE (t={:0.2f}s).'.format( toc-tic))
-            mind = [i for i, mDet in enumerate([1, 10, 100]) if mDet == maxDets]
+            iStr = ' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}'
-                # areaRng
+                if iouThr is not None:
-            print(iStr.format(titleStr, typeStr, iouStr, areaStr, maxDetsStr, '%.3f'%(float(mean_s))))
+            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))
-
+        def _summarizeDets():
-        self.stats[11] = _summarize(0,areaRng='large')
+        iouType = self.params.iouType
-    def __init__(self):
+    def setDetParams(self):
-        self.useCats = 1
+        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)
-import _mask as _mask
+import _mask
-toBbox      = _mask.toBbox
+
-from __future__ import print_function
+from rcnn.logger import logger
-            print(self._anchors)
+        logger.debug('feat_stride: %s' % self._feat_stride)
-            print('scale: {}'.format(im_info[2]))
+        logger.debug('im_info: %s' % im_info)
-            print("resudial: {}".format((scores.shape[2] - height, scores.shape[3] - width)))
+        logger.debug('score map size: (%d, %d)' % (scores.shape[2], scores.shape[3]))
-from __future__ import print_function
+import logging
-        if DEBUG:
+        if logger.level == logging.DEBUG:
-            print('num bg: {}'.format((labels == 0).sum()))
+        if logger.level == logging.DEBUG:
-            print('ratio: {:.3f}'.format(float(self._fg_num) / float(self._bg_num)))
+            logger.debug("self._count: %d" % self._count)
-from __future__ import print_function
+from ..logger import logger
-    print('Called with argument:', args)
+    logger.info('Called with argument: %s' % args)
-from __future__ import print_function
+from ..logger import logger
-from __future__ import print_function
+from ..logger import logger
-    print('Called with argument:', args)
+    logger.info('Called with argument: %s' % args)
-import logging
+from ..logger import logger
-    pprint.pprint(config)
+    logger.info(pprint.pformat(config))
-    pprint.pprint(out_shape_dict)
+    logger.info('output shape %s' % pprint.pformat(out_shape_dict))
-    print('lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters)
+    logger.info('lr %f lr_epoch_diff %s lr_iters %s' % (lr, lr_epoch_diff, lr_iters))
-    print('Called with argument:', args)
+    logger.info('Called with argument: %s' % args)
-import logging
+from ..logger import logger
-    pprint.pprint(config)
+    logger.info(pprint.pformat(config))
-    print('providing maximum shape', max_data_shape, max_label_shape)
+    logger.info('providing maximum shape %s %s' % (max_data_shape, max_label_shape))
-    pprint.pprint(out_shape_dict)
+    logger.info('output shape %s' % pprint.pformat(out_shape_dict))
-    print('lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters)
+    logger.info('lr %f lr_epoch_diff %s lr_iters %s' % (lr, lr_epoch_diff, lr_iters))
-    print('Called with argument:', args)
+    logger.info('Called with argument: %s' % args)
-from __future__ import print_function
+from ..logger import logger
-    print('filtered %d roidb entries: %d -> %d' % (num - num_after, num, num_after))
+    logger.info('load data: filtered %d roidb entries: %d -> %d' % (num - num_after, num, num_after))
-from __future__ import print_function
+from rcnn.logger import logger
-
+from rcnn.logger import logger
-    logging.info('########## TRAIN RPN WITH IMAGENET INIT')
+    logger.info('########## TRAIN RPN WITH IMAGENET INIT')
-    logging.info('########## GENERATE RPN DETECTION')
+    logger.info('########## GENERATE RPN DETECTION')
-    logging.info('########## TRAIN RCNN WITH IMAGENET INIT AND RPN DETECTION')
+    logger.info('########## TRAIN RCNN WITH IMAGENET INIT AND RPN DETECTION')
-    logging.info('########## TRAIN RPN WITH RCNN INIT')
+    logger.info('########## TRAIN RPN WITH RCNN INIT')
-    logging.info('########## GENERATE RPN DETECTION')
+    logger.info('########## GENERATE RPN DETECTION')
-    print('Called with argument:', args)
+    logger.info('Called with argument: %s' % args)
-import logging
+from rcnn.logger import logger
-    pprint.pprint(config)
+    logger.info(pprint.pformat(config))
-    print('providing maximum shape', max_data_shape, max_label_shape)
+    logger.info('providing maximum shape %s %s' % (max_data_shape, max_label_shape))
-    pprint.pprint(out_shape_dict)
+    logger.info('output shape %s' % pprint.pformat(out_shape_dict))
-    print('lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters)
+    logger.info('lr %f lr_epoch_diff %s lr_iters %s' % (lr, lr_epoch_diff, lr_iters))
-    print('Called with argument:', args)
+    logger.info('Called with argument: %s' % args)
-__version__ = "0.10.0"
+__version__ = "0.10.1"
-            with open(fullpath) as fin:
+            with open(fullpath, 'rb') as fin:
-__version__ = "0.9.5"
+__version__ = "0.10.0"
-        self._output_prefix = output_prefix
+def test_residual_fused():
-    print(outputs.list_arguments())
+def test_residual_bidirectional():
-                out.append(l)
+                if '%matplotlib' not in l:
-    NB_TESTER.run_test()
+#pylint: disable=no-member, too-many-locals, too-many-branches, no-self-use, broad-except, lost-exception, too-many-nested-blocks, too-few-public-methods, invalid-name
-            btn += 'onclick="window.location=\'%s\'"><span class="glyphicon glyphicon-download-alt"></span> %s </a></button>\n' % (f, f)
+            btn += 'onclick="window.location=\'%s\'"><span class="glyphicon glyphicon-download-alt"></span> %s </button>\n' % (f, f)
-                    '<!--' in l or '-->' in l):
+                    '<!--' in l or '-->' in l or
-    handle : DataIterHandle
+    handle : DataIterHandle, required
-        ``header.label`` can be a number or an array.
+        ``header.label`` can be a number or an array. See more detail in ``IRHeader``.
-        string to pack
+        Raw image string to be packed.
-        image format option for ``cv2.imdecode``.
+        Image format option for ``cv2.imdecode``.
-        ``header.label`` can be a number or an array.
+        ``header.label`` can be a number or an array. See more detail in ``IRHeader``.
-        image to pack
+        Image to be packed.
-       Upsample result if `src` is smaller than `size`.
+    Upsample result if `src` is smaller than `size`.
-         When shrinking an image, it will generally look best with AREA-based
+        When shrinking an image, it will generally look best with AREA-based
-    for sequential data, by default `layout` is set to ``NTC`` where
+    For sequential data, by default `layout` is set to ``NTC``, where
-        """Get a string representation of the symbol."""
+        """Gets a string representation of the symbol."""
-        """Get name string from the symbol, this function only works for non-grouped symbol.
+        """Gets name string from the symbol, this function only works for non-grouped symbol.
-        """Get the autodiff of current symbol.
+        """Gets the autodiff of current symbol.
-    """Given the "legs" of a right triangle, return its hypotenuse.
+    """Given the "legs" of a right triangle, returns its hypotenuse.
-    Equivalent to "sqrt(left**2 + right**2)", element-wise.
+    Equivalent to :math:`\\sqrt(left^2 + right^2)`, element-wise.
-    """Return a new symbol of given shape and type, filled with zeros.
+    """Returns a new symbol of given shape and type, filled with zeros.
-    """Return a new symbol of given shape and type, filled with ones.
+    """Returns a new symbol of given shape and type, filled with ones.
-    """Return evenly spaced values within a given interval.
+    """Returns evenly spaced values within a given interval.
-            btn += '<a href="%s"><span class="glyphicon glyphicon-download-alt"></span> %s </a></button>\n' % (f, f)
+            btn += '<button type="button" class="btn btn-default download" '
-        The initial number of updates
+        The initial number of updates.
-        """Register a new optimizer.
+        """Registers a new optimizer.
-        """Instantiate an optimizer with a given name and kwargs.
+        """Instantiates an optimizer with a given name and kwargs.
-        We can use the alias `create` for ``Optimizer.create_optimizer``
+        .. note:: We can use the alias `create` for ``Optimizer.create_optimizer``.
-        """Create auxiliary state for a given weight
+        """Creates auxiliary state for a given weight.
-        """[DEPRECATED] set lr scale. Use set_lr_mult instead."""
+        """[DEPRECATED] Sets lr scale. Use set_lr_mult instead."""
-        """Update num_update
+        """Updates num_update.
-        Parameters:
+        Parameters
-        """Get the learning rate given the index of the weight.
+        """Gets the learning rate given the index of the weight.
-        """get weight decay for index.
+        """Gets weight decay for index.
-    by :class:`.Optimizer`:
+    by :class:`.Optimizer`.
-    """The DCASGD optimizer
+    """The DCASGD optimizer.
-    Delay Compensation for Distributed Deep Learning*, available at https://arxiv.org/abs/1609.08326
+    This class implements the optimizer described in *Asynchronous Stochastic Gradient Descent
-    by :class:`.Optimizer`:
+    by :class:`.Optimizer`.
-                      * grad * grad * (weight - previous_weight))
+                             * grad * grad * (weight - previous_weight))
-                      * grad * grad * (weight - previous_weight))
+                         * grad * grad * (weight - previous_weight))
-    This optimizer updates each weight by:
+    This optimizer updates each weight by::
-    https://papers.nips.cc/paper/4883-stochastic-gradient-riemannian-langevin-dynamics-on-the-probability-simplex.pdf
+    https://papers.nips.cc/paper/4883-stochastic-gradient-riemannian-langevin-dynamics-on-the-probability-simplex.pdf.
-    """[Deprecated] Same as sgd. Left here for backward compatibility."""
+    """[DEPRECATED] Same as `SGD`. Left here for backward compatibility."""
-    Stochastic Optimization*, available at http://arxiv.org/abs/1412.6980
+    Stochastic Optimization*, available at http://arxiv.org/abs/1412.6980.
-    by :class:`.Optimizer`:
+    by :class:`.Optimizer`.
-    """AdaGrad optimizer
+    """AdaGrad optimizer.
-    http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf
+    http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf.
-    by :class:`.Optimizer`:
+    by :class:`.Optimizer`.
-    For details of the update algorithm see :class:`~mxnet.ndarray.rmsprop_update`
+    For details of the update algorithm see :class:`~mxnet.ndarray.rmsprop_update`.
-    For details of the update algorithm see :class:`~mxnet.ndarray.rmspropalex_update`
+    For details of the update algorithm see :class:`~mxnet.ndarray.rmspropalex_update`.
-    by :class:`.Optimizer`:
+    by :class:`.Optimizer`.
-        Clips weights into range ``[-clip_weights, clip_weights]``
+        Clips weights into range ``[-clip_weights, clip_weights]``.
-    learning rate method*, available at https://arxiv.org/abs/1212.5701
+    learning rate method*, available at https://arxiv.org/abs/1212.5701.
-    by :class:`.Optimizer`:
+    by :class:`.Optimizer`.
-    Reference:Ad Click Prediction: a View from the Trenches
+    """The Ftrl optimizer.
-
+    eta :
-        # accumulated g and delta initlization
+        # accumulated g and delta initialization
-            ((self.beta + sqrt(n)) / lr + wd) * (NDArray.abs(dn) > self.lamda1)
+                    ((self.beta + sqrt(n)) / lr + wd) * (NDArray.abs(dn) > self.lamda1)
-        """Create a state to duplicate weight"""
+        """Creates a state to duplicate weight."""
-        """performs w += rescale_grad * grad"""
+        """Performs w += rescale_grad * grad."""
-        """Update weight given gradient and index."""
+        """Updates weight given gradient and index."""
-        """Set updater states."""
+        """Sets updater states."""
-        """Get updater states."""
+        """Gets updater states."""
-    """Return a clossure of the updater needed for kvstore.
+    """Returns a closure of the updater needed for kvstore.
-         The clossure of the updater.
+         The closure of the updater.
-    pad = 0
+    Convert convolution layer parameter from Caffe to MXNet
-        pad = 0 if len(param.pad) == 0 else param.pad[0]
+        if len(param.pad) > 0:
-        kernel_size = param.kernel_size[0]
+
-                    stride, stride, not param.bias_term)
+
-    for d_src, d_targets, axis in zip(data, targets, major_axis):
+    for d_src, d_targets, axis in zip(data, targets, major_axis): # pylint: disable=too-many-nested-blocks
-                    end[axis] = slice_idx.stop
+                    do_crop = (slice_idx.start != 0 or shape[axis] != slice_idx.stop)
-                        nd.crop(d_src, begin=tuple(begin), end=tuple(end), out=d_dst)
+                    if do_crop:
-                        d_dst_copy.copyto(d_dst)
+                        d_src.copyto(d_dst)
-    """Show a progress bar.
+    """Displays a progress bar, indicating the percentage of batches processed within each epoch.
-        total batch size
+        total number of batches per epoch
-        length or progress bar
+        number of chars to define maximum length of progress bar
-        sys.stdout.write('[%s] %s%s\r' % (prog_bar, percents, '%'))
+        logging.info('[%s] %s%s\r', prog_bar, percents, '%')
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-            raise ValueError("no experiments done on num_layers {}, you can do it youself".format(num_layers))
+            raise ValueError("no experiments done on num_layers {}, you can do it yourself".format(num_layers))
-    substract pixel size and transform to correct format
+    transform into mxnet tensor,
-    Continous Multi-Layer Perceptron Q-Value Network
+    Continuous Multi-Layer Perceptron Q-Value Network
-"""Read invidual image files and perform augmentations."""
+"""Read individual image files and perform augmentations."""
-        intializer applied to unpacked weights. Fall back to global
+        initializer applied to unpacked weights. Fall back to global
-        >>> # weight is updated via gradient descient
+        >>> # weight is updated via gradient descent
-    The F1 score is equvalent to weighted average of the precision and recall,
+    The F1 score is equivalent to weighted average of the precision and recall,
-    This calss implements the AdaGrad optiizer described in *Adaptive Subgradient
+    This class implements the AdaGrad optimizer described in *Adaptive Subgradient
-        >>> mod = mx.mod.Module(out)
+    >>> # An example of creating a mxnet module.
-            >>> mod.score(val_dataiter, ['mse', 'acc'])
+        >>> # An example of using score for prediction.
-        >>>        # batch is the data batch from the data iterator
+        ...     # pred is a list of outputs from the module
-            >>> mod.predict(eval_data=val_dataiter, num_batch=10)
+        >>> # An example of using `predict` for prediction.
-                        eval_metric='acc', num_epoch=10, begin_epoch=3)
+        >>> # An example of using fit for training.
-        """Get parameters, those are potentially copies of the the actual parameters used
+        """Gets parameters, those are potentially copies of the the actual parameters used
-            'fc2_bias': <NDArray 64 @cpu(0)>, 'fc1_bias': <NDArray 128 @cpu(0)>}, {})
+        >>> # An example of getting module parameters.
-        """Initialize the parameters and auxiliary states.
+        """Initializes the parameters and auxiliary states.
-            >>> mod.init_params()
+        >>> # An example of initializing module parameters.
-        """Assign parameter and aux state values.
+        """Assigns parameter and aux state values.
-            >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)
+        >>> # An example of setting module parameters.
-        """Save model parameters to file.
+        """Saves model parameters to file.
-            >>> mod.save_params('myfile')
+        >>> # An example of saving module parameters.
-        """Load model parameters from file.
+        """Loads model parameters from file.
-            >>> mod.load_params('myfile')
+        >>> # An example of loading module parameters.
-        """Get states from all devices
+        """Gets states from all devices
-        """Set value for states. Only one of states & value can be specified.
+        """Sets value for states. Only one of states & value can be specified.
-        """Install monitor on all executors."""
+        """Installs monitor on all executors."""
-        '''Prepare the module for processing a data batch.
+        '''Prepares the module for processing a data batch.
-               0.10000272  0.10000113  0.09999088  0.09999888]]
+        >>> # An example of forward computation.
-                 ...]]
+        >>> # An example of backward computation.
-        """Get outputs of the previous forward computation.
+        """Gets outputs of the previous forward computation.
-               0.10000272  0.10000113  0.09999088  0.09999888]]
+        >>> # An example of getting forward output.
-        """Get the gradients to the inputs, computed in the previous backward computation.
+        """Gets the gradients to the inputs, computed in the previous backward computation.
-                ...]]
+        >>> # An example of getting input gradients.
-                ...]]
+        >>> # An example of updating module parameters.
-            >>> mod.update_metric(metric, data_batch.label)
+        >>> # An example of updating evaluation metric.
-            >>> mod.bind(data_shapes=train_iter.provide_data, label_shapes=train_iter.provide_label)
+        >>> # An example of binding symbols.
-            >>> mod.init_optimizer(optimizer='sgd', optimizer_params=(('learning_rate', 0.005),))
+        >>> # An example of initializing optimizer.
-        """Get the symbol associated with this module.
+        """Gets the symbol associated with this module.
-        training (in this case, label information is not available).
+        A list of `(name, shape)` pairs.
-        """Get output shapes.
+        """Gets output shapes.
-        """Get current parameters.
+        """Gets current parameters.
-        (`NDArray`).
+        `(arg_params, aux_params)`
-        """Assign parameter and aux state values.
+        """Assigns parameters and aux state values.
-            >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)
+        >>> # An example of setting module parameters.
-        """Initialize parameters.
+        """Initializes parameters.
-        """Get states from all devices
+        """Gets states from all devices.
-        elements are `NDArray`.
+        list of NDArrays or list of list of NDArrays
-        """Set value for states. Only one of states & value can be specified.
+        """Sets value for states. Only one of states & values can be specified.
-        """Switch to a different bucket. This will change ``self.curr_module``.
+        """Switches to a different bucket. This will change ``self.curr_module``.
-        """Install and initialize optimizers.
+        """Installs and initializes optimizers.
-        '''Prepare a data batch for forward.
+        """Prepares a data batch for forward.
-        '''
+        """
-        """Update parameters according to installed optimizer and the gradient computed
+        """Updates parameters according to installed optimizer and the gradient computed
-        """Get outputs from a previous forward computation.
+        """Gets outputs from a previous forward computation.
-        elements are numpy arrays.
+        list of numpy arrays or list of list of numpy arrays
-        """Get the gradients with respect to the inputs of the module.
+        """Gets the gradients with respect to the inputs of the module.
-        elements are `NDArray`.
+        list of NDArrays or list of list of NDArrays
-        """Evaluate and accumulate evaluation metric on outputs of the last forward computation.
+        """Evaluates and accumulates evaluation metric on outputs of the last forward computation.
-        """ Install monitor on all executors """
+        """Installs monitor on all executors """
-        Instead they are initialized to 0 and can be set by set_states()
+        Instead they are initialized to 0 and can be set by `set_states()`.
-        """Create a model from previously saved checkpoint.
+        """Creates a model from previously saved checkpoint.
-        Use mx.callback.module_checkpoint as epoch_end_callback to save during training.
+        """Saves current progress to checkpoint.
-            The file prefix to checkpoint to
+            The file prefix to checkpoint to.
-            The current epoch number
+            The current epoch number.
-            Whether to save optimizer states for continue training
+            Whether to save optimizer states to continue training.
-        """Get data shapes.
+        """Gets data shapes.
-        """Get label shapes.
+        """Gets label shapes.
-            A list of `(name, shape)` pairs. The return value could be ``None`` if
+        A list of `(name, shape)` pairs.
-        """Get output shapes.
+        """Gets output shapes.
-        """Get current parameters.
+        """Gets current parameters.
-        `NDArray`) mapping.
+        `(arg_params, aux_params)`
-        """Initialize the parameters and auxiliary states.
+        """Initializes the parameters and auxiliary states.
-            If not None, should be a dictionary of existing arg_params. Initialization
+            If not ``None``, should be a dictionary of existing arg_params. Initialization
-        """Assign parameter and aux state values.
+        """Assigns parameter and aux state values.
-            >>> mod.set_params(arg_params=arg_params, aux_params=aux_params)
+        >>> # An example of setting module parameters.
-        """Bind the symbols to construct executors. This is necessary before one
+        """Binds the symbols to construct executors. This is necessary before one
-        """Reshape the module for new input shapes.
+        """Reshapes the module for new input shapes.
-        """ Installs monitor on all executors. """
+        """Installs monitor on all executors. """
-        to do computation on the device.
+        """Gets parameters, those are potentially copies of the the actual parameters used
-        contains parameters.
+        ``({}, {})``, a pair of empty dict.
-        """Initialize the parameters and auxiliary states. By default this function
+        """Initializes the parameters and auxiliary states. By default this function
-        """Update parameters according to the installed optimizer and the gradients computed
+        """Updates parameters according to the installed optimizer and the gradients computed
-        ubclass should override this method if needed.
+        """Evaluates and accumulates evaluation metric on outputs of the last forward computation.
-        """Bind the symbols to construct executors. This is necessary before one
+        """Binds the symbols to construct executors. This is necessary before one
-        should
+        """Installs and initializes optimizers. By default we do nothing. Subclass should
-        """Compute the shapes of outputs. As a loss module with outputs, we simply
+        """Computes the shapes of outputs. As a loss module with outputs, we simply
-        """Get outputs of the previous forward computation. As a output loss module,
+        """Gets outputs of the previous forward computation. As a output loss module,
-        """Get the gradients to the inputs, computed in the previous backward computation.
+        """Gets the gradients to the inputs, computed in the previous backward computation.
-        """Install monitor on all executors."""
+        """Installs monitor on all executors."""
-    handy utility.
+    .. note::
-        """Add a module to the chain.
+        """Adds a module to the chain.
-            >>> seq_mod.add(mod2)
+        >>> # An example of addinging two modules to a chain.
-        """Get data shapes.
+        """Gets data shapes.
-        """Get label shapes.
+        """Gets label shapes.
-        """Get output shapes.
+        """Gets output shapes.
-        """Get current parameters.
+        """Gets current parameters.
-            each a dictionary of name to parameters (in `NDArray`) mapping. This
+            A pair of dictionaries each mapping parameter names to NDArray values. This
-        """Initialize parameters.
+        """Initializes parameters.
-        """Bind the symbols to construct executors. This is necessary before one
+        """Binds the symbols to construct executors. This is necessary before one
-        """Install and initialize optimizers.
+        """Installs and initializes optimizers.
-        """Update parameters according to installed optimizer and the gradient computed
+        """Updates parameters according to installed optimizer and the gradient computed
-        """Get outputs from a previous forward computation.
+        """Gets outputs from a previous forward computation.
-        """Get the gradients with respect to the inputs of the module.
+        """Gets the gradients with respect to the inputs of the module.
-        list of NDArray or list of list of NDArray
+        list of NDArrays or list of list of NDArrays
-        """Evaluate and accumulate evaluation metric on outputs of the last forward computation.
+        """Evaluates and accumulates evaluation metric on outputs of the last forward computation.
-        """ Install monitor on all executors."""
+        """Installs monitor on all executors."""
-        return self._dataframes['train']
+        return self._dataframes['eval']
-        return self._dataframes['train']
+        return self._dataframes['epoch']
-        Key for invalid token. Use '\n' for end
+    invalid_key : str, default '\\n'
-    cells : RNNCells or list of RNNCells
+    cells : RNNCell or list of RNNCells
-    cells : RNNCells or list of RNNCells
+    cells : RNNCell or list of RNNCells
-        RNN cells used by this module.
+    cells : RNNCell or list of RNNCells
-        """Reset before re-using the cell for another graph"""
+        """Reset before re-using the cell for another graph."""
-    """Simple recurrent neural network cell
+    """Simple recurrent neural network cell.
-    """Sequantially stacking multiple RNN cells
+    """Sequantially stacking multiple RNN cells.
-    """Apply Zoneout on base cell"""
+    """Apply Zoneout on base cell."""
-    """Bidirectional RNN cell
+    """Bidirectional RNN cell.
-                         ret_type)
+_ndarray_cls = None
-    signature = ndsignature + signature
+def _set_ndarray_class(cls):
-    global handle
+def _imperative_invoke(handle, ndargs, keys, vals, out):
-        ctypes.c_void_p(%d),
+        ctypes.c_void_p(handle),
-        c_array(NDArrayHandle, ndargs),
+        c_array(NDArrayHandle, [arr.handle for arr in ndargs]),
-        c_array(ctypes.c_char_p, [c_str(val) for val in vals])))
+        c_array(ctypes.c_char_p, [c_str(str(val)) for val in vals])))
-from .base import mx_uint, NDArrayHandle, check_call
+from .base import c_array, py_str, c_str, mx_real_t, _Null  # pylint: disable=unused-import
-        from ._ctypes.ndarray import NDArrayBase, _init_ndarray_module
+        from ._ctypes.ndarray import NDArrayBase, _set_ndarray_class, _imperative_invoke
-        from ._cy3.ndarray import NDArrayBase, _init_ndarray_module
+        from ._cy3.ndarray import NDArrayBase, _set_ndarray_class, _imperative_invoke
-        from ._cy2.ndarray import NDArrayBase, _init_ndarray_module
+        from ._cy2.ndarray import NDArrayBase, _set_ndarray_class, _imperative_invoke
-
+    from ._ctypes.ndarray import NDArrayBase, _set_ndarray_class, _imperative_invoke
-_init_ndarray_module(NDArray, "mxnet")
+    def detach(self):
-    return [] # disable cython due to some users have compile errors.
+    if not with_cython:
-      ext_modules=config_cython())
+      ext_modules=config_cython(),
-# pylint: disable=invalid-name, no-member, too-many-arguments, too-many-locals, too-many-branches, too-many-statements, broad-except, line-too-long, unused-import
+# pylint: disable=too-many-lines
-from .context import cpu, gpu, Context
+from .context import Context
-    >>> _parse_aux_states(fc2, {'batchnorm0_moving_var': mean_states, 'batchnorm0_moving_mean': var_states}, None)
+    >>> _parse_aux_states(fc2, {'batchnorm0_moving_var': mean_states,
-            except Exception as e:
+            except AssertionError as e:
-                except Exception as e:
+                except AssertionError as e:
-        information of the data or the label.
+    information of the data or the label.
-        C is number of channels, H is the height and W is the width of the image.
+    The `layout` describes how the axes in `shape` should be interpreted,
-         and C is the number of channels.
+    for sequential data, by default `layout` is set to ``NTC`` where
-   See mxnet.recordio.pack and mxnet.recordio.pack_img for example uses.
+See mxnet.recordio.pack and mxnet.recordio.pack_img for example uses.
-    """Data description
+    """DataDesc is used to store name, shape, type and layout
-See mxnet.recordio.pack and mxnet.recordio.pack_img for example uses.
+   See mxnet.recordio.pack and mxnet.recordio.pack_img for example uses.
-    Higher order bits of the unique id, should be set to 0 (in most cases).
+    flag : int
-        """Gets a debug string.
+        """Gets a debug string of symbol.
-        debug_str : string
+        string
-    """ Raise base to an exp.
+    """Returns element-wise result of base element raised to powers from exp element.
-    exp: Symbol or Number
+    base : Symbol or scalar
-    result: Symbol or Number
+    Symbol or scalar
-    """ maximum left and right
+    """Returns element-wise maximum of the input elements.
-    right: Symbol or Number
+    left : Symbol or scalar
-    result: Symbol or Number
+    Symbol or scalar
-    """ minimum left and right
+    """Returns element-wise minimum of the input elements.
-    right: Symbol or Number
+    left : Symbol or scalar
-    result: Symbol or Number
+    Symbol or scalar
-        return left if left > right else right
+        return left if left < right else right
-    """ minimum left and right
+    """Given the "legs" of a right triangle, return its hypotenuse.
-    right: Symbol or Number
+    left : Symbol or scalar
-    result: Symbol or Number
+    Symbol or scalar
-    """Centrally crop src with size. Upsample result if src is smaller than size."""
+    """Crops the image `src` to the given `size` by trimming on all four
-    ...    batch.data[0].shape
+    ...     print batch.data[0].asnumpy()
-    ...    batchidx += 1
+    ...     batchidx += 1
-    ...    batchidx += 1
+    ...     batchidx += 1
-        be prepended with prefix
+        Names of all variables created by this container will
-        """Get a variable with name or create a new one if missing.
+        """Get the variable given a name if one exists or create a new one if missing.
-        created if None.
+    prefix : str, optional
-        """Construct symbol for one step of RNN.
+        """Unroll the RNN for one time step.
-            state from previous step or begin_state().
+        states : list of sym.Variable
-            state to next step of RNN.
+            Symbol corresponding to the output from the RNN when unrolling
-            starting states for first RNN step
+            Starting states for the first RNN step.
-        weight matrices
+        weight matrices.
-            usually from Module.get_output()
+            Dictionary containing packed weights.
-            this cell unpacked.
+            Dictionary with unpacked weights associated with
-        """Pack separate weight matrices into fused
+        """Pack separate weight matrices into a single packed
-            dictionary containing unpacked weights.
+            Dictionary containing unpacked weights.
-            this cell packed.
+            Dictionary with packed weights associated with
-            if inputs is a single Symbol (usually the output
+            If `inputs` is a single Symbol (usually the output
-            If inputs is a list of symbols (usually output of
+            If `inputs` is a list of symbols (usually output of
-            layout of input symbol. Only used if inputs
+        begin_state : nested list of Symbol, optional
-        merge_outputs : bool
+        merge_outputs : bool, optional
-            has the same structure as begin_state()
+        outputs : list of Symbol or Symbol
-        Data type of the array we want to convert to.
+        Data type of the array we want to convert to, such as mx_float.
-                             [3, 4]])
+        >>> x = mx.nd.array([[1, 2], [3, 4]])
-                         [4, 5, 6]])
+    >>> X = mx.nd.array([[1, 2, 3], [4, 5, 6]])
-    (3, 2)
+    (3L, 2L)
-          [ 1.]], dtype=float32)
+           [ 1.]], dtype=float32)
-          [ 1.]], dtype=float32)
+           [ 1.]], dtype=float32)
-          [ 1.]], dtype=float32)
+           [ 1.]], dtype=float32)
-          [ 1.]], dtype=float32)
+           [ 1.]], dtype=float32)
-          [ 1.]], dtype=float32)
+           [ 1.]], dtype=float32)
-    >>> y = mx.nd.reshape(x,shape=(2, 2))
+    >>> y = mx.nd.reshape(x, shape=(2, 2))
-def generate_doxygen_xml(app):
+def generate_doxygen(app):
-    # app.connect("builder-inited", generate_doxygen_xml)
+    app.connect("builder-inited", generate_doxygen)
-    """Parse the given location to a dictionary.
+    """Parses the given location to a dictionary.
-    location : ``None`` or list of ``np.ndarray`` or dict of str to ``np.ndarray``.
+        Symbol containing op
-    dict of str to np.ndarray
+    dict
-    """
+    """Parses the given auxiliary states to a dictionary.
-    aux_states : None or list of np.ndarray or dict of str to np.ndarray.
+        Symbol containing op
-    dict of str to np.ndarray.
+    dict
-            inner elements should have the same the same order as mxnet.sym.list_arguments().
+            inner elements should have the same order as mxnet.sym.list_arguments().
-    >>> mat2 = mx.nd.array([[5, 6], [7, 8]])
+    >>> mat1 = np.array([[1, 2], [3, 4]])
-    >>> mat2 = mx.nd.array([[5, 6], [7, 8]])
+    >>> mat1 = np.array([[1, 2], [3, 4]])
-    """The base class of a data iterator.
+    """The base class for an MXNet data iterator.
-        The batch size, namely the number of examples in a batch.
+        The batch size, namely the number of items in the batch.
-    multiple modules (e.g. stochastic depth network).
+    A module represents a computation component. One can think of module as a computation machine.
-    - Initial state: Memory is not allocated yet, thus the moule is not ready for computation yet.
+    - Initial state: Memory is not allocated yet, so the module is not ready for computation yet.
-    - Optimizer installed: An optimizer can be installed to a module. After this, the parameters
+    - Parameters are initialized: For modules with parameters, doing computation before
-          has been initialized.
+        - `params_initialized`: `bool`, indicates whether the parameters of this module
-           computed results.
+          computed results.
-    - other properties (mostly for backward compatability)
+    - other properties (mostly for backward compatibility)
-        ``eval_metric``.
+        """Runs prediction on ``eval_data`` and evaluates the performance according to
-        eval_metric : EvalMetric
+            Evaluation data to run prediction on.
-            >>> #Evaluate accuracy on val_dataiter
+            >>> # Evaluate accuracy on val_dataiter
-        """Iterate over predictions.
+        """Iterates over predictions.
-                # batch is the data batch from the data iterator
+        Example Usage:
-        """Run prediction and collect the outputs.
+        """Runs prediction and collects the outputs.
-            Defaults to ``None``, indicating running all the batches in the data iterator.
+            Defaults to ``None``, indicates running all the batches in the data iterator.
-            Defaults to ``True``, indicating whether we should reset the data iter before start
+            Defaults to ``True``, indicates whether we should reset the data iter before
-        >>> mod.predict(eval_data=val_dataiter, num_batch=10)
+        An example of using `predict` for prediction::
-        """Train the module parameters.
+        """Trains the module parameters.
-            after each epoch.
+            If not ``None``, will be used as validation set and the performance
-            'ce' (CrossEntropy), 'f1', 'mae', 'mse', 'rmse', 'top_k_accuracy'
+            'ce' (CrossEntropy), 'f1', 'mae', 'mse', 'rmse', 'top_k_accuracy'.
-            Defaults to 'sgd'
+            Defaults to 'sgd'.
-            These will be called at the end of each minibatch during evaluation.
+            These will be called at the end of each mini-batch during evaluation.
-            `arg_params` has higher priority to `initializer`.
+            `arg_params` has a higher priority than `initializer`.
-            Defaults to ``False``. Indicate whether we allow missing parameters when `arg_params`
+            Defaults to ``False``. Indicates whether to allow missing parameters when `arg_params`
-            Defaults to ``False``. Indicate whether we should force initialization even if the
+            Defaults to ``False``. Indicates whether to force initialization even if the
-            this value as N+1.
+            Defaults to 0. Indicates the starting epoch. Usually, if resumed from a
-            Number of epochs to run training.
+            Number of epochs for training.
-            >>> mod.fit(train_data=train_dataiter, eval_data=val_dataiter,
+            >>> #Assume loading a previously checkpointed model
-                        num_epoch=10)
+                        arg_params=arg_params, aux_params=aux_params,
-        'fc2_bias': <NDArray 64 @cpu(0)>, 'fc1_bias': <NDArray 128 @cpu(0)>}, {})
+            >>> print mod.get_params()
-            >>>     mx.model.load_checkpoint(model_prefix, n_epoch_load)
+            >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, n_epoch_load)
-        """Update parameters according to the installed optimizer and the gradients computed
+        """Updates parameters according to the installed optimizer and the gradients computed
-        """Evaluate and accumulate evaluation metric on outputs of the last forward computation.
+        """Evaluates and accumulates evaluation metric on outputs of the last forward
-        """Bind the symbols to construct executors. This is necessary before one
+        """Binds the symbols to construct executors. This is necessary before one
-            Typically is ``data_iter.provide_label``.
+        data_shapes : list of (str, tuple) or DataDesc objects
-        """Install and initialize optimizers.
+        """Installs and initializes optimizers.
-            Defaults to `'sgd'`
+            Defaults to `'sgd'`.
-            optimizer in the case an optimizer is already installed.
+            Defaults to ``False``, indicates whether to force re-initializing an optimizer
-        """Install and initialize optimizers.
+        """Installs and initializes optimizers.
-        """Borrow optimizer from a shared module. Used in bucketing, where exactly the same
+        """Borrows optimizer from a shared module. Used in bucketing, where exactly the same
-        """Update parameters according to the installed optimizer and the gradients computed
+        """Updates parameters according to the installed optimizer and the gradients computed
-        """Get outputs of the previous forward computation.
+        """Gets outputs of the previous forward computation.
-        """Get the gradients with respect to the inputs of the module.
+        """Gets the gradients with respect to the inputs of the module.
-        """Get states from all devices
+        """Gets states from all devices.
-        """Set value for states. Only one of states & value can be specified.
+        """Sets value for states. Only one of the states & value can be specified.
-        """Evaluate and accumulate evaluation metric on outputs of the last forward computation.
+        """Evaluates and accumulates evaluation metric on outputs of the last forward computation.
-        """Synchronize parameters from devices to CPU. This function should be called after
+        """Synchronizes parameters from devices to CPU. This function should be called after
-        """Save optimizer (updater) state to file
+        """Saves optimizer (updater) state to a file.
-        """Load optimizer (updater) state from file
+        """Loads optimizer (updater) state from a file.
-        """ Install monitor on all executors """
+        """ Installs monitor on all executors. """
-    """Load symbol from a JSON file.
+    """Loads symbol from a JSON file.
-    """Load symbol from json string.
+    """Loads symbol from json string.
-import shutil
+import shutil
-            ret = ret + indentStr + '  %s = %d' % (self.enumValues[i], i)
+            ret = ret + indentStr + '  %s = %d' % (gen_enum_value(self.enumValues[i]), i)
-        return self.name + "::" + value
+        return self.name + "::" + gen_enum_value(value)
-      shutil.move(temp_file_name, output_file)
+      shutil.move(temp_file_name, output_file)
-from .. import symbol, init, ndarray, _symbol_internal
+from .. import symbol, init, ndarray
-                               p=p))
+        mask = lambda p, like: symbol.Dropout(symbol.ones_like(like), p=p)
-                rand_start_min = round(np.random.uniform(0, min_hw - self.cut_off_size - 1))
+                rand_start_max = int(np.random.uniform(0, max_hw - self.cut_off_size - 1))
-                rand_start = round(np.random.uniform(0, max_hw - min_hw - 1))
+                rand_start = int(np.random.uniform(0, max_hw - min_hw - 1))
-                print 'argument "%s" of operator "%s" has unknown type "%s"' % (argName, opName, typeString)
+                print('argument "%s" of operator "%s" has unknown type "%s"' % (argName, opName, typeString))
-        if name.value[0]=='_':     # get rid of functions like __init__
+        if name.value.decode('utf-8').startswith('_'):     # get rid of functions like __init__
-                      argDescs[i])
+            arg = Arg(name.value.decode('utf-8'),
-        op = Op(name.value, description.value, args)
+        op = Op(name.value.decode('utf-8'), description.value.decode('utf-8'), args)
-    output_file = '../../include/mxnet-cpp/op.h'
+    output_file = '../include/mxnet-cpp/op.h'
-        with open(temp_file_name, 'w') as f:
+        with codecs.open(temp_file_name, 'w', 'utf-8') as f:
-      os.remove(output_file)
+    except Exception as e:
-
+    if os.path.exists(output_file):
-import logging
+
-        'mean' : (123,117,104),
+        'mean' : (123, 117, 104),
-        'mean': (123.68,116.779,103.939),
+        'mean': (123.68, 116.779, 103.939),
-        'mean' : (123.68,116.779,103.939),
+        'mean' : (123.68, 116.779, 103.939),
-    print('Model is saved into '+model_name)
+    fname, _ = convert_caffe_model(args.model_name, model_meta_info[args.model_name])
-    img_mean_np[[0,2],:,:] = img_mean_np[[2,0],:,:]
+    img_mean_np[[0, 2], :, :] = img_mean_np[[2, 0], :, :]
-import numpy as np
+import mxnet as mx
-    arg_shapes, output_shapes, aux_shapes = sym.infer_shape(data=tuple(input_dim))
+    arg_shapes, _, aux_shapes = sym.infer_shape(data=tuple(input_dim))
-                or layer_type == 'PReLU':
+        if layer_type == 'Convolution' or layer_type == 'InnerProduct' \
-                    wmat_dim = [layer_blobs[0].num, layer_blobs[0].channels, layer_blobs[0].height, layer_blobs[0].width]
+                    wmat_dim = [layer_blobs[0].num, layer_blobs[0].channels,
-            sys.stdout.write('converting layer {0}, wmat shape = {1}'.format(layer_name, wmat.shape))
+            sys.stdout.write('converting layer {0}, wmat shape = {1}'.format(
-            print ('converting scale layer, beta shape = {}, gamma shape = {}'.format(beta.shape, gamma.shape))
+            print('converting scale layer, beta shape = {}, gamma shape = {}'.format(
-            	rescale_factor = 1 / rescale_factor
+                rescale_factor = 1 / rescale_factor
-            		bn_index = idx
+                if layer.name == bn_name:
-            eps_symbol = float( sym.attr_dict()[bn_name + '_moving_mean']['eps'] )
+            eps_symbol = float(sym.attr_dict()[bn_name + '_moving_mean']['eps'])
-            print ('converting batchnorm layer, mean shape = {}, var shape = {}'.format(mean.shape, var.shape))
+            print('converting batchnorm layer, mean shape = {}, var shape = {}'.format(
-            print ('\tskipping layer {} of type {}'.format(layer_name, layer_type))
+            print('\tskipping layer {} of type {}'.format(layer_name, layer_type))
-                    Note that only basic functions are implemented. You are welcomed to contribute to this file.')
+    parser = argparse.ArgumentParser(
-                   (param.num_output, pad, pad, kernel_size, kernel_size, stride, stride, not param.bias_term)
+                   (param.num_output, pad, pad, kernel_size, kernel_size,
-    input_name, input_dim, layer = _get_input(proto)
+    input_name, input_dim, layers = _get_input(proto)
-                    + "data = mx.symbol.Variable(name='data')\n";
+    symbol_string = "import mxnet as mx\ndata = mx.symbol.Variable(name='data')\n"
-    for i in range(len(layer)):
+    for i, layer in enumerate(layers):
-        if layer[i].type == 'Convolution' or layer[i].type == 4:
+        name = re.sub('[-/]', '_', layer.name)
-            param_string = _convert_conv_param(layer[i].convolution_param)
+            param_string = _convert_conv_param(layer.convolution_param)
-        if layer[i].type == 'Deconvolution' or layer[i].type == 39:
+        if layer.type == 'Deconvolution' or layer.type == 39:
-            param_string = _convert_conv_param(layer[i].convolution_param)
+            param_string = _convert_conv_param(layer.convolution_param)
-        if layer[i].type == 'Pooling' or layer[i].type == 17:
+        if layer.type == 'Pooling' or layer.type == 17:
-            param_string = _convert_pooling_param(layer[i].pooling_param)
+            param_string = _convert_pooling_param(layer.pooling_param)
-        if layer[i].type == 'ReLU' or layer[i].type == 18:
+        if layer.type == 'ReLU' or layer.type == 18:
-        if layer[i].type == 'TanH' or layer[i].type == 23:
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-        if layer[i].type == 'Sigmoid' or layer[i].type == 19:
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-        if layer[i].type == 'LRN' or layer[i].type == 15:
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-            param = layer[i].lrn_param
+            param = layer.lrn_param
-        if layer[i].type == 'InnerProduct' or layer[i].type == 14:
+        if layer.type == 'InnerProduct' or layer.type == 14:
-            param = layer[i].inner_product_param
+            param = layer.inner_product_param
-        if layer[i].type == 'Dropout' or layer[i].type == 6:
+        if layer.type == 'Dropout' or layer.type == 6:
-            param = layer[i].dropout_param
+            param = layer.dropout_param
-        if layer[i].type == 'Softmax' or layer[i].type == 20:
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-        if layer[i].type == 'Flatten' or layer[i].type == 8:
+        if layer.type == 'Flatten' or layer.type == 8:
-        if layer[i].type == 'Split' or layer[i].type == 22:
+        if layer.type == 'Split' or layer.type == 22:
-        if layer[i].type == 'Concat' or layer[i].type == 3:
+        if layer.type == 'Concat' or layer.type == 3:
-        if layer[i].type == 'Crop':
+        if layer.type == 'Crop':
-        if layer[i].type == 'BatchNorm':
+        if layer.type == 'BatchNorm':
-            param = layer[i].batch_norm_param
+            param = layer.batch_norm_param
-            	epsilon = 1e-04 
+            if (epsilon <= 1e-05):
-            need_flatten[name] = need_flatten[mapping[layer[i].bottom[0]]]
+                param.use_global_stats, epsilon)
-        if layer[i].type == 'PReLU':
+            prev_name = re.sub('[-/]', '_', layers[i-1].name)
-            param = layer[i].prelu_param
+            param = layer.prelu_param
-        if layer[i].type == 'Eltwise':
+            need_flatten[name] = need_flatten[mapping[layer.bottom[0]]]
-        if layer[i].type == 'Reshape':
+        if layer.type == 'Reshape':
-            param = layer[i].reshape_param
+            param = layer.reshape_param
-        	need_flatten[name] = need_flatten[mapping[layer[i].bottom[0]]] 
+        if layer.type == 'AbsVal':
-            assert len(layer[i].bottom) == 1
+            assert len(layer.bottom) == 1
-            raise ValueError('Unknown layer %s!' % layer[i].type)
+            raise ValueError('Unknown layer %s!' % layer.type)
-            bottom = layer[i].bottom
+            bottom = layer.bottom
-            mapping[layer[i].top[j]] = name
+        for j in range(len(layer.top)):
-    exec(sym)
+    exec(sym)                   # pylint: disable=exec-used
-    exec("ret = " + output_name, globals(), _locals)
+    exec("ret = " + output_name, globals(), _locals)  # pylint: disable=exec-used
-import os, sys
+import os
-import logging
+from test_score import download_data  # pylint: disable=wrong-import-position
-
+    """test model on imagenet """
-    acc = [mx.metric.create('acc'), mx.metric.create('top_k_accuracy', top_k = 5)]
+    acc = [mx.metric.create('acc'), mx.metric.create('top_k_accuracy', top_k=5)]
-                     label_name = 'prob_label',
+                     data_val=val_data,
-if __name__ == '__main__':
+def main():
-        cell.add(mx.rnn.LSTMCell(100, prefix='rnn_stack%d_'%i))
+        if i == 1:
-            return '\x1b[34m'
+        return '\x1b[34m'
-            return 'U'
+        return 'U'
-    """Get customized logger.
+    """Gets a customized logger.
-        level: Level to log.
+    ## get_logger call with WARNING level.
-        A logger.
+    ## get_logger call with DEBUG level.
-            labels = OrderedDict(zip(self.label_names, labels_slice))
+            labels_ = OrderedDict(zip(self.label_names, labels_slice))
-            eval_metric.update_dict(labels, preds)
+            eval_metric.update_dict(labels_, preds)
-    check_consistency(sym, ctx_list)
+  ctx_list_v1_2D = [
-    test_batchnorm_with_type()
+    test_batchnorm_versions()
-                    if name.startswith(prefix):
+                    if prefix in name:
-            moving_average_factor = layer_blobs[2].data
+            rescale_factor = layer_blobs[2].data
-            arg_params[maf_name][:] = moving_average_factor
+            # Get the original epsilon
-            param_string = 'use_global_stats=%s, fix_gamma=False' % param.use_global_stats
+            # CuDNN requires eps to be greater than 1e-05
-        model_loaded.score(eval_data=data_train, num_batch=None, eval_metric=eval_metric, reset=True)
+        is_batchnorm = args.config.getboolean('arch', 'is_batchnorm')
-    After binding, a modulse should be able to report the following richer information:
+    After binding, a module should be able to report the following richer information:
-    buff = np.zeros((n*X.shape[1], n*X.shape[2], X.shape[3]), dtype=np.uint8)
+    buff = np.zeros((int(n*X.shape[1]), int(n*X.shape[2]), int(X.shape[3])), dtype=np.uint8)
-    cv2.waitKey(1)
+    plt.imshow(buff)
-            weight[:] -= lr * grad/(mx.nd.sqrt(n) + self.epsilon)
+            weight[:] -= lr * grad/(mx.nd.sqrt(n + self.epsilon))
-            delta[:] = (self.gamma2) * delta - lr * grad/(mx.nd.sqrt(n - g*g) + self.epsilon)
+            delta[:] = (self.gamma2) * delta - lr * grad/(mx.nd.sqrt(n - g*g + self.epsilon))
-                                     param.epoch, count, speed, name, value)
+                    msg = 'Epoch[%d] Batch [%d]\tSpeed: %.2f samples/sec'
-        return False
+        return isinstance(other, Context) and \
-    """Returns a training TrainingStateScope
+def train_section():
-        with autograd.train():
+        with autograd.train_section():
-    """Returns a testing TrainingStateScope.
+def test_section():
-        with autograd.train():
+        with autograd.train_section():
-            with autograd.test():
+            with autograd.test_section():
-        with train():
+        with train_section():
-        self.kwargs = kwargs
+        self._kwargs = kwargs
-        return json.dumps([self.__class__.__name__.lower(), self.kwargs])
+        return json.dumps([self.__class__.__name__.lower(), self._kwargs])
-            _INITIALIZER_REGISTRY[klass.lower()](**kwargs)._init_weight(desc, arr)
+            create(init)._init_weight(desc, arr)
-        self.kwargs = {'factor_type': factor_type, 'slope': slope}
+        self._kwargs = {'factor_type': factor_type, 'slope': slope}
-# pylint: disable=no-member
+# pylint: disable=no-member, too-many-lines
-        self.num = num
+    Parameters
-            self.sum_metric = [0.0] * self.num
+        self.num_inst = 0
-                return (self.name, self.sum_metric / self.num_inst)
+        if self.num_inst == 0:
-            return (names, values)
+            return (self.name, self.sum_metric / self.num_inst)
-        return zip(name, value)
+        return list(zip(name, value))
-        return "EvalMetric: {}".format(dict(self.get_name_value()))
+# pylint: disable=invalid-name
-            self.metrics = []
+    def __init__(self, metrics=None, name='composite',
-        self.metrics.append(metric)
+        self.metrics.append(create(metric))
-        results = []
+        values = []
-        return (names, results)
+            name, value = metric.get()
-        super(Accuracy, self).__init__('accuracy')
+    def __init__(self, axis=1, name='accuracy',
-                pred_label = ndarray.argmax_channel(pred_label)
+                pred_label = ndarray.argmax(pred_label, axis=self.axis)
-            self.top_k = 1
+    def __init__(self, top_k=1, name='top_k_accuracy',
-        super(F1, self).__init__('f1')
+    def __init__(self, name='f1',
-        super(Perplexity, self).__init__('Perplexity')
+    def __init__(self, ignore_label, axis=-1, name='perplexity',
-        super(MAE, self).__init__('mae')
+    def __init__(self, name='mae',
-        super(MSE, self).__init__('mse')
+    def __init__(self, name='mse',
-        super(RMSE, self).__init__('rmse')
+    def __init__(self, name='rmse',
-        super(CrossEntropy, self).__init__('cross-entropy')
+    def __init__(self, eps=1e-8, name='cross-entropy',
-        super(Torch, self).__init__(name)
+
-        self.num_inst += 1
+            self.sum_metric += ndarray.sum(pred).asscalar()
-        super(Caffe, self).__init__('caffe')
+@register
-    def __init__(self, feval, name=None, allow_extra_outputs=False):
+    def __init__(self, feval, name=None, allow_extra_outputs=False,
-        super(CustomMetric, self).__init__(name)
+        super(CustomMetric, self).__init__(
-            metrics.keys()))
+from collections import OrderedDict
-                               for name in self.symbol.list_outputs()]
+                               for name in self.output_names]
-                _load_label(data_batch, self.label_arrays, self.label_layouts)
+        if self.label_arrays is not None and data_batch.label:
-    def get_outputs(self, merge_multi_context=True):
+    def get_outputs(self, merge_multi_context=True, begin=0, end=None):
-                   for i in range(len(self.execs[0].outputs))]
+                   for i in range(begin, end)]
-        """Accumulate the performance according to `eval_metric` on all devices.
+        """Accumulate the performance according to `eval_metric` on all devices
-            eval_metric.update(labels_slice, texec.outputs)
+            labels = OrderedDict(zip(self.label_names, labels_slice))
-            One shape dimension can be -1. In this case, the value is inferred
+
-def zeros(shape, ctx=None, dtype=mx_real_t):
+def zeros(shape, ctx=None, dtype=mx_real_t, **kwargs):
-def ones(shape, ctx=None, dtype=mx_real_t):
+def ones(shape, ctx=None, dtype=mx_real_t, **kwargs):
-            self.kwargs['clip_gradient'] = self.clip_gradient
+        kwargs = {'rescale_grad': self.rescale_grad}
-                           lr=lr, wd=wd, **self.kwargs)
+                           lr=lr, wd=wd, **kwargs)
-                       lr=lr, wd=wd, **self.kwargs)
+                       lr=lr, wd=wd, **kwargs)
-            self.kwargs['clip_gradient'] = self.clip_gradient
+        self.epsilon = epsilon
-                    lr=lr, wd=wd, **self.kwargs)
+                    lr=lr, wd=wd, **kwargs)
-            self.kwargs['clip_weights'] = self.clip_weights
+
-                weight, grad, n, out=weight, lr=lr, wd=wd, **self.kwargs)
+                weight, grad, n, out=weight, lr=lr, wd=wd, **kwargs)
-                               lr=lr, wd=wd, **self.kwargs)
+                               lr=lr, wd=wd, **kwargs)
-                            'Grouped' if name is None else name)
+        if name is None:
-        attr['__init__'] = init.dumps()
+        if not isinstance(init, string_types):
-from mxnet.contrib.autograd import grad, grad_and_loss, train, test
+from mxnet.contrib.autograd import *
-    with train():
+    with train_section():
-        with test():
+        with test_section():
-    test_iter()
+    import nose
-    --------
+    Example
-            If ``True````, will force re-initialize even if already initialized.
+            If ``True``, will force re-initialize even if already initialized.
-                 'type_dict': {'ews_arg0': np.float32, 'ews_arg1': np.float32}}]
+    dev_types = [[mx.gpu(0), [np.float64, np.float32, np.float16]],
-    """Randomly crop src with size. Upsample result if src is smaller than size."""
+    """Randomly crop `src` with `size` (width, height).
-        """Set individual learning rate for each weight.
+        """Sets an individual learning rate multiplier for each parameter.
-            but we recommend using name and symbol.
+        args_lr_mult : dict of str/int to float
-        """Set individual weight decay for each weight.
+        """Sets an individual weight decay multiplier for each parameter.
-        end with _weight, if param_idx2name is provided.
+        .. note:: The default weight decay multiplier for a `Variable`
-            but we recommend using name and symbol.
+            For each of its key-value entries, the weight decay multipler for the
-        """ Registers an optimizer with the store.
+        """ Registers an optimizer with the kvstore.
-        this action is done.
+        When using a single machine, this function updates the local optimizer.
-            the optimizer.
+            The new optimizer for the store
-        """Saves optimizer (updater) state to file.
+        """Saves the optimizer (updater) state to a file. This is often used when checkpointing
-            Path to output states file.
+            Path to the output states file.
-        """Loads optimizer (updater) state from file.
+        """Loads the optimizer (updater) state from the file.
-    name : {'local'}
+    name : {'local', 'device', 'dist_sync', 'dist_device_sync', 'dist_async'}
-       no_bias=False):
+       no_bias=False
-    net = mx.sym.Activation(data=net, act_type=act_type)
+    if act_type is not None:
-                is_batchnorm=False
+                is_batchnorm=False,
-                    hidden = net[seq_index]
+                    if dropout_rate > 0:
-                                    act_type=act_type_list[layer_index],
+                                    act_type=None,
-                                               beta=beta_list[layer_index])
+                        hidden = batchnorm(net=hidden,
-    """Scale down crop size if it's bigger than image size."""
+    """Scales down crop size if it's larger than image size.
-    """Resize shorter edge to size."""
+    """Resizes shorter edge to size.
-        """Peforms a synchronized copy from the array.
+        """Performs a synchronized copy from the `source_array` to the current array.
-        source_array : array_like)
+        source_array : array_like
-        """Returns a sliced NDArray that shares memory with current one.
+        """Returns a sliced NDArray that shares memory with the current one.
-            Starting index of slice.
+            Starting inclusive index of slice in the first dim.
-            Finishing index of slice.
+            Finishing exclusive index of slice in the first dim.
-        """Returns a sliced view of this array.
+        """Returns a view of the array sliced at `idx` in the first dim.
-            index of sub array.
+            index for slicing the `NDArray` in the first dim.
-    """Compare foward call to expected value.
+    """Compares a symbol's forward results with the expected ones.
-    """Compare backward call to expected value.
+    """Compares a symbol's backward results with the expected ones.
-        Label dimension.
+    label_width : int, optional
-        """Update the weight given the corresponding gradient and state.
+        """Updates the given parameter using the corresponding gradient and state.
-            An unique index to identify the weight.
+            The unique index of the parameter into the individual learning
-            The weight
+            The parameter to be updated.
-            The gradient of the objective with respect to this weight.
+            The gradient of the objective with respect to this parameter.
-            The state associated with this weight.
+            The state returned by `create_state()`.
-    next step.
+    The label at each sequence step is the following token
-        encoded sentences
+        Encoded sentences.
-        format of data and label. 'NT' means (batch_size, length)
+        Batch size of the data.
-    """Executor is the actual executing object of MXNet."""
+    """Executor is the object providing efficient symbolic graph execution and optimization.
-        """Install callback.
+        """Install callback for monitor.
-    The optimizer updates the weight by:
+    The optimizer updates the weight by::
-      weight = weight - state
+        state = momentum * state + lr * rescale_grad * clip(grad, clip_gradient) + wd * weight
-        Small value to avoid divided by 0.
+        Small value to avoid division by 0.
-        Decay factor of moving average for ``gradient^2``.
+        A decay factor of moving average over past squared gradient.
-        A "momentum" factor. Only used if ``centered=True``.
+        A "momentum" factor. Only used if `centered`=``True``.
-        Use Graves' or Tieleman & Hinton's version of RMSProp.
+        Flag to control which version of RMSProp to use.
-        clip weights into range ``[-clip_weights, clip_weights]``
+        Clips weights into range ``[-clip_weights, clip_weights]``
-        """Save the initializer to string"""
+        """Saves the initializer to string
-    """Convert a symbol to a dot object for visualization.
+    """Creates a visualization (Graphviz digraph object) of the given computation graph.
-        Title of the dot graph.
+    title: str, optional
-        or `*_bias` will be hidden.
+        A symbol from the computation graph. The generated digraph will visualize the part
-        The dot object of `symbol`.
+    -------
-    """Callback to checkpoint the model to prefix every epoch.
+    """A callback that saves a model checkpoint every few epochs.
-    	How many epochs to wait before checkpointing. Defaults to 1.
+        Prefix for the checkpoint filenames.
-        The callback function that can be passed as ``iter_end_callback`` to fit.
+        A callback function that can be passed as `epoch_end_callback` to fit.
-    data : list of NDArray
+    data : list of `NDArray`, each array containing `batch_size` examples.
-    label : list of NDArray, optional
+    label : list of `NDArray`, each array often containing a 1-dimensional array. optional
-          examples read is less than the batch size.
+          The number of examples padded at the end of a batch. It is used when the
-          When working with Module this is the order of the label_names argument.
+          The bucket key, used for bucketing module.
-    """Iterating on either ``mx.nd.NDArray`` or ``numpy.ndarray``.
+    """Returns an iterator for ``mx.nd.NDArray`` or ``numpy.ndarray``.
-        Input data
+        The input data.
-        Input label
+        The input label.
-        Batch Size
+        Batch size of data.
-        Whether to shuffle the data
+        Whether to shuffle the data.
-        How to handle the last batch, can be 'pad', 'discard' or
+        How to handle the last batch. This parameter can be 'pad', 'discard' or
-        The data name
+        The data name.
-        The label name
+        The label name.
-    """Initialize the weight to a scalar value."""
+    """Initializes the weights to a scalar value.
-    """Initialize the weight with Xavier or other similar schemes.
+    """Returns an initializer performing "Xavier" initialization for weights.
-        Random generator type, can be ```gaussian`` or ``uniform``.
+        Random generator type, can be ``'gaussian'`` or ``'uniform'``.
-        Can be ``avg``, ``in``, or ``out``.
+        Can be ``'avg'``, ``'in'``, or ``'out'``.
-        Scale of random number range.
+        Scale of random number.
-        Can be ``avg``, ``in``, or ``out``.
+        Can be ``'avg'``, ``'in'``, or ``'out'``.
-    """Base class of all evaluation metrics."""
+    """Base class for all evaluation metrics.
-        """Update the internal evaluation.
+    def update(self, labels, preds):
-        labels : list of NDArray
+        labels : list of `NDArray`
-        preds : list of NDArray
+        preds : list of `NDArray`
-        """Clear the internal statistics to initial state."""
+        """Resets the internal evaluation result to initial state."""
-        """Get the current evaluation result.
+        """Gets the current evaluation result.
-           Value of the evaluation.
+        names : list of str
-        """Get zipped name and value pairs."""
+        """Returns zipped name and value pairs.
-    """Manage multiple evaluation metrics."""
+    """Manages multiple evaluation metrics.
-        """Add a child metric."""
+        """Adds a child metric.
-        """Get a child metric."""
+        """Returns a child metric.
-    """Calculate accuracy."""
+    """Computes accuracy classification score.
-    """Calculate top k predictions accuracy."""
+    """Computes top k predictions accuracy.
-    """Calculate the F1 score of a binary classification problem."""
+    """Computes the F1 score of a binary classification problem.
-    """Calculate perplexity.
+    """Computes perplexity.
-        all entries if None.
+        counting. By default, sets to -1.
-    """Calculate Mean Absolute Error (MAE) loss."""
+    """Computes Mean Absolute Error (MAE) loss.
-    """Calculate Mean Squared Error (MSE) loss."""
+    """Computes Mean Squared Error (MSE) loss.
-    """Calculate Root Mean Squred Error (RMSE) loss."""
+    """Computes Root Mean Squred Error (RMSE) loss.
-    """Calculate Cross Entropy loss."""
+    """Computes Cross Entropy loss.
-    """Custom evaluation metric that takes a NDArray function.
+    """Computes a customized evaluation metric.
-    allow_extra_outputs : bool
+        The name of the metric. (the default is None).
-        in outputs for forwarding.
+        in outputs for forwarding. (the default is False).
-    shape1 = (2, 3, 2, 3)
+    shape1 = (2, 3, 3, 5)
-    shape2 = (2, 3, 2, 3, 3)
+    shape2 = (2, 3, 3, 5, 4)
-    else:
+    elif mode =="train" or mode == "load":
-        datagen.load_validation_data(val_json)
+        #test bigramphems
-            datagen.sample_normalize(normalize_target_k, True)
+            if overwrite_meta_files:
-        max_label_length = datagen.get_max_label_length(partition="train")
+        max_label_length = datagen.get_max_label_length(partition="train",is_bi_graphemes=is_bi_graphemes)
-        max_label_length = datagen.get_max_label_length(partition="test")
+        max_label_length = datagen.get_max_label_length(partition="test",is_bi_graphemes=is_bi_graphemes)
-        shuffle=False
+        sort_by_duration = True
-	shuffle=True
+        sort_by_duration = False
-                          shuffle=shuffle)
+                          is_bi_graphemes=is_bi_graphemes)
-                                    shuffle=False)
+                                    sort_by_duration=False,
-                                                 load_optimizer_states=False)
+        model_loaded = mx.module.Module.load(prefix=model_path, epoch=model_num_epoch, context=contexts,
-
+    # set parameters from cfg file
-    # check the number of gpus is positive divisor of the batch size
+    # check the number of gpus is positive divisor of the batch size for data parallel
-
+from stt_bi_graphemes_util import generate_bi_graphemes_label
-    def get_max_label_length(self, partition):
+    def get_max_label_length(self, partition, is_bi_graphemes=False):
-        self.max_label_length = max([len(text) for text in texts])
+        if is_bi_graphemes:
-    def prepare_minibatch(self, audio_paths, texts, overwrite=False):
+    def prepare_minibatch(self, audio_paths, texts, overwrite=False, is_bi_graphemes=False):
-            y[i, :len(texts[i])] = label
+            if is_bi_graphemes:
-                 shuffle=True, partition="train"):
+                 is_bi_graphemes=False, partition="train",):
-                                                                                    texts)
+            durations, audio_paths, texts = datagen.sort_by_duration(durations,
-            self.texts = texts
+            durations = durations
-                data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=True)
+                data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=True, is_bi_graphemes=self.is_bi_graphemes)
-                data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=False)
+                data_set = self.datagen.prepare_minibatch(audio_paths, texts, overwrite=False, is_bi_graphemes=self.is_bi_graphemes)
-    def __init__(self, batch_size, num_gpu, seq_length, is_epoch_end=False):
+    def __init__(self, batch_size, num_gpu, seq_length, is_epoch_end=False, is_logging=True):
-
+        self.batch_loss = 0.
-
+        if self.is_logging:
-                    labelUtil.convert_num_to_word(p), this_cer, l_distance, len(l)))
+                if self.is_logging:
-
+                if self.is_epoch_end:
-            outputs[i].copyto(eval_data.init_state_arrays[i - 1])
+#tensorboard setting
-    eval_metric = STTMetric(batch_size=batch_size, num_gpu=num_gpu, seq_length=seq_len)
+    eval_metric = STTMetric(batch_size=batch_size, num_gpu=num_gpu, seq_length=seq_len,is_logging=enable_logging_validation_metric,is_epoch_end=True)
-    lr_scheduler = SimpleLRScheduler(learning_rate, momentum=momentum, optimizer=optimizer)
+    learning_rate_annealing = args.config.getfloat('train', 'learning_rate_annealing')
-    n_epoch = begin_epoch
+    mode = args.config.get('common', 'mode')
-    if begin_epoch == 0:
+    if begin_epoch == 0 and mode == 'train':
-    reset_optimizer()
+    lr_scheduler = SimpleLRScheduler(learning_rate=learning_rate)
-        eval_metric.reset()
+        loss_metric.reset()
-                module.update_metric(eval_metric, data_batch.label)
+            # tensorboard setting
-        #module.score(eval_data=data_val, num_batch=None, eval_metric=eval_metric, reset=True)
+
-    log.info('FINISH')
+        lr_scheduler.learning_rate=learning_rate/learning_rate_annealing
-                                            data_iter, X.shape[0], self.xpu).values()[0]
+                X_i = list(model.extract_feature(self.internals[i-1], self.args, self.auxs,
-                                 X.shape[0], self.xpu).values()[0]
+        Y = list(model.extract_feature(self.loss, self.args, self.auxs, data_iter,
-        with open(fname, 'w') as fout:
+        with open(fname, 'wb') as fout:
-        with open(fname) as fin:
+        with open(fname, 'rb') as fin:
-        z = model.extract_feature(self.feature, args, None, test_iter, N, self.xpu).values()[0]
+        z = list(model.extract_feature(self.feature, args, None, test_iter, N, self.xpu).values())[0]
-                z = model.extract_feature(self.feature, args, None, test_iter, N, self.xpu).values()[0]
+                z = list(model.extract_feature(self.feature, args, None, test_iter, N, self.xpu).values())[0]
-        with open(eval_file, 'w') as f:
+        with open(eval_file, 'wb') as f:
-        with open(annocache, 'w') as f:
+        with open(annocache, 'wb') as f:
-        with open(annocache, 'r') as f:
+        with open(annocache, 'rb') as f:
-            shared_exe = self._buckets.values()[0]['exe'].values()[0]
+            shared_exe = list(list(self._buckets.values())[0]['exe'].values())[0]
-        with open(cache_file, 'w') as f:
+        with open(cache_file, 'wb') as f:
-        with open(cache_file, 'r') as f:
+        with open(cache_file, 'rb') as f:
-    w[Y_pred[i], Y[i]] += 1
+    w[Y_pred[i], int(Y[i])] += 1
-    ctc = mx.sym.ctc_loss(in_var, labels_var)
+    ctc = mx.contrib.sym.ctc_loss(in_var, labels_var)
-    _run_cmd("cd %s/.. && cp make/config.mk config.mk && make -j$(nproc)" %
+    _run_cmd("cd %s/.. && cp make/config.mk config.mk && make -j$(nproc) DEBUG=1" %
-    # app.connect("builder-inited", build_scala_docs)
+    app.connect("builder-inited", build_scala_docs)
-
+import mock
-doc_root = 'http://mxnet.dmlc.ml/'
+copyright = u'2015-2017, %s' % author
-    '.Rmd': MarkdownParser,
+    '.md': parser.CommonMarkParser,
-# release = mxnet.__version__
+# from mxnet import libinfo
-    app.add_transform(AutoStructify)
+"""A sphnix-doc plugin to build mxnet docs"""
-    """Initialize by loading data from file or dict.
+    """Initializes variables by loading data from file or dict.
-    param: str or dict of str->NDArray
+    param: str or dict of str->`NDArray`
-        Default initializer when name is not found in param.
+        Default initializer when name is not found in `param`.
-        Log source when initializing.
+        Flag for enabling logging of source when initializing.
-    """Calculate and log training speed periodically.
+    """Logs training speed and evaluation metrics periodically.
-        batch_size of data.
+        Batch size of data.
-        Defaults to calculating & logging every 50 batches.
+        Specifies how frequently training speed and evaluation metrics
-        Reset the metric after each log.
+        Reset the evaluation metrics after each log.
-    """Read/write RecordIO format data.
+    """Reads/writes `RecordIO` data format, supporting sequential read and write.
-        uri path to recordIO file.
+        Path to the record file.
-        "r" for reading or "w" writing.
+        'w' for write or 'r' for read.
-        """Open record file."""
+        """Opens the record file."""
-        """Close record file."""
+        """Closes the record file."""
-        this will truncate the file to empty."""
+        """Resets the pointer to first item.
-        """Write a string buffer as a record.
+        """Inserts a string buffer as a record.
-        """Read a record as string.
+        """Returns record as a string.
-    """Read/write RecordIO format data supporting random access.
+    """Reads/writes `RecordIO` data format, supporting random access.
-        Path to index file.
+        Path to the index file.
-        Path to record file. Only support file types that are seekable.
+        Path to the record file. Only supports seekable file types.
-        'w' for write or 'r' for read
+        'w' for write or 'r' for read.
-        """Query current read head position."""
+        """Sets the current read pointer position.
-        """Query current write head position."""
+        """Returns the current position of write head.
-        """Read record with index."""
+        """Returns the record at given index.
-        """Write record with index."""
+        """Inserts input record at given index.
-        update_dict = {name: nd for name, nd in zip(sym.list_arguments(), exe.grad_arrays) if nd}
+        update_dict = {name: nd for name, nd in zip(sym.list_arguments(), exe.grad_arrays) if nd is not None}
-                    self.exector.grad_arrays) if nd}
+                    self.exector.grad_arrays) if nd is not None}
-    such as dropout operators.
+    This affects the behavior of modules in MXNet that uses random number generators,
-    generated from GPU0 can be different from CPU.
+    Random number generators in MXNet are device specific. Therefore, random numbers
-        """Returns all outputs in a list"""
+        """Returns a generator object of symbol.
-        """x.__rsub__(y) <=> y-x """
+        """x.__rsub__(y) <=> y-x
-        """x.__rdiv__(y) <=> y/x """
+        """x.__rdiv__(y) <=> y/x
-        """x.__neg__(y) <=> -x """
+        """x.__neg__() <=> -x
-        """Compose symbol on inputs.
+        """Composes symbol using inputs.
-            provide positional arguments
+            Positional arguments.
-            provide keyword arguments
+            Keyword arguments.
-        the resulting symbol
+            The resulting symbol.
-        """Compose symbol on inputs.
+        """Composes symbol using inputs.
-        This call mutates the current symbol.
+        x._compose(y, z) <=> x(y,z)
-            Positional arguments
+            Positional arguments.
-            Keyword arguments
+            Keyword arguments.
-        """Gets attribute string from the symbol. This function only works for non-grouped symbols.
+        """Returns the attribute string for corresponding input key from the symbol.
-            The desired attribute value, returns None if attribute does not exist.
+            The desired attribute value, returns ``None`` if the attribute does not exist.
-            A dicitonary mapping attribute keys to values.
+        ret : Dict of str to str
-        ret : dict of str to dict
+        ret : Dict of str to dict
-        # pylint: enable=too-many-locals
+            # pylint: enable=too-many-locals
-        """Infers the shape partially. This functions works the same way as `infer_shape`,
+        """Infers the shape partially.
-        # pylint: enable=too-many-locals
+            # pylint: enable=too-many-locals
-        You also get the benefit being able to directly load/save from cloud storage(S3, HDFS)
+        The advantage of `load`/`save` functions is that the file contents are language agnostic.
-            - /path-to/my-local-symbol
+            The name of the file.
-        Allows specifying data types.
+        """Binds current symbol to get an executor, allocate all the arguments needed.
-        for arguments and auxiliary states that user did not specify explicitly.
+        Example usage:
-            - 'add' means everytime gradient is add to the specified NDArray.
+            To specify how we should update the gradient to the `args_grad`.
-        type_dict  : dict of str->numpy.dtype
+        type_dict  : Dict of str->numpy.dtype
-        group2ctx : dict of string to mx.Context
+        group2ctx : Dict of string to mx.Context
-        kwargs : dict of str->shape
+        kwargs : Dict of str->shape
-            The generated Executor
+            The generated executor
-        """Bind current symbol to get an executor.
+        """Binds the current symbol to an executor and returns it.
-            - If type is dict of str to `NDArray`, then it maps the name of arguments
+            - If the input type is a list of `NDArray`, the order should be same as the order
-            When specified, args_grad provide NDArrays to hold
+            When specified, `args_grad` provides NDArrays to hold
-            - If type is dict of str to `NDArray`, then it maps the name of arguments
+            - If the input type is a list of `NDArray`, the order should be same as the order
-              for needed argument gradient.
+            - When the type is a dict of str to `NDArray`, one only need to provide the dict
-            Specifies how we should update the gradient to the args_grad.
+            To specify how we should update the gradient to the `args_grad`.
-            - 'write' means everytime gradient is write to specified args_grad `NDArray`.
+            - 'write' means everytime gradient is write to specified `args_grad` `NDArray`.
-            list_auxiliary_states is not empty.
+            Input auxiliary states to the symbol, only needed when the output of
-            - In either case, all the auxiliary_states need to be provided.
+            - If the input type is a list of `NDArray`, the order should be same as the order
-        group2ctx : dict of string to mx.Context
+        group2ctx : Dict of string to mx.Context
-            The generated Executor
+            The generated executor
-        to an argument, and do not have gradient. But still be useful
+        Auxiliary states are the special states of symbols that do not correspond
-        Users can give up gradient by using a dict in `args_grad` and only specify
+        One can give up gradient by using a dict in `args_grad` and only specify
-        """Evaluate a symbol given arguments
+        """Evaluates a symbol given arguments.
-        Eval allows simpler syntax for less cumbersome introspection.
+        This function allows simpler syntax for less cumbersome introspection.
-            - In either case, all the arguments must be provided.
+        kwargs : Keyword arguments of type `NDArray`
-        When called on a single symbol (not a group),
+        result :  a list of NDArrays corresponding to the values taken by each symbol when
-    """Create a symbolic variable with specified name.
+    """Creates a symbolic variable with specified name.
-        Additional attributes to set on the variable.
+        Variable name.
-        If the user specified a different shape for this variable using
+        The shape of a variable. If specified, this will be used during the shape inference.
-        The learning rate muliplier for this variable.
+        The learning rate multiplier for input variable.
-        Weight decay muliplier for this variable.
+        Weight decay multiplier for input variable.
-        The dtype for this variable. If not specified, this value will be inferred.
+        The dtype for input variable. If not specified, this value will be inferred.
-    kwargs : other additional attribute variables
+        Initializer for this variable to (optionally) override the default initializer.
-    """Create an evaluation metric.
+    """Creates evaluation metric from metric names or instances of EvalMetric
-        providing statistics given pred, label NDArray.
+        Specifies the metric to create.
-    """Constructing a context.
+    """Constructs a context.
-    Context can also be used a way to change default context.
+    Context can also be used as a way to change the default context.
-    >>>     gpu_array = mx.nd.ones((2, 3))
+    ...     gpu_array = mx.nd.ones((2, 3))
-        """Return device type of current context.
+        """Returns the device type of current context.
-        """Compare two contexts. Two contexts are equal if they
+        """Compares two contexts. Two contexts are equal if they
-                self.device_id == other.device_id:
+                        self.device_id == other.device_id:
-    """Return a CPU context.
+    """Returns a CPU context.
-    """Return a GPU context.
+    """Returns a GPU context.
-    """Return the current context.
+    """Returns the current context.
-                + int(node["attr"]["num_filter"])
+            cur_param = pre_filter * int(node["attr"]["num_filter"])
-                                                    node["attr"]["num_filter"])
+            label = r"Convolution\n%s/%s, %s" % ("x".join(_str2tuple(node["attr"]["kernel"])),
-                                                if "stride" in node["attr"] else '1')
+            label = r"Pooling\n%s, %s/%s" % (node["attr"]["pool_type"],
-    """Initialize the weight to 0."""
+    """Initializes weights to zero.
-    """Initialize the weight to 1."""
+    """Initializes weights to one.
-    """Initialize the weight with value uniformly sampled from ``[-scale, scale]``.
+    """Initializes weights with random values uniformly sampled from a given range.
-        The scale of uniform distribution.
+        The bound on the range of the generated random values.
-    """Initialize the weight with value sampled according to ``normal(0, sigma)``.
+    """Initializes weights with random values sampled from a normal distribution
-        Standard deviation for gaussian distribution.
+        Standard deviation of the normal distribution.
-    """Register an intializer to the initializer factory."""
+    """Registers a custom initializer.
-    Jozefowicz et al. 2015 recommends setting this to 1.0.
+        Jozefowicz et al. 2015 recommends setting this to 1.0.
-    
+
-    
+
-    
+        exe.backward([out_grad])
-    
+
-    
+
-    
+
-    
+
-                
+
-                
+
-    
+
-    
+
-   
+
-    
+
-    
+
-            exe.backward([out_grad])  
+            exe.backward([out_grad])
-         
+
-            exe.backward([out_grad])  
+            exe.backward([out_grad])
-def check_with_device(device):
+def check_with_device(device, dtype):
-                ('std',  lambda x, params: np.std(x) - params['scale'], tol)
+                ('mean', lambda x, params: np.mean(x.astype(np.float64) - params['loc']),  tol),
-                ('std', lambda x, params: np.std(x) - np.sqrt(params['mu'] + params['alpha'] * params['mu'] ** 2 ), tol)
+                ('mean', lambda x, params: np.mean(x.astype(np.float64)) - (params['low'] + params['high']) / 2.0, tol),
-
+    if device.device_type == 'cpu':
-        params.update(shape=shape, ctx=device)
+        params.update(shape=shape, dtype=dtype, ctx=device)
-        assert same(ret1, ret2), \
+        assert device.device_type == 'gpu' or same(ret1, ret2), \
-        params.update(shape=shape)
+        params.update(shape=shape, dtype=dtype)
-        xgrad = mx.nd.zeros(shape, ctx=device)
+        x = mx.nd.zeros(shape, dtype=dtype, ctx=device)
-        assert same(un1.asnumpy(), un2.asnumpy()), \
+        assert device.device_type == 'gpu' or same(un1.asnumpy(), un2.asnumpy()), \
-                    assert np.abs(check_func(samples, params)) < tol, "symbolic test: %s check for `%s` did not pass" % (check_name, name)
+        # check multi-distribution sampling, only supports cpu for now
-    check_with_device(mx.cpu())
+    check_with_device(mx.context.current_context(), 'float16')
-    """Load array from file.
+    """Loads an array from file.
-    """Save a list of arrays of a str->array dict into file.
+    """Saves a list of arrays or a dict of str->array to file.
-        The data for saving.
+        The data to save.
-    """Seed the random number generators in MXNet.
+    """Seeds the random number generators in MXNet.
-    prefix : str, default 'rnn_'
+    prefix : str, default 'lstm_'
-    """Create a customized metric from numpy function.
+    """Creates a custom evaluation metric that receives its inputs as numpy arrays.
-        should return a single float.
+        Custom evaluation function that receives labels and predictions for a minibatch
-        in outputs for forwarding.
+        Name of the custom metric.
-    label : list of NDArray
+    label : list of NDArray, optional
-    def __init__(self, data, label, pad=None, index=None,
+    def __init__(self, data, label=None, pad=None, index=None,
-                                      if name in data_names]
+            self.input_grad_arrays = [[exec_.grad_arrays[self.arg_names.index(name)]
-
+import mxnet.ndarray as nd
-  dshape = (3, 8, 7)
+    dtype = np.float16
-  sym = mx.sym.Activation(data=sym, act_type='relu', __layout__='TNC')
+    sym = mx.sym.Variable('data')
-  mod.forward(mx.io.DataBatch(data=[mx.nd.ones(dshape, dtype=dtype)],
+    mod = mx.mod.Module(sym, ('data',), None, context=[mx.cpu(0), mx.cpu(1)])
-  mod.backward([mx.nd.ones(dshape, dtype=dtype)])
+    mod.backward([mx.nd.ones(dshape, dtype=dtype)])
-  for x in mod.get_outputs():
+    for x in mod.get_outputs():
-    def _init_params(self, input_shapes, overwrite=False):
+    def _init_params(self, inputs, overwrite=False):
-        assert(arg_shapes is not None)
+        assert arg_shapes is not None
-        aux_params = {k : nd.zeros(s) for k, s in zip(aux_names, aux_shapes)}
+        param_name_attrs = [x for x in zip(arg_names, arg_shapes, arg_dtypes)
-                self._init_params(dict(data.provide_data+data.provide_label))
+                self._init_params(data.provide_data+data.provide_label)
-    """parse data_shapes into DataDesc format and check that names match"""
+    """parse data_attrs into DataDesc format and check that names match"""
-    """Make resize shorter edge to size augumenter."""
+    """Make resize shorter edge to size augmenter."""
-        """Augumenter body"""
+        """Augmenter body"""
-    """Make random crop augumenter"""
+    """Make random crop augmenter"""
-        """Augumenter body"""
+        """Augmenter body"""
-    """Make random crop with random resizing and random aspect ratio jitter augumenter."""
+    """Make random crop with random resizing and random aspect ratio jitter augmenter."""
-        """Augumenter body"""
+        """Augmenter body"""
-        """Augumenter body"""
+        """Augmenter body"""
-        """Augumenter body"""
+        """Augmenter body"""
-            """Augumenter body"""
+            """Augmenter body"""
-            """Augumenter body"""
+            """Augmenter body"""
-            """Augumenter body"""
+            """Augmenter body"""
-        """Augumenter body"""
+        """Augmenter body"""
-        """Augumenter body"""
+        """Augmenter body"""
-        """Augumenter body"""
+        """Augmenter body"""
-        """Augumenter body"""
+        """Augmenter body"""
-    """Create augumenter list."""
+    """Creates an augmenter list."""
-    Supports reading from both .rec files and raw image files with image list.
+    """Image data iterator with a large number of augmentation choices.
-    to use data partition (for distributed training) or shuffling.
+    To load input images from .rec files, use `path_imgrec` parameter and to load from raw image
-    To load from raw image files, specify path_imglist and path_root.
+    To use data partition (for distributed training) or shuffling, specify `path_imgidx` parameter.
-        Data shape in (channels, height, width).
+        Data shape in (channels, height, width) format.
-        dimension of label
+        Label dimension.
-        Created with tools/im2rec.py or bin/im2rec
+        Path to image record file (.rec).
-        path to image list (.lst)
+        Path to image list (.lst).
-        Format: index\t[one or more label separated by \t]\trelative_path_from_root.
+        Format: Tab separated record of index, one or more labels and relative_path_from_root.
-        each item is a list [imagelabel: float or list of float, imgpath].
+        A list of images with the label(s).
-        Root folder of image files
+        Root folder of image files.
-        Whether to shuffle all images at the start of each iteration.
+        Whether to shuffle all images at the start of each iteration or not.
-        Partition index
+        Partition index.
-        data name for provided symbols
+        Data name for provided symbols.
-        label name for provided symbols
+        Label name for provided symbols.
-        More arguments for creating augumenter. See mx.image.CreateAugmenter.
+        More arguments for creating augmenter. See mx.image.CreateAugmenter.
-        """checks that the input data shape is valid"""
+        """Checks if the input data shape is valid"""
-        """checks that data is valid"""
+        """Checks if the input data is valid"""
-        """decodes a sting or byte string into an image."""
+        """Decodes a string or byte string to an NDArray.
-        """reads image from fname and returns the raw bytes to be decoded."""
+        """Reads an input image `fname` and returns the decoded raw bytes.
-        """transforms data with specificied augmentation."""
+        """Transforms input data with specified augmentation."""
-        """final postprocessing step before image is loaded into the batch."""
+        """Final postprocessing step before image is loaded into the batch."""
-        net = bi_lstm_unroll(net=net,
+    mode = args.config.get("common", "mode")
-                             num_lstm_layer=num_rnn_layer,
+                             num_hidden_gru_list=num_hidden_rnn_list,
-        net = bi_gru_unroll(net=net,
+        elif rnn_type == "bigru":
-                            is_batchnorm=is_batchnorm)
+                            label=label,
-    return net
+        conv_layer1_filter_dim = tuple(json.loads(args.config.get("arch", "conv_layer1_filter_dim")))
-                          shuffle=False)
+                          sort_by_duration=sort_by_duration,
-        do_training(args=args, module=model_loaded, data_train=data_train, data_val=data_val, begin_epoch=model_num_epoch)
+        do_training(args=args, module=model_loaded, data_train=data_train, data_val=data_val, begin_epoch=model_num_epoch+1)
-                print(audio_path, text)
+                #print(audio_path, text)
-            module.update_metric(eval_metric, data_batch.label)
+            if (nbatch+1) % show_every == 0:
-    """Decode an image from string. Requires OpenCV to work.
+    """Decode an image to an NDArray.
-        Output buffer. Use None for automatic allocation.
+    buf : str/bytes or numpy.ndarray
-    shape : int or tuple of int
+    shape : int or tuple of int or list of int
-        """Return all outputs in a list"""
+        """Returns all outputs in a list"""
-        """x.__add__(y) <=> x+y """
+        """x.__add__(y) <=> x+y
-        """x.__sub__(y) <=> x-y """
+        """x.__sub__(y) <=> x-y
-        """x.__mul__(y) <=> x*y """
+        """x.__mul__(y) <=> x*y
-        """x.__div__(y) <=> x/y """
+        """x.__div__(y) <=> x/y
-        """x.__pow__(y) <=> x**y """
+        """x.__pow__(y) <=> x**y
-        """x.__eq__(y) <=> x==y """
+        """x.__eq__(y) <=> x==y
-        """x.__ne__(y) <=> x!=y """
+        """x.__ne__(y) <=> x!=y
-        """x.__gt__(y) <=> x>y """
+        """x.__gt__(y) <=> x>y
-        """x.__ge__(y) <=> x>=y """
+        """x.__ge__(y) <=> x>=y
-        """x.__lt__(y) <=> x<y """
+        """x.__lt__(y) <=> x<y
-        """x.__le__(y) <=> x<=y """
+        """x.__le__(y) <=> x<=y
-        Get an output of this symbol
+        Returns a sliced view of the input symbol.
-            indexing key
+            Indexing key
-        """Get attribute string from the symbol. This function only works for non-grouped symbols.
+        """Gets attribute string from the symbol. This function only works for non-grouped symbols.
-        """Get all attributes from the symbol.
+        """Gets all attributes from the symbol.
-        """Recursively get all attributes from the symbol and its children.
+        """Recursively gets all attributes from the symbol and its children.
-        """Set an attribute of the symbol.
+        """Sets an attribute of the symbol.
-        """Get a new grouped symbol sgroup. The output of sgroup is a list of the
+        """Gets a new grouped symbol `sgroup`. The output of `sgroup` is a list of
-        >>> b = mxnet.sym.var('b')
+
-            used to compute the symbol
+            used to compute the symbol.
-        inputs to output nodes of the original symbol
+        """Gets a new grouped symbol whose output contains
-        """List all the arguments in the symbol.
+        """Lists all the arguments in the symbol.
-        >>> b = mxnet.sym.var('b')
+        Example usage:
-        """List all outputs in the symbol.
+        """Lists all the outputs in the symbol.
-        """List all auxiliary states in the symbol.
+        """Lists all the auxiliary states in the symbol.
-            List the names of the auxiliary states.
+            List of the auxiliary states in input symbol.
-        and all outputs.
+        """Infers the type of all arguments and all outputs, given the known types
-        >>> b = mxnet.sym.var('b')
+        >>> a = mx.sym.var('a')
-        ([numpy.float32, numpy.float32], [numpy.float32], [])
+        >>> arg_types, out_types, aux_types = c.infer_type(a='float32')
-            Unknown type can be marked as None
+            Type of known arguments in a positional way.
-            Provide keyword arguments of known types.
+            Keyword arguments of known types.
-            The order is in the same order as list_arguments()
+            List of argument types.
-            The order is in the same order as list_outputs()
+            List of output types.
-            The order is in the same order as list_auxiliary_states()
+            List of auxiliary state types.
-        and all outputs.
+        """Infers the shapes of all arguments and all outputs given the known shapes of
-        Inconsistencies in the known shapes will cause an error to be raised.
+        This function takes the known shapes of some arguments in either positional way
-        >>> b = mxnet.sym.var('b')
+        >>> a = mx.sym.var('a')
-        ([(3L, 3L), (3L, 3L)], [(3L, 3L)], [])
+        >>> arg_shapes, out_shapes, aux_shapes = c.infer_shape(a=(3,3))
-            Unknown shape can be marked as None
+            Shape of arguments in a positional way.
-            Provide keyword arguments of known shapes.
+            Keyword arguments of the known shapes.
-            The order is in the same order as list_arguments()
+            List of argument shapes.
-            The order is in the same order as list_outputs()
+            List of output shapes.
-            The order is in the same order as list_auxiliary_states()
+            List of auxiliary state shapes.
-        results can be returned.
+        """Infers the shape partially. This functions works the same way as `infer_shape`,
-        """Get a debug string.
+        """Gets a debug string.
-        """Save symbol into file.
+        """Saves symbol to a file.
-        """Save symbol into a JSON string.
+        """Saves symbol to a JSON string.
-    """Initialize with multiple initializers.
+    """Initialize parameters using multiple initializers.
-        List of regular expression patterns to match parameter names.
+        List of regular expressions matching parameter names.
-        List of Initializer corrosponding to patterns.
+        List of initializers corresponding to `patterns`.
-    def __init__(self, batch_size, frequent=50):
+    def __init__(self, batch_size, frequent=50, auto_reset=True):
-                    param.eval_metric.reset()
+                    if self.auto_reset:
-import os, pickle, gzip
+import os, pickle, gzip, argparse
-batch_size = 100
+def get_model(use_gpu):
-mp1 = mx.symbol.Pooling(data = act1, name = 'mp1', kernel=(2,2), stride=(2,2), pool_type='max')
+    conv2= mx.symbol.Convolution(data = mp1, name='conv2', num_filter=32, kernel=(3,3), stride=(2,2))
-mp2 = mx.symbol.Pooling(data = act2, name = 'mp2', kernel=(2,2), stride=(2,2), pool_type='max')
+    fl = mx.symbol.Flatten(data = mp2, name="flatten")
-softmax = mx.symbol.SoftmaxOutput(data = fc2, name = 'sm')
+    num_epoch = 1
-get_data.GetMNIST_ubyte()
+def get_iters():
-        batch_size=batch_size, shuffle=True, flat=False, silent=False)
+    batch_size = 100
-
+# run as a script
-    test_mnist()
+    parser = argparse.ArgumentParser()
-            Values corresponding to the Keys
+            Values corresponding to the keys.
-            Keys
+            Keys.
-            Values corresponding to the Keys
+            Values corresponding to the keys.
-            other push actions
+            other push actions.
-        The returned values are gauranteed to the latest values in the store.
+        The returned values are gauranteed to be the latest values in the store.
-            Values corresponding to the Keys.
+            Values corresponding to the keys.
-            other pull actions
+            other pull actions.
-            the optimizer
+            the optimizer.
-            the head of the command
+            the head of the command.
-            the body of the command
+            the body of the command.
-        The type of KVStore
+        The type of KVStore.
-        for k, v in callback_args.iteritems():
+        for k, v in callback_args.items():
-    inputs = {k: array(v) for k, v in inputs.iteritems()}
+    inputs = {k: array(v) for k, v in inputs.items()}
-    if {name} is not _Null:
+    if {name} is not None:
-
+    # test ability to turn off training on bias
-    cgroup.add_argument('--exts', type=list, default=['.jpeg', '.jpg'],
+    cgroup.add_argument('--exts', nargs='+', default=['.jpeg', '.jpg'],
-
+import math
-        self.num_inst += 1
+        self.sum_metric += loss
-    pass
+    def __repr__(self):
-        assert(same(s1.asnumpy(), s2.asnumpy()))
+    if state1 is not None and state2 is not None:
-        assert_almost_equal(s1.asnumpy(), s2.asnumpy(), rtol=1e-4, atol=1e-5)
+    if state1 is not None and state2 is not None:
-from .ndarray import NDArray, zeros, clip, sqrt
+from .ndarray import NDArray, zeros, clip, sqrt, sign
-    return mx.symbol.SoftmaxOutput(data=fc1, name='softmax', label=label)
+    fc1 = mx.symbol.Cast(data=fc1, dtype=np.float32)
-# pylint: disable=invalid-name, protected-access, too-many-arguments,  global-statement
+# pylint: disable=invalid-name, protected-access, too-many-arguments
-from ..base import c_array, py_str, c_str, mx_uint
+from ..base import c_array, py_str, c_str, mx_uint, _Null
-    idtype = None
+
-        dtype = arg_types[i]
+        name, atype = arg_names[i], arg_types[i]
-                vals.append(np.dtype(val).name)
+            dtype_name = name
-            return _ndarray_cls(ctypes.cast(output_vars[0], NDArrayHandle))
+                ndsignature.append('%s=None'%name)
-    return generic_ndarray_function
+            signature.append('%s=_Null'%name)
-    """Turn on or turn off operator recording.
+
-    recording: bool
+    is_train: bool
-        ctypes.c_int(recording)))
+    def __init__(self, enter_state):
-        set_recording(False)
+        with train():
-from mxnet.contrib.autograd import grad, grad_and_loss
+from mxnet.contrib.autograd import grad, grad_and_loss, train, test
-            'inputs': [ ('lambda', [ [ 1.0, 8.5 ], [ 2.7 , 0.5 ] ]) ],
+            'params': { 'lam': 4.0 },
-                ('std', lambda x, params: np.std(x) - 1.0 / params['lambda'], tol)
+                ('mean', lambda x, params: np.mean(x) - 1.0 / params['lam'], tol),
-            'inputs': [ ('lambda', [ [ 1.0, 8.5 ], [ 2.7 , 0.5 ] ]) ],
+            'params': { 'lam': 4.0 },
-                ('std', lambda x, params: np.std(x) - np.sqrt(params['lambda']), tol)
+                ('mean', lambda x, params: np.mean(x) - params['lam'], tol),
-    Deprecated, use ``one_hot`` instead.
+    .. note:: `onehot_encode` is deprecated. Use `one_hot` instead.
-"""Read and write for the RecrodIO data format."""
+"""Read and write for the RecordIO data format."""
-    """Read/write RecordIO formmat data.
+    """Read/write RecordIO format data.
-    """Read/write RecordIO formmat data supporting random access.
+    """Read/write RecordIO format data supporting random access.
-    check_consistency(sym, ctx_list)
+    # wider tolerance needed for true-fp16 NCHW test above
-    check_consistency(sym, ctx_list)
+    sym_no_cudnn = mx.sym.Convolution(num_filter=3, kernel=(3,), pad=(1,), cudnn_off=True, name='conv')
-    check_consistency(sym, ctx_list)
+    sym_no_cudnn = mx.sym.Convolution(num_filter=3, kernel=(3,), stride=(2,), cudnn_off=True, name='conv')
-    check_consistency(sym, ctx_list)
+    sym_no_cudnn = mx.sym.Convolution(num_filter=3, kernel=(3,), dilate=(2,), cudnn_off=True, name='conv')
-    check_consistency(sym, ctx_list)
+    sym_no_cudnn = mx.sym.Convolution(num_filter=3, kernel=(3,3), pad=(1,1), cudnn_off=True, name='conv')
-    check_consistency(sym, ctx_list)
+    sym_no_cudnn = mx.sym.Convolution(num_filter=3, kernel=(3,3), stride=(2,2), cudnn_off=True, name='conv')
-    check_consistency(sym, ctx_list)
+    sym_no_cudnn = mx.sym.Convolution(num_filter=3, kernel=(3,3), dilate=(2,2), cudnn_off=True, name='conv')
-    check_consistency(sym, ctx_list)
+    sym_no_cudnn = mx.sym.Convolution(num_filter=3, kernel=(2,3,3), pad=(1,1,1), cudnn_off=True, name='conv')
-
+    sym_no_cudnn = mx.sym.Convolution(num_filter=3, kernel=(2,3,3), stride=(2,2,2), cudnn_off=True, name='conv')
-
+    # wider tolerance needed for true-fp16 test above
-    Return ctype arrays for the key-value args, for internal use.
+    Returns ctype arrays for the key-value args. For internal use.
-        """Initialize a new KVStore.
+        """Initializes a new KVStore.
-        """ Initialize a single or a sequence of key-value pairs into the store.
+        """ Initializes a single or a sequence of key-value pairs into the store.
-        This function returns after data have been initialized successfully.
+        For each key, one must `init` it before calling `push` or `pull`.
-            The values.
+            Values corresponding to the Keys
-        1. This function returns after adding an operator to the engine.
+        """ Pushes a single or a sequence of key-value pairs into the store.
-        3. There is no synchronization between workers. One can use ``_barrier()``
+        This function returns immediately after adding an operator to the engine.
-            According values
+            Values corresponding to the Keys
-            to be executed before other push actions.
+            Higher priority push operations are likely to be executed before
-        Data consistency:
+        """ Pulls a single value or a sequence of values from the store.
-        further read on out will be blocked until it is finished.
+        This function returns immediately after adding an operator to the engine.
-        key are finished.
+        `pull` is executed asynchronously after all previous `push` and `pull` calls
-        3. It pulls the newest value from the store.
+        The returned values are gauranteed to the latest values in the store.
-            According values.
+            Values corresponding to the Keys.
-            to be executed before other push actions.
+            The priority of the pull operation.
-        """Register an optimizer to the store
+        """ Registers an optimizer with the store.
-        will pack this optimizer and send it to all servers. It returns after
+        When there are multiple machines, this operation (invoked from a worker node)
-        """Get the type of this kvstore.
+        """ Returns the type of this kvstore.
-        """Get the rank of this worker node.
+        """ Returns the rank of this worker node.
-            The rank of this node, which is in [0, get_num_workers())
+            The rank of this node, which is in range [0, num_workers())
-        """Get the number of worker nodes.
+        """Returns the number of worker nodes.
-        """Save optimizer (updater) state to file.
+        """Saves optimizer (updater) state to file.
-        """Load optimizer (updater) state from file.
+        """Loads optimizer (updater) state from file.
-        """Set a push updater into the store.
+        """Sets a push updater into the store.
-        multiple machines.
+        This function only changes the local store. When running on multiple machines one must
-        """Global barrier among all worker nodes.
+        """Invokes global barrier among all worker nodes.
-        finished.
+        For example, assume there are `n` machines. We would like machine `0` to first
-        """Send a command to all server nodes.
+        """Sends a command to all server nodes.
-        ``KVStoreServer.controller``
+        Sending command to a server node will cause that server node to invoke
-        This function returns after the command has been executed in all server
+        This function returns after the command has been executed on all server
-    """Create a new KVStore.
+    """Creates a new KVStore.
-    """Returns a new array of given shape and type, filled with the given value ``val``.
+    """Returns a new array of given shape and type, filled with the given value `val`.
-        The shape of the empty array.
+    shape : int or tuple of int
-        Fill value
+        Fill value.
-        An optional value type (default is `float32`).
+        Device context (default is the current default context).
-        A created array
+        `NDArray` filled with `val`, with the given shape, ctx, and dtype.
-    """Creates a new array from any object exposing the array interface.
+    """Creates an array from any object exposing the array interface.
-        Any object exposing the array interface, an object whose ``__array__``
+        An object exposing the array interface, an object whose `__array__`
-        An optional device context (default is the current default context).
+        Device context (default is the current default context).
-        ``source_array.dtype``, otherwise default to ``float32``.
+        The data type of the output array. The default dtype is ``source_array.dtype``
-        An ``NDArray`` array with the same contets as the ``source_array``.
+        An `NDArray` with the same contents as the `source_array`.
-    >>> mx.nd.array(np.zeros((3,2)))
+    >>> mx.nd.array(np.zeros((3, 2)))
-    >>> mx.nd.array(np.zeros((3,2)), mx.gpu(0))
+    >>> mx.nd.array(np.zeros((3, 2)), mx.gpu(0))
-    and to ``numpy.arange``, but returns an ``NDArray``.
+    Values are generated within the half-open interval [`start`, `stop`). In other
-        A optional spacing between values, the default value is 1.
+    start : float, optional
-        The repeating time of all elements.
+        Number of times to repeat each element. The default repeat count is 1.
-
+        Device context. Default context is the current default context.
-        The value type of the NDArray, default to np.float32.
+        The data type of the `NDArray`. The default datatype is `np.float32`.
-        The created NDArray
+        `NDArray` of evenly spaced values in the specified range.
-    >>> mx.nd.arange(2,6).asnumpy()
+    >>> mx.nd.arange(2, 6).asnumpy()
-    >>> mx.nd.arange(2,6,step=2).asnumpy()
+    >>> mx.nd.arange(2, 6, step=2).asnumpy()
-    >>> mx.nd.arange(2,6,step=2,repeat=3,dtype='int32').asnumpy()
+    >>> mx.nd.arange(2, 6, step=1.5, repeat=2).asnumpy()
-    ret = np.array([])
+    ret = []
-        ret = np.concatenate([ret, gen_feature(c)])
+        ret.append(gen_feature(c))
-        self.provide_data = [('data', (batch_size, 10 * 80))] + init_states
+        self.provide_data = [('data', (batch_size, 80, 10))] + init_states
-    """Randomly crop src with size. Upsample result if src is smaller than size."""
+    """Centrally crop src with size. Upsample result if src is smaller than size."""
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    """Returns the result of element-wise **equal to**(==) comparison operation with
+    """Returns the result of element-wise **equal to** (==) comparison operation with
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    """Returns the result of element-wise **not equal to**(!=) comparison operation
+    """Returns the result of element-wise **not equal to** (!=) comparison operation
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    """Returns the result of element-wise **greater than**(>) comparison operation
+    """Returns the result of element-wise **greater than** (>) comparison operation
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    """Returns the result of element-wise **greater than or equal to**(>=) comparison
+    """Returns the result of element-wise **greater than or equal to** (>=) comparison
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    """Returns the result of element-wise **lesser than**(<) comparison operation with broadcasting.
+    """Returns the result of element-wise **lesser than** (<) comparison operation
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    """Returns the result of element-wise **lesser than or equal to**(<=) comparison
+    """Returns the result of element-wise **lesser than or equal to** (<=) comparison
-    then the arrays are broadcastable to a common shape.
+       If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-    """Same as ``divide``.
+
-                      arg_params=arg_params)
+    def test_embedding_helper(data_types, weight_types, low_pad, high_pad):
-    mu, sigma = 10, 2
+    tol = 0.1
-    assert abs(np.std(ret1.asnumpy()) - sigma) < 0.1
+    for symbdic in symbols:
-LABEL_NAMES = ['cls_prob_label']
+LABEL_NAMES = None
-    cls_prob = mx.symbol.SoftmaxOutput(name='cls_prob', data=cls_score)
+    cls_prob = mx.symbol.softmax(name='cls_prob', data=cls_score)
-    cls_prob = mx.symbol.SoftmaxOutput(name='cls_prob', data=cls_score)
+    cls_prob = mx.symbol.softmax(name='cls_prob', data=cls_score)
-    cls_prob = mx.symbol.SoftmaxOutput(name='cls_prob', data=cls_score)
+    cls_prob = mx.symbol.softmax(name='cls_prob', data=cls_score)
-    label_names = ['cls_prob_label']
+    label_names = None
-        """Broadcasts an array to a new shape.
+        """Broadcasts the input array to a new shape.
-        the number of dimensions such as from 2D to 3D.
+        Broadcasting is only allowed on axes with size 1. The new shape cannot change
-    """Add arguments, element-wise with broadcasting.
+    """Returns element-wise sum of the input arrays with broadcasting.
-    Equivalent to ``lhs + rhs``
+    Equivalent to ``lhs + rhs``, ``mx.nd.broadcast_add(lhs, rhs)`` and
-        broadcastable to a common shape
+         Second array to be added.
-        The sum of lhs and rhs, element-wise.
+        The element-wise sum of the input arrays.
-    """Subtracts arguments element-wise with broadcasting.
+    """Returns element-wise difference of the input arrays with broadcasting.
-    Equivalent to ``lhs - rhs``.
+        If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array to be subtracted.
-        The difference of lhs and rhs, element-wise.
+        The element-wise difference of the input arrays.
-    """Multiplies arguments element-wise with broadcasting.
+    """Returns element-wise product of the input arrays with broadcasting.
-    Equivalent to ``lhs * rhs``.
+        If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array to be multiplied.
-        The multiplication of lhs and rhs, element-wise.
+        The element-wise multiplication of the input arrays.
-    """Divides arguments element-wise with broadcasting.
+    """Returns element-wise division of the input arrays with broadcasting.
-    Equivalent to ``lhs / rhs``.
+        If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array in division.
-        The quotient of ``lhs/rhs``, element-wise.
+        The element-wise division of the input arrays.
-    >>> z = mx.nd.arange(2).reshape((1,2))
+    >>> x = mx.nd.ones((2,3))*6
-           [ 0.5,  0.5,  0.5]], dtype=float32)
+    >>> (x/3).asnumpy()
-           [ inf,   1.]], dtype=float32)
+    array([[ 3.,  3.,  3.],
-    """First array elements raised to powers from second array, element-wise
+    """Returns result of first array elements raised to powers from second array, element-wise
-    Equivalent to ``base ** exp``.
+    Equivalent to ``base ** exp`` and ``mx.nd.broadcast_power(lhs, rhs)``.
-        The arrays to be added. If ``base.shape != exp.shape``, they must be
+         The exponent array. If ``base.shape != exp.shape``, they must be
-    """Element-wise maximum of array elements with broadcasting.
+    """Returns element-wise maximum of the input arrays with broadcasting.
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be
-        The maximum of lhs and rhs, element-wise.
+        The element-wise maximum of the input arrays.
-    """Element-wise minimum of array elements with broadcasting.
+    """Returns element-wise minimum of the input arrays with broadcasting.
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be
-        The minimum of lhs and rhs, element-wise.
+        The element-wise minimum of the input arrays.
-    """Returns (lhs == rhs), element-wise with broadcasting.
+    """Returns the result of element-wise **equal to**(==) comparison operation with
-    Equivalent to ``lhs == rhs``
+    .. note::
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be
-        For each element in lhs, rhs, return True if lhs is equal to rhs and False otherwise.
+        Output array of boolean values.
-    """Returns (lhs != rhs), element-wise with broadcasting.
+    """Returns the result of element-wise **not equal to**(!=) comparison operation
-    Equivalent to ``lhs != rhs``.
+        If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-        broadcastable to a common shape,
+         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be
-        For each element in lhs, rhs, return True if lhs is not equal to rhs and False otherwise.
+        Output array of boolean values.
-    """Returns (lhs > rhs), element-wise with broadcasting.
+    """Returns the result of element-wise **greater than**(>) comparison operation
-    Equivalent to ``lhs > rhs``.
+    For each element in input arrays, return 1(true) if lhs elements are greater than rhs,
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be
-        For each element in lhs, rhs, return True if lhs is greater than rhs and False otherwise.
+        Output array of boolean values.
-    """Returns (lhs >= rhs), element-wise with broadcasting.
+    """Returns the result of element-wise **greater than or equal to**(>=) comparison
-    Equivalent to ``lhs >= rhs``.
+    Equivalent to ``lhs >= rhs`` and ``mx.nd.broadcast_greater_equal(lhs, rhs)``.
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be
-        rhs and False otherwise.
+        Output array of boolean values.
-    """Returns (lhs < rhs), element-wise with broadcasting.
+    """Returns the result of element-wise **lesser than**(<) comparison operation with broadcasting.
-    Equivalent to ``lhs < rhs``.
+        If the corresponding dimensions of two arrays have the same size or one of them has size 1,
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be
-        For each element in lhs, rhs, return True if lhs is lesser than rhs and False otherwise.
+        Output array of boolean values.
-    """Returns (lhs <= rhs), element-wise with broadcasting.
+    """Returns the result of element-wise **lesser than or equal to**(<=) comparison
-    Equivalent to ``lhs <= rhs``.
+    .. note::
-        The arrays to be added. If ``lhs.shape != rhs.shape``, they must be
+         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be
-        rhs and False otherwise.
+        Output array of boolean values.
-    """Turn on or turn of operator recording.
+    """Turn on or turn off operator recording.
-    grad_reqs: list of string
+    grad_reqs: list of string
-def grad_and_loss(func):
+def grad_and_loss(func, argnum=None):
-        for x in args:
+        variables = args
-        mark_variables(args, grads)
+        grads = [zeros_like(x) for x in variables]
-def grad(func):
+def grad(func, argnum=None):
-    grad_with_loss_func = grad_and_loss(func)
+    grad_with_loss_func = grad_and_loss(func, argnum)
-    f = kwargs["func"]
+    func   = kwargs["func"]
-    grad_func = grad_and_loss(f)
+    grad_func = grad_and_loss(func, argnum)
-    res = f(*args)
+    res = func(*args)
-            prob = prob.asnumpy()
+            label = label.as_in_context(pred.context).reshape((label.size,))
-        self.sum_metric += numpy.exp(loss / num)
+                ignore = label == self.ignore_label
-        if state:
+        if state is not None:
-from ..base import _LIB, check_call
+from ..base import _LIB, check_call, string_types
-from ..ndarray import NDArray
+from ..ndarray import NDArray, zeros_like
-def mark_variables(variables):
+def mark_variables(variables, gradients, grad_reqs='write'):
-    for var in variables:
+    gradient_handles = []
-        c_array(NDArrayHandle, variable_handles)))
+        c_array(NDArrayHandle, variable_handles),
-    return [NDArray(NDArrayHandle(grad_handles[i])) for i in range(num_grad.value)]
+        c_array(NDArrayHandle, output_handles)))
-        mark_variables(args)
+        grads = [zeros_like(x) for x in args]
-        return grad_vals, outputs
+        compute_gradient([outputs] if isinstance(outputs, NDArray) else outputs)
-            reqs_array = c_array(mx_uint, [mx_uint(req_map[grad_req])] * len(listed_arguments))
+            if grad_req not in _GRAD_REQ_MAP:
-            reqs_array = c_array(mx_uint, [mx_uint(req_map[item]) for item in grad_req])
+            reqs_array = c_array(mx_uint, [mx_uint(_GRAD_REQ_MAP[item]) for item in grad_req])
-                    req_array.append(mx_uint(req_map[grad_req[name]]))
+                    req_array.append(mx_uint(_GRAD_REQ_MAP[grad_req[name]]))
-import find_mxnet
+
-        os.environ["MXNET_EXEC_BULK_BWD_TRAIN"] = "0"
+        prev_bulk_inf_val = mx.test_utils.set_env_var("MXNET_EXEC_BULK_EXEC_INFERENCE", "0", "1")
-       os.environ["MXNET_EXEC_BULK_BWD_TRAIN"] = prev_bwd_var
+       mx.test_utils.set_env_var("MXNET_EXEC_BULK_EXEC_INFERENCE", prev_bulk_inf_val)
-        Shape of target NDArray.
+        Shape of target `NDArray`.
-        The device id of the device. ``device_id`` is not needed for CPU.
+        The device id of the device. `device_id` is not needed for CPU.
-            ExecutorHandle generated by calling Bind.
+            ExecutorHandle generated by calling `bind`.
-        in the same order as ctx.
+        in the same order as `ctx`.
-            KVStore handle of C API.
+            `KVStore` handle of C API.
-        2. ``push`` is always called after all previous push and pull on the same
+        2. `push` is always called after all previous push and pull on the same
-        3. There is no synchronization between workers. One can use _barrier()
+        3. There is no synchronization between workers. One can use ``_barrier()``
-        2. ``pull`` is always called after all previous push and pull on the same
+        2. `pull` is always called after all previous push and pull on the same
-        out: NDArray or list of NDArray or list of lists of NDArrays
+        out: NDArray or list of NDArray or list of list of NDArray
-        KVStoreServer.controller
+        ``KVStoreServer.controller``
-        Model parameter, dict of name to ``NDArray`` of net's weights.
+    arg_params : dict of str to `NDArray`.
-    """ Initialize kvstore"""
+    """Initialize kvstore"""
-    """ Perform update of param_arrays from grad_arrays on kvstore."""
+    """Perform update of param_arrays from grad_arrays on kvstore."""
-    """ Perform update of param_arrays from grad_arrays not on kvstore."""
+    """Perform update of param_arrays from grad_arrays not on kvstore."""
-    - This function will inplace update the ``NDArrays`` in ``arg_params`` and ``aux_states``.
+    - This function will inplace update the NDArrays in `arg_params` and `aux_states`.
-        """Run the model on X and calculate the score with eval_metric.
+        """Run the model given an input and calculate the score
-            The number of batch to run. Go though all batches if ``None``.
+            The number of batches to run. Go though all batches if ``None``.
-            Training data. If X is a DataIter, the name or (if name not available)
+            Training data. If `X` is a `DataIter`, the name or (if name not available)
-            If X is numpy.ndarray/NDArray, y is required to be set.
+            If X is ``numpy.ndarray`` or `NDArray`, `y` is required to be set.
-            the same as X, i.e. the number of data points and labels should be equal.
+            the same as `X`, i.e. the number of data points and labels should be equal.
-            it should be (valid_data, valid_label).
+            it should be ``(valid_data, valid_label)``.
-            in the same order as ctx.
+            in the same order as `ctx`.
-        The advantage of ``load` and ``save`` (as compared to ``pickle``) is that
+        You can also use `pickle` to do the job if you only work on Python.
-        One can also directly ``load``/``save` from/to cloud storage(S3, HDFS)
+        One can also directly `load`/`save` from/to cloud storage(S3, HDFS)
-            Other parameters for model, including ``num_epoch``, optimizer and ``numpy_batch_size``.
+            Other parameters for model, including `num_epoch`, optimizer and `numpy_batch_size`.
-            If X is a ``numpy.ndarray``, y must be set.
+            If `X` is a ``numpy.ndarray``, `y` must be set.
-            be (``valid_data``, ``valid_label``).
+            If `eval_set` is ``numpy.ndarray`` pair, it should
-            in the same order as ctx.
+            in the same order as `ctx`.
-easier to be composed. So it is more like the Torch modules.
+"""A module is like a FeedForward model. But we would like to make it
-    """Check that all input names are in symbol's argument"""
+    """Check that all input names are in symbol's arguments."""
-    """Check that input names matches input data descriptors"""
+    """Check that input names matches input data descriptors."""
-    """The base class of a modules.
+    """The base class of a module.
-    especially in the case when we need to use imperative API to work with
+    A module represents a computation component. Modules are designed so that
-    - Parameter initialized. For modules with parameters, doing computation before initializing
+    - Initial state: Memory is not allocated yet, thus the moule is not ready for computation yet.
-    - Optimizer installed. An optimizer can be installed to a module. After this, the parameters
+    - Optimizer installed: An optimizer can be installed to a module. After this, the parameters
-    following information in its raw stage (before binded)
+    In order for a module to interact with others, it must be able to report the
-    - `output_names`: list of string indicating the names of required outputs.
+    - `data_names`: list of type string indicating the names of the required input data.
-    And also the following richer information after binded:
+    After binding, a modulse should be able to report the following richer information:
-        - `params_initialized`: `bool`, indicating whether the parameters of this modules
+        - `binded`: `bool`, indicates whether the memory buffers needed for computation
-        - `optimizer_initialized`: 'bool`, indicating whether an optimizer is defined
+        - `optimizer_initialized`: `bool`, indicates whether an optimizer is defined
-          input data is needed. Might be useful when implementing composition of modules.
+        - `inputs_need_grad`: `bool`, indicates whether gradients with respect to the
-          we could directly provide the data arrays. But in the case of data parallelization,
+          we could directly provide the data arrays. But in the case of data parallelism,
-          is not binded for training.
+          is not bound for training.
-          is a dictionary of name to `NDArray` mapping. Those `NDArray` always lives on
+          is a dictionary of name to ``NDArray`` mapping. Those `NDArray` always lives on
-        - `set_params(arg_params, aux_params)`: assign parameters to the devices
+        - ``set_params(arg_params, aux_params)``: assign parameters to the devices
-        - `init_params(...)`: a more flexible interface to assign or initialize the parameters.
+        - ``init_params(...)``: a more flexible interface to assign or initialize the parameters.
-    - `score`: run prediction on a data set and evaluate performance
+    - `fit`: train the module parameters on a data set.
-        """
+        """A convenient function that calls both ``forward`` and ``backward``."""
-        `eval_metric`.
+        """Run prediction on ``eval_data`` and evaluate the performance according to
-            Number of batches to run. Default is `None`, indicating run until the `DataIter`
+            Number of batches to run. Defaults to ``None``, indicating run until the `DataIter`
-            Default `True`, indicating whether we should reset `eval_data` before starting
+            Defaults to ``True``. Indicates whether we should reset `eval_data` before starting
-            training, this will correspond to the training epoch number.
+            Defaults to 0. For compatibility, this will be passed to callbacks (if any).
-            Default is `None`, indicating running all the batches in the data iterator.
+            Default is ``None``, indicating running all the batches in the data iterator.
-            Default is `True`, indicating whether we should reset the data iter before start
+            Default is ``True``, indicating whether we should reset the data iter before start
-        then in the case of a single output, `out1` is returned instead of `[out1]`.
+        When `merge_batches` is ``True`` (by default), the return value will be a list
-        `[[out1_batch1, out2_batch1], [out1_batch2], ...]`. This mode is useful because
+        When `merge_batches` is ``False``, the return value will be a nested list like
-        just call `.asnumpy()` on each of the `NDArray`.
+        The objects in the results have type `NDArray`. If you need to work with a numpy array,
-            Default is `None`, indicating running all the batches in the data iterator.
+            Defaults to ``None``, indicating running all the batches in the data iterator.
-            Default is `True`, see the doc for return values.
+            Defaults to ``True``, see above for return values.
-            Default is `True`, indicating whether we should reset the data iter before start
+            Defaults to ``True``, indicating whether we should reset the data iter before start
-            Default is `False`, see the doc for return values.
+            Defaults to ``False``, see above for return values.
-            Predict results
+            Prediction results.
-            If not `None`, will be used as validation set and evaluate the performance
+            If not ``None``, will be used as validation set and evaluate the performance
-            Default `'accuracy'`. The performance measure used to display during training.
+            Defaults to 'accuracy'. The performance measure used to display during training.
-        epoch_end_callback : function or list of function
+        epoch_end_callback : function or list of functions
-            Default `'local'`.
+            Defaults to 'local'.
-            Default `'sgd'`
+            Defaults to 'sgd'
-            The default value is not a `dict`, just to avoid pylint warning on dangerous
+            Defaults to ``(('learning_rate', 0.01),)``. The parameters for
-            These will be called at the end of each minibatch during evaluation
+            These will be called at the end of each minibatch during evaluation.
-            Will be called to initialize the module parameters if not already initialized.
+            The initializer is called to initialize the module parameters when they are
-            Default `None`, if not `None`, should be existing parameters from a trained
+            Defaults to ``None``, if not ``None``, should be existing parameters from a trained
-            Default `None`. Similar to `arg_params`, except for auxiliary states.
+            Defaults to ``None``. Similar to `arg_params`, except for auxiliary states.
-            and `aux_params` are not `None`. If this is `True`, then the missing parameters
+            Defaults to ``False``. Indicate whether we allow missing parameters when `arg_params`
-            Default `False`. Whether to force rebinding the executors if already binded.
+            Defaults to ``False``. Whether to force rebinding the executors if already bound.
-            Default `False`. Indicate whether we should force initialization even if the
+            Defaults to ``False``. Indicate whether we should force initialization even if the
-            Default `0`. Indicate the starting epoch. Usually, if we are resuming from a
+            Defaults to 0. Indicate the starting epoch. Usually, if we are resuming from a
-        list `[]`.
+        function, or it is not bound for training, then this should return an empty
-            a pair of dictionary of name to value mapping.
+        ``(arg_params, aux_params)``
-            If not None, should be a dictionary of existing arg_params. Initialization
+            If not ``None``, should be a dictionary of existing `arg_params`. Initialization
-            If not None, should be a dictionary of existing aux_params. Initialization
+            If not ``None``, should be a dictionary of existing `aux_params`. Initialization
-            If true, params could contain missing values, and the initializer will be
+            If ``True``, params could contain missing values, and the initializer will be
-            If true, will force re-initialize even if already initialized.
+            If ``True``, `force_init` will force re-initialize even if already initialized.
-            If true, params could contain missing values, and the initializer will be
+            If ``True``, params could contain missing values, and the initializer will be
-            If true, will force re-initialize even if already initialized.
+            If ``True``, will force re-initialize even if already initialized.
-        elements are `NDArray`.
+        If `merge_multi_context` is ``True``, returns output of form ``[out1, out2]``.
-            will be collected from multiple devices. A `True` value indicate that we
+            Defaults to ``True``. In the case when data-parallelism is used, the states
-            States
+        A list of ``NDArray`` or a list of list of ``NDArray``.
-            [state2_dev1, state2_dev2]].
+        states : list of list of NDArray
-            a single scalar value for all state arrays.
+            A single scalar value for all state arrays.
-        """Install monitor on all executors"""
+        """Install monitor on all executors."""
-            Default is `None`, which means `is_train` takes the value of `self.for_training`.
+            Default is ``None``, which means `is_train` takes the value of ``self.for_training``.
-        might live on different devices.
+        If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise,
-            will be collected from multiple devices. A `True` value indicate that we
+            Defaults to ``True``. In the case when data-parallelism is used, the outputs
-        list of NDArray or list of list of NDArray
+        list of `NDArray` or list of list of `NDArray`.
-        might live on different devices.
+        If `merge_multi_context` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
-            will be collected from multiple devices. A `True` value indicate that we
+            Defaults to ``True``. In the case when data-parallelism is used, the gradients
-              Input gradients
+              Input gradients.
-            Typically is `data_iter.provide_data`.
+            Typically is ``data_iter.provide_data``.
-            Typically is `data_iter.provide_label`.
+            Typically is ``data_iter.provide_label``.
-            Default is `True`. Whether the executors should be bind for training.
+            Default is ``True``. Whether the executors should be bind for training.
-            Default is `False`. Whether the gradients to the input data need to be computed.
+            Default is ``False``. Whether the gradients to the input data need to be computed.
-            binded. But with this `True`, the executors will be forced to rebind.
+            Default is ``False``. This function does nothing if the executors are already
-            Default is `None`. This is used in bucketing. When not `None`, the shared module
+            Default is ``None``. This is used in bucketing. When not ``None``, the shared module
-            Default `'local'`.
+            Defaults to `'local'`.
-            Default `'sgd'`
+            Defaults to `'sgd'`
-            Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,
+            Defaults to ``(('learning_rate', 0.01),)``. The default value is not a dictionary,
-            Default `False`, indicating whether we should force re-initializing the
+            Defaults to ``False``, indicating whether we should force re-initializing the
-    """A bucketing module is a module that support bucketing.
+    """This module helps to deal efficiently with varying-length inputs.
-        `(symbol, data_names, label_names)`.
+        ``(symbol, data_names, label_names)``.
-        Default `cpu()`
+        Defaults to ``mx.cpu()``
-        Default `None`, indicating uniform workload.
+        Defaults to ``None``, indicating uniform workload.
-        Default `None`, indicating no network parameters are fixed.
+        Defaults to ``None``, indicating no network parameters are fixed.
-        states are similar to data and label, but not provided by data iterator.
+        States are similar to data and label, but not provided by data iterator.
-        the module does not need labels, or if the module is not binded for
+        A list of `(name, shape)` pairs. The return value could be ``None`` if
-        `NDArray`) mapping.
+        `(arg_params, aux_params)`, each a dictionary mapping names to parameters
-            Default `None`. Existing parameters. This has higher priority than `initializer`.
+            Defaults to ``None``. Existing parameters. This has higher priority
-            Default `None`. Existing auxiliary states. This has higher priority than `initializer`.
+            Defaults to ``None``. Existing auxiliary states. This has higher priority
-            missing values will be filled with `initializer`.
+            Allow missing values in `arg_params` and `aux_params` (if not ``None``).
-            Default `False`.
+            Defaults to ``False``.
-        is like `[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]`. All the output
+        If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it
-            [state2_dev1, state2_dev2]].
+            Source states arrays formatted like ``[[state1_dev1, state1_dev2],
-            a single scalar value for all state arrays.
+            A single scalar value for all state arrays.
-        """Binding for a `BucketingModule` means setting up the buckets and bind the
+        """Binding for a `BucketingModule` means setting up the buckets and binding the
-        binded afterwards with `switch_bucket`.
+        bound afterwards with `switch_bucket`.
-            Default is `True`.
+            Default is ``True``.
-            Default is `False`.
+            Default is ``False``.
-            Default is `False`.
+            Default is ``False``.
-            Default is `None`. This value is currently not used.
+            Default is ``None``. This value is currently not used.
-            self.logger.warning('Already binded, ignoring bind()')
+            self.logger.warning('Already bound, ignoring bind()')
-        """Switch to a different bucket. This will change `self.curr_module`.
+        """Switch to a different bucket. This will change ``self.curr_module``.
-            Typically `data_batch.provide_data`.
+            Typically ``data_batch.provide_data``.
-            Typically `data_batch.provide_label`.
+            Typically ``data_batch.provide_label``.
-            Default `'local'`.
+            Defaults to `'local'`.
-            Default `'sgd'`
+            Defaults to `'sgd'`
-            Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,
+            Defaults to `(('learning_rate', 0.01),)`. The default value is not a dictionary,
-            Default `False`, indicating whether we should force re-initializing the
+            Defaults to ``False``, indicating whether we should force re-initializing the
-            Default is `None`, in which case `is_train` is take as `self.for_training`.
+            Defaults to ``None``, in which case `is_train` is take as ``self.for_training``.
-            will be collected from multiple devices. A `True` value indicate that we
+            Defaults to ``True``. In the case when data-parallelism is used, the outputs
-        is like `[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]`. All the output
+        If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it
-            will be collected from multiple devices. A `True` value indicate that we
+            Defaults to ``True``. In the case when data-parallelism is used, the outputs
-        is like `[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]`. All the output
+        If `merge_multi_context` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
-            Typically `data_batch.label`.
+            Typically ``data_batch.label``.
-    """Load a list of arrays into a list of arrays specified by slices"""
+    """Load a list of arrays into a list of arrays specified by slices."""
-    """Load data into sliced arrays"""
+    """Load data into sliced arrays."""
-    """Load label into sliced arrays"""
+    """Load label into sliced arrays."""
-    """DataParallelExecutorGroup is a group of executors that lives on a group of devices.
+    """A group of executors that lives on a group of devices.
-        If not `None`, could be a list of numbers that specify the workload to be assigned
+        If not ``None``, could be a list of numbers that specify the workload to be assigned
-        Default is `None`. This is used in bucketing. When not `None`, it should be a executor
+        Defaults to ``None``. This is used in bucketing. When not ``None``, it should be a executor
-        space for gradient, nor do gradient calculation.
+        Parameters to be fixed during training. For these parameters, not gradients
-            target parameter arrays
+            Target parameter arrays.
-            target aux arrays
+            Target aux arrays.
-        is like `[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]`. All the output
+        If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it
-        """Get states from all devices
+        """Get states from all devices.
-            will be collected from multiple devices. A `True` value indicate that we
+            Default is ``True``. In the case when data-parallelism is used, the states
-        is like `[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]`. All the output
+        If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it
-            Default is `True`. In the case when data-parallelism is used, the outputs
+            Defaults to ``True``. In the case when data-parallelism is used, the outputs
-        is like `[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]`. All the output
+        If `merge_multi_context` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
-        `self.for_training` is `True`.
+        ``self.for_training`` is ``True``.
-            """Internal helper to get a memory block or re-use by re-shaping"""
+            """Internal helper to get a memory block or re-use by re-shaping."""
-        Default is `('data')` for a typical model used in image classification.
+        Defaults to `('data')` for a typical model used in image classification.
-        Default is `('softmax_label')` for a typical model used in image
+        Defaults to `('softmax_label')` for a typical model used in image
-        Default is `logging`.
+        Defaults to `logging`.
-        Default is `cpu()`.
+        Defaults to ``mx.cpu()``.
-        Default `None`, indicating uniform workload.
+        Default ``None``, indicating uniform workload.
-        Default `None`, indicating no network parameters are fixed.
+        Default ``None``, indicating no network parameters are fixed.
-            Default is `cpu()`.
+            Default is ``cpu()``.
-            Default `None`, indicating uniform workload.
+            Default ``None``, indicating uniform workload.
-            Default `None`, indicating no network parameters are fixed.
+            Default ``None``, indicating no network parameters are fixed.
-            the module does not need labels, or if the module is not binded for
+            A list of `(name, shape)` pairs. The return value could be ``None`` if
-            If not None, should be a dictionary of existing aux_params. Initialization
+            If not ``None``, should be a dictionary of existing aux_params. Initialization
-            If true, params could contain missing values, and the initializer will be
+            If ``True``, params could contain missing values, and the initializer will be
-            If true, will force re-initialize even if already initialized.
+            If ``True``, will force re-initialize even if already initialized.
-            Dictionary of name to value (`NDArray`) mapping.
+            Dictionary of name to `NDArray`.
-            Dictionary of name to value (`NDArray`) mapping.
+            Dictionary of name to `NDArray`.
-            If true, params could contain missing values, and the initializer will be
+            If ``True``, params could contain missing values, and the initializer will be
-            If true, will force re-initialize even if already initialized.
+            If ``True````, will force re-initialize even if already initialized.
-            >>>     mx.model.load_checkpoint(model_prefix, n_epoch_load)
+            mx.model.load_checkpoint(model_prefix, n_epoch_load)
-            Typically is `data_iter.provide_data`.
+            Typically is ``data_iter.provide_data``.
-            Typically is `data_iter.provide_label`.
+            Typically is ``data_iter.provide_label``.
-            Default is `True`. Whether the executors should be bind for training.
+            Default is ``True``. Whether the executors should be bound for training.
-            Default is `False`. Whether the gradients to the input data need to be computed.
+            Default is ``False``. Whether the gradients to the input data need to be computed.
-            binded. But with this `True`, the executors will be forced to rebind.
+            Default is ``False``. This function does nothing if the executors are already
-            Default is `None`. This is used in bucketing. When not `None`, the shared module
+            Default is ``None``. This is used in bucketing. When not ``None``, the shared module
-            self.logger.warning('Already binded, ignoring bind()')
+            self.logger.warning('Already bound, ignoring bind()')
-            Typically is `data_iter.provide_data`.
+            Typically is ``data_iter.provide_data``.
-            Typically is `data_iter.provide_label`.
+            Typically is ``data_iter.provide_label``.
-            Default `False`, indicating whether we should force re-initializing the
+            Default ``False``, indicating whether we should force re-initializing the
-            Default is `None`, which means `is_train` takes the value of `self.for_training`.
+            Default is ``None``, which means ``is_train`` takes the value of ``self.for_training``.
-        is like `[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]`. All the output
+        If ``merge_multi_context`` is ``True``, it is like ``[out1, out2]``. Otherwise, it
-            will be collected from multiple devices. A `True` value indicate that we
+            Default is ``True``. In the case when data-parallelism is used, the outputs
-            Output
+            Output.
-        is like `[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]`. All the output
+        If ``merge_multi_context`` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
-            will be collected from multiple devices. A `True` value indicate that we
+            Default is ``True``. In the case when data-parallelism is used, the outputs
-        is like `[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]`. All the output
+        If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it
-            will be collected from multiple devices. A `True` value indicate that we
+            Default is ``True``. In the case when data-parallelism is used, the states
-            [state2_dev1, state2_dev2]].
+            source states arrays formatted like ``[[state1_dev1, state1_dev2],
-            Typically `data_batch.label`.
+            Typically ``data_batch.label``.
-        latest parameters from `self._arg_params` and `self._aux_params`.
+        latest parameters from ``self._arg_params`` and ``self._aux_params``.
-        Names of the labels expected by the module. Could be `None` if the
+        Names of the labels expected by the module. Could be ``None`` if the
-        list `[]`.
+        function, or it is not bound for training, then this should return an empty
-        `({}, {})`, a pair of empty dict. Subclass should override this method if
+        ``({}, {})``, a pair of empty dict. Subclass should override this method if
-            If not None, should be a dictionary of existing arg_params. Initialization
+            If not ``None``, should be a dictionary of existing `arg_params`. Initialization
-            If not None, should be a dictionary of existing aux_params. Initialization
+            If not ``None``, should be a dictionary of existing `aux_params`. Initialization
-            If true, params could contain missing values, and the initializer will be
+            If ``True``, params could contain missing values, and the initializer will be
-            If true, will force re-initialize even if already initialized.
+            If ``True``, will force re-initialize even if already initialized.
-            Typically `data_batch.label`.
+            Typically ``data_batch.label``.
-            Typically is `data_iter.provide_data`.
+            Typically is ``data_iter.provide_data``.
-            Typically is `data_iter.provide_label`.
+            Typically is ``data_iter.provide_label``.
-            Default is `True`. Whether the executors should be bind for training.
+            Default is ``True``. Whether the executors should be bind for training.
-            Default is `False`. Whether the gradients to the input data need to be computed.
+            Default is ``False``. Whether the gradients to the input data need to be computed.
-            binded. But with this `True`, the executors will be forced to rebind.
+            Default is ``False``. This function does nothing if the executors are already
-            Default is `None`. This is used in bucketing. When not `None`, the shared module
+            Default is ``None``. This is used in bucketing. When not ``None``, the shared module
-            self.logger.warning('Already binded, ignoring bind()')
+            self.logger.warning('Already bound, ignoring bind()')
-        outputs. This method can assume that the `data_shapes` and `label_shapes`
+        outputs. This method can assume that the ``data_shapes`` and ``label_shapes``
-        Default `['data']`. Names of the data expected by this module.
+        Defaults to ``['data']``. Names of the data expected by this module.
-        Default `['softmax_label']`. Names of the labels expected by the module.
+        Default ``['softmax_label']``. Names of the labels expected by the module.
-        Optional. If not `None`, should be a function that takes `scores`
+        Optional. If not ``None``, should be a function that takes `scores`
-            Default is `None`, which means `is_train` takes the value of `self.for_training`.
+            Default is ``None``, which means `is_train` takes the value of ``self.for_training``.
-            Should always be `True`, because we do not use multiple contexts for computing.
+            Should always be ``True``, because we do not use multiple contexts for computing.
-        should take `self._scores` and `self._labels` and then compute the
+        should take ``self._scores`` and ``self._labels`` and then compute the
-        `self._scores_grad`.
+        ``self._scores_grad``.
-            Should always be `True` because we do not use multiple context for computation.
+            Should always be ``True`` because we do not use multiple context for computation.
-        """Install monitor on all executors"""
+        """Install monitor on all executors."""
-            the module does not need labels, or if the module is not binded for
+            the module does not need labels, or if the module is not bound for
-            Default `None`. Existing parameters. This has higher priority than `initializer`.
+            Default ``None``. Existing parameters. This has higher priority
-            Default `None`. Existing auxiliary states. This has higher priority than `initializer`.
+            Default ``None``. Existing auxiliary states. This has higher priority
-            missing values will be filled with `initializer`.
+            Allow missing values in `arg_params` and `aux_params` (if not ``None``).
-            Default `False`.
+            Default ``False``.
-            Default is `True`. Whether the executors should be bind for training.
+            Default is ``True``. Whether the executors should be bind for training.
-            Default is `False`. Whether the gradients to the input data need to be computed.
+            Default is ``False``. Whether the gradients to the input data need to be computed.
-            binded. But with this `True`, the executors will be forced to rebind.
+            Default is ``False``. This function does nothing if the executors are already
-            Default is `None`. Currently shared module is not supported for `SequentialModule`.
+            Default is ``None``. Currently shared module is not supported for `SequentialModule`.
-            self.logger.warning('Already binded, ignoring bind()')
+            self.logger.warning('Already bound, ignoring bind()')
-            Default `(('learning_rate', 0.01),)`. The default value is not a dictionary,
+            Default ``(('learning_rate', 0.01),)``. The default value is not a dictionary,
-            Default `False`, indicating whether we should force re-initializing the
+            Default ``False``, indicating whether we should force re-initializing the
-            Default is `None`, in which case `is_train` is take as `self.for_training`.
+            Default is ``None``, in which case `is_train` is take as ``self.for_training``.
-            will be collected from multiple devices. A `True` value indicate that we
+            Default is ``True``. In the case when data-parallelism is used, the outputs
-            out2_dev2]]`. All the output elements are numpy arrays.
+            If `merge_multi_context` is ``True``, it is like ``[out1,
-            will be collected from multiple devices. A `True` value indicate that we
+            Default is ``True``. In the case when data-parallelism is used, the outputs
-            is like `[[grad1_dev1, grad1_dev2], [grad2_dev1, grad2_dev2]]`. All the output
+            If `merge_multi_context` is ``True``, it is like ``[grad1, grad2]``. Otherwise, it
-            Typically `data_batch.label`.
+            Typically ``data_batch.label``.
-        """ Install monitor on all executors """
+        """ Install monitor on all executors."""
-        Takes an ``NDArray`` and returns an ``NDArray``. Defaults to mean
+        Takes an `NDArray` and returns an `NDArray`. Defaults to mean
-        Only tensors with names that match ``name_pattern`` will be included.
+        Only tensors with names that match `name_pattern` will be included.
-    User can also inherit this object to change naming behavior.
+    Developers can also inherit from this class to change naming behavior.
-        the user specified name will be used.
+        This is the default implementation.
-        name based on hint string.
+        When user does not specify a name, we automatically generate a
-            The name user specified.
+            The name specified by the user.
-            A canonical name for the user.
+            A canonical name for the symbol.
-    """A name manager that always attach a prefix to all names.
+    """A name manager that attaches a prefix to all names.
-"""NDArray API of mxnet."""
+"""NDArray API of MXNet."""
-    Empty handle can be used to hold result.
+    Empty handle can be used to hold a result.
-        A new empty NDArray handle.
+        A new empty `NDArray` handle.
-        A new empty NDArray handle.
+        A new empty `NDArray` handle.
-"""Weight updating functions"""
+"""Weight updating functions."""
-        Multiply the gradient with ``rescale_grad`` before updating. Often
+        Multiply the gradient with `rescale_grad` before updating. Often
-        optimizer with ``create_optimizer`` later.
+        optimizer with `create_optimizer` later.
-        We can use the alias ``create`` for ``Optimizer.create_optimizer``
+        We can use the alias `create` for ``Optimizer.create_optimizer``
-        for a given weight which will be used in ``update``. This function is
+        for a given weight which will be used in `update`. This function is
-            The weight
+            The weight.
-        Returns 0 for non-weights if the name of weights are provided for __init__.
+        Returns 0 for non-weights if the name of weights are provided for `__init__`.
-"""profiler setting methods."""
+"""Profiler setting methods."""
-"""Random number interface of mxnet."""
+"""Random number interface of MXNet."""
-"""Read and write for the RecrodIO data format"""
+"""Read and write for the RecrodIO data format."""
-    """Pack an string into MXImageRecord.
+    """Pack a string into MXImageRecord.
-        String buffer from MXRecordIO.read.
+        String buffer from ``MXRecordIO.read``.
-    and call it with NDArray.
+    This class allow you to write CUDA kernels in Python
-            List of inputs. Can contain different ndarrays than those used for the constructor,
+        inputs : list of NDArray
-        outputs : list of ndarray
+        outputs : list of NDArray
-"""Symbolic configuration API of mxnet."""
+"""Symbolic configuration API of MXNet."""
-            provide positional arguments
+            Positional arguments
-            provide keyword arguments
+            Keyword arguments
-        the resulting symbol
+            The resulting symbol.
-            The name of this symbol, returns None for grouped symbol.
+            The name of this symbol, returns ``None`` for grouped symbol.
-            a dicitonary mapping attribute keys to values
+            A dicitonary mapping attribute keys to values.
-        For example. A._set_attr(foo="bar") adds the key, value pair `"foo: bar"`
+        For example. A._set_attr(foo="bar") adds the mapping ``"{foo: bar}"``
-            inputs None will be returned.
+            inputs then ``None`` will be returned.
-        returns : list of string
+        list of str
-        include the ``moving_mean`` and ``moving_variance`` in BatchNorm.
+        include the `moving_mean` and `moving_variance` in `BatchNorm`.
-        A tuple of Nones is returned if there is not enough information to deduce the missing types.
+        A tuple of ``None`` values is returned if there is not enough information
-        be raised.
+        way. A tuple of ``None`` vakyes is returned if there is not enough information
-        """Partially infer the shape. The same as infer_shape, except that the partial
+        """Partially infer the shape. The same as `infer_shape`, except that the partial
-        """Helper function to get ndarray lists handles from various inputs.
+        """Helper function to get NDArray lists handles from various inputs.
-        This function will ask user to pass in ndarray of position
+        This function will ask user to pass in an `NDArray` of position
-            The dict mapping the ``ctx_group`` attribute to the context assignment.
+            The dict mapping the `ctx_group` attribute to the context assignment.
-              to the corresponding NDArray.
+            - If type is list of `NDArray`, the position is in the same order of list_arguments.
-        args_grad : list of NDArray or dict of str to NDArray, optional
+        args_grad : list of NDArray or dict of str to `NDArray`, optional
-            - If type is dict of str to NDArray, then it maps the name of arguments
+            - If type is list of `NDArray`, the position is in the same order of list_arguments.
-            - When the type is dict of str to NDArray, users only need to provide the dict
+            - When the type is dict of str to `NDArray`, users only need to provide the dict
-            - 'write' means everytime gradient is write to specified args_grad NDArray.
+            - 'write' means everytime gradient is write to specified args_grad `NDArray`.
-        aux_states : list of NDArray, or dict of str to NDArray, optional
+        aux_states : list of `NDArray`, or dict of str to `NDArray`, optional
-              to the corresponding NDArray,
+            - If type is list of `NDArray`, the position is in the same order
-            The dict mapping the ``ctx_group`` attribute to the context assignment.
+            The dict mapping the `ctx_group` attribute to the context assignment.
-            sequences, etc. The returned executor shares state with shared_exec, and should not be
+            sequences, etc. The returned executor shares state with `shared_exec`, and should not be
-        User can give up gradient by using a dict in args_grad and only specify
+        Auxiliary states are special states of symbols that do not correspond
-              to the corresponding NDArray.
+            - If type is list of `NDArray`, the position is in the same order of `list_arguments`.
-    You also get the benefit being able to directly load/save from cloud storage(S3, HDFS)
+    You also get the benefit being able to directly load/save from cloud storage(S3, HDFS).
-        A json string.
+        A JSON string.
-        The value type of the inner value, default to np.float32
+        The value type of the inner value, default to ``np.float32``.
-        The created Symbol
+        The created Symbol.
-        The value type of the inner value, default to np.float32
+        The value type of the inner value, default to ``np.float32``.
-        Spacing between values
+        Spacing between values.
-        The value type of the inner value, default to np.float32
+        The value type of the inner value, default to ``np.float32``.
-    """Set default ``ctx``."""
+    """Set default context."""
-        Same as Numpy.
+        Same as NumPy.
-        Numpy reducing function like `np.sum` or `np.max`
+        A NumPy reducing function like ``np.sum`` or ``np.max``.
-    """Find the location of maximum violation."""
+    """Finds and returns the location of maximum violation."""
-    """Test that two numpy arrays are almost equal (ignoring NaN in either array).
+    """Test that two NumPy arrays are almost equal (ignoring NaN in either array).
-    """Test that two numpy arrays are almost equal (ignoring NaN in either array).
+    """Test that two NumPy arrays are almost equal (ignoring NaN in either array).
-    location : ``None`` or list of ``np.ndarray`` or dict of str to np.ndarray
+    location : ``None`` or list of ``np.ndarray`` or dict of str to ``np.ndarray``.
-    dict of str to np.ndarray.
+    dict of str to np.ndarray
-        The auxiliary states required when generating the executor for the symbol
+        The auxiliary states required when generating the executor for the symbol.
-        Delta for the finite difference method that approximates the gradient
+        Delta for the finite difference method that approximates the gradient.
-        relative error eps used when comparing numeric grad to symbolic grad
+        relative error eps used when comparing numeric grad to symbolic grad.
-        Whether to use is_train=True when computing the finite-difference
+        Whether to use is_train=True when computing the finite-difference.
-        Check the gradient computation on the specified device
+        Check the gradient computation on the specified device.
-            contain all the numpy arrays corresponding to `sym.list_arguments()`
+            Contains all the numpy arrays corresponding to `sym.list_arguments()`.
-            contain the mapping between argument names and their values
+            Contains the mapping between argument names and their values.
-            contain arrays corresponding to exe.outputs
+            Contains arrays corresponding to exe.outputs.
-            contain mapping between sym.list_output() and exe.outputs
+            Contains mapping between sym.list_output() and exe.outputs.
-        relative error to check to
+        Relative error to check to.
-            contain all the numpy arrays corresponding to sym.list_auxiliary_states
+            Contains all the NumPy arrays corresponding to sym.list_auxiliary_states
-            contain the mapping between names of auxiliary states and their values
+            Contains the mapping between names of auxiliary states and their values.
-            contain all the numpy arrays corresponding to mxnet.sym.list_arguments
+            Contains all the NumPy arrays corresponding to ``mx.sym.list_arguments``.
-            contain the mapping between argument names and their values
+            Contains the mapping between argument names and their values.
-        numpy arrays corresponding to sym.outputs for incomming gradient
+        NumPys arrays corresponding to sym.outputs for incomming gradient.
-            contains arrays corresponding to exe.outputs
+            Contains arrays corresponding to ``exe.outputs``.
-            contains arrays corresponding to exe.grad_arrays
+            Contains arrays corresponding to exe.grad_arrays
-            contains mapping between sym.list_arguments() and exe.outputs
+            Contains mapping between ``sym.list_arguments()`` and exe.outputs.
-        relative error to check to
+        Relative error to check to.
-        gradient requirements. 'write', 'add' or 'null'
+        Gradient requirements. 'write', 'add' or 'null'.
-        running context
+        Running context.
-    """Check the running speed of a symbol
+    """Check the running speed of a symbol.
-        symbol to run the speed test
+        Symbol to run the speed test.
-        location to evaluate the inner executor
+        Location to evaluate the inner executor.
-        running context
+        Running context.
-        repeat times
+        Repeat times.
-        gradient requirements
+        Gradient requirements.
-            test the forward_backward speed
+            Test the forward_backward speed.
-            only test the forward speed
+            Only test the forward speed.
-        symbol(s) to run the consistency test
+        Symbol(s) to run the consistency test.
-        running context. See example for more detail.
+        Running context. See example for more detail.
-        standard deviation of the inner normal distribution. Used in initialization
+        Standard deviation of the inner normal distribution. Used in initialization.
-        gradient requirement.
+        Gradient requirement.
-Install torch and Compile with USE_TORCH=1 to use this module."""
+Install Torch and compile with USE_TORCH=1 to use this module."""
-            Positional arguments of input scalars and NDArray.
+            Positional arguments of inputs (both scalar and `NDArray`).
-        shape string
+        Shape string.
-    List of str to represent shape.
+    list of str
-        dict of shapes, str->shape (tuple), given input shapes.
+        A dict of shapes, str->shape (tuple), given input shapes.
-        total length of printed lines
+        Rotal length of printed lines
-        void
+    None
-            void
+        None
-    """Convert symbol to dot object for visualization.
+    """Convert a symbol to a dot object for visualization.
-        The dot object of symbol.
+        The dot object of `symbol`.
-        """Internal helper to figure out if node should be hidden with ``hide_weights``.
+        """Internal helper to figure out if node should be hidden with `hide_weights`.
-                                     'params':_base_model_url+'imagenet/resnext/101-layers-64-groups/resnext-101-64x4d-0000.params'},
+    'imagenet1k-resnext-101-64x4d': {'symbol':_base_model_url+'imagenet/resnext/101-layers/resnext-101-64x4d-symbol.json',
-        if state:
+        if state is not None:
-            g[:] = 0
+        g[:] = 0
-    assert same(tensor.reshape((-1, )), true_res)
+    assert same(tensor.reshape((-1, )).asnumpy(), true_res.asnumpy())
-    assert same(tensor.reshape((2, -1)), true_res)
+    assert same(tensor.reshape((2, -1)).asnumpy(), true_res.asnumpy())
-    assert same(tensor.reshape((-1, 2)), true_res)
+    assert same(tensor.reshape((-1, 2)).asnumpy(), true_res.asnumpy())
-    assert np.all(np.array(y) == x.asnumpy())
+    for i in range(x.size):
-            force_rebind=True, grad_req="write")
+        # NOTE(reed): Reshape to set the data shape.
-    any_done = False
+    all_done = False
-    while not any_done:
+    while not all_done:
-
+        # NOTE(reed): Reshape to set the data shape.
-        if t == t_max and not any_done:
+        if t == t_max:
-    assert output == res
+    assert same(output.asnumpy(), res.asnumpy())
-        assert a == b
+        assert same(a.asnumpy(), b.asnumpy())
-    f_exp_grad    = lambda x: [x]
+    f_exp_grad    = lambda x: [nd.exp(x)]
-        raise NotImplementedError()
+        return [ele['shape'] for ele in self.state_info]
-        for shape in self.state_shape:
+        for info in self.state_info:
-            if shape is None:
+            if info is None:
-                             shape=shape, **kwargs)
+                             **kwargs)
-        return [(0, self._num_hidden)]
+    def state_info(self):
-        return [(0, self._num_hidden), (0, self._num_hidden)]
+    def state_info(self):
-        return [(0, self._num_hidden)]
+    def state_info(self):
-    def state_shape(self):
+    def state_info(self):
-        return [(b*self._num_layers, 0, self._num_hidden)]*n
+        return [{'shape': (b*self._num_layers, 0, self._num_hidden), '__layout__': 'LNC'}
-        return _cells_state_shape(self._cells)
+    def state_info(self):
-            n = len(cell.state_shape)
+            n = len(cell.state_info)
-            n = len(cell.state_shape)
+            n = len(cell.state_info)
-    def state_shape(self):
+    def state_info(self):
-        return self.base_cell.state_shape
+    def state_info(self):
-        return _cells_state_shape(self._cells)
+    def state_info(self):
-                                            begin_state=states[:len(l_cell.state_shape)],
+                                            begin_state=states[:len(l_cell.state_info)],
-                                            begin_state=states[len(l_cell.state_shape):],
+                                            begin_state=states[len(l_cell.state_info):],
-def var(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None, init=None):
+def var(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None, init=None, **kwargs):
-        w, h = float(w*sh)/h, sh
+        w, h = float(w * sh) / h, sh
-        w, h = sw, float(h*sw)/w
+        w, h = sw, float(h * sw) / w
-        new_h, new_w = size*h/w, size
+        new_h, new_w = size * h / w, size
-        new_h, new_w = size, size*w/h
+        new_h, new_w = size, size * w / h
-    out = nd.crop(src, begin=(y0, x0, 0), end=(y0+h, x0+w, int(src.shape[2])))
+    out = nd.crop(src, begin=(y0, x0, 0), end=(y0 + h, x0 + w, int(src.shape[2])))
-    y0 = (h - new_h)/2
+    x0 = int((w - new_w) / 2)
-        max_area = w*int(w/new_ratio)
+        max_area = w * int(w / new_ratio)
-        max_area = h*int(h*new_ratio)
+        max_area = h * int(h * new_ratio)
-    min_area *= h*w
+    min_area *= h * w
-    new_h = int(np.sqrt(new_area/new_ratio))
+    new_w = int(np.sqrt(new_area * new_ratio))
-            gray = (3.0*(1.0-alpha)/gray.size)*nd.sum(gray)
+            gray = src * coef
-            gray = src*coef
+            gray = src * coef
-            gray *= (1.0-alpha)
+            gray *= (1.0 - alpha)
-        rgb = np.dot(eigvec*alpha, eigval)
+        rgb = np.dot(eigvec * alpha, eigval)
-        auglist.append(RandomSizedCropAug(crop_size, 0.3, (3.0/4.0, 4.0/3.0), inter_method))
+        auglist.append(RandomSizedCropAug(crop_size, 0.3, (3.0 / 4.0, 4.0 / 3.0), inter_method))
-        assert std is not None
+    elif std is not None:
-                self.imgrec = recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, 'r') # pylint: disable=redefined-variable-type
+                self.imgrec = recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, 'r')  # pylint: disable=redefined-variable-type
-                self.imgrec = recordio.MXRecordIO(path_imgrec, 'r') # pylint: disable=redefined-variable-type
+                self.imgrec = recordio.MXRecordIO(path_imgrec, 'r')  # pylint: disable=redefined-variable-type
-                key = str(index) # pylint: disable=redefined-variable-type
+                key = str(index)  # pylint: disable=redefined-variable-type
-            self.seq = self.seq[part_index*C:(part_index+1)*C]
+            C = N / num_parts
-        return io.DataBatch([batch_data], [batch_label], batch_size-i)
+        return io.DataBatch([batch_data], [batch_label], batch_size - i)
-        subprocess.call('cd %s; cp make/readthedocs.mk config.mk' % folder, shell = True)
+        subprocess.call('cd %s; cp make/config.mk config.mk' % folder, shell = True)
-        """Returns a view of this array with a new shape without altering any data.
+        """Returns a **view** of this array with a new shape without altering any data.
-               'out : NDArray or list of NDArray\n' +
+               'out : NDArray or list of NDArrays\n' +
-        'ndarray-or-symbol':'Symbol',\
+        'NDArray-or-Symbol':'Symbol',\
-        'ndarray-or-symbol[]':'const std::vector<Symbol>&',\
+        'NDArray-or-Symbol[]':'const std::vector<Symbol>&',\
-                         [py_str(arg_types[i]) for i in range(narg)],
+                         arg_names,
-            arguments.append(py_str(arg_names[i]))
+    ndargs_pos = {}
-                ndargs.append(i)
+        keys = []
-                pos_args.append(str(i))
+                assert i < len(arg_names), \
-            kwargs['dtype'] = np.dtype(kwargs['dtype']).name
+        num_ndargs = len(ndargs)
-        else:
+        for key, val in kwargs.items():
-            c_array(ctypes.c_char_p, [c_str(str(i)) for i in kwargs.values()])))
+            ctypes.c_int(len(keys)),
-    doc_str = _re.sub('ndarray-or-symbol', 'NDArray', doc_str)
+    doc_str = _re.sub('NDArray-or-Symbol', 'NDArray', doc_str)
-    doc_str = _re.sub('ndarray-or-symbol', 'Symbol', doc_str)
+    doc_str = _re.sub('NDArray-or-Symbol', 'Symbol', doc_str)
-    >>> cpu_array = mx.md.ones((2, 3))
+    >>> cpu_array = mx.nd.ones((2, 3))
-    >>>     gpu_array = mx.md.ones((2, 3))
+    >>>     gpu_array = mx.nd.ones((2, 3))
-        return inputs, states
+        return inputs, next_states
-    def test_normal_case():
+    def test_normal_case(index_type=np.int32):
-                mx.nd.array(indices, ctx=default_context(), dtype=np.int32),
+                mx.nd.array(indices, ctx=default_context(), dtype=index_type),
-    test_normal_case()
+    test_normal_case(index_type=np.int32)
-        check_numeric_gradient(sym, [data, index])
+    def test_pick_helper(index_type=np.int32):
-ï»¿import sys
+import sys
-        stride=(1,1), act_type="relu", use_batchnorm=False)
+    conv10_2, relu10_2 = conv_act_layer(relu10_1, "10_2", 256, kernel=(3,3), pad=(1,1), \
-        stride=(1,1), act_type="relu", use_batchnorm=False)
+    conv11_2, relu11_2 = conv_act_layer(relu11_1, "11_2", 256, kernel=(3,3), pad=(1,1), \
-    r_shape[np.where(r_axis_flags == 0)] = 1
+def gen_broadcast_data(idx):
-def gen_binary_data():
+def gen_broadcast_data_int(idx):
-        d = gen_data()
+        d = gen_data(i)
-        d = gen_data()
+        d = gen_data(i)
-
+        check_binary_op_forward(c, lambda a, b: (a == b).astype(a.dtype), gen_broadcast_data_int)
-        """Returns a VIEW of this array with a new shape without altering any data.
+        """Returns a view of this array with a new shape without altering any data.
-               'out : NDArray or list of NDArrays\n' +
+               'out : NDArray or list of NDArray\n' +
-from .base import numeric_types
+from .base import numeric_types
-                 shuffle=False, part_index=0, num_parts=1, aug_list=None, imglist=None, **kwargs):
+                 shuffle=False, part_index=0, num_parts=1, aug_list=None, imglist=None,
-        assert(path_imgrec or path_imglist or (isinstance(imglist, list)))
+        assert path_imgrec or path_imglist or (isinstance(imglist, list))
-                self.imgrec = recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, 'r')
+                self.imgrec = recordio.MXIndexedRecordIO(path_imgidx, path_imgrec, 'r') # pylint: disable=redefined-variable-type
-        self.provide_data = [('data', (batch_size,) + data_shape)]
+        self.check_data_shape(data_shape)
-            self.provide_label = [('softmax_label', (batch_size, label_width))]
+            self.provide_label = [(label_name, (batch_size, label_width))]
-            self.provide_label = [('softmax_label', (batch_size,))]
+            self.provide_label = [(label_name, (batch_size,))]
-                return label, img
+                return label, self.read_image(fname)
-                    logging.debug('Invalid image, skipping.')
+                data = [self.imdecode(s)]
-                for d in data:
+                data = self.augmentation_transform(data)
-                    batch_data[i][:] = nd.transpose(d, axes=(2, 0, 1))
+                    batch_data[i][:] = self.postprocess_data(datum)
-        target_shape = target_shape if target_shape is not None else (0, 0))
+    if target_shape:
-from tools.rand_sampler import RandCropper, RandPadder
+from utils import DotDict, namedtuple_with_defaults, zip_namedtuple, config_as_dict
-cfg.ROOT_DIR = os.path.join(os.path.dirname(__file__), '..')
+RandCropper = namedtuple_with_defaults('RandCropper',
-cfg.TRAIN.RESIZE_EPOCH = 1 # save model every N epoch
+RandPadder = namedtuple_with_defaults('RandPadder',
-cfg.VALID.RAND_SEED = None
+cfg.valid = DotDict()
-                       'padding': 56}
+                       'comp_id': 'comp4',}
-        return self.labels[index, :, :]
+        return self.labels[index]
-                    continue
+                # if not self.config['use_difficult'] and difficult == 1:
-                label.append([cls_id, xmin, ymin, xmax, ymax])
+                label.append([cls_id, xmin, ymin, xmax, ymax, difficult])
-        return np.array(labels)
+        return temp
-        return self.labels[index, :, :]
+        return self.labels[index]
-        return np.array(labels)
+        return temp
-        .get_symbol(len(CLASSES), nms_thresh, force_nms)
+    if net is not None:
-                        choices=['vgg16_reduced'], help='which network to use')
+    parser.add_argument('--network', dest='network', type=str, default='vgg16_ssd_300',
-    detector = get_detector(args.network, args.prefix, args.epoch,
+    network = None if args.deploy_net else args.network
-                        choices=['vgg16_reduced'], help='which network to use')
+    parser.add_argument('--network', dest='network', type=str, default='vgg16_ssd_300',
-        _, args, auxs = mx.model.load_checkpoint(model_prefix, epoch)
+        load_symbol, args, auxs = mx.model.load_checkpoint(model_prefix, epoch)
-                        choices=['vgg16_reduced'], help='which network to use')
+    parser.add_argument('--rec-path', dest='rec_path', help='which record file to use',
-    parser.add_argument('--cpu', dest='cpu', help='use cpu to evaluate',
+    parser.add_argument('--cpu', dest='cpu', help='use cpu to evaluate, this can be slow',
-    evaluate_net(args.network, args.dataset, args.devkit_path,
+    # parse # classes and class_names if applicable
-                 nms_thresh=args.nms_thresh, force_nms=args.force_nms)
+                 args.prefix, args.epoch, ctx, batch_size=args.batch_size,
-from detect.detector import Detector
+import mxnet as mx
-                 batch_size=1, nms_thresh=0.5, force_nms=False):
+def evaluate_net(net, path_imgrec, num_classes, mean_pixels, data_shape,
-    Evaluate entire dataset, basically simple wrapper for detections
+    evalute network given validation record file
-        resize input data shape
+    ----------
-        load model prefix
+        model prefix of saved checkpoint
-        evaluation set
+        mx.gpu() or mx.cpu()
-        using batch_size for evaluation
+        validation batch size
-        force suppress different categories
+    force_nms : boolean
-            rand_samplers=[], rand_mirror=False, is_train=False, shuffle=False)
+    # args
-        imdb.evaluate_detections(detections)
+            .get_symbol(num_classes, nms_thresh, force_nms)
-        raise NotImplementedError("No support for dataset: " + dataset)
+        metric = MApMetric(ovp_thresh, use_difficult, class_names)
-    conv = mx.symbol.Convolution(data=from_layer, kernel=kernel, pad=pad, \
+    bias = mx.symbol.Variable(name="conv{}_bias".format(name),
-                    clip=True, interm_layer=0):
+                    clip=True, interm_layer=0, steps=[]):
-            from_layer = normalization[k] * mx.symbol.broadcast_mul(lhs=scale, rhs=from_layer)
+                shape=(1, num_channels.pop(0), 1, 1),
-        loc_pred = mx.symbol.Convolution(data=from_layer, kernel=(3,3), \
+        bias = mx.symbol.Variable(name="{}_loc_pred_conv_bias".format(from_name),
-        cls_pred = mx.symbol.Convolution(data=from_layer, kernel=(3,3), \
+        bias = mx.symbol.Variable(name="{}_cls_pred_conv_bias".format(from_name),
-            clip=clip, name="{}_anchors".format(from_name))
+            clip=clip, name="{}_anchors".format(from_name), steps=step)
-def get_symbol_train(num_classes=20):
+def get_symbol_train(num_classes=20, nms_thresh=0.5, force_suppress=False, nms_topk=400):
-        global_pool=True, kernel=(1,1), name='pool10')
+    conv10_2, relu10_2 = conv_act_layer(relu10_1, "10_2", 256, kernel=(3,3), pad=(0,0), \
-    sizes = [[.1], [.2,.276], [.38, .461], [.56, .644], [.74, .825], [.92, 1.01]]
+    from_layers = [relu4_3, relu7, relu8_2, relu9_2, relu10_2, relu11_2]
-        [1,2,.5,3,1./3], [1,2,.5,3,1./3]]
+        [1,2,.5], [1,2,.5]]
-        num_channels=num_channels, clip=True, interm_layer=0)
+        num_channels=num_channels, clip=False, interm_layer=0, steps=steps)
-        ignore_label=-1, use_ignore=True, grad_scale=3., multi_output=True, \
+        ignore_label=-1, use_ignore=True, grad_scale=1., multi_output=True, \
-    out = mx.symbol.Group([cls_prob, loc_loss, cls_label])
+    out = mx.symbol.Group([cls_prob, loc_loss, cls_label, det])
-def get_symbol(num_classes=20, nms_thresh=0.5, force_suppress=True):
+def get_symbol(num_classes=20, nms_thresh=0.5, force_suppress=False, nms_topk=400):
-        variances=(0.1, 0.1, 0.2, 0.2))
+        variances=(0.1, 0.1, 0.2, 0.2), nms_topk=nms_topk)
-                    choices = ['vgg16_reduced'],
+parser.add_argument('--network', type=str, default='vgg16_ssd_300',
-                        choices=['vgg16_reduced'], help='which network to use')
+    parser.add_argument('--train-path', dest='train_path', help='train record to use',
-                        default=100, type=int)
+                        default=240, type=int)
-    parser.add_argument('--lr', dest='learning_rate', type=float, default=0.001,
+    parser.add_argument('--label-width', dest='label_width', type=int, default=350,
-    parser.add_argument('--wd', dest='weight_decay', type=float, default=0.0001,
+    parser.add_argument('--wd', dest='weight_decay', type=float, default=0.0005,
-    parser.add_argument('--lr-ratio', dest='lr_refactor_ratio', type=float, default=0.9,
+    parser.add_argument('--lr-steps', dest='lr_refactor_step', type=str, default='150, 200',
-              args.devkit_path, args.batch_size,
+    # context list
-              args.lr_refactor_ratio, args.monitor, args.log_file)
+              args.lr_refactor_step, args.lr_refactor_ratio,
-        super(MultiBoxMetric, self).__init__(['Acc', 'ObjectAcc', 'SmoothL1'], 3)
+    def __init__(self, eps=1e-8):
-        self.num_inst[1] += mask.size
+        indices = np.int64(label[mask])
-        self.num_inst[2] += loc_loss.shape[0]
+        self.sum_metric[1] += np.sum(loc_loss)
-        """Returns a view of this array with a new shape without altering any data.
+        """Returns a VIEW of this array with a new shape without altering any data.
-               'out : NDArray or list of NDArray\n' +
+               'out : NDArray or list of NDArrays\n' +
-# -*- coding: utf-8 -*-
+_DEV_MODE = int(os.getenv('DEV', '0'))
-        subprocess.call('cd %s; rm -rf build' % folder, shell = True)
+        if not _DEV_MODE:       # do an incremental build for dev mode
-build_scala_docs(root_path)
+
-      print "Done killing"
+    print host
-                self.defaultString = '"' + self.defaultString + '"'
+                self.defaultString = self.MakeCString(self.defaultString)
-        f.write(patternStr % ParseAllOps())
+    temp_file_name = ""
-import sys
+ï»¿import sys
-ï»¿import sys
+import sys
-"""MXNet: a concise, fast and flexible framework for deep learning. """
+"""MXNet: a concise, fast and flexible framework for deep learning."""
-"""NDArray namespace used to register internal functions"""
+"""NDArray namespace used to register internal functions."""
-"""Symbol namespace used to register internal functions"""
+"""Symbol namespace used to register internal functions."""
-""" ctypes library of mxnet and helper functions """
+"""ctypes library of mxnet and helper functions."""
-    """Error that will be throwed by all mxnet functions"""
+    """Error that will be throwed by all mxnet functions."""
-    """Check the return value of C API call
+    """Check the return value of C API call.
-    Wrap every API call with this function
+    This function will raise an exception when an error occurs.
-        return value from API calls
+        return value from API calls.
-        """Create ctypes char * from a python string
+        """Create ctypes char * from a Python string.
-            python string
+            Python string.
-            A char pointer that can be passed to C API
+            A char pointer that can be passed to C API.
-        """Create ctypes char * from a python string
+        """Create ctypes char * from a Python string.
-            python string
+            Python string.
-            A char pointer that can be passed to C API
+            A char pointer that can be passed to C API.
-    """Create ctypes array from a python array
+    """Create ctypes array from a Python array.
-        data type of the array we want to convert to
+        Data type of the array we want to convert to.
-        data content
+        Data content.
-        Created ctypes array
+        Created ctypes array.
-        pointer to the raw memory region
+        Pointer to the raw memory region.
-        the length of the buffer
+        The length of the buffer.
-        The raw byte memory buffer
+        The raw byte memory buffer.
-    """Convert a ctypes pointer to a numpy array
+    """Convert a ctypes pointer to a numpy array.
-    The result numpy array shares the memory with the pointer
+    The resulting NumPy array shares the memory with the pointer.
-        shape of target ndarray
+        Shape of target NDArray.
-        A numpy array : numpy array
+        A numpy array : numpy array.
-    """Append the definition position to each function contained in module
+    """Append the definition position to each function contained in module.
-        """add fileinto to a object
+        """Add fileinto to a object.
-        The file prefix to checkpoint to
+        The file prefix for this checkpoint.
-        How many epochs to wait before checkpointing. Default is 1.
+        How many epochs to wait before checkpointing. Defaults to 1.
-        Whether to save optimizer states for continue training
+        Indicates whether or not to save optimizer states for continued training.
-        The file prefix to checkpoint to
+        The file prefix for this checkpoint.
-    	How many epochs to wait before checkpointing. Default is 1.
+    	How many epochs to wait before checkpointing. Defaults to 1.
-        The callback function that can be passed as iter_end_callback to fit.
+        The callback function that can be passed as ``iter_end_callback`` to fit.
-        Reset the metric after each log
+        Reset the metric after each log.
-        batch_size of data
+        batch_size of data.
-    """
+    """Just logs the eval metrics at the end of an epoch."""
-        String representing the device type
+        String representing the device type.
-        The device id of the device, needed for GPU
+        The device id of the device, needed for GPU.
-    This function is a short cut for Context('cpu', device_id)
+    This function is a short cut for ``Context('cpu', device_id)``.
-        The device id of the device. device_id is not needed for CPU.
+        The device id of the device. ``device_id`` is not needed for CPU.
-    This function is a short cut for Context('gpu', device_id)
+    This function is a short cut for Context('gpu', device_id).
-        The device id of the device, needed for GPU
+        The device id of the device, needed for GPU.
-    """ a wrapper for the user-defined handle """
+    """A wrapper for the user-defined handle."""
-    """ Executor is the actual executing object of MXNet."""
+    """Executor is the actual executing object of MXNet."""
-            ExecutorHandle generated by calling Bind
+            ExecutorHandle generated by calling Bind.
-        Symbol.bind : to create executor
+        Symbol.bind : to create executor.
-        """list all the output ndarray
+        """List all the output NDArray.
-            whether this forward is for evaluation purpose. If True,
+            Whether this forward is for evaluation purpose. If True,
-            The dictionary that maps name of arguments to NDArrays.
+            The dictionary that maps the names of arguments to NDArrays.
-            Parameters, dict of name to NDArray of arguments
+            Parameters, dict of name to NDArray of arguments.
-            Whether allow extra parameters that are not needed by symbol
+            Whether allow extra parameters that are not needed by symbol.
-            If there is additional parameters in the dict but allow_extra_params=False
+            If there is additional parameters in the dict but ``allow_extra_params=False``.
-            new shape for arguments.
+            New shape for arguments.
-"""Executor manager"""
+"""Executor manager."""
-        in the same order as ctx
+        in the same order as ctx.
-        The network configuration
+        The network configuration.
-    """Load a list of arrays into a list of arrays specified by slices"""
+    """Load a list of arrays into a list of arrays specified by slices."""
-    """Load data into sliced arrays"""
+    """Load data into sliced arrays."""
-    """Load label into sliced arrays"""
+    """Load label into sliced arrays."""
-        List of devices for training (data parallelization)
+        List of devices for training (data parallelization).
-        """ load data and labels into arrays """
+        """Load data and labels into arrays."""
-        """ Perform a forward pass on each executor """
+        """Perform a forward pass on each executor."""
-        """ Perform a backward pass on each executor """
+        """Perform a backward pass on each executor."""
-        """ Update evaluation metric with label and current outputs """
+        """Update evaluation metric with label and current outputs."""
-        output symbol
+        Output symbol.
-        devices to run on
+        Devices to run on.
-        in the same order as ctx
+        in the same order as ctx.
-    sym_gen : a function that generate new Symbols depending on different
+    sym_gen : A function that generate new Symbols depending on different
-        """ Install monitor on all executors """
+        """Install monitor on all executors."""
-        """ set parameter and aux values
+        """Set parameter and aux values.
-            source parameter arrays
+            Source parameter arrays
-            source aux arrays
+            Source aux arrays.
-        """ Copy data from each executor to `arg_params` and `aux_params`
+        """ Copy data from each executor to ```arg_params`` and ``aux_params``.
-            target parameter arrays
+            Target parameter arrays.
-            target aux arrays
+            Target aux arrays.
-        """shared parameter arrays"""
+        """Shared parameter arrays."""
-        """shared gradient arrays"""
+        """Shared gradient arrays."""
-        """shared aux states"""
+        """Shared aux states."""
-        """ load data and labels into arrays """
+        """Load data and labels into arrays."""
-        """run forward on the current executor"""
+        """Run forward on the current executor."""
-        """run backward on the current executor"""
+        """Run backward on the current executor."""
-        """update metric with the current executor"""
+        """Update metric with the current executor."""
-    """Scale down crop size if it's bigger than image size"""
+    """Scale down crop size if it's bigger than image size."""
-    """Resize shorter edge to size"""
+    """Resize shorter edge to size."""
-    """Crop src at fixed location, and (optionally) resize it to size"""
+    """Crop src at fixed location, and (optionally) resize it to size."""
-    """Randomly crop src with size. Upsample result if src is smaller than size"""
+    """Randomly crop src with size. Upsample result if src is smaller than size."""
-    """Randomly crop src with size. Upsample result if src is smaller than size"""
+    """Randomly crop src with size. Upsample result if src is smaller than size."""
-    """Normalize src with mean and std"""
+    """Normalize src with mean and std."""
-    """Randomly crop src with size. Randomize area and aspect ratio"""
+    """Randomly crop src with size. Randomize area and aspect ratio."""
-    """Make resize shorter edge to size augumenter"""
+    """Make resize shorter edge to size augumenter."""
-    """Make random crop with random resizing and random aspect ratio jitter augumenter"""
+    """Make random crop with random resizing and random aspect ratio jitter augumenter."""
-    """Make center crop augmenter"""
+    """Make center crop augmenter."""
-    """Apply random brightness, contrast and saturation jitter in random order"""
+    """Apply random brightness, contrast and saturation jitter in random order."""
-    """Add PCA based noise"""
+    """Add PCA based noise."""
-    """Mean and std normalization"""
+    """Mean and std normalization."""
-    """Random horizontal flipping"""
+    """Random horizontal flipping."""
-    """Create augumenter list"""
+    """Create augumenter list."""
-        Number of examples per batch
+        Number of examples per batch.
-        Format: index\t[one or more label separated by \t]\trelative_path_from_root
+        Format: index\t[one or more label separated by \t]\trelative_path_from_root.
-        each item is a list [imagelabel: float or list of float, imgpath]
+        each item is a list [imagelabel: float or list of float, imgpath].
-        More arguments for creating augumenter. See mx.image.CreateAugmenter
+        More arguments for creating augumenter. See mx.image.CreateAugmenter.
-        """helper function for reading in next sample"""
+        """Helper function for reading in next sample."""
-"""Weight initialization"""
+"""Weight initializer."""
-    """Descriptor for initialization pattern.
+    """Descriptor for the initialization pattern.
-        name of variable
+        Name of variable.
-        attributes of this variable taken from Symbol.attr_dict
+        Attributes of this variable taken from ``Symbol.attr_dict``.
-        global initializer to fallback to.
+        Global initializer to fallback to.
-    """
+    """Register an intializer to the initializer factory."""
-    """
+    """The base class of an initializer."""
-            Initialization pattern descriptor
+            Initialization pattern descriptor.
-            The array to be Initialized
+            The array to be initialized.
-            name of corrosponding ndarray
+            Name of corrosponding NDArray.
-            ndarray to be Initialized
+            NDArray to be initialized.
-        """Abstract method to Initialize weight"""
+        """Abstract method to Initialize weight."""
-    """Initialize by loading data from file or dict
+    """Initialize by loading data from file or dict.
-        param file or dict mapping name to NDArray.
+        Parameter file or dict mapping name to NDArray.
-        default initializer when name is not found in param.
+        Default initializer when name is not found in param.
-        log source when initializing.
+        Log source when initializing.
-    """Initialize with multiple initializers
+    """Initialize with multiple initializers.
-        list of regular expression patterns to match parameter names.
+        List of regular expression patterns to match parameter names.
-        list of Initializer corrosponding to patterns
+        List of Initializer corrosponding to patterns.
-    """Initialize the weight to 0"""
+    """Initialize the weight to 0."""
-    """Initialize the weight to 1"""
+    """Initialize the weight to 1."""
-    """Initialize the weight to a scalar value"""
+    """Initialize the weight to a scalar value."""
-    """Initialize the weight with value uniformly sampled from ``[-scale, scale]``
+    """Initialize the weight with value uniformly sampled from ``[-scale, scale]``.
-        The scale of uniform distribution
+        The scale of uniform distribution.
-    """Initialize the weight with value sampled according to ``normal(0, sigma)``
+    """Initialize the weight with value sampled according to ``normal(0, sigma)``.
-    """Initialize weight as orthogonal matrix
+    """Initialize weight as orthogonal matrix.
-    https://arxiv.org/abs/1312.6120
+    https://arxiv.org/abs/1312.6120.
-        scaling factor of weight
+        Scaling factor of weight.
-        use "uniform" or "normal" random number to initialize weight
+        Use "uniform" or "normal" random number to initialize weight.
-        Can be ``avg``, ``in``, or ``out``
+        Can be ``avg``, ``in``, or ``out``.
-        scale of random number range
+        Scale of random number range.
-    https://arxiv.org/abs/1502.01852
+    https://arxiv.org/abs/1502.01852.
-        Can be ``avg``, ``in``, or ``out``
+        Can be ``avg``, ``in``, or ``out``.
-    """Initialize weight for upsampling layers"""
+    """Initialize weight for upsampling layers."""
-    """Initialize parameters for fused rnn layers
+    """Initialize parameters for fused rnn layers.
-"""Data iterators for common data formats"""
+"""Data iterators for common data formats."""
-         The class
+         The class.
-         Data name
+         Data name.
-         Data shape
+         Data shape.
-         Data type
+         Data type.
-         Data layout
+         Data layout.
-        concatenate along the batch_size dimension. Axis can be -1, which means
+        When data parallelism is used, the data will be automatically split and
-          A list of input data
+          A list of input data.
-          A list of input labels
+          A list of input labels.
-          examples read is less than the batch size
+          examples read is less than the batch size.
-          The example indices in this batch
+          The example indices in this batch.
-          The *i*-th elements describes the name and shape of ``data[i]``
+          The *i*-th elements describes the name and shape of ``data[i]``.
-          The *i*-th elements describes the name and shape of ``label[i]``
+          The *i*-th elements describes the name and shape of ``label[i]``.
-    """The base class of a data iterator
+    """The base class of a data iterator.
-        The batch size, namely the number of examples in a batch
+        The batch size, namely the number of examples in a batch.
-        """
+        """Reset the iterator to the begin of the data."""
-            If the end of the data is reached
+            If the end of the data is reached.
-            The data of current batch.
+            The data of the current batch.
-        """Get label of current batch.
+        """Get label of the current batch.
-            The label of current batch.
+            The label of the current batch.
-            The indices of examples in the current batch
+            The indices of examples in the current batch.
-        """Get the number of padding examples in current batch.
+        """Get the number of padding examples in the current batch.
-            Number of padding examples in current batch
+            Number of padding examples in the current batch.
-    """Resize a data iterator to given number of batches
+    """Resize a data iterator to a given number of batches.
-        The data iterator to be resized
+        The data iterator to be resized.
-        Whether to reset internal iterator on ResizeIter.reset
+        Whether to reset internal iterator on ResizeIter.reset.
-    """Performs pre-fetch for other data iterators
+    """Performs pre-fetch for other data iterators.
-        The data iterators to be pre-fetched
+        The data iterators to be pre-fetched.
-        in iter[i].provide_data
+        in iter[i].provide_data.
-        Similar to rename_data
+        Similar to ``rename_data``.
-        """The name and shape of data provided by this iterator"""
+        """The name and shape of data provided by this iterator."""
-        """The name and shape of label provided by this iterator"""
+        """The name and shape of label provided by this iterator."""
-        """Ignore roll over data and set to start"""
+        """Ignore roll over data and set to start."""
-        """Load data from underlying arrays, internal use only"""
+        """Load data from underlying arrays, internal use only."""
-    """A python wrapper a C++ data iterator
+    """A python wrapper a C++ data iterator.
-        the handle to the underlying C++ Data Iterator
+        The handle to the underlying C++ Data Iterator.
-            the resulting data iterator
+            The resulting data iterator.
-    Return ctype arrays for the key-value args, for internal use
+    Return ctype arrays for the key-value args, for internal use.
-    """ a wrapper for the user-defined handle """
+    """A wrapper for the user-defined handle."""
-            KVStore handle of C API
+            KVStore handle of C API.
-        This function returns after data have been initialized successfully
+        This function returns after data have been initialized successfully.
-        1. this function returns after adding an operator to the engine.
+        1. This function returns after adding an operator to the engine.
-        2. push is always called after all previous push and pull on the same
+        2. ``push`` is always called after all previous push and pull on the same
-        to sync all workers
+        3. There is no synchronization between workers. One can use _barrier()
-        1. this function returns after adding an operator to the engine. But any
+        1. This function returns after adding an operator to the engine. But any
-        key are finished
+        2. ``pull`` is always called after all previous push and pull on the same
-            Keys
+            Keys.
-            According values
+        out: NDArray or list of NDArray or list of lists of NDArrays
-        """Get the type of this kvstore
+        """Get the type of this kvstore.
-        """Get the rank of this worker node
+        """Get the rank of this worker node.
-        """Get the number of worker nodes
+        """Get the number of worker nodes.
-            The number of worker nodes
+            The number of worker nodes.
-        """Save optimizer (updater) state to file
+        """Save optimizer (updater) state to file.
-        """Load optimizer (updater) state from file
+        """Load optimizer (updater) state from file.
-        multi-machines.
+        multiple machines.
-            the updater function
+            The updater function.
-        """Global barrier among all worker nodes
+        """Global barrier among all worker nodes.
-        init the values, and then pull the inited value to all machines. Before
+        For example, assume there are n machines. We want to let machine 0 first
-        """Send a command to all server nodes
+        """Send a command to all server nodes.
-        nodes
+        nodes.
-        - dist works for multi-machines (multiple processes)
+        - local works for multiple devices on a single machine (single process).
-        The created KVStore
+        The created KVStore.
-        raise TypeError('name need to be string')
+        raise TypeError('name must be a string')
-""" a server node for the key value store """
+"""A server node for the key value store."""
-    """The key-value store server"""
+    """The key-value store server."""
-        """return the server controller"""
+        """Return the server controller."""
-            """server controler"""
+            """Server controler."""
-        """run the server, whose behavior is like
+        """Run the server, whose behavior is like.
-    """Start server/scheduler"""
+    """Start server/scheduler."""
-        List of all found path to the libraries
+        List of all found path to the libraries.
-"""Scheduling learning rate"""
+"""Scheduling learning rate."""
-    """Base class of a learning rate scheduler
+    """Base class of a learning rate scheduler.
-    A scheduler returns a new learning rate based on the number of updates have
+    A scheduler returns a new learning rate based on the number of updates that have
-        The initial learning rate
+        The initial learning rate.
-        """Return a new learning rate
+        """Return a new learning rate.
-    """Reduce the learning rate by a factor for every *n* steps
+    """Reduce the learning rate by a factor for every *n* steps.
-        changes the learning rate for every n updates
+        Changes the learning rate for every n updates.
-        the factor to change the learning rate
+        The factor to change the learning rate.
-        stop to change the learning rate if it is less than this value
+        Stop updating the learning rate if it is less than this value.
-    """Reduce the learning rate by given a list of steps
+    """Reduce the learning rate by given a list of steps.
-        the factor to change the learning rate
+        The factor to change the learning rate.
-        """Get zipped name and value pairs"""
+        """Get zipped name and value pairs."""
-    """Calculate accuracy"""
+    """Calculate accuracy."""
-    """Calculate top k predictions accuracy"""
+    """Calculate top k predictions accuracy."""
-        counting. usually should be -1. Include
+        Index of invalid label to ignore when
-    """Calculate Mean Absolute Error loss"""
+    """Calculate Mean Absolute Error (MAE) loss."""
-    """Calculate Mean Squared Error loss"""
+    """Calculate Mean Squared Error (MSE) loss."""
-    """Calculate Root Mean Squred Error loss"""
+    """Calculate Root Mean Squred Error (RMSE) loss."""
-    """Calculate Cross Entropy loss"""
+    """Calculate Cross Entropy loss."""
-    """Dummy metric for torch criterions"""
+    """Dummy metric for torch criterions."""
-        The name of the metric
+        The name of the metric.
-        for a minibatch, each as numpy arrays.  This function
+        for a minibatch, each as NumPy arrays.  This function
-"""learning rate scheduler"""
+"""Learning rate scheduler."""
-    """Base class of learning rate scheduler"""
+    """Base class of learning rate scheduler."""
-        Call to schedule current learning rate
+        Call to schedule current learning rate.
-            Current iteration count
+            Current iteration count.
-    """Reduce learning rate in factor
+    """Reduce learning rate in factor.
-        schedule learning rate after every round
+        Schedule learning rate after every round.
-        reduce learning rate factor
+        Reduce learning rate factor.
-        Call to schedule current learning rate
+        Call to schedule current learning rate.
-            Current iteration count
+            Current iteration count.
-    This function select and create a proper kvstore if given the kvstore type
+    This function select and create a proper kvstore if given the kvstore type.
-        The kvstore
+        The kvstore.
-        Model parameter, dict of name to NDArray of net's weights.
+    arg_params : dict of str to ``NDArray``.
-    is None, a single function, or a list.
+    is ``None``, a single function, or a list.
-        The network configuration
+        The network configuration.
-        ceil(num_train_examples / batch_size)
+        ``ceil(num_train_examples / batch_size)``.
-        The KVStore
+        The KVStore.
-        whether or not perform weight updating on kvstore
+        Whether or not perform weight updating on kvstore.
-        in the same order as ctx
+        in the same order as ``ctx``.
-    - This function will inplace update the NDArrays in arg_params and aux_states.
+    - This function will inplace update the ``NDArrays`` in ``arg_params`` and ``aux_states``.
-        The input symbol
+        The input Symbol.
-    - parameters will be loaded from ``prefix-epoch.params``.
+    - Symbol will be loaded from ``prefix-symbol.json``.
-        ceil(num_train_examples / batch_size)
+        ``ceil(num_train_examples / batch_size)``.
-        contain extra parameters than needed.
+        to be passed by aux_params and ``arg_params``.
-        """Initialize weight parameters and auxiliary states"""
+        """Initialize weight parameters and auxiliary states."""
-            the number of batch to run. Go though all batches if None
+            The number of batch to run. Go though all batches if ``None``.
-        """Run the model on X and calculate the score with eval_metric
+        """Run the model on X and calculate the score with eval_metric.
-            The metric for calculating score
+            The metric for calculating score.
-            the number of batch to run. Go though all batches if None
+            The number of batch to run. Go though all batches if ``None``.
-            the final score
+            The final score.
-            position, of its outputs should match the corresponding variable
+            Training data. If X is a DataIter, the name or (if name not available)
-            While y can be 1D or 2D (with 2nd dimension as 1), its 1st dimension must be
+            While y can be 1D or 2D (with 2nd dimension as 1), its first dimension must be
-            based on minibatch.
+            The evaluation metric. This could be the name of evaluation metric
-            For print purpose
+            A callback that is invoked at end of each batch for purposes of printing.
-            in the same order as ctx
+            in the same order as ctx.
-        - 'dist_async', multi-machines with partical asynchronous
+        - 'dist_sync', multiple machines communicating via BSP.
-        You also get the benefit being able to directly load/save from cloud storage(S3, HDFS)
+        You can also use ``pickle`` to do the job if you only work on Python.
-            other parameters for model, including num_epoch, optimizer and numpy_batch_size
+            Other parameters for model, including ``num_epoch``, optimizer and ``numpy_batch_size``.
-        This function will be more consistent with functional
+        This function is more consistent with functional
-            The symbol configuration of computation network.
+            The symbol configuration of a computation network.
-            Training data
+            Training data.
-            If X is numpy.ndarray y is required to set
+            If X is a ``numpy.ndarray``, y must be set.
-            To use multi GPU training, pass in a list of gpu contexts.
+            To use multi-GPU training, pass in a list of GPU contexts.
-            Training parameter, number of training epochs(epochs).
+            The number of training epochs(epochs).
-            ceil(num_train_examples / batch_size)
+            ``ceil(num_train_examples / batch_size)``.
-            Training parameter, name or optimizer object for training.
+            The name of the chosen optimizer, or an optimizer object, used for training.
-            Training parameter, the initialization scheme used.
+            The initialization scheme used.
-            If eval_set is numpy.ndarray pair, it should be (valid_data, valid_label)
+            If ``eval_set`` is ``numpy.ndarray`` pair, it should
-            based on minibatch.
+            The evaluation metric. Can be the name of an evaluation metric
-            For print purpose
+            A callback that is invoked at end of each batch for print purposes.
-           In default uses 'local', often no need to change for single machiine.
+           The KVStore or a string kvstore type: 'local', 'dist_sync', 'dis_async'.
-            in the same order as ctx
+            in the same order as ctx.
-        Takes a NDArray and returns a NDArray. defaults to mean
+        A function that computes statistics of tensors.
-        For example, '.*weight|.*output' will print all weights and outputs;
+        Only tensors with names that match ``name_pattern`` will be included.
-        Supports installing to multiple exes
+        Supports installing to multiple exes.
-            the Executor (returned by symbol.bind) to install to.
+            The Executor (returned by symbol.bind) to install to.
-        Call before forward"""
+        """Start collecting stats for current batch.
-        """End collecting and print results"""
+        """End collecting and print results."""
-        source_array.dtype, otherwise default to `float32`.
+
-    """Base class for operators implemented in python
+    """Base class for operators implemented in Python.
-        the default need_top_grad() function returns this value
+        the default need_top_grad() function returns this value.
-        This Should only be called once per instance if operator contains
+        This should only be called once per instance if the operator contains
-            a list of input arguments (symbols)
+            a list of input arguments (symbols).
-        """forward interface. override to create new operators
+        """Forward interface. Override to create new operators.
-        """backward interface. override to create new operators
+        """Backward interface. Can override when creating new operators.
-        """infer_shape interface. override to create new operators
+        """Interface for ``infer_shape``. Can override when creating new operators.
-            list of argument shapes in the same order as
+            List of argument shapes in the same order as
-            list of argument shapes. Can be modified from in_shape.
+            List of argument shapes. Can be modified from in_shape.
-            list of output shapes calculated from in_shape,
+            List of output shapes calculated from in_shape,
-        """list_outputs interface. override to create new operators
+        """Interface for ``list_outputs``. Can override when creating new operators.
-            list of output blob names.
+            List of output blob names.
-        """list_arguments interface. override to create new operators
+        """Interface for ``list_arguments``. Can override when creating new operators.
-        """forward interface. override to create new operators
+        """Forward interface. Can override when creating new operators.
-        """backward interface. override to create new operators
+        """Backward interface. Can override when creating new operators.
-        to determine whether this operator needs gradient input for above.
+        The default declare_backward_dependency function. Use this value
-        """infer_shape interface. override to create new operators
+        """infer_shape interface. Can override when creating new operators.
-            list of argument shapes in the same order as
+            List of argument shapes in the same order as
-            list of argument shapes. Can be modified from in_shape.
+            List of argument shapes. Can be modified from in_shape.
-            list of output shapes calculated from in_shape,
+            List of output shapes calculated from in_shape,
-            list of aux shapes calculated from in_shape,
+            List of aux shapes calculated from in_shape,
-        """list_outputs interface. override to create new operators
+        """list_outputs interface. Can override when creating new operators.
-            list of output blob names.
+            List of output blob names.
-        """list_arguments interface. override to create new operators
+        """list_arguments interface. Can override when creating new operators.
-            list of argument blob names.
+            List of argument blob names.
-        """list_auxiliary_states interface. override to create new operators
+        """list_auxiliary_states interface. Can override when creating new operators.
-    """CustomOp registry"""
+    """CustomOp registry."""
-        """Get index for new entry"""
+        """Get index for new entry."""
-            """Structure that holds Callback information. Passed to CustomOpProp"""
+            """Structure that holds Callback information. Passed to CustomOpProp."""
-                """C Callback for CustomOpProp::InferShape"""
+                """C Callback for ``CustomOpProp::InferShape``."""
-    """The base class of all optimizers.
+    """The base class inherited by all optimizers.
-        A dictionary that maps int index to string name
+        A dictionary that maps int index to string name.
-        Clip the gradient into ``[-clip_gradient, clip_gradient]``
+        Clip the gradient by projecting onto the box ``[-clip_gradient, clip_gradient]``.
-        The learning rate
+        The initial learning rate.
-        The learning rate scheduler
+        The learning rate scheduler.
-        The weight decay (or L2 regularization) coefficient add to all the weights
+        The weight decay (or L2 regularization) coefficient. Modifies objective
-        The Symbol this optimizer is applying to
+        The Symbol this optimizer is applying to.
-        """Register an new optimizer
+        """Register a new optimizer.
-        """Create an optimizer by given name and kwargs
+        """Instantiate an optimizer with a given name and kwargs.
-            Parameters for the optimizer
+            Parameters for the optimizer.
-            The created optimizer.
+            An instantiated optimizer.
-        each weight at the beginning.
+        Some optimizers require additional states, e.g. as momentum, in addition
-            An unique index to identify the weight
+            An unique index to identify the weight.
-            The state associated with the weight
+            The state associated with the weight.
-        """Update the weight by given the gradients and state
+        """Update the weight given the corresponding gradient and state.
-            An unique index to identify the weight
+            An unique index to identify the weight.
-            The gradient w.r.t. the weight
+            The gradient of the objective with respect to this weight.
-            The state associated with this weight
+            The state associated with this weight.
-        """Set individual learning rate for each weight
+        """Set individual learning rate for each weight.
-            setting multipler by index is supported for backward compatibility,
+            Set the lr multipler for name/index to float.
-            setting multipler by index is supported for backward compatibility,
+            Set the wd multipler for name/index to float.
-        """update num_update
+        """Update num_update
-            The index will be updated
+            The index to be updated.
-        """get the learning rate given the index of the weight.
+        """Get the learning rate given the index of the weight.
-            The index for weight
+            The index corresponding to the weight.
-            learning rate for this index
+            Learning rate for this index.
-            The index for weight
+            The index for weight.
-            weight decay for this index
+            Weight decay for this index.
-    """The SGD optimizer with momentum and weight decay
+    """The SGD optimizer with momentum and weight decay.
-    The optimizer updates the weight by::
+    The optimizer updates the weight by:
-    This optimizer accepts the following parameters in addition to :class:`.Optimizer`:
+    This optimizer accepts the following parameters in addition to those accepted
-       The momentum value
+       The momentum value.
-    This optimizer implements the paper *Asynchronous Stochastic Gradient Descent with
+    This class implements the optimizer described in *Asynchronous Stochastic Gradient Descent with
-    This optimizer accepts the following parameters in addition to :class:`.Optimizer`:
+    This optimizer accepts the following parameters in addition to those accepted
-       The momentum value
+       The momentum value.
-       scale DC value
+       Scale DC value.
-    """Nesterov accelerated SGD
+    """Nesterov accelerated SGD.
-    This optimizer update the weight by::
+    This optimizer updates each weight by:
-        weight = weight - lr * grad
+        weight = weight - (lr * (grad + momentum * state))
-    """Stochastic Gradient Riemannian Langevin Dynamics
+    """Stochastic Gradient Riemannian Langevin Dynamics.
-    Dynamics on the Probability Simplex*, available at
+    This class implements the optimizer described in the paper *Stochastic Gradient
-    """The Adam optimizer
+    """The Adam optimizer.
-    available at http://arxiv.org/abs/1412.6980
+    This class implements the optimizer described in *Adam: A Method for
-    This optimizer accepts the following parameters in addition to :class:`.Optimizer`:
+    This optimizer accepts the following parameters in addition to those accepted
-        Small value to avoid divided by 0
+        Small value to avoid divided by 0.
-    Online Learning and Stochastic Optimization*, available at
+    This calss implements the AdaGrad optiizer described in *Adaptive Subgradient
-    This optimizer accepts the following parameters in addition to :class:`.Optimizer`:
+    This optimizer accepts the following parameters in addition to those accepted
-        Small value to avoid divided by 0
+        Small value to avoid division by 0.
-    Two versions of RMSProp are implemented.
+    Two versions of RMSProp are implemented:
-    Tieleman & Hinton, 2012
+    Tieleman & Hinton, 2012.
-    This optimizer accepts the following parameters in addition to :class:`.Optimizer`:
+    This optimizer accepts the following parameters in addition to those accepted
-        decay factor of moving average for gradient^2.
+        Decay factor of moving average for ``gradient^2``.
-        "momentum" factor. Only used if centered=True
+        A "momentum" factor. Only used if ``centered=True``.
-        Small value to avoid divided by 0
+        Small value to avoid division by 0.
-        Use Graves or Tielemans & Hintons version of RMSProp
+        Use Graves' or Tieleman & Hinton's version of RMSProp.
-    available at https://arxiv.org/abs/1212.5701
+    This class implements AdaDelta, an optimizer described in  *ADADELTA: An adaptive
-    This optimizer accepts the following parameters in addition to :class:`.Optimizer`:
+    This optimizer accepts the following parameters in addition to those accepted
-        Decay rate for both squared gradients and delta x
+        Decay rate for both squared gradients and delta.
-        Small value to avoid divided by 0
+        Small value to avoid division by 0.
-    """Updater for kvstore"""
+    """Updater for kvstore."""
-        """Update weight given gradient and index"""
+        """Update weight given gradient and index."""
-        """set updater states"""
+        """Set updater states."""
-        """get updater states"""
+        """Get updater states."""
-    """Return a clossure of the updater needed for kvstore
+    """Return a clossure of the updater needed for kvstore.
-         The optimizer
+         The optimizer.
-         The clossure of the updater
+         The clossure of the updater.
-        be 'symbolic' or 'all'. Default is `symbolic`.
+        Indicates whether to enable the profiler, can
-        'profile.json'.
+        The name of output trace file. Defaults to 'profile.json'.
-        Indicting whether to run the profiler, can
+        Indicates whether to run the profiler, can
-    in advance in case your program cannot exit normally"""
+    in advance in case your program cannot exit normally."""
-"""Random Number interface of mxnet."""
+"""Random number interface of mxnet."""
-    """Seed the random number generators in mxnet.
+    """Seed the random number generators in MXNet.
-    such as Dropout operators.
+    This seed will affect behavior of functions in this module.
-    This means if you set the same seed, the random number sequence
+    The random number generator of MXNet is, by default, device-specific.
-    """Read/write RecordIO formmat data
+    """Read/write RecordIO formmat data.
-        """Open record file"""
+        """Open record file."""
-        """close record file"""
+        """Close record file."""
-        this will truncate the file to empty"""
+        this will truncate the file to empty."""
-        """Write a string buffer as a record
+        """Write a string buffer as a record.
-            buffer to write.
+            Buffer to write.
-        """Read a record as string
+        """Read a record as string.
-            buffer read.
+            Buffer read.
-        Path to index file
+        Path to index file.
-        data type for keys
+        Data type for keys.
-        """Query current read head position"""
+        """Query current read head position."""
-        """Query current write head position"""
+        """Query current write head position."""
-        """Read record with index"""
+        """Read record with index."""
-        """Write record with index"""
+        """Write record with index."""
-    """pack an string into MXImageRecord
+    """Pack an string into MXImageRecord.
-        header.label can be a number or an array.
+        Header of the image record.
-    """unpack a MXImageRecord to string
+    """Unpack a MXImageRecord to string.
-        string buffer from MXRecordIO.read
+        String buffer from MXRecordIO.read.
-        header of the image record
+        Header of the image record.
-        unpacked string
+        Unpacked string.
-    """unpack a MXImageRecord to image
+    """Unpack a MXImageRecord to image.
-        string buffer from MXRecordIO.read
+        String buffer from ``MXRecordIO.read``.
-        image format option for cv2.imdecode
+        image format option for ``cv2.imdecode``.
-        header of the image record
+        Header of the image record.
-        unpacked image
+        Unpacked image.
-    """pack an image into MXImageRecord
+    """Pack an image into ``MXImageRecord``.
-        header.label can be a number or an array.
+        Header of the image record.
-        quality for JPEG encoding. 1-100, or compression for PNG encoding. 1-9.
+        Quality for JPEG encoding in range 1-100, or compression for PNG encoding in range 1-9.
-        Encoding of the image. .jpg for JPEG, .png for PNG.
+        Encoding of the image (.jpg for JPEG, .png for PNG).
-        The packed string
+        The packed string.
-    assert ret, 'failed encoding image'
+    assert ret, 'failed to encode image'
-    and call them with NDArray.
+    This class allow you to write a CUDA kernels in Python
-        name of the kernel
+        Name of the kernel.
-        list of input names and ndarray
+        List of input names and ndarray.
-        list of output names and ndarray
+        List of output names and ndarray.
-        the actual kernel code.
+        The actual kernel code.
-        For example, if name = "mykernel" and
+        For example, if ``name = "mykernel"`` and
-        the kernel that is compile will be:
+        then the compiled kernel will be:
-        """run the kernel.
+        """Run the kernel.
-            but must have the same shape and in the same order.
+            List of inputs. Can contain different ndarrays than those used for the constructor,
-            but must have the same shape and in the same order.
+            List of outputs. Can contain different ndarrays than used for the constructor,
-            grid dimension for kernel launch
+            Grid dimension for kernel launch.
-            block dimension for kernel launch
+            Block dimension for kernel launch.
-    """Set default ctx"""
+    """Set default ``ctx``."""
-    """Compatible reduce for old version numpy
+    """Compatible reduce for old version of NumPy.
-        Same as Numpy
+        Same as NumPy.
-        Same as Numpy
+        Same as NumPy.
-        Same as Numpy
+        Same as Numpy.
-    """find location of maximum violation"""
+    """Find the location of maximum violation."""
-    """Test if two numpy arrays are the same
+    """Test if two NumPy arrays are the same.
-        The checking threshold. Default threshold will be used if set to None
+        The checking threshold. Default threshold will be used if set to ``None``.
-        The relative threshold. Default threshold will be used if set to None
+        The relative threshold. Default threshold will be used if set to ``None``.
-        The absolute threshold. Default threshold will be used if set to None
+        The absolute threshold. Default threshold will be used if set to ``None``.
-        The relative threshold. Default threshold will be used if set to None
+        The relative threshold. Default threshold will be used if set to ``None``.
-        The absolute threshold. Default threshold will be used if set to None
+        The absolute threshold. Default threshold will be used if set to ``None``.
-    """Retry n times before failing for stochastic test cases"""
+    """Retry n times before failing for stochastic test cases."""
-        """Decorate a test case"""
+        """Decorate a test case."""
-            """Wrapper for tests function"""
+            """Wrapper for tests function."""
-    also converted to numpy arrays.
+    Primarily used in doctest to test the functionality of a symbol.
-        If None, will take the default context.
+        If ``None``, will take the default context.
-        Mapping each input name to a numpy array.
+        Mapping each input name to a NumPy array.
-    be returned as a list of numpy arrays.
+    be returned as a list of NumPy arrays.
-    """Parse the given location to a dictionary
+    """Parse the given location to a dictionary.
-    location : None or list of np.ndarray or dict of str to np.ndarray
+    location : ``None`` or list of ``np.ndarray`` or dict of str to np.ndarray
-    dict of str to np.ndarray
+    dict of str to np.ndarray.
-    aux_states : None or list of np.ndarray or dict of str to np.ndarray
+    aux_states : None or list of np.ndarray or dict of str to np.ndarray.
-    dict of str to np.ndarray
+    dict of str to np.ndarray.
-        exectutor that computes the forward pass
+        Executor that computes the forward pass.
-        epsilon for the finite-difference method
+        Epsilon for the finite-difference method.
-Install torch and Compile with USE_TORCH=1 to use this module"""
+Install torch and Compile with USE_TORCH=1 to use this module."""
-        """Invoke this function by passing in parameters
+        """Invoke this function by passing in parameters.
-            Positional arguments of input scalars and NDArray
+            Positional arguments of input scalars and NDArray.
-    """convert shape string to list, internal use only
+    """Convert shape string to list, internal use only.
-    list of str to represent shape
+    List of str to represent shape.
-    """convert symbol for detail information
+    """Convert symbol for detail information.
-        symbol to be visualized
+        Symbol to be visualized.
-        dict of shapes, str->shape (tuple), given input shapes
+        dict of shapes, str->shape (tuple), given input shapes.
-        relative or absolute positions of log elements in each line
+        Relative or absolute positions of log elements in each line.
-        """print format row
+        """Print format row.
-            information field
+            Information field.
-            field length ratio
+            Field length ratio.
-            node information
+            Node information.
-            node shape information
+            Node shape information.
-            node total parameters
+            Node total parameters.
-    """convert symbol to dot object for visualization
+    """Convert symbol to dot object for visualization.
-        title of the dot graph
+        Title of the dot graph.
-        dict of node's attributes
+        dict of node's attributes.
-        or `*_bias` will be hidden
+        If True (default), then inputs with names like `*_weight`
-        dot object of symbol
+        The dot object of symbol.
-        raise TypeError("symbol must be Symbol")
+        raise TypeError("symbol must be a Symbol")
-        """Internal helper to figure out if node should be hidden with hide_weights
+        """Internal helper to figure out if node should be hidden with ``hide_weights``.
-        Equivalent to ``mx.nd.transpose(self)``.
+        Equivalent to ``mx.nd.transpose(self)`` except that
-        and returns a copy rather than a view of the array.
+        Unlike ``numpy.ndarray.T``, this function returns a copy
-            raise ValueError('Only 2D matrix is allowed to be transposed')
+        if len(self.shape) < 2:
-            next_data_batch = data_iter.next()
+            next_data_batch = next(data_iter)
-                    next_data_batch = data_iter.next()
+                    next_data_batch = next(data_iter)
-from .. import symbol, init, ndarray
+from .. import symbol, init, ndarray, _symbol_internal
-        raise NotImplementedError
+        cell, p_outputs, p_states = self.base_cell, self.zoneout_outputs, self.zoneout_states
-os.environ["MXNET_CPU_WORKER_NTHREADS"] = "2"
+    def infer_type(self, in_type):
-from .ndarray import NDArray
+from .ndarray import NDArray, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
-    MXNET_CPU_WORKER_NTHREADS must be greater than 1 for custom op to work on CPU
+    def infer_type(self, in_type):
-        class CustomOpInfo(Structure):
+        class MXCallbackList(Structure):
-                ('p_delete', c_void_p)
+                ('num_callbacks', c_int),
-        deps_functype = CFUNCTYPE(c_bool, c_int_p, c_int_p, c_int_p,
+        fb_functype = CFUNCTYPE(c_int, c_int, POINTER(c_void_p), POINTER(c_int),
-        createop_functype = CFUNCTYPE(c_bool, c_char_p, c_int, POINTER(POINTER(mx_uint)),
+        createop_functype = CFUNCTYPE(c_int, c_char_p, c_int, POINTER(POINTER(mx_uint)),
-        req_enum = ['null', 'write', 'inplace', 'add']
+                                      POINTER(MXCallbackList), c_void_p)
-                                          None, None, None)
+                    callbacks = [del_functype(delete_entry),
-                                      None, None, None, None, None, None, None)
+            callbacks = [del_functype(delete_entry),
-                                     POINTER(c_char_p), POINTER(CustomOpPropInfo))
+        creator_functype = CFUNCTYPE(c_int, c_char_p, c_int, POINTER(c_char_p),
-    assert_almost_equal(conv_args_grad[1].asnumpy(), deconv_args_grad[1].asnumpy(), rtol=1e-3)
+    assert_almost_equal(conv_args_grad[1].asnumpy(), deconv_args_grad[1].asnumpy(), rtol=1e-3, atol=1e-2)
-        outputs, _ = _normalize_sequence(None, outputs, layout, merge_outputs)
+        outputs, _ = _normalize_sequence(length, outputs, layout, merge_outputs)
-        outputs, _ = _normalize_sequence(None, outputs, layout, merge_outputs)
+        outputs, _ = _normalize_sequence(length, outputs, layout, merge_outputs)
-        np.testing.assert_allclose(arr1.asnumpy(), arr2.asnumpy(), rtol=1e-3)
+        np.testing.assert_allclose(arr1.asnumpy(), arr2.asnumpy(), rtol=1e-3, atol=1e-4)
-            item = [int(line[0])] + [line[-1]] + [float(i) for i in line[1:-1]]
+            line_len = len(line)
-__version__ = "0.9.4"
+__version__ = "0.9.5"
-    """Calculate perplexity
+    """Calculate perplexity.
-    def __init__(self, ignore_label):
+    def __init__(self, ignore_label, axis=-1):
-            pred = ndarray.batch_take(pred, label)
+            pred = ndarray.pick(pred, label, axis=self.axis)
-def array(source_array, ctx=None, dtype=mx_real_t):
+
-        An optional value type (default is `float32`).
+        An optional value type. If source_array is NDArray then defaults to
-            raise TypeError('source_array must be array like object')
+    if isinstance(source_array, NDArray):
-    def __new__(cls, name, attrs=None):
+    def __new__(cls, name, attrs=None, global_init=None):
-        intializer applied to unpacked weights.
+        intializer applied to unpacked weights. Fall back to global
-        if not isinstance(init, Initializer):
+        if isinstance(init, string_types):
-                                       num_layers=num_layers, mode=mode,
+        super(FusedRNN, self).__init__(init=init.dumps() if init is not None else None,
-    def _init_weight(self, _, arr):
+    def _init_weight(self, desc, arr):
-            desc = InitDesc(name)
+            arg_desc = InitDesc(name, global_init=desc.global_init)
-            if self._mode == 'lstm' and name.endswith("f_bias"):
+            if self._mode == 'lstm' and name.endswith("_f_bias"):
-                self._init(desc, args[name])
+                self._init(arg_desc, args[name])
-                 dropout=0., get_next_state=False, initializer=None, forget_bias=1.0,
+                 dropout=0., get_next_state=False, forget_bias=1.0,
-
+        initializer = init.FusedRNN(None, num_hidden, num_layers, mode,
-        >>> print(time.time() - tic)
+        >>> print(time.time() - tic) # doctest: +SKIP
-        >>> print(time.time() - tic)
+        >>> print(time.time() - tic) # doctest: +SKIP
-    >>> mx.nd.ones((1, 2), 2.0, dtype='float16').asnumpy()
+    >>> mx.nd.full((1, 2), 2.0, dtype='float16').asnumpy()
-    ...     y_np = x.reshape((dims[0], np.prod(dims[1:])))
+    ...     y_np = x.reshape((dims[0], np.prod(dims[1:]).astype('int32')))
-    doctest.testmod(mxnet.symbol_doc, globs=globs)
+    doctest.testmod(mxnet.symbol_doc, globs=globs, verbose=True)
-    doctest.testmod(mxnet.ndarray, globs=globs)
+    doctest.testmod(mxnet.ndarray, globs=globs, verbose=True)
-        self.mod = mx.mod.Module(symbol, context=ctx)
+        self.mod = mx.mod.Module(symbol, label_names=None, context=ctx)
-    tmp = mx.symbol.contrib.MultiBoxTarget(
+    tmp = mx.contrib.symbol.MultiBoxTarget(
-    out = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
+    out = mx.contrib.symbol.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
-        rois = mx.symbol.contrib.Proposal(
+        rois = mx.contrib.symbol.Proposal(
-        rois = mx.symbol.contrib.Proposal(
+        rois = mx.contrib.symbol.Proposal(
-        anchors = mx.symbol.contrib.MultiBoxPrior(from_layer, sizes=size_str, ratios=ratio_str, \
+        anchors = mx.contrib.symbol.MultiBoxPrior(from_layer, sizes=size_str, ratios=ratio_str, \
-        group = mx.symbol.contrib.Proposal(
+        group = mx.contrib.symbol.Proposal(
-        rois = mx.symbol.contrib.Proposal(
+        rois = mx.contrib.symbol.Proposal(
-        rois = mx.symbol.contrib.Proposal(
+        rois = mx.contrib.symbol.Proposal(
-        (4,)
+        (4L,)
-        (2, 3, 4)
+        (2L, 3L, 4L)
-        <class 'numpy.float32'>
+        <type 'numpy.float32'>
-        <class 'numpy.int32'>
+        <type 'numpy.int32'>
-               [ 1.,  1.,  1.]], dtype=int32)
+        array([[1, 1, 1],
-        >>> type(x.asscalar)
+        >>> type(x.asscalar())
-        <class 'numpy.int32'>
+        <type 'numpy.int32'>
-- the name `numpy`
+- the name `test_utils` for `mx.test_utils` (e.g. `test_utils.reldiff`)
-    ...     ('softrelu', lambda x: numpy.log(1 + numpy.exp(x)))
+    ...     ('relu', lambda x: np.maximum(x, 0)),
-    >>> x = numpy.ones(shape)
+    >>> x = np.ones(shape)
-    >>> test_utils.almost_equal(x, y, threshold=0)
+    >>> test_utils.almost_equal(x, y)
-    >>> numpy.abs(x.mean() - y.mean()) < 0.1
+    >>> np.abs(x.mean() - y.mean()) < 0.1
-    >>> set(numpy.unique(y)) == set([0, 2])
+    >>> set(np.unique(y)) == set([0, 2])
-    >>> x = numpy.random.choice(vocab_size, batch_size)
+    >>> x = np.random.choice(vocab_size, batch_size)
-    ...     y_np = x.reshape((dims[0], numpy.prod(dims[1:])))
+    ...     y_np = x.reshape((dims[0], np.prod(dims[1:])))
-    >>> out_np = numpy.dot(x, w.T) + b
+    >>> out_np = np.dot(x, w.T) + b
-    >>> x = c.bind(dev, args={'a': mxnet.nd.ones((2, 2)), 'b' : mxnet.nd.ones((2, 2))})
+    >>> dev = mx.context.cpu();
-    >>> x = c.bind(dev, args={'a': mxnet.nd.ones((2, 2)), 'b' : mxnet.nd.ones((1, 1))})
+    >>> x = c.bind(dev, args={'a': mx.nd.ones((2, 2)), 'b' : mx.nd.ones((1, 1))})
-    >>> x = c.bind(dev, args={'a': mxnet.nd.ones((2, 1)), 'b' : mxnet.nd.ones((1, 2))})
+    >>> x = c.bind(dev, args={'a': mx.nd.ones((2, 1)), 'b' : mx.nd.ones((1, 2))})
-    >>> x = c.bind(dev, args={'a': mxnet.nd.ones((1, 2)), 'b' : mxnet.nd.ones((2, 1))})
+    >>> x = c.bind(dev, args={'a': mx.nd.ones((1, 2)), 'b' : mx.nd.ones((2, 1))})
-    globs = {'numpy': numpy, 'mxnet': mxnet, 'test_utils': mxnet.test_utils}
+    globs = {'np': numpy, 'mx': mxnet, 'test_utils': mxnet.test_utils, 'SymbolDoc': mxnet.symbol_doc.SymbolDoc}
-
+def test_ndarray():
-    """Return a new empty handle.
+    """Returns a new empty handle.
-    Empty handle can be used to hold result
+    Empty handle can be used to hold result.
-        A new empty ndarray handle
+        A new empty NDArray handle.
-    Empty handle is only used to hold results
+    Empty handle is only used to hold results.
-        A new empty ndarray handle
+        A new empty NDArray handle.
-    """Wait all async operation to finish in MXNet
+    """Wait for all async operations to finish in MXNet.
-    This function is used for benchmark only
+    This function is used for benchmarking only.
-    """An array object represents a multidimensional, homogeneous array of
+    """An array object representing a multidimensional, homogeneous array of
-        """Return a string representation of the array"""
+        """Returns a string representation of the array."""
-            The indexing keys
+            The indexing key.
-            The value to set
+            The value to set.
-        Return a sliced view of this array
+        Returns a sliced view of this array.
-            indexing keys
+            Indexing key.
-        """Peform an synchronize copy from the array.
+        """Peforms a synchronized copy from the array.
-            The data source we should like to copy from.
+            The data source we would like to copy from.
-                raise TypeError('array must be an array_like data,' +
+                raise TypeError('array must consist of array-like data,' +
-            raise ValueError('Shape inconsistant: expected %s vs got %s'%(
+            raise ValueError('Shape inconsistent: expected %s vs got %s'%(
-        """Return a sliced NDArray that shares memory with current one.
+        """Returns a sliced NDArray that shares memory with current one.
-        """Return a sliced view of this array
+        """Returns a sliced view of this array.
-        """Return a view of this array with a new shape without changing the data
+        """Returns a view of this array with a new shape without altering any data.
-            ``np.prod(new_shape)`` should be equal to ``np.prod(self.shape)``
+            ``np.prod(new_shape)`` should be equal to ``np.prod(self.shape)``.
-            An array with desired shape that is sharing data with this array.
+            An array with desired shape that shares data with this array.
-        """Broadcast an array to a new shape.
+        """Broadcasts an array to a new shape.
-            array, even the new shape is as same as ``self.shape``
+            A NDArray with the desired shape that is not sharing data with this
-        """Wait until all previous writes operations on current array are finished.
+        """Waits until all previous write operations on the current array are finished.
-        The method guarantees that all previous writes operations that pushed
+        This method guarantees that all previous write operations that pushed
-            The data type
+            This NDArray's data type.
-        """Return a copy of the array with axes transposed
+        """Returns a copy of the array with axes transposed.
-        Equals to ``mx.nd.transpose(self)``
+        Equivalent to ``mx.nd.transpose(self)``.
-        and returns a copy rather than a view of the array
+        Unlike ``numpy.ndarray.T``, this function only supports 2-D arrays,
-        """Return a ``numpy.ndarray`` object with value copied from this array
+        """Returns a ``numpy.ndarray`` object with value copied from this array.
-        """Return a scalar with value copied from this array
+        """Returns a scalar whose value is copied from this array.
-        It equals to ``self.asnumpy()[0]``. This ndarray must have shape (1,).
+        This function is equivalent to ``self.asnumpy()[0]``. This NDArray must have shape (1,).
-        """Return a copy of the array that is casted to a specified type.
+        """Returns a copy of the array after casting to a specified type.
-        """Copy the value of this array to another array.
+        """Copies the value of this array to another array.
-        the target context, then the value is copied.
+        ``self`` to ``other``.
-            The destination array or context
+        other : NDArray or Context
-            The target array
+            The copied array. If ``other`` is an ``NDArray``, then the return value
-                              RuntimeWarning)
+                warnings.warn('You are attempting to copy an array to itself', RuntimeWarning)
-            raise TypeError('copyto do not support type ' + str(type(other)))
+            raise TypeError('copyto does not support type ' + str(type(other)))
-        """Make a copy of the ndarray on the same context
+        """Makes a copy of this ``NDArray``, keeping the same context.
-        """Return an array on the target device with value as same as this array
+        """Returns an array on the target device with the same value as this array.
-            The target array
+            The target array.
-    """One hot encoding indices into matrix out.
+    """One-hot encoding indices into matrix out.
-    """Return a new array of given shape and type, without initializing entries
+    """Returns a new array of given shape and type, without initializing entries.
-        The shape of the empty array
+        The shape of the empty array.
-        An optional device context (default is the current default context)
+        An optional device context (default is the current default context).
-        An optional value type (default is `float32`)
+        An optional value type (default is `float32`).
-        A created array
+        A created array.
-    """Return a new array of given shape and type, filled with zeros.
+    """Returns a new array filled with all zeros, with the given shape and type.
-        The shape of the empty array
+        The shape of the empty array.
-        An optional device context (default is the current default context)
+        An optional device context (default is the current default context).
-        An optional value type (default is `float32`)
+        An optional value type (default is `float32`).
-    """Return a new array of given shape and type, filled with ones.
+    """Returns a new array filled with all ones, with the given shape and type.
-        The shape of the empty array
+        The shape of the empty array.
-        An optional device context (default is the current default context)
+        An optional device context.
-        An optional value type (default is `float32`)
+        An optional value type (default is `float32`).
-        A created array
+        A new array of the specified shape filled with all ones.
-    """Return a new array of given shape and type, filled with given value.
+    """Returns a new array of given shape and type, filled with the given value ``val``.
-        The shape of the empty array
+        The shape of the empty array.
-        An optional device context (default is the current default context)
+        An optional device context (default is the current default context).
-        An optional value type (default is `float32`)
+        An optional value type (default is `float32`).
-    """Create a new array from any object exposing the array interface
+    """Creates a new array from any object exposing the array interface.
-        An optional device context (default is the current default context)
+        An optional device context (default is the current default context).
-        An optional value type (default is `float32`)
+        An optional value type (default is `float32`).
-        A created array
+        An ``NDArray`` array with the same contets as the ``source_array``.
-    """Return evenly spaced values within a given interval.
+    """Returns evenly spaced values within a given interval.
-    function and ``numpy.arrage``, but returns a NDArray.
+    Values are generated within the half-open interval [start, stop). In other
-        An optional start of interval, the default value is 0
+        An optional start of interval, the default value is 0.
-        A optional spacing between values, the default value is 1
+        A optional spacing between values, the default value is 1.
-        An optional value type (default is `float32`)
+        An optional value type (default is `float32`).
-        The value type of the NDArray, default to np.float32
+        The value type of the NDArray, default to np.float32.
-    The function will perform numpy-like broadcasting if needed and call different functions
+    """ Helper function for element-wise operation.
-        left hande side operand
+        Left-hand side operand.
-        right hand side operand
+        Right-hand operand,
-        function to be called if both lhs and rhs are of NDArray type
+        Function to be called if both lhs and rhs are of ``NDArray`` type.
-        function to be called if both lhs and rhs are numeric values
+        Function to be called if both lhs and rhs are numeric values.
-        function to be called if lhs is NDArray while rhs is numeric value
+        Function to be called if lhs is ``NDArray`` while rhs is numeric value
-        function to be called if lhs is numeric value while rhs is NDArray;
+        Function to be called if lhs is numeric value while rhs is ``NDArray``;
-    """Add arguments, element-wise with broadcasting
+    """Add arguments, element-wise with broadcasting.
-    Equals to ``lhs + rhs``
+    Equivalent to ``lhs + rhs``
-    """Subtract arguments element-wisely with broadcasting
+    """Subtracts arguments element-wise with broadcasting.
-    Equals to ``lhs - rhs``
+    Equivalent to ``lhs - rhs``.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-    """Multiply arguments element-wisely with broadcasting
+    """Multiplies arguments element-wise with broadcasting.
-    Equals to ``lhs * rhs``
+    Equivalent to ``lhs * rhs``.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-    """Divide arguments element-wisely with broadcasting
+    """Divides arguments element-wise with broadcasting.
-    Equals to ``lhs / rhs``
+    Equivalent to ``lhs / rhs``.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-    Equals to ``base ** exp``
+    Equivalent to ``base ** exp``.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-    """Return (lhs == rhs), element-wise with broadcasting
+    """Returns (lhs == rhs), element-wise with broadcasting.
-    Equals ``lhs == rhs``
+    Equivalent to ``lhs == rhs``
-        broadcastable to a common shape
+        broadcastable to a common shape.
-    """Return (lhs != rhs), element-wise with broadcasting
+    """Returns (lhs != rhs), element-wise with broadcasting.
-    Equals ``lhs != rhs``
+    Equivalent to ``lhs != rhs``.
-        broadcastable to a common shape
+        broadcastable to a common shape,
-    """Return (lhs > rhs), element-wise with broadcasting
+    """Returns (lhs > rhs), element-wise with broadcasting.
-    Equals ``lhs > rhs``
+    Equivalent to ``lhs > rhs``.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-    """Return (lhs >= rhs), element-wise with broadcasting
+    """Returns (lhs >= rhs), element-wise with broadcasting.
-    Equals ``lhs >= rhs``
+    Equivalent to ``lhs >= rhs``.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-    """Return (lhs < rhs), element-wise with broadcasting
+    """Returns (lhs < rhs), element-wise with broadcasting.
-    Equals ``lhs < rhs``
+    Equivalent to ``lhs < rhs``.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-    """Return (lhs <= rhs), element-wise with broadcasting
+    """Returns (lhs <= rhs), element-wise with broadcasting.
-    Equals ``lhs <= rhs``
+    Equivalent to ``lhs <= rhs``.
-        broadcastable to a common shape
+        broadcastable to a common shape.
-    """Same as ``divide``
+    """Same as ``divide``.
-        The filename
+        The filename.
-        Loaded data
+        Loaded data.
-        raise TypeError('fname need to be string')
+        raise TypeError('fname required to be a string')
-    """Save a list of arrays of a str->array dict into file
+    """Save a list of arrays of a str->array dict into file.
-        The data for saving
+        The filename.
-    arrays : list of NDArray
+    arrays : list of `NDArray`
-        binary image data
+        Binary image data
-        clip decoded image to rectangle (x0, y0, x1, y1)
+        Clip decoded image to rectangle (x0, y0, x1, y1).
-        output buffer. can be 3 dimensional (c, h, w) or 4 dimensional (n, c, h, w)
+        Output buffer. Can be 3 dimensional (c, h, w) or 4 dimensional (n, c, h, w).
-        output decoded image to i-th slice of 4 dimensional buffer
+        Output decoded image to i-th slice of 4 dimensional buffer.
-        number of channels to output. Decode to grey scale when channels = 1.
+        Number of channels to output. Decode to grey scale when channels = 1.
-        subtract mean from decode image before outputing.
+        Subtract mean from decode image before outputing.
-        """Get attribute string from the symbol, this function only works for non-grouped symbol.
+        """Get attribute string from the symbol. This function only works for non-grouped symbols.
-            The key to get attribute from.
+            The key corresponding to the desired attribute.
-            The attribute value of the key, returns None if attribute do not exist.
+            The desired attribute value, returns None if attribute does not exist.
-        """Recursively get all attributes from the symbol and its childrens
+        """Recursively get all attributes from the symbol and its children.
-            Values of the returned dict are dictionaries that map attribute keys to values
+            There is a key in the returned dict for every child with non-empty attribute set.
-        """Set the attribute of the symbol.
+        """Set an attribute of the symbol.
-        internal outputs of this symbol.
+        """Get a new grouped symbol sgroup. The output of sgroup is a list of the
-            The internal of the symbol.
+            A symbol group containing all internal and leaf nodes of the computation graph
-            List of all the arguments.
+            List containing the names of all the arguments required to compute the symbol.
-        Most operators do not have Auxiliary states.
+        Auxiliary states are special states of symbols that do not correspond to an argument,
-        """Infer the type of outputs and arguments of given known types of arguments.
+        """Given known types for some arguments, infers the type all arguments
-        An error will be raised if there is inconsistency found in the known types passed in.
+        Example usage:
-        """Infer the shape of outputs and arguments of given known shapes of arguments.
+        """Given known shapes for some arguments, infers the shapes of all arguments
-        An error will be raised if there is inconsistency found in the known shapes passed in.
+        You can pass in the known shapes in either positional way or keyword argument
-                        raise TypeError('Argument need to be shapes(tuple)')
+                        raise TypeError('Arguments must be shapes (tuple)')
-                raise ValueError('Length of %s do not match number of arguments' % arg_key)
+                raise ValueError('Length of %s does not match the number of arguments' % arg_key)
-                    raise TypeError('Only Accept list of NDArrays or dict of str to NDArray')
+                    raise TypeError('Only accept list of NDArrays or dict of str to NDArray')
-                        raise TypeError('Only Accept list of NDArrays or dict of str to NDArray')
+                        raise TypeError('Only accept list of NDArrays or dict of str to NDArray')
-            raise TypeError('Only Accept list of NDArrays or dict of str to NDArray')
+            raise TypeError('Only accept list of NDArrays or dict of str to NDArray')
-        keyword argument when calling shape inference, this shape information will be ignored.
+        The shape of a variable. If specified, this will be used during shape inference.
-        Specify learning rate muliplier for this variable.
+        The learning rate muliplier for this variable.
-        Specify weight decay muliplier for this variable.
+        Weight decay muliplier for this variable.
-        Similar to shape, we can specify dtype for this variable.
+        The dtype for this variable. If not specified, this value will be inferred.
-        Specify initializer for this variable to override the default initializer
+        Initializer for this variable to (optionally) override the default initializer
-        The created variable symbol.
+        A symbol corresponding to an input to the computation graph.
-    """Create a symbol that groups symbols together.
+    """Creates a symbol that contains a collection of other symbols, grouped together.
-        The created group symbol.
+        A group symbol.
-            raise TypeError('Expect Symbols in the list input')
+            raise TypeError('Expected a list of symbols as input')
-        raise TypeError('fname need to be string')
+        raise TypeError('fname required to be string')
-        cell = stack
+        cell = mx.rnn.SequentialRNNCell()
-                mode='lstm', bidirectional=args.bidirectional)
+                                   mode='lstm', bidirectional=args.bidirectional)
-from ..base import string_types
+from ..base import string_types, numeric_types
-        return ('',)
+        return ()
-               input_prefix='', layout='NTC', merge_outputs=None):
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
-            assert len(inputs) == length
+        inputs, _ = _normalize_sequence(length, inputs, layout, False)
-            outputs = symbol.Concat(*outputs, dim=axis)
+        outputs, _ = _normalize_sequence(None, outputs, layout, merge_outputs)
-        return ['']
+        return ('',)
-        """
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
-                assert axis == 0, "Unsupported layout %s"%layout
+        inputs, axis = _normalize_sequence(length, inputs, layout, True)
-            inputs = symbol.Concat(*inputs, dim=0)
+            assert axis == 0, "Unsupported layout %s"%layout
-            outputs = symbol.SwapAxis(outputs, dim1=0, dim2=1)
+        if axis == 1:
-        """Unfuse the fused RNN
+        """Unfuse the fused RNN in to a stack of rnn cells.
-                    output_prefix='%sbi_%s_%d'%(self._prefix, self._mode, i)))
+                    output_prefix='%sbi_l%d_'%(self._prefix, i)))
-        return stack
+            if self._dropout > 0 and i != self._num_layers - 1:
-            "After applying modifier cells (e.g. DropoutCell) the base " \
+            "After applying modifier cells (e.g. ZoneoutCell) the base " \
-        """
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
-                                                                       merge_outputs))
+            inputs, states = cell.unroll(length, inputs=inputs, begin_state=states, layout=layout,
-    on it (e.g. Dropout), and returns a new cell.
+    on it (e.g. Zoneout), and returns a new cell.
-        """
+
-            assert len(inputs) == length
+    def unroll(self, length, inputs, begin_state=None, layout='NTC', merge_outputs=None):
-    mod2.forward(batch)
+    mod1.forward(batch, is_train=False)
-                bidirectional=True)
+        fused = mx.rnn.FusedRNNCell(
-    outputs, _ = cell.unroll(3, input_prefix='rnn_')
+    inputs = [mx.sym.Variable('rnn_t%d_data'%i) for i in range(3)]
-    outputs, _ = cell.unroll(3, input_prefix='rnn_')
+    inputs = [mx.sym.Variable('rnn_t%d_data'%i) for i in range(3)]
-    outputs, _ = cell.unroll(3, input_prefix='rnn_')
+    inputs = [mx.sym.Variable('rnn_t%d_data'%i) for i in range(3)]
-    outputs, _ = cell.unroll(3, input_prefix='rnn_')
+    inputs = [mx.sym.Variable('rnn_t%d_data'%i) for i in range(3)]
-    outputs, _ = cell.unroll(3, input_prefix='rnn_')
+    inputs = [mx.sym.Variable('rnn_t%d_data'%i) for i in range(3)]
-    outputs, _ = cell.unroll(3, input_prefix='rnn_')
+    cell = mx.rnn.FusedRNNCell(100, num_layers=3, mode='lstm',
-    assert outputs.list_outputs() == ['test_bi_lstm_0t0_output', 'test_bi_lstm_0t1_output', 'test_bi_lstm_0t2_output']
+    assert outputs.list_outputs() == ['test_bi_l2_t0_output', 'test_bi_l2_t1_output', 'test_bi_l2_t2_output']
-    test_pooling_with_type()
+    test_pooling_versions()
-        """Abstruct method to Initialize weight"""
+        """Abstract method to Initialize weight"""
-    """Initialze parameters for fused rnn layers
+
-    def __init__(self, init, num_hidden, num_layers, mode, bidirectional=False):
+    def __init__(self, init, num_hidden, num_layers, mode, bidirectional=False, forget_bias=1.0):
-                                       bidirectional=bidirectional)
+                                       bidirectional=bidirectional, forget_bias=forget_bias)
-        self._init = init
+        self._bidirectional = bidirectional
-                                     self._mode, self._bidirectional, prefix='')
+                                     self._mode, self._bidirectional,
-            self._init(desc, args[name])
+            # for lstm bias, we use a custom initializer
-    def __init__(self, num_hidden, prefix='lstm_', params=None):
+    def __init__(self, num_hidden, prefix='lstm_', params=None, forget_bias=1.0):
-        self._iB = self.params.get('i2h_bias')
+        # we add the forget_bias to i2h_bias, this adds the bias to the forget gate activation
-                 dropout=0., get_next_state=False, initializer=None,
+                 dropout=0., get_next_state=False, initializer=None, forget_bias=1.0,
-                initializer, num_hidden, num_layers, mode, bidirectional)
+                initializer, num_hidden, num_layers, mode, bidirectional, forget_bias)
-    cell = mx.rnn.LSTMCell(100, prefix='rnn_')
+    cell = mx.rnn.LSTMCell(100, prefix='rnn_', forget_bias=1.0)
-                module.backward([mx.nd.array(adv), h])
+                h = -args.beta*(mx.nd.log(pi+1e-7)*pi)
-        return DataBatch([data], [label],
+        return DataBatch([data], [label], pad=0,
-        rois = mx.symbol.Proposal(
+        rois = mx.symbol.contrib.Proposal(
-        rois = mx.symbol.Proposal(
+        rois = mx.symbol.contrib.Proposal(
-        group = mx.symbol.Proposal(
+        group = mx.symbol.contrib.Proposal(
-        rois = mx.symbol.Proposal(
+        rois = mx.symbol.contrib.Proposal(
-        rois = mx.symbol.Proposal(
+        rois = mx.symbol.contrib.Proposal(
-        anchors = mx.symbol.MultiBoxPrior(from_layer, sizes=size_str, ratios=ratio_str, \
+        anchors = mx.symbol.contrib.MultiBoxPrior(from_layer, sizes=size_str, ratios=ratio_str, \
-    tmp = mx.symbol.MultiBoxTarget(
+    tmp = mx.symbol.contrib.MultiBoxTarget(
-    out = mx.symbol.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
+    out = mx.symbol.contrib.MultiBoxDetection(*[cls_prob, loc_preds, anchor_boxes], \
-        if function.__name__.startswith('_'):
+        if function.__name__.startswith('_contrib_'):
-        if function.__name__.startswith('_'):
+        if function.__name__.startswith('_contrib_'):
-          'mxnet._cy2', 'mxnet._cy3', 'mxnet.notebook'
+          'mxnet._cy2', 'mxnet._cy3', 'mxnet.notebook', 'mxnet.contrib'
-        mpre = np.concatenate([0.], prec, [0.])
+        mrec = np.concatenate(([0.], rec, [1.]))
-"""Image IO API of mxnet."""
+"""Read invidual image files and perform augmentations."""
-"""NDArray interface of mxnet"""
+"""Data iterators for common data formats"""
-    """Named data desc description contains name, shape, type and other extended attributes.
+    """Data description
-        data-parallelism device.
+        int
-        types : type tuple list with (name, type) tuples
+        shapes : a tuple of (name, shape)
-    """Default object for holding a mini-batch of data and related information."""
+    """A data batch.
-    """DataIter object in mxnet. """
+    """The base class of a data iterator
-        self.batch_size = 0
+    Parameters
-        """Reset the iterator. """
+        """Reset the iterator to the begin of the data
-        DataBatch(self.getdata(), self.getlabel(), self.getpad(), None)
+        """Get next data batch from iterator.
-        data : DataBatch
+        DataBatch
-        """Iterate to next batch.
+        """Move to the next batch.
-        has_next : boolean
+        boolean
-        data : NDArray
+        list of NDArray
-        label : NDArray
+        list of NDArray
-            The index of current batch
+            The indices of examples in the current batch
-        pad : int
+        int
-    to padding from internal iterator.
+    """Resize a data iterator to given number of batches
-    """
+        The data iterator to be resized
-    prefetching. For example:
+    """Performs pre-fetch for other data iterators
-        one or more DataIters (or any class with "reset" and "next" methods)
+        The data iterators to be pre-fetched
-        i-th element is a renaming map for i-th iter, in the form of
+        The *i*-th element is a renaming map for the *i*-th iter, in the form of
-                           rename_data=[{'data': 'data1'}, {'data': 'data2'}])
+    >>> iter1 = mx.io.NDArrayIter({'data':mx.nd.ones((100,10))}, batch_size=25)
-            e.set()
+        for i in self.data_taken:
-            e.set()
+        for i in self.data_taken:
-            e.wait()
+        for i in self.data_ready:
-            e.set()
+        for i in self.data_ready:
-            e.wait()
+        for i in self.data_ready:
-                e.set()
+            for i in self.data_ready:
-    """NDArrayIter object in mxnet. Taking NDArray or numpy array to get dataiter.
+    """Iterating on either ``mx.nd.NDArray`` or ``numpy.ndarray``.
-        Same as data, but is not fed to the model during testing.
+    data: array or list of array or dict of string to array
-    shuffle: bool
+    shuffle: bool, optional
-    for training and can cause problems if used for prediction.
+    last_batch_handle : str, optional
-        super(NDArrayIter, self).__init__()
+                 last_batch_handle='pad', data_name='data',
-        self.data = _init_data(data, allow_empty=False, default_name='data')
+        self.data = _init_data(data, allow_empty=False, default_name=data_name)
-    """DataIter built in MXNet. List all the needed functions here.
+    """A python wrapper a C++ data iterator
-        """
+        # Set the iterator to simply return always first batch. This can be used
-               'iterator: DataIter\n'+
+               'MXDataIter\n'+
-"""Python interface for DLMC RecrodIO data format"""
+"""Read and write for the RecrodIO data format"""
-    """Python interface for read/write RecordIO data formmat
+    """Read/write RecordIO formmat data
-    Support random access.
+    """Read/write RecordIO formmat data supporting random access.
-_IRSize = struct.calcsize(_IRFormat)
+_IR_FORMAT = 'IfQQ'
-    s = struct.pack(_IRFormat, *header) + s
+    s = struct.pack(_IR_FORMAT, *header) + s
-    s = s[_IRSize:]
+    header = IRHeader(*struct.unpack(_IR_FORMAT, s[:_IR_SIZE]))
-def np_softmax(x):
+def np_softmax(x, axis=-1):
-    x = x - np.max(x, axis=-1).reshape(x.shape[:-1] + (1,))
+    x = x - np.max(x, axis=axis, keepdims=True)
-    x /= np.sum(x, axis=-1).reshape(x.shape[:-1] + (1,))
+    x /= np.sum(x, axis=axis, keepdims=True)
-                print np.std(np.bincount(y.astype(np.int))), np.bincount(y.astype(np.int))
+                print(np.std(np.bincount(y_pred)), np.bincount(y_pred))
-                print np.sum(y_pred != self.y_pred), 0.001*y_pred.shape[0]
+                print(np.sum(y_pred != self.y_pred), 0.001*y_pred.shape[0])
-    print 'Training...'
+    print('Training...')
-                print 'epoch:', epoch, 'iter:', t, 'metric:', mACC.get(), mG.get(), mD.get()
+                print('epoch:', epoch, 'iter:', t, 'metric:', mACC.get(), mG.get(), mD.get())
-            print 'Saving...'
+            print('Saving...')
-            print "Train subset has ", len(tr_idx), " cases. Validation subset has ", len(va_idx), "cases"
+            print("Train subset has ", len(tr_idx), " cases. Validation subset has ", len(va_idx), "cases")
-        print "Train subset has ", len(tr_idx), " cases. Validation subset has ", len(va_idx), "cases" 
+        print("Train subset has ", len(tr_idx), " cases. Validation subset has ", len(va_idx), "cases")
-print "Time required for prediction", time.time()-tic
+print("Time required for prediction", time.time()-tic)
-    print "Saving csv to %s" % submission_path
+    print("Saving csv to %s" % submission_path)
-    print "Compress with gzip"
+    print("Compress with gzip")
-    print "  stored in %s.gz" % submission_path
+    print("  stored in %s.gz" % submission_path)
-        print self.vocab_size
+        print(self.vocab_size)
-        print 'begin'
+        print('begin')
-        print self.vocab_size
+        print(self.vocab_size)
-        print 'begin'
+        print('begin')
-    print batch_size*n/(time.time() - tic)
+    print(batch_size*n/(time.time() - tic))
-    print "execution begin"
+    print("execution begin")
-    print "execution end"
+    print("execution end")
-    print 'class ---- [[x1, x2, y1, y2, confidence]]'
+    print('class ---- [[x1, x2, y1, y2, confidence]]')
-            print boxes
+            print('---------', CLASSES[ind], '---------')
-        print 'results saved to %s' % result_file
+        print('results saved to %s' % result_file)
-            'data %.4fs net %.4fs' % (t1, t2)
+        print('generating %d/%d' % (i + 1, imdb.num_images),
-    print 'wrote rpn proposals to {}'.format(rpn_file)
+    print('wrote rpn proposals to {}'.format(rpn_file))
-        print 'testing {}/{} data {:.4f}s net {:.4f}s post {:.4f}s'.format(i, imdb.num_images, t1, t2, t3)
+        print('testing {}/{} data {:.4f}s net {:.4f}s post {:.4f}s'.format(i, imdb.num_images, t1, t2, t3))
-        print 'num_images', self.num_images
+        print('num_images', self.num_images)
-            print '{} gt roidb loaded from {}'.format(self.name, cache_file)
+            print('{} gt roidb loaded from {}'.format(self.name, cache_file))
-        print 'wrote gt roidb to {}'.format(cache_file)
+        print('wrote gt roidb to {}'.format(cache_file))
-            print 'Collecting %s results (%d/%d)' % (cls, cls_ind, self.num_classes - 1)
+            print('Collecting %s results (%d/%d)' % (cls, cls_ind, self.num_classes - 1))
-        print 'Writing results json to %s' % res_file
+        print('Writing results json to %s' % res_file)
-        print 'coco eval results saved to %s' % eval_file
+        print('coco eval results saved to %s' % eval_file)
-        print '%-15s %5.1f' % ('all', 100 * ap_default)
+        print('~~~~ Mean and per-category AP @ IoU=%.2f,%.2f] ~~~~' % (IoU_lo_thresh, IoU_hi_thresh))
-            print '%-15s %5.1f' % (cls, 100 * ap)
+            print('%-15s %5.1f' % (cls, 100 * ap))
-        print '~~~~ Summary metrics ~~~~'
+        print('~~~~ Summary metrics ~~~~')
-        print 'loading {}'.format(rpn_file)
+        print('loading {}'.format(rpn_file))
-            print 'appending ground truth annotations'
+            print('appending ground truth annotations')
-        print 'append flipped images to roidb'
+        print('append flipped images to roidb')
-        print 'average number of proposal', total_counts / self.num_images
+            print('percentage of', area_name, area_count / total_counts)
-            print 'average recall for {}: {:.3f}'.format(area_name, ar)
+            print('average recall for {}: {:.3f}'.format(area_name, ar))
-                print 'recall @{:.2f}: {:.3f}'.format(threshold, recall)
+                print('recall @{:.2f}: {:.3f}'.format(threshold, recall))
-        print 'num_images', self.num_images
+        print('num_images', self.num_images)
-            print '{} gt roidb loaded from {}'.format(self.name, cache_file)
+            print('{} gt roidb loaded from {}'.format(self.name, cache_file))
-        print 'wrote gt roidb to {}'.format(cache_file)
+        print('wrote gt roidb to {}'.format(cache_file))
-            print '{} ss roidb loaded from {}'.format(self.name, cache_file)
+            print('{} ss roidb loaded from {}'.format(self.name, cache_file))
-            print 'appending ground truth annotations'
+            print('appending ground truth annotations')
-        print 'wrote ss roidb to {}'.format(cache_file)
+        print('wrote ss roidb to {}'.format(cache_file))
-            print 'Writing {} VOC results file'.format(cls)
+            print('Writing {} VOC results file'.format(cls))
-        print 'VOC07 metric? ' + ('Y' if use_07_metric else 'No')
+        print('VOC07 metric? ' + ('Y' if use_07_metric else 'No'))
-        print 'saving annotations cache to {:s}'.format(annocache)
+                print('reading annotations for {:d}/{:d}'.format(ind + 1, len(image_filenames)))
-        print 'gt_boxes', gt_boxes
+        print('anchors:')
-        print 'inds_inside', len(inds_inside)
+        print('total_anchors', total_anchors)
-        print 'anchors shape', anchors.shape
+        print('anchors shape', anchors.shape)
-        print 'stdevs', stds
+        print('means', means)
-        print 'rpn: num_negatives', np.sum(labels == 0)
+        print('rpn: max max_overlaps', np.max(max_overlaps))
-        print 'rpn: num_negative avg', _bg_sum / _count
+        print('rpn: num_positive avg', _fg_sum / _count)
-        print 'bbox regression: this should not happen'
+        print('bbox regression: this should not happen')
-        print 'something wrong : zero ground truth rois'
+        print('something wrong : zero ground truth rois')
-    print 'add bounding box regression targets'
+    print('add bounding box regression targets')
-    print 'prepare roidb'
+    print('prepare roidb')
-    print 'add bounding box regression targets'
+    print('add bounding box regression targets')
-            print 'loading annotations into memory...'
+            print('loading annotations into memory...')
-            print 'Done (t=%0.2fs)'%(time.time()- tic)
+            print('Done (t=%0.2fs)'%(time.time()- tic))
-        print 'creating index...'
+        print('creating index...')
-        print 'index created!'
+        print('index created!')
-            print '%s: %s'%(key, value)
+            print('%s: %s'%(key, value))
-                print ann['caption']
+                print(ann['caption'])
-        print 'Loading and preparing results...     '
+        print('Loading and preparing results...     ')
-        print 'DONE (t=%0.2fs)'%(time.time()- tic)
+        print('DONE (t=%0.2fs)'%(time.time()- tic))
-            print 'Please specify target directory'
+            print('Please specify target directory')
-            print 'downloaded %d/%d images (t=%.1fs)'%(i, N, time.time()- tic)
+            print('downloaded %d/%d images (t=%.1fs)'%(i, N, time.time()- tic))
-                        print 'debug'
+                        print('debug')
-        print 'Running per image evaluation...      '
+        print('Running per image evaluation...      ')
-        print 'DONE (t=%0.2fs).'%(toc-tic)
+        print('DONE (t=%0.2fs).'%(toc-tic))
-        print 'Accumulating evaluation results...   '
+        print('Accumulating evaluation results...   ')
-            print 'Please run evaluate() first'
+            print('Please run evaluate() first')
-        print 'DONE (t=%0.2fs).'%( toc-tic )
+        print('DONE (t=%0.2fs).'%( toc-tic ))
-            print iStr.format(titleStr, typeStr, iouStr, areaStr, maxDetsStr, '%.3f'%(float(mean_s)))
+            print(iStr.format(titleStr, typeStr, iouStr, areaStr, maxDetsStr, '%.3f'%(float(mean_s))))
-            print self._anchors
+            print('feat_stride: {}'.format(self._feat_stride))
-            print 'scale: {}'.format(im_info[2])
+            print('im_size: ({}, {})'.format(im_info[0], im_info[1]))
-            print "resudial: {}".format((scores.shape[2] - height, scores.shape[3] - width))
+            print('score map size: {}'.format(scores.shape))
-            print 'num bg: {}'.format((labels == 0).sum())
+            print("labels=", labels)
-            print 'ratio: {:.3f}'.format(float(self._fg_num) / float(self._bg_num))
+            print("self._count=", self._count)
-    print 'Called with argument:', args
+    print('Called with argument:', args)
-    print args
+    print(args)
-    print 'Called with argument:', args
+    print('Called with argument:', args)
-    print 'output shape'
+    print('output shape')
-    print 'lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters
+    print('lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters)
-    print 'Called with argument:', args
+    print('Called with argument:', args)
-    print 'providing maximum shape', max_data_shape, max_label_shape
+    print('providing maximum shape', max_data_shape, max_label_shape)
-    print 'output shape'
+    print('output shape')
-    print 'lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters
+    print('lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters)
-    print 'Called with argument:', args
+    print('Called with argument:', args)
-                print 'Swapping BGR of caffe into RGB in mxnet'
+                print('Swapping BGR of caffe into RGB in mxnet')
-            print 'converting layer {0}, wmat shape = {1}, bias shape = {2}'.format(layer_name, wmat.shape, bias.shape)
+            print('converting layer {0}, wmat shape = {1}, bias shape = {2}'.format(layer_name, wmat.shape, bias.shape))
-                print weight_name + ' not found in arg_shape_dic.'
+                print(weight_name + ' not found in arg_shape_dic.')
-    print 'filtered %d roidb entries: %d -> %d' % (num - num_after, num, num_after)
+    print('filtered %d roidb entries: %d -> %d' % (num - num_after, num, num_after))
-    print args
+    print(args)
-    print 'Called with argument:', args
+    print('Called with argument:', args)
-    print 'providing maximum shape', max_data_shape, max_label_shape
+    print('providing maximum shape', max_data_shape, max_label_shape)
-    print 'output shape'
+    print('output shape')
-    print 'lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters
+    print('lr', lr, 'lr_epoch_diff', lr_epoch_diff, 'lr_iters', lr_iters)
-    print 'Called with argument:', args
+    print('Called with argument:', args)
-                print 'h', h[0].asnumpy()
+                print('pi', pi[0].asnumpy())
-            print final_score.squeeze()
+            print(score.squeeze())
-    print dataiter.provide_data
+    print(dataiter.provide_data)
-        print 'unable to open port' 
+        print('unable to open port')
-        print batch_size*100/(time.time() - tic)
+        print(batch_size*100/(time.time() - tic))
-    print memory.get_batch(5)
+    print(memory.obss)
-                    print "WARNING: detected shape " + str(self.data[i_bucket][:, idx].shape)
+                    print("WARNING: detected shape " + str(self.data[i_bucket][:, idx].shape))
-                print >> sys.stderr, "Missing labels for: ", self.utt_id
+                print(sys.stderr, "Missing labels for: ", self.utt_id)
-            print 'dim = {}'.format(dim)
+            print('Num samples = {}'.format(numSamples))
-            print 'Process file : "{}"'.format(featureFile)
+            print('Process file : "{}"'.format(featureFile))
-        print 'Read {} samples'.format(stats.GetNumberOfSamples())
+        print('Read {} samples'.format(stats.GetNumberOfSamples()))
-            print err, tok
+            print(err, tok)
-                print "unrecognized type"
+                print("unrecognized type")
-            print "error"
+            print("error")
-            print "unrecognized token", tok
+            print("unrecognized token", tok)
-    print filename
+    print(filename)
-    print "isBinary:", fileIsBinary(filename)
+    print("isBinary:", fileIsBinary(filename))
-            print err, tok
+            print(err, tok)
-        print "Warning dropout factors ignored here"
+        print("Warning dropout factors ignored here")
-            #print nnet[-1]["weights"][0][0:10]
+            #print(nnet[-1]["weights"][0][0:10])
-    print "-------- Foo class example --------"
+    print("-------- Foo class example --------")
-    print "Calling Foo_bar(): ",
+    print("Calling Foo_bar(): ",)
-    print "Result of Foo_sizex(): ", kaldi.Foo_sizex(a)
+    print()
-    print "-------- Kaldi SBFMReader and MatrixF class example --------"
+    print()
-    print "  This should match data.txt"
+    print("Read utterance:")
-    
+from __future__ import print_function
-    print "***********************" + s + "*************************"
+    print("***********************" + s + "*************************")
-    print "hi"
+    print("hi")
-    run_DNN(mnist_conf)
+    run_DNN(mnist_conf)
-    print 'Generated number: ' + num
+    print('Generated number: ' + num)
-    print 'Predicted label: ' + str(p)
+    print('Predicted label: ' + str(p))
-    print 'Predicted number: ' + pred
+    print('Predicted number: ' + pred)
-"""Initialization helper for mxnet"""
+"""Weight initialization"""
-    """Register optimizers to the optimizer factory"""
+    """Register an intializer to the initializer factory
-                super(Constant, self).__init__(value=value)
+    """The base class of an initializer.
-        """Save initializer to string"""
+        """Save the initializer to string"""
-        """Override () function to do Initialization
+        """Initialize an array
-            Initialization pattern Descriptor
+            Initialization pattern descriptor
-            ndarray to be Initialized
+            The array to be Initialized
-    """Initialize by loading pretrained param from file or dict
+    """Initialize by loading data from file or dict
-    """Initialize with mixed Initializer
+    """Initialize with multiple initializers
-    """Initialize the weight with uniform [-scale, scale]
+    """Initialize the weight with value uniformly sampled from ``[-scale, scale]``
-    """Initialize the weight with normal(0, sigma)
+    """Initialize the weight with value sampled according to ``normal(0, sigma)``
-    """Intialize weight as Orthogonal matrix
+    """Initialize weight as orthogonal matrix
-        u, _, v = np.linalg.svd(tmp, full_matrices=False)
+        u, _, v = np.linalg.svd(tmp, full_matrices=False) # pylint: disable=invalid-name
-            q = u
+            res = u
-        arr[:] = q
+            res = v
-    """Initialize the weight with Xavier or similar initialization scheme.
+    """Initialize the weight with Xavier or other similar schemes.
-        Use ```gaussian``` or ```uniform``` to init
+        Random generator type, can be ```gaussian`` or ``uniform``.
-        Use ```avg```, ```in```, or ```out``` to init
+        Can be ``avg``, ``in``, or ``out``
-        Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.
+    """Initialize the weight according to a MSRA paper.
-        Use ```avg```, ```in```, or ```out``` to init
+        Can be ``avg``, ``in``, or ``out``
-    """Initialize weight for upsampling layer"""
+    """Initialize weight for upsampling layers"""
-    """Initialze parameters for fused rnn layer
+    """Initialze parameters for fused rnn layers
-"""
+"""Scheduling learning rate"""
-        self.base_lr = 0.01
+    """Base class of a learning rate scheduler
-        Call to schedule current learning rate
+        """Return a new learning rate
-        non-decreasing, and increases at most by one.
+        The ``num_update`` is the upper bound of the number of updates applied to
-        a weight/index
+        Assume the optimizer has udpated *i*-th weight by *k_i* times, namely
-        See more details in https://github.com/dmlc/mxnet/issues/625
+            num_update = max([k_i for all i])
-    """Reduce learning rate in factor
+    """Reduce the learning rate by a factor for every *n* steps
-    be
+    It returns a new learning rate by::
-    base_lr * factor^(floor(n/step))
+        base_lr * pow(factor, floor(num_update/step))
-        the factor for reducing the learning rate
+    step : int
-    """Reduce learning rate in factor at steps specified in a list
+    """Reduce the learning rate by given a list of steps
-    be
+       step[k] <= num_update and num_update < step[k+1]
-    base_lr * factor^(sum((step/n)<=1)) # step is an array
+    Then calculate the new learning rate by::
-        schedule learning rate after n updates
+        The list of steps to schedule a change
-        the factor for reducing the learning rate
+        the factor to change the learning rate
-"""Common Optimization algorithms with regularizations."""
+"""Weight updating functions"""
-    opt_registry = {}
+    """The base class of all optimizers.
-        return klass
+    Parameters
-        """Create an optimizer with specified name.
+    param_idx2name : dict from int to string, optional
-            of a subclass of Optimizer. Case insensitive.
+    clip_gradient : float, optional
-            Rescaling factor on gradient. Normally should be 1/batch_size.
+    learning_rate : float, optional
-            Parameters for optimizer
+    lr_scheduler : LRScheduler, optional
-            raise ValueError('Cannot find optimizer %s' % name)
+    wd : float, optional
-        override in implementations."""
+        """Create auxiliary state for a given weight
-        """Update the parameters. override in implementations"""
+        """Update the weight by given the gradients and state
-        """set lr scale is deprecated. Use set_lr_mult instead."""
+    def set_lr_scale(self, args_lrscale): # pylint: disable=unused-argument
-        """Set individual learning rate multipler for parameters
+        """Set individual learning rate for each weight
-        """Set individual weight decay multipler for parameters.
+        """Set individual weight decay for each weight.
-        update num_update
+        """update num_update
-        """get learning rate for index.
+        """get the learning rate given the index of the weight.
-
+register = Optimizer.register   # pylint: disable=invalid-name
-    """A very simple SGD optimizer with momentum and weight regularization.
+    """The SGD optimizer with momentum and weight decay
-       momentum value
+    The optimizer updates the weight by::
-        L2 regularization coefficient add to all the weights
+      state = momentum * state + lr * rescale_grad * clip(grad, clip_gradient) + wd * weight
-        clip gradient in range [-clip_gradient, clip_gradient]
+    This optimizer accepts the following parameters in addition to :class:`.Optimizer`:
-        special treat weight decay in parameter ends with bias, gamma, and beta
+    Parameters
-    """ DCASGD optimizer with momentum and weight regularization.
+    """The DCASGD optimizer
-                    Delay Compensation for Distributed Deep Learning"
+    This optimizer implements the paper *Asynchronous Stochastic Gradient Descent with
-       momentum value
+       The momentum value
-    https://github.com/torch/optim/blob/master/sgd.lua
+    """Nesterov accelerated SGD
-        rescaling factor of gradient. Normally should be 1/batch_size.
+    """Stochastic Gradient Riemannian Langevin Dynamics
-        clip gradient in range [-clip_gradient, clip_gradient]
+    This optimizer implements the paper *Stochastic Gradient Riemannian Langevin
-@register
+@register  # pylint: disable=invalid-name
-    """Adam optimizer as described in [King2014]_.
+    """The Adam optimizer
-       http://arxiv.org/abs/1412.6980
+    This optimizer implements *Adam: A Method for Stochastic Optimization*,
-    https://github.com/mila-udem/blocks/blob/master/blocks/algorithms/__init__.py#L765
+    This optimizer accepts the following parameters in addition to :class:`.Optimizer`:
-        clip gradient in range [-clip_gradient, clip_gradient]
+        Small value to avoid divided by 0
-                 decay_factor=(1 - 1e-8), **kwargs):
+                 **kwargs):
-    """AdaGrad optimizer of Duchi et al., 2011,
+    """AdaGrad optimizer
-    in some cases.
+    This optimizer implements *Adaptive Subgradient Methods for
-        clip gradient in range [-clip_gradient, clip_gradient]
+        Small value to avoid divided by 0
-    """RMSProp optimizer of Tieleman & Hinton, 2012,
+    """The RMSProp optimizer.
-    For centered=False, the code follows the version in
+    If ``centered=False``, we follow
-    http://arxiv.org/pdf/1308.0850v5.pdf Eq(38) - Eq(45) by Alex Graves, 2013.
+    If ``centered=True``, we follow http://arxiv.org/pdf/1308.0850v5.pdf (38)-(45)
-        Only used if centered=True
+        "momentum" factor. Only used if centered=True
-        Default value is set to 1e-8.
+        Small value to avoid divided by 0
-
+        clip weights into range ``[-clip_weights, clip_weights]``
-    *ADADELTA: An adaptive learning rate method.*
+    """The AdaDelta optimizer.
-    http://arxiv.org/abs/1212.5701
+    This optimizer accepts the following parameters in addition to :class:`.Optimizer`:
-        clip gradient in range [-clip_gradient, clip_gradient]
+        Small value to avoid divided by 0
-create = Optimizer.create_optimizer
+create = Optimizer.create_optimizer  # pylint: disable=invalid-name
-    """updater for kvstore"""
+    """Updater for kvstore"""
-    if initializer != None:
+    if initializer is not None:
-        if text2id == None:
+        if text2id is None:
-        if read_content == None:
+        if read_content is None:
-    if eval_metrics == None:
+    if eval_metrics is None:
-            eval_metrics.append(mx.metric.create('top_k_accuracy', top_k = top_k))
+            eval_metrics.append(mx.metric.create('top_k_accuracy', top_k=top_k))
-            if epoch_end_callback != None:
+            if epoch_end_callback is not None:
-       if label_map != None:
+       if label_map is not None:
-   data = np.array(data,dtype=np.str_)
+   data = np.array(data, dtype=np.str_)
-              assert max_rnn_exec != None
+              assert max_rnn_exec is not None
-    if dic == None:
+    if dic is None:
-        if self.num != None:
+        if self.num is not None:
-            if i == None:
+            if i is None:
-    if dshape != None:
+    if dshape is not None:
-        if not annotation_file == None:
+        if annotation_file is not None:
-        if not iscrowd == None:
+        if iscrowd is not None:
-    def download( self, tarDir = None, imgIds = [] ):
+    def download(self, tarDir=None, imgIds=[]):
-        if text2id == None:
+        if text2id is None:
-        if read_content == None:
+        if read_content is None:
-        if text2id == None:
+        if text2id is None:
-        if read_content == None:
+        if read_content is None:
-        return numpy.power(self.GetStd(),2)
+        return numpy.power(self.GetStd(), 2)
-            if stats == None:
+            if stats is None:
-            header[0]['dim']=self.mean.shape[0]
+            header[0]['magicNumber'] = 21812
-    parser.add_argument('filename',help="Name of the stat file")
+    parser.add_argument('filename', help="Name of the stat file")
-        self.out_scp = open(self.scp_path,"w")
+        self.out_ark = open(self.ark_path, "w")
-            self.out_ark = open(self.ark_path,"w")
+            self.out_ark = open(self.ark_path, "w")
-        if self.out_scp != None:
+        if self.out_scp is not None:
-        if self.out_scp != None:
+        if self.out_scp is not None:
-        if self.out_scp != None:
+        if self.out_scp is not None:
-		return n
+class KaldiParser(object):
-	return binary
+    f = open(filename, "rb")
-	return net
+    f = open(filename, "rb")
-	print filename
+    filename = "exp/dnn4_pretrain-dbn_dnn/nnet_6.dbn_dnn.init"
-	a = file2nnet_binary(filename)
+    print "isBinary:", fileIsBinary(filename)
-	"""
+    """
-                if batch_end_callback != None:
+                if batch_end_callback is not None:
-                if eval_batch_end_callback != None:
+                if eval_batch_end_callback is not None:
-            if eval_end_callback != None:
+            if eval_end_callback is not None:
-            if batch_end_callback != None:
+            if batch_end_callback is not None:
-                    if initializer != None:
+                    if initializer is not None:
-    if shape != None:
+    if shape is not None:
-    if shape != None:
+    if shape is not None:
-from .context import Context
+from .context import Context, cpu
-
+    sym = mx.sym.Convolution(num_filter=3, kernel=(3,3), pad=(1,1), cudnn_off=True, name='conv')
-    ctx_list = [{'ctx': mx.gpu(0), 'conv_data': (2, 2, 5, 7, 7), 'type_dict': {'conv_data': np.float64}},
+    # 3D convolution
-            for nbatch, data_batch in enumerate(train_data):
+            nbatch = 0
-def test_bind():
+def test_bind(disable_bulk_exec=False):
-    test_bind()
+    test_bind(disable_bulk_exec=False)
-          image_shape='3,224,224', data_nthreads=4, label_name='softmax_label'):
+          image_shape='3,224,224', data_nthreads=4, label_name='softmax_label', max_num_examples=None):
-from common.util import download_file, get_gpus
+VAL_DATA='data/val-5k-256.rec'
-
+    return mx.test_utils.download(
-    accs = [.72, .75, .765, .76]
+    models = ['imagenet1k-resnet-50', 'imagenet1k-resnet-152']
-        (speed,) = score(model=m, data_val='data/val-5k-256.rec',
+        (speed,) = score(model=m, data_val=VAL_DATA,
-        print('testing %s, acc = %f, speed = %f img/sec' % (m, r, speed))
+        print('Tested %s, acc = %f, speed = %f img/sec' % (m, r, speed))
-    g = 0.72
+    g = 0.75
-                     data_val='data/val-5k-256.rec',
+                     data_val=VAL_DATA,
-    gpus = get_gpus()
+    gpus = mx.test_utils.list_gpus()
-    test_imagenet1k_inception_bn(gpus=gpus, batch_size=batch_size)
+    test_imagenet1k_resnet(**kwargs)
-        'caffemodel' : 'http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel' ,
+        # 'caffemodel' : 'http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel',
-        'caffemodel' : 'http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel',
+        # 'caffemodel' : 'http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel',
-    prototxt = _download_file(meta_info['prototxt'], model_name+'_deploy.prototxt')
+    prototxt = mx.test_utils.download(meta_info['prototxt'], model_name+'_deploy.prototxt')
-    caffemodel = _download_file(meta_info['caffemodel'], model_name+'.caffemodel')
+    caffemodel = mx.test_utils.download(meta_info['caffemodel'], model_name+'.caffemodel')
-        mean = _download_file(mean, model_name+'_mean.binaryproto')
+        mean = mx.test_utils.download(mean, model_name+'_mean.binaryproto')
-        input_dim = layer[0].input_param.shape._values[0].dim
+        input_dim = layer[0].input_param.shape[0].dim
-    models = ['bvlc_googlenet', 'vgg-16', 'vgg-19', 'resnet-50']
+    models = ['bvlc_googlenet', 'vgg-16', 'resnet-50']
-        return []
+    re = ''
-def download(url, fname=None, overwrite=False):
+def download(url, fname=None, dirname=None, overwrite=False):
-                os.makedirs(dir_name)
+    if dirname is None:
-                    raise OSError('failed to create ' + dir_name)
+                    raise OSError('failed to create ' + dirname)
-        os.chdir("..")
+def _get_model():
-def DumpImages(shape):
+def _dump_images(shape):
-        os.system("wget http://data.mxnet.io/data/inception-v3-dump.npz -P data/")
+def _get_data(shape):
-    GetTestData(shape)
+    _get_model()
-        DumpImages(shape)
+        _dump_images(shape)
-        disp_batches = 100,
+        batch_size     = 64,
-                assert new_vocab, "Unknow token %s"%word
+                assert new_vocab, "Unknown token %s"%word
-        with open(fname, 'rb') as f:
+        with open(caffemodel_fname, 'rb') as f:
-            Default `'acc'`. The performance measure used to display during training.
+            Default `'accuracy'`. The performance measure used to display during training.
-                    mode='lstm', prefix='lstm_%d'%layer,
+                    mode='lstm', prefix='lstm_%d'%layer, dropout=dropout,
-        cell = mx.rnn.FusedRNNCell(args.num_hidden, num_layers=args.num_layers,
+        cell = mx.rnn.FusedRNNCell(args.num_hidden, num_layers=args.num_layers, dropout=args.dropout,
-                                'wd': args.wd },
+        optimizer_params    = opt_params, 
-        os.system("wget http://deeplearning.net/data/mnist/mnist.pkl.gz -P data/")
+        os.system("wget -q http://deeplearning.net/data/mnist/mnist.pkl.gz -P data/")
-        os.system("wget http://data.mxnet.io/mxnet/data/mnist.zip -P data/")
+        os.system("wget -q http://data.mxnet.io/mxnet/data/mnist.zip -P data/")
-        os.system("wget http://data.mxnet.io/mxnet/data/cifar10.zip -P data/")
+        os.system("wget -q http://data.mxnet.io/mxnet/data/cifar10.zip -P data/")
-        for t in range(10):
+        for t in range(5):
-        arrayMask[int(lengths[i]):, i] = value 
+        arrayMask[int(lengths[i]):, i] = value
-        exe = result.simple_bind(default_context(), a=data_shape, 
+        exe = result.simple_bind(default_context(), a=data_shape,
-                idx_shape += (np.random.randint(low=3, high=5), ) 
+                idx_shape += (np.random.randint(low=3, high=5), )
-        
+
-        
+
-                    
+
-                        
+
-                    
+
-            
+
-                    
+
-                    
+
-                        
+
-                                out_grad[i, channel, yout,xout] 
+                                out_grad[i, channel, yout,xout]
-                    
+
-    
+
-    
+
-    
+
-            data_grad, grid_grad = bilinear_backward_numpy(out_grad,exe.arg_dict['data'].asnumpy(), 
+            data_grad, grid_grad = bilinear_backward_numpy(out_grad,exe.arg_dict['data'].asnumpy(),
-            
+
-        for t in range(50):
+        for t in range(10):
-                d = random.randint(1, 10)
+                d = random.randint(1, 5)
-                    mode='lstm', prefix='fused_rnn' + str(layer),
+                    mode='lstm', prefix='lstm_%d'%layer,
-        stack.add(cell)
+    if not args.stack_rnn:
-                                                         num_outputs=length, sequeeze_axis=1))
+                                                         num_outputs=length, squeeze_axis=1))
-                                                         num_outputs=length, sequeeze_axis=1))
+                                                         num_outputs=length, squeeze_axis=1))
-        name : InitDesc
+        desc : InitDesc
-            The order is in the same order as list_auxiliary()
+            The order is in the same order as list_auxiliary_states()
-            The order is in the same order as list_auxiliary()
+            The order is in the same order as list_auxiliary_states()
-            cells.append(cell)
+        stack = mx.rnn.SequentialRNNCell()
-        cell = mx.rnn.FusedRNNCell(args.num_hidden, num_layers=args.num_layers, mode='lstm')
+        cell = mx.rnn.FusedRNNCell(args.num_hidden, num_layers=args.num_layers,
-            output, _ = cell.unroll(seq_len, inputs=embed, merge_outputs=True, layout='TNC')
+        output, _ = cell.unroll(seq_len, inputs=embed, merge_outputs=True, layout='TNC')
-        pred = mx.sym.Reshape(output, shape=(-1, args.num_hidden))
+        pred = mx.sym.Reshape(output,
-        stack.add(mx.rnn.LSTMCell(num_hidden=args.num_hidden, prefix='lstm_l%d_'%i))
+        cell = mx.rnn.LSTMCell(num_hidden=args.num_hidden, prefix='lstm_l%d_'%i)
-        pred = mx.sym.Reshape(outputs, shape=(-1, args.num_hidden))
+        pred = mx.sym.Reshape(outputs,
-        return args.copy()
+        args = args.copy()
-        return args.copy()
+        args = args.copy()
-               input_prefix='', layout='NTC', merge_outputs=False):
+               input_prefix='', layout='NTC', merge_outputs=None):
-            if False, return outputs as a list of Symbols.
+            If False, return outputs as a list of Symbols.
-        return args
+    @property
-        self._h2h_bias = self.params.get("h2h_bias")
+        self._iW = self.params.get("i2h_weight")
-        return args
+    @property
-                                    bias=self._i2h_bias,
+                                    weight=self._iW,
-                                    bias=self._h2h_bias,
+                                    weight=self._hW,
-        h2h_z, h2h_r, h2h = symbol.SliceChannel(h2h, num_outputs=3, name="%s_h2h_slice" % name)
+        i2h_r, i2h_z, i2h = symbol.SliceChannel(i2h, num_outputs=3, name="%s_i2h_slice" % name)
-                                        name="%s_z_act" % name)
+        update_gate = symbol.Activation(i2h_z + h2h_z, act_type="sigmoid",
-        self._num_weights = len(self._weight_names)
+        self._directions = ['l', 'r'] if bidirectional else ['l']
-        d = ['l', 'r']
+        gate_names = self._gate_names
-                    if i > 0:
+        for layer in range(self._num_layers):
-                    name = '%s%s%d_h2h%s_weight'%(self._prefix, d[j], i, c[k])
+                for gate in gate_names:
-                    name = '%s%s%d_i2h%s_bias'%(self._prefix, d[j], i, c[k])
+        for layer in range(self._num_layers):
-                    name = '%s%s%d_h2h%s_bias'%(self._prefix, d[j], i, c[k])
+                for gate in gate_names:
-        m = self._num_weights
+        b = len(self._directions)
-        c = self._weight_names
+        m = self._num_gates
-               input_prefix='', layout='NTC', merge_outputs=False):
+               input_prefix='', layout='NTC', merge_outputs=None):
-        if not merge_outputs:
+        if merge_outputs is not None and not merge_outputs:
-        return sum([c.state_shape for c in self._cells], [])
+        return _cells_state_shape(self._cells)
-        return sum([c.begin_state(**kwargs) for c in self._cells], [])
+        return _cells_begin_state(self._cells, **kwargs)
-        return args
+        return _cells_unpack_weights(self._cells, args)
-        return args
+        return _cells_pack_weights(self._cells, args)
-                         'type_dict': {'take_indices': np.float64, 
+                idx_shape += (np.random.randint(low=3, high=5), )
-                         'type_dict': {'take_indices': np.float32, 
+                        {'ctx': mx.gpu(0), 'take_indices': idx_shape,
-                         'type_dict': {'take_indices': np.float16, 
+                        {'ctx': mx.gpu(0), 'take_indices': idx_shape,
-                         'type_dict': {'take_indices': np.float64, 
+                        {'ctx': mx.cpu(0), 'take_indices': idx_shape,
-                         'type_dict': {'take_indices': np.float32, 
+                        {'ctx': mx.cpu(0), 'take_indices': idx_shape,
-                         'type_dict': {'take_indices': np.float16, 
+                        {'ctx': mx.cpu(0), 'take_indices': idx_shape,
-                                                            size=idx_shape), 
+            arg_params = {'take_indices': np.random.randint(low=0,
-            check_consistency(sym, ctx_list, 
+            check_consistency(sym, ctx_list,
-            rets.append(nd.concatenate(tensors, axis=axis, always_copy=False))
+            # pylint: disable=no-member,protected-access
-from .base import numeric_types
+from .base import numeric_types
-                self.imgrec = recordio.MXRecordIO(path_imgrec, 'r')
+                self.imgrec = recordio.MXRecordIO(path_imgrec, 'r') # pylint: disable=redefined-variable-type
-                key = str(index)
+                key = str(index) # pylint: disable=redefined-variable-type
-            data = OrderedDict([(default_name, data[0])])
+            data = OrderedDict([(default_name, data[0])]) # pylint: disable=redefined-variable-type
-            data = OrderedDict([('_%d_%s' % (i, default_name), d) for i, d in enumerate(data)])
+            data = OrderedDict( # pylint: disable=redefined-variable-type
-            hdlr = logging.StreamHandler()
+            hdlr = logging.StreamHandler() # pylint: disable=redefined-variable-type
-from .callback import LogValidationMetricsCallback
+from .callback import LogValidationMetricsCallback # pylint: disable=wrong-import-position
-        for key in kwargs.keys():
+        for key in kwargs:
-            buck = bisect.bisect_left(buckets, len(sentences[i]))
+        for i, sent in enumerate(sentences):
-            buff[:len(sentences[i])] = sentences[i]
+            buff[:len(sent)] = sent
-                                        mode, bidirectional)
+            initializer = init.FusedRNN( # pylint: disable=redefined-variable-type
-            states = {'state': states[0], 'state_cell': states[1]}
+            states = {'state': states[0], 'state_cell': states[1]} # pylint: disable=redefined-variable-type
-import mxnet as mx
+import numpy as np
-    for node in nodes:
+    for node in nodes:          # pylint: disable=too-many-nested-blocks
-          image_shape='3,224,224', data_nthreads=4):
+def score(model, data_val, metrics, gpus, batch_size, rgb_mean=None, mean_img=None,
-    rgb_mean = [float(i) for i in rgb_mean.split(',')]
+    if mean_img is not None:
-        mean_b             = rgb_mean[2],
+        label_name         = label_name,
-        rand_mirror        = False)
+        rand_mirror        = False,
-        model, os.path.join(dir_path, 'model'))
+    if isinstance(model, str):
-    mod = mx.mod.Module(symbol=sym, context=devs)
+    mod = mx.mod.Module(symbol=sym, context=devs, label_names=[label_name,])
-    download_file('http://data.mxnet.io/data/val-5k-256.rec', 'data/val-5k-256.rec')
+    if not os.path.isdir('data'):
-
+import subprocess
-    main()
+import re
-    aux_names = prob.list_auxiliary_states()
+import caffe_parser
-    for layer_name, layer_type, layer_blobs in iter:
+    layers, names = caffe_parser.read_caffemodel(prototxt_fname, caffemodel_fname)
-                    print ('Swapping BGR of caffe into RGB in mxnet')
+                    # Swapping BGR of caffe into RGB in mxnet
-    return prob, arg_params, aux_params, input_dim
+    if output_prefix is not None:
-import sys
+import caffe_parser
-        solver_config = caffe.proto.caffe_pb2.NetParameter()
+def _get_input(proto):
-    return parser_object
+        raise ValueError('Cannot find input size')
-def conv_param_to_string(param):
+def _convert_conv_param(param):
-    prev_bn = None
+    prev_name = None
-            param_string = conv_param_to_string(layer[i].convolution_param)
+            param_string = _convert_conv_param(layer[i].convolution_param)
-            param_string = conv_param_to_string(layer[i].convolution_param)
+            param_string = _convert_conv_param(layer[i].convolution_param)
-                raise Exception("Unknown Pooling Method!")
+            param_string = _convert_pooling_param(layer[i].pooling_param)
-                           (param.alpha, param.beta, param.k, param.local_size)
+            param_string = "alpha=%f, beta=%f, knorm=%f, nsize=%d" % (
-            param_string = "num_hidden=%d, no_bias=%s" % (param.num_output, not param.bias_term)
+            param_string = "num_hidden=%d, no_bias=%s" % (
-            type_string = 'split'
+            type_string = 'split'  # will process later
-            prev_bn = re.sub('[-/]', '_', layer[i-1].name)
+            prev_name = re.sub('[-/]', '_', layer[i-1].name)
-                           (param.shape.dim[0], param.shape.dim[1], param.shape.dim[2], param.shape.dim[3])
+            param_string = "shape=(%s)" % (','.join(param.shape.dim),)
-
+            symbol_string += "%s = %s\n" % (name, prev_name)
-            raise Exception('Unknown Layer %s!' % layer[i].type)
+            raise ValueError('Unknown layer %s!' % layer[i].type)
-                        (flatten_name, flatten_name, mapping[bottom[0]])
+                    symbol_string += "%s=mx.symbol.Flatten(name='%s', data=%s)\n" % (
-                    (name, type_string, name, mapping[bottom[0]], param_string)
+                symbol_string += "%s = %s(name='%s', data=%s %s)\n" % (
-                    (name, type_string, name, ','.join([mapping[x] for x in bottom]), param_string)
+                symbol_string += "%s = %s(name='%s', *[%s] %s)\n" % (
-          + sym
+def convert_symbol(prototxt_fname):
-
+    parser = argparse.ArgumentParser(
-    main()
+"""Test converted models
-    depth network).
+    """The base class of a modules.
-        just call `.asnumpy()` on each of the `NDArray`.
+        list of NDArray or list of list of NDArray
-            >>> mod.predict(eval_data=val_dataiter, num_batch=10)
+        >>> #Predict on the first 10 batches of val_dataiter
-        `(arg_params, aux_params)`, a pair of dictionary of name to value mapping.
+        `(arg_params, aux_params)`
-            'fc2_bias': <NDArray 64 @cpu(0)>, 'fc1_bias': <NDArray 128 @cpu(0)>}, {})
+        >>> print mod.get_params()
-        elements are `NDArray`.
+        list of NDArray or list of list of NDArray
-        might live on different devices.
+        list of NDArray or list of list of NDArray
-        might live on different devices.
+        list of NDArray or list of list of NDArray
-        training (in this case, label information is not available).
+            A list of `(name, shape)` pairs. The return value could be `None` if
-        elements are `NDArray`.
+        list of NDArray or list of list of NDArray
-        elements are `NDArray`.
+        list of NDArray or list of list of NDArray
-        elements are `NDArray`.
+        list of NDArray or list of list of NDArray
-        series of `add` calls.
+        self
-        first module is the data shape of a `SequentialModule`.
+        list
-        training (in this case, label information is not available).
+        list
-        module is the output shape of a `SequentialModule`.
+        list
-        in the modules.
+        (arg_params, aux_params)
-        elements are numpy arrays.
+        list of NDArray or list of list of NDArray
-        elements are `NDArray`.
+        list of NDArray or list of list of NDArray
-    def label_from_inde(self, index):
+    def label_from_index(self, index):
-            return None
+            return (None,
-            return zeros(weight.shape, weight.context, dtype=weight.dtype)
+            return (zeros(weight.shape, weight.context, dtype=weight.dtype), # momentum
-            mom = state
+        mom, previous_weight = state
-            weight[:] += mom
+            mom[:] += -lr * (grad + wd * weight + self.lamda \
-                self.weight_previous[index] = weight
+            assert(self.momentum == 0.0)
-    'omp.h', 'execinfo.h', 'packet/sse-inl.h', 'emmintrin.h'
+    'omp.h', 'execinfo.h', 'packet/sse-inl.h', 'emmintrin.h', 'thrust/device_vector.h'
-sys.path.insert(0, os.path.join(curr_path, "../../example/image-classification/symbol"))
+sys.path.insert(0, os.path.join(curr_path, "../../example/image-classification/symbols"))
-  print "usage: %s <hostfile>" % sys.argv[0]
+if len(sys.argv) != 4:
-prog_name = "train_imagenet"
+user = sys.argv[2]
-    "xargs kill"
+    "ps aux | "
-  print "Done killing"
+# Kill program on remote machines
-        if not (dtype.startswith('NDArray') or dtype.startswith('Symbol')):
+        if not (dtype.startswith('NDArray') or
-warnings.filterwarnings('default', category=DeprecationWarning)
+import inspect
-    a new empty ndarray handle
+    handle
-    a new empty ndarray handle
+    handle
-    """NDArray object in mxnet.
+    """An array object represents a multidimensional, homogeneous array of
-    NDArray is basic ndarray/Tensor like data structure in mxnet.
+        """Return a string representation of the array"""
-        """Set ndarray value.
+    def __setitem__(self, key, value):
-        The following modes are supported:
+        Set self[key] to value.
-        - `array[:, i, j:k] = value`: each index could be a python slice or an int.
+        Parameters
-            sliced_arr = self._at(in_slice)
+            raise ValueError('Failed to assign to a readonly NDArray')
-                sliced_arr = self._slice(in_slice.start, in_slice.stop)
+        if isinstance(key, py_slice):
-        if isinstance(in_slice, tuple):
+        if isinstance(key, tuple):
-            for slice_i in in_slice:
+            assert len(key) == len(my_shape)
-            for i, slice_i in enumerate(in_slice):
+            for i, slice_i in enumerate(key):
-            return self
+    def __getitem__(self, key):
-        source_array : array_like
+        source_array : array_like)
-        """Return a sub NDArray that shares memory with current one.
+        """Return a sliced view of this array
-        """Return a reshaped NDArray that shares memory with current one.
+    def reshape(self, shape):
-            new shape of NDArray
+        shape : tuple of int
-                                         c_array(ctypes.c_int, new_shape),
+                                         len(shape),
-        the same with `numpy`'s broadcasting
+        """Broadcast an array to a new shape.
-            the broadcast shape
+        ----------
-        function returns.
+        """Wait until all previous writes operations on current array are finished.
-        a tuple representing shape of current ndarray
+        """Tuple of array dimensions.
-        an int representing size of current ndarray
+        """Number of elements in the array.
-            The context of current NDArray.
+        """Device context of the array.
-        """Get data type of current NDArray.
+        """Data-type of the arrayâs elements.
-        an numpy.dtype object representing type of current ndarray
+        numpy.dtype
-        """Get transpose of current NDArray"""
+        """Return a copy of the array with axes transposed
-            A copy of array content.
+        """Return a ``numpy.ndarray`` object with value copied from this array
-        """Return a CPU scalar(float) of current ndarray.
+        """Return a scalar with value copied from this array
-        This ndarray must have shape (1,)
+        It equals to ``self.asnumpy()[0]``. This ndarray must have shape (1,).
-            The scalar representation of the ndarray.
+        Examples
-        """Return a copied numpy array of current array with specified type.
+        """Return a copy of the array that is casted to a specified type.
-            A copy of array content.
+        dtype : numpy.dtype or str
-        """Copy the content of current array to other.
+        """Copy the value of this array to another array.
-        will be created as target
+        If ``other`` is a ``NDArray`` object, then ``other.shape`` and
-            Target NDArray or context we want to copy data to.
+        other : NDArray or Context)
-            The copy target NDArray
+        NDArray
-        """Make a copy of the current ndarray on the same context
+        """Make a copy of the ndarray on the same context
-            The copy
+        Returns
-        made.
+        """Return an array on the target device with value as same as this array
-            The target context we want the return value to live in.
+            The target context.
-        A copy or `self` as an `NDArray` that lives in the target context.
+        NDArray
-
+    Deprecated, use ``one_hot`` instead.
-        An NDArray containing indices of the categorical features.
+    shape : int or tuple of int
-        The result holder of the encoding.
+    Returns
-        Same as out.
+    NDArray
-    return _internal._onehot_encode(indices, out, out=out)
+    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype)
-    """Create an empty uninitialized new NDArray, with specified shape.
+    Parameters
-        shape of the NDArray.
+    source_array : array_like
-        The context of the NDArray, default to current default context.
+        An optional device context (default is the current default context)
-        The created NDArray.
+    NDArray
-    return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))
+    return _internal._arange(start=start, stop=stop, step=step, repeat=repeat,
-    ----------
+    --------
-    out: NDArray
+    --------
-    """ Perform element-wise addition
+    """Add arguments, element-wise with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """ Perform element-wise subtract
+    """Subtract arguments element-wisely with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """ Perform element-wise multiplication
+    """Multiply arguments element-wisely with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """ Perform element-wise divide
+    """Divide arguments element-wisely with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """ Perform power operator
+def power(base, exp):
-        right hand side operand
+    base : scalar or NDArray
-        result array
+    --------
-        rhs,
+        base,
-    """ Perform maximum operator
+    """Element-wise maximum of array elements with broadcasting.
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """ Perform minimum operator
+    """Element-wise minimum of array elements with broadcasting.
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """Return (lhs == rhs) element-wise.
+    """Return (lhs == rhs), element-wise with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """Return (lhs != rhs) element-wise.
+    """Return (lhs != rhs), element-wise with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """Return (lhs > rhs) element-wise.
+    """Return (lhs > rhs), element-wise with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """Return (lhs >= rhs) element-wise.
+    """Return (lhs >= rhs), element-wise with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """Return (lhs < rhs) element-wise.
+    """Return (lhs < rhs), element-wise with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    """Return (lhs <= rhs) element-wise.
+    """Return (lhs <= rhs), element-wise with broadcasting
-        right hand side operand
+    lhs : scalar or array
-        result array
+    NDArray
-    regardless of input types.
+    """Same as ``divide``
-    return multiply(arr, -1.0)
+    """Numerical negative, element-wise.
-    """Create a new NDArray filled with 0, with specified shape.
+    Equals ``-arr``
-        The value type of the NDArray, default to np.float32
+    arr : NDArray
-        The created NDArray.
+    NDArray
-    # pylint: enable= no-member, protected-access
+    return multiply(arr, -1.0)
-    """Create a new NDArray filled with 1, with specified shape.
+    See more details in ``save``.
-        The value type of the NDArray, default to np.float32
+    fname : str
-        The created NDArray.
+    list of NDArray or dict of str to NDArray
-    # pylint: enable= no-member, protected-access
+    if not isinstance(fname, string_types):
-        The value type of the NDArray, default to np.float32
+def save(fname, data):
-    return arr
+    Examples of filenames:
-    """Create a new NDArray that copies content from source_array.
+    - ``/path/to/file``
-        The created NDArray.
+    fname : str
-    return arr
+    handles = []
-    """Concatenate a list of NDArrays along the first dimension.
+    """DEPRECATED, use ``concat`` instead
-    An `NDArray` that lives on the same context as `arrays[0].context`.
+    NDArray
-    """Decode an image from string. Requires OpenCV to work.
+    """DEPRECATED, use mx.img instead
-        desc += '\nThis function support variable length of positional input.'
+    # if key_var_num_args:
-# Use different verison of SymbolBase
+# Use different version of SymbolBase
-        """Invoke symbol as function on inputs.
+        """Compose symbol on inputs.
-def Variable(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None, init=None):
+def var(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None, init=None):
-        See Also https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html.
+    """Return a new symbol of given shape and type, filled with zeros.
-        See Also https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html.
+    """Return a new symbol of given shape and type, filled with ones.
-        See Also https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html.
+    """Return evenly spaced values within a given interval.
-               'symbol: Symbol\n' +
+               'Symbol\n' +
-    prob, input_dim = proto2symbol(args.caffe_prototxt)
+    sym, arg_params, aux_params, input_dim = process_caffe_model(args.caffe_prototxt, args.caffe_model)
-        net_caffe = caffe.Net(args.caffe_prototxt, args.caffe_model, caffe.TEST)
+        net_caffe = caffe.Net(caffe_prototxt, caffe_model, caffe.TEST)
-        layers = parse.parse_caffemodel(args.caffe_model)
+        layers = parse.parse_caffemodel(caffe_model)
-
+    aux_params = {}
-                                layer_blobs[0].width]
+                    wmat_dim = [layer_blobs[0].num, layer_blobs[0].channels, layer_blobs[0].height, layer_blobs[0].width]
-            bias = np.array(layer_blobs[1].data)
+
-                    print('Swapping BGR of caffe into RGB in mxnet')
+                    print ('Swapping BGR of caffe into RGB in mxnet')
-            print('converting layer {0}, wmat shape = {1}, bias shape = {2}'.format(layer_name, wmat.shape, bias.shape))
+            assert(wmat.flags['C_CONTIGUOUS'] is True)
-    model.init_params(arg_params=arg_params, aux_params={})
+        elif layer_type == 'Scale':
-    model.save_checkpoint(args.save_model_name, 1)
+    prev_bn = None
-            param_string = 'use_global_stats=%s' % param.use_global_stats
+            param_string = 'use_global_stats=%s, fix_gamma=False' % param.use_global_stats
-        if type_string == '':
+        if layer[i].type == 'Eltwise':
-        if type_string != 'split':
+        elif type_string != 'split':
-                                     (flatten_name, flatten_name, mapping[bottom[0]])
+                        (flatten_name, flatten_name, mapping[bottom[0]])
-                                 (name, type_string, name, mapping[bottom[0]], param_string)
+                    (name, type_string, name, mapping[bottom[0]], param_string)
-                                 (name, type_string, name, ','.join([mapping[x] for x in bottom]), param_string)
+                    (name, type_string, name, ','.join([mapping[x] for x in bottom]), param_string)
-
+# When training a deep, complex model, it's recommended to stack fused RNN cells (one
-    cell = mx.rnn.FusedRNNCell(args.num_hidden, num_layers=args.num_layers, mode='lstm')
+    if args.stack_rnn:
-        output, _ = cell.unroll(seq_len, inputs=embed, merge_outputs=True, layout='TNC')
+        if args.stack_rnn:
-from .ndarray import sgd_update, sgd_mom_update, adam_update, rmsprop_update
+from .ndarray import sgd_update, sgd_mom_update, adam_update, rmsprop_update, rmspropalex_update
-    by Alex Graves, 2013.
+    For centered=False, the code follows the version in
-        Default value is set to 0.002.
+        Default value is set to 0.001.
-        Default value is set to 0.95.
+        decay factor of moving average for gradient^2.
-        rescaling factor of gradient. Normally should be 1/batch_size.
+        rescaling factor of gradient.
-                 epsilon=1e-8, **kwargs):
+    def __init__(self, learning_rate=0.001, gamma1=0.9, gamma2=0.9,
-        self.kwargs = {'gamma1': gamma1, 'gamma2': gamma2, 'epsilon': epsilon,
+        self.centered = centered
-        """Create additional optimizer state: mean, variance
+        """Create additional optimizer state.
-        return (zeros(weight.shape, weight.context),  # n
+        if self.centered:
-                       lr=lr, wd=wd, **self.kwargs)
+        if not self.centered:
-        grad *= self.rescale_grad
+
-    by Alex Graves, 2013.
+    For centered=False, the code follows the version in
-        Default value is set to 0.002.
+        Default value is set to 0.001.
-        Default value is set to 0.95.
+        Default value is set to 0.9.
-                 epsilon=1e-8, **kwargs):
+    def __init__(self, learning_rate=0.001, gamma1=0.9, gamma2=0.9,
-        """Create additional optimizer state: mean, variance
+        """Create additional optimizer state.
-                mx.nd.zeros(weight.shape, weight.context))  # delta
+        if self.centered:
-        weight[:] += delta
+        grad = grad * self.rescale_grad + wd * weight
-              {'rescale_grad': 0.8, 'wd': 0.05}]
+              {'rescale_grad': 0.8, 'wd': 0.05},
-from ..io import DataDesc
+from .base_module import BaseModule, _check_input_names, _parse_data_desc
-            self._label_shapes = None
+        self._data_shapes, self._label_shapes = _parse_data_desc(
-            self._label_shapes = None
+        self._data_shapes, self._label_shapes = _parse_data_desc(
-            outputs = list(symbol.SliceChannel(outputs, axis=axis, num_outputs=length,
+            outputs = list(symbol.SliceChannel(outputs, axis=0, num_outputs=length,
-    mx.viz.print_summary(fc2)
+    sc1 = mx.symbol.SliceChannel(data=fc2, num_outputs=10, name="slice_1", squeeze_axis=0)
-    mx.viz.print_summary(fc2, shape)
+    mx.viz.print_summary(sc1, shape)
-    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label, start_label=start_label)
+    sentences, vocab = mx.rnn.encode_sentences(lines, vocab=vocab, invalid_label=invalid_label,
-                        force_rebind=False, shared_module=self._curr_module)
+                        force_rebind=False, shared_module=self._buckets[self._default_bucket_key])
-
+        #In the future we should have a better way to profile memory per device (haibin)
-                    raise ValueError('Find name \"%s\" that is not in the arguments' % name)
+            elif not allow_extra_params:
-            aux_params = {}
+            return
-                    raise ValueError('Find name %s that is not in the auxiliary states' % name)
+            elif not allow_extra_params:
-# pylint: disable=too-many-instance-attributes, too-many-arguments
+# pylint: disable=too-many-instance-attributes, too-many-arguments, protected-access
-        return self._curr_module.get_params()
+        self._curr_module._params_dirty = self._params_dirty
-
+    def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True):
-            stack.add(mx.rnn.LSTMCell(num_hidden=args.num_hidden, prefix='lstm_l%d_'%i))
+        stack.reset()
-            whether this forward is for evaluation purpose.
+            whether this forward is for evaluation purpose. If True,
-import logging
+import logging
-from .base_module import BaseModule
+from .base_module import BaseModule, _check_input_names
-                 logger=logging, context=ctx.cpu(), work_load_list=None):
+    def __init__(self, sym_gen, default_bucket_key=None, logger=logging,
-
+
-                        context=self._context, work_load_list=self._work_load_list)
+                        context=self._context, work_load_list=self._work_load_list,
-                            work_load_list=self._work_load_list)
+                            work_load_list=self._work_load_list,
-                 logger=logging, fixed_param_names=None, grad_req='write'):
+                 for_training, inputs_need_grad, shared_group=None, logger=logging,
-            if name in self.param_names: # model parameter
+            if name in self.param_names: # model parameters
-            else: # data or label
+            else: # data, label, or states
-from .base_module import BaseModule
+from .base_module import BaseModule, _check_input_names
-                 logger=logging, context=ctx.cpu(), work_load_list=None, fixed_param_names=None):
+                 logger=logging, context=ctx.cpu(), work_load_list=None,
-        data_names = list(data_names)
+        data_names = list(data_names) if data_names is not None else []
-        input_names = data_names + label_names
+        input_names = data_names + label_names + state_names
-                                                     grad_req=grad_req)
+                                                     grad_req=grad_req,
-        return output, output
+        return output, [output]
-            states = {'state': states}
+            states = {'state': states[0]}
-            outputs, states = rnn[0], rnn[1]
+            outputs, states = rnn[0], [rnn[1]]
-            states = symbol.Dropout(data=states, p=self.dropout_states)
+            states = [symbol.Dropout(data=i, p=self.dropout_states) for i in states]
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exe_c.forward()
+    exe_c.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-    exe_square.forward()
+    exe_square.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-    exe.forward()
+    exe.forward(is_train=True)
-    exe.forward()
+    exe.forward(is_train=True)
-        y.forward()
+        y.forward(is_train=True)
-        y.forward()
+        y.forward(is_train=True)
-            exec1.forward()
+            exec1.forward(is_train=True)
-            exec2.forward()
+            exec2.forward(is_train=True)
-    exe1.forward()
+    exe1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exec1.forward()
+    exec1.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-    exe_test.forward()
+    exe_test.forward(is_train=True)
-        exe.forward()
+        exe.forward(is_train=True)
-        exe.forward()
+        exe.forward(is_train=True)
-        exe.forward()
+        exe.forward(is_train=True)
-        exe.forward()
+        exe.forward(is_train=True)
-        exe.forward()
+        exe.forward(is_train=True)
-        exe_add.forward()
+        exe_add.forward(is_train=True)
-            exe.forward()
+            exe.forward(is_train=True)
-            exe_addto.forward()
+            exe_addto.forward(is_train=True)
-            exe.forward()
+            exe.forward(is_train=True)
-    yexec.forward()
+    yexec.forward(is_train=True)
-            inputs = symbol.Concat(*inputs, dim=1)
+            inputs = [symbol.expand_dims(i, axis=0) for i in inputs]
-    """Initialize the weight to 1"""
+    """Initialize the weight to a scalar value"""
-                                                      **dict(data_shapes_i + label_shapes_i))
+                self.execs[i] = self._default_execs[i].reshape(
-                optimizer_params['rescale_grad'] = 1.0/batch_size
+                optimizer_params['rescale_grad'] = rescale_grad
-from ..base import numeric_types, string_types
+from ..base import string_types
-    def begin_state(self, init_sym=symbol.zeros, **kwargs):
+    def begin_state(self, func=symbol.zeros, **kwargs):
-            ones, uniform, normal, etc.
+        func : callable, default symbol.zeros
-            more keyword arguments passed to init_sym. For example
+            more keyword arguments passed to func. For example
-                                shape=shape, **kwargs)
+        states = []
-        return recursive(state_shape)
+                state = func(name='%sbegin_state_%d'%(self._prefix, self._init_counter),
-        return (0, self._num_hidden)
+        return [(0, self._num_hidden)]
-        h2h = symbol.FullyConnected(data=states, weight=self._hW, bias=self._hB,
+        h2h = symbol.FullyConnected(data=states[0], weight=self._hW, bias=self._hB,
-            return (b*self._num_layers, 0, self._num_hidden)
+        n = (self._mode == 'lstm') + 1
-            inputs = symbol.Concat(inputs, dim=0)
+            inputs = [symbol.expand_dims(i, axis=1) for i in inputs]
-        return [c.state_shape for c in self._cells]
+        return sum([c.state_shape for c in self._cells], [])
-        return [c.begin_state(**kwargs) for c in self._cells]
+        return sum([c.begin_state(**kwargs) for c in self._cells], [])
-        for cell, state in zip(self._cells, states):
+        p = 0
-        return inputs, next_states
+        return inputs, sum(next_states, [])
-        """Get a new grouped symbol whose output contains all the internal outputs of this symbol.
+        """Get a new grouped symbol whose output contains
-            return self._infer_shape_impl(False, *args, **kwargs)
+            res = self._infer_shape_impl(False, *args, **kwargs)
-                              'fc2_weight', 'fc2_bias']
+    assert net1.list_arguments() == ['data', 'fc1_weight', 'fc1_bias', 'fc2_weight', 'fc2_bias']
-    model.save("ocr")
+    model.save(prefix)
-        self.predictor.forward(data=img)
+        self.predictor.forward(data=img, **self.init_state_dict)
-    def __init__(self, num_classes, data_shape, max_iter):
+    def __init__(self, num_classes, data_shape, max_iter, dtype):
-        self.label = mx.nd.array(label)
+        self.data = mx.nd.array(data, dtype=self.dtype)
-        return [('data',self.data.shape)]
+        return [mx.io.DataDesc('data', self.data.shape, self.dtype)]
-        return [('softmax_label',(self.batch_size,))]
+        return [mx.io.DataDesc('softmax_label', (self.batch_size,), self.dtype)]
-        train = SyntheticDataIter(args.num_classes, data_shape, 50)
+        train = SyntheticDataIter(args.num_classes, data_shape, 50, dtype)
-       rnd_type='gaussian', factor_type="in", magnitude=2)
+    if args.network == 'alexnet':
-    conv1 = mx.symbol.Convolution(
+    conv1 = mx.symbol.Convolution(name='conv1',
-    lrn1 = mx.symbol.LRN(data=pool1, alpha=0.0001, beta=0.75, knorm=1, nsize=5)
+        data=lrn1, pool_type="max", kernel=(3, 3), stride=(2,2))
-        data=lrn1, kernel=(5, 5), pad=(2, 2), num_filter=256)
+    conv2 = mx.symbol.Convolution(name='conv2',
-    lrn2 = mx.symbol.LRN(data=pool2, alpha=0.0001, beta=0.75, knorm=1, nsize=5)
+    lrn2 = mx.symbol.LRN(data=relu2, alpha=0.0001, beta=0.75, knorm=2, nsize=5)
-        data=lrn2, kernel=(3, 3), pad=(1, 1), num_filter=384)
+    conv3 = mx.symbol.Convolution(name='conv3',
-    conv4 = mx.symbol.Convolution(
+    conv4 = mx.symbol.Convolution(name='conv4',
-    conv5 = mx.symbol.Convolution(
+    conv5 = mx.symbol.Convolution(name='conv5',
-    fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=4096)
+    fc1 = mx.symbol.FullyConnected(name='fc1', data=flatten, num_hidden=4096)
-    fc2 = mx.symbol.FullyConnected(data=dropout1, num_hidden=4096)
+    fc2 = mx.symbol.FullyConnected(name='fc2', data=dropout1, num_hidden=4096)
-    fc3 = mx.symbol.FullyConnected(data=dropout2, num_hidden=num_classes)
+    fc3 = mx.symbol.FullyConnected(name='fc3', data=dropout2, num_hidden=num_classes)
-         return pallete
+    # this function is to get the colormap for visualizing the segmentation mask
-            outputs = list(symbol.SliceChannel(outputs, aixs=axis, num_outputs=length,
+            outputs = list(symbol.SliceChannel(outputs, axis=axis, num_outputs=length,
-        """Igore roll over data and set to start"""
+        """Ignore roll over data and set to start"""
-        if isinstance(in_slice, slice):
+        if isinstance(in_slice, py_slice):
-                assert isinstance(slice_i, (slice, int))
+                assert isinstance(slice_i, (py_slice, int))
-                if isinstance(slice_i, slice):
+                if isinstance(slice_i, py_slice):
-        if not isinstance(in_slice, slice) or in_slice.step is not None:
+        if not isinstance(in_slice, py_slice) or in_slice.step is not None:
-        for t in range(5):
+        for t in range(50):
-        assert_almost_equal(exe.grad_dict['affine'].asnumpy(), grad_est, rtol=1e-3)
+        assert_almost_equal(exe.grad_dict['affine'].asnumpy(), grad_est, rtol=1e-3, atol=1e-5)
-        assert_almost_equal(exe.grad_dict['affine'].asnumpy(), grad_est + grid_grad_npy, rtol=1e-2)
+        assert_almost_equal(exe.grad_dict['affine'].asnumpy(), grad_est + grid_grad_npy, rtol=1e-2, atol=1e-5)
-        data = mx.img.imresize(data, self._data_shape[0], self._data_shape[1], interp_method)
+        data = mx.img.imresize(data, self._data_shape[1], self._data_shape[0], interp_method)
-    'omp.h', 'execinfo.h', 'packet/sse-inl.h'
+    'omp.h', 'execinfo.h', 'packet/sse-inl.h', 'emmintrin.h'
-    print >>out, "//===== EXPANDING: %s =====\n" %x
+    whtspace = '  '*expand.treeDepth
-    print >>out, "//===== EXPANDED: %s =====\n" %x
+            expand.treeDepth-=1
-
+# Write to amalgamation file
-                'nnpack' not in h): sysheaders.append(h)
+                'nnpack' not in h and
-    kwargs = [{}]
+    kwargs = [{},
-              {'rescale_grad': 0.8}]
+              {'rescale_grad': 0.8},
-        num_input = arr.size/b/h/m - (self._num_layers - 1)*(h+b*h+2) - h - 2
+        num_input = arr.size//b//h//m - (self._num_layers - 1)*(h+b*h+2) - h - 2
-    if not os.path.exists('data/cifar10.zip'):
+    if (not os.path.exists('data/cifar/train.rec')) or \
-    assert_allclose(mod1.get_outputs()[0].asnumpy(), mod2.get_outputs()[0].asnumpy(), rtol=1e-2)
+    assert_allclose(mod1.get_outputs()[0].asnumpy(), mod2.get_outputs()[0].asnumpy(), rtol=1e-2, atol=1e-4)
-    x2 = np.array([[0, 1.1, 1.1, 6.2, 6.2], [2, 6.1, 2.1, 8.2, 11.2], [1, 3.1, 1.1, 5.2, 10.2], [0, 3, 3, 3, 3]])
+    x1 = np.random.rand(4, 3, 12, 8).astype('float32')
-                               numeric_eps=1e-4, rtol=1e-1, atol=1E-4)
+    check_numeric_gradient(sym=test, location=[x1, x2],
-    
+    np.random.seed(1234)
-from lstm import lstm_unroll
+from lstm_proj import lstm_unroll
-        out = np.zeros(data.shape[:2] + grid.shape[2:])
+        out = np.zeros(data.shape[:2] + grid.shape[2:], dtype=np.float32)
-                    xWeightTopLeft = 1-abs(xcoord - xInTopLeft)
+                    xcoord = np.float32((grid[i, 0, yout, xout] + 1) * (input_width-1) / 2.0)
-                    yWeightTopLeft = 1-abs(ycoord - yInTopLeft)
+                    yWeightTopLeft = np.float32(1-(ycoord - yInTopLeft))
-                        
+
-                            if between(xInTopLeft,0,input_width-1) and between(yInTopLeft,0,input_height-1) else 0
+                            if between(xInTopLeft,0,input_width-1) and between(yInTopLeft,0,input_height-1) else 0.0
-                            if between(xInTopLeft+1,0,input_width-1) and between(yInTopLeft,0,input_height-1) else 0
+                            if between(xInTopLeft+1,0,input_width-1) and between(yInTopLeft,0,input_height-1) else 0.0
-                            if between(xInTopLeft,0,input_width-1) and between(yInTopLeft+1,0,input_height-1) else 0
+                            if between(xInTopLeft,0,input_width-1) and between(yInTopLeft+1,0,input_height-1) else 0.0
-                            if between(xInTopLeft+1,0,input_width-1) and between(yInTopLeft+1,0,input_height-1) else 0
+                            if between(xInTopLeft+1,0,input_width-1) and between(yInTopLeft+1,0,input_height-1) else 0.0
-        grid_grad = np.zeros(grid.shape)
+        data_grad = np.zeros(data.shape, dtype=np.float32)
-        out = np.zeros(data.shape)
+                    
-
+                    xInTopLeft = int(floor(xcoord))
-            exe.arg_dict['grid'][:] = tmp + np.random.normal(0,100,size=grid_shape) 
+            exe.arg_dict['data'][:] = np.random.uniform(low=-0.1, high=0.1,size=data_shape).astype(np.float32)
-            assert_almost_equal(exe.outputs[0].asnumpy(), out, rtol=1e-3)
+            assert_almost_equal(exe.outputs[0].asnumpy(), out, rtol=1e-3,atol=1e-5)
-            out_grad = np.random.normal(size=data_shape[:2] + grid_shape[2:])
+            out_grad = np.random.uniform(low=-0.01, high=0.01,size=data_shape[:2] + grid_shape[2:]).astype(np.float32)
-            assert_almost_equal(exe.grad_dict['grid'].asnumpy(), grid_grad, rtol=1e-4)
+            assert_almost_equal(exe.grad_dict['data'].asnumpy(), data_grad, rtol=1e-3, atol=1e-5)
-            grid_initial_grid = np.random.normal(size=exe_addto.grad_dict['grid'].shape)
+            data_initial_grid = np.random.normal(size=exe_addto.grad_dict['data'].shape).astype(np.float32)
-            assert_almost_equal(exe_addto.grad_dict['grid'].asnumpy(), grid_grad + grid_initial_grid, rtol=1e-4)
+            assert_almost_equal(exe_addto.grad_dict['data'].asnumpy(), data_grad + data_initial_grid, rtol=1e-3,atol=1e-5)
-# pylint: disable=too-many-arguments
+# pylint: disable=too-many-arguments, too-many-locals
-                    assert_almost_equal(out, args['data'].asnumpy()[:, :, h//4:h-h//4, w//4:w-w//4], rtol=1e-2)
+                    assert_almost_equal(out, args['data'].asnumpy()[:, :, h//4:h-h//4, w//4:w-w//4], rtol=1e-2, atol=1e-4)
-                    assert_almost_equal(out_grad.asnumpy(), grad_grad[0].asnumpy()[:, :, h//4:h-h//4, w//4:w-w//4], rtol=1e-2)
+                    assert_almost_equal(out_grad.asnumpy(), grad_grad[0].asnumpy()[:, :, h//4:h-h//4, w//4:w-w//4], rtol=1e-2, atol=1e-4)
-        embed = mx.sym.Embedding(data=data, input_dim=len(vocab), output_dim=args.num_embed,name='embed')
+        embed = mx.sym.Embedding(data=data, input_dim=len(vocab),
-        outputs, states = mx.rnn.rnn_unroll(stack, seq_len, inputs=embed)
+        outputs, states = stack.unroll(seq_len, inputs=embed, merge_outputs=True)
-        pred = mx.sym.Reshape(pred, shape=(-1, args.num_hidden))
+        pred = mx.sym.Reshape(outputs, shape=(-1, args.num_hidden))
-# pylint: disable=too-many-branches
+# pylint: disable=too-many-branches, too-many-arguments
-    """docstring for Bilinear"""
+    """Initialize weight for upsampling layer"""
-                 buckets=None, data_name='data', label_name='softmax_label'):
+    def __init__(self, sentences, batch_size, buckets=None, invalid_label=-1,
-        for i in xrange(len(sentences)):
+        for i in range(len(sentences)):
-
+        self.nddata = []
-
+        if self.major_axis == 1:
-                         [ndarray.array(label, dtype=self.dtype)],
+        return DataBatch([data], [label],
-from .. import symbol
+import warnings
-    """Unroll an RNN cell across time steps.
+    """Deprecated. Please use cell.unroll instead"""
-        is a single Symbol.
+    cells : subclass of BaseRNNCell
-    return outputs, states
+    period = int(max(1, period))
-# pylint: disable=no-member, invalid-name, protected-access
+# pylint: disable=no-member, invalid-name, protected-access, no-self-use
-from .. import symbol
+import warnings
-
+    def unpack_weights(self, args):
-        return (0, self._num_hidden)
+    def unpack_weights(self, args):
-        forget_gate = symbol.Activation(slice_gates[2], act_type="sigmoid",
+        forget_gate = symbol.Activation(slice_gates[1], act_type="sigmoid",
-
+    def unpack_weights(self, args):
-
+    def unpack_weights(self, args):
-
+def check_rnn_consistency(cell1, cell2):
-        assert_almost_equal(exe.grad_dict['affine'].asnumpy(), grad_est + grid_grad_npy, rtol=1e-3)
+        assert_almost_equal(exe.grad_dict['affine'].asnumpy(), grad_est + grid_grad_npy, rtol=1e-2)
-    outputs, _ = mx.rnn.rnn_unroll(cell, 3, input_prefix='rnn_')
+    outputs, _ = cell.unroll(3, input_prefix='rnn_')
-    outputs, _ = mx.rnn.rnn_unroll(cell, 3, input_prefix='rnn_')
+    outputs, _ = cell.unroll(3, input_prefix='rnn_')
-    outputs, _ = mx.rnn.rnn_unroll(cell, 3, input_prefix='rnn_')
+    outputs, _ = cell.unroll(3, input_prefix='rnn_')
-        dtype : numpy.dtype or string
+        dtype : str or numpy.dtype
-def empty(shape, ctx=None, dtype=mx_real_t):
+def empty(shape, ctx=None, dtype=None):
-def zeros(shape, ctx=None, dtype=mx_real_t):
+def zeros(shape, ctx=None, dtype=None):
-def ones(shape, ctx=None, dtype=mx_real_t):
+def ones(shape, ctx=None, dtype=None):
-def full(shape, val, ctx=None, dtype=mx_real_t):
+def full(shape, val, ctx=None, dtype=None):
-def array(source_array, ctx=None, dtype=mx_real_t):
+def array(source_array, ctx=None, dtype=None):
-
+    dtype : str or numpy.dtype, optional
-
+    if dtype is None:
-def arange(start, stop=None, step=1.0, repeat=1, ctx=None, dtype=mx_real_t):
+def arange(start, stop=None, step=1.0, repeat=1, ctx=None, dtype=None):
-    dtype : type, optional
+    dtype : str or numpy.dtype, optional
-def zeros(shape, dtype=_numpy.float32, **kwargs):
+def zeros(shape, dtype=None, **kwargs):
-        The value type of the NDArray, default to np.float32
+    dtype : str or numpy.dtype, optional
-def ones(shape, dtype=_numpy.float32, **kwargs):
+def ones(shape, dtype=None, **kwargs):
-        The value type of the NDArray, default to np.float32
+    dtype : str or numpy.dtype, optional
-def arange(start, stop=None, step=1.0, repeat=1, name=None, dtype=_numpy.float32):
+def arange(start, stop=None, step=1.0, repeat=1, name=None, dtype=None):
-        The value type of the NDArray, default to np.float32
+    dtype : str or numpy.dtype, optional
-        return io.DataBatch([batch_data], [batch_label], batch_size-1-i)
+        return io.DataBatch([batch_data], [batch_label], batch_size-i)
-        return self._curr_module.label_shapes
+        return self._curr_module.output_shapes
-def default_numerical_threshold():
+def get_atol(atol=None):
-    return 1e-6
+    return 1e-5 if rtol is None else rtol
-    """print location of maximum violation"""
+
-    return idx
+    return idx, np.max(violation)
-    Calculated by :math:`\\frac{|a-b|_1}{|a|_1 + |b|_1}`
+def almost_equal(a, b, rtol=None, atol=None):
-    return ret
+    rtol = get_rtol(rtol)
-    return not np.isnan(rel) and rel <= threshold
+    index, rel = find_max_violation(a, b, rtol, atol)
-    """Test that two numpy arrays are almost equal. Raise exception message if not.
+def almost_equal_ignore_nan(a, b, rtol=None, atol=None):
-        The checking threshold. Default threshold will be used if set to None
+    rtol : None or float
-    return rel
+    a = np.copy(a)
-def almost_equal_ignore_nan(a, b, rtol=None, atol=None):
+    return almost_equal(a, b, rtol, atol)
-    return rel_approx_equal or abs_approx_equal
+    assert_almost_equal(a, b, rtol, atol, names)
-    f_x = executor.outputs[0].asnumpy()[0]
+    for k, v in location.items():
-            v.ravel()[i] += eps
+            v.ravel()[i] += eps/2.0
-            approx_grads[k].ravel()[i] = (f_eps - f_x) / eps
+            f_neps = executor.outputs[0].asnumpy()
-                           grad_nodes=None, use_forward_train=True, ctx=None):
+def check_numeric_gradient(sym, location, aux_states=None, numeric_eps=1e-3, rtol=1e-2,
-    out = mx.sym.sum(sym * proj)
+    out = sym * proj
-            arr_l = [fd_grad, sym_grad]
+            assert_almost_equal(fd_grad, sym_grad, rtol, atol,
-            arr_l = [fd_grad, sym_grad - orig_grad]
+            assert_almost_equal(fd_grad, sym_grad - orig_grad, rtol, atol,
-            arr_l = [orig_grad, sym_grad]
+            assert_almost_equal(orig_grad, sym_grad, rtol, atol,
-def check_symbolic_forward(sym, location, expected, check_eps=1E-4, aux_states=None, ctx=None):
+            raise ValueError("Invalid grad_req %s for argument %s"%(grad_req[name], name))
-def check_symbolic_backward(sym, location, out_grads, expected, check_eps=1e-5,
+        assert_almost_equal(expect, output, rtol, atol,
-            arr_l = [expected[name], grads[name]]
+            assert_almost_equal(expected[name], grads[name], rtol, atol,
-            arr_l = [expected[name], grads[name] - args_grad_npy[name]]
+            assert_almost_equal(expected[name], grads[name] - args_grad_npy[name],
-            arr_l = [args_grad_npy[name], grads[name]]
+            assert_almost_equal(args_grad_npy[name], grads[name],
-            raise Exception(msg)
+            raise ValueError("Invalid grad_req %s for argument %s"%(grad_req[name], name))
-                npt.assert_allclose(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
+                assert_almost_equal(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
-                print_max_err_loc(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
+                else:
-                    npt.assert_allclose(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
+                    assert_almost_equal(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
-                    print(e)
+                    traceback.print_exc()
-            assert reldiff(out1, out2) < 2e-3
+            assert_almost_equal(out1, out2, rtol=2e-3)
-            assert reldiff(out1, out2) < 1e-6
+            assert_almost_equal(out1, out2)
-    assert reldiff(-npy, (-arr).asnumpy()) < 1e-6
+    assert_almost_equal(npy, arr.asnumpy())
-    assert reldiff(npy, arr.asnumpy()) < 1e-6
+    assert_almost_equal(npy, arr.asnumpy())
-    assert reldiff(c, C.asnumpy()) < 1e-5
+    assert_almost_equal(c, C.asnumpy())
-    assert reldiff(c, C.asnumpy()) < 1e-5
+    assert_almost_equal(c, C.asnumpy())
-    assert reldiff(c, C.asnumpy()) < 1e-5
+    assert_almost_equal(c, C.asnumpy())
-    assert reldiff(c, C.asnumpy()) < 1e-5
+    assert_almost_equal(c, C.asnumpy())
-        assert_almost_equal(pred, gt, default_numerical_threshold())
+        assert_almost_equal(pred, gt)
-            assert reldiff(result.asnumpy(), data_real[idx_real]) < 1e-6
+            assert_almost_equal(result.asnumpy(), data_real[idx_real])
-        assert same(a.asnumpy(), out_grad.asnumpy())
+        assert_almost_equal(a.asnumpy(), out_grad.asnumpy())
-    assert same(out1.asnumpy(), ret)
+    assert_almost_equal(out1.asnumpy(), ret)
-            assert same(grad.asnumpy(), np_grad + 1)
+            assert_almost_equal(grad.asnumpy(), np_grad + 1)
-                assert reldiff(outputs[i].asnumpy(), gt.reshape(outputs[i].shape)) < 1e-5
+                assert_almost_equal(outputs[i].asnumpy(), gt.reshape(outputs[i].shape))
-                assert reldiff(outputs[i].asnumpy(), gt) < 1e-5
+                assert_almost_equal(outputs[i].asnumpy(), gt)
-                                          axis=axis)) < 1e-5
+            assert_almost_equal(exe.grad_arrays[0].asnumpy(),
-                           np.concatenate(out_grads_npy, axis=axis)) < 1e-5
+            assert_almost_equal(exe.grad_arrays[0].asnumpy(),
-    assert reldiff(npout, out1) < 1e-6
+    assert_almost_equal(npout, out1)
-    assert reldiff(npout, arr_grad.asnumpy()) < 1e-6
+    assert_almost_equal(npout, arr_grad.asnumpy())
-    assert(reldiff(grad0[int(shape[0]/2):], grad1[int(shape[0]/2):]) < 1e-5)
+    assert abs(np.sum(grad1[:int(shape[0]/2)])) < 1e-5
-    assert_allclose(out, np_softmax(x.asnumpy()), rtol=1e-4)
+    assert_almost_equal(out, np_softmax(x.asnumpy()), rtol=1e-4)
-    assert_allclose(grad.asnumpy(), np_softmax(x.asnumpy()) - l.asnumpy(), rtol=1e-4)
+    assert_almost_equal(grad.asnumpy(), np_softmax(x.asnumpy()) - l.asnumpy(), rtol=1e-4)
-    assert reldiff(x.asnumpy(), exec1.outputs[0].asnumpy()) < 1e-5
+    assert_almost_equal(x.asnumpy(), exec1.outputs[0].asnumpy())
-    assert reldiff(dy.asnumpy(), dx.asnumpy()) < 1e-5
+    assert_almost_equal(dy.asnumpy(), dx.asnumpy())
-    assert reldiff(out, swap_) < 1e-6
+    assert_almost_equal(out, swap_)
-    check_numeric_gradient(y, [x])
+    check_numeric_gradient(y, [x], numeric_eps=1E-3)
-        assert_allclose(mx_out, forward_gt(x_npy, y_npy))
+        assert_almost_equal(mx_out, forward_gt(x_npy, y_npy))
-            assert_allclose(mx_rscalar_out, forward_gt(x_npy, 1))
+            assert_almost_equal(mx_lscalar_out, forward_gt(1, y_npy))
-    assert reldiff(exe_test.outputs[0].asnumpy(), np.dot(np_onehot, np_weight)) < 1e-6
+    assert_almost_equal(exe_test.outputs[0].asnumpy(), np.dot(np_onehot, np_weight))
-    assert reldiff(grad_map["embed_weight"].asnumpy(), np.dot(np_onehot.T, np_grad)) < 1e-6
+    assert_almost_equal(grad_map["embed_weight"].asnumpy(), np.dot(np_onehot.T, np_grad))
-    assert reldiff(exe_square.outputs[0].asnumpy(), data_tmp * data_tmp) < 1e-6
+    assert_almost_equal(exe_square.outputs[0].asnumpy(), data_tmp * data_tmp)
-    assert reldiff(arr_grad.asnumpy(), 2.0 * data_tmp) < 1e-6
+    assert_almost_equal(arr_grad.asnumpy(), 2.0 * data_tmp)
-    assert reldiff(out, npout) < 1e-6
+    assert_almost_equal(out, npout)
-    assert reldiff(arr_grad.asnumpy(), npout_grad) < 1e-6
+    assert_almost_equal(arr_grad.asnumpy(), npout_grad)
-    assert reldiff(out, npout) < 1e-6
+    assert_almost_equal(out, npout)
-    assert reldiff(out, npout) < 1e-6
+    assert_almost_equal(out, npout)
-    assert reldiff(arr_grad.asnumpy(), npout_grad) < 1e-6
+    assert_almost_equal(arr_grad.asnumpy(), npout_grad)
-    assert reldiff(out, npout) < 1e-6
+    assert_almost_equal(out, npout)
-    assert reldiff(arr_grad2.asnumpy(), npout_grad2) < 1e-6
+    assert_almost_equal(arr_grad1.asnumpy(), npout_grad1)
-    assert reldiff(out, npout) < 1e-6
+    assert_almost_equal(out, npout)
-    assert reldiff(arr_grad1.asnumpy(), npout_grad1) < 1e-6
+    assert_almost_equal(arr_grad1.asnumpy(), npout_grad1)
-    assert reldiff(out, npout) < 1e-6
+    assert_almost_equal(out, npout)
-    assert reldiff(arr_grad.asnumpy(), npout_grad) < 1e-6
+    assert_almost_equal(arr_grad.asnumpy(), npout_grad)
-    assert reldiff(out, args_grad[0].asnumpy()) < 1e-6
+    assert_almost_equal(out, args_grad[0].asnumpy(), rtol=1E-3, atol=1e-4)
-    assert reldiff(out + args_grad_addto_npy[0], args_grad_addto[0].asnumpy()) < 1e-6
+    assert_almost_equal(out + args_grad_addto_npy[0], args_grad_addto[0].asnumpy(), rtol=1e-4, atol=1e-4)
-    assert reldiff(conv_args_grad[1].asnumpy(), deconv_args_grad[1].asnumpy()) < 1e-6
+    assert_almost_equal(conv_args_grad[1].asnumpy(), deconv_args_grad[1].asnumpy(), rtol=1e-3)
-                   deconv_addto_args_grad[1].asnumpy()) < 1e-6
+    assert_almost_equal(conv_args_grad[1].asnumpy() + deconv_addto_args_grad_npy[1],
-        data_tmp = np.random.normal(size=shape)
+        data_tmp = np.random.normal(-0.1, 0.1, size=shape)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, check_eps=0.16)
+        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, check_eps=0.16)
+        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, rtol=0.16)
-            assert err_forward < 1E-4
+            assert_almost_equal(net.outputs[0].asnumpy(), groundtruth, rtol=1e-4)
-            assert err_backward < 1E-4
+            assert_almost_equal(grad_nd.asnumpy(), grad_groundtruth, rtol=1e-4)
-                    reldiff(out, args['data'].asnumpy()[:, :, h//4:h-h//4, w//4:w-w//4]) < 1e-6
+                    assert_almost_equal(out, args['data'].asnumpy()[:, :, h//4:h-h//4, w//4:w-w//4], rtol=1e-2)
-                    reldiff(out_grad.asnumpy(), grad_grad[0].asnumpy()[:, :, h//4:h-h//4, w//4:w-w//4]) < 1e-6
+                    assert_almost_equal(out_grad.asnumpy(), grad_grad[0].asnumpy()[:, :, h//4:h-h//4, w//4:w-w//4], rtol=1e-2)
-                assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-3
+                assert_almost_equal(outputs[0].asnumpy(), c_npy, rtol=1e-3)
-                assert reldiff(exe.grad_dict['b'].asnumpy(), bgrad_npy) < 1E-3
+                assert_almost_equal(exe.grad_dict['a'].asnumpy(), agrad_npy, rtol=1e-3)
-        check_numeric_gradient(dot_sym_xT_yT(), [m1_npy.T, m2_npy.T], check_eps=2e-2)
+        m1_npy = np.random.uniform(-1, 1, ashape)
-                    assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-3
+                    assert_almost_equal(outputs[0].asnumpy(), c_npy, rtol=1e-3, atol=1e-4)
-                    assert reldiff(exe.grad_dict['b'].asnumpy(), bgrad_npy) < 1E-3
+                    assert_almost_equal(exe.grad_dict['a'].asnumpy(), agrad_npy, rtol=1e-3, atol=1e-4)
-                                   bgrad_npy + b_init_grad_npy) < 1E-3
+                    assert_almost_equal(exe_add.grad_dict['a'].asnumpy(),
-    assert np.abs(exe1.outputs[0].asnumpy()-forward_result).mean() < 1e-4
+    assert_almost_equal(exe1.outputs[0].asnumpy(), forward_result, rtol=1e-4, atol=1e-4)
-    assert_almost_equal(exe1.grad_dict['img2'].asnumpy(), grad2, threshold=1E-3)
+    assert_almost_equal(exe1.grad_dict['img1'].asnumpy(), grad1, rtol=1e-3, atol=1e-4)
-    assert_allclose(x_np, exec1.outputs[0].asnumpy())
+    assert_almost_equal(x_np, exec1.outputs[0].asnumpy())
-    assert_allclose(grad_np, grad.asnumpy())
+    assert_almost_equal(grad_np, grad.asnumpy())
-    assert_allclose(x_np, exec1.outputs[0].asnumpy())
+    assert_almost_equal(x_np, exec1.outputs[0].asnumpy())
-    assert_allclose(grad_np, grad.asnumpy())
+    assert_almost_equal(grad_np, grad.asnumpy())
-    test = mx.symbol.ROIPooling(data=data, rois=rois, pooled_size=(6, 6), spatial_scale=1)
+    test = mx.symbol.ROIPooling(data=data, rois=rois, pooled_size=(4, 4), spatial_scale=1)
-    x2 = np.array([[0, 1, 1, 6, 6], [2, 6, 2, 7, 11], [1, 3, 1, 5, 10], [0, 3, 3, 3, 3]])
+    x2 = np.array([[0, 1.1, 1.1, 6.2, 6.2], [2, 6.1, 2.1, 8.2, 11.2], [1, 3.1, 1.1, 5.2, 10.2], [0, 3, 3, 3, 3]])
-                           numeric_eps=1e-3, check_eps=1e-2)
+    # TODO: fails randomly. investigate and remove retry.
-    assert_allclose(out, np_out, rtol=1e-5)
+    assert_almost_equal(out, np_out)
-    check_numeric_gradient(Y, [x.asnumpy()], numeric_eps=1e-3, check_eps=1e-2)
+    check_numeric_gradient(Y, [x.asnumpy()], numeric_eps=1e-2, rtol=1e-2)
-    eps = 0.0234
+    eps = 0.001
-    beta = mx.random.uniform(-1, 1, shape[1], ctx=mx.cpu()).copyto(xpu)
+    x = mx.random.normal(0, 1, shape, ctx=mx.cpu()).copyto(xpu)
-    check_numeric_gradient(Y, {'X':x.asnumpy(), 'G':gamma.asnumpy(), 'B':beta.asnumpy()}, numeric_eps=1e-2, check_eps=0.16)
+    assert_almost_equal(out, np_out, rtol=1e-4)
-    assert_allclose(exe.outputs[0].asnumpy(), np_out, rtol=1e-5)
+    assert_almost_equal(exe.outputs[0].asnumpy(), np_out, rtol=1e-5)
-    check_numeric_gradient(out, [in_data], numeric_eps=1e-3, check_eps=1e-2)
+    check_numeric_gradient(out, [in_data], numeric_eps=1e-3, rtol=1e-2, atol=1e-3)
-    assert_allclose(out, np_out, rtol=1e-5)
+    assert_almost_equal(out, np_out, rtol=1e-5)
-        numeric_eps=1e-3, check_eps=1)
+        numeric_eps=1e-3, rtol=1e-2)
-    assert reldiff(out, npout) < 1e-6, "%s mathematical forward failed\n%s\n\n%s" % (name, out, npout)
+    assert_almost_equal(out, npout)
-        name, arr_grad2, npout_grad)
+    assert_almost_equal(arr_grad1, npout_grad1)
-    assert reldiff(out, npout) < 1e-6, "%s mathematical forward failed\n%s\n\n%s" % (name, out, npout)
+    assert_almost_equal(out, npout)
-        name, arr_grad, npout_grad)
+    assert_almost_equal(arr_grad, npout_grad)
-    assert reldiff(out, npout) < 1e-6, "%s mathematical forward failed\n%s\n\n%s" % (name, out, npout)
+    assert_almost_equal(out, npout)
-        assert_allclose(exe.outputs[0].asnumpy(), np_func(shape=shape, dtype=dtype))
+        assert_almost_equal(exe.outputs[0].asnumpy(), np_func(shape=shape, dtype=dtype))
-            assert_almost_equal(pred, gt, default_numerical_threshold())
+            assert_almost_equal(pred, gt)
-            assert axis is None or axis ==1
+            assert axis is None or axis == 1
-    a_npy = np.random.normal(size=(5, 5, 5, 5))
+
-    b = mx.sym.topk(a, axis=3, is_ascend=False, ret_typ="indices", k=3)
+
-    a = mx.sym.Variable('a')
+
-    a = mx.sym.Variable('a')
+
-    a = mx.sym.Variable('a')
+
-        assert reldiff(exe.outputs[0].asnumpy(), data_real[idx_real]) < 1e-6
+        assert_almost_equal(exe.outputs[0].asnumpy(), data_real[idx_real])
-        assert reldiff(exe.grad_dict['a'].asnumpy(), grad_in) < 1e-6
+        assert_almost_equal(exe.grad_dict['a'].asnumpy(), grad_in)
-        reldiff(output[0,1], xv.T) < 1E-6
+        assert_almost_equal(output[0,0], yv.T)
-        assert reldiff(exe.grad_dict['affine'].asnumpy()[0], grad_est) < 1E-6
+        assert_almost_equal(exe.grad_dict['affine'].asnumpy(), grad_est, rtol=1e-3)
-        assert reldiff(exe.grad_dict['affine'].asnumpy()[0], grad_est + grid_grad_npy) < 1E-6
+        assert_almost_equal(exe.grad_dict['affine'].asnumpy(), grad_est + grid_grad_npy, rtol=1e-3)
-        reldiff(output[0,1], xv.T) < 1E-6
+        xv, yv = np.meshgrid(np.arange(target_shape[0])+1, np.arange(target_shape[1])+1)
-        assert reldiff(exe.grad_dict['flow'].asnumpy(), grad_est) < 1E-6
+        assert_almost_equal(exe.grad_dict['flow'].asnumpy(), grad_est, rtol=1e-3)
-        assert reldiff(exe_add.grad_dict['flow'].asnumpy(), grad_est + flow_grad_npy) < 1E-6
+        assert_almost_equal(exe_add.grad_dict['flow'].asnumpy(), grad_est + flow_grad_npy, rtol=1e-3, atol=1e-5)
-            assert reldiff(exe.outputs[0].asnumpy(),out) < 1E-5
+            assert_almost_equal(exe.outputs[0].asnumpy(), out, rtol=1e-3)
-            assert reldiff(exe.grad_dict['grid'].asnumpy(),grid_grad) < 1E-5
+            assert_almost_equal(exe.grad_dict['data'].asnumpy(), data_grad, rtol=1e-3)
-            assert reldiff(exe_addto.grad_dict['grid'].asnumpy(), grid_grad + grid_initial_grid) < 1E-5
+            assert_almost_equal(exe_addto.grad_dict['data'].asnumpy(), data_grad + data_initial_grid, rtol=1e-3)
-            assert_almost_equal(exe.grad_arrays[0].asnumpy(), X.astype(dsttype).astype(srctype), threshold=5e-4)
+            assert_almost_equal(exe.outputs[0].asnumpy(), X.astype(srctype).astype(dsttype), rtol=1e-3)
-        assert_almost_equal(expected_grad, arr_grad.asnumpy(), threshold=5e-4)
+        assert_almost_equal(expected_grad, arr_grad.asnumpy(), rtol=1e-3)
-        check_numeric_gradient(test, [data_tmp], numeric_eps=1e-3, check_eps=0.01)
+        check_numeric_gradient(test, [data_tmp], numeric_eps=1e-3, rtol=1e-2)
-        assert_almost_equal(expected_grad, arr_grad.asnumpy(), threshold=5e-4)
+        assert_almost_equal(expected_grad, arr_grad.asnumpy(), rtol=1e-3)
-        check_numeric_gradient(test, [data_tmp], numeric_eps=1e-3, check_eps=0.01)
+        check_numeric_gradient(test, [data_tmp], numeric_eps=1e-2, rtol=1e-2)
-    assert(reldiff(w1.asnumpy(), w2.asnumpy()) < 1e-5)
+        assert_almost_equal(s1.asnumpy(), s2.asnumpy(), rtol=1e-4, atol=1e-5)
-            a = np.random.random_integers(0, 100, shape)
+            a = np.random.randint(0, 100, shape)
-                {'ctx': mx.cpu(0), 'up_arg0': (2, 2, 2, 10), 'type_dict': {'up_arg0': np.float32}}]
+    ctx_list = [{'ctx': mx.gpu(0), 'up_data': (2, 2, 2, 10), 'type_dict': {'up_data': np.float64}},
-    sym = mx.sym.UpSampling(scale=2, num_filter=2, name='up', sample_type = 'nearest', num_args=1)
+    sym = mx.sym.UpSampling(scale=2, num_filter=2, name='up', sample_type='nearest', num_args=1)
-                            mx.nd.array((args.mean_r, args.mean_g, args.mean_b)).reshape((3,1,1)),
+                            (args.mean_r, args.mean_g, args.mean_b),
-        self.mean_pixels = mx.nd.array(mean_pixels).reshape((3,1,1))
+        self.mean_pixels = mean_pixels
-    mod = mx.mod.Module(symbol=fc2, label_names=None)
+    mod = mx.mod.Module(symbol=fc2, label_names=None, context=default_context())
-    g1 = mx.random.uniform(shape=shape)
+    w1 = mx.random.uniform(shape=shape, ctx=default_context())
-    g2 = g1.copyto(mx.cpu())
+    w2 = w1.copyto(default_context())
-    mxnet_path = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), os.pardir))
+    mxnet_path = os.path.realpath(os.path.join(os.path.dirname(os.path.realpath(__file__)), os.pardir))
-        if os.path.abspath(f).startswith(mxnet_path) and fn not in visited:
+        if f.startswith(mxnet_path) and fn not in visited:
-            assert reldiff(exe.grad_dict['data'].asnumpy(),data_grad) < 1E-4
+            assert reldiff(exe.grad_dict['data'].asnumpy(),data_grad) < 2E-4
-            assert reldiff(exe_addto.grad_dict['data'].asnumpy(), data_grad + data_initial_grid) < 1E-4
+            assert reldiff(exe_addto.grad_dict['data'].asnumpy(), data_grad + data_initial_grid) < 2E-4
-        label = op
+        label = name
-    check_numeric_gradient(dot_sym_xT_yT(), [m1_npy.T, m2_npy.T])
+    for ashape, bshape in [((3, 4), (4, 5)), ((2,3,4), (4, 5, 6))]:
-    test_dot()
+def test_repeat():
-        for key in kwargs.iterkeys():
+        for key in kwargs.keys():
-                assert not known_names.has_key(name), "Duplicated parameter names: " + \
+                assert not name in known_names, "Duplicated parameter names: " + \
-            _check_name(aux_names, aux_params.iterkeys(), self._modules, i_layer)
+            _check_name(arg_names, arg_params.keys(), self._modules, i_layer)
-            if meta.has_key(SequentialModule.META_TAKE_LABELS) and \
+            if SequentialModule.META_TAKE_LABELS in meta and \
-        for i_layer, module in reversed(zip(range(len(self._modules)), self._modules)):
+        for i_layer, module in reversed(list(zip(range(len(self._modules)), self._modules))):
-            if meta.has_key(SequentialModule.META_TAKE_LABELS) and \
+            if SequentialModule.META_TAKE_LABELS in meta and \
-
+Reference:
-import find_mxnet
+Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012.
-    softmax = mx.symbol.SoftmaxOutput(data=fc1, name='softmax')
+def get_symbol(num_classes, **kwargs):
-Reference:
+"""References:
-Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012.
+
-    softmax = mx.symbol.SoftmaxOutput(data=fc3, name='softmax')
+def ConvFactory(data, num_filter, kernel, stride=(1,1), pad=(0, 0), name=None, suffix=''):
-from __future__ import absolute_import
+from __future__ import absolute_import, print_function
-    """Base class for Initializer."""
+    """Base class for Initializer.
-    def __call__(self, name, arr):
+    subclasses should call base class with all keyword arguments. For example::
-        raise ValueError('Unknown initialization pattern for %s' % name)
+        raise ValueError(
-
+@register
-
+@register
-
+@register
-__version__ = "0.9.3"
+__version__ = "0.9.4"
-            'Please use mxnet.mod.Module instead.\033[0m')
+        warnings.warn(
-from ..initializer import Uniform
+from ..initializer import Uniform, InitDesc
-            _impl(name, arr, arg_params)
+            desc = InitDesc(name, attrs.get(name, None))
-            _impl(name, arr, aux_params)
+            desc = InitDesc(name, attrs.get(name, None))
-def Variable(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None):
+def Variable(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None, init=None):
-                output.wait_to_read()
+import mxnet as mx
-          'mxnet', 'mxnet.module', 'mxnet._ctypes',
+          'mxnet', 'mxnet.module', 'mxnet._ctypes', 'mxnet.rnn',
-sys.path.insert(0, "../../python")
+import argparse
-    num_lstm_layer = 2
+    import logging
-    momentum = 0.0
+    #buckets = []
-    dummy_data = False
+    start_label = 1
-    contexts = [mx.context.gpu(i) for i in range(1)]
+    train_sent, vocab = tokenize_text("./data/ptb.train.txt", start_label=start_label,
-    vocab = default_build_vocab("./data/ptb.train.txt")
+    data_train  = mx.rnn.BucketSentenceIter(train_sent, args.batch_size, buckets=buckets,
-        symbol = sym_gen
+        data = mx.sym.Variable('data')
-                                 initializer=mx.init.Xavier(factor_type="in", magnitude=2.34))
+        stack = mx.rnn.SequentialRNNCell()
-    logging.basicConfig(level=logging.DEBUG, format=head)
+        outputs = [mx.sym.expand_dims(x, axis=1) for x in outputs]
-              batch_end_callback=mx.callback.Speedometer(batch_size, 50),)
+        label = mx.sym.Reshape(label, shape=(-1,))
-            Rescaling factor on gradient.
+            Rescaling factor on gradient. Normally should be 1/batch_size.
-        rescaling factor of gradient.
+        rescaling factor of gradient. Normally should be 1/batch_size.
-        rescaling factor of gradient.
+        rescaling factor of gradient. Normally should be 1/batch_size.
-        rescaling factor of gradient.
+        rescaling factor of gradient. Normally should be 1/batch_size.
-        rescaling factor of gradient.
+        rescaling factor of gradient. Normally should be 1/batch_size.
-        rescaling factor of gradient.
+        rescaling factor of gradient. Normally should be 1/batch_size.
-        rescaling factor of gradient.
+        rescaling factor of gradient. Normally should be 1/batch_size.
-        rescaling factor of gradient.
+        rescaling factor of gradient. Normally should be 1/batch_size.
-from .ndarray import sgd_update, sgd_mom_update, adam_update
+from .ndarray import sgd_update, sgd_mom_update, adam_update, rmsprop_update
-        super(RMSProp, self).__init__(**kwargs)
+    def __init__(self, learning_rate=0.001, gamma1=0.95, gamma2=0.9,
-
+        rmsprop_update(weight, grad, n, g, delta, out=weight,
-    fc2 = mx.sym.FullyConnected(data=fc1, name='fc2', num_hidden=10, wd_mult=0.5)
+def test_adam():
-    args2 = {k: v.asnumpy() for k, v in args2.items()}
+# RMSProp
-    assert not mx.test_utils.almost_equal(args1['fc2_weight'], args2['fc2_weight'], 1e-1)
+    def create_state(self, index, weight):
-    g1 = mx.random.uniform(shape=shape)
+        """
-    g2 = g1.copyto(mx.cpu())
+    def update(self, index, weight, grad, state):
-        assert(same(s1.asnumpy(), s2.asnumpy()))
+        Parameters
-    opt2.update(0, w2, g2, state2)
+        weight : NDArray
-    assert(reldiff(w1.asnumpy(), w2.asnumpy()) < 1e-5)
+        grad : NDArray
-def test_adam():
+def test_rms():
-    opt2 = mx.optimizer.Adam
+    opt1 = PyRMSProp
-              {'rescale_grad': 0.1}]
+              {'clip_gradient': 0.4, 'rescale_grad': 0.14},
-
+    test_rms()
-        os.path.mkdir("checkpoint")
+        os.mkdir("checkpoint")
-            raise ValueError("Input shape is incompete")
+            raise ValueError("Input shape is incomplete")
-            raise ValueError("Input shape is incompete")
+            raise ValueError("Input shape is incomplete")
-            type_dict = {k: mx_real_t for k in self.list_arguments()}
+            attrs = self.attr_dict()
-                            int(bbox.find('ymax').text)]
+        obj_dict['bbox'] = [int(float(bbox.find('xmin').text)),
-    image_ids = [image_ids[x] for x in sorted_inds]
+    if bbox.shape[0] > 0:
-
+    test_bilinear_sampler_with_type()
-    
+        # check addto
-            
+            assert reldiff(exe.grad_dict['data'].asnumpy(),data_grad) < 1E-4
-            assert  reldiff(exe.grad_dict['grid'].asnumpy(),grid_grad) < 1E-5
+def test_index2d():
-shutil.rmtree(os.path.join(CURRENT_DIR, 'mxnet'))#, ignore_errors=True)
+
-    'omp.h'
+    'omp.h', 'execinfo.h', 'packet/sse-inl.h'
-import caffe_pb2
+import caffe_parse.caffe_pb2 as caffe_pb2
-    '''
+
-    f = open(filepath, 'rb')
+    """
-    netparam.ParseFromString(contents)
+    net_param = caffe_pb2.NetParameter()
-    layers = find_layers(netparam)
+    layers = find_layers(net_param)
-        return netparam.layer
+
-        raise Exception ("Couldn't find layers")
+        raise Exception("Couldn't find layers")
-    
+
-    
+
-    
+
-                assert(len(layer_blobs) == 1)
+                assert (len(layer_blobs) == 1)
-            assert(len(layer_blobs) == 2)
+            assert (len(layer_blobs) == 2)
-                    wmat_dim = [layer_blobs[0].num, layer_blobs[0].channels, layer_blobs[0].height, layer_blobs[0].width]
+                    wmat_dim = [layer_blobs[0].num, layer_blobs[0].channels, layer_blobs[0].height,
-            if channels == 3 or channels == 4: # RGB or RGBA
+            if channels == 3 or channels == 4:  # RGB or RGBA
-                    print 'Swapping BGR of caffe into RGB in mxnet'
+                    print('Swapping BGR of caffe into RGB in mxnet')
-            print 'converting layer {0}, wmat shape = {1}, bias shape = {2}'.format(layer_name, wmat.shape, bias.shape)
+            assert (wmat.flags['C_CONTIGUOUS'] is True)
-            
+
-                print weight_name + ' not found in arg_shape_dic.'
+                print(weight_name + ' not found in arg_shape_dic.')
-            learning_rate=0.05, momentum=0.9, wd=0.0001)
+    model = mx.mod.Module(symbol=prob, label_names=['prob_label', ])
-    model.save(args.save_model_name)
+from __future__ import print_function
-def readProtoSolverFile(filepath):
+
-    return readProtoFile(filepath, solver_config)
+    return read_proto_file(file_path, solver_config)
-    file = open(filepath, "r")
+def read_proto_file(file_path, parser_object):
-        raise self.ProcessException("ERROR (" + filepath + ")!")
+        raise Exception("ERROR (" + file_path + ")!")
-def convParamToString(param):
+
-        kernel_size, stride, stride, not param.bias_term)
+    param_string = "num_filter=%d, pad=(%d,%d), kernel=(%d,%d), stride=(%d,%d), no_bias=%s" % \
-    proto = readProtoSolverFile(proto_file)
+    proto = read_proto_solver_file(proto_file)
-    input_dim = [1, 3, 224, 224] # default
+        raise Exception('Invalid proto file.')
-    elif len(proto.input_shape) > 0: 
+    elif len(proto.input_shape) > 0:
-    elif (layer[0].type == "Input"):
+    elif layer[0].type == "Input":
-        raise Exception('Invalid proto file.')   
+        raise Exception('Invalid proto file.')
-    # We assume the first bottom blob of first layer is the output from data layer
+        # We assume the first bottom blob of first layer is the output from data layer
-    need_flatten = {input_name : False}
+    mapping = {input_name: 'data'}
-            param_string = convParamToString(layer[i].convolution_param)
+            param_string = conv_param_to_string(layer[i].convolution_param)
-            param_string = convParamToString(layer[i].convolution_param)
+            param_string = conv_param_to_string(layer[i].convolution_param)
-            if param.global_pooling == True:
+            if param.global_pooling:
-                    param.kernel_size, param.stride, param.stride)
+                param_string += "pad=(%d,%d), kernel=(%d,%d), stride=(%d,%d)" % \
-                param_string = param_string + ", pool_type='max'"
+                param_string += ", pool_type='max'"
-                param_string = param_string + ", pool_type='avg'"
+                param_string += ", pool_type='avg'"
-                (param.alpha, param.beta, param.k, param.local_size)
+            param_string = "alpha=%f, beta=%f, knorm=%f, nsize=%d" % \
-                        (flatten_name, flatten_name, mapping[bottom[0]])
+                    symbol_string += "%s=mx.symbol.Flatten(name='%s', data=%s)\n" % \
-                    (name, type_string, name, mapping[bottom[0]], param_string)
+                symbol_string += "%s = %s(name='%s', data=%s %s)\n" % \
-                    (name, type_string, name, ','.join([mapping[x] for x in bottom]), param_string)
+                symbol_string += "%s = %s(name='%s', *[%s] %s)\n" % \
-            + sym
+          + "data = mx.symbol.Variable(name='data')\n" \
-    exec("ret = " + output_name)
+    _locals = locals()
-def protoBlobFileToND(protofile):
+
-    file = open(protofile, "r")
+    file = open(proto_file, "r")
-        raise self.ProcessException("ERROR (" + protofile + ")!")
+        raise Exception("ERROR (" + proto_file + ")!")
-    main()
+    main()
-
+            x_grad_npy = np.random.normal(size=x.shape)
-    assert np.abs(exe1.grad_dict['img2'].asnumpy() - grad2).mean() < 1e-3
+    assert_almost_equal(exe1.grad_dict['img1'].asnumpy(), grad1, threshold=1E-3)
-                           grad_nodes={'data':'add', 'rois':'write'},
+                           grad_nodes={'data':'write', 'rois':'null'},
-    check_numeric_gradient(test, [data_tmp])
+    check_numeric_gradient(test, [data_tmp], check_eps=2E-2)
-def test_order(ctx=default_context()):
+def test_order():
-    b = mx.sym.topk(a, axis=None, is_ascend=True, ret_typ="value", k=10)
+    b = mx.sym.topk(a, axis=None, is_ascend=True, ret_typ="value", k=7)
-                           expected=[gt_topk(dat=a_npy, axis=None, ret_typ="value", k=10,
+                           expected=[gt_topk(dat=a_npy, axis=None, ret_typ="value", k=7,
-        mpre = np.concatenate([0.], prec, [0.])
+        mrec = np.concatenate(([0.], rec, [1.]))
-                    new_arg_dict[name] = nd.empty(new_shape, ctx=arr.context)
+                    new_arg_dict[name] = nd.empty(new_shape, ctx=arr.context, dtype=arr.dtype)
-                        new_grad_dict[name] = nd.empty(new_shape, ctx=darr.context)
+                        new_grad_dict[name] = nd.empty(new_shape, ctx=darr.context, dtype=arr.dtype)
-                    new_aux_dict[name] = nd.empty(new_shape, ctx=arr.context)
+                    new_aux_dict[name] = nd.empty(new_shape, ctx=arr.context, dtype=arr.dtype)
-                 for_training, inputs_need_grad, shared_group=None, input_types=None,
+                 for_training, inputs_need_grad, shared_group=None,
-        self.execs = None
+        self.execs = []
-
+        self.data_shapes = None
-
+    def _collect_arrays(self):
-        if label_shapes is not None:
+                            for name, _ in self.data_shapes]
-                                 for name, _ in label_shapes]
+                                 for name, _ in self.label_shapes]
-        data_names = [x[0] for x in data_shapes]
+        data_names = [x[0] for x in self.data_shapes]
-            input_types = self.input_types
+        input_types = {x.name: x.dtype for x in data_shapes}
-            shape = list(shape)
+        for desc, axis in zip(shapes, major_axis):
-            sliced_shapes.append((k, tuple(shape)))
+            sliced_shapes.append(DataDesc(desc.name, tuple(shape), desc.dtype, desc.layout))
-                                                     grad_req=grad_req, input_types=input_types)
+                                                     grad_req=grad_req)
-def zeros(shape, dtype=_numpy.float32):
+def zeros(shape, dtype=_numpy.float32, **kwargs):
-    return _internal._zeros(shape=shape, dtype=dtype)
+    return _internal._zeros(shape=shape, dtype=dtype, **kwargs)
-def ones(shape, dtype=_numpy.float32):
+def ones(shape, dtype=_numpy.float32, **kwargs):
-    return _internal._ones(shape=shape, dtype=dtype)
+    return _internal._ones(shape=shape, dtype=dtype, **kwargs)
-    test = 2 / (4-((1+data+1)*2/5)-0.2)
+    test = 2 / (4-((1+data+1)*2/5)-0.8-(data!=0))
-    npout_1 = (4-((1+data_tmp+1)*2/5)-0.2)
+    npout_1 = (4-((1+data_tmp+1)*2/5)-0.8-(data_tmp!=0))
-release = '0.9.2'
+version = '0.9.3'
-__version__ = "0.9.2"
+__version__ = "0.9.3"
-    logging.info('Stop luancher')
+    logging.info('Stop launcher')
-    logging.info('Stop luancher')
+    logging.info('Stop launcher')
-    exit(0)
+minimum = int(sys.argv[6]) if len(sys.argv) > 5 else 0
-        blacklist += ['packet/sse-inl.h']
+def pprint(lst):
-def find_source(name, start):
+
-        if x == name or x.endswith('/' + name): candidates.append(x)
+        if x == name:
-def expand(x, pending):
+
-        source = find_source(h, x)
+        source = find_source(h, x, stage)
-            expand(source, pending + [x])
+            expand(source, pending + [x], stage)
-f = open(sys.argv[3], 'wb')
+f = open(sys.argv[5], 'wb')
-num_round = 39
+prefix = "resnet/resnet-18"
-                      open(param_file).read(),
+param_file = "%s-0000.params" % prefix
-synset = [l.strip() for l in open('Inception/synset.txt').readlines()]
+synset = [l.strip() for l in open('resnet/synset.txt').readlines()]
-    return normed_img
+    return sample
-batch = PreprocessImage('./download.png', True)
+batch = PreprocessImage('./download.jpg', True)
-
+def test_clip():
-parser.add_argument('--model-prefix', type=str,default= "./models/sample_net",
+parser.add_argument('--save-model-prefix', type=str,default= "./models/sample_net",
-                importlib.import_module('symbol.'+ args[0])
+                importlib.import_module('symbols.'+ args[0])
-    net = import_module('symbol.'+network)
+    net = import_module('symbols.'+network)
-    net = import_module('symbol.'+args.network)
+    net = import_module('symbols.'+args.network)
-    net = import_module('symbol.'+args.network)
+    net = import_module('symbols.'+args.network)
-    net = import_module('symbol.'+args.network)
+    net = import_module('symbols.'+args.network)
-from helper.processing.nms import nms
+import numpy as np
-    return detector
+from rcnn.symbol import get_vgg_test, get_vgg_rpn_test
-def demo_net(detector, image_name, vis=False):
+def demo_net(predictor, image_name, vis=False):
-    :param detector: Detector
+    generate data_batch -> im_detect -> post process
-    scores, boxes = detector.im_detect(im_array, im_info)
+    data_batch, data_names, im_scale = generate_batch(im)
-        cls_scores = scores[:, cls_ind]
+        cls_scores = scores[:, cls_ind, np.newaxis]
-        keep = nms(dets.astype(np.float32), NMS_THRESH)
+        dets = np.hstack((cls_boxes, cls_scores)).astype(np.float32)[keep, :]
-        vis_all_detection(im_array, boxes_this_image, CLASSES, 0)
+        vis_all_detection(data_dict['data'].asnumpy(), boxes_this_image, CLASSES, im_scale)
-        save_all_detection(im_array, boxes_this_image, CLASSES, 0)
+        result_file = image_name.replace('.', '_result.')
-                        default=0, type=int)
+    parser.add_argument('--image', help='custom image', type=str)
-if __name__ == '__main__':
+
-    demo_net(detector, args.image)
+    ctx = mx.gpu(args.gpu)
-config.MAX_SIZE = 1000
+# network related params
-config.GPU_ID = 0
+# dataset related params
-config.TRAIN.BATCH_SIZE = 128  # used in grad_scale
+# size of images for each device, 2 for rcnn, 1 for rpn and e2e
-config.TRAIN.BATCH_IMAGES = 2
+# rcnn rois batch size
-# R-CNN bounding box regression
+config.TRAIN.BG_THRESH_LO = 0.0
-config.TRAIN.BBOX_INSIDE_WEIGHTS = np.array([1.0, 1.0, 1.0, 1.0])
+config.TRAIN.BBOX_WEIGHTS = np.array([1.0, 1.0, 1.0, 1.0])
-config.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)
+# rpn bounding box regression params
-config.END2END = 0
+config.TRAIN.CXX_PROPOSAL = True
-config.TRAIN.RPN_MIN_SIZE = 16
+config.TRAIN.RPN_POST_NMS_TOP_N = 2000
-config.TRAIN.IMS_PER_BATCH = 1
+
-config.TEST.DEDUP_BOXES = 1. / 16.
+config.TEST.CXX_PROPOSAL = True
-config.TEST.RPN_MIN_SIZE = 16
+config.TEST.RPN_MIN_SIZE = config.RPN_FEAT_STRIDE
-from rcnn.config import config
+import mxnet as mx
-                        print "\t\t\t\t\t\t\tTrain-{}={},\t{}={},\t{}={}".format(name[3], value[3], name[4], value[4], name[5], value[5])
+                    s = "Epoch[%d] Batch [%d]\tSpeed: %.2f samples/sec\tTrain-" % (param.epoch, count, speed)
-                    fixed_param_names.append(name)
+        if fixed_param_prefix is not None:
-        max_shapes_dict = dict(self._max_data_shapes + self._max_label_shapes)
+        max_shapes_dict = dict()
-                max_label_shapes.append((name, shape))
+        if label_shapes is not None:
-        input_shapes = dict(data_batch.provide_data + data_batch.provide_label)
+import cPickle
-        return batch
+from imdb import IMDB
-    return keep
+    return keep
-import cPickle
+
-from helper.processing.bbox_process import unique_boxes, filter_small_boxes
+from pascal_voc_eval import voc_eval
-    def __init__(self, image_set, year, root_path, devkit_path):
+    def __init__(self, image_set, root_path, devkit_path):
-        :param year: 2007, 2010, 2012
+        :param image_set: 2007_trainval, 2007_test, etc
-        self.image_set = image_set
+        year, image_set = image_set.split('_')
-        self.num_classes = 21
+        self.num_classes = len(self.classes)
-        filename = os.path.join(self.data_path, 'Annotations', index + '.xml')
+        roi_rec = dict()
-        return self.selective_search_roidb(gt_roidb)
+        roi_rec.update({'boxes': boxes,
-    def selective_search_roidb(self, gt_roidb):
+    def selective_search_roidb(self, gt_roidb, append_gt=False):
-        :return: roidb of selective search (ground truth included)
+        :param append_gt: append ground truth
-        if self.image_set != 'test':
+        if append_gt:
-            roidb = self.load_selective_search_roidb(None)
+            roidb = self.load_selective_search_roidb(gt_roidb)
-        cache_dir = os.path.join(self.cache_path, self.name)
+        annocache = os.path.join(self.cache_path, self.name + '_annotations.pkl')
-            rec, prec, ap = voc_eval(filename, annopath, imageset_file, cls, cache_dir,
+            rec, prec, ap = voc_eval(filename, annopath, imageset_file, cls, annocache,
-def voc_eval(detpath, annopath, imageset_file, classname, cache_dir, ovthresh=0.5, use_07_metric=False):
+def voc_eval(detpath, annopath, imageset_file, classname, annocache, ovthresh=0.5, use_07_metric=False):
-    :param cache_dir: caching annotations
+    :param annocache: caching annotations
-    if not os.path.isfile(cache_file):
+    if not os.path.isfile(annocache):
-            cPickle.dump(recs, f)
+        print 'saving annotations cache to {:s}'.format(annocache)
-        with open(cache_file, 'r') as f:
+        with open(annocache, 'r') as f:
-        return scores, pred_boxes
+import numpy as np
-    return label
+"""
-
+from ..cython.bbox import bbox_overlaps_cython
-def bbox_transform(ex_rois, gt_rois):
+def bbox_overlaps_py(boxes, query_boxes):
-def bbox_pred(boxes, box_deltas, is_train=False):
+def nonlinear_pred(boxes, box_deltas):
-        dy = np.array(map(lambda x: np.sign(x)*10 if abs(x) > 10 else x, dy))
+
-    return boxes
+def iou_transform(ex_rois, gt_rois):
-def clip_pad(boxes, pad_shape):
+
-    :return: [n, c, h, w]
+    Transform the set of class-agnostic boxes into class-specific boxes
-    return boxes
+    if boxes.shape[0] == 0:
-    return labels, rois, bbox_targets, bbox_inside_weights
+from symbol_vgg import *
-import logging
+from rcnn.processing.bbox_transform import bbox_pred, clip_boxes
-    def __init__(self, feat_stride, scales, ratios, is_train=False, output_score=False):
+    def __init__(self, feat_stride, scales, ratios, output_score,
-        self._feat_stride = float(feat_stride)
+        self._feat_stride = feat_stride
-        self._ratios = np.fromstring(ratios[1:-1], dtype=float, sep=',').tolist()
+        self._ratios = np.fromstring(ratios[1:-1], dtype=float, sep=',')
-
+        nms = gpu_nms_wrapper(self._threshold, in_data[0].context.device_id)
-        min_size = config[self.cfg_key].RPN_MIN_SIZE
+
-            raise ValueError("there is nan in input bbox_deltas")
+
-            height, width = int(im_info[0] / self._feat_stride), int(im_info[1] / self._feat_stride)
+        # use real image size instead of padded feature map sizes
-            print "resudial = ", scores.shape[2] - height, scores.shape[3] - width
+            print "resudial: {}".format((scores.shape[2] - height, scores.shape[3] - width))
-        bbox_deltas = clip_pad(bbox_deltas, (height, width))
+        bbox_deltas = self._clip_pad(bbox_deltas, (height, width))
-
+        keep = self._filter_boxes(proposals, min_size * im_info[2])
-        keep = nms(np.hstack((proposals, scores)), nms_thresh)
+        det = np.hstack((proposals, scores)).astype(np.float32)
-                logging.log(logging.ERROR, "currently len(keep) is zero")
+
-    def __init__(self, feat_stride, scales, ratios, is_train=False, output_score=False):
+    def __init__(self, feat_stride='16', scales='(8, 16, 32)', ratios='(0.5, 1, 2)', output_score='False',
-        self._feat_stride = feat_stride
+        self._feat_stride = int(feat_stride)
-            self.cfg_key = 'TEST'
+        self._output_score = strtobool(output_score)
-        score_shape = (config[cfg_key].RPN_POST_NMS_TOP_N, 1)
+        output_shape = (self._rpn_post_nms_top_n, 5)
-        return ProposalOperator(self._feat_stride, self._scales, self._ratios, self._is_train, self._output_score)
+        return ProposalOperator(self._feat_stride, self._scales, self._ratios, self._output_score,
-from config import config
+import proposal
-        data=data, kernel=(3, 3), pad=(1, 1), num_filter=64, name="conv1_1")
+        data=data, kernel=(3, 3), pad=(1, 1), num_filter=64, workspace=2048, name="conv1_1")
-        data=relu1_1, kernel=(3, 3), pad=(1, 1), num_filter=64, name="conv1_2")
+        data=relu1_1, kernel=(3, 3), pad=(1, 1), num_filter=64, workspace=2048, name="conv1_2")
-        data=pool1, kernel=(3, 3), pad=(1, 1), num_filter=128, name="conv2_1")
+        data=pool1, kernel=(3, 3), pad=(1, 1), num_filter=128, workspace=2048, name="conv2_1")
-        data=relu2_1, kernel=(3, 3), pad=(1, 1), num_filter=128, name="conv2_2")
+        data=relu2_1, kernel=(3, 3), pad=(1, 1), num_filter=128, workspace=2048, name="conv2_2")
-        data=pool2, kernel=(3, 3), pad=(1, 1), num_filter=256, name="conv3_1")
+        data=pool2, kernel=(3, 3), pad=(1, 1), num_filter=256, workspace=2048, name="conv3_1")
-        data=relu3_1, kernel=(3, 3), pad=(1, 1), num_filter=256, name="conv3_2")
+        data=relu3_1, kernel=(3, 3), pad=(1, 1), num_filter=256, workspace=2048, name="conv3_2")
-        data=relu3_2, kernel=(3, 3), pad=(1, 1), num_filter=256, name="conv3_3")
+        data=relu3_2, kernel=(3, 3), pad=(1, 1), num_filter=256, workspace=2048, name="conv3_3")
-        data=pool3, kernel=(3, 3), pad=(1, 1), num_filter=512, name="conv4_1")
+        data=pool3, kernel=(3, 3), pad=(1, 1), num_filter=512, workspace=2048, name="conv4_1")
-        data=relu4_1, kernel=(3, 3), pad=(1, 1), num_filter=512, name="conv4_2")
+        data=relu4_1, kernel=(3, 3), pad=(1, 1), num_filter=512, workspace=2048, name="conv4_2")
-        data=relu4_2, kernel=(3, 3), pad=(1, 1), num_filter=512, name="conv4_3")
+        data=relu4_2, kernel=(3, 3), pad=(1, 1), num_filter=512, workspace=2048, name="conv4_3")
-        data=pool4, kernel=(3, 3), pad=(1, 1), num_filter=512, name="conv5_1")
+        data=pool4, kernel=(3, 3), pad=(1, 1), num_filter=512, workspace=2048, name="conv5_1")
-        data=relu5_1, kernel=(3, 3), pad=(1, 1), num_filter=512, name="conv5_2")
+        data=relu5_1, kernel=(3, 3), pad=(1, 1), num_filter=512, workspace=2048, name="conv5_2")
-        data=relu5_2, kernel=(3, 3), pad=(1, 1), num_filter=512, name="conv5_3")
+        data=relu5_2, kernel=(3, 3), pad=(1, 1), num_filter=512, workspace=2048, name="conv5_3")
-def get_vgg_rcnn(num_classes=21):
+def get_vgg_rcnn(num_classes=config.NUM_CLASSES):
-    bbox_outside_weight = mx.symbol.Variable(name='bbox_outside_weight')
+    bbox_weight = mx.symbol.Variable(name='bbox_weight')
-    bbox_outside_weight = mx.symbol.Reshape(data=bbox_outside_weight, shape=(-1, 4 * num_classes), name='bbox_outside_weight_reshape')
+    bbox_weight = mx.symbol.Reshape(data=bbox_weight, shape=(-1, 4 * num_classes), name='bbox_weight_reshape')
-        name='roi_pool5', data=relu5_3, rois=rois, pooled_size=(7, 7), spatial_scale=0.0625)
+        name='roi_pool5', data=relu5_3, rois=rois, pooled_size=(7, 7), spatial_scale=1.0 / config.RCNN_FEAT_STRIDE)
-    cls_prob = mx.symbol.SoftmaxOutput(name='cls_prob', data=cls_score, label=label)
+    cls_prob = mx.symbol.SoftmaxOutput(name='cls_prob', data=cls_score, label=label, normalization='batch')
-    bbox_loss = mx.sym.MakeLoss(name='bbox_loss', data=bbox_loss_)
+    bbox_loss_ = bbox_weight * mx.symbol.smooth_l1(name='bbox_loss_', scalar=1.0, data=(bbox_pred - bbox_target))
-def get_vgg_rcnn_test(num_classes=21):
+def get_vgg_rcnn_test(num_classes=config.NUM_CLASSES):
-        name='roi_pool5', data=relu5_3, rois=rois, pooled_size=(7, 7), spatial_scale=0.0625)
+        name='roi_pool5', data=relu5_3, rois=rois, pooled_size=(7, 7), spatial_scale=1.0 / config.RCNN_FEAT_STRIDE)
-def get_vgg_rpn(num_classes=21, num_anchors=9):
+def get_vgg_rpn(num_anchors=config.NUM_ANCHORS):
-    bbox_outside_weight = mx.symbol.Variable(name='bbox_outside_weight')
+    bbox_weight = mx.symbol.Variable(name='bbox_weight')
-        data=rpn_cls_score, shape=(0, 2, -1), name="rpn_cls_score_reshape")
+        data=rpn_cls_score, shape=(0, 2, -1, 0), name="rpn_cls_score_reshape")
-    bbox_loss = mx.sym.MakeLoss(name='bbox_loss', data=bbox_loss_)
+    bbox_loss_ = bbox_weight * mx.symbol.smooth_l1(name='bbox_loss_', scalar=3.0, data=(rpn_bbox_pred - bbox_target))
-def get_vgg_rpn_test(num_classes=21, num_anchors=9):
+def get_vgg_rpn_test(num_anchors=config.NUM_ANCHORS):
-        op_type='proposal', feat_stride=16, scales=(8, 16, 32), ratios=(0.5, 1, 2), output_score=True)
+    if config.TEST.CXX_PROPOSAL:
-def get_vgg_test(num_classes=21, num_anchors=9):
+def get_vgg_test(num_classes=config.NUM_CLASSES, num_anchors=config.NUM_ANCHORS):
-        op_type='proposal', feat_stride=16, scales=(8, 16, 32), ratios=(0.5, 1, 2))
+    if config.TEST.CXX_PROPOSAL:
-        name='roi_pool5', data=relu5_3, rois=rois, pooled_size=(7, 7), spatial_scale=0.0625)
+        name='roi_pool5', data=relu5_3, rois=rois, pooled_size=(7, 7), spatial_scale=1.0 / config.RCNN_FEAT_STRIDE)
-def get_faster_rcnn(num_classes=21, num_anchors=9):
+
-    Faster R-CNN with VGG 16 conv layers
+    Faster R-CNN end-to-end with VGG 16 conv layers
-    gt_boxes = mx.symbol.Reshape(data=gt_boxes, shape=(-1, 5), name='gt_boxes_reshape')
+    rpn_label = mx.symbol.Variable(name='label')
-    ## RPN
+    # RPN layers
-                                       normalization='valid', use_ignore=True, ignore_label=-1, name="rpn_cls_loss")
+    rpn_cls_prob = mx.symbol.SoftmaxOutput(data=rpn_cls_score_reshape, label=rpn_label, multi_output=True,
-    rpn_bbox_loss = mx.sym.MakeLoss(name='rpn_bbox_loss', data=rpn_bbox_loss_)
+    rpn_bbox_loss_ = rpn_bbox_weight * mx.symbol.smooth_l1(name='rpn_bbox_loss_', scalar=3.0, data=(rpn_bbox_pred - rpn_bbox_target))
-    # R-CNN
+    # Fast R-CNN
-
+        name='roi_pool5', data=relu5_3, rois=rois, pooled_size=(7, 7), spatial_scale=1.0 / config.RCNN_FEAT_STRIDE)
-    cls_prob = mx.symbol.SoftmaxOutput(name='cls_prob', data=cls_score, label=rois[1], normalization='batch')
+    cls_prob = mx.symbol.SoftmaxOutput(name='cls_prob', data=cls_score, label=label, normalization='batch')
-    bbox_loss = mx.sym.MakeLoss(name='bbox_loss', data=bbox_loss_, grad_scale=1.0 / config.TRAIN.BATCH_SIZE)
+    bbox_loss_ = bbox_weight * mx.symbol.smooth_l1(name='bbox_loss_', scalar=1.0, data=(bbox_pred - bbox_target))
-    group = mx.symbol.Group([mx.sym.BlockGrad(rois[1]), rpn_cls_loss, rpn_bbox_loss, cls_prob, bbox_pred])  # rois[1] is used for evaluation
+    label = mx.symbol.Reshape(data=label, shape=(config.TRAIN.BATCH_IMAGES, -1), name='label_reshape')
-    return group
+    group = mx.symbol.Group([rpn_cls_prob, rpn_bbox_loss, cls_prob, bbox_loss, mx.symbol.BlockGrad(label)])
-    cv2.imwrite("result.jpg", im)
+import argparse
-if __name__ == '__main__':
+def parse_args():
-              args.has_rpn)
+    ctx = mx.gpu(args.gpu)
-from utils.combine_model import combine_model
+from rcnn.config import config, default, generate_config
-                    ctx, begin_epoch, rpn_epoch, rcnn_epoch, frequent, kv_store, work_load_list=None):
+def alternate_train(args, ctx, pretrained, epoch,
-              'model/rpn1', ctx, begin_epoch, rpn_epoch, frequent, kv_store, work_load_list)
+    train_rpn(args.network, args.dataset, args.image_set, args.root_path, args.dataset_path,
-    test_rpn(image_set, year, root_path, devkit_path, 'model/rpn1', rpn_epoch, ctx[0])
+    image_sets = [iset for iset in args.image_set.split('+')]
-               'model/rcnn1', ctx, begin_epoch, rcnn_epoch, frequent, kv_store, work_load_list)
+    train_rcnn(args.network, args.dataset, args.image_set, args.root_path, args.dataset_path,
-              'model/rpn2', ctx, begin_epoch, rpn_epoch, frequent, kv_store, work_load_list)
+    train_rpn(args.network, args.dataset, args.image_set, args.root_path, args.dataset_path,
-    test_rpn(image_set, year, root_path, devkit_path, 'model/rpn2', rpn_epoch, ctx[0])
+    image_sets = [iset for iset in args.image_set.split('+')]
-               'model/rcnn2', ctx, begin_epoch, rcnn_epoch, frequent, kv_store, work_load_list)
+    train_rcnn(args.network, args.dataset, args.image_set, args.root_path, args.dataset_path,
-                        default=None, type=list)
+    # general
-if __name__ == '__main__':
+
-                    args.frequent, args.kv_store, args.work_load_list)
+    print 'Called with argument:', args
-import os
+import pprint
-                  work_load_list=None, resume=False, use_flip=True, factor_step=50000):
+import numpy as np
-    config.END2END = 1
+
-    sym = get_faster_rcnn(num_classes=num_classes)
+
-    config.TRAIN.BATCH_SIZE *= len(ctx)  # no used here
+    batch_size = len(ctx)
-                       ('gt_boxes', (config.TRAIN.IMS_PER_BATCH, 5*100))]  # assume at most 100 object in image
+    max_data_shape = [('data', (input_batch_size, 3, max([v[0] for v in config.SCALES]), max([v[1] for v in config.SCALES])))]
-        fixed_param_prefix = ['conv1', 'conv2', 'conv3', 'conv4', 'conv5']
+    # infer shape
-        fixed_param_prefix = ['conv1', 'conv2']
+        arg_params, aux_params = load_param(pretrained, epoch, convert=True)
-    bbox_metric = SmoothL1LossMetric()
+    mod = MutableModule(sym, data_names=data_names, label_names=label_names,
-                        'wd': wd,
+    # callback
-        mon = mx.mon.Monitor(100, norm_stat)
+                        'lr_scheduler': lr_scheduler,
-            arg_params=args, aux_params=auxs, begin_epoch=begin_epoch, num_epoch=num_epoch)
+            batch_end_callback=batch_end_callback, kvstore=args.kvstore,
-                        help='if true, then will use monitor debug')
+    parser = argparse.ArgumentParser(description='Train Faster R-CNN network')
-if __name__ == '__main__':
+
-                  args.work_load_list, args.resume, not args.no_flip, args.factor_step)
+    print 'Called with argument:', args
-    assert (exec1.outputs[0].asnumpy()== np.ones(shape)).all()
+def test_binary_logic():
-    """Try to configure cython and retyurn cython configuration"""
+    """Try to configure cython and return cython configuration"""
-            self._mean_pixels = mean_pixels
+        self._mean_pixels = mx.nd.array(mean_pixels).reshape((3,1,1))
-        self.mean_pixels = mean_pixels
+        self.mean_pixels = mx.nd.array(mean_pixels).reshape((3,1,1))
-release = '0.9.1'
+version = '0.9.2'
-__version__ = "0.9.1"
+__version__ = "0.9.2"
-                            (args.mean_r, args.mean_g, args.mean_b),
+                            mx.nd.array((args.mean_r, args.mean_g, args.mean_b)).reshape((3,1,1)),
-    net = block8(net, with_act=False, input_num_channel=2080)
+    net = block8(net, with_act=False, input_num_channels=2080)
-                 shuffle=False, part_index=0, num_parts=1, aug_list=None, **kwargs):
+                 shuffle=False, part_index=0, num_parts=1, aug_list=None, imglist=None, **kwargs):
-        assert path_imgrec or path_imglist
+        assert(path_imgrec or path_imglist or (isinstance(imglist, list)))
-        """Get the number of worker ndoes
+        """Get the number of worker nodes
-from __future__ import absolute_import
+from __future__ import absolute_import, print_function
-            '\033[91m[Deprecation Warning] mxnet.model.FeedForward has been deprecated. ' + \
+        print('\033[91m[Deprecation Warning] mxnet.model.FeedForward has been deprecated. ' + \
-        if layer_type == 'Convolution' or layer_type == 'InnerProduct' or layer_type == 4 or layer_type == 14:
+        if layer_type == 'Convolution' or layer_type == 'InnerProduct' or layer_type == 4 or layer_type == 14 \
-        self.captcha = ImageCaptcha(fonts=['./data/Xerox.ttf'])
+        # you can get this font from http://font.ubuntu.com/
-    contexts = [mx.context.gpu(1)]
+    contexts = [mx.context.gpu(0)]
-              batch_end_callback=mx.callback.Speedometer(BATCH_SIZE, 50),)
+              batch_end_callback=mx.callback.Speedometer(BATCH_SIZE, 50),
-        'trace.json'.
+        'profile.json'.
-            record.write_idx(item[0], s)
+            if s is not None:
-    def __init__(self):
+    def __init__(self, eps=1e-8):
-            self.sum_metric += (-numpy.log(prob)).sum()
+            self.sum_metric += (-numpy.log(prob + self.eps)).sum()
-            channels = layer_blobs[0].channels;
+            channels = wmat_dim[1]
-        batch_label = nd.zeros(self.provide_label[0][1])
+        batch_data = nd.empty((batch_size, c, h, w))
-                    batch_data[i][:] = d
+                    batch_data[i][:] = nd.transpose(d, axes=(2, 0, 1))
-        batch_data = nd.transpose(batch_data, axes=(0, 3, 1, 2))
+
-def test_batch_dot(ctx=default_context()):
+def test_batch_dot():
-                    b_init_grad_npy = np.random.normal(size=(batch_size, k, n))
+                    c = mx.sym.batch_dot(a, b, transpose_a=transpose_a, transpose_b=transpose_b)
-    return arr
+    if ctx is None:
-    return arr
+    if ctx is None:
-    assert_allclose(exec1.outputs[0].asnumpy(), np.zeros((3,4)))
+    def test_basic_val_init(sym_func, np_func, shape, dtype):
-    pooling = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pool_type="max", name=('max_pool_%s_pool' % name))
+    pooling = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type="max", name=('max_pool_%s_pool' % name))
-        self.save_params('%s-%04d.params'%(prefix, epoch))
+        param_name = '%s-%04d.params' % (prefix, epoch)
-            self.save_optimizer_states('%s-%04d.states'%(prefix, epoch))
+            state_name = '%s-%04d.states' % (prefix, epoch)
-        self._mean_pixels = mean_pixels
+        if isinstance(mean_pixels, list):
-        batch_data = []
+        batch_data = mx.nd.zeros((self.batch_size, 3, self._data_shape[0], self._data_shape[1]))
-            img = cv2.imread(im_path)
+            with open(im_path, 'rb') as fp:
-            batch_data.append(data)
+            batch_data[i] = data
-        self._data = {'data': mx.nd.array(np.array(batch_data))}
+        self._data = {'data': batch_data}
-                    data = data[ymin:ymax, xmin:xmax, :]
+                    data = mx.img.fixed_crop(data, xmin, ymin, xmax-xmin, ymax-ymin)
-                    data = np.full((new_height, new_width, 3), 128.)
+                    data = mx.nd.full((new_height, new_width, 3), 128, dtype='uint8')
-        data = transform(data, self._mean_pixels)
+        data = mx.img.imresize(data, self._data_shape[0], self._data_shape[1], interp_method)
-              args.data_shape, (args.mean_r, args.mean_g, args.mean_b),
+              args.data_shape, [args.mean_r, args.mean_g, args.mean_b],
-def color_normalize(src, mean, std):
+def color_normalize(src, mean, std=None):
-def full(shape, val, ctx=None):
+def full(shape, val, ctx=None, dtype=mx_real_t):
-    val : float
+    val : float or int
-    arr = empty(shape, ctx)
+    arr = empty(shape, ctx, dtype)
-        assert os.path.exists(label_file), 'Path does not exist: {}'.format(image_file)
+        assert os.path.exists(label_file), 'Path does not exist: {}'.format(label_file)
-
+    # Test dot with transpose flag using gradient checker.
-    positive_examples = list(open("./data/rt-polaritydata/rt-polarity.pos").readlines())
+    pos_path = "./data/rt-polaritydata/rt-polarity.pos"
-    negative_examples = list(open("./data/rt-polaritydata/rt-polarity.neg").readlines())
+    negative_examples = list(open(neg_path).readlines())
-        super(ImageListIter, self).__init__()
+        mx.io.DataIter.__init__(self)
-        delta[:] = (self.gamma2) * delta - lr * (grad/sqrt(n - g*g + 1e-4) + wd * weight)
+        delta[:] = (self.gamma2) * delta - lr * (grad/(sqrt(n - g*g) + 1e-8) + wd * weight)
-    tower_conv1_2 = ConvFactory(net, 256, (3, 1), pad=(1, 0))
+    tower_conv1_1 = ConvFactory(tower_conv1_0, 224, (1, 3), pad=(0, 1))
-    net = mx.symbol.Dropout(data=net, p=0.8)
+    net = mx.symbol.Dropout(data=net, p=0.2)
-    # pylint: disable= no-member
+    # pylint: disable= no-member, undefined-variable
-            return _internal._plus(self, other, out=self)
+            return broadcast_add(self, other, out=self)
-            return _internal._minus(self, other, out=self)
+            return broadcast_sub(self, other, out=self)
-            return _internal._mul(self, other, out=self)
+            return broadcast_mul(self, other, out=self)
-            return _internal._div(self, other, out=self)
+            return broadcast_div(self, other, out=self)
-    subprocess.call('cd ' + r_root +'; R -e "roxygenize2::roxygenize()"; R CMD Rd2pdf . --no-preview -o ' + pdf_path, shell = True)
+    subprocess.call('cd ' + r_root +'; R -e "roxygen2::roxygenize()"; R CMD Rd2pdf . --no-preview -o ' + pdf_path, shell = True)
-    In order for a module to interactive with others, a module should be able to report the
+    In order for a module to interact with others, a module should be able to report the
-    subprocess.call('cd ' + r_root +'; R CMD Rd2pdf . --no-preview -o ' + pdf_path, shell = True)
+    subprocess.call('cd ' + r_root +'; R -e "roxygenize2::roxygenize()"; R CMD Rd2pdf . --no-preview -o ' + pdf_path, shell = True)
-release = '0.8.0'
+version = '0.9.1'
-__version__ = "0.8.0"
+__version__ = "0.9.1"
-            group = internals[name].attr("ctx_group")
+            group = internals[name].attr("__ctx_group__")
-def Variable(name, attr=None, shape=None, lr_mult=None, wd_mult=None):
+def Variable(name, attr=None, shape=None, lr_mult=None, wd_mult=None, dtype=None):
-                                        'group': '1'})
+                                        'group': '1',
-    assert contain({'__mood__': 'so so'}, op.list_attr())
+                            num_filter=1, attr={'__mood__': 'so so', 'wd_mult': 'x'})
-                            num_filter=1, attr={'__mood__': 'so so'})
+                            num_filter=1, attr={'__mood__': 'so so'}, lr_mult=1)
-        'conv': {'kernel': '(1, 1)', '__mood__': 'so so', 'num_filter': '1'},
+        'conv': {'kernel': '(1, 1)', '__mood__': 'so so', 'num_filter': '1', 'lr_mult': '1', '__lr_mult__': '1'},
-    test_attr_dict()
+    test_attr_dict()
-            self._updater = opt.get_updater(optimizer)
+        else:
-    return updater
+    return Updater(optimizer)
-    cgroup.add_argument('--shuffle', default=True, help='If this is set as True, \
+    cgroup.add_argument('--shuffle', type=bool, default=True, help='If this is set as True, \
-    rgroup.add_argument('--pack-label', default=False,
+    rgroup.add_argument('--pack-label', type=bool, default=False,
-        lr             = .1,
+        lr             = .05,
-        lr             = .1,
+        lr             = .05,
-            'wd' : args.wd}
+            'wd' : args.wd,
-    def __init__(self, data_names, data, label_names, label, bucket_key):
+    def __init__(self, data_names, data, data_layouts, label_names, label, label_layouts, bucket_key):
-        return [(n, x.shape) for n, x in zip(self.data_names, self.data)]
+        return [mx.io.DataDesc(n, x.shape, layout=l) for n, x, l in zip(self.data_names, self.data, self.data_layouts)]
-        return [(n, x.shape) for n, x in zip(self.label_names, self.label)]
+        return [mx.io.DataDesc(n, x.shape, layout=l) for n, x, l in zip(self.label_names, self.label, self.label_layouts)]
-            self.provide_label = [('softmax_label', (self.default_bucket_key, batch_size))]
+            self.provide_data = [mx.io.DataDesc('data', (self.default_bucket_key, batch_size), layout='TN')] + init_states
-            data_batch = SimpleBatch(data_names, data_all, label_names, label_all,
+            data_batch = SimpleBatch(data_names, data_all, [x.layout for x in self.provide_data],
-    init_c = [('LSTM_state_cell', (num_lstm_layer, batch_size, num_hidden))]
+    init_h = [mx.io.DataDesc('LSTM_state', (num_lstm_layer, batch_size, num_hidden), layout='TNC')]
-import re
+# pylint: disable=W0622
-        raise NotImplementedError()
+    def __repr__(self):
-    def get_batch_axis(self, name):
+    @staticmethod
-            The name of the blob (could be data, label or output names).
+        layout : str
-        dimension. Axis can be -1, which means the array thing will be copied for each
+        dimension. Axis can be -1, which means the whole array will be copied for each
-        # it returns -1, which is what we expect
+            return 0
-
+from ..io import DataDesc
-                 logger=logging, fixed_param_names=None, layout_mapper=None, grad_req='write'):
+                 logger=logging, fixed_param_names=None, grad_req='write'):
-        data_names = [x[0] for x in data_shapes]
+        data_shapes = [x if isinstance(x, DataDesc) else DataDesc(*x) for x in data_shapes]
-        self.output_layouts = [self.layout_mapper.get_batch_axis(name)
+
-                      for (name, _) in data_shapes]
+        major_axis = [DataDesc.get_batch_axis(x.layout) for x in data_shapes]
-        self._label_shapes = label_shapes
+        self._data_shapes = \
-                    input_types[item[0]] = mx_real_t
+        input_types = {x.name: x.dtype for x in self._data_shapes}
-                                                     label_shapes, self._param_names,
+                                                     self._work_load_list, self._data_shapes,
-                                                     layout_mapper=self.layout_mapper,
+def print_max_err_loc(a, b, rtol=1e-7, atol=0):
-        symbol to run the consistency test
+    sym : Symbol or list of Symbols
-    sym = mx.sym.Convolution(num_filter=3, kernel=(3,3), name='conv')
+    sym1 = mx.sym.Convolution(num_filter=3, kernel=(3,3), name='conv')
-                {'ctx': mx.cpu(0), 'conv_data': (2, 2, 10, 10), 'type_dict': {'conv_data': np.float32}}]
+                {'ctx': mx.cpu(0), 'conv_data': (2, 2, 10, 10), 'type_dict': {'conv_data': np.float32}},
-    check_consistency(sym, ctx_list)
+    # this is unstable
-    test_convolution_with_type()
+    test_batchnorm_with_type()
-    #test_multi_softmax_with_shape((3,4,5), mx.gpu())
+import mxnet as mx
-    assert reldiff(out, npout) < 1e-6, "%s mathematical forward failed\n%s\n\n%s" % (name, out, npout)
+    assert reldiff(out, npout) < 1e-5, "%s mathematical forward failed\n%s\n\n%s" % (name, out, npout)
-    assert reldiff(arr_grad, npout_grad) < 1e-6, "%s mathematical backward failed\n%s\n\n%s" % (
+    assert reldiff(arr_grad, npout_grad) < 1e-5, "%s mathematical backward failed\n%s\n\n%s" % (
-# pylint: disable=import-error, no-name-in-module
+# pylint: disable=import-error, no-name-in-module, undefined-variable
-        _internal._plus,
+        broadcast_add,
-        _internal._minus,
+        broadcast_sub,
-        _internal._mul,
+        broadcast_mul,
-        _internal._div,
+        broadcast_div,
-        _internal._power,
+        broadcast_power,
-        _internal._maximum,
+        broadcast_maximum,
-        _internal._minimum,
+        broadcast_minimum,
-        _internal._equal,
+        broadcast_equal,
-        _internal._not_equal,
+        broadcast_not_equal,
-        _internal._greater,
+        broadcast_greater,
-        _internal._greater_equal,
+        broadcast_greater_equal,
-        _internal._lesser,
+        broadcast_lesser,
-        _internal._lesser_equal,
+        broadcast_lesser_equal,
-
+from numpy.testing import assert_allclose
-            library_dirs = ['mxnet', '../build/Release']
+            library_dirs = ['mxnet', '../build/Release', '../build']
-        # initializer   = mx.init.Xavier(factor_type="in", magnitude=2.34),
+    model = mx.mod.Module(
-        X                  = train,
+    model.fit(train,
-        epoch_end_callback = checkpoint)
+        epoch_end_callback = checkpoint,
-                            pre_filter = pre_filter + int(shape[0])
+                        if key in shape_dict:
-                + int(node["param"]["num_filter"])
+                * int(_str2tuple(node["attr"]["kernel"])[0]) \
-            cur_param = pre_filter * (int(node["param"]["num_hidden"]) + 1)
+            cur_param = pre_filter * (int(node["attr"]["num_hidden"]) + 1)
-            cur_param = int(num_filter) * 2
+            if show_shape:
-                    out_shape = shape_dict[key][1:]
+                if key in shape_dict:
-                    source: "caffe/examples/mnist/mnist_train_lmdb" \
+                    source: "mnist_train_lmdb" \
-                    source: "caffe/examples/mnist/mnist_test_lmdb" \
+                    source: "mnist_test_lmdb" \
-import os
+import os, sys
-        label=os.path.join(basedir, "../image-classification/mnist/train-labels-idx1-ubyte"),
+        image=os.path.join(basedir, "data", "train-images-idx3-ubyte"),
-        label=os.path.join(basedir, "../image-classification/mnist/t10k-labels-idx1-ubyte"),
+        image=os.path.join(basedir, "data", "t10k-images-idx3-ubyte"),
-import os
+import os, sys
-        label=os.path.join(basedir, "../image-classification/mnist/train-labels-idx1-ubyte"),
+        image=os.path.join(basedir, "data", "train-images-idx3-ubyte"),
-        label=os.path.join(basedir, "../image-classification/mnist/t10k-labels-idx1-ubyte"),
+        image=os.path.join(basedir, "data", "t10k-images-idx3-ubyte"),
-from train_cifar10 import get_iterator
+sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
-args.data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "image-classification", "cifar10")) + '/'
+args.data_dir = os.path.join(os.path.dirname(__file__), "data")
-train, val = get_iterator(args, kv)
+
-from symbol_resnet import get_conv
+def get_conv(
-        label="../image-classification/mnist/train-labels-idx1-ubyte",
+        image=os.path.join(basedir, "data", "train-images-idx3-ubyte"),
-        label="../image-classification/mnist/t10k-labels-idx1-ubyte",
+        image=os.path.join(basedir, "data", "t10k-images-idx3-ubyte"),
-    assert np.abs(exe1.outputs[0].asnumpy()-forward_result).mean()<1e-4
+    assert np.abs(exe1.outputs[0].asnumpy()-forward_result).mean() < 1e-4
-    assert np.abs(exe1.grad_dict['img2'].asnumpy() - grad2).mean() < 1e-4
+    assert np.abs(exe1.grad_dict['img1'].asnumpy() - grad1).mean() < 1e-3
-    def test_reduce_inner(numpy_reduce_func, numpy_reduce_grad_func, mx_reduce_sym):
+    def test_reduce_inner(numpy_reduce_func, numpy_reduce_grad_func, mx_reduce_sym, nan_prob = 0):
-            assert err_forward < 1E-4
+            equal_forward = almost_equal_ignore_nan(net.outputs[0].asnumpy(), sum_groundtruth, 1E-4, 1E-4)
-            assert err_backward < 1E-4
+            bc_grad_groundtruth = np.broadcast_to(grad_groundtruth, grad_nd.shape)
-        None)
+        _internal._lesser_scalar)
-        None)
+        _internal._lesser_equal_scalar)
-        None)
+        _internal._greater_scalar)
-        None)
+        _internal._greater_equal_scalar)
-    wt = w + wshift
+    wt = mx.sym.broadcast_add(w, wshift)
-def _gen_broadcast_data():
+def gen_broadcast_data():
-def _check_broadcast_op_forward(symbol, baseline):
+def gen_binary_data():
-        d = _gen_broadcast_data()
+        d = gen_data()
-            err, d[0].shape, d[1].shape)
+        assert_allclose(x, y.outputs[0].asnumpy(), rtol=1e-3, atol=1e-5)
-def _check_broadcast_op_backward(symbol, baseline):
+def check_binary_op_backward(symbol, baseline, gen_data):
-        d = _gen_broadcast_data()
+        d = gen_data()
-            err_1, err_2, d[0].shape, d[1].shape)
+        assert_allclose(x_1, y_1.asnumpy(), rtol=1e-3, atol=1e-5)
-        _check_broadcast_op_backward(c, lambda g_out, a, b: (g_out, g_out))
+        check_binary_op_forward(c, lambda a, b: a + b, gen_broadcast_data)
-        _check_broadcast_op_backward(c, lambda g_out, a, b: (g_out, - g_out))
+        check_binary_op_forward(c, lambda a, b: a - b, gen_broadcast_data)
-        _check_broadcast_op_backward(c, lambda g_out, a, b: (g_out * b, g_out * a))
+        check_binary_op_forward(c, lambda a, b: a * b, gen_broadcast_data)
-        _check_broadcast_op_backward(c, lambda g_out, a, b: (g_out / b, - g_out * a / (b * b)))
+        check_binary_op_forward(c, lambda a, b: a / b, gen_broadcast_data)
-                                                             g_out * a ** b * np.log(a)))
+        check_binary_op_forward(c, lambda a, b: a ** b, gen_broadcast_data)
-    exec1 = z.bind(default_context(),args={'y': mx.nd.ones(shape) })
+    exec1 = z.bind(default_context(),args={'y': mx.nd.ones(shape)})
-    exec1 = z.bind(default_context(),args={'y': mx.nd.ones(shape) })
+    exec1 = z.bind(default_context(),args={'y': mx.nd.ones(shape)})
-    z = y >= 1
+    z = 0 >= y
-    exec1 = z.bind(default_context(),args={'y': mx.nd.ones(shape) })
+    exec1 = z.bind(default_context(),args={'y': mx.nd.ones(shape)})
-from .base import _LIB
+from .base import _LIB, numeric_types
-                                                      axis=axes, keepdims=keepdims)
+                                                      outdata=sum_groundtruth,
-                        outgrad.reshape(np_reduce(data, axis, 1, np.sum).shape),
+                      lambda outgrad, data, outdata, axis, keepdims, keepdim_shape:
-    logging.basicConfig(level=logging.DEBUG, format=head)
+import logging
-    logging.basicConfig(level=logging.DEBUG, format=head)
+import logging
-    
+
-    logging.basicConfig(level=logging.DEBUG, format=head)
+import logging
-    logging.basicConfig(level=logging.DEBUG, format=head)
+import logging
-                        'Please use mxnet.mod.Module instead.')
+        logging.warning(
-                            self.name)
+                            'Grouped' if name is None else name)
-                                                    _str2tuple(node["attr"]["stride"])[0],
+                                                    _str2tuple(node["attr"]["stride"])[0]
-                                                _str2tuple(node["attr"]["stride"])[0])
+                                                _str2tuple(node["attr"]["stride"])[0]
-                label = node["param"]["op_type"]
+                label = node["attr"]["op_type"]
-# pylint: disable=arguments-differ, too-many-arguments
+# pylint: disable=arguments-differ, too-many-arguments, no-member
-from .base import check_call, ctypes2docstring, MXNetError
+from .base import check_call, MXNetError
-        assert len(pos_args) <= len(arguments)
+        if len(pos_args) > len(arguments):
-                      lambda x: (1 / x)) 
+                      lambda x: (1 / x))
-	assert not mx.test_utils.almost_equal(args1['fc2_weight'], args2['fc2_weight'], 1e-1)
+    data = mx.sym.Variable('data')
-	test_lr_wd_mult()
+    test_adam()
-    ctx = [mx.gpu(0)]
+    ctx = [mx.cpu(0)]
-        check_call(_LIB.NNSymbolSetAttrs(
+        check_call(_LIB.MXSymbolSetAttrs(
-        name = kwargs.pop('name', None)
+
-        check_call(_LIB.NNSymbolCreateAtomicSymbol(
+        check_call(_LIB.MXSymbolCreateAtomicSymbol(
-            s._set_attr(**attr)
+
-            return attr
+            return attr if attr else {}
-                    self.lr_mult[k[:-len('_lr_mult')]] = float(v)
+            attr = self.sym.attr_dict()
-                    self.wd_mult[k[:-len('_wd_mult')]] = float(v)
+            attr = self.sym.attr_dict()
-            **not** be pre-pended with the symbol name.
+        Returns
-        f_handle = _LIB.MXSymbolListAttr if recursive else _LIB.MXSymbolListAttrShallow
+        f_handle = _LIB.MXSymbolListAttrShallow
-                       for name in self.list_auxiliary_states()]
+            attr_dict = self.attr_dict()
-def Variable(name, attr=None, shape=None):
+def Variable(name, attr=None, shape=None, lr_mult=None, wd_mult=None):
-        ret._set_attr(**attr)
+    if lr_mult is not None:
-    ret = np.max(diff / norm)
+    diff = np.sum(np.abs(a - b))
-    with mx.AttrScope(group='4', data='great'):
+    with mx.AttrScope(__group__='4', __data__='great'):
-        with mx.AttrScope(init_bias='0.0'):
+        with mx.AttrScope(__init_bias__='0.0'):
-    assert fc2.attr('init_bias') == '0.0'
+    assert fc1.attr('__data__') == 'great'
-        if y[k] != v:
+        if isinstance(y[k], dict):
-    assert contain({'mood': 'so so'}, op.list_attr())
+                            num_filter=1, attr={'__mood__': 'so so'})
-# from setuptools import setup
+if "--inplace" in sys.argv:
-        from distutils.extension import Extension
+    wshift = mx.sym.Variable("wshift", shape=(1,))
-    wt = w + 19
+    # broadcast add here, not being able to deduce shape correctly
-    out  = mx.symbol.Activation(data=x2h+h2h, name='out', act_type='relu')
+    out  = mx.symbol.Activation(data=mx.sym.elemwise_add(x2h, h2h), name='out', act_type='relu')
-    c = a+b
+    c = mx.symbol.elemwise_add(a, b)
-from .ndarray import NDArray, zeros, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
+from .ndarray import NDArray, zeros as _nd_zeros, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
-            zeros(shape, dev, dtype=dtype)
+            _nd_zeros(shape, dev, dtype=dtype)
-                    grad_ndarrays[name] = zeros(shape, dev, dtype=dtype)
+                    grad_ndarrays[name] = _nd_zeros(shape, dev, dtype=dtype)
-        aux_ndarrays = [zeros(shape, dev, dtype=dtype)
+        aux_ndarrays = [_nd_zeros(shape, dev, dtype=dtype)
-    print(len(aux_shapes))
+
-    norm = np.maximum(np.abs(a), np.abs(b)) + 1e-07
+    norm = np.maximum(np.abs(a), np.abs(b)) + 1e07
-    norm = np.maximum(np.abs(a), np.abs(b)) + 1e07
+    norm = np.maximum(np.abs(a), np.abs(b)) + 1e-07
-            out_ele_shape[axis] /= num_outputs
+            out_ele_shape[axis] //= num_outputs
-        sym = mx.sym.SliceChannel(data=data, num_outputs=num_outputs, axis=axis)
+        sym = mx.sym.SliceChannel(data=data, num_outputs=num_outputs, axis=axis, squeeze_axis=squeeze_axis)
-    exe_test = test.bind(mx.cpu(), args=[arr_data1, arr_data2], args_grad=[arr_grad1, arr_grad2])
+    exe_test = test.bind(default_context(), args=[arr_data1, arr_data2], args_grad=[arr_grad1, arr_grad2])
-    exe_test = test.bind(mx.cpu(), args=[arr_data], args_grad=[arr_grad])
+    exe_test = test.bind(default_context(), args=[arr_data], args_grad=[arr_grad])
-    exe_test = test.bind(mx.cpu(), args=[arr_data1, arr_data2], args_grad=[arr_grad1, arr_grad2])
+    exe_test = test.bind(default_context(), args=[arr_data1, arr_data2], args_grad=[arr_grad1, arr_grad2])
-    exe_test = test.bind(mx.cpu(), args=[arr_data], args_grad=[arr_grad])
+    exe_test = test.bind(default_context(), args=[arr_data], args_grad=[arr_grad])
-OptimizerCreator = ctypes.c_void_p
+            assert d_targets[-1][0].stop == d_src.shape[0], \
-
+class ccSGD(SGD):
-    def test_reduce_inner(numpy_reduce_func, nd_reduce_func):
+    def test_reduce_inner(numpy_reduce_func, nd_reduce_func, multi_axes):
-                axes = tuple(range(ndim))
+            keepdims = np.random.randint(0, 2)
-                axes = tuple(axes)
+                axes = np.random.randint(0, ndim)
-                      mx.nd.sum)
+                      mx.nd.sum, True)
-                      mx.nd.max)
+                      mx.nd.max, True)
-                      mx.nd.min)
+                      mx.nd.min, True)
-    exe = y.simple_bind(mx.cpu(), x=(5,4))
+    exe = y.simple_bind(mx.cpu(), x=(5,4), grad_req=[])
-# pylint: disable=no-member, protected-access
+# pylint: disable=no-member, protected-access, unused-import, no-name-in-module
-
+from ._ndarray_internal import _sample_uniform as uniform
-        assert abs(np.mean(un1.asnumpy()) - (a+b)/2) < 0.1
+    a, b = -10, 10
-                        raise ValueError('Must specify all the arguments in %s' % arg_key)
+                        raise ValueError('key `%s` is missing in `%s`' % (name, arg_key))
-    """Scale shorter edge to size"""
+def resize_short(src, size, interp=2):
-    """Make scale shorter edge to size augumenter"""
+def ResizeAug(size, interp=2):
-        return [resize(src, size, interp)]
+        return [resize_short(src, size, interp)]
-def CreateAugmenter(data_shape, scale=0, rand_crop=False, rand_resize=False, rand_mirror=False,
+def CreateAugmenter(data_shape, resize=0, rand_crop=False, rand_resize=False, rand_mirror=False,
-        auglist.append(ScaleAug(scale, inter_method))
+    if resize > 0:
-# pylint: disable=too-many-arguments, too-many-locals, no-name-in-module, too-many-branches
+# pylint: disable=too-many-arguments, too-many-locals, no-name-in-module, too-many-branches, too-many-statements
-from __future__ import absolute_import
+from __future__ import absolute_import, print_function
-    src = src - mean
+    src -= mean
-        src = src / std
+        src /= std
-def random_size_crop(src, size, min_area=0.08, ratio=(3.0/4.0, 4.0/3.0), interp=2):
+def random_size_crop(src, size, min_area, ratio, interp=2):
-            new_w, new_h = new_h, new_w
+    new_ratio = random.uniform(*ratio)
-            continue
+    min_area *= h*w
-        y0 = random.randint(0, h - new_h)
+    assert new_w <= w and new_h <= h
-        return out, (x0, y0, new_w, new_h)
+    out = fixed_crop(src, x0, y0, new_w, new_h, size, interp)
-    return random_crop(src, size)
+def ScaleAug(size, interp=2):
-        return random_crop(src, size, interp)[0]
+        return [random_crop(src, size, interp)[0]]
-def RandomSizedCropAug(size, min_area=0.08, ratio=(3.0/4.0, 4.0/3.0), interp=2):
+def RandomSizedCropAug(size, min_area, ratio, interp=2):
-        return random_size_crop(src, size, min_area, ratio, interp)[0]
+        return [random_size_crop(src, size, min_area, ratio, interp)[0]]
-        return center_crop(src, size, interp)[0]
+        return [center_crop(src, size, interp)[0]]
-            src = i(src)
+        for t in ts:
-            return src
+            return [src]
-            return src
+            src += gray
-            return src
+            src += gray
-        return src
+        src += nd.array(rgb)
-        return color_normalize(src, mean, std)
+        return [color_normalize(src, mean, std)]
-        return src
+        return [src]
-        return src
+        return [src]
-def CreateAugmenter(data_shape, rand_crop=False, rand_resize=False, rand_mirror=False,
+def CreateAugmenter(data_shape, scale=0, rand_crop=False, rand_resize=False, rand_mirror=False,
-        auglist.append(RandomSizedCropAug(crop_size, inter_method))
+        auglist.append(RandomSizedCropAug(crop_size, 0.3, (3.0/4.0, 4.0/3.0), inter_method))
-    if mean:
+    if mean is not None:
-        self.provide_label = [('softmax_label', (batch_size, label_width))]
+        if label_width > 1:
-        batch_label = nd.zeros((batch_size, self.label_width))
+        batch_label = nd.zeros(self.provide_label[0][1])
-                data = imdecode(data)
+            while i < batch_size:
-                batch_label[i][:] = label
+                    data = [ret for src in data for ret in aug(src)]
-    opencv_available = False
+    cv2 = None
-    assert opencv_available
+    assert cv2 is not None
-    assert opencv_available
+    assert cv2 is not None
-        for path, _, files in os.walk(root, followlinks=True):
+        for path, dirs, files in os.walk(root, followlinks=True):
-        for k, v in cat.items():
+        for k, v in sorted(cat.items(), key=lambda x: x[1]):
-        for fname in os.listdir(root):
+        for fname in sorted(os.listdir(root)):
-            library_dirs = ['mxnet']
+            library_dirs = ['mxnet', '../build/Release']
-            pred_label = ndarray.argmax_channel(pred_label).asnumpy().astype('int32')
+            if pred_label.shape != label.shape:
-            print(len(cat), path)
+        for path, _, files in os.walk(root, followlinks=True):
-        write_list(args.prefix + str_chunk + '_train.lst', chunk[sep_test:sep_test + sep])
+        if args.train_ratio == 1.0:
-        print('imread error trying to load file: %s ' % fullpath, e)
+        print('imread error trying to load file: %s ' % fullpath)
-        print('imread read blank (None) image for file: %s' % fullpath, e)
+        print('imread read blank (None) image for file: %s' % fullpath)
-        header = mx.recordio.IRHeader(0, item[2], item[0], 0)
+    rgroup.add_argument('--pass-through', type=bool, default=False,
-                    import multiprocessing
+                if args.num_thread > 1 and multiprocessing is not None:
-                except ImportError:
+                else:
-                    record = mx.recordio.MXRecordIO(os.path.join(working_dir, fname_rec), 'w')
+                    record = mx.recordio.MXIndexedRecordIO(os.path.join(working_dir, fname_idx),
-                        image_encode(args, item, q_out)
+                    for i, item in enumerate(image_list):
-                        fidx.write('%d\t%d\n'%(item[0], record.tell()))
+                        record.write_idx(item[0], s)
-                 shuffle=False, part_index=0, num_parts=1, **kwargs):
+                 shuffle=False, part_index=0, num_parts=1, aug_list=None, **kwargs):
-                self.imgidx = self.imgrec.idx
+                self.imgidx = list(self.imgrec.keys)
-                        break
+                imgkeys = []
-                    imglist[int(line[0])] = (label, line[-1])
+                    key = int(line[0])
-                self.seq = self.imgidx.keys()
+        if self.imgrec is None:
-        self.auglist = CreateAugmenter(data_shape, **kwargs)
+        if aug_list is None:
-                label, fname = self.imglist[self.seq[self.cur]]
+                label, fname = self.imglist[idx]
-        return io.DataBatch(batch_data, batch_label, batch_size-1-i)
+        return io.DataBatch([batch_data], [batch_label], batch_size-1-i)
-        super(MXIndexedRecordIO, self).__init__(uri, flag)
+        self.keys = []
-                    self.idx[key_type(line[0])] = int(line[1])
+        self.fidx = None
-            super(MXIndexedRecordIO, self).open()
+        self.fidx.close()
-
+        self.fidx.write('%s\t%d\n'%(str(key), pos))
-    keys = reader.keys()
+    keys = reader.keys
-def image_encode(args, item, q_out):
+def image_encode(args, i, item, q_out):
-        print('imread error trying to load file: %s ' % fullpath)
+        print('imread error trying to load file: %s ' % fullpath, e)
-        print('imread read blank (None) image for file: %s' % fullpath)
+        print('imread read blank (None) image for file: %s' % fullpath, e)
-    except Exception:
+        q_out.put((i, s, item))
-        print('pack_img error on file: %s' % fullpath)
+        print('pack_img error on file: %s' % fullpath, e)
-        if item is None:
+        deq = q_in.get()
-        image_encode(args, item, q_out)
+        i, item = deq
-    while True:
+    buf = {}
-        fout.write(line)
+        if deq is not None:
-    os.rename(fname+'.tmp', fname)
+            if count % 1000 == 0:
-                        q_in[i % len(q_in)].put(item)
+                        q_in[i % len(q_in)].put((i, item))
-#from setuptools import setup
+
-        #from setuptools.extension import Extension
+        # from setuptools.extension import Extension
-from .ndarray import NDArray, zeros, clip, sqrt, square
+from .ndarray import NDArray, zeros, clip, sqrt
-            weight[:] += mom
+            sgd_mom_update(weight, grad, state, out=weight,
-
+            sgd_update(weight, grad, out=weight,
-        self.decay_factor = decay_factor
+        self.kwargs = {'beta1': beta1, 'beta2': beta2, 'epsilon': epsilon,
-            weight[:] -= (lr * wd) * weight
+        mean, var = state
-        write_list(args.prefix + str_chunk + '_test.lst', chunk[:sep_test])
+        if args.test_ratio:
-    fname_rec = os.path.basename(fname)
+    fname = os.path.basename(fname)
-    record = mx.recordio.MXRecordIO(os.path.join(working_dir, fname_rec), 'w')
+    record = mx.recordio.MXIndexedRecordIO(os.path.join(working_dir, fname_idx),
-        record.write(s)
+        record.write_idx(item[0], s)
-                    fname_rec = os.path.basename(fname)
+                    fname = os.path.basename(fname)
-    ret = diff / norm
+    diff = np.abs(a - b)
-            assert reldiff(out1, out2) < 1e-3
+            assert reldiff(out1, out2) < 2e-3
-            ndim = np.random.randint(1, 8)
+            ndim = np.random.randint(1, 6)
-            ndim = np.random.randint(1, 8)
+            ndim = np.random.randint(1, 6)
-    ndim = np.random.randint(1, 8)
+    ndim = np.random.randint(1, 6)
-            ndim = np.random.randint(1, 8)
+            ndim = np.random.randint(1, 6)
-        ndim = np.random.randint(1, 8)
+        ndim = np.random.randint(1, 6)
-    test_sequence_mask()
+    test_sequence_mask()
-    check_call(_LIB.NNListAllOpNames(ctypes.byref(size),
+    check_call(_LIB.MXListAllOpNames(ctypes.byref(size),
-import sys
+
-from .base import mx_uint, NDArrayHandle, FunctionHandle
+from .base import mx_uint, NDArrayHandle, check_call
-from .base import check_call, build_param_doc as _build_param_doc
+# Use different verison of SymbolBase
-class NDArray(object):
+class NDArray(NDArrayBase):
-        handle = this['handle']
+        handle = self.handle
-        self.__dict__.update(state)
+            self.handle = handle
-_init_ndarray_module()
+# coding: utf-8
-from setuptools import setup
+from distutils.core import setup
-        from setuptools.extension import Extension
+        #from setuptools.extension import Extension
-                "mxnet/%s/%s" % (subdir, fn[:-4]),
+                "mxnet/%s/.%s" % (subdir, fn[:-4]),
-from distutils.core import setup
+#from distutils.core import setup
-        from distutils.extension import Extension
+        from setuptools.extension import Extension
-      packages=['mxnet', 'mxnet.module', 'mxnet.notebook'],
+      packages=[
-from ..base import SymbolHandle
+from ..base import SymbolHandle, OpHandle
-def _make_atomic_symbol_function(handle):
+def _make_atomic_symbol_function(handle, name):
-    name = ctypes.c_char_p()
+    real_name = ctypes.c_char_p()
-        handle, ctypes.byref(name), ctypes.byref(desc),
+        handle, ctypes.byref(real_name), ctypes.byref(desc),
-    func_name = py_str(name.value)
+    func_name = name
-def _init_symbol_module(root):
+
-    plist = ctypes.POINTER(ctypes.c_void_p)()
+    _set_symbol_class(symbol_class)
-    module_internal = sys.modules["%s._symbol_internal" % root]
+    check_call(_LIB.NNListAllOpNames(ctypes.byref(size),
-        function = _make_atomic_symbol_function(hdl)
+        op_names.append(py_str(plist[i]))
-SymbolCreatorHandle = ctypes.c_void_p
+OpHandle = ctypes.c_void_p
-        from ._ctypes.symbol import SymbolBase, _set_symbol_class
+        from ._ctypes.symbol import SymbolBase, _init_symbol_module
-        from ._cy3.symbol import SymbolBase, _set_symbol_class, _init_symbol_module
+        from ._cy3.symbol import SymbolBase, _init_symbol_module
-        from ._cy2.symbol import SymbolBase, _set_symbol_class, _init_symbol_module
+        from ._cy2.symbol import SymbolBase, _init_symbol_module
-    from ._ctypes.symbol import SymbolBase, _set_symbol_class, _init_symbol_module
+    from ._ctypes.symbol import SymbolBase, _init_symbol_module
-_set_symbol_class(Symbol)
+_init_symbol_module(Symbol, "mxnet")
-import os, sys
+import os
-
+    ret_type = py_str(ret_type.value) if ret_type.value is not None else ''
-                         py_str(ret_type.value))
+                         ret_type)
-from .base import c_array, mx_float, py_str, c_str, mx_real_t
+from .base import c_array, py_str, c_str, mx_real_t
-                c_array(ctypes.c_char_p, [str(i).encode('ascii') for i in kwargs.values()])))
+            handle,
-            return [NDArray(ctypes.cast(output_vars[i], NDArrayHandle)) for i in range(num_output.value)]
+            return [NDArray(ctypes.cast(output_vars[i], NDArrayHandle))
-        from .ctypes.symbol import SymbolBase, _set_symbol_class
+        from ._ctypes.symbol import SymbolBase, _set_symbol_class
-    from .ctypes.symbol import SymbolBase, _set_symbol_class, _init_symbol_module
+except ImportError:
-    except:
+    except ImportError:
-    lib = ctypes.cdll.LoadLibrary(lib_path[0])
+    lib = ctypes.CDLL(lib_path[0], ctypes.RTLD_GLOBAL)
-    """Convert ctypes returned doc string information into parameters docstring.
+def build_param_doc(arg_names, arg_types, arg_descs, remove_dup=True):
-    arg_names : ctypes.POINTER(ctypes.c_char_p)
+    arg_names : list of str
-    arg_types : ctypes.POINTER(ctypes.c_char_p)
+    arg_types : list of str
-    arg_descs : ctypes.POINTER(ctypes.c_char_p)
+    arg_descs : list of str
-        key = py_str(arg_names[i])
+    for key, type_info, desc in zip(arg_names, arg_types, arg_descs):
-            ret += '\n    ' + py_str(arg_descs[i])
+        if len(desc) != 0:
-    param_str = ctypes2docstring(num_args, arg_names, arg_types, arg_descs)
+
-from .base import check_call, ctypes2docstring
+from .base import check_call, build_param_doc as _build_param_doc
-    param_str = ctypes2docstring(num_args, arg_names, arg_types, arg_descs)
+    narg = int(num_args.value)
-from __future__ import absolute_import
+from __future__ import absolute_import as _abs
-import numpy
+
-from .symbol_doc import SymbolDoc
+from .attribute import AttrScope
-    """Symbol is symbolic graph of the mxnet."""
+# Use different verison of SymbolBase
-        self.handle = handle
+class Symbol(SymbolBase):
-        return copy.deepcopy(self)
+        return self.__deepcopy__(None)
-        handle = this['handle']
+        handle = self.handle
-        return this
+            return {'handle': self.tojson()}
-        self.__dict__.update(state)
+            self.handle = handle
-        s = copy.deepcopy(self)
+        s = self.__copy__()
-                    s = numpy.dtype(s).type
+                    s = _numpy.dtype(s).type
-                v = numpy.dtype(v).type
+                v = _numpy.dtype(v).type
-
+# Initialize the atomic symbol in startups
-from .base import c_array, py_str, ctypes2docstring
+from .base import c_array, py_str, build_param_doc as _build_param_doc
-    param_str = ctypes2docstring(num_args, arg_names, arg_types, arg_descs)
+    narg = int(num_args.value)
-from setuptools import setup
+import os, sys
-      url='https://github.com/dmlc/mxnet')
+      url='https://github.com/dmlc/mxnet',
-    # Get the information from the function
+    key_var_num_args = ctypes.c_char_p()
-    check_call(_LIB.MXFuncGetInfo(
+    check_call(_LIB.MXSymbolGetAtomicSymbolInfo(
-    func_name = py_str(name.value)
+    key_var_num_args = py_str(key_var_num_args.value)
-    doc_str = doc_str % (py_str(desc.value), param_str)
+               'out : NDArray or list of NDArray\n' +
-        return out
+    arguments = []
-                raise TypeError('expect %d out in %s', n_mutate_vars, func_name)
+            output_vars = kwargs['out']
-            return mutate_vars[0]
+            output_vars = ctypes.POINTER(NDArrayHandle)()
-            return mutate_vars
+            return [NDArray(ctypes.cast(output_vars[i], NDArrayHandle)) for i in range(num_output.value)]
-    return ret_function
+    generic_ndarray_function.__name__ = func_name
-
+    check_call(_LIB.MXSymbolListAtomicSymbolCreators(ctypes.byref(size),
-            check_with_uniform(mx.nd.square, 2, dim, np.square, rmin=0)
+            check_with_uniform(mx.nd.sqrt, 1, dim, np.sqrt, rmin=0)
-                                                    node["param"]["num_filter"])
+            label = r"Convolution\n%sx%s/%s, %s" % (_str2tuple(node["attr"]["kernel"])[0],
-            label = r"FullyConnected\n%s" % node["param"]["num_hidden"]
+            label = r"FullyConnected\n%s" % node["attr"]["num_hidden"]
-            label = r"%s\n%s" % (op, node["param"]["act_type"])
+            label = r"%s\n%s" % (op, node["attr"]["act_type"])
-                                                _str2tuple(node["param"]["stride"])[0])
+            label = r"Pooling\n%s, %sx%s/%s" % (node["attr"]["pool_type"],
-
+    assert contain({'data_mood': 'angry', 'conv_mood': 'so so',
-    sym = mx.sym.MakeLoss(sym)
+    sym = mx.sym.MakeLoss(sym)
-release = '0.7.0'
+version = '0.8.0'
-__version__ = "0.7.0"
+__version__ = "0.8.0"
-# pylint: disable=protected-access, logging-format-interpolation, invalid-name, no-member
+# pylint: disable=protected-access, logging-format-interpolation, invalid-name, no-member, too-many-branches
-import os
+import os
-            except OSError as exc: 
+            except OSError as exc:
-      packages=['mxnet', 'mxnet.module'],
+      packages=['mxnet', 'mxnet.module', 'mxnet.notebook'],
-
+    networks = ['alexnet', 'vgg', 'inception-bn', 'inception-v3', 'resnet-50', 'resnet-152']
-        min_random_scale = 0.533, # assume input image has min size 480, 0.533 = 256/480
+        min_random_scale = 1, # if input image has min size k, suggest to use
-def train(network, data_pair, num_epoch, learning_rate, optimizer='sgd', opt_args=None):
+def train(network, data_pair, num_epoch, learning_rate, optimizer='sgd', opt_args=None, ctx=[mx.gpu(0)]):
-        ctx = [mx.gpu(0)],  # can be change to [mx.gpu(0), mx.gpu(1)] if there are 2 gpus
+        ctx = ctx,
-    model.fit(X = train, 
+    model.fit(X = train,
-                return
+            if not self.activated or not self.re_prog.match(py_str(name)):
-from common.util import download_file
+from common.util import download_file, get_gpus
-    batch_size = 32
+    assert len(gpus) > 0
-    print "Validation error:", ae_model.eval(val_X)
+    print("Training error:", ae_model.eval(train_X))
-    for i in xrange(sample_num):
+    for i in range(sample_num):
-                print sample_test_regression(exe, X=X_test, Y=Y_test, sample_pool=sample_pool,
+                print("Current Iter Num: %d" % (i + 1), "Time Spent: %f" % (end - start), "MSE:",
-                                             save_path='regression_HMC.txt')
+                                             save_path='regression_HMC.txt'))
-    print 'accept ratio', accept_num / float(sample_num)
+    print('accept ratio', accept_num / float(sample_num))
-    for i in xrange(total_iter_num):
+    for i in range(total_iter_num):
-            print "Current Iter Num: %d" % (i + 1), "Time Spent: %f" % (end - start)
+            print("Current Iter Num: %d" % (i + 1), "Time Spent: %f" % (end - start))
-    for i in xrange(total_iter_num):
+    for i in range(total_iter_num):
-                print "Current Iter Num: %d" % (i + 1), "Time Spent: %f" % (end - start)
+                print("Current Iter Num: %d" % (i + 1), "Time Spent: %f" % (end - start))
-                print "Test %d/%d=%f" % (test_correct, test_total, test_acc)
+                print("Test %d/%d=%f" % (test_correct, test_total, test_acc))
-                print sample_test_regression(exe=exe, sample_pool=sample_pool,
+                print("Current Iter Num: %d" % (i + 1), "Time Spent: %f" % (end - start), "MSE:",
-                                             save_path='regression_SGLD.txt')
+                                             save_path='regression_SGLD.txt'))
-    for i in xrange(total_iter_num):
+    for i in range(total_iter_num):
-                print "Current Iter Num: %d" % (i + 1), "Time Spent: %f" % (end - start)
+                print("Current Iter Num: %d" % (i + 1), "Time Spent: %f" % (end - start))
-                print "Teacher: Test ACC %d/%d=%f, Train ACC %d/%d=%f" \
+                print("Student: Test ACC %d/%d=%f, Train ACC %d/%d=%f" % (test_correct, test_total,
-                         teacher_train_correct, teacher_train_total, teacher_train_acc)
+                         teacher_train_correct, teacher_train_total, teacher_train_acc))
-                print sample_test_regression(exe=student_exe, X=X_test, Y=Y_test,
+                print("Current Iter Num: %d" % (i + 1), "Time Spent: %f" % (end - start), "MSE:",
-                                             save_path='regression_DSGLD.txt')
+                                             save_path='regression_DSGLD.txt'))
-            print "Iter:%d, Time spent: %f" % (i + 1, end - start)
+            print("Iter:%d, Time spent: %f" % (i + 1, end - start))
-        print 'Downloading data from %s to %s' % (origin, data_path)
+        print('Downloading data from %s to %s' % (origin, data_path))
-        print 'Done!'
+        print('Done!')
-        for i in xrange(testing_data.shape[0]):
+        for i in range(testing_data.shape[0]):
-        for i in xrange(testing_data.shape[0]):
+        for i in range(testing_data.shape[0]):
-            for j in xrange(len(param_list)):
+            for j in range(len(param_list)):
-        print rvocab[np.argmax(prob, axis = 1)[k]]
+        print(rvocab[np.argmax(prob, axis = 1)[k]])
-    for l, n in len_dict.iteritems(): # TODO: There are better heuristic ways to do this    
+    for l, n in len_dict.items(): # TODO: There are better heuristic ways to do this
-        print buckets
+        print(buckets)
-            
+
-            
+
-
+from __future__ import print_function
-            print >> logs, 'reset learning rate to %g' % opt.lr
+            print('reset learning rate to %g' % opt.lr,file=logs)
-            print >> logs, 'Saved checkpoint to %s' % param_name
+            print('Saved checkpoint to %s' % param_name,file=logs)
-                --- Dev Accuracy thus far: %.3f' % (iteration, train_time, train_acc, dev_acc)
+        print('Iter [%d] Train: Time: %.3fs, Training Accuracy: %.3f \
-    print 'Loading data...'
+    print('Loading data...')
-    print 'dev shape:', x_dev.shape
+    print('Train/Dev split: %d/%d' % (len(y_train), len(y_dev)))
-    print 'embedding size', num_embed
+    print('sentence max words', sentence_size)
-    print 'vocab_size', vocab_size
+    print('Train/Dev split: %d/%d' % (len(y_train), len(y_dev)))
-    print 'embedding size', num_embed
+    print('batch size', batch_size)
-
+from __future__ import print_function
-    imagenet_args=['python',  'train_imagenet.py',  '--gpus', ','.join(str(i) for i in xrange(num_gpus)), \
+    imagenet_args=['python',  'train_imagenet.py',  '--gpus', ','.join(str(i) for i in range(num_gpus)), \
-        return self._current / self.batch_size
+        return self._current // self.batch_size
-                idx = (self._current + i + self._size / 2) % self._size
+                idx = (self._current + i + self._size // 2) % self._size
-            print 'Writing {} VOC results file'.format(cls)
+            print('Writing {} VOC results file'.format(cls))
-        print 'VOC07 metric? ' + ('Y' if use_07_metric else 'No')
+        print('VOC07 metric? ' + ('Y' if use_07_metric else 'No'))
-            raise ValueError, "classes should be list/tuple or text file"
+            raise ValueError("classes should be list/tuple or text file")
-    print "Saved symbol: {}-symbol.json".format(save_prefix)
+    print("Saved model: {}-{:04d}.param".format(save_prefix, args.epoch))
-                num_images, time_elapsed)
+            print("Detection time for {} images: {:.4f} sec".format(
-
+from __future__ import print_function
-import cPickle
+try:
-        print 'saving annotations cache to {:s}'.format(cache_file)
+                print('reading annotations for {:d}/{:d}'.format(ind + 1, len(image_filenames)))
-            cPickle.dump(recs, f)
+            pickle.dump(recs, f)
-            recs = cPickle.load(f)
+            recs = pickle.load(f)
-        raise NotImplementedError, "No support for dataset: " + dataset
+        raise NotImplementedError("No support for dataset: " + dataset)
-    print net.tojson()
+    print(net.tojson())
-        raise NotImplementedError, "Dataset " + dataset + " not supported"
+        raise NotImplementedError("Dataset " + dataset + " not supported")
-        batches_per_epoch = ((imdb.num_images - 1) / batch_size + 1) * resize_epoch
+        batches_per_epoch = ((imdb.num_images - 1) // batch_size + 1) * resize_epoch
-    iter_refactor = lr_refactor_epoch * imdb.num_images / train_iter.batch_size
+    iter_refactor = lr_refactor_epoch * imdb.num_images // train_iter.batch_size
-epoch_size = num_examples / batch_size
+epoch_size = num_examples // batch_size
-
+from __future__ import print_function
-print 'Accuracy:', model.score(test_iter)*100, '%'
+print('Accuracy:', model.score(test_iter)*100, '%')
-print x.asnumpy()
+print(x.asnumpy())
-print y.asnumpy()
+print(y.asnumpy())
-print x.asnumpy()
+print(x.asnumpy())
-print x.asnumpy()
+print(x.asnumpy())
-print mx.th.cdiv(x,y).asnumpy()
+print(mx.th.cdiv(x,y).asnumpy())
-        print 'iter'
+        print('iter')
-    print 'begin fit'
+    print('begin fit')
-        c = int(buf[i / 20])
+        c = int(buf[i // 20])
-    print 'begin fit'
+    print('begin fit')
-def pack_img(header, img, quality=80, img_fmt='.jpg'):
+def pack_img(header, img, quality=95, img_fmt='.jpg'):
-    rgroup.add_argument('--quality', type=int, default=80,
+    rgroup.add_argument('--quality', type=int, default=95,
-        help='Whether to also pack multi dimensional label in the record file') 
+        help='Whether to also pack multi dimensional label in the record file')
-def get_symbol(num_classes=10, **kwargs):
+def get_symbol(num_classes=10, add_stn=False, **kwargs):
-    'nvml.h', 'opencv2/opencv.hpp', 'sys/stat.h', 'sys/types.h', 'cuda.h', 'cuda_fp16.h'
+    'nvml.h', 'opencv2/opencv.hpp', 'sys/stat.h', 'sys/types.h', 'cuda.h', 'cuda_fp16.h',
-            if h not in blacklist and h not in sysheaders: sysheaders.append(h)
+            if (h not in blacklist and
-    symbol_string, output_name = proto2script(sys.argv[1])
+    symbol_string, output_name, input_dim = proto2script(sys.argv[1])
-    if height <= 28:            # such as cifar10
+    if height <= 32:            # such as cifar10
-    return pypandoc.convert(phase, 'rst', format='md')
+    return pypandoc.convert(phase, 'rst', format='md').replace('\n', ' ').replace('\r', '')
-                out += c + '\n'
+                out += convert_md_phase(c)+ '\n'
-    subprocess.call('cd ../recommonmark/; git pull', shell=True)
+def build_table(table):
-
+if not os.path.exists('../recommonmark'):
-                             'params':_base_model_url+'imagenet-11k/resnet-152/resnet-152-0000.params'}
+                             'params':_base_model_url+'imagenet-11k/resnet-152/resnet-152-0000.params'},
-                      self.soft_target_tau * self.policy.arg_dict[name]
+                arr[:] = (1.0 - self.soft_target_tau) * arr[:] + \
-                      self.soft_target_tau * self.qfunc.arg_dict[name]
+                arr[:] = (1.0 - self.soft_target_tau) * arr[:] + \
-import argparse
+import subprocess
-def test_imagenet1k_resnet(args):
+def test_imagenet1k_resnet(**kwargs):
-                         rgb_mean='0,0,0', metrics=acc, **vars(args))
+                         rgb_mean='0,0,0', metrics=acc, **kwargs)
-def test_imagenet1k_inception_bn(args):
+def test_imagenet1k_inception_bn(**kwargs):
-                     rgb_mean='123.68,116.779,103.939', metrics=acc, **vars(args))
+                     rgb_mean='123.68,116.779,103.939', metrics=acc, **kwargs)
-    args = parser.parse_args()
+    gpus = get_gpus()
-    test_imagenet1k_inception_bn(args)
+    test_imagenet1k_resnet(gpus=gpus, batch_size=batch_size)
-sys.path.insert(0, os.path.join(curr_path, "../../example/image-classification"))
+sys.path.insert(0, os.path.join(curr_path, "../../example/image-classification/symbol"))
-                        help='the depth of network, only valid for resnet')
+    parser.add_argument('--num-layers', type=int, default=152,
-                        help='input data shape')
+    parser.add_argument('--image-shape', type=str, default='3,224,224',
-    args = parse_args();
+def run(network, optimizer, gpus, kv_store, image_shape, disp_batches,
-        optimizer = None
+    devs = [mx.gpu(int(i)) for i in gpus.split(',')]
-        kv.set_optimizer(optimizer)
+        opt = mx.optimizer.Optimizer.create_optimizer(optimizer)
-    data_shape = tuple([int(s) for s in args.data_shape.split(',')])
+    symbol = import_module(network).get_symbol(image_shape=image_shape, **kwargs)
-    for b in range(0, args.num_batches+1):
+
-            if optimizer == None:
+        if test_results:
-            toc /= args.disp_batches
+        if b % disp_batches == 0:
-                    b, toc, size*2*(len(devs)-1)/len(devs)/toc/1e3, err))
+                    r.iter, r.time, r.bandwidth, r.error))
-    run()
+    args = parse_args();
-
+from past.builtins import xrange
-                \nThe network_name is a valid model defined as network_name.py in the image-classification/symbol folder.'
+                print('expected network attributes in format network_name:batch_size:image_size \
-                print e
+                print('expected network attributes in format network_name:batch_size:image_size \
-            per_unit = [(num_layers-2)/9]
+            per_unit = [(num_layers-2)//9]
-            per_unit = [(num_layers-2)/6]
+            per_unit = [(num_layers-2)//6]
-        print 'testing %s, acc = %f, speed = %f img/sec' % (m, r, speed)
+        print('testing %s, acc = %f, speed = %f img/sec' % (m, r, speed))
-    print 'Tested %s acc = %f, speed = %f img/sec' % (m, r, speed)
+    print('Tested %s acc = %f, speed = %f img/sec' % (m, r, speed))
-    i=max_count
+    i=1
-    while i >= 1:
+    while i <= max_count:
-    return s[::-1]
+        i=i*2
-                print 'exiting1'
+                print 'expected network attributes in format network_name:batch_size:image_size \
-                importlib.import_module('symbol_' + args[0]).get_symbol('1000')
+                importlib.import_module('symbol.'+ args[0])
-                print 'expected network attributes in format network_name:batch_size:image_size'
+                print 'expected network attributes in format network_name:batch_size:image_size \
-    parser.add_argument('--gpu_count', type=int, help='number of gpus on each worker to use', required=True)
+    parser.add_argument('--networks', dest='networks', nargs= '+', type=str, help= 'one or more networks in the format network_name:batch_size:image_size \
-                   '--data-shape', str(data_shape), '--num-epochs', '1' ,'--kv-store', kv_store, '--benchmark']
+                   '--image-shape', '3,' + str(data_shape) + ',' + str(data_shape), '--num-epochs', '1' ,'--kv-store', kv_store, '--benchmark', '1', '--disp-batches', '10']
-    sys.path.append(os.path.join(curr_path, "../../python"))
+    sys.path.append(os.path.join(curr_path, "../../../python"))
-    train_mnist.train_model.fit(args, symbol_generator, get_iterator(buckets))
+import argparse
-def get_symbol(num_classes = 1000):
+def get_symbol(num_classes, **kwargs):
-def get_symbol(num_classes = 1000):
+def get_symbol(num_classes = 1000, **kwargs):
-def get_symbol(num_classes=1000):
+def get_symbol(num_classes=1000, **kwargs):
-Haffner. "Gradient-based learning applied to document recognition."
+LeCun, Yann, Leon Bottou, Yoshua Bengio, and Patrick Haffner.
-def get_symbol(num_classes = 1000, add_stn=False):
+def get_symbol(num_classes=10, **kwargs):
-def get_symbol(num_classes = 1000):
+def get_symbol(num_classes, **kwargs):
-    return mx.symbol.SoftmaxOutput(data=fc, name='softmax')
+"""
-import mxnet as mx
+import os
-        part_index  = kv.rank)
+import logging
-    return (train, val)
+def download_cifar10():
-    train_model.fit(args, net, get_iterator)
+    fit.fit(args, sym, data.get_rec_iter)
-    return (train, val)
+import argparse
-    train_model.fit(args, net, get_iterator)
+if __name__ == '__main__':
-import mxnet as mx
+"""
-    os.chdir("..")
+import logging
-def get_loc(data, attr={'lr_mult':'0.01'}):
+def read_data(label, image):
-    when num-epoch >=15
+    download and read data into numpy
-    return loc
+    base_url = 'http://yann.lecun.com/exdb/mnist/'
-def get_mlp():
+def to4d(img):
-    multi-layer perceptron
+    reshape to 4D arrays
-    return mlp
+    return img.reshape(img.shape[0], 1, 28, 28).astype(np.float32)/255
-def get_lenet(add_stn=False):
+def get_mnist_iter(args, kv):
-    Proceedings of the IEEE (1998)
+    create data iterator with NDArrayIter
-            part_index  = kv.rank)
+    (train_lbl, train_img) = read_data(
-                        help='the gpus will be used, e.g "0,1,2,3"')
+if __name__ == '__main__':
-
+    fit.add_fit_args(parser)
-        net = get_lenet()
+    # load network
-    train_model.fit(args, net, get_iterator(data_shape))
+    fit.fit(args, sym, get_mnist_iter)
-    BatchNorm = mx.sym.CuDNNBatchNorm
+    BatchNorm = mx.sym.BatchNorm
-    if args.benchmark:
+    if hasattr(args, 'benchmark') and args.benchmark:
-            epoch_end_callback = checkpoint)
+            epoch_end_callback = checkpoint)
-                    choices = ['alexnet', 'vgg', 'googlenet', 'inception-bn', 'inception-bn-full', 'inception-v3'],
+                    choices = ['alexnet', 'vgg', 'googlenet', 'inception-bn',
-parser.add_argument('--data-dir', type=str, required=True,
+mutually_exclusive_parser_group.add_argument('--data-dir', type=str,
-                    help='the gpus will be used, e.g "0,1,2,3"')
+                    help='gpus to be used, e.g "0,1,2,3"')
-parser.add_argument('--log-file', type=str, 
+parser.add_argument('--log-file', type=str,
-train_model.fit(args, net, get_iterator)
+if args.benchmark:
-        epoch_end_callback = checkpoint)
+    if args.benchmark:
-    relu5_2 = mx.symbol.Activation(data=conv5_2, act_type="relu", name="conv1_2")
+    relu5_2 = mx.symbol.Activation(data=conv5_2, act_type="relu", name="relu5_2")
-kv = mx.kvstore.create(args.kv_store)
+def do_train(args, callback_args=None):
-        epoch_end_callback=checkpoint)
+import find_mxnet
-import math
+import math
-    """Calculate training speed in frequent
+    """Calculate and log training speed periodically.
-        calculation frequent
+        How many batches between calculations.
-                        batch_end_callback(batch_end_params)
+                    _multiple_callbacks(batch_end_callback, batch_end_params)
-                epoch_end_callback(epoch, symbol, arg_params, aux_params)
+        _multiple_callbacks(epoch_end_callback, epoch, symbol, arg_params, aux_params)
-                logger.info('Epoch[%d] Validation-%s=%f', epoch, name, value)
+                    _multiple_callbacks(eval_batch_end_callback, batch_end_params)
-                    batch_end_callback(batch_end_params)
+                _multiple_callbacks(batch_end_callback, batch_end_params)
-            work_load_list=None, monitor=None, eval_batch_end_callback=None):
+            work_load_list=None, monitor=None, eval_end_callback=LogValidationMetricsCallback(),
-# pylint: disable=too-many-arguments, too-many-locals, too-many-public-methods, too-many-branches
+# pylint: disable=fixme, too-many-arguments, too-many-locals, too-many-public-methods, too-many-branches
-    args, auxs = load_param(prefix, epoch, convert=True, ctx=ctx)
+    args, auxs, _ = load_param(prefix, epoch, convert=True, ctx=ctx)
-    args, auxs = load_param(pretrained, epoch, convert=True)
+    args, auxs, _ = load_param(pretrained, epoch, convert=True)
-    args, auxs = load_param(pretrained, epoch, convert=True)
+    args, auxs, _ = load_param(pretrained, epoch, convert=True)
-        img = cv2.imread(os.path.join(args.root, item[1]), args.color)
+        img = cv2.imread(fullpath, args.color)
-        print('imread error:', item[1])
+        traceback.print_exc()
-        print('read none error:', item[1])
+        print('imread read blank (None) image for file: %s' % fullpath)
-        print('pack_img error:', item[1], e)
+    except Exception:
-    def __init__(self, data, label=None, batch_size=1, shuffle=False, last_batch_handle='pad'):
+    def __init__(self, data, label=None, batch_size=1, shuffle=False,
-        self.label = _init_data(label, allow_empty=True, default_name='softmax_label')
+        self.label = _init_data(label, allow_empty=True, default_name=label_name)
-                print('Error in NDArrayOp.forward: ', str(e))
+            except Exception:
-                print('Error in NDArrayOp.backward: ', str(e))
+            except Exception:
-                print('Error in NDArrayOp.infer_shape: ', str(e))
+            except Exception:
-                print('Error in NDArrayOp.list_outputs: ', str(e))
+            except Exception:
-                print('Error in NDArrayOp.list_arguments: ', str(e))
+            except Exception:
-                print('Error in NDArrayOp.declare_backward_dependency: ', str(e))
+            except Exception:
-                    print('Error in %s.infer_shape: '%reg_name, str(e))
+                except Exception:
-                    print('Error in %s.list_outputs: '%reg_name, str(e))
+                except Exception:
-                    print('Error in %s.list_arguments: '%reg_name, str(e))
+                except Exception:
-                    print('Error in %s.list_auxiliary_states: '%reg_name, str(e))
+                except Exception:
-                    print('Error in %s.declare_backward_dependency: '%reg_name, str(e))
+                except Exception:
-                            print('Error in CustomOp.forward: ', str(e))
+                        except Exception:
-                            print('Error in CustomOp.backward: ', str(e))
+                        except Exception:
-                            print('Error in CustomOp.delete: ', str(e))
+                        except Exception:
-                    print('Error in %s.create_operator: '%reg_name, str(e))
+                except Exception:
-                    print('Error in CustomOpProp.delete: ', str(e))
+                except Exception:
-    """Generate normal(Gaussian) distribution N(mean, stdvar^2) with shape.
+    """Generate normal(Gaussian) distribution N(loc, scale^2) with shape.
-                print(e)
+                traceback.print_exc()
-from .base import check_call, ctypes2docstring
+from .base import check_call, ctypes2docstring, MXNetError
-        return self._infer_shape_impl(False, *args, **kwargs)
+        try:
-    or any class with "reset" and "read" methods) and combine them with
+    or any class with "reset" and "next" methods) and combine them with
-        one or more DataIters (or any class with "reset" and "read" methods)
+        one or more DataIters (or any class with "reset" and "next" methods)
-        if out_shapes == None:
+        if out_shapes is None:
-            line += str(fields[i])
+        for i, field in enumerate(fields):
-        node = nodes[i]
+    for i, node in enumerate(nodes):
-def plot_network(symbol, title="plot", save_format='pdf', shape=None, node_attrs={}):
+def plot_network(symbol, title="plot", save_format='pdf', shape=None, node_attrs={},
-        dict of shapes, str->shape (tuple), given input shapes
+        If supplied, the visualization will include the shape
-    heads = set(conf["heads"][0])  # TODO(xxx): check careful
+    def looks_like_weight(name):
-    for i, node in enumerate(nodes):
+    hidden_nodes = set()
-            else:
+            if looks_like_weight(node["name"]):
-    for i, node in enumerate(nodes):
+    for node in nodes:
-                if input_node["op"] != "null" or item[0] in heads:
+                if input_name not in hidden_nodes:
-
+def build_scala_docs(root_path):
-                assert name in self.grad_req, "argument %s not found in grad_req"%name
+        if not for_training:
-                    if grad_req[name] != 'null':
+                    if self.grad_req[name] != 'null':
-                    if grad_req[name] != 'null':
+                    if self.grad_req[name] != 'null':
-                if grad_req[name] != 'null':
+                if self.grad_req[name] != 'null':
-                                    grad_req=grad_req, shared_exec=shared_exec)
+                                    grad_req=self.grad_req, shared_exec=shared_exec)
-# pylint: disable=too-many-instance-attributes, too-many-arguments, protected-access
+# pylint: disable=too-many-instance-attributes, too-many-arguments, protected-access, too-many-branches
-                input_types[item[0]] = mx_real_t
+
-            raise ValueError('array shape do not match the shape of NDArray')
+            raise ValueError('Shape inconsistant: expected %s vs got %s'%(
-def check_consistency(sym, ctx_list, scale=1.0, grad_req='write'):
+def check_consistency(sym, ctx_list, scale=1.0, grad_req='write',
-            arr[:] = iarr.astype(arr.dtype)
+    if tol is None:
-    # forward
+    assert len(ctx_list) > 1
-        exe.backward(exe.outputs[0])
+        for name, arr in exe.arg_dict.items():
-    dtypes = [arr.dtype for arr in outputs]
+    dtypes = [np.dtype(exe.outputs[0].dtype) for exe in exe_list]
-    #forward predict
+    # test
-            arr2 = arr2.astype(dtypes[i])
+        for name, arr in zip(output_names, exe.outputs):
-                npt.assert_allclose(arr1, arr2, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
+                npt.assert_allclose(arr, gtarr, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
-    check_consistency(sym, ctx_list, grad_req={'embedding_data': 'null','embedding_weight': 'write'})
+    arg_params = {'embedding_data': np.random.randint(low=0, high=10, size=(2, 10))}
-        #print np.sum(posteriors[1][:])
+        posteriors = preds[0].asnumpy().astype('float32')
-        return [(n, x.shape) for n, x in zip(self.label_names, self.label)]
+        if len(self.label_names):
-            data_name='data', label_name='softmax_label', has_label=True, load_label_mean=True):
+            init_states, delay=5, feat_dim=40, label_dim=1955,
-        self.label_mean_sets=label_mean_sets
+        self.train_sets = train_sets
-            for i,v in enumerate(feats):
+            for i, v in enumerate(feats):
-                self.data[i_bucket].append((feats,tgts))
+                self.data[i_bucket].append((feats, tgts))
-        self.provide_label = []
+        self.provide_label = None
-                 has_label=True, do_shuffling=True, pad_zeros=False):
+                 has_label=True, do_shuffling=True, pad_zeros=False, time_major=False):
-            self.label = [mx.nd.zeros((batch_size, truncate_len))]
+        self.time_major = time_major
-        self.provide_label = []
+        self.provide_label = None
-        np_label_buffer = np.zeros((self.batch_size, self.truncate_len))
+        
-                        state[i:i+1] = 0.1
+                        if self.time_major:
-                    np_label_buffer[i][:n_take] = self.labels[idx][idx_take]
+                    if self.time_major:
-                        np_label_buffer[i][n_take:] = 0
+                        if self.time_major:
-
+            
-            data_batch = SimpleBatch(data_names, self.data + self.init_state_arrays,
+ 
-            buckets = [ i for i in range(1,max_bucket) ]
+            buckets = [i for i in range(1, max_bucket)]
-            for k, v in self.label
+            for k, v in self.data
-        urllib.urlretrieve("http://data.dmlc.ml/mxnet/data/cifar10.zip", zippath)
+        urllib.urlretrieve("http://data.mxnet.io/mxnet/data/cifar10.zip", zippath)
-        os.system("wget http://data.dmlc.ml/mxnet/data/data/cifar10.zip")
+        os.system("wget http://data.mxnet.io/mxnet/data/data/cifar10.zip")
-        os.system('wget http://data.dmlc.ml/mxnet/data/cifar10.zip')
+        os.system('wget http://data.mxnet.io/mxnet/data/cifar10.zip')
-        urllib.urlretrieve("http://data.dmlc.ml/mxnet/data/mnist.zip", zippath)
+        urllib.urlretrieve("http://data.mxnet.io/mxnet/data/mnist.zip", zippath)
-        urllib.urlretrieve("http://data.dmlc.ml/mxnet/data/cifar10.zip", zippath)
+        urllib.urlretrieve("http://data.mxnet.io/mxnet/data/cifar10.zip", zippath)
-        os.system("wget http://data.dmlc.ml/mxnet/data/mnist.zip -P data/")
+        os.system("wget http://data.mxnet.io/mxnet/data/mnist.zip -P data/")
-        os.system("wget http://data.dmlc.ml/mxnet/data/cifar10.zip -P data/")
+        os.system("wget http://data.mxnet.io/mxnet/data/cifar10.zip -P data/")
-    image_list = []
+    i = 0
-                    yield (len(image_list), os.path.relpath(fpath, root), cat[path])
+                    yield (i, os.path.relpath(fpath, root), cat[path])
-                yield (len(image_list), os.path.relpath(fpath, root), 0)
+                yield (i, os.path.relpath(fpath, root), 0)
-                           for k, v in train_data.provide_data + train_data.provide_label}
+            data_shapes = {}
-                                    shared_data_arrays=self.shared_data_arrays[i])
+                                    shared_data_arrays=self.shared_data_arrays[i],
-from collections import OrderedDict
+from collections import OrderedDict, namedtuple
-                       for r, i in zip(self.rename_data, self.iters)], [])
+            return sum([[
-                       for r, i in zip(self.rename_label, self.iters)], [])
+            return sum([[
-        return [(k, tuple([self.batch_size] + list(v.shape[1:]))) for k, v in self.data]
+        return [
-
+        return [
-        self.provide_label = [(label_name, label.shape)]
+        self.provide_data = [DataDesc(data_name, data.shape, data.dtype)]
-    def _init_predictor(self, input_shapes):
+    def _init_predictor(self, input_shapes, type_dict=None):
-            self.ctx[0], grad_req='null', **dict(input_shapes))
+            self.ctx[0], grad_req='null', type_dict=type_dict, **dict(input_shapes))
-        self._init_predictor(data_shapes)
+        type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
-        self._init_predictor(data_shapes)
+        type_dict = dict((key, value.dtype) for (key, value) in self.arg_params.items())
-            param_arrays = [nd.zeros(x[0].shape) for x in self._exec_group.param_arrays]
+            param_arrays = [
-            aux_arrays = [nd.zeros(x[0].shape) for x in self._exec_group.aux_arrays]
+            aux_arrays = [
-                                                     grad_req=grad_req)
+                                                     grad_req=grad_req, input_types=input_types)
-        # when offers mean_img, each image will substract the mean value at each pixel
+        # when offers mean_img, each image will subtract the mean value at each pixel
-    substract pixel size and transform to correct format
+    subtract pixel size and transform to correct format
-    substract pixel size and transform to correct format
+    subtract pixel size and transform to correct format
-        substract mean from decode image before outputing.
+        subtract mean from decode image before outputing.
-                  [(2, 3, 5, 5), (0, 0, 0, 0), True, (2, 3, 5, 5)]]
+    test_cases = [
-        os.system("mv cifar/* .; rm -rf cifar; rm cifar10.zip")
+        import urllib, zipfile, glob
-        _download(args.data_dir)
+        _download(data_dir)
-        mean_img    = os.path.join(args.data_dir, "mean.bin"),
+        path_imgrec = os.path.join(data_dir, "train.rec"),
-        mean_img    = os.path.join(args.data_dir, "mean.bin"),
+        path_imgrec = os.path.join(data_dir, "test.rec"),
-        os.system("unzip -u mnist.zip; rm mnist.zip")
+        import urllib, zipfile
-            _download(args.data_dir)
+            _download(data_dir)
-            os.system("mv cifar/* .; rm -rf cifar; rm cifar10.zip")
+        import urllib, zipfile, glob
-        _download(args.data_dir)
+        _download(data_dir)
-        mean_img    = args.data_dir + "mean.bin",
+        path_imgrec = data_dir + "train.rec",
-        mean_img    = args.data_dir + "mean.bin",
+        path_imgrec = data_dir + "test.rec",
-    
+
-    test_instance_normalization()
+    test_instance_normalization()
-            kwargs = dict([(keys[i], vals[i]) for i in range(argc)])
+            kwargs = dict([(py_str(keys[i]), py_str(vals[i])) for i in range(argc)])
-    Calculated by :math:`\\frac{|a-b|^2}{|a|^2 + |b|^2}`
+    Calculated by :math:`\\frac{|a-b|_1}{|a|_1 + |b|_1}`
-    return reldiff(a, b) <= threshold
+    rel = reldiff(a, b)
-    aux_states : None or list of numpy.ndarray or dict of str to numpy.ndarray
+    aux_states : None or list of numpy.ndarray or dict of str to numpy.ndarray, optional
-
+    use_forward_train : bool, optional
-    approx_grads = {k:np.zeros(v.shape, dtype=np.float32) for k, v in location.items()}
+    approx_grads = {k: np.zeros(v.shape, dtype=np.float32)
-                executor.arg_dict[key][:] = val
+            v.ravel()[i] += eps
-
+            v.ravel()[i] = old_value.ravel()[i]
-    check_slice_channel(2, 16)
+    def check_slice_channel(data_ndim, axis, num_outputs, squeeze_axis):
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-3, check_eps=0.16)
+        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, check_eps=0.16)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-3, check_eps=0.16)
+        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-2, check_eps=0.16)
-        assert err_1 < 1e-5 and err_2 < 1e-5, 'lhs error %f, rhs error %f, shapes are %s %s' % (
+        assert err_1 < 1e-3 and err_2 < 1e-3, 'lhs error %f, rhs error %f, shapes are %s %s' % (
-                workspace=workspace_default, name="bigscore")
+    bigscore = mx.symbol.Deconvolution(data=input, kernel=kernel, stride=stride, adj=(stride[0]-1, stride[1]-1),
-                workspace=workspace_default, name="score2")  # 2X
+                 adj=(1, 1), workspace=workspace_default, name="score2")  # 2X
-                workspace=workspace_default, name="score_pool4")
+                 workspace=workspace_default, name="score_pool4")
-                workspace=workspace_default, name="score2")  # 2X
+                adj=(1, 1), workspace=workspace_default, name="score2")  # 2X
-                workspace=workspace_default, name="score4") # 4X
+                adj=(1, 1), workspace=workspace_default, name="score4") # 4X
-              end2end=False):
+def test_rcnn(imageset, year, root_path, devkit_path, prefix, epoch, ctx, vis=False, has_rpn=True, proposal='rpn'):
-                                     ])
+                                     "ph2h_weight",
-    in_gate = mx.sym.Activation(slice_gates[0], act_type="sigmoid")
+
-    out_gate = mx.sym.Activation(slice_gates[3], act_type="sigmoid")
+
-                                     ph2h_weight = mx.sym.Variable("l%d_ph2h_weight" % i)
+                                     ph2h_weight = mx.sym.Variable("l%d_ph2h_weight" % i),
-    
+    pred = mx.sym.Reshape(pred, shape=(-1, num_label))
-    preds = preds.reshape((-1, preds.shape[2]))
+    preds = preds.reshape((-1, preds.shape[1]))
-    preds = preds.reshape((-1, preds.shape[2]))
+    preds = preds.reshape((-1, preds.shape[1]))
-            module.init_optimizer(kvstore='local',
+            module.init_optimizer(kvstore='device',
-            module.init_optimizer(kvstore='local',
+            module.init_optimizer(kvstore='device',
-                    lr_scheduler.effective_sample_count = data_batch.effective_sample_count
+                    lr_scheduler.effective_sample_count = 1#data_batch.effective_sample_count
-            input_types = {}
+            input_types = {k: mx_real_t for k in input_shapes.keys()}
-def Variable(name, attr=None, shape=None, dtype=None):
+def Variable(name, attr=None, shape=None):
-        mean_img    = args.data_dir + "mean.bin",
+        path_imgrec = os.path.join(args.data_dir, "train.rec"),
-        mean_img    = args.data_dir + "mean.bin",
+        path_imgrec = os.path.join(args.data_dir, "test.rec"),
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-                    force_rebind=False, shared_module=None)
+                    force_rebind=False, shared_module=None, grad_req=grad_req)
-from ..base import mx_real_t
+    grad_req : str, list of str, dict of str to str
-                 logger=logging, fixed_param_names=None, layout_mapper=None):
+                 logger=logging, fixed_param_names=None, layout_mapper=None, grad_req='write'):
-            _load_label(data_batch, self.label_arrays, self.label_layouts)
+            assert not is_train or data_batch.label
-            input_types = {k: mx_real_t for k in input_shapes.keys()}
+            input_types = {}
-                    grad_req[name] = 'write'
+                    grad_req[name] = self.grad_req[name]
-                    grad_req[name] = 'write' if self.inputs_need_grad else 'null'
+                    grad_req[name] = self.grad_req[name] if self.inputs_need_grad else 'null'
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-                                                     layout_mapper=self.layout_mapper)
+                                                     layout_mapper=self.layout_mapper,
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-                        force_rebind=force_rebind, shared_module=None)
+                        force_rebind=force_rebind, shared_module=None, grad_req=grad_req)
-def Variable(name, attr=None, shape=None):
+def Variable(name, attr=None, shape=None, dtype=None):
-    save_dict.update({('aux:%s' % k) : v for k, v in aux_params.items()})
+    save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}
-        save_dict.update({('aux:%s' % k) : v for k, v in aux_params.items()})
+        save_dict = {('arg:%s' % k) : v.as_in_context(cpu()) for k, v in arg_params.items()}
-from .context import cpu, gpu
+from .context import cpu, gpu, Context
-    return cpu()
+    return Context.default_ctx
-                           grad_nodes=None, use_forward_train=True, ctx=mx.cpu()):
+                           grad_nodes=None, use_forward_train=True, ctx=None):
-def check_symbolic_forward(sym, location, expected, check_eps=1E-4, aux_states=None, ctx=mx.cpu()):
+def check_symbolic_forward(sym, location, expected, check_eps=1E-4, aux_states=None, ctx=None):
-                            aux_states=None, grad_req='write', ctx=mx.cpu()):
+                            aux_states=None, grad_req='write', ctx=None):
-def check_speed(sym, location=None, ctx=mx.cpu(), N=20, grad_req=None, typ="whole",
+def check_speed(sym, location=None, ctx=None, N=20, grad_req=None, typ="whole",
-from mxnet.test_utils import check_consistency
+from mxnet.test_utils import check_consistency, set_default_context
-    exec1 = out.bind(mx.Context('cpu'),
+    exec1 = out.bind(default_context(),
-    exe = op.bind(mx.cpu(), args=[e_nd], args_grad=grad_nd)
+    exe = op.bind(default_context(), args=[e_nd], args_grad=grad_nd)
-    exec1 = out.bind(mx.Context('cpu'),
+    exec1 = out.bind(default_context(),
-    arr_label = mx.random.uniform(0, 1, shape[0])
+    arr_data = mx.random.uniform(-1, 1, shape, ctx=mx.cpu()).copyto(default_context())
-    exec1 = out.bind(mx.cpu(),
+    exec1 = out.bind(default_context(),
-    l = mx.random.uniform(-1, 1, shape, ctx = xpu)
+    x = mx.random.uniform(-1, 1, shape, ctx=mx.cpu()).copyto(xpu)
-    assert_allclose(out, np_softmax(x.asnumpy()))
+    assert_allclose(out, np_softmax(x.asnumpy()), rtol=1e-4)
-    assert_allclose(grad.asnumpy(), np_softmax(x.asnumpy()) - l.asnumpy())
+    assert_allclose(grad.asnumpy(), np_softmax(x.asnumpy()) - l.asnumpy(), rtol=1e-4)
-    check_softmax_with_shape((3, 4, 2), mx.cpu(), preserve_shape=True)
+    check_softmax_with_shape((3, 4), default_context(), preserve_shape=False)
-    x = mx.random.uniform(-1, 1, shape, ctx = xpu)
+    x = mx.random.uniform(-1, 1, shape, ctx=mx.cpu()).copyto(xpu)
-    exec1 = s.bind(mx.cpu(), args=[x], args_grad = {'X': dx})
+    exec1 = s.bind(default_context(), args=[x], args_grad = {'X': dx})
-    exe_c = swap.bind(mx.cpu(), args=[arr_data])
+    exe_c = swap.bind(default_context(), args=[arr_data])
-    exe_test = embed.simple_bind(mx.cpu(), grad_req={'data': 'null', 'embed_weight': 'write'}, data=(batch,))
+    exe_test = embed.simple_bind(default_context(), grad_req={'data': 'null', 'embed_weight': 'write'}, data=(batch,))
-    exe_square = square.bind(mx.cpu(), args=[arr_data], args_grad=[arr_grad])
+    exe_square = square.bind(default_context(), args=[arr_data], args_grad=[arr_grad])
-    exe_test = test.bind(mx.cpu(), args=[arr_data], args_grad=[arr_grad])
+    exe_test = test.bind(default_context(), args=[arr_data], args_grad=[arr_grad])
-    exe_test = test.bind(mx.cpu(), args=[arr_data])
+    exe_test = test.bind(default_context(), args=[arr_data])
-    exe_test = test.bind(mx.cpu(), args=[arr_data], args_grad=[arr_grad])
+    exe_test = test.bind(default_context(), args=[arr_data], args_grad=[arr_grad])
-    exe_test = test.bind(mx.cpu(), args=[arr_data1,arr_data2], args_grad=[arr_grad1,arr_grad2])
+    exe_test = test.bind(default_context(), args=[arr_data1,arr_data2], args_grad=[arr_grad1,arr_grad2])
-    exe_test = test.bind(mx.cpu(), args=[arr_data1], args_grad=[arr_grad1])
+    exe_test = test.bind(default_context(), args=[arr_data1], args_grad=[arr_grad1])
-    exe_test = test.bind(mx.cpu(), args=[arr_data], args_grad=[arr_grad])
+    exe_test = test.bind(default_context(), args=[arr_data], args_grad=[arr_grad])
-    input_data = mx.random.uniform(-5, 5, input_shape)
+    input_data = mx.random.uniform(-5, 5, input_shape, ctx=mx.cpu()).copyto(default_context())
-        (num_filter, input_shape[1]) + kernel)
+        (num_filter, input_shape[1]) + kernel, ctx=mx.cpu()).copyto(default_context())
-    exe = deconv.bind(mx.cpu(), args=args, args_grad=args_grad)
+    exe = deconv.bind(default_context(), args=args, args_grad=args_grad)
-    conv_data = mx.random.uniform(-5, 5, input_shape)
+    conv_data = mx.random.uniform(-5, 5, input_shape, ctx=mx.cpu()).copyto(default_context())
-        mx.random.normal(0, 1,(num_filter, input_shape[1]) + kernel)
+        mx.random.normal(0, 1,(num_filter, input_shape[1]) + kernel, ctx=mx.cpu()).copyto(default_context())
-    conv_out_grad = mx.random.normal(0, 2, exe_conv.outputs[0].shape)
+    exe_conv = conv.bind(default_context(), args=conv_args, args_grad=conv_args_grad)
-    exe_deconv = deconv.bind(mx.cpu(), args=deconv_args, args_grad=deconv_args_grad)
+    exe_deconv = deconv.bind(default_context(), args=deconv_args, args_grad=deconv_args_grad)
-    arr = {'arg_%d'%i: mx.random.uniform(-10.0, 10.0, shape) for i, shape in zip(range(len(shapes)), shapes)}
+    arr = {'arg_%d'%i: mx.random.uniform(-10.0, 10.0, shape, ctx=mx.cpu()).copyto(default_context()) for i, shape in zip(range(len(shapes)), shapes)}
-    exe = up.bind(mx.cpu(), args=arr, args_grad=arr_grad)
+    exe = up.bind(default_context(), args=arr, args_grad=arr_grad)
-    exe2 = y2.simple_bind(mx.cpu(), x=shape, w=(num_filter, shape[1]//num_group, kernel[0], kernel[1]), b=(num_filter,))
+    exe1 = y1.simple_bind(default_context(), x=shape)
-        y = symbol.bind(mx.cpu(), args={'a': mx.nd.array(d[0]), 'b' : mx.nd.array(d[1])})
+        y = symbol.bind(default_context(), args={'a': mx.nd.array(d[0]), 'b' : mx.nd.array(d[1])})
-        y = symbol.bind(mx.cpu(), args={'a': mx.nd.array(d[0]), 'b' : mx.nd.array(d[1])},
+        y = symbol.bind(default_context(), args={'a': mx.nd.array(d[0]), 'b' : mx.nd.array(d[1])},
-    be = net.bind(mx.cpu(), args={ 'input' : spike_img, 'test_convolution_weight' : kernel_weights},
+    be = net.bind(default_context(), args={ 'input' : spike_img, 'test_convolution_weight' : kernel_weights},
-    be = net.bind(mx.cpu(), args={ 'input' : input_grad, 'test_convolution_weight' : kernel_weights})
+    be = net.bind(default_context(), args={ 'input' : input_grad, 'test_convolution_weight' : kernel_weights})
-    be = net.bind(mx.cpu(), args={ 'input' : white_in, 'test_convolution_weight' : rnd_kernel},
+    be = net.bind(default_context(), args={ 'input' : white_in, 'test_convolution_weight' : rnd_kernel},
-    be = net.bind(mx.cpu(), args={ 'input' : white_in, 'test_convolution_weight' : dkernel})
+    be = net.bind(default_context(), args={ 'input' : white_in, 'test_convolution_weight' : dkernel})
-        exe = net.simple_bind(mx.cpu(), data=src_shape)
+        exe = net.simple_bind(default_context(), data=src_shape)
-            net = b.bind(mx.cpu(), args={'a': mx.nd.array(dat_npy)},
+            net = b.bind(default_context(), args={'a': mx.nd.array(dat_npy)},
-            net = sym_bcast.bind(mx.cpu(), args={'a': mx.nd.array(dat_npy)},
+            net = sym_bcast.bind(default_context(), args={'a': mx.nd.array(dat_npy)},
-            exec1 = Y.bind(mx.cpu(), args = [x], args_grad = {'X': xgrad})
+            exec1 = Y.bind(default_context(), args = [x], args_grad = {'X': xgrad})
-                    dev = mx.cpu()
+                    dev = default_context()
-                    args['data'] = mx.random.normal(0, 1, data_shape, dev)
+                    args['data'] = mx.random.normal(0, 1, data_shape, ctx=mx.cpu()).copyto(dev)
-def test_dot(ctx=mx.cpu()):
+def test_dot(ctx=default_context()):
-def test_batch_dot(ctx=mx.cpu()):
+def test_batch_dot(ctx=default_context()):
-    exe1 = net1.simple_bind(mx.cpu(),img1=img1.shape,img2=img1.shape)
+    exe1 = net1.simple_bind(default_context(),img1=img1.shape,img2=img1.shape)
-    out_grad1 = mx.nd.array(a,mx.cpu())
+    out_grad1 = mx.nd.array(a,default_context())
-    xpu = mx.cpu()
+    xpu = default_context()
-    xpu = mx.cpu()
+    xpu = default_context()
-    check_softmax_with_ignore_label(mx.cpu())
+    check_softmax_with_ignore_label(default_context())
-def simple_forward(sym, ctx=None, **inputs):
+def simple_forward(sym, ctx=None, is_train=False, **inputs):
-    exe.forward()
+    exe.forward(is_train=is_train)
-        layout_mapper = train_data.layout_mapper if hasattr(train_data, 'layout_mapper') else None
+        if hasattr(train_data, 'layout_mapper'):
-                  for_training=True, force_rebind=force_rebind, layout_mapper=layout_mapper)
+                  for_training=True, force_rebind=force_rebind)
-             layout_mapper=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None):
-             layout_mapper=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None):
-            (time-major? batch-major?).
+        module.layout_mapper = self.layout_mapper
-                    force_rebind=False, shared_module=None, layout_mapper=layout_mapper)
+                    force_rebind=False, shared_module=None)
-
+            module.layout_mapper = self.layout_mapper
-                        layout_mapper=self._layout_mapper)
+                        force_rebind=False, shared_module=self._curr_module)
-             layout_mapper=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None):
-                                                     layout_mapper=layout_mapper)
+                                                     layout_mapper=self.layout_mapper)
-             layout_mapper=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None):
-             layout_mapper=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None):
-                        force_rebind=force_rebind, shared_module=None, layout_mapper=layout_mapper)
+                        force_rebind=force_rebind, shared_module=None)
-                                                              self.contexts[i]))
+                    # pylint: disable=no-member
-                                                            islice.stop, label.context))
+                    # pylint: disable=no-member
-                    d_src.copy_slice_to(axis, slice_idx.start, slice_idx.stop, d_dst)
+                    # copy slice
-        """Set ndarray value"""
+        """Set ndarray value.
-            raise TypeError('type %s not supported' % str(type(value)))
+        if isinstance(in_slice, slice):
-                ctx=arrays[0].context, dtype=dtype)
+    ret_shape = shape_rest1 + (shape_axis,) + shape_rest2
-        ret.assign_slice_from(axis, idx, idx+arr.shape[axis], arr)
+        if axis == 0:
-    assert same(arr.asnumpy()[:, 2:4, :, :], sub_arr.asnumpy())
+def test_ndarray_crop():
-    assert not same(arr.asnumpy()[:, 2:4, :, :], sub_arr.asnumpy())
+    # crop assign
-    assert same(arr.asnumpy()[:, 2:4, :, :], sub_arr.asnumpy())
+    # crop assign with scalar
-    test_ndarray_slice_along_axis()
+        # TODO: check epoch_size for 'dist_sync'
-        if num_update > self.count + self.step:
+        # NOTE: use while rather than if  (for continuing training via load_epoch)
-        if self.cur_step_ind <= len(self.step)-1:
+        # NOTE: use while rather than if  (for continuing training via load_epoch)
-            for s in shapes]
+    arrays = [np.random.randn(*s).astype(default_dtype())
-    
+
-        if name in globs:
+        if name in globs and globs[name] is not getattr(module, name):
-    globs = {'mxnet': mxnet, 'test_utils': mxnet.test_utils}
+    globs = {'numpy': numpy, 'mxnet': mxnet, 'test_utils': mxnet.test_utils}
-"""Extra symbol documents"""
+"""Extra symbol documents
-    """The basic class"""
+    """The base class for attaching doc to operators."""
-     [[ 3.  4.  5.  3.  4.  5.]]]
+    Concat two (or more) inputs along a specific dimension:
-    """add with broadcast
+class BroadcastPlusDoc(SymbolDoc):
-    >>> x = c.bind(dev, args={'a': mx.nd.ones((2,2)), 'b' : mx.nd.ones((2,2))})
+    >>> a = Variable('a')
-    >>> x = c.bind(dev, args={'a': mx.nd.ones((2,2)), 'b' : mx.nd.ones((1,1))})
+
-    >>> x = c.bind(dev, args={'a': mx.nd.ones((2,1)), 'b' : mx.nd.ones((1,2))})
+
-    >>> x = c.bind(dev, args={'a': mx.nd.ones((1,2)), 'b' : mx.nd.ones((2,1))})
+
-# pylint: disable=invalid-name, no-member, too-many-arguments, too-many-locals, too-many-branches, too-many-statements, broad-except, line-too-long
+# pylint: disable=invalid-name, no-member, too-many-arguments, too-many-locals, too-many-branches, too-many-statements, broad-except, line-too-long, unused-import
-        annopath = os.path.join(self.data_path, 'Annotations', '{:s}.xml')
+        annopath = os.path.join(self.data_path, 'Annotations', '{0!s}.xml')
-                self.symbol.infer_shape(data=self.arg_params['data'].shape, rois=self.arg_params['rois'].shape)
+            # fill in label and aux
-        self.executor.forward(is_train=False)
+            # execute
-        bbox_deltas = output_dict['bbox_pred_reshape_output'].asnumpy()[0]
+        scores = executor.output_dict['cls_prob_reshape_output'].asnumpy()[0]
-            rois = output_dict['rois_output'].asnumpy()
+            rois = executor.output_dict['rois_output'].asnumpy()
-    group = mx.symbol.Group([rois[1], rpn_cls_loss, rpn_bbox_loss, cls_prob, bbox_pred])  # rois[1] is used for evaluation
+    group = mx.symbol.Group([mx.sym.BlockGrad(rois[1]), rpn_cls_loss, rpn_bbox_loss, cls_prob, bbox_pred])  # rois[1] is used for evaluation
-                                        else mx.lr_scheduler.FactorScheduler(factor_step, 0.1),
+                        'lr_scheduler': mx.lr_scheduler.FactorScheduler(factor_step, 0.1),
-        executor : mxnet.Executor
+        executor : Executor
-        label="../image-classification/mnist/train-labels-idx1-ubyte",
+        image=os.path.join(basedir, "../image-classification/mnist/train-images-idx3-ubyte"),
-        label="../image-classification/mnist/t10k-labels-idx1-ubyte",
+        image=os.path.join(basedir, "../image-classification/mnist/t10k-images-idx3-ubyte"),
-demo_data_model_parallelism = False
+demo_data_model_parallelism = True
-        label="../image-classification/mnist/train-labels-idx1-ubyte",
+        image=os.path.join(basedir, "../image-classification/mnist/train-images-idx3-ubyte"),
-        label="../image-classification/mnist/t10k-labels-idx1-ubyte",
+        image=os.path.join(basedir, "../image-classification/mnist/t10k-images-idx3-ubyte"),
-        os.system("mv cifar/* .; rm -rf cifar; rm cifar10.zip")
+            os.system("wget http://data.dmlc.ml/mxnet/data/cifar10.zip")
-                  for_training=True, force_rebind=force_rebind)
+                  for_training=True, force_rebind=force_rebind, layout_mapper=layout_mapper)
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-                    force_rebind=False, shared_module=None)
+                    force_rebind=False, shared_module=None, layout_mapper=layout_mapper)
-                        force_rebind=False, shared_module=self._curr_module)
+                        force_rebind=False, shared_module=self._curr_module,
-from ..executor_manager import _split_input_slice, _load_data, _load_label
+from ..executor_manager import _split_input_slice
-def _merge_multi_context(outputs):
+
-    return outputs
+    rets = []
-                 logger=logging, fixed_param_names=None):
+                 logger=logging, fixed_param_names=None, layout_mapper=None):
-        self.decide_slices(data_shapes)
+        self.layout_mapper = layout_mapper or DefaultLayoutMapper()
-            list of (name, shape) specifying the shapes for the input data.
+            list of (name, shape) specifying the shapes for the input data or label.
-            assert shape[1][0] == self.batch_size, "all the data must have the same batch size"
+        major_axis = [self.layout_mapper.get_batch_axis(name)
-        self.slices = _split_input_slice(self.batch_size, self.workload)
+        return major_axis
-        _load_data(data_batch, self.data_arrays)
+        _load_data(data_batch, self.data_arrays, self.data_layouts)
-                _load_label(data_batch, self.label_arrays)
+        if self.label_arrays is not None:
-        return zip(self.symbol.list_outputs(), shapes)
+
-            outputs = _merge_multi_context(outputs)
+            outputs = _merge_multi_context(outputs, self.output_layouts)
-            return _merge_multi_context(self.input_grad_arrays)
+            return _merge_multi_context(self.input_grad_arrays, self.data_layouts)
-                               for grad in out_grads]
+            out_grads_slice = []
-            labels_slice = [label[islice] for label in labels]
+            labels_slice = []
-        data_shapes = self._sliced_shape(data_shapes, i)
+        data_shapes = self._sliced_shape(data_shapes, i, self.data_layouts)
-            label_shapes = self._sliced_shape(label_shapes, i)
+            label_shapes = self._sliced_shape(label_shapes, i, self.label_layouts)
-    def _sliced_shape(self, shapes, i):
+    def _sliced_shape(self, shapes, i, major_axis):
-                for k, v in shapes]
+        sliced_shapes = []
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-                                                     fixed_param_names=self._fixed_param_names)
+                                                     fixed_param_names=self._fixed_param_names,
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-             inputs_need_grad=False, force_rebind=False, shared_module=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None,
-                        force_rebind=force_rebind, shared_module=None)
+                        force_rebind=force_rebind, shared_module=None, layout_mapper=layout_mapper)
-    def _copy_slice_to(self, axis, start, stop, target):
+    def copy_slice_to(self, axis, start, stop, target):
-def concatenate(arrays, always_copy=True):
+def concatenate(arrays, axis=0, always_copy=True):
-    shape_rest = arrays[0].shape[1:]
+    shape_axis = arrays[0].shape[axis]
-        assert shape_rest == arr.shape[1:]
+        shape_axis += arr.shape[axis]
-    ret = empty((shape0,) + shape_rest, ctx=arrays[0].context, dtype=dtype)
+    ret = empty(shape_rest1 + (shape_axis,) + shape_rest2,
-        idx += arr.shape[0]
+        ret.assign_slice_from(axis, idx, idx+arr.shape[axis], arr)
-    arr._copy_slice_to(1, 1, 3, sub_arr)
+    arr.copy_slice_to(1, 2, 4, sub_arr)
-    assert same(arr.asnumpy()[:, 1:3, :, :], sub_arr.asnumpy())
+    assert same(arr.asnumpy()[:, 2:4, :, :], sub_arr.asnumpy())
-    assert not same(arr.asnumpy()[:, 1:3, :, :], sub_arr.asnumpy())
+    assert not same(arr.asnumpy()[:, 2:4, :, :], sub_arr.asnumpy())
-    x = x - np.max(x, axis=1).reshape(x.shape[0], 1)
+    # fix for old numpy on Travis not supporting keepdims
-    x /= np.sum(x, axis=1).reshape(x.shape[0], 1)
+    # x /= np.sum(x, axis=-1, keepdims=True)
-def check_softmax_with_shape(shape, xpu):
+def check_softmax_with_shape(shape, xpu, preserve_shape=False):
-    Y = mx.symbol.SoftmaxOutput(data=X, label=L)
+    Y = mx.symbol.SoftmaxOutput(data=X, label=L, preserve_shape=preserve_shape)
-    check_softmax_with_shape((3, 4), mx.cpu())
+    check_softmax_with_shape((3, 4), mx.cpu(), preserve_shape=False)
-    
+
-    
+
-    
+
-    
+
-    
+
-                
+
-                
+
-                    
+
-                    # location in data2 
+
-                    
+
-    # compute output's dimension 
+
-    
+
-    
+
-                
+
-                
+
-                    
+
-                    # location in data2 
+
-                    
+
-    
+
-    
+
-    exe1.forward()  
+    exe1.forward()
-    # out_grad 
+
-    # backward error 
+    # backward error
-    
+
-    
+
-    
+
-    
+
-    bn = mx.symbol.BatchNorm(data=conv, name='bn_%s%s' %(name, suffix))
+    bn = mx.symbol.BatchNorm(data=conv, fix_gamma=fix_gamma, eps=eps, momentum=bn_mom, name='bn_%s%s' %(name, suffix))
-    pool1 = mx.symbol.Pooling(data=conv1, kernel=(3, 3), stride=(2, 2), name='pool1', pool_type='max')
+    conv1 = ConvFactory(data=data, num_filter=64, kernel=(7, 7), stride=(2, 2), pad=(3, 3), name='1')
-    pool2 = mx.symbol.Pooling(data=conv2, kernel=(3, 3), stride=(2, 2), name='pool2', pool_type='max')
+    conv2red = ConvFactory(data=pool1, num_filter=64, kernel=(1, 1), stride=(1, 1), name='2_red')
-from rcnn.tester import vis_all_detection
+from rcnn.tester import vis_all_detection, save_all_detection
-    sym = get_vgg_test()
+    args, auxs, num_class = load_param(prefix, epoch, convert=True, ctx=ctx)
-def demo_net(detector, image_name):
+def demo_net(detector, image_name, vis=False):
-    vis_all_detection(im_array, boxes_this_image, CLASSES, 0)
+    if vis:
-def bbox_pred(boxes, box_deltas):
+def bbox_pred(boxes, box_deltas, is_train=False):
-
+    if is_train:
-def transform(im, pixel_means):
+def transform(im, pixel_means, need_mean=False):
-    im -= pixel_means
+    if need_mean:
-
+config.TRAIN.BBOX_MEANS_INV = (0.0, 0.0, 0.0, 0.0)
-                inds = np.reshape(inds[row_perm, :], (-1, ))
+                if inds.shape[0] % 2:
-                 feat_stride=16, anchor_scales=(8, 16, 32), anchor_ratios=(0.5, 1, 2), allowed_border=0):
+                 feat_stride=16, anchor_scales=(8, 16, 32), anchor_ratios=(0.5, 1, 2), allowed_border=0, need_mean=True):
-            return [('data', self.data[0].shape)]
+            provide_data_ = [('data', self.data[0].shape)]
-                    ('bbox_outside_weight', self.label[3].shape)]
+            provide_label_ = [('label', self.label[0].shape),
-                inds = np.reshape(inds[row_perm, :], (-1, ))
+                if inds.shape[0] % 2:
-            self.data, self.label = minibatch.get_minibatch(roidb, self.num_classes, self.mode)
+            self.data, self.label = minibatch.get_minibatch(roidb, self.num_classes, self.mode, need_mean=self.need_mean)
-                data, label = minibatch.get_minibatch(iroidb, self.num_classes, self.mode)
+                data, label = minibatch.get_minibatch(iroidb, self.num_classes, self.mode, need_mean=self.need_mean)
-                del data['im_info']
+                # del data['im_info']
-        super(AccuracyMetric, self).__init__('Accuracy')
+    def __init__(self, use_ignore=False, ignore=None, ex_rpn=False):
-        self.has_rpn = config.TRAIN.HAS_RPN
+        self.ex_rpn = ex_rpn  # used in end2end joint training, export rpn loss
-            label = labels[0].asnumpy().reshape(-1,).astype('int32')
+            if config.END2END != 1:
-        super(LogLossMetric, self).__init__('LogLoss')
+    def __init__(self, use_ignore=False, ignore=None, ex_rpn=False):
-        self.has_rpn = config.TRAIN.HAS_RPN
+        self.ex_rpn = ex_rpn
-            cls = pred_cls[np.arange(label.shape[0]), label]
+            if config.END2END != 1:
-        self.has_rpn = config.TRAIN.HAS_RPN
+    def __init__(self, ex_rpn=False):
-            bbox_loss = bbox_loss.reshape(first_dim, -1)
+            if config.END2END != 1:
-def get_minibatch(roidb, num_classes, mode='test'):
+def get_minibatch(roidb, num_classes, mode='test', need_mean=True):
-    im_array, im_scales = get_image_array(roidb, config.SCALES, random_scale_indexes)
+    im_array, im_scales = get_image_array(roidb, config.SCALES, random_scale_indexes, need_mean=need_mean)
-def get_image_array(roidb, scales, scale_indexes):
+def get_image_array(roidb, scales, scale_indexes, need_mean=True):
-        im_tensor = image_processing.transform(im, config.PIXEL_MEANS)
+        im_tensor = image_processing.transform(im, config.PIXEL_MEANS, need_mean=need_mean)
-        negative_weights = (1.0 - config.TRAIN.RPN_POSTIVE_WEIGHT) / np.sum(labels == 1)
+        assert ((config.TRAIN.RPN_POSITIVE_WEIGHT > 0) & (config.TRAIN.RPN_POSITIVE_WEIGHT < 1))
-from helper.processing.bbox_transform import bbox_pred, clip_boxes
+from helper.processing.bbox_transform import bbox_pred, clip_boxes, clip_pad
-
+        if np.isnan(scores).any():
-
+        if self.cfg_key == 'TRAIN':
-
+            print "resudial = ", scores.shape[2] - height, scores.shape[3] - width
-
+        bbox_deltas = clip_pad(bbox_deltas, (height, width))
-
+            if len(keep) == 0:
-        pass
+        self.assign(in_grad[0], req[0], 0)
-        self._is_train = is_train
+        self._is_train = True if is_train == 'True' else False
-import rpn.proposal
+import rpn.proposal, rpn.proposal_target
-    test_rcnn(args.image_set, args.year, args.root_path, args.devkit_path, args.prefix, args.epoch, ctx, args.vis, args.has_rpn)
+    test_rcnn(args.image_set, args.year, args.root_path, args.devkit_path, args.prefix, args.epoch, ctx, args.vis,
-def test_rcnn(imageset, year, root_path, devkit_path, prefix, epoch, ctx, vis=False, has_rpn=True, proposal='rpn'):
+def test_rcnn(imageset, year, root_path, devkit_path, prefix, epoch, ctx, vis=False, has_rpn=True, proposal='rpn',
-    args, auxs = load_param(prefix, epoch, convert=True, ctx=ctx)
+    args, auxs, _ = load_param(prefix, epoch, convert=True, ctx=ctx)
-
+from mxnet.model import save_checkpoint
-    return arg_params, aux_params
+    return arg_params, aux_params, num_classes
-                                    for r, i in zip(rename_label, iters)], [])
+        self.rename_data = rename_data
-                                           self.next_batch[0].index)
+                                           self.next_batch[0].index,
-    data_shape = (3, 28, 28)
+def get_iterator(args, kv, data_shape=(3, 28, 28)):
-train_model.fit(args, net, get_iterator)
+if __name__ == '__main__':
-        label="data/train-labels-idx1-ubyte",
+        image="../image-classification/mnist/train-images-idx3-ubyte",
-        label="data/t10k-labels-idx1-ubyte",
+        image="../image-classification/mnist/t10k-images-idx3-ubyte",
-        
+
-        
+
-            self.label_mean = feats/np.sum(feats) 
+            self.label_mean = feats/np.sum(feats)
-        
+
-        
+
-            
+
-    symbol.save('%s-symbol.json' % prefix)
+    if symbol is not None:
-            0,64,128 ]
+def getpallete(num_cls):
-MOCK_MODULES = ['numpy', 'scipy', 'scipy.sparse', 'sklearn', 'matplotlib']
+MOCK_MODULES = ['numpy', 'numpy.testing', 'scipy', 'scipy.sparse', 'sklearn', 'matplotlib']
-from numpy.testing import assert_allclose, build_err_msg
+import numpy.testing as npt
-    -----------
+    Parameters
-    Calculated by \frac{|a-b|^2)}{|a|^2 + |b|^2}
+    Calculated by :math:`\\frac{|a-b|^2}{|a|^2 + |b|^2}`
-    sym : mx.Symbol
+    sym : Symbol
-    sym : mx.Symbol
+    sym : Symbol
-    executor : `mxnet.executor.Executor`
+    Parameters
-    sym : `mxnet.symbol.Symbol`
+    Parameters
-        - In either case, value of all the arguments must be provided.
+        - if type is list of numpy.ndarray
-        when computing the finite-difference
+        Whether to use is_train=True when computing the finite-difference
-                                names=["NUMERICAL", "BACKWARD"])
+            msg = npt.build_err_msg(arr_l,
-    sym : mxnet.symbol.Symbol
+    sym : Symbol
-    location : list of np.ndarray of dict
+    location : list of np.ndarray or dict of str to np.ndarray
-        if location is a dict, it contains the mapping between argument names and their values
+
-        if dict, contains mapping between sym.list_output() and exe.outputs
+
-            it contains the mapping between names of auxiliary states and their values
+        - if type is list of np.ndarray
-                                names=["EXPECTED", "FORWARD"])
+            msg = npt.build_err_msg([expect, output],
-    sym : mxnet.symbol.Symbol
+    sym : Symbol
-    location : list np.ndarray
+    location : list of np.ndarray or dict of str to np.ndarray
-        if location is a dict, it contains the mapping between argument names and their values
+
-        if dict, contains mapping between sym.list_output() and exe.outputs
+
-        if dict, contains mapping between sym.list_arguments() and exe.outputs
+
-                                names=["EXPECTED", "BACKWARD"])
+            msg = npt.build_err_msg(arr_l,
-    location : none or dict of np.ndarray
+        symbol to run the speed test
-
+        gradient requirement.
-                    {'ctx': mx.cpu(0), 'conv_data': (2, 2, 10, 10), 'type_dict': {'conv_data': np.float32}}]
+    >>> ctx_list =\
-                     'type_dict': {'concat_arg0': np.float32, 'concat_arg1': np.float32}}]
+    >>> ctx_list = \
-                assert_allclose(arr1, arr2, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
+                npt.assert_allclose(arr1, arr2, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
-                assert_allclose(arr1, arr2, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
+                npt.assert_allclose(arr1, arr2, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
-        out_grads : NDArray or list of NDArray, optional
+        out_grads : NDArray or list of NDArray or dict of str to NDArray, optional
-    return np.sum(a != b) == 0
+from mxnet.test_utils import *
-    test_reduce_inner(lambda data, axis, keepdims:_np_reduce(data, axis, keepdims, np.sum),
+    test_reduce_inner(lambda data, axis, keepdims:np_reduce(data, axis, keepdims, np.sum),
-    test_reduce_inner(lambda data, axis, keepdims:_np_reduce(data, axis, keepdims, np.max),
+    test_reduce_inner(lambda data, axis, keepdims:np_reduce(data, axis, keepdims, np.max),
-    test_reduce_inner(lambda data, axis, keepdims:_np_reduce(data, axis, keepdims, np.min),
+    test_reduce_inner(lambda data, axis, keepdims:np_reduce(data, axis, keepdims, np.min),
-                         check_symbolic_forward, reldiff, _np_reduce)
+from mxnet.test_utils import *
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-3, check_eps=5e-2)
+        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-3, check_eps=0.16)
-        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-3, check_eps=5e-2)
+        check_numeric_gradient(test, [data_tmp, gamma, beta], [rolling_mean, rolling_std], numeric_eps=1e-3, check_eps=0.16)
-    test_reduce_inner(lambda data, axis, keepdims:_np_reduce(data, axis, keepdims, np.sum),
+    test_reduce_inner(lambda data, axis, keepdims:np_reduce(data, axis, keepdims, np.sum),
-                        outgrad.reshape(_np_reduce(data, axis, 1, np.sum).shape),
+                        outgrad.reshape(np_reduce(data, axis, 1, np.sum).shape),
-            grad_groundtruth = _np_reduce(outgrad_npy, axis=axis, keepdims=True,
+            grad_groundtruth = np_reduce(outgrad_npy, axis=axis, keepdims=True,
-    check_numeric_gradient(test, [x1, x2], numeric_eps=1e-4, check_eps=1e-1)
+    check_numeric_gradient(test, [x1, x2], numeric_eps=1e-3, check_eps=1e-2)
-    from dmlc_tracker import opts
+    try:
-
+    update_on_kvstore = True
-                logging.info('Auto-select kvstore type = %s', kvstore)
+            if kvstore is 'local':
-    if not kv or 'local_allreduce' in kv.type:
+    if kv is None:
-            if kvstore and kvstore.type == 'dist_sync':
+            if kvstore and 'dist' in kvstore.type and not '_async' in kvstore.type:
-    def test_reshape_new(src_shape, shape_args, dst_shape):
+    def test_reshape_new(src_shape, shape_args, reverse, dst_shape):
-        net = mx.sym.Reshape(net, shape=shape_args)
+        net = mx.sym.Reshape(net, shape=shape_args, reverse=reverse)
-            %(str(src_shape), str(shape_args), str(dst_shape), str(output_shape[0]))
+            'Src Shape = %s, Shape Arguments = %s, Reverse = %s, Dst Shape = %s, ' \
-                                                                     str(shape_args), str(dst_shape))
+            'Src Shape = %s, Shape Arguments = %s, Reverse = %s, Dst Shape = %s'\
-                                                                     str(shape_args), str(dst_shape))
+            'Src Shape = %s, Shape Arguments = %s, Reverse = %s, Dst Shape = %s'\
-                  [(2, 4, 5, 3), (-1, 2, 2, 1), (30, 2, 2, 1)]]]
+    test_cases = [[(2, 3, 5, 5), (0, -1), False, (2, 75)],
-        test_reshape_new(test_case[0], test_case[1], test_case[2])
+        test_reshape_new(*test_case)
-def plot_network(symbol, title="plot", shape=None, node_attrs={}):
+    Parameters
-    dot = Digraph(name=title)
+    dot = Digraph(name=title, format=save_format)
-        self.executor = self.sym.simple_bind(ctx=mx.cpu(), **input_shapes)
+        self.executor = self.sym.simple_bind(ctx=ctx, **input_shapes)
-# pylint: disable=invalid-name
+# pylint: disable=invalid-name, no-member
-import numpy as np
+import numpy as np
-        if arg_shapes == None:
+        if arg_shapes is None:
-
+import logging
-    if need_grad == False:
+    if need_grad is False:
-    elif need_grad == True:
+    elif need_grad is True:
-    elif need_grad is set:
+    elif isinstance(need_grad, set):
-        name = arg_names[i]
+    for i, name in enumerate(arg_names):
-        for i in range(len(ctx)):
+        for i, ctxi in enumerate(ctx):
-            train_exec = _bind_exec(sym, ctx[i], data_shapes, self.param_names,
+            train_exec = _bind_exec(sym, ctxi, data_shapes, self.param_names,
-# pylint: disable=invalid-name, protected-access, fixme, too-many-arguments, W0221, W0201, no-self-use
+# pylint: disable=invalid-name, protected-access, fixme, too-many-arguments, W0221, W0201, no-self-use, no-member
-import numpy as np
+import ctypes
-            return False
+        return self.cursor < self.num_data
-            c_key_i, c_val_i = _ctype_key_value(keys[i], vals[i])
+        for key, val in zip(keys, vals):
-            if self.init_logginig == False:
+            if not self.init_logginig:
-from . import ndarray
+
-        if self.num == None:
+        if self.num is None:
-        if self.num == None:
+        if self.num is None:
-            label = labels[i].asnumpy().astype('int32')
+        for label, pred_label in zip(labels, preds):
-            label = labels[i].asnumpy().astype('int32')
+        for label, pred_label in zip(labels, preds):
-            label = labels[i].asnumpy().astype('int32')
+        for label, pred in zip(labels, preds):
-        if self.init == False:
+        if not self.init:
-import numpy as np
+from collections import namedtuple
-        param_on_devs = param_arrays[idx]
+    for idx, param_on_devs in enumerate(param_arrays):
-            if do_reset == True:
+            if do_reset:
-import numpy as np
+import numpy as np
-        assert merge_multi_context == True
+        assert merge_multi_context is True
-        assert merge_multi_context == True
+        assert merge_multi_context is True
-from .base import mx_uint, mx_float, NDArrayHandle, FunctionHandle
+from .base import c_array, mx_float, py_str, c_str, mx_real_t
-                c_array(ctypes.c_char_p, [c_str(str(i)) for i in kwargs.values()])))
+            handle, \
-c_int_p = POINTER(c_int)
+from ctypes import c_void_p, c_int, c_char, c_char_p, cast, c_bool
-            this['handle'] = False
+        this['handle'] = this.get('handle', None) is not None
-import ctypes
+import ctypes
-import json
+import json
-        if out_shapes == None:
+        if out_shapes is None:
-        node = nodes[i]
+    for i, node in enumerate(nodes):
-        node = nodes[i]
+    for i, node in enumerate(nodes):
-import os, sys
+def get_network_from_json_file(file_name):
-    parser = argparse.ArgumentParser(description='train an image classifer on mnist')
+    parser = argparse.ArgumentParser(description='train an image classifier on mnist')
-                        help='the cnn to use')
+                        help='the cnn to use (mlp | lenet | <path to network json file>')
-        data_shape = (1, 28, 28)
+    elif args.network == 'lenet':
-        train_model.fit(args, net, get_iterator(data_shape), mx.metric.Caffe())
+        train_model.fit(args, net, get_iterator(data_shape, use_caffe_data), mx.metric.Caffe())
-        train_model.fit(args, net, get_iterator(data_shape))
+        train_model.fit(args, net, get_iterator(data_shape, use_caffe_data))
-    def get_iterator_impl(args, kv):
+def get_iterator(data_shape, use_caffe_data):
-        flat = False if len(data_shape) == 3 else True
+        flat = False if len(data_shape) != 1 else True
-    return get_iterator_impl
+    def get_iterator_impl_caffe(args, kv):
-                wmat[:, [0, 2], :, :] = wmat[:, [2, 0], :, :]
+            channels = layer_blobs[0].channels;
-        mpi.submit(args)
+    
-        raise RuntimeError('Unknown submission cluster type %s' % args.cluster)
+      if args.cluster == 'ssh':
-        retcode = subprocess.call("cd %s; make" % folder, shell = True)
+        subprocess.call('cd %s; cp make/readthedocs.mk config.mk' % folder, shell = True)
-            new_n = self.data_list[0].shape[0] - self.data_list[0].shape[0] % batch_size
+            new_n = self.data[0][1].shape[0] - self.data[0][1].shape[0] % batch_size
-    heads = set([x[0] for x in conf["heads"]])  # TODO(xxx): check careful
+    heads = set(conf["heads"][0])  # TODO(xxx): check careful
-        header of the image record
+        header of the image record.
-    return header, s[_IRSize:]
+    s = s[_IRSize:]
-    png_formats = set(['.png', '.PNG'])
+    jpg_formats = ['.JPG', '.JPEG']
-    if img_fmt in jpg_formats:
+    if img_fmt.upper() in jpg_formats:
-    elif img_fmt in png_formats:
+    elif img_fmt.upper() in png_formats:
-
+
-                    image_list.append((len(image_list), os.path.relpath(fpath, root), cat[path]))
+                    yield (len(image_list), os.path.relpath(fpath, root), cat[path])
-
+                yield (len(image_list), os.path.relpath(fpath, root), 0)
-            line += '%s\n' % image_list[i][1]
+        for i, item in enumerate(image_list):
-
+    image_list = list(image_list)
-        for line in fin.readlines():
+        while True:
-# Error would happen
+            item = [int(line[0])] + [line[-1]] + [float(i) for i in line[1:-1]]
-        img = cv2.imread(os.path.join(pari_path, item[1]), color_modes[args.color])
+        img = cv2.imread(os.path.join(args.root, item[1]), args.color)
-        print ('imread error:', item[1])
+        print('imread error:', item[1])
-        print ('read none error:', item[1])
+        print('read none error:', item[1])
-    header = mx.recordio.IRHeader(0, item[2], item[0], 0)
+    if len(item) > 3 and args.pack_label:
-        print ('pack_img error:', item[1])
+        q_out.put((s, item))
-    while not q_in.empty():
+    while True:
-def write_worker(q_out, fname, saving_folder):
+def write_worker(q_out, fname, working_dir):
-    
+    count = 0
-            # write_list(lst_file, sink)
+        deq = q_out.get()
-        if len(sink) % 1000 == 0:
+
-            print ('time:', cur_time - pre_time, ' count:', len(sink))
+            print('time:', cur_time - pre_time, ' count:', count)
-def main():
+def parse_args():
-    parser.add_argument('prefix', help='prefix of input/output files.')
+    parser.add_argument('prefix', help='prefix of input/output lst and rec files.')
-                        help='folder in which .rec files will be saved.')
+    rgroup.add_argument('--pack-label', default=False,
-    data_path = args.root
+if __name__ == '__main__':
-    print ('total: ', len(source))
+        if os.path.isdir(args.prefix):
-        print 'imread error:', item[1]
+        print ('imread error:', item[1])
-        print 'read none error:', item[1]
+        print ('read none error:', item[1])
-        print 'pack_img error:', item[1]
+        print ('pack_img error:', item[1])
-            print 'time:', cur_time - pre_time, ' count:', len(sink)
+            print ('time:', cur_time - pre_time, ' count:', len(sink))
-                print 'Creating .rec file from', f, 'in', args.saving_folder
+                print ('Creating .rec file from', f, 'in', args.saving_folder)
-                print 'time:', cur_time - pre_time, ' count:', cnt
+                print ('time:', cur_time - pre_time, ' count:', cnt)
-    print 'total: ', len(source)
+    print ('total: ', len(source))
-    main()
+# -*- coding: utf-8 -*-
-    return ctypes.c_char_p(string.encode('utf-8'))
+if sys.version_info[0] < 3:
-                c_array(ctypes.c_char_p, [str(i).encode('ascii') for i in kwargs.values()])))
+                c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()]), \
-                c_array(ctypes.c_char_p, [str(i).encode('ascii') for i in kwargs.values()])))
+                c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()]), \
-                c_array(ctypes.c_char_p, [str(i).encode('ascii') for i in kwargs.values()])))
+                c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()]), \
-    def read_idx(self, idx=None):
+    def read_idx(self, idx):
-            self.seek(idx)
+        self.seek(idx)
-        center = factor - 1
+        center = factor - 1.0
-    return (stride, (kernel-1)/2-pad)
+    return (stride, (kernel-stride)/2-pad)
-    return ctypes.c_char_p(bytes(string, 'utf-8'))
+    return ctypes.c_char_p(string.encode('utf-8'))
-        buf : string
+        buf : string (python2), bytes (python3)
-                                                    c_str(buf),
+                                                    ctypes.c_char_p(buf),
-    N = 10
+    N = 255
-        writer.write(str(i))
+        if sys.version_info[0] < 3:
-
+        if sys.version_info[0] < 3:
-    N = 10
+    N = 255
-        writer.write_idx(i, str(i))
+        if sys.version_info[0] < 3:
-        assert res == bytes(str(k), 'utf-8')
+    for i in keys:
-    return ctypes.c_char_p(string.encode('utf-8'))
+    return ctypes.c_char_p(bytes(string, 'utf-8'))
-        self.uri = ctypes.c_char_p(uri)
+        self.uri = c_str(uri)
-                                                    ctypes.c_char_p(buf),
+                                                    c_str(buf),
-        super(Torch, self).__init__('torch')
+    def __init__(self, name='torch'):
-        super(Torch, self).__init__(name)
+    def __init__(self):
-
+import os, sys
-        super(Torch, self).__init__('torch')
+    def __init__(self, name='torch'):
-from rcnn.symbol import get_vgg_rcnn_test
+from rcnn.symbol import get_vgg_test
-    sym = get_vgg_rcnn_test()
+    sym = get_vgg_test()
-    im_array, im_scale = resize(im, config.TEST.SCALES[0], config.TRAIN.MAX_SIZE)
+    config.TEST.HAS_RPN = True
-    roi_array = np.hstack((batch_index_array, projected_rois))
+    im_info = np.array([[im_array.shape[2], im_array.shape[3], im_scale]], dtype=np.float32)
-    scores, boxes = detector.im_detect(im_array, roi_array)
+    scores, boxes = detector.im_detect(im_array, im_info)
-                        default=9, type=int)
+    parser = argparse.ArgumentParser(description='Demonstrate a Faster R-CNN network')
-    demo_net(detector, os.path.join(os.getcwd(), 'data', 'demo', '001551'))
+    demo_net(detector, args.image)
-# import numpy as np
+    fixed_param_prefix : list of str, indicating fixed parameters
-                 max_data_shapes=None, max_label_shapes=None):
+                 max_data_shapes=None, max_label_shapes=None, fixed_param_prefix=None):
-        # self._monitor_weight = None
+        if self._fixed_param_prefix is None:
-                        context=self._context, work_load_list=self._work_load_list)
+                        context=self._context, work_load_list=self._work_load_list,
-                            work_load_list=self._work_load_list)
+                            work_load_list=self._work_load_list,
-                    box *= scale
+                    box[:, :4] *= scale
-import logging
+from rcnn.config import config
-from rcnn.symbol import get_vgg_rcnn_test
+from rcnn.symbol import get_vgg_test, get_vgg_rcnn_test
-from utils.load_data import load_test_rpn_roidb
+from utils.load_data import load_gt_roidb, load_test_ss_roidb, load_test_rpn_roidb
-    logger.setLevel(logging.INFO)
+def test_rcnn(imageset, year, root_path, devkit_path, prefix, epoch, ctx, vis=False, has_rpn=True, proposal='rpn'):
-    voc, roidb = load_test_rpn_roidb(imageset, year, root_path, devkit_path)
+    # get test data iter
-
+    parser.add_argument('--has_rpn', dest='has_rpn', help='generate proposals on the fly',
-    test_net(args.image_set, args.year, args.root_path, args.devkit_path, args.prefix, args.epoch, ctx, args.vis)
+    test_rcnn(args.image_set, args.year, args.root_path, args.devkit_path, args.prefix, args.epoch, ctx, args.vis,
-    config.TEST.RPN_POST_NMS_TOP_N = 2000
+def test_rpn(image_set, year, root_path, devkit_path, prefix, epoch, ctx, vis=False):
-from utils.load_data import load_rpn_roidb
+from utils.load_data import load_ss_roidb, load_rpn_roidb
-              prefix, ctx, begin_epoch, end_epoch, frequent, kv_store, work_load_list=None, resume=False):
+def train_rcnn(image_set, year, root_path, devkit_path, pretrained, epoch,
-    voc, roidb, means, stds = load_rpn_roidb(image_set, year, root_path, devkit_path, flip=True)
+    voc, roidb, means, stds = eval('load_' + proposal + '_roidb')(image_set, year, root_path, devkit_path, flip=True)
-            fixed_params_names.append(name)
+    if config.TRAIN.FINETUNE:
-                        max_data_shapes=max_data_shape)
+                        max_data_shapes=max_data_shape, fixed_param_prefix=fixed_param_prefix)
-    parser = argparse.ArgumentParser(description='Train a Region Proposal Network')
+    parser = argparse.ArgumentParser(description='Train a Fast R-CNN Network')
-              args.kv_store, args.work_load_list, args.resume)
+    train_rcnn(args.image_set, args.year, args.root_path, args.devkit_path, args.pretrained, args.epoch,
-def train_net(image_set, year, root_path, devkit_path, pretrained, epoch,
+def train_rpn(image_set, year, root_path, devkit_path, pretrained, epoch,
-            fixed_params_names.append(name)
+    if config.TRAIN.FINETUNE:
-                        max_data_shapes=max_data_shape, max_label_shapes=max_label_shape)
+                        max_data_shapes=max_data_shape, max_label_shapes=max_label_shape,
-    train_net(args.image_set, args.year, args.root_path, args.devkit_path, args.pretrained, args.epoch,
+    train_rpn(args.image_set, args.year, args.root_path, args.devkit_path, args.pretrained, args.epoch,
-                 logger=logging):
+                 logger=logging, fixed_param_names=None):
-                if name in self.param_names:
+                if name in self.param_names and name not in self.fixed_param_names:
-                 logger=logging, context=ctx.cpu(), work_load_list=None):
+                 logger=logging, context=ctx.cpu(), work_load_list=None, fixed_param_names=None):
-                                                     shared_group, logger=self.logger)
+                                                     shared_group, logger=self.logger,
-                assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-5
+                assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-3
-                assert reldiff(exe.grad_dict['b'].asnumpy(), bgrad_npy) < 1E-5
+                assert reldiff(exe.grad_dict['a'].asnumpy(), agrad_npy) < 1E-3
-                    exe = c.simple_bind(ctx=ctx, a=a_npy.shape, b=b_npy.shape)
+                    exe = c.simple_bind(ctx=ctx, a=a_npy.shape, b=b_npy.shape, grad_req='write')
-                    assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-5
+                    assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-3
-
+                    assert reldiff(exe.grad_dict['a'].asnumpy(), agrad_npy) < 1E-3
-                    assert reldiff(exe.grad_dict['b'].asnumpy(), bgrad_npy) < 1E-3
+    mxnet_path = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), os.pardir))
-        if not f or f.endswith('.o') or f == '\\': continue
+        if not f or f.endswith('.o:') or f == '\\': continue
-        if fn.find('/usr/') < 0 and fn not in visited:
+        if os.path.abspath(f).startswith(mxnet_path) and fn not in visited:
-            raise TypeError(("Invalid type '%s' for %s, "  % (type(v), k)) + \
+        if not isinstance(v, NDArray):
-            self.label = [(k, v[idx]) for k, v in self.label]
+            self.data = [(k, array(v.asnumpy()[idx], v.context)) for k, v in self.data]
-            return [array(x[1][self.cursor:self.cursor+self.batch_size]) for x in data_source]
+            return [x[1][self.cursor:self.cursor+self.batch_size] for x in data_source]
-                                         axis=0)) for x in data_source]
+            return [concatenate([x[1][self.cursor:], x[1][:pad]]) for x in data_source]
-                newsize = (args.resize, img.shape[0] * args.resize / img.shape[1]
+                newsize = (args.resize, img.shape[0] * args.resize / img.shape[1])
-        :param box_list: [image_index][box_index][x1, x2, y1, y2]
+        :param box_list: [image_index] ndarray of [box_index][x1, x2, y1, y2]
-            if gt_roidb is not None:
+            if gt_roidb is not None and gt_roidb[i]['boxes'].size > 0:
-                       'use_diff': True}
+                       'use_diff': False,
-        import xml.dom.minidom as minidom
+        import xml.etree.ElementTree as ET
-        objs = data.getElementsByTagName('object')
+        tree = ET.parse(filename)
-                str(get_data_from_tag(obj, "name")).lower().strip()]
+            x1 = float(bbox.find('xmin').text) - 1
-            box_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1)  # pascal voc dataset starts from 1.
+            boxes = raw_data[i][:, (1, 0, 3, 2)] - 1  # pascal voc dataset starts from 1.
-        print 'ground truth annotations loaded from cache file {}'.format(cache_file)
+import numpy as np
-    targets[ex_inds, 4] = targets_dh
+    targets[ex_inds, 1:] = bbox_transform(ex_rois, gt_rois)
-This file has functions about bounding box post processing.
+This file has functions about bounding box processing.
-    ctr_y = boxes[:, 1] + 0.5 * heights
+    widths = boxes[:, 2] - boxes[:, 0] + 1.0
-    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w
+    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * (pred_w - 1.0)
-    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h
+    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * (pred_h - 1.0)
-    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w
+    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * (pred_w - 1.0)
-    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h
+    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * (pred_h - 1.0)
-    boxes[:, 0::4] = np.maximum(boxes[:, 0::4], 0)
+    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)
-    boxes[:, 1::4] = np.maximum(boxes[:, 1::4], 0)
+    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)
-    boxes[:, 2::4] = np.minimum(boxes[:, 2::4], im_shape[1] - 1)
+    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)
-    boxes[:, 3::4] = np.minimum(boxes[:, 3::4], im_shape[0] - 1)
+    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)
-    im_size_max = np.min(im_shape[0:2])
+    im_size_max = np.max(im_shape[0:2])
-def tensor_vstack(im_list):
+def tensor_vstack(tensor_list, pad=0):
-    :return: im_tensor [batch, channel, height, width]
+    vertically stack tensors
-    return im_tensor
+    ndim = len(tensor_list[0].shape)
-                squared_sums[cls, :] += (targets[cls_indexes, 1:] ** 2).sum(axis=0)
+    if config.TRAIN.BBOX_NORMALIZATION_PRECOMPUTED:
-    stds = np.sqrt(squared_sums / class_counts - means ** 2)
+        means = sums / class_counts
-                                 param.epoch, count, speed, name, value, cls, cls_value, bbox, bbox_value)
+                                 param.epoch, count, speed, name[0], value[0], name[1], value[1], name[2], value[2])
-config.TRAIN.MAX_SIZE = 1000
+# R-CNN and RPN
-config.TRAIN.BATCH_SIZE = 128
+# R-CNN bounding box regression
-config.TEST.SCALES = (600, )
+# R-CNN testing
-    def im_detect(self, im_array, roi_array):
+    def im_detect(self, im_array, im_info=None, roi_array=None):
-        if config.TEST.DEDUP_BOXES > 0:
+        if config.TEST.DEDUP_BOXES > 0 and not config.TEST.HAS_RPN:
-            self.symbol.infer_shape(data=self.arg_params['data'].shape, rois=self.arg_params['rois'].shape)
+        # fill in data
-
+
-        pred_boxes = bbox_pred(roi_array[:, 1:], bbox_deltas)
+        # save output
-        if config.TEST.DEDUP_BOXES > 0:
+        if config.TEST.DEDUP_BOXES > 0 and not config.TEST.HAS_RPN:
-    def __init__(self):
+    def __init__(self, use_ignore=False, ignore=None):
-        cls = pred_cls[np.arange(label.shape[0]), label]
+        if self.has_rpn:
-        label = labels[0].asnumpy()
+        bbox_loss = preds[1].asnumpy()
-training minibatch =
+RPN:
-testing minibatch =
+    'im_info': [num_images, 4] (optional)}
-    num_images = 1 and num_rois = all rois
+    'rois': [num_images, num_rois, 5]}
-def get_minibatch(roidb, num_classes):
+def get_minibatch(roidb, num_classes, mode='test'):
-    :param roidb: subset of main database
+    :param roidb: a list of dict, whose length controls batch size
-    :return: minibatch: {'data', 'rois', 'labels', 'bbox_targets', 'bbox_inside_weights', 'bbox_outside_weights'}
+    :param mode: controls whether blank label are returned
-    im_array, im_scales = get_image_array(roidb, config.TEST.SCALES, random_scale_indexes)
+    random_scale_indexes = npr.randint(0, high=len(config.SCALES), size=num_images)
-        rois_array.append(rois_array_this_image)
+                # project im_rois
-    rois_array = np.vstack(rois_array)
+                # add labels
-    return testbatch
+            rois_array = np.array(rois_array)
-        im, im_scale = image_processing.resize(im, target_size, config.TRAIN.MAX_SIZE)
+        im, im_scale = image_processing.resize(im, target_size, config.MAX_SIZE)
-from collections import namedtuple
+                 kv_store='local',
-                 optimizer='sgd', **kwargs):
+                 optimizer='sgd',
-            self.ctx = mx.cpu()
+            self.ctx = [mx.cpu()]
-        self.executor = None
+        self.mutable_data_shape = mutable_data_shape
-    def get_params(self, grad_req):
+        self.check_params()
-        arg_shapes, out_shapes, aux_shapes = self.symbol.infer_shape(data=(1, 3, 224, 224), rois=(1, 5))
+        self.arg_names = arg_names
-            self.grad_params = {}
+            param_names = []
-                    self.grad_params[name] = mx.nd.zeros(shape, self.ctx)
+                    if not (config.TRAIN.FINETUNE and name.startswith('conv')):
-        self.aux_params = {k: mx.nd.zeros(s, self.ctx) for k, s in zip(aux_names, aux_shapes)}
+        self.aux_names = aux_names
-                                       ['epoch', 'nbatch', 'eval_metric', 'cls_metric', 'bbox_metric'])
+
-        self.updater = mx.optimizer.get_updater(self.optimizer)
+        self.get_params(grad_req, train_data.provide_data + train_data.provide_label)
-        cls_metric = metric.LogLossMetric()
+        if config.TRAIN.HAS_RPN is True:
-            logger.info("                     --->Epoch[%d] Train-%s=%f", epoch, name, value)
+        self.optimizer = mx.optimizer.create(self.optimizer,
-def get_symbol_vgg(num_classes=21):
+def get_vgg_conv(data):
-    :param num_classes: used to determine output size
+    shared convolutional layers
-    bbox_loss_outside_weight = mx.symbol.Variable(name='bbox_loss_outside_weight')
+
-    cls_prob = mx.symbol.SoftmaxOutput(name='cls_prob', data=cls_score, label=cls_prob_label)
+    cls_prob = mx.symbol.SoftmaxOutput(name='cls_prob', data=cls_score, label=label)
-    bbox_loss_ = bbox_loss_outside_weight * \
+    bbox_loss_ = bbox_outside_weight * \
-                                     data=bbox_loss_inside_weight * (bbox_pred - bbox_loss_target))
+                                     data=bbox_inside_weight * (bbox_pred - bbox_target))
-def get_symbol_vgg_test(num_classes=21):
+def get_vgg_rcnn_test(num_classes=21):
-    Fast R-CNN test with VGG 16 conv layers
+    Fast R-CNN Network with VGG
-    relu5_3 = mx.symbol.Activation(data=conv5_3, act_type="relu", name="relu5_3")
+
-    thresh = 0.1
+    thresh = 0.05
-        im = image_processing.transform(im, config.PIXEL_MEANS)
+        if config.TEST.HAS_RPN:
-            vis_all_detection(im, boxes_this_image,
+            # visualize the testing scale
-def vis_all_detection(im_array, detections, imdb_classes=None, thresh=0.):
+def vis_all_detection(im_array, detections, imdb_classes=None, thresh=0.7):
-                                     edgecolor=color, linewidth=2)
+                                     edgecolor=color, linewidth=3.5)
-                                   rect.get_xy(), color='w')
+                plt.gca().text(bbox[0], bbox[1] - 2,
-    test_net(args.image_set, args.year, args.root_path, args.devkit_path, args.prefix, args.epoch, ctx)
+import argparse
-from rcnn.config import config
+from rcnn.config import config
-        keep = nms(dets, NMS_THRESH)
+        keep = nms(dets.astype(np.float32), NMS_THRESH)
-    return voc, ss_roidb
+import argparse
-              args.prefix, ctx, args.begin_epoch, args.end_epoch, args.frequent)
+# This script will not work unless all paths are set right
-        assert ctx is not None
+        if ctx is None:
-        return broadcast_to(self, shape=tuple(shape))
+        cur_shape = self.shape
-# pylint: disable= too-many-lines, redefined-builtin
+# pylint: disable= too-many-lines, redefined-builtin, protected-access
-            return NDArray._plus(self, other, out=self)
+            return _internal._plus(self, other, out=self)
-            return NDArray._plus_scalar(self, float(other), out=self)
+            return _internal._plus_scalar(self, float(other), out=self)
-            return NDArray._minus(self, other, out=self)
+            return _internal._minus(self, other, out=self)
-            return NDArray._minus_scalar(self, float(other), out=self)
+            return _internal._minus_scalar(self, float(other), out=self)
-        return NDArray._mul_scalar(self, -1.0)
+        return _internal._mul_scalar(self, -1.0)
-            return NDArray._mul(self, other, out=self)
+            return _internal._mul(self, other, out=self)
-            return NDArray._mul_scalar(self, float(other), out=self)
+            return _internal._mul_scalar(self, float(other), out=self)
-            return NDArray._div(self, other, out=self)
+            return _internal._div(self, other, out=self)
-            return NDArray._div_scalar(self, float(other), out=self)
+            return _internal._div_scalar(self, float(other), out=self)
-            NDArray._set_value(float(value), out=self)
+            _internal._set_value(float(value), out=self)
-            return NDArray._copyto(self, out=other)
+            return _internal._copyto(self, out=other)
-            return NDArray._copyto(self, out=hret)
+            return _internal._copyto(self, out=hret)
-    return NDArray._onehot_encode(indices, out, out=out)
+    return _internal._onehot_encode(indices, out, out=out)
-        NDArray._plus,
+        _internal._plus,
-        NDArray._plus_scalar,
+        _internal._plus_scalar,
-        NDArray._minus,
+        _internal._minus,
-        NDArray._rminus_scalar)
+        _internal._minus_scalar,
-        NDArray._mul,
+        _internal._mul,
-        NDArray._mul_scalar,
+        _internal._mul_scalar,
-        NDArray._div,
+        _internal._div,
-        NDArray._rdiv_scalar)
+        _internal._div_scalar,
-        NDArray._power,
+        _internal._power,
-        NDArray._rpower_scalar)
+        _internal._power_scalar,
-        NDArray._maximum,
+        _internal._maximum,
-        NDArray._maximum_scalar,
+        _internal._maximum_scalar,
-        NDArray._minimum,
+        _internal._minimum,
-        NDArray._minimum_scalar,
+        _internal._minimum_scalar,
-                                 str_img=str_img)
+        return _internal._imdecode(mean, index,
-                                 out=out)
+        return _internal._imdecode(mean, index,
-        # if function name starts with underscore, register as static method of NDArray
+        # if function name starts with underscore, register as internal namespace
-            setattr(NDArray, function.__name__, staticmethod(function))
+            setattr(module_internal, function.__name__, function)
-                                    **kwargs)
+        sym = symbol._internal._Native(*args,
-                                     **kwargs)
+        sym = symbol._internal._NDArray(*args,
-from .ndarray import NDArray, empty
+from .ndarray import empty
-    return NDArray._sample_uniform(low=low, high=high, shape=out.shape, out=out)
+    return _internal._sample_uniform(low=low, high=high, shape=out.shape, out=out)
-    return NDArray._sample_normal(loc=loc, scale=scale, shape=out.shape, out=out)
+    return _internal._sample_normal(loc=loc, scale=scale, shape=out.shape, out=out)
-
+from . import _symbol_internal as _internal
-            return Symbol._Plus(self, other)
+            return _internal._Plus(self, other)
-            return Symbol._PlusScalar(self, scalar=other)
+            return _internal._PlusScalar(self, scalar=other)
-            return Symbol._Minus(self, other)
+            return _internal._Minus(self, other)
-            return Symbol._MinusScalar(self, scalar=other)
+            return _internal._MinusScalar(self, scalar=other)
-            return Symbol._RMinusScalar(self, scalar=other)
+            return _internal._RMinusScalar(self, scalar=other)
-            return Symbol._Mul(self, other)
+            return _internal._Mul(self, other)
-            return Symbol._MulScalar(self, scalar=other)
+            return _internal._MulScalar(self, scalar=other)
-            return Symbol._Div(self, other)
+            return _internal._Div(self, other)
-            return Symbol._DivScalar(self, scalar=other)
+            return _internal._DivScalar(self, scalar=other)
-            return Symbol._RDivScalar(self, scalar=other)
+            return _internal._RDivScalar(self, scalar=other)
-            return Symbol._Power(self, other)
+            return _internal._Power(self, other)
-            return Symbol._PowerScalar(self, scalar=other)
+            return _internal._PowerScalar(self, scalar=other)
-            setattr(Symbol, function.__name__, staticmethod(function))
+            setattr(module_internal, function.__name__, function)
-        return Symbol._Power(base, exp)
+        return _internal._Power(base, exp)
-        return Symbol._PowerScalar(base, scalar=exp)
+        return _internal._PowerScalar(base, scalar=exp)
-        return Symbol._RPowerScalar(exp, scalar=base)
+        return _internal._RPowerScalar(exp, scalar=base)
-        return Symbol._Maximum(left, right)
+        return _internal._Maximum(left, right)
-        return Symbol._MaximumScalar(left, scalar=right)
+        return _internal._MaximumScalar(left, scalar=right)
-        return Symbol._MaximumScalar(right, scalar=left)
+        return _internal._MaximumScalar(right, scalar=left)
-        return Symbol._Minimum(left, right)
+        return _internal._Minimum(left, right)
-        return Symbol._MinimumScalar(left, scalar=right)
+        return _internal._MinimumScalar(left, scalar=right)
-        return Symbol._MinimumScalar(right, scalar=left)
+        return _internal._MinimumScalar(right, scalar=left)
-                assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-5
+                assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-3
-                assert reldiff(exe.grad_dict['b'].asnumpy(), bgrad_npy) < 1E-5
+                assert reldiff(exe.grad_dict['a'].asnumpy(), agrad_npy) < 1E-3
-                    assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-5
+                    assert reldiff(outputs[0].asnumpy(), c_npy) < 1E-3
-                    assert reldiff(exe.grad_dict['b'].asnumpy(), bgrad_npy) < 1E-5
+                    assert reldiff(exe.grad_dict['a'].asnumpy(), agrad_npy) < 1E-3
-        if self.params_initialized:
+        elif self.params_initialized:
-                newsize = (img.shape[0] * args.resize / img.shape[1], args.resize)
+                newsize = (args.resize, img.shape[0] * args.resize / img.shape[1]
-                newsize = (args.resize, img.shape[1] * args.resize / img.shape[0])
+                newsize = (img.shape[1] * args.resize / img.shape[0], args.resize)
-                 lr_scheduler=None, sym=None):
+                 lr_scheduler=None, sym=None, begin_num_update=0):
-        self.num_update = 0
+        self.begin_num_update = begin_num_update
-            self._index_update_count[index] = 0
+            self._index_update_count[index] = self.begin_num_update
-        if not self._buckets.has_key(bucket_key):
+        if not bucket_key in self._buckets:
-        for mod in self._buckets.itervalues():
+        for mod in self._buckets.values():
-    pool = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pool_type='max', attr=mirror_attr)
+    pool = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max', attr=mirror_attr)
-    pooling = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pool_type="max", name=('max_pool_%s_pool' % name))
+    pooling = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type="max", name=('max_pool_%s_pool' % name))
-    pooling = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pool_type="max", name=('max_pool_%s_pool' % name))
+    pooling = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type="max", name=('max_pool_%s_pool' % name))
-        os.system("wget http://webdocs.cs.ualberta.ca/~bx3/data/cifar10.zip")
+        os.system("wget http://data.dmlc.ml/mxnet/data/cifar10.zip")
-# consumption with slightly slower computation speed (due to extra forward 
+# consumption with slightly slower computation speed (due to extra forward
-        os.system("wget http://webdocs.cs.ualberta.ca/~bx3/data/cifar10.zip")
+        os.system("wget http://data.dmlc.ml/mxnet/data/data/cifar10.zip")
-        os.system('wget http://webdocs.cs.ualberta.ca/~bx3/data/cifar10.zip')
+        os.system('wget http://data.dmlc.ml/mxnet/data/cifar10.zip')
-        os.system("wget http://webdocs.cs.ualberta.ca/~bx3/data/mnist.zip")
+        os.system("wget http://data.dmlc.ml/mxnet/data/mnist.zip")
-        os.system("wget http://webdocs.cs.ualberta.ca/~bx3/data/cifar10.zip")
+           os.system("wget http://data.dmlc.ml/mxnet/data/cifar10.zip")
-        os.system("wget http://webdocs.cs.ualberta.ca/~bx3/data/mnist.zip -P data/")
+        os.system("wget http://data.dmlc.ml/mxnet/data/mnist.zip -P data/")
-        os.system("wget http://webdocs.cs.ualberta.ca/~bx3/data/cifar10.zip -P data/")
+        os.system("wget http://data.dmlc.ml/mxnet/data/cifar10.zip -P data/")
-    args['cls_score_weight'] = mx.random.normal(mean=0, stdvar=0.01, shape=arg_shape_dict['cls_score_weight'], ctx=ctx)
+    args['cls_score_weight'] = mx.random.normal(loc=0, scale=0.01, shape=arg_shape_dict['cls_score_weight'], ctx=ctx)
-    args['bbox_pred_weight'] = mx.random.normal(mean=0, stdvar=0.001, shape=arg_shape_dict['bbox_pred_weight'], ctx=ctx)
+    args['bbox_pred_weight'] = mx.random.normal(loc=0, scale=0.001, shape=arg_shape_dict['bbox_pred_weight'], ctx=ctx)
-                wmat_dim = layer_blobs[0].shape.dim
+            if getattr(layer_blobs[0].shape, 'dim', None) is not None:
-                wmat_dim = [layer_blobs[0].num, layer_blobs[0].channels, layer_blobs[0].height, layer_blobs[0].width]
+                wmat_dim = list(layer_blobs[0].shape)
-        buf = "0" + buf
+    buf = ""
-    for i in range(4):
+    for i in range(len(buf)):
-    return ret        
+    return ret
-        l = label[i]
+        l = remove_blank(label[i])
-
+                
-from .ndarray import NDArray, zeros, clip, sqrt
+from .ndarray import NDArray, zeros, clip, sqrt, square
-        variance[:] = self.beta2 * variance + (1. - self.beta2) * grad * grad
+        mean *= self.beta1
-        weight[:] -= lr*mean/(sqrt(variance) + self.epsilon)
+        weight -= lr*mean/(sqrt(variance) + self.epsilon)
-parser.add_argument('--gpus', type=str, default='0',
+parser.add_argument('--gpus', type=str,
-parser.add_argument('--gpus', type=str, default='0',
+parser.add_argument('--gpus', type=str,
-parser.add_argument('--gpus', type=str, default='0',
+parser.add_argument('--gpus', type=str,
-parser.add_argument('--gpus', type=str, default='0',
+parser.add_argument('--gpus', type=str,
-parser.add_argument('--gpus', type=str, default='0',
+parser.add_argument('--gpus', type=str,
-parser.add_argument('--gpus', type=str, default='0',
+parser.add_argument('--gpus', type=str,
-parser.add_argument('--gpus', type=str, default='0',
+parser.add_argument('--gpus', type=str,
-                    raise ValueError('only accept keyword argument of NDArrays')
+                if not isinstance(array, (NDArray, np.ndarray)):
-                array.copyto(arg_dict[name])
+                if arg_dict[name].shape != array.shape:
-    # Generate random data that has ndim between 1-7 and all the shape dims between 1-10
+    # Generate random data that has ndim between 1-7 and all the shape dims between 1-5
-    shape = np.random.randint(1, 11, size=(ndim,))
+    shape = np.random.randint(1, 6, size=(ndim,))
-            # Generate random data that has ndim between 1-7 and all the shape dims between 1-10
+            # Generate random data that has ndim between 1-7 and all the shape dims between 1-5
-            shape = np.random.randint(1, 11, size=(ndim,))
+            shape = np.random.randint(1, 6, size=(ndim,))
-        # Generate random data that has ndim between 1-7 and all the shape dims between 1-10
+        # Generate random data that has ndim between 1-7 and all the shape dims between 1-5
-        target_shape = np.random.randint(1, 11, size=(ndim,))
+        target_shape = np.random.randint(1, 6, size=(ndim,))
-# pylint: disable=too-many-arguments, too-many-locals, too-many-public-methods
+# pylint: disable=too-many-arguments, too-many-locals, too-many-public-methods, too-many-branches
-            validation_metric=None):
+            validation_metric=None, monitor=None):
-
+        if monitor is not None:
-
+
-def get_lenet():
+def get_lenet(add_stn=False):
-                        choices = ['mlp', 'lenet'],
+                        choices = ['mlp', 'lenet', 'lenet-stn'],
-                param.kernel_size, param.stride, param.stride)
+            param_string = ''
-            output_name = name
+        if layer[i].type == 'Crop':
-
+        output_name = name
-                    assert allow_missing
+                    if not allow_missing:
-        return ret
+        return broadcast_to(self, shape=tuple(shape))
-            assert err_forward < 1E-6
+            err_forward = reldiff(net.outputs[0].asnumpy(), sum_groundtruth)
-            assert err_backward < 1E-6
+            err_backward = reldiff(grad_nd.asnumpy(), grad_groundtruth)
-            b = mx.symbol.broadcast_axis(a, axis=axis, size=size)
+    for i in range(sample_num):
-                         args_grad={'a': grad_nd})
+            net = sym_bcast.bind(mx.cpu(), args={'a': mx.nd.array(dat_npy)},
-            assert err_forward < 1E-8
+            err_forward = reldiff(net.outputs[0].asnumpy(), groundtruth)
-
+            err_backward = reldiff(grad_nd.asnumpy(), grad_groundtruth)
-    dll_path = [curr_path, api_path]
+    cmake_build_path = os.path.join(curr_path, '../../build/Release/')
-import sys
+ï»¿import sys
-def check_consistency(sym, ctx_list, scale=1.0):
+def check_consistency(sym, ctx_list, scale=1.0, grad_req='write'):
-    exe_list = [sym.simple_bind(grad_req='write', **ctx) for ctx in ctx_list]
+    exe_list = [sym.simple_bind(grad_req=grad_req, **ctx) for ctx in ctx_list]
-    grads = [[grad.asnumpy() for grad in exe.grad_arrays] for exe in exe_list]
+    # lazy solution handling None grad
-    exe = sym.simple_bind(grad_req='write', **ctx)
+def check_speed(sym, ctx, scale=1.0, N=100, grad_req='write'):
-            
+
-            ndarray_ret = nd_reduce_func(arr=mx.nd.array(dat), axis=axes, keepdims=keepdims)
+            ndarray_ret = nd_reduce_func(mx.nd.array(dat), axis=axes, keepdims=keepdims)
-            assert ndarray_ret.shape == numpy_ret.shape
+            assert (ndarray_ret.shape == numpy_ret.shape) or \
-            b = mx_reduce_sym(a, axis=axes, keepdims=keepdims)
+            if axes is None:
-            axis = np.random.randint(0, ndim)
+            axis = tuple(set(np.random.randint(0, ndim, np.random.randint(1, ndim + 1))))
-            shape[axis] = 1
+            size = tuple([shape[ele] for ele in axis])
-            assert err_backward < 1E-8
+            err_backward = np.square(grad_nd.asnumpy() - grad_groundtruth).sum()\
-    return NDArray._random_uniform(low, high, out=out)
+    return NDArray._sample_uniform(low=low, high=high, shape=out.shape, out=out)
-def normal(mean, stdvar, shape=None, ctx=None, out=None):
+def normal(loc, scale, shape=None, ctx=None, out=None):
-    mean : float
+    loc : float
-    stdvar : float
+    scale : float
-    return NDArray._random_gaussian(mean, stdvar, out=out)
+    return NDArray._sample_normal(loc=loc, scale=scale, shape=out.shape, out=out)
-
+def check_symbolic_random(dev):
-def do_checkpoint(prefix):
+def do_checkpoint(prefix, period=1):
-        save_checkpoint(prefix, iter_no + 1, sym, arg, aux)
+        if (iter_no + 1) % period == 0:
-    f = open(filepath)
+    f = open(filepath, 'rb')
-    def set_params(self, arg_params, aux_params):
+    def set_params(self, arg_params, aux_params, allow_missing=False, force_init=True):
-                         allow_missing=False, force_init=True)
+                         allow_missing=allow_missing, force_init=force_init)
-                if cache.has_key(name):
+                if name in cache:
-                    initializer(name, arr)
+                    if initializer != None:
-    cgroup.add_argument('--train_ratio', type=float, default=1.0,
+    cgroup.add_argument('--train-ratio', type=float, default=1.0,
-    cgroup.add_argument('--test_ratio', type=float, default=0,
+    cgroup.add_argument('--test-ratio', type=float, default=0,
-    rgroup.add_argument('--center_crop', type=bool, default=False,
+    rgroup.add_argument('--center-crop', type=bool, default=False,
-    rgroup.add_argument('--saving_folder', type=str, default='.',
+    rgroup.add_argument('--saving-folder', type=str, default='.',
-    main()
+def check_deconvolution_target_shape(input_shape, kernel, stride, pad, adj, target_shape=None):
-    label = mx.sym.Reshape(data=label, target_shape=(0,))
+    label = mx.sym.Reshape(data=label, shape=(-1,))
-    l[:] = np.random.randint(0, shape[1]-1, (shape[0],))
+    l = mx.random.uniform(-1, 1, shape, ctx = xpu)
-    print(exec1.outputs[0].asnumpy())
+    out = exec1.outputs[0].asnumpy()
-    print(grad.asnumpy())
+    assert_allclose(grad.asnumpy(), np_softmax(x.asnumpy()) - l.asnumpy())
-    sample_num = 1000
+    sample_num = 200
-    return [(np.random.random(u), np.random.random(v)) for (u,v) in shape_pairs]
+    # Generate random data that has ndim between 1-7 and all the shape dims between 1-10
-    for d in _gen_broadcast_data():
+    sample_num = 200
-        out = d[0] + d[1]
+    sample_num = 200
-        assert err_1 < 1e-6 and err_2 < 1e-6, 'lhs error %f, rhs error %f, shapes are %s %s' % (
+        assert err_1 < 1e-5 and err_2 < 1e-5, 'lhs error %f, rhs error %f, shapes are %s %s' % (
-    sample_num = 1000
+    sample_num = 200
-    sample_num = 1000
+    sample_num = 200
-import cv, cv2
+import cv2
-	    subdirs.sort()
+            subdirs.sort()
-            line = '%d\t'%image_list[i][0]
+        n_images = xrange(len(image_list))
-            line += '%s\n'%image_list[i][1]
+                line += '%d\t' % j
-    random.shuffle(image_list)
+
-            str_chunk = '_%d'%i
+    chunk_size = (N + args.chunks - 1) / args.chunks
-        write_list(prefix_out+str_chunk+'_val.lst', chunk[sep_test+sep:])
+        sep = int(chunk_size * args.train_ratio)
-            item = [int(line[0])] + [line[-1]] + [float(i) for i in line[1:-1]]
+            item = [int(line[0])] + [line[-1]] + [int(i) for i in line[1:-1]]
-def write_record(args, image_list):
+
-                    1: cv2.IMREAD_COLOR}
+                   0: cv2.IMREAD_GRAYSCALE,
-    
+
-                img = img[margin:margin+img.shape[1], :]
+                margin = (img.shape[0] - img.shape[1]) / 2;
-                img = img[:, margin:margin+img.shape[0]]
+                margin = (img.shape[1] - img.shape[0]) / 2;
-                newsize = (img.shape[0]*args.resize/img.shape[1], args.resize)
+                newsize = (img.shape[0] * args.resize / img.shape[1], args.resize)
-                newsize = (args.resize, img.shape[1]*args.resize/img.shape[0])
+                newsize = (args.resize, img.shape[1] * args.resize / img.shape[0])
-            print 'pack_img error:',item[1]
+            print 'pack_img error:', item[1]
-    def write_worker(q_out, prefix):
+    def write_worker(q_out, fname, saving_folder):
-	record = mx.recordio.MXRecordIO(prefix+'.rec', 'w')
+        os.chdir(saving_folder)
-                write_list(prefix+'.lst', sink)
+                write_list(fname_rec + '.lst', sink)
-                for i in range(args.num_thread)]
+                        for i in range(args.num_thread)]
-        write_process = multiprocessing.Process(target=write_worker, args=(q_out,args.prefix))
+        write_process = multiprocessing.Process(target=write_worker, args=(q_out, fname, args.saving_folder))
-	record = mx.recordio.MXRecordIO(args.prefix+'.rec', 'w')
+        os.chdir(args.saving_folder)
-    	make a record database by reading from an image list')
+        make a record database by reading from an image list')
-        help='If this is set im2rec will create image list(s) by traversing root folder\
+                        help='If this is set im2rec will create image list(s) by traversing root folder\
-        help='list of acceptable image extensions.')
+    cgroup.add_argument('--exts', type=list, default=['.jpeg', '.jpg'],
-        help='Ratio of images to use for training.')
+                        help='Ratio of images to use for training.')
-	help='Ratio of images to use for testing.')
+                        help='Ratio of images to use for testing.')
-        help='If true recursively walk through subdirs and assign an unique label\
+                        help='If true recursively walk through subdirs and assign an unique label\
-        help='resize the shorter edge of image to the newsize, original images will\
+                        help='resize the shorter edge of image to the newsize, original images will\
-        help='specify whether to crop the center image to make it rectangular.')
+                        help='specify whether to crop the center image to make it rectangular.')
-        help='JPEG quality for encoding, 1-100; or PNG compression for encoding, 1-9')
+                        help='JPEG quality for encoding, 1-100; or PNG compression for encoding, 1-9')
-        help='number of thread to use for encoding. order of images will be different\
+                        help='number of thread to use for encoding. order of images will be different\
-        help='specify the color mode of the loaded image.\
+                        help='specify the color mode of the loaded image.\
-
+                        help='specify the encoding of the images.')
-                  args.exts, args.chunks, args.train_ratio, args.test_ratio)
+        make_list(args)
-                print 'OK'
+            if f.startswith(args.prefix) is True and f.endswith('.lst') is True:
-                print 'not OK'
+                write_record(args, image_list, f)
-    im_size_max = np.max(im_shape[0:2])
+    im_size_max = np.min(im_shape[0:2])
-                                 param.epoch, count, speed, name[0], value[0], name[1], value[1], name[2], value[2])
+                                 param.epoch, count, speed, name, value, cls, cls_value, bbox, bbox_value)
-    def __init__(self, roidb, ctx, batch_size=2, shuffle=False, mode='train', work_load_list=None):
+    def __init__(self, roidb, batch_size=2, shuffle=False, mode='train'):
-        self.work_load_list = work_load_list
+        self.data_name = self.data.keys()
-        return [('data', self.data[0].shape), ('rois', self.data[1].shape)]
+        return [(k, v.shape) for k, v in self.data.items()]
-                ('bbox_loss_outside_weight', self.label[3].shape)]
+        return [(k, v.shape) for k, v in self.label.items()]
-                                       pad=self.getpad(), index=self.getindex())
+            return mx.io.DataBatch(data=self.data, label=self.label,
-            return 0
+        return self.batch_size - self.size % self.batch_size
-                mx.nd.array(self.batch['bbox_inside_weights']), mx.nd.array(self.batch['bbox_outside_weights'])]
+            self.data = {'data': self.batch['data'],
-        batch = minibatch.get_minibatch(roidb, self.num_classes, self.ctx, self.work_load_list)
+        cur_to = min(cur_from + self.batch_size, self.size)
-        label = labels[1].asnumpy()
+        bbox_loss = preds[0].asnumpy()
-def get_minibatch(roidb, num_classes, ctx, work_load_list=None):
+
-    for im_i, idx in enumerate(idx_in_slice):
+    for im_i in range(num_images):
-        batch_index = idx * np.ones((rois.shape[0], 1))
+        batch_index = im_i * np.ones((rois.shape[0], 1))
-                 max_data_shape=None, **kwargs):
+                 optimizer='sgd', **kwargs):
-        self.kv_store = kv_store
+        self.grad_params = None
-            param_names = []
+            self.grad_params = {}
-            self.param_names = list(param_names)
+                    self.grad_params[name] = mx.nd.zeros(shape, self.ctx)
-
+        speedometer_param = namedtuple('BatchEndParams',
-        eval_metric = metric.Accuracy()
+        eval_metric = mx.metric.create("accuracy")
-                                     mutable_data_shape=True, max_data_shape=self.max_data_shape)
+        # begin training
-    test_data = ROIIter(roidb, ctx=ctx, batch_size=1, shuffle=False, mode='test')
+    test_data = ROIIter(roidb, batch_size=1, shuffle=False, mode='test')
-              prefix, ctx, begin_epoch, end_epoch, frequent, kv_store, work_load_list=None):
+              prefix, ctx, begin_epoch, end_epoch, frequent):
-    train_data = ROIIter(roidb, ctx=ctx,  batch_size=config.TRAIN.BATCH_IMAGES, shuffle=True, mode='train', work_load_list=work_load_list)
+    train_data = ROIIter(roidb, batch_size=config.TRAIN.BATCH_IMAGES, shuffle=True, mode='train')
-    args, auxs = load_param(pretrained, epoch, convert=True, ctx=ctx[0])
+    args, auxs = load_param(pretrained, epoch, convert=True, ctx=ctx)
-    args['bbox_pred_bias'] = mx.nd.zeros(shape=arg_shape_dict['bbox_pred_bias'], ctx=ctx[0])
+    args['cls_score_weight'] = mx.random.normal(mean=0, stdvar=0.01, shape=arg_shape_dict['cls_score_weight'], ctx=ctx)
-                    learning_rate=0.001, lr_scheduler=mx.lr_scheduler.FactorScheduler(30000, 0.1), max_data_shape=[3, 1000, 1000])
+    solver = Solver(prefix, sym, ctx, begin_epoch, end_epoch, args, auxs, momentum=0.9, wd=0.0005,
-                                       mx.nd.array(means, ctx=ctx[0])
+        arg_params['bbox_pred_weight'] = (arg_params['bbox_pred_weight'].T * mx.nd.array(stds, ctx=ctx)).T
-                        default='trainval', type=str)
+                        default='train', type=str)
-                        default='0', type=str)
+    parser.add_argument('--gpu', dest='gpu_id', help='GPU device to train with',
-    ctx = [mx.gpu(int(i)) for i in args.gpu_ids.split(',')]
+    ctx = mx.gpu(args.gpu_id)
-              args.prefix, ctx, args.begin_epoch, args.end_epoch, args.frequent, args.kv_store, args.work_load_list)
+              args.prefix, ctx, args.begin_epoch, args.end_epoch, args.frequent)
-                    d_src[slice_idx].copyto(d_dst)
+                d_src[slice_idx].copyto(d_dst)
-                 max_data_shape=None, shared_group=None):
+    def __init__(self, sym, arg_names, param_names, ctx, slices, train_data, shared_group=None):
-
+            data_shapes = {k: tuple([slices[i].stop-slices[i].start] + list(v[1:]))
-            labels_slice = [label[new_slice] for label in labels]
+            labels_slice = [label[islice] for label in labels]
-                 mutable_data_shape=False, max_data_shape=None):
+                 work_load_list=None, logger=None, sym_gen=None):
-                                                 self.slices, train_data, max_data_shape)
+                                                 self.slices, train_data)
-            self.curr_execgrp = execgrp
+
-                        mutable_data_shape=False, max_data_shape=None):
+                        eval_batch_end_callback=None, sym_gen=None):
-                                                   max_data_shape=max_data_shape)
+                                                   logger=logger)
-            if do_reset is True:
+            if do_reset == True:
-    check_type_consistency(sym, ctx_list)
+    sym = mx.sym.Deconvolution(num_filter=2, kernel=(3,3), name='deconv')
-    #test_multi_softmax_with_shape((3,4,5), mx.gpu())
+    #test_softmax_with_shape((3,4), mx.gpu())
-    def __truediv__(self, other):
+    def __div__(self, other):
-    def __rtruediv__(self, other):
+    def __rdiv__(self, other):
-    def __itruediv__(self, other):
+    def __idiv__(self, other):
-        cur_shape = np.array(cur_shape)
+        cur_shape_arr = np.array(cur_shape)
-        if (cur_shape[broadcasting_axes] != 1).any():
+        broadcasting_axes = np.nonzero(cur_shape_arr != shape)
-        ret = self.reshape(tuple(cur_shape))
+        ret = self.reshape(tuple(cur_shape_arr))
-    # pylint: enable= no-member, protected-access
+#pylint: enable= too-many-arguments, no-member, protected-access
-            None)
+        lhs,
-            NDArray._rminus_scalar)
+        lhs,
-            None)
+        lhs,
-            NDArray._rdiv_scalar)
+        lhs,
-            NDArray._rpower_scalar)
+        lhs,
-
+        lhs,
-    if 'sum' == typ:
+    if typ == 'sum':
-    elif 'max' == typ:
+    elif typ == 'max':
-    elif 'min' == typ:
+    elif typ == 'min':
-    axis = sorted([x if 0 <= x else x + ndim for x in axis])
+    axis = sorted([x if x >= 0 else x + ndim for x in axis])
-            if isinstance(out, NDArray) == False:
+            if not isinstance(out, NDArray):
-            if isinstance(out, NDArray) == False:
+            if not isinstance(out, NDArray):
-    def __div__(self, other):
+    def __truediv__(self, other):
-    def __rdiv__(self, other):
+    def __rtruediv__(self, other):
-    def __idiv__(self, other):
+    def __itruediv__(self, other):
-    """ Perform element-wise addition
+def _ufunc_helper(lhs, rhs, fn_array, fn_scalar, lfn_scalar, rfn_scalar=None):
-        left hand side operand
+    lhs : NDArray or numeric value
-    rhs : Array of float value
+    rhs : NDArray or numeric value
-    out: Array
+    out: NDArray
-            return lhs + rhs
+            return fn_scalar(lhs, rhs)
-            return add(rhs, lhs)
+            if rfn_scalar is None:
-        return NDArray._plus_scalar(lhs, float(rhs))
+        return lfn_scalar(lhs, float(rhs))
-        return NDArray._plus(lhs, rhs)
+        return fn_array(lhs, rhs)
-    # pylint: enable= no-member, protected-access
+    return _ufunc_helper(
-    # pylint: enable= no-member, protected-access
+    return _ufunc_helper(
-    # pylint: enable= no-member, protected-access
+    return _ufunc_helper(
-    # pylint: enable= no-member, protected-access
+    return _ufunc_helper(
-        layer_name = layer_names[layer_idx].replace('/', '_')
+        layer_name = re.sub('[-/]', '_', layer_names[layer_idx])
-        layer_name = layer.name.replace('/', '_')
+        layer_name = re.sub('[-/]', '_', layer.name)
-    prob = proto2symbol(args.caffe_prototxt)
+    prob, input_dim = proto2symbol(args.caffe_prototxt)
-    arg_shapes, output_shapes, aux_shapes = prob.infer_shape(data=(1,3,224,224))
+    arg_shapes, output_shapes, aux_shapes = prob.infer_shape(data=tuple(input_dim))
-            wmat = np.array(layer_blobs[0].data).reshape(layer_blobs[0].num, layer_blobs[0].channels, layer_blobs[0].height, layer_blobs[0].width)
+            wmat_dim = []
-
+        raise Exception('Invalid proto file.')   
-        name = layer[i].name.replace('/', '_')
+        name = re.sub('[-/]', '_', layer[i].name)
-                kernel_size, stride, stride, not param.bias_term)
+            param_string = convParamToString(layer[i].convolution_param)
-    return symbol_string, output_name
+    return symbol_string, output_name, input_dim
-    sym, output_name = proto2script(proto_file)
+    sym, output_name, input_dim = proto2script(proto_file)
-    return ret
+    return ret, input_dim
-        if img == None:
+        if img is None:
-    def __init__(self, step, factor=1):
+    def __init__(self, step, factor=1, stop_factor_lr=1e-8):
-                         num_update, self.base_lr)
+            if self.base_lr < self.stop_factor_lr:
-    im_size_max = np.min(im_shape[0:2])
+    im_size_max = np.max(im_shape[0:2])
-                                 param.epoch, count, speed, name, value, cls, cls_value, bbox, bbox_value)
+                                 param.epoch, count, speed, name[0], value[0], name[1], value[1], name[2], value[2])
-    def __init__(self, roidb, batch_size=2, shuffle=False, mode='train'):
+    def __init__(self, roidb, ctx, batch_size=2, shuffle=False, mode='train', work_load_list=None):
-        return [(k, v.shape) for k, v in self.data.items()]
+        return [('data', self.data[0].shape), ('rois', self.data[1].shape)]
-        return [(k, v.shape) for k, v in self.label.items()]
+        return [('cls_prob_label', self.label[0].shape),
-                                   pad=self.getpad(), index=self.getindex())
+            if self.mode == 'train':
-        return self.batch_size - self.size % self.batch_size
+        if self.cur + self.batch_size > self.size:
-                          'bbox_loss_outside_weight': self.batch['bbox_outside_weights']}
+            self.data = [mx.nd.array(self.batch['data']), mx.nd.array(self.batch['rois'])]
-        batch = minibatch.get_minibatch(roidb, self.num_classes)
+        cur_to = cur_from + self.batch_size
-        label = labels[0].asnumpy()
+        bbox_loss = preds[1].asnumpy()
-def get_minibatch(roidb, num_classes):
+def get_minibatch(roidb, num_classes, ctx, work_load_list=None):
-    for im_i in range(num_images):
+    if work_load_list is None:
-        batch_index = im_i * np.ones((rois.shape[0], 1))
+        batch_index = idx * np.ones((rois.shape[0], 1))
-
+                 kv_store='local',
-                 optimizer='sgd', **kwargs):
+                 optimizer='sgd',
-        self.executor = None
+        self.max_data_shape = max_data_shape
-            self.grad_params = {}
+            param_names = []
-                    self.grad_params[name] = mx.nd.zeros(shape, self.ctx)
+                    param_names.append(name)
-                                       ['epoch', 'nbatch', 'eval_metric', 'cls_metric', 'bbox_metric'])
+
-        eval_metric = mx.metric.create("accuracy")
+        eval_metric = metric.Accuracy()
-            logger.info("                     --->Epoch[%d] Train-%s=%f", epoch, name, value)
+        self.optimizer = mx.optimizer.create(self.optimizer, rescale_grad=(1.0 / config.TRAIN.BATCH_SIZE), **self.kwargs)
-    test_data = ROIIter(roidb, batch_size=1, shuffle=False, mode='test')
+    test_data = ROIIter(roidb, ctx=ctx, batch_size=1, shuffle=False, mode='test')
-              prefix, ctx, begin_epoch, end_epoch, frequent):
+              prefix, ctx, begin_epoch, end_epoch, frequent, kv_store, work_load_list=None):
-    train_data = ROIIter(roidb, batch_size=config.TRAIN.BATCH_IMAGES, shuffle=True, mode='train')
+    train_data = ROIIter(roidb, ctx=ctx,  batch_size=config.TRAIN.BATCH_IMAGES, shuffle=True, mode='train', work_load_list=work_load_list)
-    args, auxs = load_param(pretrained, epoch, convert=True, ctx=ctx)
+    args, auxs = load_param(pretrained, epoch, convert=True, ctx=ctx[0])
-    args['bbox_pred_bias'] = mx.nd.zeros(shape=arg_shape_dict['bbox_pred_bias'], ctx=ctx)
+    args['cls_score_weight'] = mx.random.normal(mean=0, stdvar=0.01, shape=arg_shape_dict['cls_score_weight'], ctx=ctx[0])
-                    learning_rate=0.001, lr_scheduler=mx.lr_scheduler.FactorScheduler(30000, 0.1))
+    solver = Solver(prefix, sym, ctx, begin_epoch, end_epoch, kv_store, args, auxs, momentum=0.9, wd=0.0005,
-                                       mx.nd.array(means, ctx=ctx)
+        arg_params['bbox_pred_weight'] = (arg_params['bbox_pred_weight'].T * mx.nd.array(stds, ctx=ctx[0])).T
-                        default='train', type=str)
+                        default='trainval', type=str)
-                        default=0, type=int)
+    parser.add_argument('--gpus', dest='gpu_ids', help='GPU device to train with',
-    ctx = mx.gpu(args.gpu_id)
+    ctx = [mx.gpu(int(i)) for i in args.gpu_ids.split(',')]
-              args.prefix, ctx, args.begin_epoch, args.end_epoch, args.frequent)
+              args.prefix, ctx, args.begin_epoch, args.end_epoch, args.frequent, args.kv_store, args.work_load_list)
-                d_src[slice_idx].copyto(d_dst)
+                if d_src[slice_idx].shape != d_dst.shape:
-    def __init__(self, sym, arg_names, param_names, ctx, slices, train_data, shared_group=None):
+    def __init__(self, sym, arg_names, param_names,
-                           for k, v in train_data.provide_data + train_data.provide_label}
+            data_shapes = {}
-            labels_slice = [label[islice] for label in labels]
+            n = int(texec.outputs[0].shape[0] / (islice.stop - islice.start))
-                 work_load_list=None, logger=None, sym_gen=None):
+                 work_load_list=None, logger=None, sym_gen=None,
-                                                 self.slices, train_data)
+                                                 self.slices, train_data, max_data_shape)
-                        eval_batch_end_callback=None, sym_gen=None):
+                        eval_batch_end_callback=None, sym_gen=None,
-                                                   logger=logger)
+                                                   logger=logger,
-            if do_reset == True:
+            if do_reset is True:
-            i += 1
+            i += 1
-        eps=1e-5
+        # cuDNN v5 don't allow a small eps of 1e-5
-        self.kwargs["param_idx2name"] = param_idx2name
+        param_idx2name = {}
-        for path, subdirs, files in os.walk(root):
+        for path, subdirs, files in os.walk(root, followlinks=True):
-def make_list(prefix_out, root, recursive, exts, num_chunks, train_ratio):
+def make_list(prefix_out, root, recursive, exts, num_chunks, train_ratio, test_ratio):
-            write_list(prefix_out+str_chunk+'.lst', chunk)
+        sep = int(chunk_size*train_ratio)
-        record = mx.recordio.MXRecordIO(prefix+'.rec', 'w')
+	record = mx.recordio.MXRecordIO(prefix+'.rec', 'w')
-        record = mx.recordio.MXRecordIO(args.prefix+'.rec', 'w')
+	record = mx.recordio.MXRecordIO(args.prefix+'.rec', 'w')
-        an image list or creating one')
+        description='Create an image list or \
-        
+
-                  args.exts, args.chunks, args.train_ratio)
+                  args.exts, args.chunks, args.train_ratio, args.test_ratio)
-
+        files = [f for f in os.listdir('.') if os.path.isfile(f)]
-from .base import NDArrayHandle
+from .base import NDArrayHandle, py_str
-            if not self.activated or not self.re_prog.match(name):
+            if not self.activated or not self.re_prog.match(py_str(name)):
-            self.queue.append((self.step, name, self.stat_func(array)))
+            self.queue.append((self.step, py_str(name), self.stat_func(array)))
-                                     h2h_bias = mx.sym.Variable("l%d_h2h_bias" % i)))
+                                         h2h_bias = mx.sym.Variable("l%d_h2h_bias" % i)))
-                    use_loss=False):
+                    use_loss=False, buckets=None):
-    rnn_sym = lstm_unroll(num_lstm_layer=num_lstm_layer,
+    max_len = max(buckets)
-            input_shapes[name] = (batch_size, )
+        arg_names = rnn_sym.list_arguments()
-    rnn_exec = rnn_sym.bind(default_ctx, args=arg_arrays,
+              assert max_rnn_exec != None
-            assert name not in args_grad
+                            grad_req="add", group2ctx=group2ctx,
-    out_dict = dict(zip(rnn_sym.list_outputs(), rnn_exec.outputs))
+        out_dict = dict(zip(rnn_sym.list_outputs(), rnn_exec.outputs))
-    init_states = [LSTMState(c=arg_dict["l%d_init_c" % i],
+        init_states = [LSTMState(c=arg_dict["l%d_init_c" % i],
-                             h=out_dict["l%d_last_h_output" % i]) for i in range(num_lstm_layer)]
+        seq_data = [rnn_exec.arg_dict["t%d_data" % i] for i in range(seq_len)]
-        seq_labels = [rnn_exec.arg_dict["t%d_label" % i] for i in range(seq_len)]
+        if concat_decode:
-    return LSTMModel(rnn_exec=rnn_exec, symbol=rnn_sym,
+        model = LSTMModel(rnn_exec=rnn_exec, symbol=rnn_sym,
-
+        models[bucket_key] = model
-        nll += -np.sum(np.log(np.maximum(py, eps))) / len(y)
+def calc_nll(seq_label_probs, batch_size, seq_len):
-               num_round, update_period, concat_decode, use_loss,
+               num_round, update_period, concat_decode, batch_size, use_loss,
-    log_period = max(1000 / seq_len, 1)
+    #log_period = max(1000 / seq_len, 1)
-            set_rnn_inputs(m, X_train_batch, begin=begin)
+        for data_batch in X_train_batch:  
-                    train_nll += calc_nll_concat(seq_label_probs, X_val_batch, begin=begin)
+                    train_nll += calc_nll_concat(seq_label_probs, batch_size)
-                    train_nll += calc_nll(seq_label_probs, X_val_batch, begin=begin)
+                    train_nll += calc_nll(seq_label_probs, batch_size, batch_seq_length)
-            nbatch = begin + seq_len
+            nbatch += batch_size
-            set_rnn_inputs(m, X_val_batch, begin=begin)
+        nbatch = 0
-            # probability of each label class, used to evaluate nll
+            # probability of each label class, used to evaluate nll
-                    val_nll += calc_nll_concat(seq_label_probs, X_val_batch, begin=begin)
+                    val_nll += calc_nll_concat(seq_label_probs, batch_size)
-                    val_nll += calc_nll(seq_label_probs, X_val_batch, begin=begin)
+                    val_nll += calc_nll(seq_label_probs, batch_size, batch_seq_length)
-        nbatch = X_val_batch.shape[0]
+            nbatch += batch_size
-num_lstm_layer = 2
+num_lstm_layer = 8
-X_val_batch = replicate_data(X_val, batch_size)
+dic = default_build_vocab("./data/ptb.train.txt")
-ngpu = 1
+# static buckets
-                             seq_len=seq_len,
+                             seq_len=X_train_batch.default_bucket_key,
-                             initializer=mx.initializer.Uniform(0.1),dropout=0.5)
+                             initializer=mx.initializer.Uniform(0.1),dropout=0.5, buckets=buckets)
-    the_vocab = {}
+    the_vocab = {}
-                 seperate_char=' <eos> ', text2id=None, read_content=None):
+                 seperate_char=' <eos> ', text2id=None, read_content=None, model_parallel=False):
-            self.label_buffer.append(label)
+            if not self.model_parallel:
-            label_names = ['softmax_label']
+            
-        self.bucket_curr_idx = [0 for x in self.data]
+        self.bucket_curr_idx = [0 for x in self.data]
-            weight[:] += -lr * (grad + self.wd * weight)
+            weight[:] += -lr * (grad + wd * weight)
-            weight[:] += -lr * (grad + self.wd * weight)
+            weight[:] += -lr * (grad + wd * weight)
-        weight[:] += -lr * (grad / sqrt(history + self.float_stable_eps) + self.wd * weight)
+        weight[:] += -lr * (grad / sqrt(history + self.float_stable_eps) + wd * weight)
-    print("Usage: <source.d> <source.cc> <output> [minumum=0]\n"
+    print("Usage: <source.d> <source.cc> <output> [minimum=0] [android=0]\n"
-    blacklist += ['packet/sse-inl.h', 'emmintrin.h']
+    blacklist += ['packet/sse-inl.h', 'emmintrin.h', 'cblas.h']
-    print >>out, "//===== EXPANDIND: %s =====\n" %x
+    print >>out, "//===== EXPANDING: %s =====\n" %x
-                                 for i, n in enumerate(self._exec_group.param_names)})
+            if update_on_kvstore:
-            force_rebind=False, force_init=False, begin_epoch=0, num_epoch=None):
+            force_rebind=False, force_init=False, begin_epoch=0, num_epoch=None,
-                for name, val in eval_metric.get_name_value():
+                res = self.score(eval_data, validation_metric,
-                c_array(NDArrayHandle, (out.handle,)),
+        check_call(_LIB.MXFuncInvokeEx( \
-        super(ccSGD, self).__init__(**kwargs)
+    def __init__(self, momentum=0.0, rescale_grad=1., clip_gradient=-1., **kwargs):
-            [momentum, kwargs['rescale_grad'], kwargs['clip_gradient']])
+            [momentum, rescale_grad, clip_gradient])
-
+    """CustomOp registry"""
-    def binary_ndarray_function(lhs, rhs, out=None):
+    def binary_ndarray_function(lhs, rhs, out=None, **kwargs):
-                                       c_array(ctypes.c_char_p, [])))
+        check_call(_LIB.MXFuncInvokeEx(
-_registry_ref_holder = []
+class _Registry(object):
-                ('p_backward', c_void_p)
+                ('p_backward', c_void_p),
-                ('p_list_auxiliary_states', c_void_p)
+                ('p_list_auxiliary_states', c_void_p),
-                                          fb_functype(backward_entry), None, None)
+                                          fb_functype(backward_entry),
-                    op_prop._ref_holder.append(op)
+                    _registry.ref_holder[cur] = op
-                                      None, None, None, None, None, None)
+                                      del_functype(delete_entry),
-            _registry_ref_holder.append(op_prop)
+            _registry.ref_holder[cur] = op_prop
-        _registry_ref_holder.append(creator_func)
+        cur = _registry.inc()
-def check_type_consistency(sym, ctx_list):
+def check_consistency(sym, ctx_list, scale=1.0):
-           np.dtype(np.float32): 1e-4,
+           np.dtype(np.float32): 1e-3,
-    init = [np.random.normal(size=arr.shape, scale=1.0) for arr in exe_list[0].arg_arrays]
+    init = [np.random.normal(size=arr.shape, scale=scale) for arr in exe_list[0].arg_arrays]
-            assert_allclose(arr1, arr2, rtol=tol[dtypes[i]], atol=tol[dtypes[i]])
+            try:
-    check_type_consistency(sym, ctx_list)
+    check_consistency(sym, ctx_list)
-    check_type_consistency(sym, ctx_list)
+    check_consistency(sym, ctx_list)
-    check_type_consistency(sym, ctx_list)
+    check_consistency(sym, ctx_list)
-            the braodcast shape
+            the broadcast shape
-                ret = NDArray._broadcast(ret, axis, j)
+        cur_shape = np.array(cur_shape)
-def sum(arr, axis=None, keepdims=False):
+# pylint: disable=too-many-branches
-    # Sanity checks.
+    if 'sum' == typ:
-    elif isinstance(axis, tuple):
+    elif isinstance(axis, tuple) or isinstance(axis, list):
-        raise TypeError('\'%s\' object cannot be interpreted as an integer' % type(axis).__name__)
+        raise TypeError('\'%s\' object is not supported as axis.' % type(axis).__name__)
-            ret = ret.reshape(shape)
+    for i in reversed(axis):
-    def unary_ndarray_function(src, out=None):
+    def unary_ndarray_function(src, out=None, *args, **kwargs):
-                c_array(mx_float, ()), \
+                c_array(mx_float, [args[i] for i in scalar_range]), \
-                c_array(ctypes.c_char_p, [])))
+                ctypes.c_int(len(kwargs)), \
-                c_array(ctypes.c_char_p, [str(i) for i in kwargs.values()])))
+                c_array(ctypes.c_char_p, [key.encode('ascii') for key in kwargs.keys()]), \
-                         check_symbolic_forward, reldiff)
+                         check_symbolic_forward, reldiff, _np_reduce)
-
+def test_reduce():
-    test_sum_mid_internal()
+    test_reduce()
-
+#!/usr/bin/env python
-def make_text_cnn(sentence_size, num_embed, batch_size, num_label=2, filter_list=[3, 4, 5], num_filter=100, dropout=0.):
+def make_text_cnn(sentence_size, num_embed, batch_size, vocab_size,
-    # embed_layer = mx.sym.Reshape(data=embed_layer, target_shape=(1, 1, sentence_size, num_embed))
+    if not with_embedding:
-        convi = mx.sym.Convolution(data=input_x, kernel=(filter_size, num_embed), num_filter=num_filter)
+        convi = mx.sym.Convolution(data=conv_input, kernel=(filter_size, num_embed), num_filter=num_filter)
-    cnn = make_text_cnn(sentence_size, num_embed, batch_size=batch_size, dropout=dropout)
+def setup_cnn_model(ctx, batch_size, sentence_size, num_embed, vocab_size,
-    input_shapes['data'] = (batch_size, 1, sentence_size, num_embed)
+    if with_embedding:
-def train_cnn(model, X_train_batch, y_train_batch, X_dev_batch, y_dev_batch, batch_size, optimizer='rmsprop', max_grad_norm=5.0, learning_rate=0.001, epoch=200):
+def train_cnn(model, X_train_batch, y_train_batch, X_dev_batch, y_dev_batch, batch_size,
-            print >> sys.stderr, 'reset learning rate to %g' % opt.lr
+            print >> logs, 'reset learning rate to %g' % opt.lr
-        print >> sys.stderr, 'Iter [%d] Train: Time: %.3f, Training Accuracy: %.3f' % (iteration, toc - tic, num_correct * 100 / float(num_total))
+        train_time = toc - tic
-        # eval on dev set
+        # evaluate on dev set
-        print >> sys.stderr, 'Dev Accuracy thus far: %.3f' % ( num_correct * 100 / float(num_total) )
+        dev_acc = num_correct * 100 / float(num_total)
-    cnn_model = setup_cnn_model(mx.gpu(0), batch_size, sentence_size, num_embed, dropout=0.5)
+    cnn_model = setup_cnn_model(mx.gpu(1), batch_size, sentence_size, num_embed, dropout=0.5)
-    main()
+    train_without_pretrained_embedding()
-            outputs = [out[0:out.shape[0]-pad] for out in self.get_outputs()]
+            outputs = [out[0:out.shape[0]-pad].copy() for out in self.get_outputs()]
-__version__ = "0.5.0"
+__version__ = "0.7.0"
-                                      POINTER(c_int), POINTER(c_int), POINTER(CustomOpInfo), c_void_p)
+                                      POINTER(c_int), POINTER(c_int),
-                    ret[0] = CustomOpInfo(fb_functype(forward_entry), fb_functype(backward_entry), None, None)
+                    ret[0] = CustomOpInfo(fb_functype(forward_entry),
-                                POINTER(c_int), c_bool)
+                                POINTER(c_int), c_bool, c_void_p)
-        list_functype = CFUNCTYPE(c_bool, POINTER(POINTER(POINTER(c_char))))
+                                   POINTER(POINTER(mx_uint)), c_void_p)
-                                  c_int_p, POINTER(c_int_p))
+                                  c_int_p, POINTER(c_int_p), c_void_p)
-                                      POINTER(c_int), POINTER(c_int), POINTER(CustomOpInfo))
+                                      POINTER(c_int), POINTER(c_int), POINTER(CustomOpInfo), c_void_p)
-                ('list_auxiliary_states', list_functype)
+                ('list_auxiliary_states', list_functype),
-                                  tensor_shapes):
+                                  tensor_shapes, _):
-            def list_outputs_entry(out):
+            def list_outputs_entry(out, _):
-            def list_arguments_entry(out):
+            def list_arguments_entry(out, _):
-            def list_auxiliary_states_entry(out):
+            def list_auxiliary_states_entry(out, _):
-            def declare_backward_dependency_entry(out_grad, in_data, out_data, num_dep, deps):
+            def declare_backward_dependency_entry(out_grad, in_data, out_data, num_dep, deps, _):
-            def create_operator_entry(ctx, num_inputs, shapes, ndims, dtypes, ret):
+            def create_operator_entry(ctx, num_inputs, shapes, ndims, dtypes, ret, _):
-                    def forward_entry(num_ndarray, ndarraies, tags, reqs, is_train):
+                    def forward_entry(num_ndarray, ndarraies, tags, reqs, is_train, _):
-                    def backward_entry(num_ndarray, ndarraies, tags, reqs, is_train):
+                    def backward_entry(num_ndarray, ndarraies, tags, reqs, is_train, _):
-                    ret[0] = CustomOpInfo(fb_functype(forward_entry), fb_functype(backward_entry))
+                    ret[0] = CustomOpInfo(fb_functype(forward_entry), fb_functype(backward_entry), None, None)
-                                      list_functype(list_auxiliary_states_entry))
+                                      list_functype(list_auxiliary_states_entry),
-    test_scalarop();
+    test_scalarop()
-    # merge the dcit provided by user and the default one
+    # merge the dict provided by user and the default one
-                                                   node["param"]["num_filter"])
+            label = r"Convolution\n%sx%s/%s, %s" % (_str2tuple(node["param"]["kernel"])[0],
-            label = "FullyConnected\n%s" % node["param"]["num_hidden"]
+            label = r"FullyConnected\n%s" % node["param"]["num_hidden"]
-            label = "%s\n%s" % (op, node["param"]["act_type"])
+            label = r"%s\n%s" % (op, node["param"]["act_type"])
-                                               _str2tuple(node["param"]["stride"])[0])
+            label = r"Pooling\n%s, %sx%s/%s" % (node["param"]["pool_type"],
-    # case 5: test old api
+
-    ctx = mx.gpu(0), symbol = mlp, num_epoch = 20,
+    ctx = mx.cpu(0), symbol = mlp, num_epoch = 20,
-model.fit(X=train, eval_data=val)
+model.fit(X=train, eval_data=val,
-    """Base class for operator property class implemented in python
+    """Base class for operator property class implemented in python.
-            fout.write('%d \t %d \t %s\n'%(i, image_list[i][1], image_list[i][0]))
+            fout.write('%d\t%d\t%s\n'%(i, image_list[i][1], image_list[i][0]))
-    parser.add_argument('--exts', type=list, default=['.jpeg','.jpg'],
+    parser.add_argument('--exts', type=str, nargs='+', default=['.jpeg','.jpg'],
-        factor = 1
+        hw_scale = 1.
-    def simple_bind(self, ctx, grad_req='write', type_dict=None, **kwargs):
+    def simple_bind(self, ctx,
-                                                                              arg_shapes)]
+        if group2ctx is not None:
-            for name, shape, dtype in zip(self.list_arguments(), arg_shapes, arg_types):
+            for name, shape, dev, dtype in zip(
-                    grad_ndarrays[name] = zeros(shape, ctx, dtype=dtype)
+                    grad_ndarrays[name] = zeros(shape, dev, dtype=dtype)
-        executor = self.bind(ctx, arg_ndarrays, grad_ndarrays, grad_req, aux_ndarrays)
+        aux_ndarrays = [zeros(shape, dev, dtype=dtype)
-            attr = self.sym.list_attr()
+            attr = self.sym.list_attr(recursive=True)
-            attr = self.sym.list_attr()
+            attr = self.sym.list_attr(recursive=True)
-                  'existing optimizer %s.%s'%(
+            print('WARNING: New optimizer %s.%s is overriding '
-            attr = self.sym.list_attr()
+            attr = self.sym.list_attr(recursive=True)
-            attr = self.sym.list_attr()
+            attr = self.sym.list_attr(recursive=True)
-#convenience wrapper for Optimizer.Register
+# convenience wrapper for Optimizer.Register
-                     + normal(0, math.sqrt(lr), weight.shape, weight.context)
+        weight[:] += - lr/2 * (grad + wd * weight) + normal(0, math.sqrt(lr),
-        return zeros(weight.shape, weight.context) # history
+        return zeros(weight.shape, weight.context)  # history
-                zeros(weight.shape, weight.context)) # accumulated delta
+        return (zeros(weight.shape, weight.context),  # accumulated g
-        current_delta = sqrt(acc_delta + self.epsilon) / sqrt(acc_g + self.epsilon)  * grad
+        current_delta = sqrt(acc_delta + self.epsilon) / sqrt(acc_g + self.epsilon) * grad
-#backward compatibility wrapper for Optimizer.CreateOptimizer
+# backward compatibility wrapper for Optimizer.CreateOptimizer
-        """Get all attributes from the symbol and its descendents.
+    def list_attr(self, recursive=False):
-            Default `False`. When `shallow` is `False`, list recursively all the
+        recursive : bool
-            the symbol names to avoid conflicts. If `True`, then only attributes
+            the symbol names to avoid conflicts. If `False`, then only attributes
-        f_handle = _LIB.MXSymbolListAttrShallow if shallow else _LIB.MXSymbolListAttr
+        f_handle = _LIB.MXSymbolListAttr if recursive else _LIB.MXSymbolListAttrShallow
-               'symbol: Symbol\n'+
+               'symbol: Symbol\n' +
-    if  isinstance(base, Symbol) and isinstance(exp, Number):
+    if isinstance(base, Symbol) and isinstance(exp, Number):
-    if  isinstance(base, Number) and isinstance(exp, Symbol):
+    if isinstance(base, Number) and isinstance(exp, Symbol):
-    if  isinstance(base, Number) and isinstance(exp, Number):
+    if isinstance(base, Number) and isinstance(exp, Number):
-    if  isinstance(left, Symbol) and isinstance(right, Number):
+    if isinstance(left, Symbol) and isinstance(right, Number):
-    if  isinstance(left, Number) and isinstance(right, Symbol):
+    if isinstance(left, Number) and isinstance(right, Symbol):
-    if  isinstance(left, Number) and isinstance(right, Number):
+    if isinstance(left, Number) and isinstance(right, Number):
-    if  isinstance(left, Symbol) and isinstance(right, Number):
+    if isinstance(left, Symbol) and isinstance(right, Number):
-    if  isinstance(left, Number) and isinstance(right, Symbol):
+    if isinstance(left, Number) and isinstance(right, Symbol):
-    if  isinstance(left, Number) and isinstance(right, Number):
+    if isinstance(left, Number) and isinstance(right, Number):
-    assert op.list_attr(shallow=True) == {'mood': 'so so'}
+    assert op.list_attr(recursive=True) == {'data_mood': 'angry', 'conv_mood': 'so so',
-from .base import c_array, c_str, mx_uint, mx_float, ctypes2numpy_shared, NDArrayHandle
+from .base import c_array, c_str, mx_uint, mx_float, ctypes2numpy_shared, NDArrayHandle, py_str
-                out_data = [out_data[i] for i in xrange(len(self.list_outputs()))]
+                out_grad = [out_grad[i] for i in range(len(self.list_outputs()))]
-            assert op_type == reg_name
+            assert py_str(op_type) == reg_name
-                    out_data = [out_data[i] for i in xrange(len(op_prop.list_outputs()))]
+                    out_grad = [out_grad[i] for i in range(len(op_prop.list_outputs()))]
-    assert(acc1 > 0.96)
+    assert(acc1 > 0.94)
-    for i in range(shape[0]/2):
+    for i in range(int(shape[0]/2)):
-    assert reldiff(grad0[shape[0]/2:], grad1[shape[0]/2:]) < 1e-5
+    assert(abs(np.sum(grad1[:int(shape[0]/2)])) < 1e-5)
-    
+
-    
+
-            test_run_convolution_dilated_impulse_response(dil=dil, kernel_shape=ks) 
+            test_run_convolution_dilated_impulse_response(dil=dil, kernel_shape=ks)
-        This is useful in RNN, where the states are also produced 
+        This is useful in RNN, where the states are also produced
-        This is useful in RNN, where the states are also produced 
+        This is useful in RNN, where the states are also produced
-    
+
-    init_h = [('l%d_init_h'%l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]
+    init_c = [('l%d_init_c' % l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]
-            "has_labels":True
+            "separate_lines": True,
-    
+
-    
+
-
+import mxnet as mx
-
+from speechSGD import speechSGD
-                lr_scheduler.momentum = np.power(np.power(momentum, 1.0/800), data_batch.effective_sample_count)
+                lr_scheduler.momentum = np.power(np.power(momentum, 1.0/(data_train.batch_size * truncate_len)), data_batch.effective_sample_count)
-    args.config.write(sys.stdout)
+    num_hidden_proj = args.config.getint('arch', 'num_hidden_proj')
-    init_h = [('l%d_init_h'%l, (batch_size, num_hidden/2)) for l in range(num_lstm_layer)]
+    if num_hidden_proj > 0:
-            sym = lstm_unroll(num_lstm_layer, seq_len, feat_dim, num_hidden=num_hidden, num_label=label_dim, take_softmax=True)
+            sym = lstm_unroll(num_lstm_layer, seq_len, feat_dim, num_hidden=num_hidden, 
-            sym = lstm_unroll(num_lstm_layer, seq_len, feat_dim, num_hidden=num_hidden, num_label=label_dim, take_softmax=False)
+            sym = lstm_unroll(num_lstm_layer, seq_len, feat_dim, num_hidden=num_hidden, 
-                          num_label=label_dim, output_states=True)
+                          num_label=label_dim, output_states=True, num_hidden_proj=num_hidden_proj)
-def lstm(num_hidden, indata, prev_state, param, seqidx, layeridx, dropout=0.):
+def lstm(num_hidden, indata, prev_state, param, seqidx, layeridx, dropout=0., num_hidden_proj=0):
-    return LSTMState(c=next_c, h=proj_next_h)
+    if num_hidden_proj > 0:
-                num_hidden, num_label, dropout=0., output_states=False, take_softmax=True):
+                num_hidden, num_label, dropout=0., output_states=False, take_softmax=True, num_hidden_proj=0):
-                dp=0.
+            if i == 0:
-                              seqidx=seqidx, layeridx=i, dropout=dp)
+                              seqidx=seqidx, layeridx=i, dropout=dp, num_hidden_proj=num_hidden_proj)
-    hidden_final = mx.sym.Reshape(hidden_concat, target_shape=(0, num_hidden/2))
+    if num_hidden_proj > 0:
-    init_h = [('l%d_init_h'%l, (batch_size, num_hidden/2)) for l in range(num_lstm_layer)]
+    if num_hidden_proj > 0:
-    def __init__(self, dynamic_lr, effective_sample_count=1):
+    def __init__(self, dynamic_lr, effective_sample_count=1, momentum=0.9, optimizer="sgd"):
-        return self.dynamic_lr / self.effective_sample_count
+        if self.optimizer == "speechSGD":
-
+    optimizer = args.config.get('train', 'optimizer')
-    lr_scheduler = SimpleLRScheduler(learning_rate)
+    lr_scheduler = SimpleLRScheduler(learning_rate, momentum=momentum, optimizer=optimizer)
-        if optimizer == "sgd":
+        if optimizer == "sgd" or optimizer == "speechSGD":
-                              num_label=label_dim)
+                              num_label=label_dim, num_hidden_proj=num_hidden_proj)
-                          num_label=label_dim, output_states=True)
+                          num_label=label_dim, output_states=True, num_hidden_proj=num_hidden_proj)
-
+    
-
+'''
-        """Get all attributes from the symbol"""
+    def list_attr(self, shallow=False):
-            self.handle, ctypes.byref(size), ctypes.byref(pairs)))
+        f_handle = _LIB.MXSymbolListAttrShallow if shallow else _LIB.MXSymbolListAttr
-
+    test_list_attr()
-                            reqs = [req_enum[reqs[i]] for i in range(len(tensors[0]))]
+                            reqs = [req_enum[reqs[i]] for i in range(len(tensors[1]))]
-
+def test_run_convolution_dilated_impulse_response(dil=(1,1), kernel_shape=(3,3), verbose=False):
-    def forward(self, is_train, req, in_data, out_data):
+    def forward(self, is_train, req, in_data, out_data, aux):
-    def backward(self, req, out_grad, in_data, out_data, in_grad):
+    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):
-    def __init__(self, need_top_grad=False, **kwargs):
+    """Base class for operator property class implemented in python
-    
+
-            in the same order as declared in list_arguments.
+            in the same order as declared in list_outputs.
-        return in_shape, [in_shape[0]]
+        return in_shape, [in_shape[0]], []
-            declared in list_arguments.
+        arguments : list
-    def create_operator(self, ctx, shapes, dtypes):
+    def create_operator(self, ctx, in_shapes, in_dtypes):
-                deps[0] = rdeps
+def register(reg_name):
-                return False
+        infer_functype = CFUNCTYPE(c_bool, c_int, POINTER(c_int),
-            return True
+        creator_functype = CFUNCTYPE(c_bool, c_char_p, c_int, POINTER(c_char_p),
-register(CustomOpProp)
+register("custom_op")(CustomOpProp)
-
+        retcode = subprocess.call("cp -rf doxygen/html _build/html/doxygen", shell=True)
-            if not n.endswith('_weight'):
+            if not (n.endswith('_weight') or n.endswith('_gamma')):
-import re
+
-        if arg_shapes == None or arg_types == None:
+        if arg_shapes is None or arg_types is None:
-        req_map = {'null' : 0, 'write' : 1, 'add' : 3}
+        req_map = {'null': 0, 'write': 1, 'add': 3}
-def Variable(name, attr=None):
+def Variable(name, attr=None, shape=None):
-       Name of the variable.
+        Name of the variable.
-       Additional attributes to set on the variable.
+        Additional attributes to set on the variable.
-import numpy as np
+
-                                     h2h_bias = mx.sym.Variable("l%d_h2h_bias" % i)))
+        param_cells.append(LSTMParam(i2h_weight=mx.sym.Variable("l%d_i2h_weight" % i),
-                dp=0.
+            if i == 0:
-    sm = mx.sym.SoftmaxOutput(data=pred, label=label, ignore_label=0, use_ignore=True, name='softmax')
+    sm = mx.sym.SoftmaxOutput(data=pred, label=label, ignore_label=0,
-    sm = mx.sym.SoftmaxOutput(data=pred, label=label, ignore_label=0, use_ignore=True, name='softmax')
+    sm = mx.sym.SoftmaxOutput(data=pred, label=label, ignore_label=0, 
-    labels = labels.T.reshape((-1,))
+    labels = labels.reshape((-1,))
-    batch_end_callbacks = [mx.callback.Speedometer(batch_size, 100)]
+    batch_end_callbacks = [mx.callback.Speedometer(batch_size, 
-                               allow_extra_outputs=eval_allow_extra)
+    eval_metric = [mx.metric.np(Acc_exclude_padding, allow_extra_outputs=eval_allow_extra),
-                                            'clip_gradient': clip_gradient})
+    def reset_optimizer():
-    import ConfigParser as configparser
+from config_util import parse_args, get_checkpoint_path, parse_contexts
-
+
-            contexts[i] = mx.context.cpu(int(ctx[3:]))
+    contexts = parse_contexts(args)
-                                   **dict(optimizer_params))
+                                   **optimizer_params)
-    def __init__(self, data_names, data, label_names, label, bucket_key, utt_id=None):
+    def __init__(self, data_names, data, label_names, label, bucket_key,
-                                     min(utt_inside_idx[i]+self.truncate_len, fea_utt.shape[0]))
+                                     min(utt_inside_idx[i]+self.truncate_len,
-                        np_label_buffer[i][n_take] = 0
+                        np_data_buffer[i][n_take:] = 0
-                                     utt_id=utt_id_buffer)
+                                     utt_id=utt_id_buffer,
-                else np.zeros(((len(x)/self.batch_size + 1) * self.batch_size, buckets[i], self.feat_dim)) 
+                if len(x) % self.batch_size == 0
-                 else np.zeros(((len(x)/self.batch_size + 1) * self.batch_size, buckets[i])) 
+                 if len(x) % self.batch_size == 0
-                                     self.buckets[i_bucket], utt_id)
+                                     self.buckets[i_bucket], utt_id,
-    """A simple lr schedule that simply return `base_lr`. We will set `base_lr`
+    """A simple lr schedule that simply return `dynamic_lr`. We will set `dynamic_lr`
-        self.seq_len = seq_len
+    def __init__(self, dynamic_lr, effective_sample_count=1):
-        return self.base_lr / self.batch_size / self.seq_len
+        return self.dynamic_lr / self.effective_sample_count
-    lr_scheduler = SimpleLRScheduler(learning_rate, batch_size)
+    lr_scheduler = SimpleLRScheduler(learning_rate)
-                lr_scheduler.seq_len = data_batch.bucket_key
+            if data_batch.effective_sample_count is not None:
-        if n_epoch > 0 and lr_scheduler.base_lr > decay_bound and curr_acc < last_acc:
+        if n_epoch > 0 and lr_scheduler.dynamic_lr > decay_bound and curr_acc < last_acc:
-                         lr_scheduler.base_lr, lr_scheduler.base_lr / float(decay_factor))
+                         lr_scheduler.dynamic_lr, lr_scheduler.dynamic_lr / float(decay_factor))
-            lr_scheduler.base_lr /= decay_factor
+            lr_scheduler.dynamic_lr /= decay_factor
-            state[:] = 0
+            state[:] = 0.1
-
+                        state[i:i+1] = 0.1
-
+                        np_data_buffer[i][n_take:] = 0
-            optimizer = opt.create(optimizer, rescale_grad=(1.0/batch_size),
+            optimizer = opt.create(optimizer,
-        if self.param.has_key(name):
+        if name in self.param:
-    def __init__(self, data, label, pad, index,
+    def __init__(self, data, label, pad=None, index=None,
-    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8,
+    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,
-        self.time_first_index = None  # time is incremented only on the first index
+        t = self._index_update_count[index]
-            self.time += 1
+        grad *= self.rescale_grad
-        beta_1t = self.beta1 * self.decay_factor ** (t1 - 1)
+        mean[:] = self.beta1 * mean + (1. - self.beta1) * grad
-            grad = clip(grad, -self.clip_gradient, self.clip_gradient)
+        coef1 = 1. - self.beta1**t
-        variance[:] = variance_t
+            weight[:] -= (lr * wd) * weight
-    pass
+    _LUAJIT = None
-    ################################################################################
+    pred = mx.sym.Reshape(pred, target_shape=(0, seq_len, num_label))
-    pred = mx.sym.FullyConnected(data=hidden_concat, num_hidden=num_label,
+    hidden_all = [mx.sym.Reshape(x, target_shape=(0, 1, num_hidden))
-    label = mx.sym.Reshape(data=label, target_shape=(0,))
+    #label = mx.sym.transpose(data=label)
-                                         tgs[t_start:t_start+t_take]+1))
+                    self.data[i_bucket].append((feats[t_start:t_start+t_take],
-                    self.data[i].append(feats[t_start:t_start+t_take])
+                    self.data[i_bucket].append(feats[t_start:t_start+t_take])
-                    sentence[1][:delay] = [sentence[1][0]]*delay
+                    sentence[1][:delay] = sentence[1][0] # broadcast assignment
-                    self.utt_id[i].append(utt_id);
+
-                    # we just ignore the sentence it is longer than the maximum bucket size here
+                t_start -= T_OVERLAP
-                 has_label=True, do_shuffling=True):
+                 has_label=True, do_shuffling=True, pad_zeros=False):
-                    np_label_buffer[i][n_take] = 0
+                if is_pad[i] and self.pad_zeros:
-            init_states, delay=5, feat_dim=40,  n_batch=None,
+            init_states, delay=5, feat_dim=40,
-                if len(x) % self.batch_size == 0  else np.zeros(((len(x)/self.batch_size + 1) *self.batch_size, buckets[i], self.feat_dim)) for i, x in enumerate(self.data)]
+                if len(x) % self.batch_size == 0 
-                if len(x) % self.batch_size == 0  else np.zeros(((len(x)/self.batch_size + 1) *self.batch_size, buckets[i])) for i, x in enumerate(self.data)]
+                 if len(x) % self.batch_size == 0 
-        #label = [np.zeros((len(x), buckets[i])) for i, x in enumerate(self.data)]
+
-                                         do_shuffling=False)
+                                         do_shuffling=False, pad_zeros=True)
-            logging.info('Epoch[%d] !!! LR decay: %g => %g',
+            logging.info('Epoch[%d] !!! LR decay: %g => %g', n_epoch,
-    for eval_batch in eval_batch:
+    for eval_batch in eval_data:
-    init_scale = args.config.getfloat('training', 'init_scale')
+    init_type = getattr(mx.initializer, args.config.get('train', 'initializer'))
-    module.init_params(initializer=mx.initializer.Uniform(0.1))
+    module.init_params(initializer=get_initializer(args))
-                                            'momentum': momentum})
+                                            'momentum': momentum,
-        
+
-        module.score(data_val, eval_metric, epoch=n_epoch)
+        score_with_state_forwarding(module, data_val, eval_metric)
-                                            getattr(args, arg_name)), file=sys.stderr)
+                sys.stderr.write('!! CMDLine overwriting %s.%s:\n' % (sec, name))
-    print("="*80, file=sys.stderr)
+    sys.stderr.write("="*80+"\n")
-    module.init_params(initializer=mx.initializer.Uniform(0.01))
+    module.init_params(initializer=mx.initializer.Uniform(0.1))
-from utils.utils import to_bool
+from utils import to_bool
-                print('!! CMDLine overwriting %s.%s:' % (sec, name))
+                print('!! CMDLine overwriting %s.%s:' % (sec, name), file=sys.stderr)
-                                            getattr(args, arg_name)))
+                                            getattr(args, arg_name)), file=sys.stderr)
-    print("="*80)
+    print("="*80, file=sys.stderr)
-from feat_readers import stats
+from .feat_readers.common import *
-                    print "List has mis-matched number of feature files and label files"
+                    print("List has mis-matched number of feature files and label files")
-                        print "Each line in the train and eval lists must contain feature file and label file separated by space character"
+                        print(lines[i])
-                
+
-       
+
-    
+
-    print featureStats.GetInvStd()
+    print("THIS IS THE MEAN: ")
-    parser.add_argument("configfile", help="config file for training parameters")
+    parser.add_argument("--configfile", help="config file for training parameters")
-    args.config = cfg
+    if args.configfile is not None:
-    eval_metric = mx.metric.np(Acc_exclude_padding, 
+    eval_metric = mx.metric.np(Acc_exclude_padding,
-import info
+from . import info
-        print "Using custom logger"
+        print("Using custom logger")
-    print "***", act_str
+    print("***", act_str)
-    except Exception, e:
+    except Exception:
-        print "Not a pickled file... try to load as text format: " + filename
+        print("Not a pickled file... try to load as text format: " + filename)
-                 feat_dim=40, data_name='data', label_name='label',
+                 feat_dim=40, data_name='data', label_name='softmax_label',
-        self.data = [mx.nd.zeros(batch_size, truncate_len, feat_dim)]
+        self.data = [mx.nd.zeros((batch_size, truncate_len, feat_dim))]
-            self.label = [mx.nd.zeros(batch_size, truncate_len)]
+            self.label = [mx.nd.zeros((batch_size, truncate_len))]
-                tgs[:delay] = tgs[0] # boradcast assign
+                tgs[self.delay:] = tgs[:-self.delay]
-        sys.stderr.write('%d utterances loaded...\n' % len(self.utt_ids))
+            seq_len_tot += feats.shape[0]
-        if do_shuffling:
+        if self.do_shuffling:
-                        state[i] = 0
+                        state[i:i+1] = 0
-                utt_inside_idx[i] += len(idx_take)
+                n_take = idx_take.stop - idx_take.start
-    eval_metric  = mx.metric.np(Acc_exclude_padding)
+    eval_allow_extra = True if training_method == METHOD_TBPTT else False
-                                         truncate_len=truncate_len, feat_dim=feat_dim)
+                                         truncate_len=truncate_len, feat_dim=feat_dim,
-        module = mx.mod.Module(sym, context=contexts)
+        data_names = [x[0] for x in data_train.provide_data]
-
+    allow_extra_outputs : bool
-    def __init__(self, feval, name=None):
+    def __init__(self, feval, name=None, allow_extra_outputs=False):
-        check_label_shapes(labels, preds)
+        if not self._allow_extra_outputs:
-def np(numpy_feval, name=None):
+def np(numpy_feval, name=None, allow_extra_outputs=False):
-
+    allow_extra_outputs : bool
-    return CustomMetric(feval, name)
+    return CustomMetric(feval, name, allow_extra_outputs)
-                idx_take = utt_inside_idx[i]:min(utt_inside_idx[i]+self.truncate_len, fea_utt.shape[0])
+                idx_take = slice(utt_inside_idx[i],
-                    # TODO: implement it here
+                    for state in self.init_state_arrays:
-                    if next_utt_idx >= len(self.features):
+                    if is_pad[i]:
-    eval_metric  = mx.metric.np(Acc_exclude_padding)
+
-            if training_method == 'bucketing':
+            if training_method == METHOD_BUCKETING:
-    if training_method == 'bucketing':
+    if training_method == METHOD_BUCKETING:
-    elif training_method == 'truncated-bptt':
+    elif training_method == METHOD_TBPTT:
-            data_name='data', label_name='label', has_label=True):
+            data_name='data', label_name='softmax_label', has_label=True):
-            (feats, tgts, utt_id) = self.train_sets.load_next_seq();
+            (feats, tgts, utt_id) = self.train_sets.load_next_seq()
-        self.provide_label = [('softmax_label', (self.batch_size, self.default_bucket_key))]
+        self.provide_data = [(data_name, (batch_size, self.default_bucket_key, self.feat_dim))] + init_states
-        label_names = ['softmax_label']
+        data_names = [self.data_name] + init_state_names
-                num_hidden, num_label, dropout=0.):
+                num_hidden, num_label, dropout=0., output_states=False):
-from io_util import BucketSentenceIter, DataReadStream
+from io_util import BucketSentenceIter, TruncatedSentenceIter, DataReadStream
-            data_name='data', label_name='label', has_label='True'):
+            data_name='data', label_name='label', has_label=True):
-        data = [np.zeros((len(x), buckets[i], self.feat_dim)) 
+        data = [np.zeros((len(x), buckets[i], self.feat_dim))
-        label = [np.zeros((len(x), buckets[i])) 
+        label = [np.zeros((len(x), buckets[i]))
-                    sentence[1][:delay] = [sentence[1][0]]*delay 
+                    sentence[1][:delay] = [sentence[1][0]]*delay
-            logging.info("%s:%s", name, val)
+            logging.info("Epoch[%d] Dev-%s=%f", n_epoch, name, val)
-                always_output_list=False, as_iterator=False):
+                always_output_list=False):
-
+import sys
-    def __init__(self, data_names, data, label_names, label, bucket_key):
+    def __init__(self, data_names, data, label_names, label, bucket_key, utt_id=None):
-            data_name='data', label_name='label'):
+            data_name='data', label_name='label', has_label='True'):
-        print("Loading data...")
+        sys.stderr.write("Loading data...\n")
-            if tgts is None:
+            if tgts is None and self.has_label:
-                    self.data[i].append((feats,tgts+1))
+                    if self.has_label:
-            # bucket size here
+                    # we just ignore the sentence it is longer than the maximum bucket size here
-                label[i_bucket][j, :len(sentence[1])] = sentence[1]
+                if self.has_label:
-        print("Summary of dataset ==================")
+        sys.stderr.write("Summary of dataset ==================\n")
-            print("bucket of len %3d : %d samples" % (bkt, sz))
+            sys.stderr.write("bucket of len %3d : %d samples\n" % (bkt, sz))
-
+            utt_id = np.array(self.utt_id[i_bucket])[idx]
-                                     self.buckets[i_bucket])
+                                     self.buckets[i_bucket], utt_id)
-def Acc_exlude_padding(labels, preds):
+def Acc_exclude_padding(labels, preds):
-    batch_end_callbacks = [mx.callback.Speedometer(batch_size, 2)]
+    batch_end_callbacks = [mx.callback.Speedometer(batch_size, 100)]
-    eval_metric  = mx.metric.np(Acc_exlude_padding)
+    eval_metric  = mx.metric.np(Acc_exclude_padding)
-            self.logger.info('Epoch[%d] Train-%s=%f', n_epoch, name, val)
+            logging.info('Epoch[%d] Train-%s=%f', n_epoch, name, val)
-            last_params = model.get_params
+            last_params = module.get_params()
-    #    logging.info('validation-%s=%f', name, val)
+import mxnet as mx
-        data = [np.zeros((len(x), buckets[i], self.feat_dim)) if len(x) % self.batch_size == 0  else np.zeros(((len(x)/self.batch_size + 1) *self.batch_size, buckets[i], self.feat_dim)) for i, x in enumerate(self.data)]
+        data = [np.zeros((len(x), buckets[i], self.feat_dim)) 
-        label = [np.zeros((len(x), buckets[i])) if len(x) % self.batch_size == 0  else np.zeros(((len(x)/self.batch_size + 1) *self.batch_size, buckets[i])) for i, x in enumerate(self.data)]
+        label = [np.zeros((len(x), buckets[i])) 
-import configparser
+
-from io_util import BucketSentenceIter
+from lstm import lstm_unroll
-    num_lstm_layer = int(args.config.get('arch', 'num_lstm_layer'))
+    batch_size = args.config.getint('train', 'batch_size')
-    feat_dim = int(args.config.get('data', 'xdim'))
+    feat_dim = args.config.getint('data', 'xdim')
-    def __init__(self, base_lr):
+    def __init__(self, base_lr, batch_size, seq_len=1):
-    eval_metric  = mx.metric.np(Acc_exlude_padding)
+        return self.base_lr / self.batch_size / self.seq_len
-    batch_end_callbacks = [mx.callback.Speedometer(batch_size, 100)]
+    batch_end_callbacks = [mx.callback.Speedometer(batch_size, 2)]
-    decay_bound = float(args.config.get('train', 'decay_lower_bound'))
+    num_epoch = args.config.getint('train', 'num_epoch')
-        for nbatch, data_batch in enumerate(train_data):
+        for nbatch, data_batch in enumerate(data_train):
-                                             locals=locals())
+            batch_end_params = mx.model.BatchEndParam(epoch=n_epoch, nbatch=nbatch,
-        train_data.reset()
+        data_train.reset()
-    training_method = args.config.get('train', 'method', fallback='bucketing')
+    training_method = args.config.get('train', 'method')
-    contexts = re.split(r'\W+', args.config.get('train', 'context', fallback='gpu0'))
+    contexts = re.split(r'\W+', args.config.get('train', 'context'))
-    label_dim = int(args.config.get('data', 'ydim'))
+    batch_size = args.config.getint('train', 'batch_size')
-        do_training(args, module, data_train, data_val)
+        do_training(training_method, args, module, data_train, data_val)
-import math
+
-    embed_weight=mx.sym.Variable("embed_weight")
+                num_hidden, num_label, dropout=0.):
-                                      h2h_bias = mx.sym.Variable("l%d_h2h_bias" % i)))
+                                     i2h_bias = mx.sym.Variable("l%d_i2h_bias" % i),
-        data = mx.sym.Variable("t%d_data" % seqidx)
+    data = mx.sym.Variable('data')
-                                  name="t%d_embed" % seqidx)
+    dataSlice = mx.sym.SliceChannel(data=data, num_outputs=seq_len, squeeze_axis=1)
-                     param_blocks=param_blocks)
+        hidden_all.append(hidden)
-import collections
+    hidden_concat = mx.sym.Concat(*hidden_all, dim=0)
-    return result
+    ################################################################################
-    return population[idx]
+    label = mx.sym.transpose(data=label)
-    return outputs_batch
+    #label_slice = mx.sym.SliceChannel(data=label, num_outputs=seq_len)
-            
+
-           
+
-        
+
-        
+
-                sentence[1][:delay] = [sentence[1][0]]*delay 
+                sentence[1][:delay] = [sentence[1][0]]*delay
-            label[:] = self.label[i_bucket][idx] 
+            label[:] = self.label[i_bucket][idx]
-    
+
-    
+
-   
+
-    
+
-    factor   = 5
+    factor   = 2
-                    cache[name].copyto(arr)
+                    cache_arr = cache[name]
-
+from .symbol_doc import SymbolDoc
-
+    extra_doc = "\n" + '\n'.join([x.__doc__ for x in type.__subclasses__(SymbolDoc)
-            force_init=False, begin_epoch=0, num_epoch=None):
+            force_rebind=False, force_init=False, begin_epoch=0, num_epoch=None):
-                  for_training=True, force_rebind=True)
+                  for_training=True, force_rebind=force_rebind)
-        start_offset = self.out_ark.tell()
+        if self.out_scp != None:
-        self.out_scp.write(scp_out + '\n')
+        if self.out_scp != None:
-        self.out_scp.close()
+        if self.out_scp != None:
-        label = [np.zeros((len(x), buckets[i])) for i, x in enumerate(self.data)]
+        data = [np.zeros((len(x), buckets[i], self.feat_dim)) if len(x) % self.batch_size == 0  else np.zeros(((len(x)/self.batch_size + 1) *self.batch_size, buckets[i], self.feat_dim)) for i, x in enumerate(self.data)]
-
+    sm = mx.sym.SoftmaxOutput(data=pred, label=label, ignore_label=0, use_ignore=True, name='softmax')
-    num_epoch = 20
+    num_epoch = 50
-              batch_end_callback = mx.callback.Speedometer(batch_size, 100), 
+              batch_end_callback = mx.callback.Speedometer(batch_size, 100),
-              optimizer_params={'learning_rate':0.002, 'wd': 0.0})
+              optimizer_params={'learning_rate':0.002, 'wd': 0.00001})
-        if n_epoch > 0 and curr_acc < last_acc:
+        if n_epoch > 0 and lr_scheduler.base_lr > lower_bnd and curr_acc < last_acc:
-        "lst_file": "/data/sls/scratch/yzhang87/AMI/ami_sdm_baseline/exp_cntk/sdm1/cntk_train_mxnet.feats",
+        "lst_file": "/home/chiyuan/download/kaldi/egs/ami/s5/exp/sdm1/data-for-mxnet/train.feats",
-        "in": 80
+        "in": 40
-        "lst_file": "/data/scratch/yzhang87/speech/timit/cntk_train_mxnet.feats",
+DATASETS["AMI_dev"] = {
-        "lst_file": "/data/scratch/yzhang87/speech/timit/cntk_dev_mxnet.feats",
+        "lst_file": "/home/chiyuan/download/kaldi/egs/timit/s5/exp/data-for-mxnet/fake-dev.feats",
-        "in": 40
+        "in": 13
-            if utt_id == None:
+            if utt_id is None:
-    batch_size = 40
+    batch_size = 10
-    contexts = [mx.context.gpu(i) for i in range(3,4)]
+    contexts = [mx.context.gpu(i) for i in range(0,1)]
-    label_dim = 1955 + 1
+    #label_dim = 1955 + 1
-                                    buckets, batch_size, init_states)
+                                    buckets, batch_size, init_states, feat_dim=train_data["in"])
-                                    buckets, batch_size, init_states)
+                                    buckets, batch_size, init_states, feat_dim=dev_data["in"])
-              #optimizer_params={'learning_rate':0.002, 'momentum': 0.9, 'wd': 0.00001})
+
-import numpy as np
+# a dummy call just to test if the API works for merge_batches=True
-            outputs = [out[0:out.shape[0]-pad].asnumpy() for out in self.get_outputs()]
+            outputs = [out[0:out.shape[0]-pad] for out in self.get_outputs()]
-            output_list2 = [np.concatenate([out[i] for out in output_list])
+            output_list2 = [ndarray.concatenate([out[i] for out in output_list])
-            outputs = [out[0:out.shape[0]-pad] for out in self.get_outputs()]
+            outputs = [out[0:out.shape[0]-pad].asnumpy() for out in self.get_outputs()]
-    for l, n in len_dict.iteritems(): # TODO: There are better heuristic ways to do this    
+    for l, n in len_dict.items(): # TODO: There are better heuristic ways to do this    
-        for name, arr in self._arg_params.iteritems():
+        for name, arr in self._arg_params.items():
-        for name, arr in self._aux_params.iteritems():
+        for name, arr in self._aux_params.items():
-for layer, K in args.config['conv_params'].iteritems():
+for layer, K in args.config['conv_params'].items():
-for layer, K in args.config['fc_params'].iteritems():
+for layer, K in args.config['fc_params'].items():
-    for now_c, now_v in dp[now].iteritems():
+    for now_c, now_v in dp[now].items():
-  for c,v in dp[now].iteritems():
+  for c,v in dp[now].items():
-    for k, v in node['param'].iteritems():
+    for k, v in node['param'].items():
-ConvExecutor = namedtuple('ConvExecutor', ['executor', 'data', 'data_grad', 'style', 'content'])
+ConvExecutor = namedtuple('ConvExecutor', ['executor', 'data', 'data_grad', 'style', 'content', 'arg_dict'])
-def get_model(input_size, ctx):
+def get_symbol():
-    out = mx.sym.Group([style, content])
+    return style, content
-    grad_dict = dict(zip(arg_names, [mx.nd.zeros(shape, ctx=ctx) for shape in arg_shapes]))
+    grad_dict = {"data": arg_dict["data"].copyto(ctx)}
-        pretrained[key].copyto(arg_dict[name])
+        if key in pretrained:
-                        content=executor.outputs[-1])
+                        content=executor.outputs[-1],
-parser.add_argument('--lr', type=float, default=.1,
+parser.add_argument('--lr', type=float, default=.001,
-parser.add_argument('--remove-noise', type=float, default=.2,
+parser.add_argument('--remove-noise', type=float, default=.02,
-    out = denoise_tv_chambolle(out, weight=args.remove_noise, multichannel=True)
+    if args.remove_noise != 0.0:
-    return Executor(executor=executor, data=conv, data_grad=grad["conv"])
+def style_gram_symbol(input_size, style):
-style_array = [mx.nd.zeros(gram.executor.outputs[0].shape, ctx=dev) for gram in gram_executor]
+model_module =  importlib.import_module('model_' + args.model)
-
+style_array = []
-    gram_executor[i].executor.outputs[0].copyto(style_array[i])
+    style_array.append(model_executor.style[i].copyto(mx.cpu()))
-model_executor.content.copyto(content_array)
+content_array = model_executor.content.copyto(mx.cpu())
-lr = mx.lr_scheduler.FactorScheduler(step=10, factor=.9)
+lr = mx.lr_scheduler.FactorScheduler(step=80, factor=.9)
-    clip_gradient = 10)
+    wd = 0.0005,
-old_img = img.asnumpy()
+old_img = img.copyto(dev)
-    old_img = new_img
+    gnorm = mx.nd.norm(model_executor.data_grad).asscalar()
-        SaveImage(new_img, 'output/tmp_'+str(e+1)+'.jpg')
+        SaveImage(new_img.asnumpy(), 'output/tmp_'+str(e+1)+'.jpg')
-SaveImage(new_img, args.output)
+SaveImage(new_img.asnumpy(), args.output)
-            self._init_zero(name, arr)
+            self._init_one(name, arr)
-    sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, args.load_epoch) 
+    sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, args.load_epoch)
-def check_bind_with_uniform(uf, gf, dim, sf=None):
+def check_bind_with_uniform(uf, gf, dim, sf=None, lshape=None, rshape=None):
-    rhs_grad = mx.nd.empty(shape)
+    lshape = shape if lshape is None else lshape
-    out_grad = mx.nd.array(np.ones(shape))
+    out_grad = mx.nd.array(np.ones(out2.shape))
-    batch_size = 60
+    batch_size = 40
-              optimizer_params={'wd': 0.00001})
+              optimizer_params={'learning_rate':0.002, 'wd': 0.0})
-    def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8,
+    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8,
-        super(Adam, self).__init__(**kwargs)
+        super(Adam, self).__init__(learning_rate=learning_rate, **kwargs)
-            mx.metric.check_label_shapes(label, pred_label)
+def Acc_exlude_padding(labels, preds):
-            self.num_inst += len(pred_label_real)
+        ind = np.nonzero(label.flat)
-            init_states, n_batch=None,
+            init_states, delay=5, feat_dim=40,  n_batch=None,
-        self.default_bucket_key = buckets[0]
+        #self.label = [[] for k in buckets]
-                    self.data[i].append((feats,tgts))
+                    self.data[i].append((feats,tgts+1))
-                for t in range(self.default_bucket_key)]
+        #self.provide_data = [('%s/%d' % (self.data_name, t), (self.batch_size,40))
-                    yield data_batch
+        for i_bucket in self.bucket_plan:
-    loss_all = []
+    data = mx.sym.Variable('data')
-        hidden = mx.sym.Variable("data/%d" % seqidx)
+        hidden = dataSlice[seqidx]
-    return mx.sym.Group(loss_all)
+        hidden_all.append(hidden)
-    num_lstm_layer = 2
+    batch_size = 60
-    num_epoch = 50
+    num_epoch = 20
-                           num_label=label_dim)
+    state_names = [x[0] for x in init_states]
-            initializer   = mx.init.Xavier(factor_type="in", magnitude=2.34))
+    model = mx.mod.BucketingModule(sym_gen, default_bucket_key=data_train.default_bucket_key, context=contexts)
-
+    model.fit(data_train, eval_data=data_val, num_epoch=num_epoch,
-            self.num_inst += 1
+            reval = self._feval(label, pred)
-                always_output_list=False, as_iterator=False):
+                always_output_list=False):
-        label="data/train-labels-idx1-ubyte",
+        image="../image-classification/mnist/train-images-idx3-ubyte",
-        label="data/t10k-labels-idx1-ubyte",
+        image="../image-classification/mnist/t10k-images-idx3-ubyte",
-                always_output_list=False):
+                always_output_list=False, as_iterator=False):
-            Default is `None`, indicating run all the batches in the data iterator.
+            Default is `None`, indicating running all the batches in the data iterator.
-        label="data/train-labels-idx1-ubyte",
+        image="../image-classification/mnist/train-images-idx3-ubyte",
-        label="data/t10k-labels-idx1-ubyte",
+        image="../image-classification/mnist/t10k-images-idx3-ubyte",
-                always_output_list=False):
+                always_output_list=False, as_iterator=False):
-            Default is `None`, indicating run all the batches in the data iterator.
+            Default is `None`, indicating running all the batches in the data iterator.
-    Y = mx.symbol.Softmax(data=X, label=L)
+    Y = mx.symbol.SoftmaxOutput(data=X, label=L)
-    l[:] = np.random.randint(0, shape[0]-1, (shape[0],))
+    l[:] = np.random.randint(0, shape[1]-1, (shape[0],))
-    Y = mx.symbol.Softmax(data=X, label=L, multi_output=True)
+    Y = mx.symbol.SoftmaxOutput(data=X, label=L, multi_output=True)
-    print kv_type
+    print(kv_type)
-    print kv_type
+    print(kv_type)
-    idx = 1 # 0 is left for zero-padding
+    idx = 1 # 0 is left for zero-padding
-           has been allocated.
+          has been allocated.
-           has been initialized.
+          has been initialized.
-           and initialized.
+          and initialized.
-                    arg_arr = nd.zeros(arg_shape, ctx, dtype=arg_type)
+                    arg_arr = nd.zeros(arg_shape, context, dtype=arg_type)
-        z = model.extract_feature(self.feature, args, test_iter, N, self.xpu).values()[0]
+        z = model.extract_feature(self.feature, args, None, test_iter, N, self.xpu).values()[0]
-                z = model.extract_feature(self.feature, args, test_iter, N, self.xpu).values()[0]
+                z = model.extract_feature(self.feature, args, None, test_iter, N, self.xpu).values()[0]
-        solver.solve(self.xpu, self.loss, args, self.args_grad,
+        solver.solve(self.xpu, self.loss, args, self.args_grad, None,
-    
+    
-        self.optimizer.set_lr_scale(args_lrmult)
+        self.optimizer.set_lr_mult(args_lrmult)
-        self.optimizer.set_lr_scale(args_lrmult)
+        self.optimizer.set_lr_mult(args_lrmult)
-                concat_decode=True):
+                concat_decode=True, use_loss=False):
-                sm = mx.sym.SoftmaxOutput(data=fc, label=label, name="t%d_sm" % seqidx)
+                if use_loss:
-            sm = mx.sym.SoftmaxOutput(data=fc, label=label, name="sm")
+            if use_loss:
-                    group2ctx=None, concat_decode=True):
+                    group2ctx=None, concat_decode=True,
-                          concat_decode=concat_decode)
+                          concat_decode=concat_decode,
-               num_round, update_period, concat_decode,
+               num_round, update_period, concat_decode, use_loss,
-                seq_label_probs = mx.nd.choose_element_0index(m.seq_outputs[0], m.seq_labels[0])
+            if not use_loss:
-            m.rnn_exec.backward()
+                seq_loss = [x.copyto(mx.cpu()) for x in m.seq_outputs]
-                train_nll += calc_nll_concat(seq_label_probs, X_val_batch, begin=begin)
+            if not use_loss:
-                train_nll += calc_nll(seq_label_probs, X_val_batch, begin=begin)
+                train_nll += sum([x.asscalar() for x in seq_loss]) / batch_size
-                seq_label_probs = mx.nd.choose_element_0index(m.seq_outputs[0], m.seq_labels[0])
+
-                                   for out, label in zip(m.seq_outputs, m.seq_labels)]
+                seq_loss = [x.copyto(mx.cpu()) for x in m.seq_outputs]
-                val_nll += calc_nll_concat(seq_label_probs, X_val_batch, begin=begin)
+            if not use_loss:
-                val_nll += calc_nll(seq_label_probs, X_val_batch, begin=begin)
+                val_nll += sum([x.asscalar() for x in seq_loss]) / batch_size
-num_lstm_layer = 8
+num_lstm_layer = 2
-
+ngpu=1
-concat_decode = True
+concat_decode = False
-        self.kwargs["arg_names"] = arg_names
+        param_idx2name = {}
-    def __init__(self, rescale_grad=1., arg_idx2name=None, wd=0.,
+    def __init__(self, rescale_grad=1., param_idx2name=None, wd=0.,
-        self.wd_mult = {} 
+        self.wd_mult = {}
-        self.idx2name = arg_idx2name.copy()
+        if param_idx2name is None:
-       
+
-        end with _weight, if arg_idx2name is provided.
+        end with _weight, if param_idx2name is provided.
-            attr = sym.list_attr()
+            attr = self.sym.list_attr()
-    arg_names : list(str), optional
+    param_idx2name : dict of string/int to float, optional
-    arg_names : list(str), optional
+    param_idx2name : dict of string/int to float, optional
-            [momentum, rescale_grad, clip_gradient])
+            [momentum, kwargs['rescale_grad'], kwargs['clip_gradient']])
-    def __init__(self, rescale_grad=1., arg_names=None, wd=0., clip_gradient=None):
+    def __init__(self, rescale_grad=1., arg_idx2name=None, wd=0.,
-        self.lr_scale = {}
+        self.lr = learning_rate
-
+        if arg_idx2name is None:
-        """Set individual learning rate scale for parameters
+        """set lr scale is deprecated. Use set_lr_mult instead."""
-            set the lr multipler for index to float
+        args_lr_mult : dict of string/int to float
-        self.lr_scale = args_lrscale.copy()
+        self.lr_mult = {}
-                wd = self.wd
+        if index in self.wd_mult:
-        self.lr = learning_rate
+    def __init__(self, momentum=0.0, **kwargs):
-
+        lr = self._get_lr(index)
-            self.lr_scheduler.base_lr = learning_rate
+    def __init__(self, **kwargs):
-                wd = self.wd
+        lr = self._get_lr(index)
-        self.lr = learning_rate
+    def __init__(self, momentum=0.0, **kwargs):
-        lr *= self.lr_scale.get(index, 1.0)
+        lr = self._get_lr(index)
-        self.lr = learning_rate
+    def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8,
-        lr *= self.lr_scale.get(index, 1.0)
+        lr = self._get_lr(index)
-        self.lr = learning_rate
+    def __init__(self, eps=1e-7, **kwargs):
-        lr *= self.lr_scale.get(index, 1.0)
+        lr = self._get_lr(index)
-        self.lr = learning_rate
+    def __init__(self, gamma1=0.95, gamma2=0.9, **kwargs):
-        n, g, delta = state
+        lr = self._get_lr(index)
-                                       clip_gradient=clip_gradient)
+    def __init__(self, rho=0.90, epsilon=1e-5, **kwargs):
-        weight[:] -= current_delta + self.wd * weight
+        weight[:] -= current_delta + wd * weight
-        super(Test, self).__init__(rescale_grad)
+    def __init__(self, **kwargs):
-    contexts = [mx.context.gpu(i) for i in range(0,1)]
+    contexts = [mx.context.gpu(i) for i in range(3,4)]
-    def __init__(self, name='pyloss', data_names=('data'), label_names=('softmax_label'),
+    def __init__(self, name='pyloss', data_names=('data',), label_names=('softmax_label',),
-    buckets = [10, 20, 30, 40, 50, 60]
+    #buckets = [10, 20, 30, 40, 50, 60]
->>>>>>> d14cf84ed354866bedafc289c1078da05c3ed0e8
+    embed_weight = mx.sym.Variable("embed_weight")
-    embed = mx.sym.Embedding(data=data, input_dim=input_size, output_dim=num_embed, name='embed')
+    embed = mx.sym.Embedding(data=data, input_dim=input_size,
-    pred = mx.sym.FullyConnected(data=hidden_concat, num_hidden=num_label, name='pred')
+    pred = mx.sym.FullyConnected(data=hidden_concat, num_hidden=num_label,
-    
+
-    fc = mx.sym.FullyConnected(data=hidden, num_hidden=num_label, name='pred')
+    fc = mx.sym.FullyConnected(data=hidden, num_hidden=num_label,
-            subprocess.call('rm -rf build', shell = True)
+            subprocess.call('cd %s; cp make/readthedocs.mk config.mk' % folder, shell = True)
-
+if not os.path.exists('web-data'):
-extensions = ['sphinx.ext.ifconfig', 'breathe']
+extensions = ['sphinx.ext.ifconfig']
-breathe_domain_by_extension = {"h" : "cpp"}
+#breathe_domain_by_extension = {"h" : "cpp"}
-            'url_resolver': lambda url: doc_root + url,
+            'url_resolver': lambda url: github_doc_root + url,
-        Retures
+        """Get index of the current batch.
-                           rename_data=[{'data': 'data1'}, {'data': 'data2'}])
+
-    **kwargs : dict
+    kwargs : dict
-                the same as X, i.e. the number of data points and labels should be equal.
+            the same as X, i.e. the number of data points and labels should be equal.
-                it should be (valid_data, valid_label).
+            it should be (valid_data, valid_label).
-           'dist_async' : multi-machines with partical asynchronous
+           The KVStore or a string kvstore type: 'local', 'dist_sync', 'dist_async'
-        Symbol.load : the method to load the model back.
+
-           'dist_async' : multi-machines with partical asynchronous
+           The KVStore or a string kvstore type: 'local', 'dist_sync', 'dis_async'
-                                                [name + '_output'], logger=logger)
+                                               [name + '_output'], logger=logger)
-demo_data_model_parallelism = True
+demo_data_model_parallelism = False
-# pylint: disable=too-many-arguments, too-many-locals
+# pylint: disable=too-many-arguments, too-many-locals, too-many-instance-attributes
-                my_inputs_need_grad = False
+            my_inputs_need_grad = bool(inputs_need_grad or
-    n_epoch = 2
+    n_epoch = 10
-    contexts = [mx.context.cpu()]
+    num_gpu = 2
-
+# pylint: disable=too-many-instance-attributes, too-many-arguments
-# pylint: disable=too-many-arguments, too-many-locals
+# pylint: disable=too-many-arguments, too-many-locals, too-many-instance-attributes
-                my_inputs_need_grad = False
+            my_inputs_need_grad = bool(inputs_need_grad or
-demo_data_model_parallelism = True
+demo_data_model_parallelism = False
-                            if name in self.param_names]
+        if self.for_training:
-            return Symbol._MinusScalar(self, scalar=other, scalar_on_left=True)
+            return Symbol._RMinusScalar(self, scalar=other)
-            return Symbol._DivScalar(self, scalar=other, scalar_on_left=True)
+            return Symbol._RDivScalar(self, scalar=other)
-        return Symbol._PowerScalar(exp, scalar=base, scalar_on_left=True)
+        return Symbol._RPowerScalar(exp, scalar=base)
-        return Symbol._MaximumScalar(right, scalar=left, scalar_on_left=True)
+        return Symbol._MaximumScalar(right, scalar=left)
-        return Symbol._MinimumScalar(right, scalar=left, scalar_on_left=True)
+        return Symbol._MinimumScalar(right, scalar=left)
-                    ('used in layer %d (%s).' % (known_names[name], type(modules[known_names[name]])))
+                    ('used in layer %d (%s).' % (known_names[name],
-def check_bind_with_uniform(uf, gf, dim):
+def check_bind_with_uniform(uf, gf, dim, sf=None):
-    ret = uf(lhs, rhs)
+    if sf is not None:
-    rhs_arr = mx.nd.array(np.random.uniform(-10, 10, shape))
+    lhs_arr = mx.nd.array(np.random.uniform(-1, 1, shape))
-
+
-    outputs = [nd.concatenate(x) for x in outputs]
+    outputs = [nd.concatenate(x, always_copy=False) for x in outputs]
-            out_grads_slice = [grad[islice].as_in_context(self.contexts[i]) 
+            out_grads_slice = [grad[islice].as_in_context(self.contexts[i])
-def concatenate(arrays):
+def concatenate(arrays, always_copy=True):
-    
+
-    outputs = [nd.array(x) for x in outputs]
+    outputs = [nd.concatenate(x) for x in outputs]
-    # It seems using SwapAxis is not faster than directly using Slice+Concat
+    # I did not observe big speed difference between the following two ways
-    label = mx.sym.Concat(*label, dim=0)
+    label = mx.sym.transpose(data=label)
-mod1 = mx.mod.Module(act1, label_names=[])
+mod1 = mx.mod.Module(act1, label_names=[], context=contexts[0])
-mod2 = mx.mod.Module(softmax)
+mod2 = mx.mod.Module(softmax, context=contexts[1])
-                               enumerate(out_grads_slice)]
+        for i, (exec_, islice) in enumerate(zip(self.execs, self.slices)):
-    context : list
+    contexts : list
-    def __init__(self, symbol, context, workload, data_shapes, label_shapes, param_names,
+    def __init__(self, symbol, contexts, workload, data_shapes, label_shapes, param_names,
-        self.context = context
+        self.contexts = contexts
-            self.shared_data_arrays = [{} for _ in context]
+            self.shared_data_arrays = [{} for _ in contexts]
-        for i in range(len(self.context)):
+        for i in range(len(self.contexts)):
-        context = self.context[i]
+        context = self.contexts[i]
-                my_label_shapes=None
+                my_label_shapes = None
-# pylint: disable=too-many-arguments, too-many-locals
+# pylint: disable=too-many-arguments, too-many-locals, too-many-public-methods
-
+    """A SequentialModule is a container module that can chain multiple modules together.
-        if len(self._module) > 0:
+        if len(self._modules) > 0:
-        if inputs_need_grad: assert for_training is True
+        if inputs_need_grad:
-                my_data_shapes = [(new_name, shape) for (new_name, (old_name, shape))
+                my_data_shapes = [(new_name, shape) for (new_name, (_, shape))
-data = mx.symbol.Variable('relu1_output')
+data = mx.symbol.Variable('data')
-mod2 = mx.mod.Module(softmax, data_names=['relu1_output'])
+mod2 = mx.mod.Module(softmax)
-mod_seq.add(mod1).add(mod2, take_labels=True)
+mod_seq.add(mod1).add(mod2, take_labels=True, auto_wiring=True)
-
+        if len(self._module) > 0:
-    following information (after binded).
+    following information in its raw stage (before binded)
-        input_names = list(data_names) + list(label_names)
+        input_names = data_names + label_names
-            _load_label(data_batch, self.label_arrays)
+            # It could be the case that even though we are binded for training, we
-        input_names = data_names + label_names
+        input_names = list(data_names) + list(label_names)
-            assert label_shapes is not None
+            pass
-mod1 = mx.mod.Module(act1, label_names=[])
+mod1 = mx.mod.Module(act1, label_names=[], context=contexts[0])
-mod2 = mx.mod.Module(softmax)
+mod2 = mx.mod.Module(softmax, context=contexts[1])
-                               enumerate(out_grads_slice)]
+        for i, (exec_, islice) in enumerate(zip(self.execs, self.slices)):
-    context : list
+    contexts : list
-    def __init__(self, symbol, context, workload, data_shapes, label_shapes, param_names,
+    def __init__(self, symbol, contexts, workload, data_shapes, label_shapes, param_names,
-        self.context = context
+        self.contexts = contexts
-            self.shared_data_arrays = [{} for _ in context]
+            self.shared_data_arrays = [{} for _ in contexts]
-        for i in range(len(self.context)):
+        for i in range(len(self.contexts)):
-        context = self.context[i]
+        context = self.contexts[i]
-                my_label_shapes=None
+                my_label_shapes = None
-# pylint: disable=too-many-arguments, too-many-locals
+# pylint: disable=too-many-arguments, too-many-locals, too-many-public-methods
-
+    """A SequentialModule is a container module that can chain multiple modules together.
-        if len(self._module) > 0:
+        if len(self._modules) > 0:
-        if inputs_need_grad: assert for_training is True
+        if inputs_need_grad:
-                my_data_shapes = [(new_name, shape) for (new_name, (old_name, shape))
+                my_data_shapes = [(new_name, shape) for (new_name, (_, shape))
-data = mx.symbol.Variable('relu1_output')
+data = mx.symbol.Variable('data')
-mod2 = mx.mod.Module(softmax, data_names=['relu1_output'])
+mod2 = mx.mod.Module(softmax)
-mod_seq.add(mod1).add(mod2, take_labels=True)
+mod_seq.add(mod1).add(mod2, take_labels=True, auto_wiring=True)
-
+        if len(self._module) > 0:
-    following information (after binded).
+    following information in its raw stage (before binded)
-        input_names = list(data_names) + list(label_names)
+        input_names = data_names + label_names
-            _load_label(data_batch, self.label_arrays)
+            # It could be the case that even though we are binded for training, we
-        input_names = data_names + label_names
+        input_names = list(data_names) + list(label_names)
-            assert label_shapes is not None
+            pass
-    # TODO: add a Transpose operator and then we can Reshape
+    ################################################################################
-      packages=['mxnet'],
+      packages=['mxnet', 'mxnet.module'],
-# pylint: disable=invalid-name, protected-access, too-many-locals, too-many-arguments
+# pylint: disable=invalid-name, protected-access, too-many-locals, too-many-arguments, too-many-statements
-            
+
-                    arg_arr =  arg_arr.reshape(arg_shape[i])
+                    arg_arr = arg_arr.reshape(arg_shape[i])
-                                   (' be the bucket taking the largest input for better memory sharing.'))
+                                   (', which is larger than already allocated ') +
-            
+
-                 for_training, inputs_need_grad, shared_group=None, input_types=None, logger=logging):
+                 for_training, inputs_need_grad, shared_group=None, input_types=None,
-                                   (' be the bucket taking the largest input for better memory sharing.'))
+                                   (', which is larger than already allocated ') +
-                arg_arr = _get_or_reshape(name, shared_data_arrays, arg_shapes[j], arg_types[j], 
+                arg_arr = _get_or_reshape(name, shared_data_arrays, arg_shapes[j], arg_types[j],
-                                                        arg_shapes[j], arg_types[j], context, 
+                                                        arg_shapes[j], arg_types[j], context,
-    arg_shape, output_shape, aux_shape = op.infer_shape(data=e_nd.shape)
+    arg_shape, output_shape, aux_shape = op.infer_shape(data=shape)
-    data = mx.sym.Variable("data/%d" % seqidx)
+    data = mx.sym.Variable("data")
-    hidden = mx.sym.Embedding(data=data, weight=embed_weight,
+    hidden = mx.sym.Embedding(data=data,
-                              name="t%d_embed" % seqidx)
+                              name="embed")
-                              name='t%d_sm' % seqidx)
+    fc = mx.sym.FullyConnected(data=hidden, num_hidden=num_label, name='pred')
-        data_shape = [("data/0", (batch_size,))]
+        data_shape = [("data", (batch_size,))]
-        input_data.copyto(self.executor.arg_dict["data/0"])
+        input_data.copyto(self.executor.arg_dict["data"])
-
+    label = label.T.reshape((-1,))
-        label_names = ['label/%d' % t for t in range(seq_len)]
+        data_names = ['data'] + state_names
-        mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=buckets[0], context=contexts)
+        mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=data_train.default_bucket_key, context=contexts)
-                        arg_arr =  arg_arr.reshape(arg_shape[i])
+                    arg_arr =  arg_arr.reshape(arg_shape[i])
-                 for_training, inputs_need_grad, shared_group=None, input_types=None):
+                 for_training, inputs_need_grad, shared_group=None, input_types=None, logger=logging):
-                shared_data_arrays[name] = arg_arr
+                arg_arr = _get_or_reshape(name, shared_data_arrays, arg_shapes[j], arg_types[j], 
-                                                     shared_group)
+                                                     shared_group, logger=self.logger)
-               base_exec=None, shared_data_arrays=None, input_types=None):
+               base_exec=None, shared_data_arrays=None, input_types=None, logger=logging):
-            arg_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])
+            if shared_data_arrays is not None and \
-    # reshape label to collapse the channel dimension
+    # TODO: add a Transpose operator and then we can Reshape
-        self.default_bucket_key = buckets[0]
+        # pre-allocate with the largest bucket for better memory sharing
-                              for t in range(self.default_bucket_key)]
+        self.provide_data = [('data', (batch_size, self.default_bucket_key))] + init_states
-                           for t in range(self.buckets[i_bucket])]
+            data_all = [mx.nd.array(data)] + self.init_state_arrays
-    loss_all = []
+    # embeding layer
-        data = mx.sym.Variable("data/%d" % seqidx)
+        hidden = wordvec[seqidx]
-        loss_all.append(sm)
+        hidden_all.append(hidden)
-    return mx.sym.Group(loss_all)
+    hidden_concat = mx.sym.Concat(*hidden_all, dim=0)
-                arg_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])
+            # if shared_data_arrays is not None and \
-        shape = (2,2)
+        shape = (2, 2)
-    
+
-    
+
-    
+
-    
+
-    
+
- 
+
-    
+
-    
+
-    # TODO: add a Transpose operator and then we can Reshape
+    ################################################################################
-      packages=['mxnet'],
+      packages=['mxnet', 'mxnet.module'],
-# pylint: disable=invalid-name, protected-access, too-many-locals, too-many-arguments
+# pylint: disable=invalid-name, protected-access, too-many-locals, too-many-arguments, too-many-statements
-            
+
-                    arg_arr =  arg_arr.reshape(arg_shape[i])
+                    arg_arr = arg_arr.reshape(arg_shape[i])
-                                   (' be the bucket taking the largest input for better memory sharing.'))
+                                   (', which is larger than already allocated ') +
-            
+
-                 for_training, inputs_need_grad, shared_group=None, input_types=None, logger=logging):
+                 for_training, inputs_need_grad, shared_group=None, input_types=None,
-                                   (' be the bucket taking the largest input for better memory sharing.'))
+                                   (', which is larger than already allocated ') +
-                arg_arr = _get_or_reshape(name, shared_data_arrays, arg_shapes[j], arg_types[j], 
+                arg_arr = _get_or_reshape(name, shared_data_arrays, arg_shapes[j], arg_types[j],
-                                                        arg_shapes[j], arg_types[j], context, 
+                                                        arg_shapes[j], arg_types[j], context,
-    arg_shape, output_shape, aux_shape = op.infer_shape(data=e_nd.shape)
+    arg_shape, output_shape, aux_shape = op.infer_shape(data=shape)
-        label_names = ['label/%d' % t for t in range(seq_len)]
+        data_names = ['data'] + state_names
-        mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=buckets[0], context=contexts)
+        mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=data_train.default_bucket_key, context=contexts)
-        self.default_bucket_key = buckets[0]
+        # pre-allocate with the largest bucket for better memory sharing
-                              for t in range(self.default_bucket_key)]
+        self.provide_data = [('data', (batch_size, self.default_bucket_key))] + init_states
-                           for t in range(self.buckets[i_bucket])]
+            data_all = [mx.nd.array(data)] + self.init_state_arrays
-    loss_all = []
+    # embeding layer
-        data = mx.sym.Variable("data/%d" % seqidx)
+        hidden = wordvec[seqidx]
-        loss_all.append(sm)
+        hidden_all.append(hidden)
-    return mx.sym.Group(loss_all)
+    sm = mx.sym.SoftmaxOutput(data=pred, label=label, name='softmax')
-    data = mx.sym.Variable("data/%d" % seqidx)
+    data = mx.sym.Variable("data")
-    hidden = mx.sym.Embedding(data=data, weight=embed_weight,
+    hidden = mx.sym.Embedding(data=data,
-                              name="t%d_embed" % seqidx)
+                              name="embed")
-                              name='t%d_sm' % seqidx)
+    fc = mx.sym.FullyConnected(data=hidden, num_hidden=num_label, name='pred')
-
+    label = label.T.reshape((-1,))
-        data_shape = [("data/0", (batch_size,))]
+        data_shape = [("data", (batch_size,))]
-        input_data.copyto(self.executor.arg_dict["data/0"])
+        input_data.copyto(self.executor.arg_dict["data"])
-
+import numpy as np
-               base_exec=None, shared_data_arrays=None, input_types=None):
+               base_exec=None, shared_data_arrays=None, input_types=None, logger=logging):
-                assert(arg_types[i] == arg_arr.dtype)
+            
-                shared_data_arrays[name] = arg_arr
+import logging
-                 for_training, inputs_need_grad, shared_group=None, input_types=None):
+                 for_training, inputs_need_grad, shared_group=None, input_types=None, logger=logging):
-                shared_data_arrays[name] = arg_arr
+                arg_arr = _get_or_reshape(name, shared_data_arrays, arg_shapes[j], arg_types[j], 
-                                                     shared_group)
+                                                     shared_group, logger=self.logger)
-        shape = (2,2)
+        shape = (2, 2)
-    
+
-    
+
-    
+
-    
+
-    
+
- 
+
-    
+
-    
+
-    data = mx.sym.Variable("data/%d" % seqidx)
+    data = mx.sym.Variable("data")
-    hidden = mx.sym.Embedding(data=data, weight=embed_weight,
+    hidden = mx.sym.Embedding(data=data,
-                              name="t%d_embed" % seqidx)
+                              name="embed")
-                              name='t%d_sm' % seqidx)
+    fc = mx.sym.FullyConnected(data=hidden, num_hidden=num_label, name='pred')
-        data_shape = [("data/0", (batch_size,))]
+        data_shape = [("data", (batch_size,))]
-        input_data.copyto(self.executor.arg_dict["data/0"])
+        input_data.copyto(self.executor.arg_dict["data"])
-
+    label = label.T.reshape((-1,))
-        label_names = ['label/%d' % t for t in range(seq_len)]
+        data_names = ['data'] + state_names
-        mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=buckets[0], context=contexts)
+        mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=data_train.default_bucket_key, context=contexts)
-                        arg_arr =  arg_arr.reshape(arg_shape[i])
+                    arg_arr =  arg_arr.reshape(arg_shape[i])
-                 for_training, inputs_need_grad, shared_group=None, input_types=None):
+                 for_training, inputs_need_grad, shared_group=None, input_types=None, logger=logging):
-                shared_data_arrays[name] = arg_arr
+                arg_arr = _get_or_reshape(name, shared_data_arrays, arg_shapes[j], arg_types[j], 
-                                                     shared_group)
+                                                     shared_group, logger=self.logger)
-               base_exec=None, shared_data_arrays=None, input_types=None):
+               base_exec=None, shared_data_arrays=None, input_types=None, logger=logging):
-            arg_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])
+            if shared_data_arrays is not None and \
-    # reshape label to collapse the channel dimension
+    # TODO: add a Transpose operator and then we can Reshape
-        self.default_bucket_key = buckets[0]
+        # pre-allocate with the largest bucket for better memory sharing
-                              for t in range(self.default_bucket_key)]
+        self.provide_data = [('data', (batch_size, self.default_bucket_key))] + init_states
-                           for t in range(self.buckets[i_bucket])]
+            data_all = [mx.nd.array(data)] + self.init_state_arrays
-    loss_all = []
+    # embeding layer
-        data = mx.sym.Variable("data/%d" % seqidx)
+        hidden = wordvec[seqidx]
-        loss_all.append(sm)
+        hidden_all.append(hidden)
-    return mx.sym.Group(loss_all)
+    hidden_concat = mx.sym.Concat(*hidden_all, dim=0)
-                arg_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])
+            # if shared_data_arrays is not None and \
-        shape = (2,2)
+        shape = (2, 2)
-    
+
-    
+
-    
+
-    
+
-    
+
- 
+
-    
+
-    
+
-
+            """Internal helper for parameter initialization"""
-sys.path.insert(0, "../rnn")
+import os
-    num_epoch = 25
+    #num_epoch = 25
-    default_input_names = [x[0] for x in (data_train.provide_data + data_train.provide_label)]
+    state_names = [x[0] for x in init_states]
-                            context=contexts)
+        mod = mx.mod.Module(*sym_gen(buckets[0]), context=contexts)
-                                     context=contexts)
+        mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=buckets[0], context=contexts)
-                                                'image-classification', 'cifar10')) + '/'
+my_dir = os.path.dirname(__file__)
-        self.symbol = None
+        self._symbol = None
-# pylint: disable=too-many-instance-attributes, too-many-arguments
+# pylint: disable=too-many-instance-attributes, too-many-arguments, protected-access
-        self.symbol = symbol
+        self._symbol = symbol
-        self._exec_group = DataParallelExecutorGroup(self.symbol, self._context,
+        self._exec_group = DataParallelExecutorGroup(self._symbol, self._context,
-            self._aux_params = shared_module.aux_params
+            self._arg_params = shared_module._arg_params
-        self._updater = shared_module.updater
+        self._optimizer = shared_module._optimizer
-        - `update_metric(metric)`: update performance metric for the previous forward
+        - `update_metric(metric, labels)`: update performance metric for the previous forward
-        return self.curr_module.symbol
+        assert self.binded
-            self._borrow_optimizer(shared_module)
+            self.borrow_optimizer(shared_module)
-    def _borrow_optimizer(self, shared_module):
+    def borrow_optimizer(self, shared_module):
-        return self.curr_module.symbol
+"""A module is like a FeedForward model. but we would like to make it
-from . import ndarray as nd
+from .. import context as ctx
-from .executor_manager import _split_input_slice, _load_data, _load_label
+from ..base import mx_real_t
-            outputs = [nd.array(x) for x in outputs]
+            outputs = _merge_multi_context(outputs)
-        assert len(work_load_list) == len(self.context)
+            work_load_list = [1] * len(self._context)
-        self.exec_group.set_params(self._arg_params, self._aux_params)
+        self._exec_group.set_params(self._arg_params, self._aux_params)
-        self.updater = shared_module.updater
+        self._optimizer = shared_module.optimizer
-                                      self.kvstore)
+        if self._update_on_kvstore:
-                           kvstore=self.kvstore)
+            _update_params(self._exec_group.param_arrays,
-        latest parameters from `self.arg_params` and `self.aux_params`.
+        latest parameters from `self._arg_params` and `self._aux_params`.
-        self.exec_group.get_params(self.arg_params, self.aux_params)
+        self.exec_group.get_params(self._arg_params, self._aux_params)
-          not need labels (e.g. it does not contains a loss function at the top).
+        - `label_shapes`: a list of `(name, shape)`. This might be `[]` if the module does
-        image classification.
+    data_names : list of str
-                 context=ctx.cpu(), work_load_list=None):
+    def __init__(self, symbol, data_names=['data'], label_names=['softmax_label'],
-        self.context = context
+        self._context = context
-        self.work_load_list = work_load_list
+        self._work_load_list = work_load_list
-        self.aux_names = symbol.list_auxiliary_states()
+        input_names = data_names + label_names
-        self.aux_params = None
+        self._arg_params = None
-        self.updater = None
+        self._optimizer = None
-        self._reset_bind()
+        self._exec_group = None
-        can perform computation with the module.
+        self._exec_group = None
-            but with the same sets of parameters (e.g. unrolled RNNs with different lengths).
+    @property
-                                                    for_training, inputs_need_grad, shared_group)
+        assert self.binded
-            self.aux_params = shared_module.aux_params
+    @property
-            self.exec_group.set_params(self.arg_params, self.aux_params)
+    @property
-            self.borrow_optimizer(shared_module)
+    def get_params(self):
-            self.arg_params = {name:arr for name, arr in zip(self.param_names, param_arrays)}
+        if self._arg_params is None:
-            self.aux_params = {name:arr for name, arr in zip(self.aux_names, aux_arrays)}
+        if self._aux_params is None:
-        for name, arr in self.arg_params.iteritems():
+        for name, arr in self._arg_params.iteritems():
-        for name, arr in self.aux_params.iteritems():
+        for name, arr in self._aux_params.iteritems():
-        self.exec_group.set_params(self.arg_params, self.aux_params)
+        self.exec_group.set_params(self._arg_params, self._aux_params)
-        """Forward computation.
+    def bind(self, data_shapes, label_shapes=None, for_training=True,
-            Default is `None`, which means `is_train` takes the value of `self.for_training`.
+        data_shapes : list of (str, tuple)
-        self.exec_group.forward(data_batch, is_train)
+        # force rebinding is typically used when one want to switch from
-        """Backward computation.
+        if self.binded:
-        self.exec_group.backward(out_grads=out_grads)
+        self.for_training = for_training
-        return self.exec_group.get_output_shapes()
+        if not for_training:
-        """Get outputs of the previous forward computation.
+        self._data_shapes = data_shapes
-            executor.
+        if shared_module is not None:
-        return self.exec_group.get_outputs(merge_multi_context=merge_multi_context)
+        self._exec_group = DataParallelExecutorGroup(self.symbol, self._context,
-                _create_kvstore(kvstore, len(self.context), self.arg_params)
+                _create_kvstore(kvstore, len(self._context), self._arg_params)
-            batch_size = self.exec_group.batch_size
+            batch_size = self._exec_group.batch_size
-        self.updater = None
+        self._optimizer = optimizer
-            self.updater = opt.get_updater(optimizer)
+            self._updater = opt.get_updater(optimizer)
-                                param_names=self.param_names,
+                                param_arrays=self._exec_group.param_arrays,
-            kvstore.set_optimizer(self.optimizer)
+            kvstore.set_optimizer(self._optimizer)
-    def borrow_optimizer(self, shared_module):
+    def _borrow_optimizer(self, shared_module):
-        self.exec_group.update_metric(eval_metric, labels)
+        self._exec_group.update_metric(eval_metric, labels)
-    def sync_params_from_devices(self):
+    def _sync_params_from_devices(self):
-                mod.borrow_optimizer(self.curr_module)
+                mod._borrow_optimizer(self.curr_module)
-
+                arg_params, aux_params = self.get_params()
-                    callback(epoch, self.symbol, self.arg_params, self.aux_params)
+                    callback(epoch, self.symbol, arg_params, aux_params)
-
+    ################################################################################
-                    cb(batch_end_params)
+                for callback in _as_list(batch_end_callback):
-                assert len(o) == num_outputs, \
+            for out in output_list:
-            output_list2 = [np.concatenate([o[i] for o in output_list])
+            output_list2 = [np.concatenate([out[i] for out in output_list])
-                        cb(batch_end_params)
+                    for callback in _as_list(batch_end_callback):
-                    cb(epoch, self.symbol, self.arg_params, self.aux_params)
+                for callback in _as_list(epoch_end_callback):
-        - `init_optimizer`: install optimizer for parameter updating.
+
-    pred_label = preds[i][0].argmax(axis=1)
+    pred_label = preds[i][0].asnumpy().argmax(axis=1)
-    - `update_metric`: update evaluation metric
+    """The base class of a modules. A module represents a computation component. The design
-    potentially with `force_init=True`.
+    def get_output_shapes(self):
-    def backward(self):
+    def get_outputs(self, merge_multi_context=True):
-            exec_.backward()
+        if out_grads is None:
-    def forward_backward(self, data_batch, is_train=None):
+    def forward_backward(self, data_batch):
-        self.forward(data_batch, is_train=is_train)
+        self.forward(data_batch, is_train=True)
-                self.forward_backward(data_batch, is_train=True)
+                self.forward_backward(data_batch)
-    def backward(self):
+    def backward(self, out_grads=None):
-        self.exec_group.backward()
+        self.exec_group.backward(out_grads=out_grads)
-        elements are numpy arrays.
+        elements are `NDArray`.
-        return outputs
+        return self.exec_group.get_outputs(merge_multi_context=merge_multi_context)
-    def backward(self):
+    def backward(self, out_grads=None):
-        self.curr_module.backward()
+        self.curr_module.backward(out_grads=out_grads)
-from .executor_manager import _split_input_slice, _load_data, _load_label
+from .executor_group import DataParallelExecutorGroup
-# easier to be composed. So it is more like the Torch modules.
+"""A module is like a FeedForward model. but we would like to make it
-            assert s[1][0] == self.batch_size, "all the data must have the same batch size"
+        for shape in data_shapes:
-            self.label_arrays = [[(self.slices[i], e.arg_dict[name]) for i, e in enumerate(self.execs)]
+            self.label_arrays = [[(self.slices[i], e.arg_dict[name])
-        self.grad_arrays = [[e.grad_arrays[i] for e in self.execs]
+        self.param_arrays = [[exec_.arg_arrays[i] for exec_ in self.execs]
-        self.aux_arrays = [[e.aux_arrays[i] for e in self.execs]
+        self.aux_arrays = [[exec_.aux_arrays[i] for exec_ in self.execs]
-            e.copy_params_from(arg_params, aux_params)
+        for exec_ in self.execs:
-            e.forward(is_train=is_train)
+        for exec_ in self.execs:
-            e.backward()
+        for exec_ in self.execs:
-    - `binded`: `bool`, indicating whether the memory buffers needed for computation has been allocated.
+    - `binded`: `bool`, indicating whether the memory buffers needed for computation
-      symbol being used. For other modules, this value might not be well defined.
+    - `params_initialized`: `bool`, indicating whether the parameters of this modules
-      Might be useful when implementing composition of modules.
+    - `inputs_need_grad`: `bool`, indicating whether gradients with respect to the
-    always be used for this purpose, potentially with `force_init=True`.
+    If you want to read the module parameters, they could be accessed directly from
-        """Run prediction on `eval_data` and evaluate the performance according to `eval_metric`.
+    def score(self, eval_data, eval_metric, num_batch=None, batch_end_callback=None,
-            Number of batches to run. Default is `None`, indicating run until the `DataIter` finishes.
+            Number of batches to run. Default is `None`, indicating run until the `DataIter`
-            Default `True`, indicating whether we should reset `eval_data` before starting evaluating.
+            Default `True`, indicating whether we should reset `eval_data` before starting
-            this will correspond to the training epoch number.
+            Default 0. For compatibility, this will be passed to callbacks (if any). During
-    def predict(self, eval_data, num_batch=None, merge_batches=True, reset=True, always_output_list=False):
+    def predict(self, eval_data, num_batch=None, merge_batches=True, reset=True,
-            Default is `True`, indicating whether we should reset the data iter before start doing prediction.
+            Default is `True`, indicating whether we should reset the data iter before start
-        instead of `[out1]`.
+        When `merge_batches` is `True` (by default), the return value will be a list
-
+        `[[out1_batch1, out2_batch1], [out1_batch2], ...]`. This mode is useful because
-            output_list2 = [np.concatenate([o[i] for o in output_list]) for i in range(num_outputs)]
+                       'Cannot merge batches, as num of outputs is not the same ' + \
-            If not `None`, will be used as validation set and evaluate the performance after each epoch.
+            If not `None`, will be used as validation set and evaluate the performance
-            Each callback will be called with the current `epoch`, `symbol`, `arg_params` and `aux_params`.
+            Each callback will be called with the current `epoch`, `symbol`, `arg_params`
-            call to `init_params` or `fit`. `arg_params` has higher priority to `initializer`.
+            Default `None`, if not `None`, should be existing parameters from a trained
-            `initializer`.
+            Default `False`. Indicate whether we allow missing parameters when `arg_params`
-            already initialized.
+            Default `False`. Indicate whether we should force initialization even if the
-            at a previous training phase at epoch N, then we should specify this value as N+1.
+            Default `0`. Indicate the starting epoch. Usually, if we are resuming from a
-        self.init_optimizer(kvstore=kvstore, optimizer=optimizer, optimizer_params=optimizer_params)
+        self.init_optimizer(kvstore=kvstore, optimizer=optimizer,
-                self.score(eval_data, eval_metric, batch_end_callback=eval_batch_end_callback, epoch=epoch)
+                self.score(eval_data, eval_metric,
-    except under the module API.
+    """Module is a basic module that wrap a `Symbol`. It is functionally the same
-        Default is `['data', 'softmax_label']` for a typical model used in image classification.
+        Default is `['data', 'softmax_label']` for a typical model used in
-        outputs = [[e.outputs[i].asnumpy() for e in self.exec_group.execs]
+        outputs = [[exec_.outputs[i].asnumpy() for exec_ in self.exec_group.execs]
-                kvstore, len(self.context), self.arg_params)
+        (kvstore, update_on_kvstore) = \
-        from `self.arg_params` and `self.aux_params`.
+        """Synchronize parameters from devices to CPU. This function should be called after
-    def init_optimizer(self, kvstore='local', optimizer='sgd', optimizer_params={},
+    def init_optimizer(self, kvstore='local', optimizer='sgd', optimizer_params=dict(),
-        from `self.arg_params` and `self.aux_params`.
+        """Synchronize parameters from devices to CPU. This function should be called after
-
+    Notes
-    # on the specific devices (contexts) specified.
+        """Bind the symbols to construct executors. This is necessary before one
-
+        """Forward computation.
-                merge_batches=True, reset=True, always_output_list=False):
+    def predict(self, eval_data, num_batch=None, merge_batches=True, reset=True, always_output_list=False):
-        self.decide_slices(data_shapes, label_shapes)
+        # initialize some instance variables
-    def decide_slices(self, data_shapes, label_shapes):
+    def decide_slices(self, data_shapes):
-            texec.copy_params_from(arg_params, aux_params)
+        """Assign, i.e. copy parameters to all the executors.
-        """ Copy data from each executor to `arg_params` and `aux_params`
+        """ Copy data from each executor to `arg_params` and `aux_params`.
-            texec.forward(is_train=is_train)
+        for e in self.execs:
-            texec.backward()
+        for e in self.execs:
-    def score(self, eval_data, eval_metric, num_batch=None, batch_end_callback=None, reset=True):
+    def score(self, eval_data, eval_metric, num_batch=None, batch_end_callback=None, reset=True, epoch=0):
-                for cb in _as_list(eval_batch_end_callback):
+                for cb in _as_list(batch_end_callback):
-                self.score(eval_data, eval_metric, batch_end_callback=eval_batch_end_callback)
+                self.score(eval_data, eval_metric, batch_end_callback=eval_batch_end_callback, epoch=epoch)
-    os.chdir("..")
+    os.chdir(cwd)
-    mod.init_params(arg_params=arg_params, aux_params=aux_params)
+else:
-                    cache[name].copy_to(arr)
+                    cache[name].copyto(arr)
-devs = mx.cpu() if args.gpus is None else [
+devs = mx.cpu() if (args.gpus is None or args.gpus == '') else [
-    model_prefix += "-%d" % (kv.rank)
+logging.info('start training for %d epochs...', args.num_epochs)
-                self.logger.info('Epoch[%02d] Train-%s=%f', epoch, name, val)
+                self.logger.info('Epoch[%d] Train-%s=%f', epoch, name, val)
-            self.logger.info('Epoch[%02d] Time cost=%.3f', epoch, (toc-tic))
+            self.logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc-tic))
-                    self.logger.info('Epoch[%02d] Validation-%s=%f', epoch, name, val)
+                    self.logger.info('Epoch[%d] Validation-%s=%f', epoch, name, val)
-                                                'image-classification', 'cifar10'))
+                                                'image-classification', 'cifar10')) + '/'
-mod = mx.mod.Module(symbol, context=devs)
+mod = mx.mod.Module(net, context=devs)
-        batch_end_callback=mx.callback.Speedometer(args.batch_size, 50))
+        batch_end_callback=mx.callback.Speedometer(args.batch_size, 50),
-data_dir = os.path.abspath(os.path.join(os.path.dirname(__FILE__), '..', 'rnn', 'data'))
+data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'rnn', 'data'))
-    vocab = default_build_vocab(os.path.join(data_dir, "train.txt"))
+    vocab = default_build_vocab(os.path.join(data_dir, "ptb.train.txt"))
-                                  context=contexts)
+        mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=buckets[0],
-    mod.fit(data_train, eval_data=data_val,
+    mod.fit(data_train, eval_data=data_val, num_epoch=num_epoch,
-            optimizer_params={'learning_rate':0.01, 'momentum', 0.9, 'wd': 0.00001})
+            optimizer_params={'learning_rate':0.01, 'momentum': 0.9, 'wd': 0.00001})
-                       'Maybe bucketing is used?')
+                assert len(o) == num_outputs, \
-        super(BucketModule, self).__init__(logger=logger)
+        super(BucketingModule, self).__init__(logger=logger)
-        symbol = self.sym_gen(self.default_bucket_key)
+        symbol = self.sym_gen(key)
-        self.optimizer_initialized
+        self.optimizer_initialized = True
-    vocab = default_build_vocab("./data/ptb.train.txt")
+    vocab = default_build_vocab(os.path.join(data_dir, "train.txt"))
-    data_train = BucketSentenceIter("./data/ptb.train.txt", vocab,
+    data_train = BucketSentenceIter(os.path.join(data_dir, "ptb.train.txt"), vocab,
-    data_val = BucketSentenceIter("./data/ptb.valid.txt", vocab,
+    data_val = BucketSentenceIter(os.path.join(data_dir, "ptb.valid.txt"), vocab,
-softmax = mx.symbol.SoftmaxOutput(fc3, name = 'sm')
+softmax = mx.symbol.SoftmaxOutput(fc3, name = 'softmax')
-mod = mx.mod.Module(softmax, ['data', 'sm_label'])
+mod = mx.mod.Module(softmax)
-mod = mx.mod.Module(softmax, ['data', 'sm_label'])
+mod = mx.mod.Module(softmax)
-    pred_label = preds[i].argmax(axis=1)
+    pred_label = preds[i][0].argmax(axis=1)
-        output_list = [[] for _ in range(self.num_outputs)]
+        output_list = []
-                o_list.append(o_nd[:o_nd.shape[0]-pad])
+            output_list.append(self.get_outputs())
-            output_list = [np.concatenate(x) for x in output_list]
+            num_outputs = len(output_list[0])
-        return output_list
+            if num_outputs == 1:
-
+# pylint: disable=C0111,too-many-arguments,too-many-instance-attributes,too-many-locals,redefined-outer-name,fixme
-    def __init__(self, symbol, input_names, logger=logging, context=ctx.cpu(), work_load_list=None):
+    def __init__(self, symbol, input_names=['data', 'softmax_label'], logger=logging,
-                 context=ctx.cpu(), work_load_list=None):
+    def __init__(self, sym_gen, default_bucket_key=None, default_input_names=None,
-        module = Module(symbol, input_names, logger=self.logger, context=self.context,
+        symbol = self.sym_gen(self.default_bucket_key)
-            symbol, input_names = self.sym_gen(bucket_key)
+            symbol, input_names = self._gen_symbol(bucket_key)
-             inputs_need_grad=False, force_rebind=False, shared_group=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None):
-        self.arg_params = {name:arr for name, arr in zip(self.param_names, param_arrays)}
+        if self.arg_params is None:
-        self.aux_params = {name:arr for name, arr in zip(self.aux_names, aux_arrays)}
+        if self.aux_params is None:
-                grad_req[name] == 'null'
+                grad_req[name] = 'null'
-                    assert arg_arr.shape == arg_shape[j]
+                    assert arg_arr.shape == arg_shapes[j]
-                    assert arg_arr.shape == arg_shape[j]
+                    assert arg_arr.shape == arg_shapes[j]
-            aux_arrays = [nd.zeros(s, ctx, dtype=t) for s, t in zip(aux_shapes, aux_types)]
+            aux_arrays = [nd.zeros(s, context, dtype=t) for s, t in zip(aux_shapes, aux_types)]
-    def score(self, eval_data, eval_metric, num_batch=None, batch_end_callback=None):
+    def forward_backward(self, data_batch, is_train=None):
-                self.backward()
+                self.forward_backward(data_batch, is_train=True)
-
+        assert self.binded and self.params_initialized
-n_epoch = 5
+n_epoch = 2
-                            cb(batch_end_params)
+                self.score(eval_data, eval_metric, batch_end_callback=eval_batch_end_callback)
-            weight = sum(w.copyto(cpu()) for w in block) / len(block)
+            weight = sum(w.copyto(ctx.cpu()) for w in block) / len(block)
-            weight = sum(w.copyto(cpu()) for w in block) / len(block)
+            weight = sum(w.copyto(ctx.cpu()) for w in block) / len(block)
-    def forward(self, data_batch):
+    def forward(self, data_batch, is_train=None):
-        if self.for_training:
+        if is_train is None:
-            texec.forward(is_train=self.for_training)
+            texec.forward(is_train=is_train)
-    def update_metric(self, metric, labels):
+    def update_metric(self, eval_metric, labels):
-            metric.update(labels_slice, texec.outputs)
+            eval_metric.update(labels_slice, texec.outputs)
-    def __init__(self):
+    def __init__(self, logger=logging):
-        pass
+    def fit(self, train_data, eval_data=None, eval_metric='acc',
-        super(Module, self).__init__()
+    def __init__(self, symbol, input_names, logger=logging, context=ctx.cpu(), work_load_list=None):
-            logging.warning('Already binded, ignoring bind()')
+            self.logger.warning('Already binded, ignoring bind()')
-    def forward(self, data_batch):
+    def forward(self, data_batch, is_train=None):
-        self.exec_group.forward(data_batch)
+        self.exec_group.forward(data_batch, is_train)
-                        force_init=False):
+                       force_init=False):
-            logging.warning('optimizer already initialized, ignoring...')
+            self.logger.warning('optimizer already initialized, ignoring...')
-        self.exec_group.update_metric(metric, labels)
+    def update_metric(self, eval_metric, labels):
-n_epoch = 5
+
-mod.init_optimizer()
+mod.init_optimizer(optimizer_params={'learning_rate':0.01, 'momentum': 0.9})
-from . import mod
+from . import module
-class SymbolModule(BaseModule):
+class Module(BaseModule):
-        super(SymbolModule, self).__init__()
+        super(Module, self).__init__()
-    pass
+    def __init__(self):
-        if self.param_initialized:
+        if self.params_initialized:
-        if self.param_initialized and not force_init:
+        if self.params_initialized and not force_init:
-        self.param_initialized = True
+        self.params_initialized = True
-        aux_arrays = [nd.zeros(s, ctx, dtype=t) for s, t in zip(aux_shape, arg_types)]
+        aux_arrays = [nd.zeros(s, ctx, dtype=t) for s, t in zip(aux_shape, aux_types)]
-        def server_controller(cmd_id, cmd_body):
+        def server_controller(cmd_id, cmd_body, _):
-
+        _ctrl_proto = ctypes.CFUNCTYPE(None, ctypes.c_int, ctypes.c_char_p, ctypes.c_void_p)
-    if args.cluster == 'local' or args.host_file is None:
+    if args.cluster == 'local' or args.host_file is None or args.host_file == 'None':
-    os.chdir("..")
+    os.chdir(cwd)
-    mod.init_params(arg_params=arg_params, aux_params=aux_params)
+else:
-                    cache[name].copy_to(arr)
+                    cache[name].copyto(arr)
-devs = mx.cpu() if args.gpus is None else [
+devs = mx.cpu() if (args.gpus is None or args.gpus == '') else [
-    model_prefix += "-%d" % (kv.rank)
+logging.info('start training for %d epochs...', args.num_epochs)
-                self.logger.info('Epoch[%02d] Train-%s=%f', epoch, name, val)
+                self.logger.info('Epoch[%d] Train-%s=%f', epoch, name, val)
-            self.logger.info('Epoch[%02d] Time cost=%.3f', epoch, (toc-tic))
+            self.logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc-tic))
-                    self.logger.info('Epoch[%02d] Validation-%s=%f', epoch, name, val)
+                    self.logger.info('Epoch[%d] Validation-%s=%f', epoch, name, val)
-                                                'image-classification', 'cifar10'))
+                                                'image-classification', 'cifar10')) + '/'
-mod = mx.mod.Module(symbol, context=devs)
+mod = mx.mod.Module(net, context=devs)
-        batch_end_callback=mx.callback.Speedometer(args.batch_size, 50))
+        batch_end_callback=mx.callback.Speedometer(args.batch_size, 50),
-data_dir = os.path.abspath(os.path.join(os.path.dirname(__FILE__), '..', 'rnn', 'data'))
+data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'rnn', 'data'))
-    vocab = default_build_vocab(os.path.join(data_dir, "train.txt"))
+    vocab = default_build_vocab(os.path.join(data_dir, "ptb.train.txt"))
-                                  context=contexts)
+        mod = mx.mod.BucketingModule(sym_gen, default_bucket_key=buckets[0],
-    mod.fit(data_train, eval_data=data_val,
+    mod.fit(data_train, eval_data=data_val, num_epoch=num_epoch,
-            optimizer_params={'learning_rate':0.01, 'momentum', 0.9, 'wd': 0.00001})
+            optimizer_params={'learning_rate':0.01, 'momentum': 0.9, 'wd': 0.00001})
-                       'Maybe bucketing is used?')
+                assert len(o) == num_outputs, \
-        super(BucketModule, self).__init__(logger=logger)
+        super(BucketingModule, self).__init__(logger=logger)
-        symbol = self.sym_gen(self.default_bucket_key)
+        symbol = self.sym_gen(key)
-        self.optimizer_initialized
+        self.optimizer_initialized = True
-    vocab = default_build_vocab("./data/ptb.train.txt")
+    vocab = default_build_vocab(os.path.join(data_dir, "train.txt"))
-    data_train = BucketSentenceIter("./data/ptb.train.txt", vocab,
+    data_train = BucketSentenceIter(os.path.join(data_dir, "ptb.train.txt"), vocab,
-    data_val = BucketSentenceIter("./data/ptb.valid.txt", vocab,
+    data_val = BucketSentenceIter(os.path.join(data_dir, "ptb.valid.txt"), vocab,
-softmax = mx.symbol.SoftmaxOutput(fc3, name = 'sm')
+softmax = mx.symbol.SoftmaxOutput(fc3, name = 'softmax')
-mod = mx.mod.Module(softmax, ['data', 'sm_label'])
+mod = mx.mod.Module(softmax)
-mod = mx.mod.Module(softmax, ['data', 'sm_label'])
+mod = mx.mod.Module(softmax)
-    pred_label = preds[i].argmax(axis=1)
+    pred_label = preds[i][0].argmax(axis=1)
-        output_list = [[] for _ in range(self.num_outputs)]
+        output_list = []
-                o_list.append(o_nd[:o_nd.shape[0]-pad])
+            output_list.append(self.get_outputs())
-            output_list = [np.concatenate(x) for x in output_list]
+            num_outputs = len(output_list[0])
-        return output_list
+            if num_outputs == 1:
-
+# pylint: disable=C0111,too-many-arguments,too-many-instance-attributes,too-many-locals,redefined-outer-name,fixme
-    def __init__(self, symbol, input_names, logger=logging, context=ctx.cpu(), work_load_list=None):
+    def __init__(self, symbol, input_names=['data', 'softmax_label'], logger=logging,
-                 context=ctx.cpu(), work_load_list=None):
+    def __init__(self, sym_gen, default_bucket_key=None, default_input_names=None,
-        module = Module(symbol, input_names, logger=self.logger, context=self.context,
+        symbol = self.sym_gen(self.default_bucket_key)
-            symbol, input_names = self.sym_gen(bucket_key)
+            symbol, input_names = self._gen_symbol(bucket_key)
-             inputs_need_grad=False, force_rebind=False, shared_group=None):
+             inputs_need_grad=False, force_rebind=False, shared_module=None):
-        self.arg_params = {name:arr for name, arr in zip(self.param_names, param_arrays)}
+        if self.arg_params is None:
-        self.aux_params = {name:arr for name, arr in zip(self.aux_names, aux_arrays)}
+        if self.aux_params is None:
-                grad_req[name] == 'null'
+                grad_req[name] = 'null'
-                    assert arg_arr.shape == arg_shape[j]
+                    assert arg_arr.shape == arg_shapes[j]
-                    assert arg_arr.shape == arg_shape[j]
+                    assert arg_arr.shape == arg_shapes[j]
-            aux_arrays = [nd.zeros(s, ctx, dtype=t) for s, t in zip(aux_shapes, aux_types)]
+            aux_arrays = [nd.zeros(s, context, dtype=t) for s, t in zip(aux_shapes, aux_types)]
-    def score(self, eval_data, eval_metric, num_batch=None, batch_end_callback=None):
+    def forward_backward(self, data_batch, is_train=None):
-                self.backward()
+                self.forward_backward(data_batch, is_train=True)
-
+        assert self.binded and self.params_initialized
-n_epoch = 5
+n_epoch = 2
-                            cb(batch_end_params)
+                self.score(eval_data, eval_metric, batch_end_callback=eval_batch_end_callback)
-            weight = sum(w.copyto(cpu()) for w in block) / len(block)
+            weight = sum(w.copyto(ctx.cpu()) for w in block) / len(block)
-            weight = sum(w.copyto(cpu()) for w in block) / len(block)
+            weight = sum(w.copyto(ctx.cpu()) for w in block) / len(block)
-    def forward(self, data_batch):
+    def forward(self, data_batch, is_train=None):
-        if self.for_training:
+        if is_train is None:
-            texec.forward(is_train=self.for_training)
+            texec.forward(is_train=is_train)
-    def update_metric(self, metric, labels):
+    def update_metric(self, eval_metric, labels):
-            metric.update(labels_slice, texec.outputs)
+            eval_metric.update(labels_slice, texec.outputs)
-    def __init__(self):
+    def __init__(self, logger=logging):
-        pass
+    def fit(self, train_data, eval_data=None, eval_metric='acc',
-        super(Module, self).__init__()
+    def __init__(self, symbol, input_names, logger=logging, context=ctx.cpu(), work_load_list=None):
-            logging.warning('Already binded, ignoring bind()')
+            self.logger.warning('Already binded, ignoring bind()')
-    def forward(self, data_batch):
+    def forward(self, data_batch, is_train=None):
-        self.exec_group.forward(data_batch)
+        self.exec_group.forward(data_batch, is_train)
-                        force_init=False):
+                       force_init=False):
-            logging.warning('optimizer already initialized, ignoring...')
+            self.logger.warning('optimizer already initialized, ignoring...')
-        self.exec_group.update_metric(metric, labels)
+    def update_metric(self, eval_metric, labels):
-n_epoch = 5
+
-mod.init_optimizer()
+mod.init_optimizer(optimizer_params={'learning_rate':0.01, 'momentum': 0.9})
-from . import mod
+from . import module
-class SymbolModule(BaseModule):
+class Module(BaseModule):
-        super(SymbolModule, self).__init__()
+        super(Module, self).__init__()
-    pass
+    def __init__(self):
-        if self.param_initialized:
+        if self.params_initialized:
-        if self.param_initialized and not force_init:
+        if self.params_initialized and not force_init:
-        self.param_initialized = True
+        self.params_initialized = True
-        aux_arrays = [nd.zeros(s, ctx, dtype=t) for s, t in zip(aux_shape, arg_types)]
+        aux_arrays = [nd.zeros(s, ctx, dtype=t) for s, t in zip(aux_shape, aux_types)]
-                num_hidden, num_embed, num_label, dropout=0.):
+                num_hidden, num_embed, num_label, dropout=0.,
-    cls_bias = mx.sym.Variable("cls_bias")
+    with mx.AttrScope(ctx_group='embed'):
-                          h=mx.sym.Variable("l%d_init_h" % i))
+        with mx.AttrScope(ctx_group='layer%d' % i):
-                                  name="t%d_embed" % seqidx)
+        with mx.AttrScope(ctx_group='embed'):
-            last_states[i] = next_state
+            with mx.AttrScope(ctx_group='layer%d' % i):
-    out_prob = [sm]
+
-def setup_rnn_model(ctx,
+def setup_rnn_model(default_ctx,
-                    initializer, dropout=0.):
+                    initializer, dropout=0.,
-                          dropout=dropout)
+                          dropout=dropout,
-    arg_arrays = [mx.nd.zeros(s, ctx) for s in arg_shape]
+    # bind arrays
-    rnn_exec = rnn_sym.bind(ctx=ctx, args=arg_arrays,
+    rnn_exec = rnn_sym.bind(default_ctx, args=arg_arrays,
-                            grad_req="add")
+                            grad_req="add", group2ctx=group2ctx)
-    seq_labels = rnn_exec.arg_dict["label"]
+
-    seq_outputs = out_dict["sm_output"]
+
-        m.seq_labels[seqidx*batch_size : seqidx*batch_size+batch_size] = y
+        if len(m.seq_labels) == 1:
-    nll = -np.sum(np.log(seq_label_probs.asnumpy())) / len(X[0,:])
+    eps = 1e-10
-               num_round, update_period,
+               num_round, update_period, concat_decode,
-            seq_label_probs = mx.nd.choose_element_0index(m.seq_outputs,m.seq_labels)
+            # Change back to individual ops to see if fine grained scheduling helps.
-            train_nll += calc_nll(seq_label_probs, X_train_batch, begin=begin)
+            if concat_decode:
-            seq_label_probs = mx.nd.choose_element_0index(m.seq_outputs,m.seq_labels)
+            if concat_decode:
-            val_nll += calc_nll(seq_label_probs, X_val_batch, begin=begin)
+            if concat_decode:
-    seq_labels = rnn_exec.arg_dict["label"]
+
-num_hidden = 200
+num_hidden = 400
-num_lstm_layer = 2
+num_lstm_layer = 8
-model = lstm.setup_rnn_model(mx.gpu(),
+for i in range(num_lstm_layer):
-#               momentum=momentum)
+# pylint:skip-file
-            if fn is None:
+            fn_obj = getattr(module_obj, fname, None)
-            setattr(module_obj, function.__name__, function)
+            fname = function.__name__
-import functools, itertools
+import functools
-            lhs = broadcast_to(lhs, rhs.shape)
+            lhs = lhs.broadcast_to(rhs.shape)
-            rhs = broadcast_to(rhs, lhs.shape)
+            rhs = rhs.broadcast_to(lhs.shape)
-            lhs = broadcast_to(lhs, rhs.shape)
+            lhs = lhs.broadcast_to(rhs.shape)
-            rhs = broadcast_to(rhs, lhs.shape)
+            rhs = rhs.broadcast_to(lhs.shape)
-            lhs = broadcast_to(lhs, rhs.shape)
+            lhs = lhs.broadcast_to(rhs.shape)
-            rhs = broadcast_to(rhs, lhs.shape)
+            rhs = rhs.broadcast_to(lhs.shape)
-            lhs = broadcast_to(lhs, rhs.shape)
+            lhs = lhs.broadcast_to(rhs.shape)
-            rhs = broadcast_to(rhs, lhs.shape)
+            rhs = rhs.broadcast_to(lhs.shape)
-    return multiply(array, -1.0)
+def negative(arr):
-def sum(a, axis=None, keepdims=False):
+# pylint: disable=too-many-locals, invalid-name, no-member, protected-access, undefined-variable
-    ndim = len(a.shape)
+    ndim = len(arr.shape)
-    elif type(axis) is int:
+    elif isinstance(axis, int):
-    elif type(axis) is tuple:
+    elif isinstance(axis, tuple):
-        if type(i) is not int:
+        if not isinstance(i, int):
-    axis = sorted(map(lambda x: x if 0 <= x else x + ndim, axis))
+    axis = sorted([x if 0 <= x else x + ndim for x in axis])
-    def get_ranges(l):
+    def get_ranges(lst):
-                ret.append((l[i], l[j]))
+        while j < len(lst) - 1:
-        ret.append((l[i], l[j]))
+        ret.append((lst[i], lst[j]))
-        old_shape = shape
+    shape = arr.shape
-    return ret
+# pylint: enable=too-many-locals, invalid-name, no-member, protected-access, undefined-variable
-    numeric_types = (float, int)
+    numeric_types = (float, int, np.float32, np.int32)
-    numeric_types = (float, int, long)
+    numeric_types = (float, int, long, np.float32, np.int32)
-import functools
+import functools, itertools
-    if len(shape) != len(cur_shape):
+    if len(shape) < len(cur_shape):
-    ret = a
+    ret = a.reshape(cur_shape)
-        rsize = reduce(operator.mul, rhs.shape)
+        lsize = functools.reduce(operator.mul, lhs.shape)
-        rsize = reduce(operator.mul, rhs.shape)
+        lsize = functools.reduce(operator.mul, lhs.shape)
-        rsize = reduce(operator.mul, rhs.shape)
+        lsize = functools.reduce(operator.mul, lhs.shape)
-        rsize = reduce(operator.mul, rhs.shape)
+        lsize = functools.reduce(operator.mul, lhs.shape)
-    err_str = 'operands could not be broadcast together with remapped shapes [original->remapped]: {} and requested shape {}'.format(cur_shape, shape)
+    err_str = 'operands could not be broadcast together with remapped shapes'\
-    for index, (i, j) in enumerate(zip(cur_shape, shape)):
+    for axis, (i, j) in enumerate(zip(cur_shape, shape)):
-    return NDArray(handle=cur_handle, writable=a.writable)
+            ret = NDArray._broadcast(ret, axis, j)
-        return NDArray._plus(lhs, rhs)
+    elif isinstance(rhs, NDArray):
-        return NDArray._minus(lhs, rhs)
+    elif isinstance(rhs, NDArray):
-        return NDArray._mul(lhs, rhs)
+    elif isinstance(rhs, NDArray):
-        return NDArray._div(lhs, rhs)
+    elif isinstance(rhs, NDArray):
-            return lhs / rhs
+            return lhs - rhs
-def negate(array):
+def true_divide(lhs, rhs):
-            raise TypeError('type %s not supported' % str(type(other)))
+        return add(self, other)
-            raise TypeError('type %s not supported' % str(type(other)))
+        return subtract(self, other)
-            raise TypeError('type %s not supported' % str(type(other)))
+        return subtract(other, self)
-            raise TypeError('type %s not supported' % str(type(other)))
+        return multiply(self, other)
-            raise TypeError('type %s not supported' % str(type(other)))
+        return divide(self, other)
-            raise TypeError('type %s not supported' % str(type(other)))
+        return divide(other, self)
-    content = read_content(path)
+    content = default_read_content(path)
-from bucket_io import BucketSentenceIter, build_vocab
+from bucket_io import BucketSentenceIter, default_build_vocab
-    vocab = build_vocab("./data/ptb.train.txt")
+    vocab = default_build_vocab("./data/ptb.train.txt")
-def read_content(path):
+def default_read_content(path):
-def build_vocab(path):
+def default_build_vocab(path):
-def text2id(sentence, the_vocab):
+def default_text2id(sentence, the_vocab):
-                 init_states, data_name='data', label_name='label'):
+                 init_states, data_name='data', label_name='label',
-        sentences = content.split(' <eos> ')
+
-            sentence = text2id(sentence, vocab)
+            sentence = self.text2id(sentence, vocab)
-            self.data[i] = self.data[i][:bucket_n_batches[i]*self.batch_size]
+            self.data[i] = self.data[i][:int(bucket_n_batches[i]*self.batch_size)]
-
+    def reset(self):
-    embed_weight=mx.sym.Variable("embed_weight")
+
-                                      h2h_bias = mx.sym.Variable("l%d_h2h_bias" % i)))
+        param_cells.append(LSTMParam(i2h_weight=mx.sym.Variable("l%d_i2h_weight" % i),
-    last_hidden = []
+    loss_all = []
-        data = mx.sym.Variable("t%d_data" % seqidx)
+        data = mx.sym.Variable("data/%d" % seqidx)
-                dp=0.
+            if i == 0:
-                dp = dropout
+                dp_ratio = dropout
-                              seqidx=seqidx, layeridx=i, dropout=dp)
+                              seqidx=seqidx, layeridx=i, dropout=dp_ratio)
-    return nll
+        fc = mx.sym.FullyConnected(data=hidden, weight=cls_weight, bias=cls_bias,
-    print("seq_len=%d" % seq_len)
+    return mx.sym.Group(loss_all)
-    return population[idx]
+def lstm_inference_symbol(num_lstm_layer, input_size,
-                # outputs_batch[i][j] = np.random.choice(vocab, 1, p)
+    hidden = mx.sym.Embedding(data=data, weight=embed_weight,
-    return outputs_batch
+            dp = dropout
-
+import sys
-            "batch_size need to be smaller than data size when not padding."
+            "batch_size need to be smaller than data size."
-            return
+            arg_shapes, _, _ = self.symbol.infer_shape(**dict(input_shapes))
-                return io.NDArrayIter(X, y, self.numpy_batch_size,
+                return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size),
-                return io.NDArrayIter(X, y, self.numpy_batch_size, shuffle=False)
+                return io.NDArrayIter(X, y, min(X.shape[0], self.numpy_batch_size), shuffle=False)
-    """Run the doxygen make commands if we're on the ReadTheDocs server"""
+    """Run the doxygen make commands"""
-    subprocess.call('cd ../recommonmark/; git pull', shell=True)
+# run_build_mxnet("..")
-
+curr_path = os.path.abspath(os.path.dirname(__file__))
-    parser.add_argument('--sync-dir', type=str,
+                        help = 'the hostfile of slave machines which will run \
-                        directory into slave machines\'s SYNC_DIR')
+                        directory into slave machines\'s SYNC_DST_DIR if ssh \
-                        help = 'the lancher to use')
+                        choices = ['local', 'ssh', 'mpi', 'sge', 'yarn'],
-
+    args.command += unknown
-    sys.path.append(os.path.join(curr_path, "../ps-lite/tracker"))
+    args = dmlc_opts(args)
-        launcher = SSHLauncher(args, unknown)
+    if args.cluster == 'local' or args.host_file is None:
-    launcher.run()
+        raise RuntimeError('Unknown submission cluster type %s' % args.cluster)
-
+    fmt = '%(asctime)s %(levelname)s %(message)s'
-            return (self.name, value)
+            names = ['%s_%d'%(self.name, i) for i in range(self.num)]
-    def __init__(self, name):
+    def __init__(self, name, num=None):
-        self.sum_metric = 0.0
+        if self.num == None:
-            return (self.name, float('nan'))
+        if self.num == None:
-            return (self.name, self.sum_metric / self.num_inst)
+            value = [x / y if y != 0 else float('nan') for x, y in zip(self.sum_metric, self.num_inst)]
-                array.copyto(self.arg_dict[name])
+                dst = self.arg_dict[name]
-                array.copyto(self.aux_dict[name])
+                dst = self.aux_dict[name]
-               base_exec=None, shared_data_arrays=None):
+               base_exec=None, shared_data_arrays=None, input_types=None):
-    grad_arrays = {} if need_grad else None
+    grad_arrays = {} if need_grad != False else None
-                arg_arr = nd.zeros(arg_shape[i], ctx)
+                arg_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])
-                    grad_arr = nd.zeros(arg_shape[i], ctx)
+                arg_arr = nd.zeros(arg_shape[i], ctx, dtype=arg_types[i])
-                if need_grad:
+                assert arg_arr.dtype == arg_types[i]
-        aux_arrays = [nd.zeros(s, ctx) for s in aux_shape]
+        aux_arrays = [nd.zeros(s, ctx, dtype=t) for s, t in zip(aux_shape, arg_types)]
-                        grad_req='write' if need_grad else 'null', shared_exec=base_exec)
+                        grad_req=grad_req, shared_exec=base_exec)
-            weight.copyto(arg_params[name])
+            weight.astype(arg_params[name].dtype).copyto(arg_params[name])
-            weight.copyto(aux_params[name])
+            weight.astype(aux_params[name].dtype).copyto(aux_params[name])
-            return zeros(weight.shape, weight.context)
+            return zeros(weight.shape, weight.context, dtype=weight.dtype)
-                zeros(weight.shape, weight.context))  # variance
+        return (zeros(weight.shape, weight.context, dtype=weight.dtype),  # mean
-                if not (name.endswith('data') or name.endswith('label')):
+                if not isinstance(grad_req, dict) or grad_req[name] != 'null':
-sys.path.insert(0, '../unittest')
+import os
-    test_multi_softmax_with_shape((3,4,5), mx.gpu())
+    test_convolution_with_type()
-    exe_test = embed.simple_bind(mx.cpu(), data=(batch,))
+    exe_test = embed.simple_bind(mx.cpu(), grad_req={'data': 'null', 'embed_weight': 'write'}, data=(batch,))
-# steps).
+# steps). We are not including a sample running log here, as this test case
-def ConvFactory(data, num_filter, kernel, stride=(1,1), pad=(0, 0), act_type="relu"):
+def ConvFactory(data, num_filter, kernel, stride=(1,1), pad=(0, 0), act_type="relu", mirror_attr={}):
-    act = mx.symbol.Activation(data = bn, act_type=act_type)
+    act = mx.symbol.Activation(data = bn, act_type=act_type, attr=mirror_attr)
-def DownsampleFactory(data, ch_3x3):
+def DownsampleFactory(data, ch_3x3, mirror_attr):
-    conv = ConvFactory(data=data, kernel=(3, 3), stride=(2, 2), num_filter=ch_3x3, pad=(1, 1))
+    conv = ConvFactory(data=data, kernel=(3, 3), stride=(2, 2), num_filter=ch_3x3, pad=(1, 1), mirror_attr=mirror_attr)
-    pool = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pool_type='max')
+    pool = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pool_type='max', attr=mirror_attr)
-def SimpleFactory(data, ch_1x1, ch_3x3):
+def SimpleFactory(data, ch_1x1, ch_3x3, mirror_attr):
-    conv1x1 = ConvFactory(data=data, kernel=(1, 1), pad=(0, 0), num_filter=ch_1x1)
+    conv1x1 = ConvFactory(data=data, kernel=(1, 1), pad=(0, 0), num_filter=ch_1x1, mirror_attr=mirror_attr)
-    conv3x3 = ConvFactory(data=data, kernel=(3, 3), pad=(1, 1), num_filter=ch_3x3)
+    conv3x3 = ConvFactory(data=data, kernel=(3, 3), pad=(1, 1), num_filter=ch_3x3, mirror_attr=mirror_attr)
-def get_symbol(num_classes = 10):
+def get_symbol(num_classes = 10, force_mirroring=False):
-    flatten = mx.symbol.Flatten(data=pool, name="flatten1")
+    conv1 = ConvFactory(data=data, kernel=(3,3), pad=(1,1), num_filter=96, act_type="relu", mirror_attr=attr)
-net = importlib.import_module('symbol_' + args.network).get_symbol(10)
+net_gen = importlib.import_module('symbol_' + args.network)
-print("  WITH mirroring")
+print("  WITH mirroring via environment variable")
-parser.add_argument('--num-epochs', type=int, default=20,
+parser.add_argument('--num-epochs', type=int, default=1,
-            logging.info('        GPU Memory: %.2f%%' % 100.0*free / total)
+            logging.info('        GPU Memory: %.2f%%' % (100.0*free / total))
-    def __init__(self, rescale_grad=1, arg_names=None, wd=0.):
+    def __init__(self, rescale_grad=1., arg_names=None, wd=0., clip_gradient=None):
-        super(SGD, self).__init__(rescale_grad, arg_names, wd)
+        super(SGD, self).__init__(rescale_grad=rescale_grad, arg_names=arg_names, wd=wd,
-        super(SGLD, self).__init__(rescale_grad, arg_names)
+        super(SGLD, self).__init__(rescale_grad=rescale_grad, arg_names=arg_names, wd=wd,
-        super(ccSGD, self).__init__(rescale_grad, arg_names, wd)
+        super(ccSGD, self).__init__(rescale_grad=rescale_grad, arg_names=arg_names, wd=wd,
-        super(Adam, self).__init__(rescale_grad, arg_names, wd)
+        super(Adam, self).__init__(rescale_grad=rescale_grad, arg_names=arg_names, wd=wd,
-        super(AdaGrad, self).__init__(rescale_grad, arg_names, wd)
+        super(AdaGrad, self).__init__(rescale_grad=rescale_grad, arg_names=arg_names, wd=wd,
-        self.clip_gradient = clip_gradient
+        if lr_scheduler is not None:
-        return zeros(weight.shape, weight.context)
+        return zeros(weight.shape, weight.context) # history
-        super(RMSProp, self).__init__(rescale_grad, arg_names, wd)
+        super(RMSProp, self).__init__(rescale_grad=rescale_grad, arg_names=arg_names, wd=wd,
-        self.clip_gradient = clip_gradient
+        self.lr_scheduler = lr_scheduler
-        super(AdaDelta, self).__init__(**kwargs)
+                 wd=0.000001, rescale_grad=1., clip_gradient=None, arg_names=None):
-                    help='the prefix of the model to load/save')
+                    help='the prefix of the model to load')
-def fit(args, network, data_loader):
+def fit(args, network, data_loader, batch_end_callback=None):
-        batch_end_callback = mx.callback.Speedometer(args.batch_size, 50),
+        batch_end_callback = batch_end_callback,
-# pylint: disable=superfluous-parens, no-member
+# pylint: disable=superfluous-parens, no-member, invalid-name
-                dp=0.
+            if i == 0:
-                dp = dropout
+                dp_ratio = dropout
-                              seqidx=seqidx, layeridx=i, dropout=dp)
+                              seqidx=seqidx, layeridx=i, dropout=dp_ratio)
-                                    buckets, batch_size, init_states)
+    data_val = BucketSentenceIter("./data/ptb.valid.txt", vocab,
-            initializer   = mx.init.Xavier(factor_type="in", magnitude=2.34))
+    model = mx.model.FeedForward(ctx=contexts,
-              batch_end_callback = mx.callback.Speedometer(batch_size, 50),)
+              batch_end_callback=mx.callback.Speedometer(batch_size, 50),)
-# pylint:skip-file
+# pylint: disable=C0111,too-many-arguments,too-many-instance-attributes,too-many-locals,redefined-outer-name,fixme
-import mxnet as mx
+import mxnet as mx
-from lstm import LSTMState, LSTMParam, LSTMModel, lstm
+from lstm import LSTMState, LSTMParam, lstm
-        content = input.read()
+    with open(path) as ins:
-    vocab = {}
+    the_vocab = {}
-            vocab[word] = idx
+        if not word in the_vocab:
-    return vocab
+    return the_vocab
-def text2id(sentence, vocab):
+def text2id(sentence, the_vocab):
-    words = [vocab[w] for w in words if len(w) > 0]
+    words = [the_vocab[w] for w in words if len(w) > 0]
-            data_name='data', label_name='label'):
+                 init_states, data_name='data', label_name='label'):
-        self.data = [[] for k in buckets]
+        self.data = [[] for _ in buckets]
-        data = [np.zeros((len(x), buckets[i])) for i,x in enumerate(self.data)]
+        data = [np.zeros((len(x), buckets[i])) for i, x in enumerate(self.data)]
-                data[i_bucket][j,:len(sentence)] = sentence
+                data[i_bucket][j, :len(sentence)] = sentence
-            print("bucket of len %3d : %d samples" % (bkt, sz))
+        for bkt, size in zip(buckets, bucket_sizes):
-                for t in range(self.default_bucket_key)] + init_states
+                             for t in range(self.default_bucket_key)] + init_states
-                for t in range(self.default_bucket_key)]
+                              for t in range(self.default_bucket_key)]
-        bucket_plan = np.hstack([np.zeros(n, int)+i for i,n in enumerate(bucket_n_batches)])
+        bucket_plan = np.hstack([np.zeros(n, int)+i for i, n in enumerate(bucket_n_batches)])
-            label[:,-1] = 0
+            label[:, :-1] = data[:, 1:]
-                    for t in range(self.buckets[i_bucket])] + self.init_state_arrays
+                        for t in range(self.buckets[i_bucket])] + self.init_state_arrays
-                    for t in range(self.buckets[i_bucket])]
+                         for t in range(self.buckets[i_bucket])]
-                    for t in range(self.buckets[i_bucket])] + init_state_names
+                          for t in range(self.buckets[i_bucket])] + init_state_names
-                    for t in range(self.buckets[i_bucket])]
+                           for t in range(self.buckets[i_bucket])]
-                              self.buckets[i_bucket])
+                                     self.buckets[i_bucket])
-    embed_weight=mx.sym.Variable("embed_weight")
+    embed_weight = mx.sym.Variable("embed_weight")
-                                      h2h_bias = mx.sym.Variable("l%d_h2h_bias" % i)))
+        param_cells.append(LSTMParam(i2h_weight=mx.sym.Variable("l%d_i2h_weight" % i),
-        self.n_batch = n_batch
+        self.make_data_iter_plan()
-                          shape=(self.batch_size, self.vocab_size)).todense()
+    def make_data_iter_plan(self):
-        init_state_names = [x[0] for x in self.init_states]
+        bucket_plan = np.hstack([np.zeros(n, int)+i for i,n in enumerate(bucket_n_batches)])
-                    break
+        bucket_idx_all = [np.random.permutation(len(x)) for x in self.data]
-                label[i, :len(sentence)-1] = sentence[1:]
+        for i_bucket in self.bucket_plan:
-            symbol        = sym_gen,
+            symbol        = symbol,
-    def __init__(self, learning_rate=0.05, wd=0., rescale_grad=1, eps=1e-7, arg_names=None):
+    def __init__(self, learning_rate=0.05, wd=0., rescale_grad=1, eps=1e-7, clip_gradient=None,
-        return zeros(weight.shape, weight.context)   #history
+        return zeros(weight.shape, weight.context)
-        weight[:] += -self.lr * (grad / sqrt(history + self.float_stable_eps) + self.wd * weight)
+        weight[:] += -lr * (grad / sqrt(history + self.float_stable_eps) + self.wd * weight)
-#     app.add_transform(AutoStructify)
+def setup(app):
-                         param.epoch, param.nbatch, name, value)
+        if param.nbatch % period == 0 and param.eval_metric is not None:
-                                 param.epoch, count, speed, name, value)
+                    name_value = param.eval_metric.get_name_value()
-def create(metric):
+def create(metric, **kwargs):
-
+    elif isinstance(metric, list):
-        return metrics[metric.lower()]
+        return metrics[metric.lower()](**kwargs)
-        A evaluation function.
+        An evaluation function or a list of evaluation functions.
-        logger.info('Epoch[%d] Train-%s=%f', epoch, name, value)
+        name_value = eval_metric.get_name_value()
-            logger.info('Epoch[%d] Validation-%s=%f', epoch, name, value)
+            name_value = eval_metric.get_name_value()
-                    help='the prefix of the model to load/save')
+                    help='the prefix of the model to load')
-    checkpoint = None if model_prefix is None else mx.callback.do_checkpoint(model_prefix)
+    save_model_prefix = args.save_model_prefix
-    'nvml.h', 'opencv2/opencv.hpp', 'sys/stat.h', 'sys/types.h'
+    'nvml.h', 'opencv2/opencv.hpp', 'sys/stat.h', 'sys/types.h', 'cuda.h', 'cuda_fp16.h'
-            raise ValueError("Factor must be less than 1 to make lr reduce")
+        if factor > 1.0:
-            raise ValueError("Factor must be less than 1 to make lr reduce")
+        if factor > 1.0:
-    contexts = [mx.context.cpu(i) for i in range(1)]
+    contexts = [mx.context.gpu(i) for i in range(1)]
-                arg_arr = base_exec.arg_arrays[i]
+                arg_arr = base_exec.arg_dict[name]
-                    grad_arrays[name] = base_exec.grad_arrays[i]
+                    grad_arrays[name] = base_exec.grad_dict[name]
-        if self.cur_step_ind <= len(self.step)-1:        
+        if self.cur_step_ind <= len(self.step)-1:
-        'mse' : MSE()
+        'mse' : MSE(),
-            self.num_inst += numpy.prod(label.shape)
+            self.sum_metric += numpy.abs(label - pred).mean()
-            self.num_inst += numpy.prod(label.shape)
+            self.num_inst += 1 # numpy.prod(label.shape)
-        self.num_inst += 1
+            self.num_inst += 1
-
+    elif isinstance(metric, EvalMetric):
-        grad = grad*self.rescale_grad
+        grad = grad * self.rescale_grad
-        weight[:] += delta
+        weight[:] += -self.lr * (grad / sqrt(history + self.float_stable_eps) + self.wd * weight)
-                                   [output_buff[output_names[0]].asnumpy()])
+                self.metric.update([input_buffs[-1]],
-fo.writerow(fi.__next__())
+fo.writerow(fi.__next__()) # Python2: fo.writerow(fi.next())
-            q_out.put(('data', s))
+            q_out.put(('data', s, item))
-        cnt = 0
+        sink = []
-            stat, s = q_out.get()
+            stat, s, item = q_out.get()
-            if cnt % 1000 == 0:
+            sink.append(item)
-                print 'time:', cur_time - pre_time, ' count:', cnt
+                print 'time:', cur_time - pre_time, ' count:', len(sink)
-        q_out.put(('finish', ''))
+        q_out.put(('finish', '', []))
-            _, s = q_out.get()
+            _, s, _ = q_out.get()
-        img = cv2.imread(os.path.join(args.root, item[1]), color_modes[args.color])
+    total = len(source)
-        lock.release()
+
-        multi_available = True
+        import multiprocessing
-            worker(len(sink))
+        import Queue
-            if aux_params:
+            if self.aux_params:
-                self.arg_params = {k : v for k, v in arg_params.items()
+                self.arg_params = {k : v for k, v in self.arg_params.items()
-                self.aux_params = {k : v for k, v in aux_params.items()
+                self.aux_params = {k : v for k, v in self.aux_params.items()
-            self.symbol = self.sym_gen(data.default_bucket_key)
+            self.symbol = self.sym_gen(data.default_bucket_key) # pylint: disable=no-member
-            q_out.put(('data', s))
+            q_out.put(('data', s, item))
-        cnt = 0
+        sink = []
-            stat, s = q_out.get()
+            stat, s, item = q_out.get()
-            if cnt % 1000 == 0:
+            sink.append(item)
-                print 'time:', cur_time - pre_time, ' count:', cnt
+                print 'time:', cur_time - pre_time, ' count:', len(sink)
-        q_out.put(('finish', ''))
+        q_out.put(('finish', '', []))
-            _, s = q_out.get()
+            _, s, _ = q_out.get()
-        img = cv2.imread(os.path.join(args.root, item[1]), color_modes[args.color])
+    total = len(source)
-        lock.release()
+
-        multi_available = True
+        import multiprocessing
-            worker(len(sink))
+        import Queue
-            sym_gen       = sym_gen,
+            symbol        = sym_gen,
-            sym_gen=symbol_generator)
+    train_mnist.train_model.fit(args, symbol_generator, get_iterator(buckets))
-def fit(args, network, data_loader, sym_gen=None):
+def fit(args, network, data_loader):
-    def __init__(self, symbol, ctx=None, sym_gen=None,
+    def __init__(self, symbol, ctx=None,
-                              if k in aux_names}
+
-        self.aux_params = aux_params
+    def _check_arguments(self):
-sys.path.insert(0, "../../../python/")
+sys.path.insert(0, "../../python/")
-            init_states, n_batch=None, 
+            init_states, n_batch=None,
-        self.provide_data = [('%s/%d' % (self.data_name, t), (self.batch_size, self.vocab_size))
+        self.provide_data = [('%s/%d' % (self.data_name, t), (self.batch_size,))
-        return coo_matrix((np.ones(len(x)), (np.arange(len(x)), x)), 
+        return coo_matrix((np.ones(len(x)), (np.arange(len(x)), x)),
-            data_all = [mx.nd.array(self.embed_data(data[:, t]))
+            data_all = [mx.nd.array(data[:, t])
-                                       name="t%d_embed" % seqidx)
+        hidden = mx.sym.Embedding(data=data, weight=embed_weight,
-    contexts = [mx.context.gpu(i) for i in range(1)]
+    contexts = [mx.context.cpu(i) for i in range(1)]
-                    choices = ['alexnet', 'vgg', 'googlenet', 'inception-bn', 'inception-bn-full'],
+                    choices = ['alexnet', 'vgg', 'googlenet', 'inception-bn', 'inception-bn-full', 'inception-v3'],
-            init_states, n_batch=None,
+            init_states, n_batch=None, 
-        self.provide_data = [('%s/%d' % (self.data_name, t), (self.batch_size,))
+        self.provide_data = [('%s/%d' % (self.data_name, t), (self.batch_size, self.vocab_size))
-            data_all = [mx.nd.array(data[:, t])
+            data_all = [mx.nd.array(self.embed_data(data[:, t]))
-                                  name="t%d_embed" % seqidx)
+        #hidden = mx.sym.Embedding(data=data, weight=embed_weight,
-from executor_manager import _split_input_slice, _check_arguments, _load_data, _load_label
+from .executor_manager import _split_input_slice, _check_arguments, _load_data, _load_label
-            bucket_batch.provide_label = self.provide_label
+            bucket_batch.provide_data = deepcopy(self.provide_data)
-    buckets = ['foo', 'bar', 'baz']
+    buckets = [1, 2, 3]
-
+
-            yield SimpleBatch(data_names, data_all, label_names, label_all,
+            data_batch = SimpleBatch(data_names, data_all, label_names, label_all,
-    buckets = [10, 15, 20, 30, 40]
+    batch_size = 32
-    contexts = mx.context.cpu()
+    contexts = [mx.context.gpu(i) for i in range(1)]
-              batch_end_callback = mx.callback.Speedometer(batch_size, 10),)
+              batch_end_callback = mx.callback.Speedometer(batch_size, 50),)
-    return mx.sym.Group(loss_all + unpack_c + unpack_h)
+    # for i in range(num_lstm_layer):
-    model.fit(X=data_train, eval_data=data_val, eval_metric=mx.metric.Accuracy(do_check_label_shapes=False))
+    model.fit(X=data_train, eval_data=data_val,
-    def __init__(self, do_check_label_shapes=True):
+    def __init__(self):
-            check_label_shapes(labels, preds)
+        check_label_shapes(labels, preds)
-    def __init__(self):
+    def __init__(self, do_check_label_shapes=True):
-        check_label_shapes(labels, preds)
+        if self.do_check_label_shapes:
-                        eval_batch_end_callback=None):
+                        eval_batch_end_callback=None, sym_gen=None):
-                            eval_batch_end_callback=eval_batch_end_callback)
+                            eval_batch_end_callback=eval_batch_end_callback,
-    """A group of executors living on different devices, for data parallelization."""
+    """A group of executors living on different devices, for data parallelization.
-        input shapes.
+        input shapes. Used only for bucketing.
-              base_exec=None, shared_data_arrays=None):
+def _bind_exec(sym, ctx, input_shapes, param_names, need_grad=False,
-    def __init__(self, sym, param_names, ctx, slices, train_data, shared_group=None):
+    """A group of executors living on different devices, for data parallelization."""
-                                        shared_data_arrays=self.shared_data_arrays[i])
+                                    need_grad=True, base_exec=shared_exec,
-                 param_names, aux_names,
+                 arg_names, param_names, aux_names,
-        self.execgrp = DataParallelExecutorGroup(symbol, self.param_names, self.ctx,
+        self.execgrp = DataParallelExecutorGroup(symbol, self.arg_names, self.param_names, self.ctx,
-                execgrp = DataParallelExecutorGroup(symbol, self.param_names, self.ctx,
+                execgrp = DataParallelExecutorGroup(symbol, self.arg_names,
-    sym_gen: a symbol generator, used for bucketing.
+    sym_gen: a symbol generator, useful for bucketing.
-import logging
+def _bind_exec(self, sym, ctx, input_shapes, param_names, need_grad=False,
-                    shared_data_arrays=self.shared_data_arrays[i])
+            train_exec = _bind_exec(sym, ctx[i], data_shapes, self.param_names,
-                 param_names, arg_names, aux_names,
+                 param_names, aux_names,
-                self.slices, train_data)
+                                                 self.slices, train_data)
-                        self.slices, data_batch, shared_group=self.execgrp)
+                                                    self.slices, data_batch,
-from collections import namedtuple, OrderedDict
+from collections import OrderedDict
-
+    """Default object for holding a mini-batch of data and related information."""
-            bucket_key=None, provide_data=None, provide_label=None):
+                 bucket_key=None, provide_data=None, provide_label=None):
-                                                   arg_names=arg_names,
+        if shared_group is None:
-                    need_grad=True, base_exec=shared_exec)
+                    need_grad=True, base_exec=shared_exec,
-    def bind_exec(self, sym, ctx, input_shapes, param_names, need_grad=False, base_exec=None):
+    def bind_exec(self, sym, ctx, input_shapes, param_names, need_grad=False,
-                arg_arrays.append(nd.zeros(arg_shape[i], ctx))
+            name = arg_names[i]
-                        grad_arrays[arg_names[i]] = grad_arr
+                        grad_arrays[name] = grad_arr
-                        grad_arrays[arg_names[i]] = base_exec.grad_arrays[i]
+                        grad_arrays[name] = base_exec.grad_arrays[i]
-            bucket_batch = BucketBatch(self, batch)
+            bucket_batch = batch
-    net = get_lenet()
+def get_iterator(data_shape):
-        part_index  = kv.rank)
+if __name__ == '__main__':
-    return (train, val)
+    if args.network == 'mlp':
-train_model.fit(args, net, get_iterator)
+    # train
-def fit(args, network, data_loader):
+def fit(args, network, data_loader, sym_gen=None):
-        if not os.path.exists(log_dir): 
+        if not os.path.exists(log_dir):
-            self.execgrp_bucket = {train_data.default_key: self.execgrp}
+            self.execgrp_bucket = {train_data.default_bucket_key: self.execgrp}
-            self.curr_execgrp = self.execgrp
+            key = data_batch.bucket_key
-    sym_gen: a symbol generator, useful for bucketing.
+    sym_gen: a symbol generator, used for bucketing.
-                 work_load_list=None, logger=None):
+                 work_load_list=None, logger=None, sym_gen=None):
-        self.param_names = [arg_names[i] for i in self.param_idx]
+        self.param_names = param_names
-        update_dict = {k: 'write' if k in self.param_names else 'null' for k in arg_names}
+        self.execgrp = DataParallelExecutorGroup(symbol, self.param_names, self.ctx,
-        # data structure
+        self.sym_gen = sym_gen
-        for train_exec in self.train_execs:
+        if self.sym_gen is not None:
-        for texec in self.train_execs:
+        for texec in self.execgrp.train_execs:
-        _load_label(data_batch, self.label_arrays)
+        if self.sym_gen is not None:
-            texec.forward(is_train=is_train)
+        self.curr_execgrp.forward(is_train=is_train)
-            texec.backward()
+        self.curr_execgrp.backward()
-
+        self.curr_execgrp.update_metric(metric, labels)
-    def __init__(self, symbol, ctx=None,
+    def __init__(self, symbol, ctx=None, sym_gen=None,
-            metric.update(labels_slice, texec.outputs)
+# coding: utf-8
-from .executor import DataParallelExecutorManager, _check_arguments, _load_data
+from .executor_manager import DataParallelExecutorManager, _check_arguments, _load_data
-        f = shape[3] / 2.
+        f = np.ceil(shape[3] / 2.)
-            darr = self.grad_arrays[i]
+            darr = None if self.grad_arrays is None else self.grad_arrays[i]
-                "Cannot Initialize %s. Not found in loaded param " + \
+                "Cannot Initialize %s. Not found in loaded param "%name + \
-    def __init__(self, handle, symbol):
+    def __init__(self, handle, symbol, ctx, grad_req, group2ctx):
-        self._symbol = symbol
+        self._symbol = copy.deepcopy(symbol)
-    def bind(self, ctx, args, args_grad=None, grad_req='write', aux_states=None, group2ctx=None):
+    def bind(self, ctx, args, args_grad=None, grad_req='write',
-        executor = Executor(handle, self)
+        shared_handle = shared_exec.handle if shared_exec is not None else ExecutorHandle()
-                        eval_iters=None, eval_batch_end_callback=None):
+                        eval_batch_end_callback=None):
-               eval_iters=None, eval_batch_end_callback=None, **kwargs):
+               eval_batch_end_callback=None, **kwargs):
-                            epoch_size=epoch_size, eval_iters=eval_iters,
+                            epoch_size=epoch_size,
-                            'eval_metric'])
+                            'eval_metric',
-                                                     eval_metric=eval_metric)
+                                                     eval_metric=eval_metric,
-                                                     eval_metric=eval_metric)
+                                                     eval_metric=eval_metric,
-        train_data.reset()
+                logger.info('Epoch[%d] Resetting Data Iterator', epoch)
-                c_array(ctypes.c_char_p, [])))
+                ctypes.c_int(len(kwargs)), \
-def log_train_metric(period):
+def log_train_metric(period, auto_reset=False):
-            train_exec = symbol.simple_bind(ctx[i], 'write', **data_shapes)
+            train_exec = symbol.simple_bind(ctx[i], update_dict, **data_shapes)
-        for texec, islice in zip(self.train_execs, self.slices):
+        for texec in self.train_execs:
-                dev_out.copyto(cpu_out[islice])
+
-        return (self.name, self.sum_metric / self.num_inst)
+        if self.num_inst == 0:
-            self.num_inst += pred_label.shape[0]
+            self.sum_metric += (pred_label.flat == label.flat).sum()
-                        logger=None, work_load_list=None, monitor=None):
+                        logger=None, work_load_list=None, monitor=None,
-                eval_metric.update(data_batch.label, executor_manager.cpu_output_arrays)
+                # evaluate at end, so we can lazy copy
-            logger.info('Epoch[%d] Validation-%s=%f', epoch, name, value)
+
-        param_names = set(arg_names) - set(input_names)
+        param_names = [key for key in arg_names if key not in input_names]
-    def predict(self, X, num_batch=None):
+    def predict(self, X, num_batch=None, return_data=False, reset=True):
-        X.reset()
+        if reset:
-        outputs = [np.concatenate(x) for x in output_list]
+            if return_data:
-            return outputs[0]
+            outputs = outputs[0]
-            work_load_list=None, monitor=None):
+            work_load_list=None, monitor=None, eval_batch_end_callback=None):
-                            logger=logger, work_load_list=work_load_list, monitor=monitor)
+                            logger=logger, work_load_list=work_load_list, monitor=monitor,
-               kvstore='local', logger=None, work_load_list=None, **kwargs):
+               kvstore='local', logger=None, work_load_list=None,
-        model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch, epoch_size=epoch_size,
+        model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch,
-                  work_load_list=work_load_list)
+                  work_load_list=work_load_list,
-from .ndarray import NDArray, zeros, clip, sqrt, norm
+from .ndarray import NDArray, zeros, clip, sqrt
-    def __init__(self, rescale_grad=1, arg_names=None):
+    def __init__(self, rescale_grad=1, arg_names=None, wd=0.):
-        super(SGD, self).__init__(rescale_grad, arg_names)
+        super(SGD, self).__init__(rescale_grad, arg_names, wd)
-                wd = self.wd
+        wd = self._get_wd(index)
-        super(ccSGD, self).__init__(rescale_grad)
+        super(ccSGD, self).__init__(rescale_grad, arg_names, wd)
-        self.wd = wd
+        wd = self._get_wd(index)
-                                          mx_float(self.wd)))
+                                          mx_float(wd)))
-        super(Adam, self).__init__(rescale_grad, arg_names)
+        super(Adam, self).__init__(rescale_grad, arg_names, wd)
-            step += lr * self.wd * weight
+        wd = self._get_wd(index)
-        super(RMSProp, self).__init__(rescale_grad, arg_names)
+        super(RMSProp, self).__init__(rescale_grad, arg_names, wd)
-                wd = self.wd
+        wd = self._get_wd(index)
-            if name.startswith('arg:'):
+            if name.startswith('arg:') or name.startswith('aux:'):
-            pred = preds[i].asnumpy()
+            pred_label = ndarray.argmax_channel(preds[i]).asnumpy().astype('int32')
-            check_label_shapes(label, pred)
+            check_label_shapes(label, pred_label)
-from .ndarray import NDArray, zeros, clip, sqrt
+from .ndarray import NDArray, zeros, clip, sqrt, norm
-                 lr_scheduler=None):
+                 lr_scheduler=None, arg_names=None):
-            [momentum, wd, rescale_grad, clip_gradient])
+            ['momentum', 'rescale_grad', 'clip_gradient'],
-                                          mx_float(lr)))
+                                          mx_float(lr),
-	ctypes.byref(ret_type)))
+        ctypes.byref(ret_type)))
-	ctypes.byref(ret_type)))
+        ctypes.byref(ret_type)))
-	ctypes.byref(ret_type)))
+        ctypes.byref(ret_type)))
-        ctypes.byref(arg_descs)))
+        ctypes.byref(arg_descs),
-        ctypes.byref(key_var_num_args)))
+        ctypes.byref(key_var_num_args),
-        ctypes.byref(arg_descs)))
+        ctypes.byref(arg_descs),
-                    help='set image's shape')
+                    help='set image\'s shape')
-        path_imgrec = args.data_dir + args.train_dataset,
+        path_imgrec = os.path.join(args.data_dir, args.train_dataset),
-        path_imgrec = args.data_dir + args.val_dataset,
+        path_imgrec = os.path.join(args.data_dir, args.val_dataset),
-        # Dataset Paramter
+        # Dataset Parameter
-        # Dataset/Augment Paramter
+        # Dataset/Augment Parameter
-        # Batch Paramter
+        # Batch Parameter
-        # randly mirror the image horizontally
+        # randomly mirror the image horizontally
-        calcutaion frequent
+        calculation frequent
-    This function is a short cut for Context('cpu', device_id)
+    This function is a short cut for Context('gpu', device_id)
-        A list of ndarray binded to the heads of executor.
+        A list of ndarray bound to the heads of executor.
-        """Calculate the outputs specified by the binded symbol.
+        """Calculate the outputs specified by the bound symbol.
-            raise ValueError('This function support variable length of Symbol arguments.\n' +
+            raise ValueError('This function supports variable length of Symbol arguments.\n' +
-    stytole = label_data[:, 1]
+    systole = label_data[:, 1]
-            (x < np.arange(600)) for x in stytole
+    systole_encode = np.array([
-    return stytole_encode, diastole_encode
+    return systole_encode, diastole_encode
-    np.savetxt(stytole_csv, stytole_encode, delimiter=",", fmt="%g")
+def encode_csv(label_csv, systole_csv, diastole_csv):
-encode_csv("./train-label.csv", "./train-stytole.csv", "./train-diastole.csv")
+encode_csv("./train-label.csv", "./train-systole.csv", "./train-diastole.csv")
-# # Training the stytole net
+# # Training the systole net
-                           label_csv="./train-stytole.csv", label_shape=(600,),
+                           label_csv="./train-systole.csv", label_shape=(600,),
-stytole_model = mx.model.FeedForward(ctx=devs,
+systole_model = mx.model.FeedForward(ctx=devs,
-stytole_model.fit(X=data_train, eval_metric = mx.metric.np(CRPS))
+systole_model.fit(X=data_train, eval_metric = mx.metric.np(CRPS))
-# # Predict stytole
+# # Predict systole
-stytole_prob = stytole_model.predict(data_validate)
+systole_prob = systole_model.predict(data_validate)
-stytole_result = accumulate_result("./validate-label.csv", stytole_prob)
+systole_result = accumulate_result("./validate-label.csv", systole_prob)
-    if key in stytole_result:
+    if key in systole_result:
-            out.extend(list(submission_helper(stytole_result[key])))
+            out.extend(list(submission_helper(systole_result[key])))
-                self.data[k] = self.data_list[0][:new_n]
+                data_dict[k] = data_dict[k][:new_n]
-                self.label[k] = self.data_list[1][:new_n]
+                label_dict[k] = label_dict[k][:new_n]
-                self.data[k] = self.data[k][:new_n]
+                self.data[k] = self.data_list[0][:new_n]
-                self.label[k] = self.label[k][:new_n]
+                self.label[k] = self.data_list[1][:new_n]
-        self.map = zip([re.compile(p) for p in patterns], initializers)
+        self.map = list(zip([re.compile(p) for p in patterns], initializers))
-
+    
- va_fo = csv.writer(open(va_fo_name, "w"), delimiter='\t', lineterminator='\n')
+    tr_fo_name=os.path.join(args.out_folder+"tr.lst")
-        fo.writerow(item)
+    fo.writerow(item)
-		    help='the path of log file')
+parser.add_argument('--log-file', type=str,default="log_tr_va",
-                      label="Train accuracy")
+         label="Train accuracy")
-                      label="Validation accuracy")
+         label="Validation accuracy")
-fo = csv.writer(open(sys.argv[3], "w"), delimiter='\t', lineterminator='\n')
+fo_name=os.path.join(args.out_folder+args.out_file)
-if task == "train":
+if args.train:
-        lst = os.listdir(fi + head[i])
+        path = args.image_folder + head[i]
-    lst = os.listdir(fi)
+    lst = os.listdir(args.image_folder)
-        img_lst.append((cnt, 0, fi + img))
+        img_lst.append((cnt, 0, args.image_folder + img))
-    fo.writerow(item)
+        fo.writerow(item)
-    User can also inheritate this object to change naming behavior.
+    User can also inherit this object to change naming behavior.
-        Shape of input data batch.
+    monitor : Monitor, optional
-    - This function will inplace update the NDArrays in arg_parans and aux_states.
+    - This function will inplace update the NDArrays in arg_params and aux_states.
-        param_names = list(set(arg_names) - set(input_names))
+        param_names = set(arg_names) - set(input_names)
-        return (arg_names, param_names, aux_names)
+        return (arg_names, list(param_names), aux_names)
-    User can also inheritate this object to change naming behavior.
+    User can also inherit this object to change naming behavior.
-        """Return a sliiced NDArray that shares memory with current one.
+        """Return a sliced NDArray that shares memory with current one.
-        """List all auxiliary states in the symbool.
+        """List all auxiliary states in the symbol.
-        args_handle, args = self._get_ndarray_inputs('args', args, self.list_arguments(), False)
+        listed_arguments = self.list_arguments()
-                'args_grad', args_grad, self.list_arguments(), True)
+                'args_grad', args_grad, listed_arguments, True)
-            reqs_array = c_array(mx_uint, [mx_uint(req_map[grad_req])] * len(self.list_arguments()))
+            reqs_array = c_array(mx_uint, [mx_uint(req_map[grad_req])] * len(listed_arguments))
-            for name in self.list_arguments():
+            for name in listed_arguments:
-    exec4 = ret.bind(mx.Context('cpu'),
+class Orthogonal(Initializer):
-    print(net2.debug_str())
+    #print(net2.debug_str())
-    print(composed.debug_str())
+    #print(composed.debug_str())
-def test_infer_shape():
+def test_symbol_infer_shape():
-        check_call(_LIB.MXSymbolInferShape(
+        if partial:
-    data_shape = (3, 224, 224)
+    data_shape = (3, args.data_shape, args.data_shape)
-        super(Adam, self).__init__(rescale_grad, arg_names=None)
+        super(Adam, self).__init__(rescale_grad, arg_names)
-        super(Adam, self).__init__(rescale_grad)
+                 lr_scheduler=None, arg_names=None):
-    print "Usage: gen_img_list.py train/test sample_submission.csv train_folder img.lst"
+if len(sys.argv) < 3:
-fo = csv.writer(open(sys.argv[4], "w"), delimiter='\t', lineterminator='\n')
+fi = sys.argv[2]
-head = head[1:]
+
-#wirte
+#write
-
+import pandas as pd
-    y = np.concatenate([label.asnumpy() for _, label in data]).astype('int')
+    y = np.concatenate([label[0].asnumpy() for _, label, _, _ in data]).astype('int')
-    Y = np.concatenate([y.asnumpy() for _, y in data_iter])
+    Y = np.concatenate([y[0].asnumpy() for _, y, _, _ in data_iter])
-    X = np.concatenate([x.asnumpy() for x, _ in data_iter])
+    X = np.concatenate([x[0].asnumpy() for x, _, _, _ in data_iter])
-    elif os.name == "posix" and os.environ['LD_LIBRARY_PATH']:
+    elif os.name == "posix" and os.environ.get('LD_LIBRARY_PATH', None):
-    # load model?
+    # load model
-    # save model?
+    # save model
-        path_imgrec = args.data_dir + "train.rec",
+        path_imgrec = args.data_dir + args.train_dataset,
-        path_imgrec = args.data_dir + "val.rec",
+        path_imgrec = args.data_dir + args.val_dataset,
-   print("All finished, %d slices in total" % counter)
+   dr = Parallel()(delayed(get_data)(lst,preproc) for lst in frames)
-from .base import string_types
+def check_label_shapes(labels, preds, shape=0):
-        assert len(labels) == len(preds)
+        check_label_shapes(labels, preds)
-        self.num_inst += num_inst
+
-        assert len(labels) == len(preds)
+        check_label_shapes(labels, preds)
-            self.sum_metric += numpy.sum(numpy.abs(label.asnumpy() - pred.asnumpy()))
+            label = label.asnumpy()
-        assert len(labels) == len(preds)
+        check_label_shapes(labels, preds)
-            self.sum_metric += numpy.sqrt(numpy.mean((label.asnumpy() - pred.asnumpy())**2))
+            label = label.asnumpy()
-        assert len(labels) == len(preds)
+        check_label_shapes(labels, preds)
-        return numpy_feval(label.asnumpy(), pred.asnumpy())
+        return numpy_feval(label, pred)
-        raise ValueError('Cannot find metric %s' % metric)
+    try:
-  parser.add_argument('-g', '--gpus', default='0,1,2,3', help='the gpus to be used in ctx')
+  parser.add_argument('-g', '--gpus', default='0', help='the gpus to be used in ctx')
-  
+  
-  parser.add_argument('-g', '--gpus', default='0,1,2,3', help='the gpus to be used in ctx')
+  parser.add_argument('-g', '--gpus', default='0', help='the gpus to be used in ctx')
-parser.add_argument('--save-model', help='output model prefix')
+parser.add_argument('--save-model', type=str, default='new-model', help='output model prefix')
-      data = [input_node['name'] for input_node in input_nodes\
+      data = [input_node for input_node in input_nodes\
-      if utils.is_input(node):
+
-        ishape = out_shape_dic[data + '_output'][1:]
+        ishape = out_shape_dic[data['name'] + '_output'][1:]
-    res[i] = dpc[i][nowc][0]
+    res[i] = dpc[i][nowc][0] + 1
-          print '!!!',j[0]
+        g[j[0]].append(i)        
-  
+
-        print 'can not find symbol %s'%(datas[0])      
+        print 'can not find symbol %s'%(datas[0])
-          raise Exception("Invalid symbol")
+        sym = sym_factory(node, data)        
-def inception(num_classes = 21841):
+def get_symbol(num_classes = 21841):
-                    choices = ['alexnet', 'vgg', 'googlenet', 'inception-bn', 'inception-bn-full.py'],
+                    choices = ['alexnet', 'vgg', 'googlenet', 'inception-bn', 'inception-bn-full'],
-        flat = False if len(data_shape) == 3 else True
+    flat = False if len(data_shape) == 3 else True
-# exit(0)
+if use_torch_criterion:
-
+if use_torch_criterion:
-            self.sum_metric += p.asnumpy().mean()
+    def update(self, _, preds):
-print x.asnumpy()
+print x.asnumpy()
-    ctx = mx.gpu(0), symbol = mlp, num_epoch = 20,
+    ctx = mx.cpu(0), symbol = mlp, num_epoch = 20,
-model.fit(X=train, eval_data=val)
+model.fit(X=train, eval_data=val,
-                           for k, v in train_data.provide_data}
+                           for k, v in train_data.provide_data + train_data.provide_label}
-                                     c_array(ctypes.c_char_p, [])))
+        check_call(_LIB.MXFuncInvokeEx(handle,
-        check_call(_LIB.MXFuncInvoke( \
+        check_call(_LIB.MXFuncInvokeEx( \
-        check_call(_LIB.MXFuncInvoke( \
+        check_call(_LIB.MXFuncInvokeEx( \
-from .base import c_array, py_str
+from .base import c_array, py_str, ctypes2docstring
-
+    param_str = ctypes2docstring(num_args, arg_names, arg_types, arg_descs)
-                'detailed help can be found at ' +
+                'Invoke with\n{res}= mxnet.th.{name}(Parameters)\nor\n'+
-                    name=func_name[4:]))
+                    name=func_name[4:], param_str=param_str,
-                c_array(ctypes.c_char_p, kwargs.values()),))
+        for k in kwargs:
-                                     c_array(NDArrayHandle, (out.handle,))))
+                                     c_array(NDArrayHandle, (out.handle,)),
-                c_array(NDArrayHandle, (out.handle,))))
+                c_array(NDArrayHandle, (out.handle,)), \
-                c_array(NDArrayHandle, [v.handle for v in mutate_vars])))
+                c_array(NDArrayHandle, [v.handle for v in mutate_vars]), \
-# pylint: disable=invalid-name, protected-access, too-many-arguments
+# pylint: disable=invalid-name, protected-access, too-many-arguments, too-many-lines
-from .base import c_array, c_str, mx_uint, py_str, string_types
+from .base import c_array, c_str, mx_uint, py_str, string_types, mx_real_t
-from .ndarray import NDArray, zeros
+from .ndarray import NDArray, zeros, _DTYPE_NP_TO_MX, _DTYPE_MX_TO_NP
-    def simple_bind(self, ctx, grad_req='write', **kwargs):
+    def simple_bind(self, ctx, grad_req='write', type_dict=None, **kwargs):
-        if arg_shapes == None:
+        arg_types, _, aux_types = self.infer_type(**type_dict)
-        arg_ndarrays = [zeros(shape, ctx) for shape in arg_shapes]
+        arg_ndarrays = [zeros(shape, ctx, dtype=dtype)for dtype, shape in zip(arg_types,
-            for name, shape in zip(self.list_arguments(), arg_shapes):
+            for name, shape, dtype in zip(self.list_arguments(), arg_shapes, arg_types):
-                    grad_ndarrays[name] = zeros(shape, ctx)
+                    grad_ndarrays[name] = zeros(shape, ctx, dtype=dtype)
-        aux_ndarrays = [zeros(shape, ctx) for shape in aux_shapes]
+        aux_ndarrays = [zeros(shape, ctx, dtype=dtype) for shape, dtype in zip(aux_shapes,
-    reldiff = diff  / norm
+    reldiff = diff  / (norm + 1e-8)
-def check_with_uniform(uf, arg_shapes, dim=None, npuf=None, rmin=-10):
+def check_with_uniform(uf, arg_shapes, dim=None, npuf=None, rmin=-10, type_list=[np.float32]):
-    assert reldiff(out1, out2) < 1e-6
+    for dtype in type_list:
-            check_with_uniform(lambda x, y: x / y, 2, dim)
+            check_with_uniform(lambda x, y: x + y, 2, dim, type_list=all_type)
-
+def test_symbol_infer_type():
-from .base import mx_uint, mx_float, mx_float_p, NDArrayHandle, FunctionHandle
+from .base import c_array, py_str, c_str, mx_real_t
-def _new_alloc_handle(shape, ctx, delay_alloc):
+def _new_alloc_handle(shape, ctx, delay_alloc, dtype=mx_real_t):
-    check_call(_LIB.MXNDArrayCreate(
+    check_call(_LIB.MXNDArrayCreateEx(
-                source_array = np.array(source_array, dtype=np.float32)
+                source_array = np.array(source_array, dtype=self.dtype)
-        source_array = np.ascontiguousarray(source_array, dtype=np.float32)
+        source_array = np.ascontiguousarray(source_array, dtype=self.dtype)
-            source_array.ctypes.data_as(mx_float_p),
+            source_array.ctypes.data_as(ctypes.c_void_p),
-        data = np.empty(self.shape, dtype=np.float32)
+        data = np.empty(self.shape, dtype=self.dtype)
-            data.ctypes.data_as(mx_float_p),
+            data.ctypes.data_as(ctypes.c_void_p),
-            hret = NDArray(_new_alloc_handle(self.shape, other, True))
+            hret = NDArray(_new_alloc_handle(self.shape, other, True, self.dtype))
-def empty(shape, ctx=None):
+def empty(shape, ctx=None, dtype=mx_real_t):
-    return NDArray(handle=_new_alloc_handle(shape, ctx, False))
+    return NDArray(handle=_new_alloc_handle(shape, ctx, False, dtype))
-def zeros(shape, ctx=None):
+def zeros(shape, ctx=None, dtype=mx_real_t):
-    arr = empty(shape, ctx)
+    arr = empty(shape, ctx, dtype)
-def ones(shape, ctx=None):
+def ones(shape, ctx=None, dtype=mx_real_t):
-    arr = empty(shape, ctx)
+    arr = empty(shape, ctx, dtype)
-def array(source_array, ctx=None):
+def array(source_array, ctx=None, dtype=mx_real_t):
-            source_array = np.array(source_array, dtype=np.float32)
+            source_array = np.array(source_array, dtype=dtype)
-    arr = empty(source_array.shape, ctx)
+    arr = empty(source_array.shape, ctx, dtype)
-   result=[]
+   result = []
-   data,result=zip(*dr)
+   dr = Parallel()(delayed(get_data)(lst,preproc) for lst in frames)
-   result=np.ravel(result)
+   result = np.ravel(result)
-def check_concat_with_shape(shapes, dimension):
+def check_concat_with_shape(shapes, dimension, skip_second):
-                     args_grad=arr_grad)
+                     args_grad=dict_grad)
-        assert same(grad.asnumpy(), np_grad + 1)
+
-                    check_concat_with_shape(shapes,dimension)
+                    check_concat_with_shape(shapes,dimension,True)
-                check_concat_with_shape(shapes,dimension)
+                check_concat_with_shape(shapes,dimension,True)
-            check_concat_with_shape(shapes,dimension)
+            check_concat_with_shape(shapes,dimension,True)
-parser.add_argument('--save-model', help='output model prefix')
+parser.add_argument('--save-model', type=str, default='new-model', help='output model prefix')
-      data = [input_node['name'] for input_node in input_nodes\
+      data = [input_node for input_node in input_nodes\
-      if utils.is_input(node):
+
-        ishape = out_shape_dic[data + '_output'][1:]
+        ishape = out_shape_dic[data['name'] + '_output'][1:]
-    res[i] = dpc[i][nowc][0]
+    res[i] = dpc[i][nowc][0] + 1
-          print '!!!',j[0]
+        g[j[0]].append(i)        
-  
+
-        print 'can not find symbol %s'%(datas[0])      
+        print 'can not find symbol %s'%(datas[0])
-          raise Exception("Invalid symbol")
+        sym = sym_factory(node, data)        
-                    mx.nd.choose_element_0index(arr, mx.nd.array(indices)).asnumpy())
+        val = np.random.randint(shape[1], size=shape[0])
-def test_ndarray_choose():
+def test_ndarray_onehot():
-  parser.add_argument('-g', '--gpus', default='0,1,2,3', help='the gpus to be used in ctx')
+  parser.add_argument('-g', '--gpus', default='0', help='the gpus to be used in ctx')
-  
+  
-  parser.add_argument('-g', '--gpus', default='0,1,2,3', help='the gpus to be used in ctx')
+  parser.add_argument('-g', '--gpus', default='0', help='the gpus to be used in ctx')
-def get_data(lst):
+def get_data(lst,preproc):
-       img = crop_resize(f.pixel_array.astype(float) / np.max(f.pixel_array),64)
+       img = preproc(f.pixel_array.astype(float) / np.max(f.pixel_array))
-   dr=Parallel()(delayed(get_data)(lst) for lst in frames)
+   dr=Parallel()(delayed(get_data)(lst,preproc) for lst in frames)
-   print("All finished, %d slices in total" % counter)
+   dr=Parallel()(delayed(get_data)(lst) for lst in frames)
-        self.magnitude = magnitude
+        self.magnitude = float(magnitude)
-            factor = (fan_in + fan_out) / 2
+            factor = (fan_in + fan_out) / 2.0
-    def __init__(self, rescale_grad=1):
+    def __init__(self, rescale_grad=1, arg_names=None):
-        super(SGD, self).__init__(rescale_grad)
+        super(SGD, self).__init__(rescale_grad, arg_names)
-            wd = self.wd
+        wd = self.wd
-        super(RMSProp, self).__init__(rescale_grad)
+                 lr_scheduler=None, arg_names=None):
-        delta[:] = (self.gamma2) * delta - lr * (grad/sqrt(n - g*g + 1e-4) + self.wd * weight)
+        delta[:] = (self.gamma2) * delta - lr * (grad/sqrt(n - g*g + 1e-4) + wd * weight)
-               num_epoch=None, epoch_size=None, optimizer='ccsgd', initializer=Uniform(0.01),
+               num_epoch=None, epoch_size=None, optimizer='sgd', initializer=Uniform(0.01),
-                 lr_scheduler=None):
+                 lr_scheduler=None, arg_names=None):
-        # TODO(bing) implement wd_bias, wd_gamma, wd_beta
+        wd = 0.
-            mom[:] += -lr * (grad + self.wd * weight)
+            mom[:] += -lr * (grad + wd * weight)
-        self._monitor_callback = cb_type(callback)
+        cb_type = ctypes.CFUNCTYPE(None, ctypes.c_char_p, NDArrayHandle, ctypes.c_void_p)
-            self._monitor_callback))
+            self._monitor_callback,
-        solver = Solver('sgd', momentum=0.9, wd=decay, learning_rate=l_rate, lr_scheduler=lr_scheduler)
+        solver = Solver(optimizer, momentum=0.9, wd=decay, learning_rate=l_rate, lr_scheduler=lr_scheduler)
-        solver = Solver('sgd', momentum=0.9, wd=decay, learning_rate=l_rate, lr_scheduler=lr_scheduler)
+        solver = Solver(optimizer, momentum=0.9, wd=decay, learning_rate=l_rate, lr_scheduler=lr_scheduler)
-            if max_hw > cut_off_size:
+            if min_hw > self.cut_off_size:
-        img = np.expand_dims(img, axis=0)  # (1, c, h, w) or (1, h, w)
+        img = np.expand_dims(img, axis=0)  # (1, c, h, w)
-        label = np.expand_dims(label, axis=0)  # (1, c, h, w) or (1, h, w)
+        label = np.expand_dims(label, axis=0)  # (1, h, w)
-    upscore = mx.symbol.Crop(data=bigscore, crop_like=crop, offset=offset, name="upscore")
+    upscore = mx.symbol.Crop(*[bigscore, crop], offset=offset, name="upscore")
-    score_fused = mx.symbol.ElementWiseSum(*[score2, score_pool4c], name='score_fused')
+    score_pool4c = mx.symbol.Crop(*[score_pool4, score2], offset=offset()["score_pool4c"], name="score_pool4c")
-    score_fused = mx.symbol.ElementWiseSum(*[score2, score_pool4c], name='score_fused')
+    score_pool4c = mx.symbol.Crop(*[score_pool4, score2], offset=offset()["score_pool4c"], name="score_pool4c")
-    score_final = mx.symbol.ElementWiseSum(*[score4, score_pool3c], name='score_final')
+    score_pool3c = mx.symbol.Crop(*[score_pool3, score4], offset=offset()["score_pool3c"], name="score_pool3c")
-    data_shape = fcn32s_arg_params["data"].shape
+    fcnxs, fcnxs_args, fcnxs_auxs = mx.model.load_checkpoint(model_previx, epoch)
-    exector = fcn32s.bind(ctx, fcn32s_arg_params ,args_grad=None, grad_req="null", aux_states=fcn32s_arg_params)
+    fcnxs_args["softmax_label"] = mx.nd.empty(label_shape, ctx)
-                             param.epoch, count, speed, name, value)
+                                 param.epoch, count, speed, name, value)
-                             param.epoch, count, speed)
+                                 param.epoch, count, speed)
-    score2 = mx.symbol.Deconvolution(data=score, kernel=(4, 4), stride=(2, 2),num_filter=21,
+    score2 = mx.symbol.Deconvolution(data=score, kernel=(4, 4), stride=(2, 2),num_filter=numclass,
-    score_pool4 = mx.symbol.Convolution(data=pool4, kernel=(1, 1), num_filter=21,
+    score_pool4 = mx.symbol.Convolution(data=pool4, kernel=(1, 1), num_filter=numclass,
-    score4 = mx.symbol.Deconvolution(data=score_fused, kernel=(4, 4), stride=(2, 2),num_filter=21,
+    score4 = mx.symbol.Deconvolution(data=score_fused, kernel=(4, 4), stride=(2, 2),num_filter=numclass,
-    score_pool3 = mx.symbol.Convolution(data=pool3, kernel=(1, 1), num_filter=21,
+    score_pool3 = mx.symbol.Convolution(data=pool3, kernel=(1, 1), num_filter=numclass,
-    def batch_size(self):
+    def get_batch_size(self):
-        self.optimizer = opt.create(self.optimizer, rescale_grad=(1.0/train_data.batch_size), **(self.kwargs))
+        self.optimizer = opt.create(self.optimizer, rescale_grad=(1.0/train_data.get_batch_size()), **(self.kwargs))
-                logging.info("Iter[%d] Batch [%d]\tSpeed: %.2f samples/sec",
+                if param.eval_metric is not None:
-            logging.info("Update[%d]: Change learning rate to %.5f",
+            logging.info("Update[%d]: Change learning rate to %0.5e",
-            num_inst = pred_label.shape[0]
+            num_inst = pred_label.size
-    def __init__(self, interval, stat_func=None):
+    def __init__(self, interval, stat_func=None, pattern='.*', sort=False):
-            if not self.activated:
+            if not self.activated or not self.re_prog.match(name):
-        else:
+        if not self.activated:
-                res.append((n, k, str(v.asnumpy())))
+        if self.sort:
-from .ndarray import NDArray
+from .ndarray import NDArray, load
-    executor.forward()
+    executor.forward(is_train=True)
-            executor.forward()
+            executor.forward(is_train=True)
-def check_numeric_gradient(sym, location, numeric_eps=1e-4, check_eps=1e-2):
+def check_numeric_gradient(sym, location, aux_states=[], numeric_eps=1e-4, check_eps=1e-2):
-    executor = out.bind(mx.cpu(), args=arr_data, args_grad=arr_grad)
+    executor = out.bind(mx.cpu(), args=arr_data, args_grad=arr_grad, aux_states=arr_aux)
-    executor.forward()
+    executor.forward(is_train=True)
-    for numeric, symbolic in zip(numeric_gradients, symbolic_grad):
+    for name, numeric, symbolic in zip(out.list_arguments(), numeric_gradients, symbolic_grad):
-        assert rel <= check_eps
+        if rel > check_eps:
-        if not os.path.exists(args.log_dir): 
+        log_file_full_name = os.path.join(log_dir, log_file)
-	handler.setFormatter(formatter)
+        handler = logging.FileHandler(log_file_full_name)
-    	logging.basicConfig(level=logging.DEBUG, format=head)
+        logging.basicConfig(level=logging.DEBUG, format=head)
-    if img_fmt == '.jpg':
+    jpg_formats = set(['.jpg', '.jpeg', '.JPG', '.JPEG'])
-    elif img_fmt == '.png':
+    elif img_fmt in png_formats:
-    logging.info('start with arguments %s', args)
+    if 'log_file' in args and args.log_file is not None:
-    if img_fmt is '.jpg':
+    if img_fmt == '.jpg':
-    elif img_fmt is '.png':
+    elif img_fmt == '.png':
-           img = preproc(f.pixel_array / np.max(f.pixel_array))
+           img = preproc(f.pixel_array.astype(float) / np.max(f.pixel_array))
-def pack_img(header, img, quality=80, img_fmt='.JPEG'):
+def pack_img(header, img, quality=80, img_fmt='.jpg'):
-        quality for JPEG encoding. 1-100
+        quality for JPEG encoding. 1-100, or compression for PNG encoding. 1-9.
-    ret, buf = cv2.imencode(img_fmt, img, [cv2.IMWRITE_JPEG_QUALITY, quality])
+    if img_fmt is '.jpg':
-        img = cv2.imread(os.path.join(args.root, item[1]))
+        img = cv2.imread(os.path.join(args.root, item[1]), color_modes[args.color])
-        s = mx.recordio.pack_img(header, img, quality=args.quality)
+        s = mx.recordio.pack_img(header, img, quality=args.quality, img_fmt=args.encoding)
-        help='JPEG quality for encoding, 1-100.')
+        help='JPEG quality for encoding, 1-100; or PNG compression for encoding, 1-9')
-
+    rgroup.add_argument('--color', type=int, default=1, choices=[-1, 0, 1],
-# In[1]:
+"""Training script, this is converted from a ipython notebook
-noutput = 600
+    """ A lenet style net, takes difference of each frame as input.
-
+"""Preprocessing script.
-
+This script walks over the directories and dump the frames into a csv file
-
+
-
+# Load the list of all the training frames, and shuffle them
-
+# Dump the data of each frame into a CSV file, apply crop to 64 preprocessor
-
+# Generate local train/test split, which you could use to tune your model locally.
-
+
-    chunk_size = N/num_chunks
+    chunk_size = (N+num_chunks-1)/num_chunks
-    main()
+    main()
-    print("Training swith val.shape=%s" % str(X_val_batch.shape))
+    print("Training with train.shape=%s" % str(X_train_batch.shape))
-    softmax = mx.symbol.SoftmaxOutput(data = fc2)
+    softmax = mx.symbol.SoftmaxOutput(data = fc2, name='softmax')
-                 num_epoch=None, epoch_size=None, optimizer='ccsgd',
+                 num_epoch=None, epoch_size=None, optimizer='sgd',
-    softmax = mx.symbol.SoftmaxOutput(data = fc2)
+    softmax = mx.symbol.SoftmaxOutput(data = fc2, name='softmax')
-                 num_epoch=None, epoch_size=None, optimizer='ccsgd',
+                 num_epoch=None, epoch_size=None, optimizer='sgd',
-                 num_epoch=None, epoch_size=None, optimizer='sgd',
+                 num_epoch=None, epoch_size=None, optimizer='ccsgd',
-    def predict(self, X):
+    def predict(self, X, num_batch=None):
-               num_epoch=None, epoch_size=None, optimizer='sgd', initializer=Uniform(0.01),
+               num_epoch=None, epoch_size=None, optimizer='ccsgd', initializer=Uniform(0.01),
-        check_call(_LIB.MXOptimizerFindCreator(ctypes.c_char_p(name),
+        check_call(_LIB.MXOptimizerFindCreator(c_str(name),
-def pack_img(header, img, quality=80, format='.JPEG'):
+def pack_img(header, img, quality=80, img_fmt='.JPEG'):
-    assert ret
+    ret, buf = cv2.imencode(img_fmt, img, [cv2.IMWRITE_JPEG_QUALITY, quality])
-    test_prod_sum();
+class RMSE(EvalMetric):
-def pack_img(header, img, quality=80):
+def pack_img(header, img, quality=80, format='.JPEG'):
-    ret, buf = cv2.imencode('.JPEG', img, [cv2.IMWRITE_JPEG_QUALITY, quality])
+    ret, buf = cv2.imencode(format, img, [cv2.IMWRITE_JPEG_QUALITY, quality])
-    chunk_size = N/num_chunks
+    chunk_size = (N+num_chunks-1)/num_chunks
-    main()
+    main()
-# pylint: disable=invalid-name, protected-access, too-many-locals
+# pylint: disable=invalid-name, protected-access, too-many-locals, too-many-arguments
-
+def _initialize_kvstore(kvstore, param_arrays, arg_params, param_names,
-        texec.copy_params_from(arg_params, aux_params)
+    executor_manager = DataParallelExecutorManager(symbol=symbol,
-                kvstore.pull(idx, param_on_devs, priority=-idx)
+        _initialize_kvstore(kvstore=kvstore,
-    cpu_output_arrays = [nd.zeros(s) for s in output_shapes]
+    if update_on_kvstore:
-                _load_label(data_batch, label_arrays)
+
-                            updater(index*num_device+k, g, w)
+
-                eval_metric.update(data_batch.label, cpu_output_arrays)
+                eval_metric.update(data_batch.label, executor_manager.cpu_output_arrays)
-                eval_metric.update(eval_batch.label, cpu_output_arrays)
+                executor_manager.load_data_batch(eval_batch)
-                weight.copyto(aux_params[name])
+            executor_manager.copy_to(arg_params, aux_params)
-    kv = mx.kvstore.create(kv_type)
+    kv = mx.kvstore.create(args.kv_store)
-    if isinstance(kvstore, kvs.KVStore):
+    if kvstore is None:
-        raise TypeError('kvstore must be either KVStore or str')
+        raise TypeError('kvstore must be KVStore, str or None')
-    kv = mx.kvstore.create(args.kv_store)
+    kv_type = args.kv_store
-for data, label in dataiter:
+for dbatch in dataiter:
-    n_mutate_vars = 0
+
-        sym._numpy_op = self
+        # keep a reference of ourself in PythonOp so we don't get garbage collected.
-        sym._ndarray_op = self
+        # keep a reference of ourself in PythonOp so we don't get garbage collected.
-from .ndarray import NDArray, zeros, clip, sqrt
+import ctypes
-
+class ccSGD(Optimizer):
-        pass
+        self.batch_size = 0
-    def __init__(self):
+    """Base class for prefetching iterators. Takes one or more DataIters (
-        def prefetch_func(self):
+        if not isinstance(iters, list):
-                if not self.running:
+                self.data_taken[i].wait()
-                    self.next_batch = self.prefetch_next()
+                    self.next_batch[i] = self.iters[i].next()
-        self.prefetch_thread.setDaemon(True)
+                    self.next_batch[i] = None
-        raise NotImplementedError
+        self.started = False
-        self.data_taken.set()
+        for e in self.data_ready:
-        if self.current_batch is None:
+        for e in self.data_ready:
-            self.data_taken.set()
+            for batch in self.next_batch:
-    """Calculate L1 norm loss"""
+class MAE(EvalMetric):
-        super(L1, self).__init__('l1')
+        super(MAE, self).__init__('mae')
-    """Base class for prefetching iterators."""
+    """Base class for prefetching iterators. Subclass and override prefetch_next
-            assert False
+        assert_allclose(arr[name].asnumpy()*root_scale**2*scale**(2*k), arr_grad[name].asnumpy(), rtol=1e-4)
-        uri = ctypes.c_char_p(uri)
+        self.uri = ctypes.c_char_p(uri)
-            check_call(_LIB.MXRecordIOWriterCreate(uri, ctypes.byref(self.handle)))
+        self.flag = flag
-            check_call(_LIB.MXRecordIOReaderCreate(uri, ctypes.byref(self.handle)))
+        elif self.flag == "r":
-            raise ValueError("Invalid flag %s"%flag)
+            raise ValueError("Invalid flag %s"%self.flag)
-# pylint: disable=invalid-name, protected-access, fixme, too-many-arguments, W0221, W0201
+# pylint: disable=invalid-name, protected-access, fixme, too-many-arguments, W0221, W0201, no-self-use
-        """Get next data batch from iterator
+        """Get next data batch from iterator. Equivalent to
-        data : NDArray
+        data : DataBatch
-        pass
+        if self.iter_next():
-    def getdata(self, index=0):
+    def getdata(self):
-            The index of data source to retrieve.
+
-        return self.getdata(-1)
+        pass
-        pass
+        return None
-DataBatch = namedtuple('DataBatch', ['data', 'label', 'pad', 'index'])
+    def __del__(self):
-                    choices = ['alexnet', 'vgg', 'googlenet', 'inception-bn'],
+                    choices = ['alexnet', 'vgg', 'googlenet', 'inception-bn', 'inception-bn-full.py'],
-    conv_data = mx.sym.Variable(name="conv_data")
+    data_conv = mx.sym.Variable(name="data_conv")
-        data=conv_data, kernel=kernel, stride=stride, pad=pad,
+        data=data_conv, kernel=kernel, stride=stride, pad=pad,
-    deconv_data = mx.sym.Variable(name="deconv_data")
+    data_deconv = mx.sym.Variable(name="data_deconv")
-        data=deconv_data, kernel=kernel, stride=stride, pad=pad,
+        data=data_deconv, kernel=kernel, stride=stride, pad=pad,
-    conv_args["conv_data"] = conv_data
+    conv_args["data_conv"] = conv_data
-    deconv_args['deconv_data'] = deconv_data
+    deconv_args['data_deconv'] = deconv_data
-    
+
-    test =  mx.sym.rsqrt(data) + mx.sym.cos(data) + mx.sym.sin(data) 
+    test =  mx.sym.rsqrt(data) + mx.sym.cos(data) + mx.sym.sin(data)
-    
+
-            input_shapes[name] = (batch_size, input_size)
+            input_shapes[name] = (batch_size, )
-    seq_labels = [rnn_exec.arg_dict["t%d_label" % i] for i in range(seq_len)]
+    seq_labels = rnn_exec.arg_dict["label"]
-    seq_outputs = [out_dict["t%d_sm_output" % i] for i in range(seq_len)]
+    seq_outputs = out_dict["sm_output"]
-    vocab = m.seq_outputs[0].shape[1]
+    vocab = m.seq_outputs.shape[1]
-    outputs_ndarray = mx.nd.zeros(m.seq_outputs[0].shape)
+    outputs_ndarray = mx.nd.zeros(m.seq_outputs.shape)
-        outputs_ndarray[:] = m.seq_outputs[0]
+        outputs_ndarray[:] = m.seq_outputs
-                
+
-            lr = self.lr
+        lr = self.lr
-               optimizer='sgd', half_life=2,max_grad_norm = 5.0, **kwargs):
+               optimizer='rmsprop', half_life=2,max_grad_norm = 5.0, **kwargs):
-learning_rate= 1
+learning_rate= 0.1
-model = lstm.setup_rnn_model(mx.gpu(),
+model = lstm.setup_rnn_model(mx.cpu(),
-                momentum=momentum)
+                wd=wd)
-@register   
+@register
-    This code follows the version in  http://arxiv.org/pdf/1308.0850v5.pdf Eq(38) - Eq(45) by Alex Graves, 2013.
+    This code follows the version in  http://arxiv.org/pdf/1308.0850v5.pdf Eq(38) - Eq(45)
-    def __init__(self, learning_rate=0.002, gamma1 = 0.95, gamma2 = 0.9,
+    def __init__(self, learning_rate=0.002, gamma1=0.95, gamma2=0.9,
-                 rescale_grad=1, clip_gradient=None, 
+                 rescale_grad=1, clip_gradient=None,
-
+@register   
-        return buf.contents.raw
+        if buf:
-    """unpack a MXImageRecord
+    """unpack a MXImageRecord to image
-        img = cv2.imdecode(img, iscolor)
+    header, s = unpack(s)
-    return s
+    assert opencv_available
-                res.append((n, k, v.asscalar()))
+                res.append((n, k, str(v.asscalar())))
-                res.append((n, k, v.asnumpy()))
+                res.append((n, k, str(v.asnumpy())))
-            logging.info('Batch: {:7d} {:30s} {:f}'.format(n, k, v))
+            logging.info('Batch: {:7d} {:30s} {:s}'.format(n, k, v))
-        list of NDArray for data. The last one is treated as label.
+    data: NDArray or numpy.ndarray, a list of them, or a dict of string to them.
-            Training data.
+            Training data. If X is an DataIter, the name or, if not available,
-    import cv, cv2
+    import cv2
-        ret, buf = cv2.imencode('.JPEG', img, [cv.CV_IMWRITE_JPEG_QUALITY, quality])
+        ret, buf = cv2.imencode('.JPEG', img, [cv2.IMWRITE_JPEG_QUALITY, quality])
-# pylint: disable=invalid-name, protected-access, fixme, too-many-arguments
+# pylint: disable=invalid-name, protected-access, fixme, too-many-arguments, no-member
-    def setup(self, dims, pt_dropout=None, ft_dropout=None, input_act=None, internal_act='relu', output_act=None):
+    def setup(self, dims, sparseness_penalty=None, pt_dropout=None, ft_dropout=None, input_act=None, internal_act='relu', output_act=None):
-                                                        idropout, odropout, encoder_act, decoder_act)
+            istack, iargs, iargs_grad, iargs_mult, iauxs = self.make_stack(i, self.data, dims[i], dims[i+1],
-        self.decoder = self.make_decoder(self.encoder, dims, ft_dropout, internal_act, input_act)
+            self.auxs.update(iauxs)
-    def make_stack(self, istack, data, num_input, num_hidden, idropout=None,
+    def make_stack(self, istack, data, num_input, num_hidden, sparseness_penalty=None, idropout=None,
-        init = mx.initializer.Normal(0.01)
+        auxs = {}
-        return x, args, args_grad, args_mult
+        return x, args, args_grad, args_mult, auxs
-    def make_encoder(self, data, dims, dropout=None, internal_act='relu', output_act=None):
+    def make_encoder(self, data, dims, sparseness_penalty=None, dropout=None, internal_act='relu', output_act=None):
-    def make_decoder(self, feature, dims, dropout=None, internal_act='relu', input_act=None):
+    def make_decoder(self, feature, dims, sparseness_penalty=None, dropout=None, internal_act='relu', input_act=None):
-                x = mx.symbol.Dropout(data=x, p = dropout)
+                x = mx.symbol.Dropout(data=x, p=dropout)
-        data_iter = mx.io.NDArrayIter({'data': X}, batch_size=batch_size, shuffle=False,
+        data_iter = mx.io.NDArrayIter({'data': X}, batch_size=batch_size, shuffle=True,
-                X_i = model.extract_feature(self.internals[i-1], self.args,
+                X_i = model.extract_feature(self.internals[i-1], self.args, self.auxs,
-            solver.solve(self.xpu, self.stacks[i], self.args, self.args_grad, data_iter_i,
+            solver.solve(self.xpu, self.stacks[i], self.args, self.args_grad, self.auxs, data_iter_i,
-            return np.mean(np.square(label-pred))/2.0
+           return np.mean(np.square(label-pred))/2.0
-        data_iter = mx.io.NDArrayIter({'data': X}, batch_size=batch_size, shuffle=False,
+        data_iter = mx.io.NDArrayIter({'data': X}, batch_size=batch_size, shuffle=True,
-        solver.solve(self.xpu, self.loss, self.args, self.args_grad, data_iter,
+        solver.solve(self.xpu, self.loss, self.args, self.args_grad, self.auxs, data_iter,
-        Y = model.extract_feature(self.loss, self.args, data_iter,
+        Y = model.extract_feature(self.loss, self.args, self.auxs, data_iter,
-
+# pylint: skip-file
-def extract_feature(sym, args, data_iter, N, xpu=mx.cpu()):
+def extract_feature(sym, args, auxs, data_iter, N, xpu=mx.cpu()):
-    exe = sym.bind(xpu, args=args)
+    exe = sym.bind(xpu, args=args, aux_states=auxs)
-    def solve(self, xpu, sym, args, args_grad,
+    def solve(self, xpu, sym, args, args_grad, auxs,
-        exe = sym.bind(xpu, args=args, args_grad=args_grad)
+        exe = sym.bind(xpu, args=args, args_grad=args_grad, aux_states=auxs)
-    sample = sample[:, :, [2,1,0]]
+
-        if name.endswith('bias'):
+        if name.startswith('upsampling'):
-    # pylint: disable=no-self-use, missing-docstring
+    # pylint: disable=no-self-use, missing-docstring, invalid-name
-    # pylint: enable=no-self-use, missing-docstring
+    # pylint: enable=no-self-use, missing-docstring, invalid-name
-        return self.__deepcopy__()
+        return copy.deepcopy(self)
-    def __deepcopy__(self):
+    def __deepcopy__(self, _):
-        s = self.__deepcopy__()
+        s = copy.deepcopy(self)
-    content_grad = (model_executor.content - content_array) * args.content_weight
+    content_grad[:] = (model_executor.content - content_array) * args.content_weight
-    # _prepare_model()
+try:
-    exe_test = test.bind(mx.cpu(), args=[arr_data], args_grad=[arr_grad])
+    exe_test = test.bind(mx.cpu(), args=[arr_data])
-    assert reldiff(arr_grad.asnumpy(), npout_grad) < 1e-6
+# pylint: skip-file
-# dot.render('b')
+import numpy as np
-        self.monitor_callback = cb_type(callback)
+        self._monitor_callback = cb_type(callback)
-            self.monitor_callback))
+            self._monitor_callback))
-                        logger=None, work_load_list=None):
+                        logger=None, work_load_list=None, monitor=None):
-            work_load_list=None):
+            work_load_list=None, monitor=None):
-                            logger=logger, work_load_list=work_load_list)
+                            logger=logger, work_load_list=work_load_list, monitor=monitor)
-from .operator import NDArrayOp
+# coding: utf-8
-    pass
+    """Monitor outputs, weights, and gradients for debugging.
-        cb_type = ctypes.CFUNCTYPE(None, NDArrayHandle)
+        cb_type = ctypes.CFUNCTYPE(None, ctypes.c_char_p, NDArrayHandle)
-            cb_type(callback)))
+            self.monitor_callback))
-        
+
-        clip_gradient      = args.clip_gradient,
+def test_round_ceil_floor():
-parser.add_argument('--lr', type=float, default=.05,
+parser.add_argument('--lr', type=float, default=.01,
-        mean_img    = args.data_dir + "mean.bin",
+        mean_r      = 123.68,
-        mean_img    = args.data_dir + "mean.bin",
+        mean_r      = 123.68,
-
+from check_utils import (check_numeric_gradient, check_symbolic_backward,
-    data_tmp[:]=5
+    data_tmp = np.ones(shape)*5
-    out = exe_test.outputs[0].asnumpy()
+
-    npout_grad = npout_grad*2/5
+    check_symbolic_forward(test, [data_tmp], [npout])
-    assert reldiff(arr_grad.asnumpy(), npout_grad) < 1e-6
+
-    shape = (3, 4)
+    shape = (1, 1)
-
+    test = data ** 2
-    shape = (3, 4)
+    shape = (1, 1)
-    data_tmp[:]=5
+    data_tmp = np.ones(shape)*2
-    exp_grad[:]=5
+    exp_tmp = np.ones(shape)*3
-    assert_allclose(grad, npgrad)
+    check_numeric_gradient(test, [data_tmp, exp_tmp])
-    assert_allclose(grad, npgrad)
+    data_dir = data_tmp**(exp_tmp - 1) * exp_tmp
-    assert_allclose(grad, npgrad)
+    x = np.ones(shape)*3
-    def bind(self, ctx, args, args_grad=None, grad_req='write', aux_states=None):
+    def bind(self, ctx, args, args_grad=None, grad_req='write', aux_states=None, group2ctx=None):
-        # pylint: disable=too-many-locals
+        # pylint: disable=too-many-locals, too-many-branches
-                                       ctypes.byref(handle)))
+        check_call(_LIB.MXExecutorBindX(self.handle,
-    shape = (3, 4)    
+    shape = (3, 4)
-    arr_grad = mx.nd.empty(shape) 
+    arr_grad = mx.nd.empty(shape)
-    
+
-    
+
-if __name__ == '__main__': 
+
-        output_dict = {'y' : mx.nd.zeros((10,))}
+        inputs = [('x', mx.nd.zeros((10,)))]
-# pylint: disable=invalid-name, protected-access, too-many-arguments, no-self-use
+# pylint: disable=invalid-name, protected-access, too-many-arguments, no-self-use, too-many-locals, broad-except
-from ctypes import c_void_p, cast, c_int, c_char, c_char_p, cast
+from ctypes import c_void_p, cast, c_int, c_char, c_char_p, cast, c_bool
-        list_functype = CFUNCTYPE(None, POINTER(POINTER(POINTER(c_char))))
+                                POINTER(POINTER(mx_uint)), POINTER(c_int), c_void_p)
-                          tensor_shapes, tensor_tags):
+                          tensor_shapes, tensor_tags, _):
-                           tensor_shapes, tensor_tags):
+                           tensor_shapes, tensor_tags, _):
-                              tensor_shapes):
+                              tensor_shapes, _):
-        def list_outputs_entry(out):
+        def list_outputs_entry(out, _):
-        def list_arguments_entry(out):
+        def list_arguments_entry(out, _):
-        deps_functype = CFUNCTYPE(None, c_int_p, c_int_p, c_int_p, c_int_p, POINTER(c_int_p), c_void_p)
+        fb_functype = CFUNCTYPE(c_bool, c_int, POINTER(c_void_p), POINTER(c_int), c_void_p)
-            self.forward(in_data=tensors[0], out_data=tensors[1])
+            try:
-                          in_grad=tensors[2], out_grad=tensors[3])
+            try:
-                tensor_dims[i] = len(rshape[i])
+            try:
-            out[0] = cast(ret, POINTER(POINTER(c_char)))
+            try:
-            out[0] = cast(ret, POINTER(POINTER(c_char)))
+            try:
-            deps[0] = rdeps
+            try:
-        list_functype = CFUNCTYPE(None, POINTER(POINTER(POINTER(c_char))))
+        infer_functype = CFUNCTYPE(None, c_int, POINTER(c_int), POINTER(POINTER(mx_uint)), c_void_p)
-                              tensor_shapes):
+                              tensor_shapes, _):
-        def list_outputs_entry(out):
+        def list_outputs_entry(out, _):
-        def list_arguments_entry(out):
+        def list_arguments_entry(out, _):
-                                   None, None, None, None, None)
+                                   deps_functype(declare_backward_dependency),
-        fb_functype = CFUNCTYPE(None, c_int, POINTER(c_void_p), POINTER(c_int))
+        fb_functype = CFUNCTYPE(None, c_int, POINTER(c_void_p), POINTER(c_int), c_void_p)
-        def forward_entry(num_ndarray, ndarraies, tags):
+        def forward_entry(num_ndarray, ndarraies, tags, _):
-        def backward_entry(num_ndarray, ndarraies, tags):
+        def backward_entry(num_ndarray, ndarraies, tags, _):
-    - ``prefix-epoch.params`` will be saved for parameters.
+    - symbol will be loaded from ``prefix-symbol.json``.
-    initializier : initializer function, optional
+    initializer : initializer function, optional
-def plot_network(symbol, title="plot", shape=None):
+def plot_network(symbol, title="plot", shape=None, node_attrs={}):
-from .base import c_array, c_str, mx_uint, mx_float, ctypes2numpy_shared
+from ctypes import CFUNCTYPE, POINTER, Structure, pointer
-def Variable(name):
+def Variable(name, attr=None):
-    return Symbol(handle)
+    ret = Symbol(handle)
-
+        attr = AttrScope.current.get(attr)
-    cd5x5 = ConvFactory(data=cd5x5r, num_filter=num_d5x5, kernel=(3, 3), pad=(1, 1), name=('%s_5x5' % name))
+    cd5x5 = ConvFactory(data=cd5x5r, num_filter=num_d5x5, kernel=(5, 5), pad=(2, 2), name=('%s_5x5' % name))
-    cd5x5 = ConvFactory(data=cd5x5r, num_filter=num_d5x5, kernel=(3, 3), pad=(1, 1), name=('%s_double_3x3_1' % name))
+    cd5x5r = ConvFactory(data=data, num_filter=num_d5x5red, kernel=(1, 1), name=('%s_5x5' % name), suffix='_reduce')
-    conv1 = ConvFactory(data, 64, kernel=(7, 7), stride=(2,2), pad=(3, 3))
+    conv1 = ConvFactory(data, 64, kernel=(7, 7), stride=(2,2), pad=(3, 3), name="conv1")
-    conv3 = ConvFactory(conv2, 192, kernel=(3, 3), stride=(1, 1), pad=(1,1))
+    conv2 = ConvFactory(pool1, 64, kernel=(1, 1), stride=(1,1), name="conv2")
-                                  mx_uint(block_dims[0])))
+                                  mx_uint(block_dims[1]),
-    rtc = mx.rtc('abc', [('x', x)], [('y', y)], """y[threadIdx.x] = x[threadIdx.x]*5.0;""")
+    rtc = mx.rtc('abc', [('x', x)], [('y', y)], """
-    assert_allclose(y.asnumpy(), x.asnumpy()*5.0)
+    assert_allclose(y.asnumpy(), np.exp(x.asnumpy()*5.0))
-            num_inst = label.size
+            if label.shape[0] < pred_label.shape[0]:
-if __name__ == '__main__':
+
-            return Symbol._MinusScalar(self, scalar=other, scalar_on_right=True)
+            return Symbol._MinusScalar(self, scalar=other, scalar_on_left=True)
-            return Symbol._DivScalar(self, scalar=other, scalar_on_right=True)
+            return Symbol._DivScalar(self, scalar=other, scalar_on_left=True)
-        return Symbol._PowerScalar(exp, scalar=base, scalar_on_right=True)
+        return Symbol._PowerScalar(exp, scalar=base, scalar_on_left=True)
-            lr = self.lr_scheduler(self.epoch)
+            lr = self.lr_scheduler(self.num_update)
-parser.add_argument('--network', type=str, default='alexnet',
+parser.add_argument('--network', type=str, default='inception-bn',
-
+parser.add_argument('--format', type=str, default='markdown',
-        data[epoch][i*2] += val
+    data[epoch][i*2] += val
-        print "| %d | %f | %f | %.1f |" % (k+1, v[0], v[2], v[4]/v[5])
+if args.format == 'markdown':
-            step += learning_rate * self.wd * weight
+            step += lr * self.wd * weight
-def lstm_unroll(num_lstm_layer, seq_len,
+def lstm_unroll(num_lstm_layer, seq_len, input_size,
-                                      name="t%d_embed" % seqidx)
+
-            input_shapes[name] = (batch_size, input_size)
+            input_shapes[name] = (batch_size, )
-    batch_size, vocab = m.seq_data[0].shape
+    batch_size = m.seq_data[0].shape[0]
-                out=m.seq_data[seqidx])
+        mx.nd.array(x).copyto(m.seq_data[seqidx])
-    opt = mx.optimizer.create(optimizer,                              
+
-    
+
-                        grad *= (max_grad_norm / norm)                    
+                        grad *= (max_grad_norm / norm)
-        
+
-                check_concat_with_shape(shapes,dimension)            
+                check_concat_with_shape(shapes,dimension)
-                check_concat_with_shape(shapes,dimension)
+                    check_concat_with_shape(shapes,dimension)
-                check_concat_with_shape(shapes,dimension)            
+                check_concat_with_shape(shapes,dimension)
-    """Initialize the weight with Xavier initialization scheme."""
+    """Initialize the weight with Xavier or similar initialization scheme.
-
+        factor = 1
-    out_prob = []
+    label = mx.sym.Variable("label")
-        label = mx.sym.Variable("t%d_label" % seqidx)
+        
-
+        last_hidden.append(hidden)
-    seq_labels = [rnn_exec.arg_dict["t%d_label" % i] for i in range(seq_len)]
+    seq_labels = rnn_exec.arg_dict["label"]
-    seq_outputs = [out_dict["t%d_sm_output" % i] for i in range(seq_len)]
+    seq_outputs = out_dict["sm_output"]
-        m.seq_labels[seqidx][:] = y
+        m.seq_labels[seqidx*batch_size : seqidx*batch_size+batch_size] = y
-        nll += -np.sum(np.log(py)) / len(y)
+    nll = -np.sum(np.log(seq_label_probs.asnumpy())) / len(X[0,:])
-                               for out, label in zip(m.seq_outputs, m.seq_labels)]
+            seq_label_probs = mx.nd.choose_element_0index(m.seq_outputs,m.seq_labels)
-                               for out, label in zip(m.seq_outputs, m.seq_labels)]
+            seq_label_probs = mx.nd.choose_element_0index(m.seq_outputs,m.seq_labels)
-def check_concat_with_shape(shapes):
+def check_concat_with_shape(shapes, dimension):
-        target_dim += shape[1]
+        target_dim += shape[dimension]
-    out = mx.symbol.Concat(*inputs, name='conc')
+    out = mx.symbol.Concat(*inputs, name='conc',dim=dimension)
-        arr[i][:] = shapes[i][1]
+        arr[i][:] = shapes[i][dimension]
-    ret = np.concatenate([narray.asnumpy() for narray in arr], axis=1)
+    ret = np.concatenate([narray.asnumpy() for narray in arr], axis=dimension)
-        check_concat_with_shape(shapes)
+    for dimension in range(4):
-            num_inst = label.size
+            if label.shape[0] < pred_label.shape[0]:
-parser.add_argument('--kv-type', type=str, default='local',
+parser.add_argument('--kv-store', type=str, default='local',
-parser.add_argument('--kv-type', type=str, default='local',
+parser.add_argument('--kv-store', type=str, default='local',
-parser.add_argument('--kv-type', type=str, default='local',
+parser.add_argument('--kv-store', type=str, default='local',
-    kv = mx.kvstore.create(args.kv_type)
+    kv = mx.kvstore.create(args.kv_store)
-    if args.kv_type == 'dist_sync':
+    if args.kv_store == 'dist_sync':
-    mnist = fetch_mldata('MNIST original', data_home='../../data')
+    data_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))
-sys.path = ['../autoencoder'] + sys.path
+import os
-import os
+# pylint: skip-file
-        if lr_scheduler != None:
+        if lr_scheduler is not None:
-        if self.lr_scheduler != None:
+        if self.lr_scheduler is not None:
-        if self.clip_gradient != None:
+        if self.clip_gradient is not None:
-        if lr_scheduler != None:
+        if lr_scheduler is not None:
-        if self.lr_scheduler != None:
+        if self.lr_scheduler is not None:
-                         num_update, self.base_lr )
+                         num_update, self.base_lr)
-
+        self.init_logginig = False
-    load_model = {}
+    model_args = {}
-        load_model = {'arg_params' : tmp.arg_params,
+        model_args = {'arg_params' : tmp.arg_params,
-        devs = [mx.gpu(int(i)) for i in args.gpus.split(',')]
+    devs = mx.cpu() if args.gpus is None else [
-        **load_model)
+        **model_args)
-from . import misc
+# from . import misc
-        self.epoch = epoch
+        self.num_update = 0
-            lr = self.lr_scheduler(self.epoch)
+            lr = self.lr_scheduler(self.num_update)
-net = importlib.import_module(args.network).get_symbol(10)
+net = importlib.import_module('symbol_' + args.network).get_symbol(10)
-net = importlib.import_module(args.network).get_symbol(args.num_classes)
+net = importlib.import_module('symbol_' + args.network).get_symbol(args.num_classes)
-from .ndarray import NDArray, zeros, clip
+from .ndarray import NDArray, zeros, clip, sqrt
-                )
+                zeros(weight.shape, weight.context))  # variance
-                         (self.beta1**t1))
+                         math.sqrt(1. - self.beta2**t1) /
-from .ndarray import NDArray, zeros, clip
+from .ndarray import NDArray, zeros, clip, sqrt
-logging.basicConfig(level=logging.DEBUG)
+import train_model
-parser.add_argument('--network', type=str, default='inception_bn_28',
+parser.add_argument('--network', type=str, default='inception-bn-28-small',
-parser.add_argument('--num-epochs', type=int, default=10,
+parser.add_argument('--model-prefix', type=str,
-                    help='the gpus will be used, e.g "0,1,2,3"')
+parser.add_argument('--load-epoch', type=int,
-        _download(data_dir)
+# network
-        batch_size  = batch_size,
+        path_imgrec = args.data_dir + "train.rec",
-        part_index  = part_index)
+        num_parts   = kv.num_workers,
-        mean_img    = data_dir + "mean.bin",
+        path_imgrec = args.data_dir + "test.rec",
-    return (train, val)
+        data_shape  = data_shape,
-net = importlib.import_module(args.network)
+    return (train, val)
-    batch_end_callback = mx.callback.Speedometer(args.batch_size))
+# train
-                    help='the prefix of the model')
+                    help='the prefix of the model to load/save')
-logging.basicConfig(level=logging.DEBUG, format=head)
+net = importlib.import_module(args.network).get_symbol(args.num_classes)
-    part_index  = kv.rank)
+def get_iterator(args, kv):
-    checkpoint = mx.callback.do_checkpoint(model_prefix)
+    val = mx.io.ImageRecordIter(
-                  'begin_epoch' : args.load_epoch}
+    return (train, val)
-    epoch_end_callback = checkpoint)
+train_model.fit(args, net, get_iterator)
-logging.basicConfig(level=logging.DEBUG)
+import train_model
-                    help='the gpus will be used, e.g "0,1,2,3"')
+parser.add_argument('--load-epoch', type=int,
-    flat        = flat)
+def get_iterator(args, kv):
-    flat        = flat)
+    train           = mx.io.MNISTIter(
-    devs = [mx.gpu(int(i)) for i in args.gpus.split(',')]
+    return (train, val)
-    wd            = 0.00001)
+# train
-
+        # init optmizer
-import caffe
+import numpy as np
-    net_caffe = caffe.Net(args.caffe_prototxt, args.caffe_model, caffe.TEST)
+
-
+    
-                    first_conv = False
+    
-from caffe.proto import caffe_pb2
+caffe_flag = True
-    solver_config = caffe.proto.caffe_pb2.NetParameter()
+    solver_config = ''
-            stride = 1 if len(param.stride) == 0 else param.stride[0]
+            pad = 0
-                param.kernel_size[0], stride, stride, not param.bias_term)
+                (param.num_output, pad, pad, kernel_size,\
-        >>> kv.set_updater(update)
+        >>> kv._set_updater(update)
-def get_symbol(num_label = 1000):
+def get_symbol(num_classes = 1000):
-    fc3 = mx.symbol.FullyConnected(data=dropout2, num_hidden=num_label)
+    fc3 = mx.symbol.FullyConnected(data=dropout2, num_hidden=num_classes)
-Inception + Batch normalization. suitable for images has size 28 x 28
+simplified inception-bn.py for images has size around 28 x 28
-    batch_end_callback = mx.callback.Speedometer(100))
+try:
-sys.path.append(os.path.join(curr_path, "../../../python"))
+import find_mxnet
-def get_symbol(num_label = 10):
+def get_symbol(num_classes = 10):
-    fc = mx.symbol.FullyConnected(data=flatten, num_hidden=num_label, name="fc1")
+    fc = mx.symbol.FullyConnected(data=flatten, num_hidden=num_classes, name="fc1")
-    return lenet
+import find_mxnet
-            if k in self.arg_params and (not overwrite):
+            if self.arg_params and k in self.arg_params and (not overwrite):
-            if k in self.aux_params and (not overwrite):
+        for k, v in aux_params.items():
-        aux_defined = True
+        param_name_shapes = [x for x in zip(arg_names, arg_shapes) if x[0] in param_names]
-            for k, v in self.arg_params.items():
+        for k, v in arg_params.items():
-            for k, v in self.aux_params.items():
+        for k, v in self.aux_params.items():
-def _download(data_dir='data/'):
+def _download(data_dir):
-    _download()
+    if '://' not in data_dir:
-        flat=flat)
+    train           = mx.io.MNISTIter(
-        flat=flat)
+        image       = data_dir + "t10k-images-idx3-ubyte",
-               eval_data=None, eval_metric='acc', epoch_end_callback=None,
+               eval_data=None, eval_metric='acc',
-                                     **kwargs)
+        sym = symbol.Symbol._Native(*args,
-	
+
-    shape = (3, 4)    
+    shape = (3, 4)
-    arr_grad = mx.nd.empty(shape) 
+    arr_grad = mx.nd.empty(shape)
-    
+
-    
+
-    heads = set(conf["heads"][0])  # TODO(xxx): check careful
+    heads = set([x[0] for x in conf["heads"]])  # TODO(xxx): check careful
-    def __init__(self, device_type, device_id=0, work_load=1):
+    def __init__(self, device_type, device_id=0):
-def cpu(device_id=0, work_load=1):
+def cpu(device_id=0):
-    return Context('cpu', device_id, work_load)
+    return Context('cpu', device_id)
-def gpu(device_id=0, work_load=1):
+def gpu(device_id=0):
-    return Context('gpu', device_id, work_load)
+    return Context('gpu', device_id)
-def _split_input_slice(batch_size, ctx):
+def _split_input_slice(batch_size, work_load_list):
-        The list of device context used in training
+    work_load_list : list of float or int, optional
-                        logger=None):
+                        logger=None, work_load_list=None):
-    slices = _split_input_slice(train_data.batch_size, ctx)
+    if work_load_list is None:
-            epoch_end_callback=None, batch_end_callback=None, kvstore='local', logger=None):
+            epoch_end_callback=None, batch_end_callback=None, kvstore='local', logger=None,
-                            logger=logger)
+                            logger=logger, work_load_list=work_load_list)
-               kvstore='local', logger=None, **kwargs):
+               kvstore='local', logger=None, work_load_list=None, **kwargs):
-                  logger=logger)
+                  logger=logger,
-    batch_num_list = [round(work_load * batch_size / total_work_load) for work_load in work_load_list]
+    batch_num_list = [round(work_load * batch_size / total_work_load)
-    def __init__(self, device_type, device_id=0):
+
-def cpu(device_id=0):
+def cpu(device_id=0, work_load=1):
-    return Context('cpu', device_id)
+    return Context('cpu', device_id, work_load)
-def gpu(device_id=0):
+def gpu(device_id=0, work_load=1):
-    return Context('gpu', device_id)
+    return Context('gpu', device_id, work_load)
-def _split_input_slice(batch_size, num_split):
+def _split_input_slice(batch_size, ctx):
-        The number of split we want to have.
+    ctx : list of Context class
-    step = (batch_size + num_split - 1) / num_split
+    work_load_list = [float(c.work_load) for c in ctx]
-        if begin == end:
+    end = 0
-    slices = _split_input_slice(train_data.batch_size, num_device)
+    slices = _split_input_slice(train_data.batch_size, ctx)
-    num_round     = 20,
+    num_epoch     = 20,
-model = mx.model.FeedForward(ctx=gpus, symbol=softmax, num_round=num_round,
+model = mx.model.FeedForward(ctx=gpus, symbol=softmax, num_epoch=num_round,
-model = mx.model.FeedForward(ctx=gpus, symbol=softmax, num_round=num_round,
+model = mx.model.FeedForward(ctx=gpus, symbol=softmax, num_epoch=num_round,
-        cb_ptr = hex(cast(pointer(self.info_), c_void_p).value)
+        cb_ptr = format(cast(pointer(self.info_), c_void_p).value, 'x')
-                logging.log(self.level, 'iter:%d  param:%s\t\tstat(%s):%s'%(i, key, self.stat.__name__, str(self.stat(arr.asnumpy()))))
+                logging.log(self.level, 'Iter:%d  param:%s\t\tstat(%s):%s'%(i, key, self.stat.__name__, str(self.stat(arr.asnumpy()))))
-                logging.info('Iter:%d metric:%f'%(i, metric.get()[1]))
+                logging.log(self.level, 'Iter:%d  param:%s\t\tstat(%s):%s\t\tgrad_stat:%s'%(i, key, self.stat.__name__, str(self.stat(weights[key].asnumpy())), str(self.stat(arr.asnumpy()))))
-        internal_dict = {}
+        internal_dict = dict(zip(input_names, input_buffs))
-                self.iter_start_callback(i)
+                if self.iter_start_callback(i):
-                self.iter_end_callback(i) 
+                if self.iter_end_callback(i):
-        self.num_inst += 1
+            self.num_inst += 1
-sys.path.append("../../../predict/python/")
+sys.path.append("../../../amalgamation/python/")
-    exe_test = test.bind(mx.cpu(), args=[arr_data])
+    test = 2 / (4-((1+data+1)*2/5)-0.2)
-    npout = (1+data_tmp +1)*2/5-0.2+3*data_tmp+5/data_tmp-data_tmp
+    npout_1 = (4-((1+data_tmp+1)*2/5)-0.2)
-            return Symbol._PlusScalar(self, value=other)
+            return Symbol._PlusScalar(self, scalar=other)
-            return Symbol._MinusScalar(self, value=other)
+            return Symbol._MinusScalar(self, scalar=other)
-            return Symbol._MinusScalar(self, value=other, right=True)
+            return Symbol._MinusScalar(self, scalar=other, scalar_on_right=True)
-            return Symbol._MulScalar(self, value=other)
+            return Symbol._MulScalar(self, scalar=other)
-            return Symbol._DivScalar(self, value=other)
+            return Symbol._DivScalar(self, scalar=other)
-            return Symbol._DivScalar(self, value=other, right=True)
+            return Symbol._DivScalar(self, scalar=other, scalar_on_right=True)
-data_dir = "data/cifar/"
+# data_dir = "data/cifar/"
-# data_dir = "s3://dmlc/cifar10/"
+data_dir = "s3://dmlc/cifar10/"
-num_gpus = 1
+num_gpus = 4
-logging.basicConfig(level=logging.DEBUG)
+head = '%(asctime)-15s Node[' + str(kv.rank) + '] %(message)s'
-    initializer   = mx.init.Uniform(0.07))
+    initializer   = mx.init.Uniform(0.07),
-    batch_end_callback = mx.callback.Speedometer(batch_size, 10))
+    batch_end_callback = mx.callback.Speedometer(batch_size, 10),
-kv_type = 'dist_async'
+kv_type = 'dist_sync'
-batch_size_per_gpu = 40
+batch_size_per_gpu = 36
-num_gpus = 1
+num_gpus = 4
-logging.basicConfig(level=logging.DEBUG)
+head = '%(asctime)-15s Node[' + str(kv.rank) + '] %(message)s'
-    num_epoch     = 20,
+    num_epoch     = 30,
-    wd            = 0.00001)
+    wd            = 0.00001,
-          batch_end_callback = mx.callback.Speedometer(batch_size, 10))
+          batch_end_callback = mx.callback.Speedometer(batch_size, 10),
-        data_iter = mx.io.NDArrayIter([X], batch_size=batch_size, shuffle=False,
+        data_iter = mx.io.NDArrayIter({'data': X}, batch_size=batch_size, shuffle=False,
-                data_iter_i = mx.io.NDArrayIter([X_i], batch_size=batch_size,
+                X_i = model.extract_feature(self.internals[i-1], self.args,
-                         0, n_iter, self.args_mult)
+            solver.solve(self.xpu, self.stacks[i], self.args, self.args_grad, data_iter_i,
-        data_iter = mx.io.NDArrayIter([X], batch_size=batch_size, shuffle=False,
+        data_iter = mx.io.NDArrayIter({'data': X}, batch_size=batch_size, shuffle=False,
-                     0, n_iter, self.args_mult)
+        solver.solve(self.xpu, self.loss, self.args, self.args_grad, data_iter,
-        data_iter = mx.io.NDArrayIter([X], batch_size=batch_size, shuffle=False,
+        data_iter = mx.io.NDArrayIter({'data': X}, batch_size=batch_size, shuffle=False,
-        Y = model.extract_feature(self.loss, self.args, ['data'], data_iter,
+        Y = model.extract_feature(self.loss, self.args, data_iter,
-    input_buffs = [mx.nd.empty(i.shape, ctx=xpu) for i in data_iter.next()]
+def extract_feature(sym, args, data_iter, N, xpu=mx.cpu()):
-        for data, buff in zip(datas, input_buffs):
+    for batch in data_iter:
-        args = dict(args, **input_dict)
+    def solve(self, xpu, sym, args, args_grad,
-        update_dict = {name: args_grad[name] for name, nd in zip(sym.list_arguments(), exe.grad_arrays) if nd}
+        assert len(sym.list_arguments()) == len(exe.grad_arrays)
-        for i in range(begin_epoch, end_epoch):
+        for i in range(begin_iter, end_iter):
-                data_list = data_iter.next()
+                batch = data_iter.next()
-                data.copyto(input_dict[key])
+                batch = data_iter.next()
-                                   output_buff[output_names[0]].asnumpy())
+                self.metric.update([input_buffs[-1].asnumpy()],
-    out = exe_add.outputs[0].asnumpy()
+    out = exe_test.outputs[0].asnumpy()
-            return Symbol._PlusScalar(self, other)
+            return Symbol._PlusScalar(self, value=other)
-            return Symbol._MinusScalar(self,value=other)
+            return Symbol._MinusScalar(self, value=other)
-            return Symbol._MinusScalar(self,value=other,right=True)
+            return Symbol._MinusScalar(self, value=other, right=True)
-            return Symbol._DivScalar(self, value=other,right=True)
+            return Symbol._DivScalar(self, value=other, right=True)
-            return DataBatch(data=self.getdata(), label=self.getlabel(), pad=self.getpad())
+            return DataBatch(data=self.getdata(), label=self.getlabel(), \
-            return  DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(), index=self.getindex())
+            return  DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(),
-            return DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(), index=self.getindex())
+            return DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(),
-        check_call(_LIB.MXDataIterGetIndex(self.handle, 
+        check_call(_LIB.MXDataIterGetIndex(self.handle,
-        dbuffer = (ctypes.c_uint64* index_size.value).from_address(ctypes.addressof(index_data.contents))
+        address = ctypes.addressof(index_data.contents)
-DataBatch = namedtuple('DataBatch', ['data', 'label', 'pad'])
+DataBatch = namedtuple('DataBatch', ['data', 'label', 'pad', 'index'])
-            return  DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad())
+            return  DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(), index=self.getindex())
-            return DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad())
+            print 'label', self.getlabel().asnumpy()
-        return batch_size.value
+        index_size = ctypes.c_uint64(0)
-
+    def getindex(self):
-        batch_size  = batch_size)
+        batch_size  = batch_size,
-    if data_dir == "data/cifar":
+    if data_dir == "data/cifar/":
-        mean_img    = data_dir + "/cifar_mean.bin",
+        path_imgrec = data_dir + "train.rec",
-        mean_img    = data_dir + "/cifar_mean.bin",
+        path_imgrec = data_dir + "test.rec",
-    softmax = mx.symbol.SoftmaxOutput(data=fc, name="loss")
+    softmax = mx.symbol.SoftmaxOutput(data=fc, name="softmax")
-    if data_dir == 'data':
+    if data_dir == 'data/':
-        label = data_dir + "/train-labels-idx1-ubyte",
+        image = data_dir + "train-images-idx3-ubyte",
-        label = data_dir + "/t10k-labels-idx1-ubyte",
+        image = data_dir + "t10k-images-idx3-ubyte",
-    softmax = mx.symbol.SoftmaxOutput(data = fc2)
+    softmax = mx.symbol.SoftmaxOutput(data = fc2, name='softmax')
-data_dir = "data/cifar"
+data_dir = "data/cifar/"
-# data_dir = "data"
+# data_dir = "data/"
-kv_type = 'dist_sync'
+kv_type = 'local'
-        assert self.num_data > batch_size, \
+        assert self.num_data >= batch_size, \
-            return DataBatch(data=self.getdata(), label=self.getlabel(), pad=self.getpad())
+            return DataBatch(data=self.getdata(), label=self.getlabel(), \
-            return  DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(), index=self.getindex())
+            return  DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(),
-            return DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(), index=self.getindex())
+            return DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(),
-        check_call(_LIB.MXDataIterGetIndex(self.handle, 
+        check_call(_LIB.MXDataIterGetIndex(self.handle,
-        dbuffer = (ctypes.c_uint64* index_size.value).from_address(ctypes.addressof(index_data.contents))
+        address = ctypes.addressof(index_data.contents)
-DataBatch = namedtuple('DataBatch', ['data', 'label', 'pad'])
+DataBatch = namedtuple('DataBatch', ['data', 'label', 'pad', 'index'])
-            return  DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad())
+            return  DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad(), index=self.getindex())
-            return DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad())
+            print 'label', self.getlabel().asnumpy()
-        return batch_size.value
+        index_size = ctypes.c_uint64(0)
-                # this epoch is done
+                # this epoch is done possibly earlier
-            if epoch_size is None or nbatch < epoch_size:
+            if do_reset == True:
-        eval_data.reset()
+    if data_dir == "data/cifar":
-    get_data.GetCifar10()
+#!/usr/bin/env python
-
+    def getindex(self):
-        return False
+    def __init__(self):
-        return ['prob']
+        return ['output']
-    	return [in_shape[0], (in_shape[0][0],)], [in_shape[0]]
+        data_shape = in_shape[0]
-    	y /= y.sum(axis=1).reshape((x.shape[0], 1))
+        x = in_data[0]
-    	dx[np.arange(l.shape[0]), l] -= 1.0
+        l = in_data[1]
-        self.lr_mult = {}
+        self.lr_scale = {}
-        """Set individual learning rate multiplers for parameters
+    def set_lr_scale(self, args_lrscale):
-        args_lrmult : dict of index to float
+        args_lrscale : dict of index to float
-        self.lr_mult = args_lrmult.copy()
+        self.lr_scale = args_lrscale.copy()
-        lr *= self.lr_mult.get(index, default=1.0)
+        lr *= self.lr_scale.get(index, 1.0)
-            eval_data.reset()
+    train_data.reset()
-
+              data_iter, begin_epoch, end_epoch, debug = False, args_lrmult={}):
-                self.updater(key, arr, args[key], args_lrmult.get(key, 1.0))
+                self.updater(key, arr, args[key])
-    def update(self, index, weight, grad, state, lr_mult=1.0):
+    def update(self, index, weight, grad, state):
-    def update(self, index, weight, grad, state, lr_mult=1.0):
+    def update(self, index, weight, grad, state):
-        lr *= lr_mult
+        lr *= self.lr_mult.get(index, default=1.0)
-    def update(self, index, weight, grad, state, lr_mult=1.0):
+    def update(self, index, weight, grad, state):
-    def updater(index, grad, weight, lr_mult=1.0):
+    def updater(index, grad, weight):
-        optimizer.update(index, weight, grad, states[index], lr_mult=1.0)
+        optimizer.update(index, weight, grad, states[index])
-    def update(self, index, weight, grad, state):
+    def update(self, index, weight, grad, state, lr_mult=1.0):
-    def update(self, index, weight, grad, state):
+    def update(self, index, weight, grad, state, lr_mult=1.0):
-    def update(self, index, weight, grad, state):
+    def update(self, index, weight, grad, state, lr_mult=1.0):
-    def updater(index, grad, weight):
+    def updater(index, grad, weight, lr_mult=1.0):
-        optimizer.update(index, weight, grad, states[index])
+        optimizer.update(index, weight, grad, states[index], lr_mult=1.0)
-# data dir
+## in local machine:
-# batch size
+## in amazon s3:
-# learning rate
+## number of gpus used in a worker
-# local, dist_async or dist_sync
+## in local machine:
-# learning rate
+
-                        kvstore.pull(index, arg_list, priority=-index)
+        while True:
-                    batch_end_callback(batch_end_params)
+                        batch_end_callback(batch_end_params)
-            eval_metric.update(data_batch.label, cpu_output_arrays)
+                # evaluate at end, so out_cpu_array can lazy copy
-                break
+                # this epoch is done
-            train_data.reset()
+            # reset the training data if reach the end of train_data, we only
-            break
+            # this epoch is done
-                        begin_epoch, end_epoch, optimizer,
+                        begin_epoch, end_epoch, epoch_size, optimizer,
-                 num_epoch=None, optimizer='sgd',
+                 num_epoch=None, epoch_size=None, optimizer='sgd',
-               num_epoch=None, optimizer='sgd', initializer=Uniform(0.01),
+               num_epoch=None, epoch_size=None, optimizer='sgd', initializer=Uniform(0.01),
-        model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch,
+        model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch, epoch_size=epoch_size,
-            "batch_size needs to be smaller than data size when not padding."
+        self.num_data = self.data_list[0].shape[0]
-        self.reset()
+        self.first_batch = None
-
+        self.first_batch = None
-                        optimizer, kvstore, update_on_kvstore,
+                        begin_epoch, end_epoch, optimizer,
-                                                     eval_metric=eval_metric)
+
-                # this epoch is done
+            # update the parameters
-                if epoch_size is not None and nbatch == epoch_size:
+                    continue
-                    break
+                        # pull back the sum gradients, to the same locations.
-            # reset the training data if reach the end of train_data, we only
+            nbatch += 1
-            if epoch_size is None or nbatch != epoch_size:
+                                                 eval_metric=eval_metric)
-                train_data.reset()
+                    batch_end_callback(batch_end_params)
-            if epoch_size is None or nbatch == epoch_size:
+            # evaluate at end, so out_cpu_array can lazy copy
-                 num_epoch=None, epoch_size=None, optimizer='sgd',
+                 num_epoch=None, optimizer='sgd',
-                            epoch_size=self.epoch_size,
+                            begin_epoch=self.begin_epoch, end_epoch=self.num_epoch,
-               optimizer='sgd', initializer=Uniform(0.01),
+               num_epoch=None, optimizer='sgd', initializer=Uniform(0.01),
-                            **kwargs)
+        model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch,
-    exit(-1)
+parser = argparse.ArgumentParser(description='Parse mxnet output log')
-with open(sys.argv[1]) as f:
+args = parser.parse_args()
-    data[epoch][i*2] += val
+    if op == 'max' and i != 2:
-    print "| %d | %f | %f | %.1f |" % (k+1, v[0]/v[1], v[2]/v[3], v[4]/v[5])
+    if op == 'avg':
-val_iter = mx.io.NDArrayIter(data=val_data, label=val_label, batch_size=batch_size)
+train_iter = mx.io.NDArrayIter(train_data, train_label, batch_size=batch_size, shuffle=True)
-    a bottleneck.
+class PythonOp(object):
-    def __init__(self):
+    def __init__(self, need_top_grad=True):
-        return True
+    
-
+import math
-               optimizer='sgd', half_life=2, **kwargs):
+               optimizer='sgd', half_life=2,max_grad_norm = 5.0, **kwargs):
-    max_grad_norm = 5.0
+    
-                    norm = mx.nd.norm(grad).asscalar()                        
+                    l2_norm = mx.nd.norm(grad).asscalar()
-
+max_grad_norm = 5.0
-                              seqidx=seqidx, layeridx=i, dropout=0.)
+                              seqidx=seqidx, layeridx=i, dropout=dp)
-    eps = 1e-10
+    
-        nll += -np.sum(np.log(np.maximum(py, eps))) / len(y)
+        nll += -np.sum(np.log(py)) / len(y)
-                              rescale_grad=rescale_grad,
+    
-
+    last_perp = 10000000.0
-                print("Epoch [%d] Train: NLL=%.3f, Prep=%.3f" % (
+                print("Epoch [%d] Train: NLL=%.3f, Perp=%.3f" % (
-        print("Iter [%d] Train: Time: %.3f sec, NLL=%.3f, Prep=%.3f" % (
+        print("Iter [%d] Train: Time: %.3f sec, NLL=%.3f, Perp=%.3f" % (
-        print("Iter [%d] Val: NLL=%.3f, Prep=%.3f" % (
+        perp = np.exp(val_nll / nbatch)
-            opt.lr *= 0.9
+        if last_perp - 1.0 < perp:
-
+        last_perp = perp
-seq_len = 20
+seq_len = 35
-num_round = 20
+num_round = 25
-wd=0.00001
+wd=0.
-clip_gradient=1
+
-                             initializer=mx.initializer.Uniform(0.1))
+                             initializer=mx.initializer.Uniform(0.1),dropout=0.5)
-                clip_gradient=clip_gradient)
+                momentum=momentum)
-            return  self.getdata(), self.getlabel()
+            return  self.getdata(), self.getlabel(), self.getindex()
-            return self.getdata(), self.getlabel()
+            return self.getdata(), self.getlabel(), self.getindex()
-            for data, label in train_data:
+            for data, label, index in train_data:
-kv_type = 'dist_sync'
+kv_type = 'dist_async'
-learning_rate = 0.1
+learning_rate = 0.05
-    epoch_size    = math.ceil(60000/batch_size/kv.num_workers),
+    epoch_size    = 60000 / batch_size / kv.num_workers,
-    epoch_size    = math.ceil(1281167/batch_size/kv.num_workers),
+    epoch_size    = 1281167 / batch_size / kv.num_workers,
-                if epoch_size is not None and nbatch == epoch_size:
+                if epoch_size is not None and nbatch >= epoch_size:
-            if epoch_size is None or nbatch != epoch_size:
+            if epoch_size is None or nbatch < epoch_size:
-            if epoch_size is None or nbatch == epoch_size:
+            if epoch_size is None or nbatch >= epoch_size:
-            "batch_size need to be smaller than data size when not padding."
+        assert self.num_data >= batch_size, \
-
+import math
-               optimizer='sgd', half_life=2, **kwargs):
+               optimizer='sgd', half_life=2,max_grad_norm = 5.0, **kwargs):
-    max_grad_norm = 5.0
+    
-                    norm = mx.nd.norm(grad).asscalar()                        
+                    l2_norm = mx.nd.norm(grad).asscalar()
-
+max_grad_norm = 5.0
-(train, val) = common.cifar10(batch_size = 128, input_shape=(3,28,28))
+(train, val) = common.cifar10(
-                            input_shape = (1,28,28))
+(train, val) = common.mnist(
-def ilsvrc12(data_dir, batch_size, input_shape, num_parts=1, part_index=0):
+def ilsvrc12(data_dir, batch_size, num_parts=1, part_index=0):
-    softmax = mx.symbol.SotfmaxOutput(data=fc1, name='softmax')
+    softmax = mx.symbol.SoftmaxOutput(data=fc1, name='softmax')
-kv_type = 'dist_async'
+kv_type = 'dist_sync'
-learning_rate = 0.05
+learning_rate = 0.1
-kv_type = 'local'
+# local, dist_async or dist_sync
-num_gpus = 1
+num_gpus = 2
-    ctx = mx.gpu(1),
+    ctx           = [mx.gpu(i) for i in range(num_gpus)],
-    num_epoch     = 20,
+    num_epoch     = 40,
-    ctx = mx.gpu(), symbol = mlp, num_epoch = 20,
+    ctx = mx.cpu(), symbol = mlp, num_epoch = 20,
-                ('list_arguments', list_functype)
+                ('list_arguments', list_functype),
-                                 list_functype(list_arguments_entry))
+                                 list_functype(list_arguments_entry),
-kv_type = 'dist_async'
+# kv_type = 'dist_async'
-batch_size = 128
+batch_size = 256
-learning_rate = 0.05
+learning_rate = 0.1
-                            num_parts = kv.num_workers if kv.rank != 0 else 1,
+                            num_parts = kv.num_workers,
-    ctx           = [mx.gpu(i) for i in range(num_gpus)],
+    # ctx           = [mx.gpu(i) for i in range(num_gpus)],
-    num_epoch     = 1,
+    num_epoch     = 20,
-../../ps-lite/tracker/dmlc_mpi.py
+import sys
-            if epoch_size is not None and nbatch == epoch_size:
+            if epoch_size is None or nbatch == epoch_size:
-       logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc - tic))
+        name, value = eval_metric.get()
-
+all iterators are disabled randomness
-sys.path.insert(0, "../../python/")
+sys.path.insert(0, "../../../python/")
-import mxnet as mx
+import mxnet as mx
-import common
+import common
-import mxnet as mx
+import mxnet as mx
-# $ ./dmlc_local.py -n 4 -s 4 ./test_kvstore.py
+import sys
-
+import common
-import common
+    epoch_size    = 60000 / 100,
-import mxnet as mx
+import mxnet as mx
-import mxnet as mx
+import mxnet as mx
-
+import common
-                        kvstore, update_on_kvstore,
+                        begin_epoch, end_epoch, epoch_size,
-                        kvstore.pull(index, arg_list, priority=-index)
+        while True:
-        logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc - tic))
+                        batch_end_callback(batch_end_params)
-                 num_epoch=None, optimizer='sgd',
+                 num_epoch=None, epoch_size=None, optimizer='sgd',
-                            begin_epoch=self.begin_epoch, end_epoch=self.num_epoch,
+                            begin_epoch=self.begin_epoch,
-               num_epoch=None, optimizer='sgd', initializer=Uniform(0.01),
+               num_epoch=None, epoch_size=None,
-                            optimizer=optimizer, initializer=initializer, **kwargs)
+        model = FeedForward(symbol, ctx=ctx,
-            for k, v in self.arg_params.iteritems():
+            for k, v in self.arg_params.items():
-            for k, v in self.aux_params.iteritems():
+            for k, v in self.aux_params.items():
-        self.num_inst += label.size
+        self.num_inst += num_inst
-    softmax = mx.symbol.SoftmaxOutput(fc3, name = 'sm')
+    softmax = mx.symbol.SoftmaxOutput(fc3, name = 'softmax')
-    lenet = mx.symbol.SoftmaxOutput(data=fc2)
+    lenet = mx.symbol.SoftmaxOutput(data=fc2, name='softmax')
-    softmax = mx.symbol.SoftmaxOutput(data=fc, name="loss")
+    softmax = mx.symbol.SoftmaxOutput(data=fc, name="softmax")
-    y = np.concatenate([label.asnumpy() for _, label in val_dataiter]).astype('int')
+    y = np.concatenate([batch.label[0].asnumpy() for batch in val_dataiter]).astype('int')
-            The label of the data.
+        labels : list of NDArray
-            Predicted value.
+        preds : list of NDArray
-        self.sum_metric += numpy.sum(pred_label == label)
+    def update(self, labels, preds):
-        self.sum_metric += self._feval(label, pred)
+    def update(self, labels, preds):
-    y = np.concatenate([label.asnumpy() for _, label in val_dataiter]).astype('int')
+    y = np.concatenate([batch.label[0].asnumpy() for batch in val_dataiter]).astype('int')
-kv = mx.kvstore.create('dist_sync')
+# for single gpu
-          epoch_end_callback = mx.callback.Speedometer(batch_size, 5))
+          batch_end_callback = mx.callback.Speedometer(batch_size, 10))
-sys.path.insert(0, "../../python/")
+sys.path.insert(0, "../../../python/")
-    return data.items()
+    return list(data.items())
-                              seqidx=seqidx, layeridx=i, dropout=0.)
+                              seqidx=seqidx, layeridx=i, dropout=dp)
-    eps = 1e-10
+    
-        nll += -np.sum(np.log(np.maximum(py, eps))) / len(y)
+        nll += -np.sum(np.log(py)) / len(y)
-                              rescale_grad=rescale_grad,
+    
-
+    last_perp = 10000000.0
-                print("Epoch [%d] Train: NLL=%.3f, Prep=%.3f" % (
+                print("Epoch [%d] Train: NLL=%.3f, Perp=%.3f" % (
-        print("Iter [%d] Train: Time: %.3f sec, NLL=%.3f, Prep=%.3f" % (
+        print("Iter [%d] Train: Time: %.3f sec, NLL=%.3f, Perp=%.3f" % (
-        print("Iter [%d] Val: NLL=%.3f, Prep=%.3f" % (
+        perp = np.exp(val_nll / nbatch)
-            opt.lr *= 0.9
+        if last_perp - 1.0 < perp:
-
+        last_perp = perp
-seq_len = 20
+seq_len = 35
-num_round = 20
+num_round = 25
-wd=0.00001
+wd=0.
-clip_gradient=1
+
-                             initializer=mx.initializer.Uniform(0.1))
+                             initializer=mx.initializer.Uniform(0.1),dropout=0.5)
-                clip_gradient=clip_gradient)
+                momentum=momentum)
-    for k, v in data.iteritems():
+    for k, v in data.items():
-    for k, v in data.iteritems():
+    for k, v in data.items():
-from collections import namedtuple
+from collections import namedtuple, OrderedDict
-            data = {default_name: data[0]}
+            data = OrderedDict([(default_name, data[0])])
-            data = {'_%d_%s' % (i, default_name) : d for i, d in enumerate(data)}
+            data = OrderedDict([('_%d_%s' % (i, default_name), d) for i, d in enumerate(data)])
-        # disable E1101 b/c lint says numpy has no copyto function, maybe due to different version?
+        # pylint: disable=W0201
-            idx = np.arange(self.data_list[0].shape[0])
+            idx = np.arange(self.data[0][1].shape[0])
-                np.copyto(self.data_list[i], self.data_list[i][idx])
+            self.data = [(k, v[idx]) for k, v in self.data]
-            for k, _ in self.data.iteritems():
+            for k, _ in self.data:
-            for k, _ in self.label.iteritems():
+            for k, _ in self.label:
-    for data, label in train_dataiter:
+    for batch in train_dataiter:
-        npdata = data.asnumpy().flatten().sum()
+    for batch in dataiter:
-        nplabel = label.asnumpy()
+        nplabel = batch.label[0].asnumpy()
-    for data, label in dataiter:
+    for batch in dataiter:
-        assert((data.asnumpy()[:,0,0] == label).all())
+    for batch in dataiter:
-        # pylint: disable=W0201
+        # pylint: disable=W0201, E1101
-        preprocess_threads=1)
+        preprocess_threads=1,
-        preprocess_threads=1)
+        preprocess_threads=1,
-softmax = mx.symbol.SoftmaxOutput(data=fc3)
+softmax = mx.symbol.SoftmaxOutput(data=fc3, name='softmax')
-lenet = mx.symbol.SoftmaxOutput(data=fc2)
+lenet = mx.symbol.SoftmaxOutput(data=fc2, name='softmax')
-mlp = mysoftmax(data=fc3, name = 'mlp')
+mlp = mysoftmax(data=fc3, name = 'softmax')
-
+probs = model.predict(val_data)
-                                      shuffle=is_train)
+                return io.NDArrayIter(X, y, self.numpy_batch_size, shuffle=False)
-            raise TypeError('Eval data, if presented, must be a DataIter')
+        data = self._init_iter(X, y, is_train=True)
-    def __init__(self, data, label=[], batch_size=1, shuffle=False, last_batch_handle='pad'):
+    def __init__(self, data, label=None, batch_size=1, shuffle=False, last_batch_handle='pad'):
-        self.label = self._init_data(label, allow_empty=True, default_name='softmax_label')
+        self.data = _init_data(data, allow_empty=False, default_name='data')
-            for k,v in self.data.iteritems():
+            for k, _ in self.data.iteritems():
-            for k,v in self.label.iteritems():
+            for k, _ in self.label.iteritems():
-        return [(k, tuple([self.batch_size] + list(v.shape[1:]))) for k,v in self.data]
+        """The name and shape of data provided by this iterator"""
-        return [(k, tuple([self.batch_size] + list(v.shape[1:]))) for k,v in self.label]
+        """The name and shape of label provided by this iterator"""
-            return DataBatch(data=self.getdata(),label=self.getlabel(),pad=self.getpad())
+            return DataBatch(data=self.getdata(), label=self.getlabel(), pad=self.getpad())
-                                         x[1][:pad]),
+            return [array(np.concatenate((x[1][self.cursor:], x[1][:pad]),
-    def __init__(self, handle, data_name='data', label_name='softmax_label', **kwargs):
+    def __init__(self, handle, data_name='data', label_name='softmax_label', **_):
-                                pad=self.getpad())
+            return  DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad())
-                               pad=self.getpad())
+            return DataBatch(data=[self.getdata()], label=[self.getlabel()], pad=self.getpad())
-# pylint: disable=fixme, invalid-name, too-many-arguments, too-many-locals
+# pylint: disable=fixme, invalid-name, too-many-arguments, too-many-locals, too-many-lines
-        train_exec  = symbol.simple_bind(ctx[i], 'write', **data_shapes)
+                       for k, v in train_data.provide_data}
-    data_names  = [x[0] for x in train_data.provide_data]
+    data_names = [x[0] for x in train_data.provide_data]
-    data_arrays = [[(slices[i],e.arg_dict[name]) for i,e in enumerate(train_execs)]
+    data_arrays = [[(slices[i], e.arg_dict[name]) for i, e in enumerate(train_execs)]
-    label_arrays = [[(slices[i],e.arg_dict[name]) for i,e in enumerate(train_execs)]
+    label_arrays = [[(slices[i], e.arg_dict[name]) for i, e in enumerate(train_execs)]
-    aux_arrays   = [[e.aux_arrays[i] for e in train_execs] for i in range(len(aux_names))]
+    grad_arrays = [[e.grad_arrays[i] for e in train_execs] for i in param_idx]
-    begin_epoch : int,optional
+    begin_epoch : int, optional
-        arg_names   = self.symbol.list_arguments()
+        arg_names = self.symbol.list_arguments()
-        aux_names   = self.symbol.list_auxiliary_states()
+        aux_names = self.symbol.list_auxiliary_states()
-        data_names  = [x[0] for x in data_shapes]
+        data_names = [x[0] for x in data_shapes]
-        output_list = [[] for i in range(len(self._pred_exec.outputs))]
+        output_list = [[] for _ in range(len(self._pred_exec.outputs))]
-            kvstore='local', logger=None):
+            epoch_end_callback=None, batch_end_callback=None, kvstore='local', logger=None):
-                            logger=logger)
+    # def fit_old(self, X, y=None, eval_data=None, eval_metric='acc',
-    ctx = mx.cpu(), symbol = mlp, num_epoch = 2,
+    ctx = mx.cpu(), symbol = mlp, num_epoch = 20,
-model.fit(data=train, eval_data=val)
+model.fit(train, eval_data=val)
-#model.fit(data=train_data, y=train_label)
+model.fit(X=train_data, y=train_label)
-model.fit(data=train_iter, eval_data=val_iter)
+#model.fit(train_iter, eval_data=val_iter)
-    #    return next_res.value
+    def iter_next(self):
-        The input shape of the net.
+    batch_size : int
-
+    arg_names: list of str
-    def fit(self, data, eval_data=None, eval_metric='acc',
+    def fit(self, X, y=None, eval_data=None, eval_metric='acc',
-mlp = mx.symbol.SoftmaxOutput(data = fc3, name = 'mlp')
+mlp = mx.symbol.SoftmaxOutput(data = fc3, name = 'softmax')
-val_iter = mx.io.NDArrayIter(data=val_data, label=val_label, batch_size=batch_size)
+train_iter = mx.io.NDArrayIter(train_data, train_label, batch_size=batch_size, shuffle=True)
-model.fit(X=train_data, y=train_label)
+#model.fit(data=train_data, y=train_label)
-# model.fit(X=train_iter, eval_data=val_iter)
+model.fit(data=train_iter, eval_data=val_iter)
-    def __init__(self, *args, **kwargs):
+    def __init__(self, data, label=[], batch_size=1, shuffle=False, last_batch_handle='pad'):
-                data_list[i] = data_list[i].asnumpy()
+        self.data  = self._init_data(data, allow_empty=False, default_name='data')
-            idx = np.arange(data_list[0].shape[0])
+            idx = np.arange(self.data_list[0].shape[0])
-                data_list[i] = data_list[i][idx]
+                assert self.data_list[i].shape[0] == len(idx)
-        self.num_data = data_list[0].shape[0]
+            new_n = self.data_list[0].shape[0] - self.data_list[0].shape[0] % batch_size
-        self.data_list = data_list
+    def _init_data(self, data, allow_empty, default_name):
-            return (self.getdata(i) for i in range(self.num_source))
+            return DataBatch(data=self.getdata(),label=self.getlabel(),pad=self.getpad())
-        assert(index < self.num_source)
+    def _getdata(self, data_source):
-            return array(self.data_list[index][self.cursor:self.cursor+self.batch_size])
+            return [array(x[1][self.cursor:self.cursor+self.batch_size]) for x in data_source]
-                                        axis=0))
+            return [array(np.concatenate((x[1][self.cursor:],
-        return self.getdata(-1)
+        return self._getdata(self.label)
-            return  MXDataBatch(data=[self.getdata()],
+            return  DataBatch(data=[self.getdata()],
-            return MXDataBatch(data=[self.getdata()],
+            return DataBatch(data=[self.getdata()],
-    ctx = mx.cpu(), symbol = mlp, num_epoch = 20,
+    ctx = mx.cpu(), symbol = mlp, num_epoch = 2,
-            d_src[slice_idx].copyto(d_dst)
+        if isinstance(d_targets, nd.NDArray):
-    def _init_predictor(self, input_shape):
+    def _init_predictor(self, input_shapes):
-        self._pred_exec_input = pred_exec.arg_arrays[data_index]
+            self.ctx[0], grad_req='null', **dict(input_shapes))
-        X : mxnet.DataIter or numpy.ndarray
+        X : mxnet.DataIter
-        y : numpy.ndarray
+        y : numpy.ndarray or a list of numpy.ndarray if the network has multiple outputs.
-            data.copyto(self._pred_exec_input)
+        data_shapes = X.provide_data
-        return np.concatenate(outputs)
+            padded = batch.pad
-            data_index = index
+    arg_names = symbol.list_arguments()
-        if name in arg_set:
+        if name in aux_set:
-    return (data_index, label_index)
+        aux_set.add(name)
-    #out_cpu_array = nd.zeros(merged_shape, cpu())
+        for idx in range(len(param_arrays)):
-            #         weight.copyto(aux_params[name])
+    param_names = [arg_names[i] for i in param_idx]
-mlp = mx.symbol.SoftmaxOutput(data = fc3, name = 'mlp')
+mlp = mx.symbol.SoftmaxOutput(data = fc3, name = 'softmax')
-model.fit(X=train, eval_data=val)
+model.fit(data=train, eval_data=val)
-    def __init__(self, handle, data_name='data', label_name='softmax_label'):
+    def __init__(self, handle, data_name='data', label_name='softmax_label', **kwargs):
-        label = self.getlabel()
+        batch = self.next()
-    def update(self, label, pred):
+    def update_single(self, label, pred):
-        train_exec  = symbol.simple_bind(ctx=ctx[i], grad_req='write', *data_shapes)
+        data_shapes = {k: tuple([slices[i].stop-slices[i].start] + list(v[1:]))
-    param_idx = [i for i in range(arg_names) if arg_names[i] in param_names]
+    param_idx = [i for i in range(len(arg_names)) if arg_names[i] in param_names]
-    out_cpu_array = nd.zeros(merged_shape, cpu())
+    #data_index, label_index = _check_arguments(symbol)
-    output_shapes = [tuple([batch_size]+x.shape[1:]) for x in train_execs[0].outputs]
+    batch_size = train_data.batch_size
-                    dev_output.copyto(cpu_out[islice])
+                    dev_out.copyto(cpu_out[islice])
-            for index, pair in enumerate(zip(arg_blocks, grad_blocks)):
+            #for index, pair in enumerate(zip(arg_blocks, grad_blocks)):
-                        dev_output.copyto(cpu_out[islice])
+                        dev_out.copyto(cpu_out[islice])
-        input_names = [x[1] for x in input_shapes]
+        input_names = input_shapes.keys()
-        if not isinstance(data, DataIter):
+        if not isinstance(data, io.DataIter):
-        if (not eval_data is None) and not isinstance(eval_data, DataIter):
+        if (not eval_data is None) and not isinstance(eval_data, io.DataIter):
-            batch_size = input_shape[0]
+            batch_size = data.batch_size
-def _split_input_slice(input_shape, num_split):
+def _split_input_slice(batch_size, num_split):
-    return (slices, shapes)
+    return slices
-def _train_multi_device(symbol, ctx, input_shape,
+def _train_multi_device(symbol, ctx, arg_names, param_names, aux_names,
-    aux_names = symbol.list_auxiliary_states()
+    slices = _split_input_slice(train_data.batch_size, num_device)
-        for index in range(len(train_execs[0].aux_arrays))]
+    data_names  = [x[0] for x in train_data.provide_data]
-                data[islice].copyto(target)
+        #for data, label in train_data:
-                texec.outputs[0].copyto(out_cpu_array[islice])
+                for cpu_out, dev_out in zip(cpu_output_arrays, texec.outputs):
-            eval_metric.update(label, out_cpu_array)
+            eval_metric.update(data_batch.label, cpu_output_arrays)
-                    data[islice].copyto(target)
+            eval_data.reset()
-            eval_data.reset()
+                    for cpu_out, dev_out in zip(cpu_output_arrays, texec.outputs):
-                    weight.copyto(aux_params[name])
+            for name, block in zip(param_names, param_arrays):
-    def _init_params(self, input_shape):
+    def _init_params(self, input_shapes, overwrite=False):
-    def fit(self, X, y=None, eval_data=None, eval_metric='acc',
+    def fit(self, data, eval_data=None, eval_metric='acc',
-    def __init__(self, handle):
+    def __init__(self, handle, data_name='data', label_name='softmax_label'):
-        self._debug_at_begin = True
+
-            return  self.getdata(), self.getlabel()
+            return  MXDataBatch(data=[self.getdata()],
-            return self.getdata(), self.getlabel()
+            return MXDataBatch(data=[self.getdata()],
-        return next_res.value
+    #def iter_next(self):
-        return MXDataIter(iter_handle)
+        return MXDataIter(iter_handle, **kwargs)
-    sample = np.asarray(resized_img) * 256
+    sample = np.asarray(resized_img) * 255
-    # swap axes to make image from (224, 224, 4) to (3, 224, 224)
+    # swap axes to make image from (224, 224, 3) to (3, 224, 224)
-    num_round     = 20,
+    num_epoch     = 20,
-    num_round     = 10,
+    num_epoch     = 10,
-    num_round     = 4,
+    num_epoch     = 4,
-    num_round     = 20,
+    num_epoch     = 20,
-    num_round     = 4,
+    num_epoch     = 4,
-    num_round     = 3,
+    num_epoch     = 3,
-    num_round     = 4,
+    num_epoch     = 4,
-        num_round     = 10,
+        num_epoch     = 10,
-        num_round     = 3,
+        num_epoch     = 3,
-        num_round     = 4,
+        num_epoch     = 4,
-              label_pad_value=0,
+        """Igore roll over data and set to start"""
-            return self.num_pad
+            return self.cursor + self.batch_size - self.num_data
-            return io.NDArrayIter(X, y, self.numpy_batch_size, shuffle=is_train)
+            if is_train:
-# pylint: disable=invalid-name, protected-access, fixme, too-many-arguments
+# pylint: disable=invalid-name, protected-access, fixme, too-many-arguments, W0221, W0201
-    def getdata(self):
+    def getdata(self, index=0):
-        pass
+        return self.getdata(-1)
-        NDArray for label
+    data_list or data, label: a list of, or two separate NDArray or numpy.ndarray
-    the size of data does not match batch_size.
+    This iterator will pad, discard or roll over the last batch if
-                 label_pad_value=0):
+    def __init__(self, *args, **kwargs):
-            label = label.asnumpy()
+        if isinstance(args[0], list) or isinstance(args[0], tuple):
-            idx = np.arange(data.shape[0])
+            idx = np.arange(data_list[0].shape[0])
-            label = new_label
+            for i in range(self.num_source):
-        self.current_batch = -1
+        if last_batch_handle == 'discard':
-        self.current_batch = -1
+        if self.last_batch_handle == 'roll_over' and self.cursor > self.num_data:
-            self.current_batch += 1
+        self.cursor += self.batch_size
-            return self.getdata(), self.getlabel()
+            return (self.getdata(i) for i in range(self.num_source))
-        return array(self.batch_data[self.current_batch])
+    def getdata(self, index=0):
-        return array(self.batch_label[self.current_batch])
+        return self.getdata(-1)
-        if self.current_batch == self.batch_num - 1:
+        if self.last_batch_handle == 'pad' and \
-    dataiter = mx.io.NDArrayIter(datas, labels, 128, True)
+    dataiter = mx.io.NDArrayIter(datas, labels, 128, True, last_batch_handle='pad')
-    dataiter.reset()
+    dataiter = mx.io.NDArrayIter(datas, labels, 128, False, last_batch_handle='pad')
-    blacklist += ['packet/sse-inl.h']
+    blacklist += ['packet/sse-inl.h', 'emmintrin.h']
-#if !defined(__ANDROID__) && (!defined(MSHADOW_USE_SSE) || MSHADOW_USE_SSE)
+#if !defined(__ANDROID__) && (!defined(MSHADOW_USE_SSE) || MSHADOW_USE_SSE == 1)
-    'malloc.h', 'mkl.h', 'mkl_cblas.h', 'mkl_vsl.h', 'mkl_vsl_functions.h', 'nvml.h', 'opencv2/opencv.hpp', 'sys/stat.h', 'sys/types.h', 'emmintrin.h'
+    'Windows.h', 'cublas_v2.h', 'cuda/tensor_gpu-inl.cuh',
-    print("Usage: <source.d> <source.cc> <output>")
+if len(sys.argv) < 4:
-#if !defined(__ANDROID__)
+#if !defined(__ANDROID__) && (!defined(MSHADOW_USE_SSE) || MSHADOW_USE_SSE)
-    if x not in history: print 'Not processed:', x
+    if x not in history and not x.endswith('.o'):
-
+import os.path, re, StringIO
-softmax = mx.symbol.Softmax(data=fc, name="loss")
+softmax = mx.symbol.SoftmaxOutput(data=fc, name="loss")
-softmax = mx.symbol.Softmax(data=fc3)
+softmax = mx.symbol.SoftmaxOutput(data=fc3)
-    softmax = mx.symbol.Softmax(data=fc1, name='softmax')
+    softmax = mx.symbol.SoftmaxOutput(data=fc1, name='softmax')
-    softmax = mx.symbol.Softmax(data=fc1, name='softmax')
+    softmax = mx.symbol.SoftmaxOutput(data=fc1, name='softmax')
-    softmax = mx.symbol.Softmax(data=fc1, name='softmax')
+    softmax = mx.symbol.SoftmaxOutput(data=fc1, name='softmax')
-lenet = mx.symbol.Softmax(data=fc2)
+lenet = mx.symbol.SoftmaxOutput(data=fc2)
-mlp = mx.symbol.Softmax(data = fc3, name = 'mlp')
+mlp = mx.symbol.SoftmaxOutput(data = fc3, name = 'mlp')
-mlp = mx.symbol.Softmax(data = fc3, name = 'mlp')
+mlp = mx.symbol.SoftmaxOutput(data = fc3, name = 'mlp')
-out = mx.symbol.Softmax(data=net, name='softmax')
+out = mx.symbol.SoftmaxOutput(data=net, name='softmax')
-        sm = mx.sym.Softmax(data=fc, label=label, name="t%d_sm" % seqidx)
+        sm = mx.sym.SoftmaxOutput(data=fc, label=label, name="t%d_sm" % seqidx)
-    softmax = mx.symbol.Softmax(data = fc2, name = 'sm')
+    softmax = mx.symbol.SoftmaxOutput(data = fc2, name = 'sm')
-    softmax = mx.symbol.Softmax(fc3, name = 'sm')
+    softmax = mx.symbol.SoftmaxOutput(fc3, name = 'sm')
-    lenet = mx.symbol.Softmax(data=fc2)
+    lenet = mx.symbol.SoftmaxOutput(data=fc2)
-    softmax = mx.symbol.Softmax(data=fc, name="loss")
+    softmax = mx.symbol.SoftmaxOutput(data=fc, name="loss")
-    softmax = mx.symbol.Softmax(data=fc1, name='softmax')
+    softmax = mx.symbol.SotfmaxOutput(data=fc1, name='softmax')
-softmax = mx.symbol.Softmax(data = fc2, name = 'sm')
+softmax = mx.symbol.SoftmaxOutput(data = fc2, name = 'sm')
-softmax = mx.symbol.Softmax(fc3, name = 'sm')
+softmax = mx.symbol.SoftmaxOutput(fc3, name = 'sm')
-            param = layer[i].convolution_param 
+            param = layer[i].convolution_param
-            param = layer[i].lrn_param  
+            param = layer[i].lrn_param
-            type_string = 'mx.symbol.Softmax'
+            type_string = 'mx.symbol.SoftmaxOutput'
-        
+
-    main()
+    main()
-                    mx.nd.choose_element(arr, mx.nd.array(indices)).asnumpy())
+                    mx.nd.choose_element_0index(arr, mx.nd.array(indices)).asnumpy())
-            seq_label_probs = [mx.nd.choose_element(out, label).copyto(mx.cpu())
+            seq_label_probs = [mx.nd.choose_element_0index(out, label).copyto(mx.cpu())
-            seq_label_probs = [mx.nd.choose_element(out, label).copyto(mx.cpu())
+            seq_label_probs = [mx.nd.choose_element_0index(out, label).copyto(mx.cpu())
-def check_slice_channel(dim):
+def check_slice_channel(dim, num):
-        e = np.hstack((a, b, c, d))
+        shape = (2,2)
-    op = mx.sym.SliceChannel(data=data, num_outputs=4)
+    op = mx.sym.SliceChannel(data=data, num_outputs=num)
-    o4_nd = exe.outputs[3]
+    assert len(exe.outputs) == num
-    assert reldiff(o4_nd.asnumpy(), d) < 1e-5
+    for i in range(num):
-    assert reldiff(grad_nd[0].asnumpy(), np.hstack((a+4,b+3, c+2, d+1))) < 1e-5
+    for i in range(num):
-    check_slice_channel(4)
+    check_slice_channel(2, 4)
-        """Create a symbol from numpy operator. 
+        """Create a symbol from numpy operator.
-                                  list_functype(list_arguments_entry))
+                                 fb_functype(backward_entry),
-        return symbol.Symbol._Native(*args, info=cb_ptr, need_top_grad=self.need_top_grad(), **kwargs)
+        return symbol.Symbol._Native(*args,
-        return True
+        return True
-        self.get_symbol(*args, **kwargs)
+        return self.get_symbol(*args, **kwargs)
-        return in_shape, in_shape
+        return in_shape, [in_shape[0]]
-        return ['y']
+        return ['output']
-        return ['x']
+        return ['data']
-        """Create a symbol from numpy operator.
+        """Create a symbol from numpy operator. 
-"""Python interface for operators."""
+"""numpy interface for operators."""
-    of computation in symbolic graph to be writen in python. This feature
+class NumpyOp(object):
-    def __init__(self, need_top_grad=True):
+    def __init__(self):
-        """Create a symbol from python operator.
+    def get_symbol(self, *args, **kwargs):
-            """Structure that holds Callback information. Passed to PythonOpProp"""
+        class NumpyOpInfo(Structure):
-            """C Callback for PythonOp::Forward"""
+            """C Callback for NumpyOp::Forward"""
-            """C Callback for PythonOp::Backward"""
+            """C Callback for NumpyOp::Backward"""
-            """C Callback for PythonOpProp::InferShape"""
+            """C Callback for NumpyOpProp::InferShape"""
-            """C Callback for PythonOpProp::ListOutputs"""
+            """C Callback for NumpyOpProp::ListOutputs"""
-            """C Callback for PythonOpProp::ListArguments"""
+            """C Callback for NumpyOpProp::ListArguments"""
-        self.info_ = PythonOpInfo(fb_functype(forward_entry),
+        self.info_ = NumpyOpInfo(fb_functype(forward_entry),
-        return symbol.Python(*args, info=cb_ptr, need_top_grad=self.need_top_grad_)
+        return symbol.Symbol._Native(*args, info=cb_ptr, need_top_grad=self.need_top_grad(), **kwargs)
-    s = op.get_symbol([X])
+    op = mx.operator.NumpyOp()
-    exec1.backward()
+    exec1.backward(dy)
-
+def ctypes2array(ctype, ptr, length):
-            arg_params=arg_params, aux_params={}, num_round=1,
+            arg_params=arg_params, aux_params={}, num_epoch=1,
-    main()
+    main()
-"""Callback functions that can be used to track various status during iteration."""
+"""Callback functions that can be used to track various status during epoch."""
-    """Callback to checkpoint the model to prefix every iteration.
+    """Callback to checkpoint the model to prefix every epoch.
-                         param.iteration, param.nbatch, name, value)
+                         param.epoch, param.nbatch, name, value)
-                             param.iteration, count, speed)
+                             param.epoch, count, speed)
-"""Callback functions that can be used to track various status during iteration."""
+"""Callback functions that can be used to track various status during epoch."""
-    """Callback to checkpoint the model to prefix every iteration.
+    """Callback to checkpoint the model to prefix every epoch.
-                         param.iteration, param.nbatch, name, value)
+                         param.epoch, param.nbatch, name, value)
-                             param.iteration, count, speed)
+                             param.epoch, count, speed)
-num_round = 10
+num_epoch = 10
-    model = mx.model.FeedForward(ctx=gpus, symbol=softmax, num_round = num_round,
+    model = mx.model.FeedForward(ctx=gpus, symbol=softmax, num_epoch=num_epoch,
-              epoch_end_callback=mx.callback.Speedometer(batch_size))
+              batch_end_callback=mx.callback.Speedometer(batch_size))
-    ctx = dev, symbol = lenet, num_round = 20,
+    ctx = dev, symbol = lenet, num_epoch = 20,
-          epoch_end_callback=mx.callback.Speedometer(100))
+          batch_end_callback=mx.callback.Speedometer(100))
-    ctx = mx.cpu(), symbol = mlp, num_round = 20,
+    ctx = mx.cpu(), symbol = mlp, num_epoch = 20,
-    ctx = mx.cpu(), symbol = mlp, num_round = 20,
+    ctx = mx.cpu(), symbol = mlp, num_epoch = 20,
-from .base import DataIterHandle, NDArrayHandle, RecordIOHandle
+from .base import DataIterHandle, NDArrayHandle
-        return buf.contents.raw
+        return buf.contents.raw
-
+# coding: utf-8
-from .base import DataIterHandle, NDArrayHandle
+from .base import DataIterHandle, NDArrayHandle, RecordIOHandle
-    symbol_file : str
+    symbol_json_str : str
-        Path to the parameter file.
+    param_raw_bytes : str, bytes
-    def __init__(self, symbol_file, param_file, input_shapes,
+    def __init__(self, symbol_file,
-            c_str(symbol_file), c_str(param_file),
+            c_str(symbol_file),
-def load_ndarray_file(nd_file):
+def load_ndarray_file(nd_bytes):
-        The name to the ndarray file.
+    nd_bytes : str or bytes
-        c_str(nd_file), ctypes.byref(handle), ctypes.byref(olen)))
+        ptr, len(nd_bytes),
-sys.path.append("../../python/")
+sys.path.append("../../../predict/python/")
-mean_img = load_ndarray_file("Inception/mean_224.nd")["mean_img"]
+predictor = Predictor(open(symbol_file).read(),
-          epoch_end_callback = mx.callback.Speedometer(batch_size=batch_size))
+          batch_end_callback = mx.callback.Speedometer(batch_size=batch_size))
-	  iter_end_callback=mx.callback.do_checkpoint(model_prefix))
+          batch_end_callback=[mx.callback.Speedometer(batch_size), mx.callback.log_train_metric(100)],
-          epoch_end_callback=mx.callback.Speedometer(batch_size))
+          batch_end_callback=mx.callback.Speedometer(batch_size))
-                           ['iteration',
+# Parameter to pass to batch_end_callback
-                        begin_round, end_round, optimizer,
+                        begin_epoch, end_epoch, optimizer,
-                        iter_end_callback=None, epoch_end_callback=None,
+                        epoch_end_callback=None, batch_end_callback=None,
-        The begining training iteration.
+    begin_epoch : int
-        The end training iteration.
+    end_epoch : int
-        This can be used to checkpoint model each iteration.
+    epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
-    epoch_end_callback : callable(EpochEndParams)
+    batch_end_callback : callable(BatchEndParams)
-    optimizer.begin_round(begin_round)
+    optimizer.begin_epoch(begin_epoch)
-    for iteration in range(begin_round, end_round):
+    for epoch in range(begin_epoch, end_epoch):
-                epoch_end_params = EpochEndParam(iteration=iteration,
+            # batch callback (for print purpose)
-                        call(epoch_end_params)
+                if isinstance(batch_end_callback, list):
-                    epoch_end_callback(epoch_end_params)
+                    batch_end_callback(batch_end_params)
-        # reset training data after iteration finish
+        # reset training data after epoch finish
-        logger.info('Iteration[%d] Train-%s=%f', iteration, name, value)
+        logger.info('Epoch[%d] Train-%s=%f', epoch, name, value)
-        logger.info('Iteration[%d] Time cost=%.3f', iteration, (toc - tic))
+        logger.info('Epoch[%d] Time cost=%.3f', epoch, (toc - tic))
-            logger.info('Iteration[%d] Validation-%s=%f', iteration, name, value)
+            logger.info('Epoch[%d] Validation-%s=%f', epoch, name, value)
-        if iter_end_callback or iteration + 1 == end_round:
+        if epoch_end_callback or epoch + 1 == end_epoch:
-                    call(iteration, symbol, arg_params, aux_params)
+        if epoch_end_callback != None:
-    # end of all iterations
+                epoch_end_callback(epoch, symbol, arg_params, aux_params)
-def save_checkpoint(prefix, iteration, symbol, arg_params, aux_params):
+def save_checkpoint(prefix, epoch, symbol, arg_params, aux_params):
-        The iteration number of the model.
+    epoch : int
-    - ``prefix-iteration.params`` will be saved for parameters.
+    - ``prefix-epoch.params`` will be saved for parameters.
-    param_name = '%s-%04d.params' % (prefix, iteration)
+    param_name = '%s-%04d.params' % (prefix, epoch)
-def load_checkpoint(prefix, iteration):
+def load_checkpoint(prefix, epoch):
-        Iteration number of model we would like to load.
+    epoch : int
-    - ``prefix-iteration.params`` will be saved for parameters.
+    - ``prefix-epoch.params`` will be saved for parameters.
-    save_dict = nd.load('%s-%04d.params' % (prefix, iteration))
+    save_dict = nd.load('%s-%04d.params' % (prefix, epoch))
-        Training parameter, number of training rounds(iterations).
+    num_epoch : int, optional
-        The begining training iteration.
+    begin_epoch : int,optional
-                 num_round=None, optimizer='sgd',
+                 num_epoch=None, optimizer='sgd',
-                 begin_round=0,
+                 begin_epoch=0,
-        self.num_round = num_round
+        self.num_epoch = num_epoch
-        self.begin_round = begin_round
+        self.begin_epoch = begin_epoch
-            iter_end_callback=None, epoch_end_callback=None,
+            epoch_end_callback=None, batch_end_callback=None,
-            This can be used to checkpoint model each iteration.
+        epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
-        epoch_end_callback: callable(iteration)
+        batch_end_callback: callable(epoch)
-        # in first training round.
+        # in first training epoch.
-                            begin_round=self.begin_round, end_round=self.num_round,
+                            begin_epoch=self.begin_epoch, end_epoch=self.num_epoch,
-                            iter_end_callback=iter_end_callback,
+                            batch_end_callback=batch_end_callback,
-    def save(self, prefix, iteration=None):
+    def save(self, prefix, epoch=None):
-        - ``prefix-iteration.params`` will be saved for parameters.
+        - ``prefix-epoch.params`` will be saved for parameters.
-        save_checkpoint(prefix, iteration, self.symbol, self.arg_params, self.aux_params)
+        if epoch is None:
-    def load(prefix, iteration, ctx=None, **kwargs):
+    def load(prefix, epoch, ctx=None, **kwargs):
-            Iteration number of model we would like to load.
+        epoch : int
-            other parameters for model, including num_round, optimizer and numpy_batch_size
+            other parameters for model, including num_epoch, optimizer and numpy_batch_size
-        - ``prefix-iteration.params`` will be saved for parameters.
+        - ``prefix-epoch.params`` will be saved for parameters.
-        symbol, arg_params, aux_params = load_checkpoint(prefix, iteration)
+        symbol, arg_params, aux_params = load_checkpoint(prefix, epoch)
-                           begin_round=iteration,
+                           begin_epoch=epoch,
-               eval_data=None, eval_metric='acc', iter_end_callback=None,
+               num_epoch=None, optimizer='sgd', initializer=Uniform(0.01),
-            Training parameter, number of training rounds(iterations).
+        num_epoch : int, optional
-            This can be used to checkpoint model each iteration.
+        epoch_end_callback : callable(epoch, symbol, arg_params, aux_states)
-        model = FeedForward(symbol, ctx=ctx, num_round=num_round,
+        model = FeedForward(symbol, ctx=ctx, num_epoch=num_epoch,
-                  iter_end_callback=iter_end_callback,
+                  epoch_end_callback=epoch_end_callback,
-        self.iteration = 0
+        self.epoch = 0
-        """Function called to notify beginning of iteration.
+    def begin_epoch(self, epoch):
-            The iteration number.
+        epoch : int
-        self.iteration = iteration
+        self.epoch = epoch
-            lr = self.lr_scheduler(self.iteration)
+            lr = self.lr_scheduler(self.epoch)
-num_round = 1
+num_epoch = 1
-                             num_round=num_round,
+                             num_epoch=num_epoch,
-num_round = 4
+num_epoch = 4
-        iter_end_callback=mx.callback.do_checkpoint(prefix),
+        epoch_end_callback=mx.callback.do_checkpoint(prefix),
-        num_round=num_round,
+        num_epoch=num_epoch,
-    model3 = mx.model.FeedForward.load(prefix, num_round)
+    model3 = mx.model.FeedForward.load(prefix, num_epoch)
-    for i in range(num_round):
+    for i in range(num_epoch):
-    assert reldiff(out1.asnumpy(), out2) < 1e-6
+    if isinstance(out1, mx.nd.NDArray):
-
+            check_with_uniform(lambda x: mx.nd.norm(x).asscalar(), 1, dim, np.linalg.norm)
-                                 self.wd * weight)
+            mom[:] += -lr * (grad + self.wd * weight)
-            weight[:] += -lr * (grad * self.rescale_grad + self.wd * weight)
+            weight[:] += -lr * (grad + self.wd * weight)
-                raise ValueError("Data must be 2D")
+import sys, os
-# assume each worker has two gpus
+def test_init():
-            self.num_pad = batch_size - data.shape[0]
+        self.num_pad = batch_size - data.shape[0] % batch_size
-    def Register(klass):
+    def register(klass):
-    def CreateOptimizer(name, rescale_grad=1, **kwargs):
+    def create_optimizer(name, rescale_grad=1, **kwargs):
-register = Optimizer.Register
+register = Optimizer.register
-create = Optimizer.CreateOptimizer
+create = Optimizer.create_optimizer
-    layer = proto.layer
+    if len(proto.layer):
-        if layer[i].type == 'Convolution':
+        if layer[i].type == 'Convolution' or layer[i].type == 4:
-        if layer[i].type == 'Pooling':
+        if layer[i].type == 'Pooling' or layer[i].type == 17:
-        if layer[i].type == 'ReLU':
+        if layer[i].type == 'ReLU' or layer[i].type == 18:
-        if layer[i].type == 'LRN':
+            need_flatten[name] = need_flatten[mapping[layer[i].bottom[0]]]
-        if layer[i].type == 'InnerProduct':
+        if layer[i].type == 'InnerProduct' or layer[i].type == 14:
-        if layer[i].type == 'Dropout':
+        if layer[i].type == 'Dropout' or layer[i].type == 6:
-        if layer[i].type == 'Softmax':
+            need_flatten[name] = need_flatten[mapping[layer[i].bottom[0]]]
-        if layer[i].type == 'Flatten':
+        if layer[i].type == 'Flatten' or layer[i].type == 8:
-        if layer[i].type == 'Split':
+        if layer[i].type == 'Split' or layer[i].type == 22:
-        if layer[i].type == 'Concat':
+        if layer[i].type == 'Concat' or layer[i].type == 3:
-            return cls
+    opt_registry = {}
-
+@register
-        
+
-        raise ValueError('Cannot find optimizer %s' % name)
+#backward compatibility wrapper for Optimizer.CreateOptimizer
-        in_data = mx.sym.Dropout(data=in_data, p=dropout)
+        indata = mx.sym.Dropout(data=indata, p=dropout)
-class Test(object):
+class Test(Optimizer):
-
+        super(Test, self).__init__(rescale_grad)
-    def __init__(self):
+    def __init__(self, rescale_grad=1):
-        super(SGD, self).__init__()
+        super(SGD, self).__init__(rescale_grad)
-        data_shape  = (3,28,28),
+        data_shape  = input_shape,
-        num_round     = 4,
+        num_round     = 10,
-    layer = proto.layer
+    if len(proto.layer):
-        if layer[i].type == 'Convolution':
+        if layer[i].type == 'Convolution' or layer[i].type == 4:
-        if layer[i].type == 'Pooling':
+        if layer[i].type == 'Pooling' or layer[i].type == 17:
-        if layer[i].type == 'ReLU':
+        if layer[i].type == 'ReLU' or layer[i].type == 18:
-        if layer[i].type == 'LRN':
+            need_flatten[name] = need_flatten[mapping[layer[i].bottom[0]]]
-        if layer[i].type == 'InnerProduct':
+        if layer[i].type == 'InnerProduct' or layer[i].type == 14:
-        if layer[i].type == 'Dropout':
+        if layer[i].type == 'Dropout' or layer[i].type == 6:
-        if layer[i].type == 'Softmax':
+            need_flatten[name] = need_flatten[mapping[layer[i].bottom[0]]]
-        if layer[i].type == 'Flatten':
+        if layer[i].type == 'Flatten' or layer[i].type == 8:
-        if layer[i].type == 'Split':
+        if layer[i].type == 'Split' or layer[i].type == 22:
-        if layer[i].type == 'Concat':
+        if layer[i].type == 'Concat' or layer[i].type == 3:
-    def Register(cls):
+    def Register(klass):
-        name = cls.__name__.lower()
+        assert(isinstance(klass, type))
-                      cls.__module__, cls.__name__,
+                      klass.__module__, klass.__name__,
-        return cls
+        Optimizer.opt_registry[name] = klass
-        if cls_name in mcs.__optimizers__:
+
-        mcs.__optimizers__[cls_name] = cls
+                      Optimizer.opt_registry[name].__module__,
-            return Optimizer.__optimizers__[name.lower()](
+        if name.lower() in Optimizer.opt_registry:
-
+@register
-                                     **kwargs)
+#backward compatibility wrapper for Optimizer.CreateOptimizer
-        Name of required optimizer
+        Name of required optimizer. Should be the name
-        rescale_grad=rescale_grad, **kwargs)
+    return Optimizer.CreateOptimizer(name,
-class Optimizer(object):
+from six import with_metaclass
-            return cls
+
-        raise ValueError('Cannot find optimizer %s' % name)
+    return Optimizer.CreateOptimizer(name, 
-                print 'WARNING: New optimizer %s.%s is overriding ' \
+                print('WARNING: New optimizer %s.%s is overriding ' \
-                          mcs.__optimizers__[cls_name].__name__)
+                          mcs.__optimizers__[cls_name].__name__))
-    if name in Optimizer.__optimizers__:
+    if name.lower() in Optimizer.__optimizers__:
-            #Allow overriding of existing optimizer.
+        def __new__(mcs, name, bases, attrs):
-            meta.__optimizers__[cls.__name__.lower()] = cls
+            cls_name = cls.__name__.lower()
-                        **kwargs)
+            rescale_grad=rescale_grad,
-            meta.__optimizers__[cls.__name__] = cls
+            meta.__optimizers__[cls.__name__.lower()] = cls
-        
+
-        return Optimizer.__optimizers__[name](rescale_grad=rescale_grad, **kwargs)
+        return Optimizer.__optimizers__[name.lower()](
-class Test(object):
+class Test(Optimizer):
-
+        super(Test, self).__init__(rescale_grad)
-    def __init__(self):
+    def __init__(self, rescale_grad=1):
-        super(SGD, self).__init__()
+        super(SGD, self).__init__(rescale_grad)
-        in_data = mx.sym.Dropout(data=in_data, p=dropout)
+        indata = mx.sym.Dropout(data=indata, p=dropout)
-    print(rnn_sym.list_outputs())
+                          dropout=dropout)
-            print("ignore %s " % name)
+            pass
-def set_rnn_inputs(m, X, onehot, begin):
+def set_rnn_inputs(m, X, begin):
-    batch_size, vocab = onehot.shape
+    batch_size, vocab = m.seq_data[0].shape
-        m.seq_data[seqidx][:] = onehot
+        mx.nd.onehot_encode(mx.nd.array(x, ctx=m.seq_data[seqidx].context),
-def calc_nll(seq_out, X, begin):
+def calc_nll(seq_label_probs, X, begin):
-    for seqidx in range(len(seq_out)):
+    for seqidx in range(len(seq_label_probs)):
-        nll += logloss(y, seq_out[seqidx].asnumpy())
+        py = seq_label_probs[seqidx].asnumpy()
-            set_rnn_inputs(m, X_train_batch, onehot, begin=begin)
+            set_rnn_inputs(m, X_train_batch, begin=begin)
-            seq_outs = [out.copyto(mx.cpu()) for out in m.seq_outputs]
+            # probability of each label class, used to evaluate nll
-            train_nll += calc_nll(seq_outs, X_train_batch, begin=begin)
+            train_nll += calc_nll(seq_label_probs, X_train_batch, begin=begin)
-            set_rnn_inputs(m, X_val_batch, onehot, begin=begin)
+            set_rnn_inputs(m, X_val_batch, begin=begin)
-            seq_outs = [out.copyto(mx.cpu()) for out in m.seq_outputs]
+            # probability of each label class, used to evaluate nll
-            val_nll += calc_nll(seq_outs, X_val_batch, begin=begin)
+            val_nll += calc_nll(seq_label_probs, X_val_batch, begin=begin)
-https://github.com/wojzaremba/lstm/tree/master/data
+https://github.com/dmlc/web-data/tree/master/mxnet/ptb
-    return X[0 : shape[0]/seq_len *seq_len, :]
+    nstep = int(shape[0] / seq_len)
-        data_shape  = (3,28,28),
+        data_shape  = input_shape,
-        num_round     = 4,
+        num_round     = 10,
-def check_with_uniform(uf, arg_shapes, dim=None):
+def check_with_uniform(uf, arg_shapes, dim=None, npuf=None, rmin=-10):
-        npy = np.random.uniform(-10, 10, s)
+        npy = np.random.uniform(rmin, 10, s)
-    out2 = uf(*numpy_arg)
+    if npuf is None:
-        dll_path = [os.path.join(p, 'mxnet.dll') for p in dll_path]
+        dll_path = [os.path.join(p, 'libmxnet.dll') for p in dll_path]
-        ],
+          'numpy',
-      ],
+        'numpy',
-source_suffix = ['.rst', '.md']
+source_suffix = ['.rst', '.md', '.Rmd']
-                    'git clone https://github.com/dmlc/doc-image', shell = True)
+if not os.path.exists('web-data'):
-    subprocess.call('cd doc-image; git pull', shell=True)
+    subprocess.call('cd web-data; git pull', shell=True)
-if READTHEDOCS_BUILD or not os.path.exists('../recommonmark'):
+if not os.path.exists('../recommonmark'):
-            Training data
+        X : DataIter, or numpy.ndarray/NDArray
-            If X is numpy.ndarray y is required to set
+        y : numpy.ndarray/NDArray, optional
-            If eval_set is numpy.ndarray pair, it should be (valid_data, valid_label)
+        eval_data : DataIter or numpy.ndarray/list/NDArray pair
-                           begin_round=iteration
+                           begin_round=iteration,
-                            begin_round=0, end_round=self.num_round,
+                            begin_round=self.begin_round, end_round=self.num_round,
-    def updater_handle(key, lhs_handle, rhs_handle):
+    def updater_handle(key, lhs_handle, rhs_handle, _):
-            None, ctypes.c_int, NDArrayHandle, NDArrayHandle)
+            None, ctypes.c_int, NDArrayHandle, NDArrayHandle, ctypes.c_void_p)
-        check_call(_LIB.MXKVStoreSetUpdater(self.handle, self._updater_func))
+        check_call(_LIB.MXKVStoreSetUpdater(self.handle, self._updater_func, None))
-    num_round     = 1,
+    num_round     = 4,
-from common import cifar10, accuracy
+import common
-    (train, val) = cifar10(batch_size = 128, input_shape=(3,28,28))
+    (train, val) = common.cifar10(batch_size = 128, input_shape=(3,28,28))
-        symbol        = softmax,
+        symbol        = common.inception(),
-        num_round     = 1,
+        num_round     = 4,
-    return accuracy(model, val)
+    return common.accuracy(model, val)
-    # base = test_inception(mx.gpu(), 'none')
+    base = test_inception(mx.gpu(), 'none')
-    # acc3 =  test_inception(gpus, 'local_allreduce_device')
+    acc2 =  test_inception(gpus, 'local_allreduce_cpu')
-    # assert abs(base - acc3) < 1e-3
+    assert base > 0.95
-from common import mnist, accuracy, cifar10
+import common
-    (train, val) = mnist(batch_size = 100, input_shape=(1,28,28))
+    # (train, val) = common.cifar10(batch_size = 128, input_shape=(3,28,28))
-        symbol        = lenet,
+        symbol        = common.lenet(),
-    return accuracy(model, val)
+    return common.accuracy(model, val)
-            if kvstore and kvstore.type is 'dist_sync':
+            if kvstore and kvstore.type == 'dist_sync':
-
+
-    return (train, val)
+#!/usr/bin/env python
-# pylint: skip-file
+
-softmax = mx.symbol.Softmax(fc3, name = 'sm')
+import common
-                         input_shape = (784,))
+    (train, val) = common.mnist(batch_size = 100,
-        symbol        = softmax,
+        symbol        = common.mlp(),
-        num_round     = 2,
+        num_round     = 4,
-    return accuracy(model, val)
+    return common.accuracy(model, val)
-    print base
+                        kvstore, update_on_kvstore,
-                        kvstore='local', logger=None):
+                        logger=None):
-       'dist_async' : multi-machines with partical asynchronous
+    kvstore : KVStore
-       In default uses 'local', often no need to change for single machiine.
+    update_on_kvstore : bool
-    if kv:
+    # init kvstore
-            kv.set_optimizer(optimizer)
+            kvstore.set_optimizer(optimizer)
-                kv.init(index, arg_list[0])
+                kvstore.init(index, arg_list[0])
-                    kv.pull(index, arg_list, priority=-index)
+                    kvstore.pull(index, arg_list, priority=-index)
-                if kv:
+                if kvstore:
-                    kv.push(index, grad_list, priority=-index)
+                    kvstore.push(index, grad_list, priority=-index)
-                        kv.pull(index, arg_list, priority=-index)
+                        kvstore.pull(index, arg_list, priority=-index)
-                        kv.pull(index, grad_list, priority=-index)
+                        kvstore.pull(index, grad_list, priority=-index)
-                            optimizer=self.optimizer,
+                            optimizer=optimizer,
-                            kvstore=kvstore,
+                            kvstore=kvstore, update_on_kvstore=update_on_kvstore,
-    # init optimizer before give it to kv or get_updater
+    # init optmizer
-            optimizer = opt.create(optimizer, rescale_grad=(1.0/batch_size), **(self.kwargs))
+
-                            optimizer=optimizer,
+                            optimizer=self.optimizer,
-        raise RuntimeError('Cannot find find the files.\n' +
+        raise RuntimeError('Cannot find the files.\n' +
-        if self.type == 'dist_async' and is_worker.value:
+        if 'dist' in self.type and is_worker.value:
-kv = mx.kv.create('dist')
+kv = mx.kv.create('dist_sync')
-nworker = kv.get_num_workers()
+my_rank = kv.rank
-    nrepeat = 2
+    nrepeat = 3
-    kv._wait([3, 99])
+    # print val.asnumpy()
-# TODO async test, slice,
+    print 'done'
-            for _, index, pair in grad_update_order:
+            for index, pair in enumerate(zip(arg_blocks, grad_blocks)):
-            for index, pair in enumerate(zip(arg_blocks, grad_blocks)):
+            for _, index, pair in grad_update_order:
-        self._barrier()
+        ckeys, cvals = _ctype_key_value(key, value)
-        can use _wait or _wait_all to make sure it is finished.
+        1. this function returns after adding an operator to the engine.
-        if is_distributed.value and is_worker.value:
+        if self.type == 'dist_async' and is_worker.value:
-        shuffle    = True,
+        shuffle    = False,
-        learning_rate = 0.02,
+        num_round     = 3,
-    assert base > 0.5
+    assert base > 0.95
-        shuffle    = False,
+        shuffle    = True,
-    time.sleep(1)
+#!/usr/bin/env python
-from common import mnist, accuracy
+from common import mnist, accuracy, cifar10
-        learning_rate = 0.05,
+        num_round     = 10,
-    print acc
+    acc1 = test_lenet(mx.gpu(), 'none')
-    (train, val) = mnist(batch_size = 100,
+    (train, val) = mnist(batch_size = 102,
-    assert abs(base - acc) < 1e-4
+    acc1 =  test_mlp(cpus, 'local_update_cpu')
-    # assert abs(base - acc) < 1e-4
+    assert abs(base - acc1) < 1e-3
-from data import mnist
+from common import mnist, accuracy
-        X             = train,
+        symbol        = softmax,
-    return acc
+    return accuracy(model, val)
-    # assert abs(base - acc) < 1e-4
+    cpus = [mx.cpu(i) for i in range(2)]
-    test_mlp()
+# pylint: skip-file
-            for _, index, pair in grad_update_order:
+            for index, pair in enumerate(zip(arg_blocks, grad_blocks)):
-            for index, pair in enumerate(zip(arg_blocks, grad_blocks)):
+            for _, index, pair in grad_update_order:
-        shuffle    = True,
+        shuffle    = False,
-        learning_rate = 0.02,
+        num_round     = 3,
-    assert base > 0.5
+    assert base > 0.95
-        shuffle    = False,
+        shuffle    = True,
-    time.sleep(1)
+#!/usr/bin/env python
-from common import mnist, accuracy
+from common import mnist, accuracy, cifar10
-        learning_rate = 0.05,
+        num_round     = 10,
-    print acc
+    acc1 = test_lenet(mx.gpu(), 'none')
-    (train, val) = mnist(batch_size = 100,
+    (train, val) = mnist(batch_size = 102,
-    assert abs(base - acc) < 1e-4
+    acc1 =  test_mlp(cpus, 'local_update_cpu')
-    # assert abs(base - acc) < 1e-4
+    assert abs(base - acc1) < 1e-3
-from data import mnist
+from common import mnist, accuracy
-        X             = train,
+        symbol        = softmax,
-    return acc
+    return accuracy(model, val)
-    # assert abs(base - acc) < 1e-4
+    cpus = [mx.cpu(i) for i in range(2)]
-    test_mlp()
+# pylint: skip-file
-from .base import check_call, c_array, c_str, string_types, mx_uint
+from .base import check_call, c_array, c_str, string_types, mx_uint, py_str
-        Only worker 0's (get_rank() == 0) data are used.
+        Only worker 0's (rank == 0) data are used.
-        if (self.get_rank() == 0):
+        if (self.rank == 0):
-    def get_rank(self):
+    @property
-    def get_num_workers(self):
+    @property
-
+
-                    self.kvstore.get_rank(), cmd_id, cmd_body))
+                    self.kvstore.rank, cmd_id, cmd_body))
-        update_on_kvstore = False
+    (kv, update_on_kvstore) = _create_kvstore(kvstore, num_device, arg_params)
-    assert kv.get_type() == kvtype
+    assert kv.type == kvtype
-    devstr2type = {'cpu': 1, 'gpu': 2, 'cpu': 3}
+    devtype2str = {1: 'cpu', 2: 'gpu', 3: 'cpu_pinned'}
-    devstr2type = {'cpu': 1, 'gpu': 2}
+    devtype2str = {1: 'cpu', 2: 'gpu', 3: 'cpu'}
-        update_on_kvstore = True
+    update_on_kvstore = True
-                        updater(index, g, w)
+                    for k, p in enumerate(zip(arg_list, grad_list)):
-    if isinstance(kvstore, KVStore):
+    if isinstance(kvstore, kvs.KVStore):
-               logger=None, **kwargs):
+               kvstore='local', logger=None, **kwargs):
-        update_on_kvstore=True)
+        momentum=0.9)
-from . import kvstore
+from . import kvstore as kvs
-                        kv=None, logger=None):
+                        kvstore='local', logger=None):
-        Whether to perform parameter update on kvstore instead of training device.
+    kvstore: KVStore or str, optional
-        An instance of kvstore. It overwrite both kvstore_type and update_on_kvstore
+       In default uses 'local', often no need to change for single machiine.
-            logging.info('Auto-select update_on_kvstore=%s', str(update_on_kvstore))
+    # create kvstore
-        kv = None
+        raise TypeError('kvstore must be either KVStore or str')
-            kvstore=None, logger=None):
+            kvstore='local', logger=None):
-            By default, the trainer will automatically decide the policy.
+        kvstore: KVStore or str, optional
-            An instance of kvstore. It overwrite both kvstore_type and update_on_kvstore
+           In default uses 'local', often no need to change for single machiine.
-                            kv=kvstore,
+                            kvstore=kvstore,
-            Type of kvstore used for synchronization, usually no need to set.
+        kvstore: KVStore or str, optional
-        logger : logging logger, optional
+           In default uses 'local', often no need to change for single machiine.
-                  kvstore_type=kvstore_type, kvstore=kvstore,
+                  kvstore=kvstore,
-            for index, pair in enumerate(zip(arg_blocks, grad_blocks)):
+            for _, index, pair in grad_update_order:
-                        logger=None):
+                        kv=None, logger=None):
-    if kvstore_type == 'dist':
+    if kv is not None:
-            logger=None):
+            kvstore=None, logger=None):
-               update_on_kvstore=None, kvstore_type='local',
+               update_on_kvstore=None, kvstore_type='local', kvstore=None,
-                  kvstore_type=kvstore_type,
+                  kvstore_type=kvstore_type, kvstore=kvstore,
-            self._set_updater(opt.optimizer_clossure(optimizer))
+            self._set_updater(opt.get_updater(optimizer))
-    kvstore_type : {'local', 'device'}, optional
+    kvstore_type : {'local', 'device', 'dist'}, optional
-    else:
+    if kvstore_type == 'dist':
-            opt_state_blocks.append(None)
+    # init optimizer before give it to kv or get_updater
-        optimizer.update(index, weight, grad, opt_state_blocks[index])
+    if kv:
-        kv._set_updater(kv_updater)
+        # init kv
-                        optimizer.update(index, w, g, state)
+                    for w, g in zip(arg_list, grad_list):
-def optimizer_clossure(optimizer):
+def get_updater(optimizer):
-               'name : string, required.\n' +
+               'name : string, optional.\n' +
-    #test_MNISTIter()
+    test_NDArrayIter()
-                    self.kvstore.get_rank(), cmd_id, cmd_body)
+                print ("server %d, unknown command (%d, %s)" % (
-        kv.set_updater(kv_updater)
+        kv._set_updater(kv_updater)
-            self.handle, mx_uint(len(ckeys)), ckeys, cvals))
+        if (self.get_rank() == 0):
-        can use Wait or WaitAll to make sure it is finished.
+        Data consistency:
-        can use Wait or WaitAll to make sure it is finished.
+        Data consistency:
-    def set_updater(self, updater):
+    def set_optimizer(self, optimizer):
-    def barrier(self):
+    def _barrier(self):
-    def wait(self, key):
+    def _wait(self, key):
-    def wait_all(self):
+    def _wait_all(self):
-    def send_command_to_servers(self, head, body):
+    def _send_command_to_servers(self, head, body):
-
+import pickle
-    def controller(self):
+    def _controller(self):
-        def server_controller(head, body):
+        def server_controller(cmd_id, cmd_body):
-                self.kvstore.set_optimizer(body)
+            if cmd_id == 0:
-                    self.kvstore.get_rank(), head, body)
+                    self.kvstore.get_rank(), cmd_id, cmd_body)
-        >>>     else if is_key_value x: updater(x)
+        ...     if is_command x: controller(x)
-        check_call(_LIB.MXKVStoreRunServer(self.handle, _ctrl_proto(self.controller())))
+        check_call(_LIB.MXKVStoreRunServer(self.handle, _ctrl_proto(self._controller())))
-    kv.set_optimizer(opt)
+kv = mx.kv.create('dist')
-# print 'init worker %d' % my_rank
+my_rank = kv.get_rank()
-    kv.wait([3, 99])
+    kv._wait([3, 99])
-    kv.set_updater(updater)
+    kv._set_updater(updater)
-from .ndarray import NDArray, zeros
+from .ndarray import NDArray
-from .base import NDArrayHandle, KVStoreHandle
+from .base import _LIB, check_call
-class Test(Optimizer):
+class Test(object):
-
+    kv.init(99, mx.nd.ones(big_shape))
-        kv.barrier()
+        kv.push(99, mx.nd.ones(big_shape)*(my_rank+1))
-    kv.pull(3, out = val)
+    kv.wait([3, 99])
-    # print val.asnumpy()
+    val = mx.nd.zeros(shape)
-# kv.send_command_to_servers(1, 'hhh')
+    val2 = mx.nd.zeros(big_shape)
-                    print 'Swapping BGR of caffe into RGB in cxxnet'
+                    print 'Swapping BGR of caffe into RGB in mxnet'
-    need_flatten = {'data' : False}
+
-    return symbol_string
+    return symbol_string, output_name
-    sym = proto2script(proto_file)
+    sym, output_name = proto2script(proto_file)
-    return prob
+    exec("ret = " + output_name)
-        updater: function
+        updater : function
-        rank :int
+        rank : int
-        """Global barrier among all worker nodes"""
+        """Global barrier among all worker nodes
-        - dist: distributed KVStore supporting multiple machines
+        - local works for multiple devices on a single machine (single process)
-    """A key-value store server"""
+    """The key-value store server"""
-        """Initialize a new KVStore.
+        """Initialize a new KVStoreServer.
-        """return the controller"""
+        """return the server controller"""
-model.save(args.save_model_name)
+from convert_symbol import proto2symbol
-			mapping[layer[i].top[j]] = name
+def proto2script(proto_file):
-    main()
+def proto2symbol(proto_file):
-from . import kvstore_server
+# use mx.kv as short for kvstore
-from .ndarray import NDArray
+from .ndarray import NDArray, zeros
-        optimizer.update(index, grad, weight, states[index])
+        optimizer.update(index, weight, grad, states[index])
-    nrepeat = 3
+    nrepeat = 2
-    num = (nworker + 1 ) * nworker / 2 * nrepeat + 1
+    num = (nworker + 1 ) * nworker * rate / 2 * nrepeat + 1
-kv.send_command_to_servers(1, 'hhh')
+# kv.send_command_to_servers(1, 'hhh')
-            check_call(_LIB.MXKVStoreSetUpdater(self.handle, self._updater_func))
+        _updater_proto = ctypes.CFUNCTYPE(
-                # self.kvstore.set_updater(updater)
+                self.kvstore.set_optimizer(body)
-    opt : optimizer
+    opt : Optimizer
-    kv.set_updater(updater)
+    opt = mx.optimizer.create('test', rate)
-        check_call(_LIB.MXKVStoreSetUpdater(self.handle, self._updater_func))
+        is_distributed = ctypes.c_int()
-
+            if head == 0:
-
+    def send_command_to_servers(self, head, body):
-_init_kvstore_module()
+import sys
-    def __init__(self, handle):
+    def __init__(self, kvstore):
-            KVStore handle of C API
+        kvstore : KVStore
-        check_call(_LIB.MXKVStoreFree(self.handle))
+        self.kvstore = kvstore
-            yield
+            print self.kvstore.get_rank(), head, body
-#
+# run on local machine
-keys = [3]
+keys = [3, 4, 5]
-
+# print 'init worker %d' % my_rank
-val = mx.nd.zeros(shape)
+def test_sync_push_pull():
-kv.barrier()
+    kv.pull(3, out = val)
-kv.pull(3, out = val)
+kv.send_command_to_servers(1, 'hhh')
-check_diff_to_scalar(val, num)
+if __name__ == "__main__":
-    assert(np.sum(np.abs((A - x).asnumpy())) == 0)
+    assert(np.sum(np.abs((A - x).asnumpy())) == 0), A.asnumpy()
-world = kv.get_group_size()
+nworker = kv.get_group_size()
-keys = [3,4,5]
+keys = [3]
-kv.push(3, mx.nd.ones(shape)*(my_rank+1))
+nrepeat = 3
-print val.asnumpy()
+
-keys = [3, 5, 7]
+keys = [3,4,5]
-print val.asnumpy()
+# # kv.push(keys, [mx.nd.ones(shape)*(my_rank+2)] * len(keys))
-print val.asnumpy()
+# kv.push(keys, [mx.nd.ones(shape)*(my_rank+3)] * len(keys))
-print val.asnumpy()
+# kv.push(keys, [mx.nd.ones(shape)*(my_rank+4)] * len(keys))
-shape = (4, 4)
+shape = (2, 2)
-# kv.pull(3, out = val)
+# kv.push(keys, [mx.nd.ones(shape)*(my_rank+2)] * len(keys))
-                                 learning_rate=0.05, momentum=0.9, wd=0.0001)
+                                 learning_rate=0.05, momentum=0.9, wd=0.0001,
-    def load(prefix, iteration, ctx=None):
+    def load(prefix, iteration, ctx=None, **kwargs):
-                           arg_params=arg_params, aux_params=aux_params)
+                           arg_params=arg_params, aux_params=aux_params,
-        return NDArray._mul_scalar(self, -1.0, out=self)
+        return NDArray._mul_scalar(self, -1.0)
-    assert(np.sum(c.asnumpy()) < 1e-5)
+    assert(np.sum(d.asnumpy()) < 1e-5)
-        yield
+        check_call(_LIB.MXKVStoreFree(self.handle))
-        server.Run()
+        server.run()
-from .base import check_call, c_array, c_str, string_types, mx_uint
+from .base import _LIB, check_call, c_array, c_str, string_types, mx_uint
-        check_call(_LIB.MXKVStoreRunServer(self.handle))
+    def controller(self):
-    check_call(_LIB.MXKVStoreIsWorkerNode(self.handle, ctypes.byref(is_worker)))
+    check_call(_LIB.MXKVStoreIsWorkerNode(ctypes.byref(is_worker)))
-        server = KVStoreServer(self.handle)
+        name = 'dist'
-        yield
+    def Run(self):
-
+        - dist: distributed KVStore supporting multiple machines
-# pylint: disable=fixme, invalid-name, unused-argument, too-many-arguments
+# pylint: disable=fixme, invalid-name, unused-argument, too-many-arguments, no-name-in-module
-                mom[:] += -lr * (clip(grad * self.rescale_grad, self.clip_gradient) +
+                mom[:] += -lr * (clip(grad * self.rescale_grad, -self.clip_gradient,
-
+    test_clip()
-                c_array(NDArrayHandle, (src.handle)), \
+                c_array(NDArrayHandle, (src.handle,)), \
-    elif n_mutate_vars == 1 and n_used_vars == 2 and n_scalars == 0:
+    elif n_mutate_vars == 1 and n_used_vars == 1 and n_scalars == 0:
-        """
+    def __call__(self, param):
-                logging.info("Batch [%d]\tSpeed: %.2f samples/sec", count, speed)
+                logging.info("Iter[%d] Batch [%d]\tSpeed: %.2f samples/sec",
-    """Show a progress bar
+    """Show a progress bar.
-
+    def __call__(self, param):
-
+from collections import namedtuple
-                        update_on_kvstore=None,
+                        update_on_kvstore=None, kvstore_type='local',
-        A Scheduler to adjust learning rate
+    epoch_end_callback : callable(EpochEndParams)
-    update_on_kvstore: boolean, optional
+    update_on_kvstore : boolean, optional
-    if kv is None:
+    kv = kvstore.create(kvstore_type) if num_device != 1 else None
-                        call(nbatch)
+                        call(epoch_end_params)
-                    epoch_end_callback(nbatch)
+                    epoch_end_callback(epoch_end_params)
-            update_on_kvstore=None, logger=None):
+            update_on_kvstore=None, kvstore_type='local',
-
+        kvstore_type : {'local', 'device'}, optional
-                            update_on_kvstore=update_on_kvstore, logger=logger)
+                            update_on_kvstore=update_on_kvstore,
-               update_on_kvstore=None, logger=None, **kwargs):
+               update_on_kvstore=None, kvstore_type='local',
-                  update_on_kvstore=update_on_kvstore, logger=logger)
+                  update_on_kvstore=update_on_kvstore,
-pool1 = mx.symbol.Pooling(data=relu1, pool_type="max",
+tanh1 = mx.symbol.Activation(data=conv1, act_type="tanh")
-pool2 = mx.symbol.Pooling(data=relu2, pool_type="max",
+tanh2 = mx.symbol.Activation(data=conv2, act_type="tanh")
-relu3 = mx.symbol.Activation(data=fc1, act_type="relu")
+tanh3 = mx.symbol.Activation(data=fc1, act_type="tanh")
-fc2 = mx.symbol.FullyConnected(data=relu3, num_hidden=10)
+fc2 = mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
-    learning_rate = 0.01, momentum = 0.9, wd = 0.00001)
+    learning_rate = 0.05, momentum = 0.9, wd = 0.00001)
-            dll_path.append(os.path.join(api_path, '../../windows/x64/Release/'))
+            dll_path.append(os.path.join(curr_path, '../../windows/x64', vs_configuration))
-            dll_path.append(os.path.join(api_path, '../../windows/Release/'))
+            dll_path.append(os.path.join(curr_path, '../../windows', vs_configuration))
-sys.path.append("../../tests/python/common")
+# code to automatically download dataset
-sys.path.append("../../tests/python/common")
+import os
-READTHEDOCS_BUILD = (os.environ.get('READTHEDOCS', None) == 'True')
+#READTHEDOCS_BUILD = (os.environ.get('READTHEDOCS', None) == 'True')
-        scale = np.sqrt(6. / (fan_in + fan_out))
+        fan_in, fan_out = np.prod(shape[1:]), shape[0]
-from .initializer import Xavier
+from .initializer import Uniform
-                 initializer=Xavier(),
+                 initializer=Uniform(0.01),
-               num_round=None, optimizer='sgd', initializer=Xavier(),
+               num_round=None, optimizer='sgd', initializer=Uniform(0.01),
-        learning_rate=0.01, wd=0.0004,
+        learning_rate=0.1, wd=0.0004,
-__version__ = "0.1.0"
+__version__ = base.__version__
-import platform
+from . import libinfo
-    lib_path = find_lib_path()
+    lib_path = libinfo.find_lib_path()
-
+# version number
-"""Setup script for mxnet."""
+# pylint: disable=invalid-name, exec-used
-import sys
+import os
-LIB_PATH = mxnet.base.find_lib_path()
+# We can not import `mxnet.info.py` in setup.py directly since mxnet/__init__.py
-      description=mxnet.__doc__,
+      version=__version__,
-                        update_on_kvstore=False,
+                        update_on_kvstore=None,
-    - update_on_kvstore=False works better for Alexnet style net with bulk weights.
+      - It is auto selected by default.
-            update_on_kvstore=False, logger=None):
+            update_on_kvstore=None, logger=None):
-               update_on_kvstore=False, logger=None, **kwargs):
+               update_on_kvstore=None, logger=None, **kwargs):
-# pylint: disable=invalid-name
+# pylint: disable=invalid-name, pointless-string-statement
-    """Calculate logloss"""
+    # remove because it because it is too slow
-pool2 = mx.symbol.Pooling(data=relu2, kernel=(3, 3), stride=(2, 2))
+pool2 = mx.symbol.Pooling(data=relu2, kernel=(3, 3), stride=(2, 2), pool_type="max")
-pool3 = mx.symbol.Pooling(data=relu5, kernel=(3, 3), stride=(2, 2))
+pool3 = mx.symbol.Pooling(data=relu5, kernel=(3, 3), stride=(2, 2), pool_type="max")
-num_gpus = 2
+num_gpus = 4
-        mean_img           = "data/ilsvrc12/mean.bin",
+        path_imgrec        = "data/train.rec",
-        mean_img           = "data/ilsvrc12/mean.bin",
+        path_imgrec        = "data/val.rec",
-    This function will inplace update the NDArrays in arg_parans and aux_states.
+    - This function will inplace update the NDArrays in arg_parans and aux_states.
-
+    if kv is None:
-            opt_state_blocks.append(opt_list)
+            if update_on_kvstore:
-                    optimizer.update(index, w, g, state)
+                    if update_on_kvstore:
-            iter_end_callback=None, epoch_end_callback=None, logger=None):
+            iter_end_callback=None, epoch_end_callback=None,
-                            logger=logger)
+                            update_on_kvstore=update_on_kvstore, logger=logger)
-               logger=None, **kwargs):
+               update_on_kvstore=False, logger=None, **kwargs):
-                  iter_end_callback=iter_end_callback, logger=logger)
+                  iter_end_callback=iter_end_callback,
-        momentum=0.9)
+        momentum=0.9,
-            self.num_inst += label.size
+            self.sum_metric += -numpy.log(p)
-"""
+"""MXNet: a concise, fast and flexible framework for deep learning. """
-"""model helper for knowing training status"""
+# coding: utf-8
-    def _callback(iter_no, s, arg, aux):
+    def _callback(iter_no, sym, arg, aux):
-        save_checkpoint(prefix, iter_no + 1, s, arg, aux)
+        save_checkpoint(prefix, iter_no + 1, sym, arg, aux)
-                logging.info("Batch [%d]\tSpeed: %.2f samples/sec" % (count, speed))
+                logging.info("Batch [%d]\tSpeed: %.2f samples/sec", count, speed)
-        sys.stdout.write('[%s] %s%s\r' % (bar, percents, '%'))
+        prog_bar = '=' * filled_len + '-' * (self.bar_len - filled_len)
-""" code for context management """
+"""Context management API of mxnet."""
-""" code for executor. """
+# pylint: disable=invalid-name, protected-access, too-many-locals
-        """Init an executor from handle
+    def __init__(self, handle, symbol):
-        """Do forward.
+        Returns
-            whether this forward is for evaluation purpose
+        is_train: bool, optional
-        """Do backward on heads' gradient.
+    def backward(self, out_grads=None):
-            Gradient on the heads
+        out_grads : NDArray or list of NDArray, optional
-            head_grads = [head_grads]
+        if out_grads is None:
-        for obj in head_grads:
+        for obj in out_grads:
-        ndarray = c_array(NDArrayHandle, [item.handle for item in head_grads])
+        ndarray = c_array(NDArrayHandle, [item.handle for item in out_grads])
-            mx_uint(len(head_grads)),
+            mx_uint(len(out_grads)),
-""" KVStore in mxnet """
+""" Key value store interface of MXNet for parameter synchronization."""
-                assert(isinstance(v, NDArray))
+            for value in vals:
-                    c_array(NDArrayHandle, [v.handle for v in vals]))
+                    c_array(NDArrayHandle, [value.handle for value in vals]))
-        For each key, one must init it before push and pull
+        For each key, one must init it before push and pull.
-        """ Pull a single value or a sequence of values from the store
+        """ Pull a single value or a sequence of values from the store.
-# pylint: disable=invalid-name
+# coding: utf-8
-        self.sum_metric += numpy.sum(py == label)
+        pred_label = numpy.argmax(pred, axis=1)
-
+# pylint: disable=invalid-name
-
+# pylint: enable=invalid-name
-# pylint: disable=too-many-branches, too-many-statements, unused-argument
+# pylint: disable=too-many-branches, too-many-statements
-                aux_params[name].copyto(w)
+
-                texec.forward()
+                texec.forward(is_train=True)
-"""NDArray interface of mxnet"""
+"""NDArray API of mxnet."""
-        - /path-to/my-local-ndarray
+
-        - /path-to/my-local-ndarray
+
-"""
+# pylint: disable=invalid-name, protected-access, too-many-arguments
-
+        executor = Executor(handle, self)
-# pylint: disable=unused-variable
+# pylint: disable=invalid-name, too-many-locals, fixme
-    test_NumpyIter()
+    #test_NDArrayIter()
-    #test_Cifar10Rec()
+    test_Cifar10Rec()
-                assert name in self.arg_params
+                if not name in self.arg_params:
-import numpy as np
+import numpy
-        self.sum_metric += np.sum(py == label)
+        py = numpy.argmax(pred, axis=1)
-            self.batch_label[i, 0:actual_size, ::] = label[loc:loc+actual_size, ::]
+            self.batch_label[i, 0:actual_size] = label[loc:loc+actual_size]
-        self.num_pad = data.shape[0] % batch_size
+        if data.shape[0] > batch_size:
-    def update(self, pred, label):
+    def update(self, label, pred):
-
+
-    def update(self, pred, label):
+    def update(self, label, pred):
-            name = 'custom(%s)' % name
+    """Custom evaluation metric that takes a NDArray function.
-        self.sum_metric += self._feval(pred, label)
+    def update(self, label, pred):
-            eval_metric.update(out_cpu_array, label)
+            eval_metric.update(label, out_cpu_array)
-                eval_metric.update(out_cpu_array, label)
+                eval_metric.update(label, out_cpu_array)
-    label = label.asnumpy().astype('int32')
+def accuracy(label, pred):
-        eval_metric=accuracy,
+        eval_metric=mx.metric.np(accuracy),
-        check_call(_LIB.MXExecutorForward(self.handle, is_train))
+        check_call(_LIB.MXExecutorForward(
-        check_call(_LIB.MXExecutorBackward(self.handle, len(head_grads), ndarray))
+        check_call(_LIB.MXExecutorBackward(
-            handle, len(param_keys),
+            handle,
-from .base import mx_uint, mx_float, NDArrayHandle, FunctionHandle
+from .base import mx_uint, mx_float, mx_float_p, NDArrayHandle, FunctionHandle
-        int(delay_alloc),
+        mx_uint(len(shape)),
-            length = ctypes.c_ulong()
+            length = ctypes.c_size_t()
-            length = ctypes.c_ulong(len(buf))
+            length = ctypes.c_size_t(len(buf))
-            source_array.size))
+            source_array.ctypes.data_as(mx_float_p),
-            data.size))
+            data.ctypes.data_as(mx_float_p),
-                                  len(handles),
+                                  mx_uint(len(handles)),
-            self.handle, len(indptr) - 1,
+            self.handle,
-                                       len(args),
+                                       ctypes.c_int(ctx.device_typeid),
-                                       len(aux_states),
+                                       mx_uint(len(aux_states)),
-        len(ihandles), c_array(SymbolHandle, ihandles), ctypes.byref(handle)))
+        mx_uint(len(ihandles)),
-            handle, len(param_keys),
+            handle,
-        """
+        """Reset the iterator. """
-        """get next data batch from iterator
+        """Get next data batch from iterator
-        labels and images for the next batch
+        data : NDArray
-        """iterate to next data with return value
+        """Iterate to next batch.
-        return true if success
+        has_next : boolean
-        """get data from batch
+        """Get data of current batch.
-        data ndarray for the next batch
+        data : NDArray
-        """get label from batch
+        """Get label of current batch.
-        label ndarray for the next batch
+        pad : int
-    """NumpyIter object in mxnet. Taking Numpy Array to get dataiter.
+class NDArrayIter(DataIter):
-        Numpy ndarray for label
+    data : NDArray or numpy.ndarray
-        padding value for label
+
-        self.label_pad = label_pad
+    def __init__(self, data, label,
-            idx = np.arange(self.data.shape[0])
+        if shuffle:
-            self.label = new_label
+            new_data = np.zeros(data.shape)
-        self.batch_num = int(math.ceil(float(self.data.shape[0]) / self.batch_size))
+        self.batch_num = int(math.ceil(float(data.shape[0]) / batch_size))
-            batch_data_shape.append(self.data.shape[i])
+        batch_data_shape.append(batch_size)
-        self.loc = 0
+        batch_label_shape.append(batch_size)
-            self.loc += self.batch_size
+            actual_size = min(data.shape[0] - loc, batch_size)
-        """
+    def getpad(self):
-        """
+
-
+        # reset training data after iteration finish
-            eval_data.reset()
+            eval_data.reset()
-                 num_round=None, optimizer='sgd', initializer=Xavier(),
+                 num_round=None, optimizer='sgd',
-
+    def _init_iter(self, X, y, is_train):
-        outputs = []
+        X = self._init_iter(X, None, is_train=False)
-        input_shape = self._get_input_shape(X)
+        X = self._init_iter(X, y, is_train=True)
-def test_NumpyIter():
+def test_NDArrayIter():
-    dataiter = mx.io.NumpyIter(datas, labels, 128, True)
+    dataiter = mx.io.NDArrayIter(datas, labels, 128, True)
-    labelcount = [0 for i in range(10)] 
+    labelcount = [0 for i in range(10)]
-# import library
+sys.path.append("../../tests/python/common")
-import copy
+logger = logging.getLogger()
-    return relu
+# Basic Conv + BN + ReLU factory
-    param = {}
+# A Simple Downsampling Factory
-    conv3x3 = ConvFactory(**param)
+    conv = ConvFactory(data=data, kernel=(3, 3), stride=(2, 2), num_filter=ch_3x3, pad=(1, 1))
-    pool_cnt += 1
+    pool = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pool_type='max')
-    concat_cnt += 1
+    concat = mx.symbol.Concat(*[conv, pool])
-
+# A Simple module
-
+    conv1x1 = ConvFactory(data=data, kernel=(1, 1), pad=(0, 0), num_filter=ch_1x1)
-
+    conv3x3 = ConvFactory(data=data, kernel=(3, 3), pad=(1, 1), num_filter=ch_3x3)
-    concat_cnt += 1
+    concat = mx.symbol.Concat(*[conv1x1, conv3x3])
-pool = mx.symbol.Pooling(data=in5b, pool_type="avg", kernel=(7,7), name="pool%d" % pool_cnt)
+pool = mx.symbol.Pooling(data=in5b, pool_type="avg", kernel=(7,7), name="global_pool")
-                                 lr_scheduler=mx.misc.FactorScheduler(2))
+                                 learning_rate=0.05, momentum=0.9, wd=0.0001)
-
+def clip(arr, value):
-from .ndarray import NDArray, zeros
+from .ndarray import NDArray, zeros, clip
-                mom[:] += -lr * (grad.clip(self.clip_gradient) * self.rescale_grad +
+                mom[:] += -lr * (clip(grad * self.rescale_grad, self.clip_gradient) +
-    net2 = mx.symbol.Activation(data=net2)
+    net2 = mx.symbol.Activation(data=net2, act_type='relu')
-from .base import check_call, c_array, c_str, string_types
+from .base import check_call, c_array, c_str, string_types, mx_uint
-        check_call(_LIB.MXKVStoreInit(self.handle, len(ckeys), ckeys, cvals))
+        check_call(_LIB.MXKVStoreInit(
-    def push(self, key, value):
+    def push(self, key, value, priority=0):
-        value: NDArray or list of NDArray or list of list of NDArray
+        value : NDArray or list of NDArray or list of list of NDArray
-        check_call(_LIB.MXKVStorePush(self.handle, len(ckeys), ckeys, cvals))
+        check_call(_LIB.MXKVStorePush(
-    def pull(self, key, out=None):
+    def pull(self, key, out=None, priority=0):
-        check_call(_LIB.MXKVStorePull(self.handle, len(ckeys), ckeys, cvals))
+        check_call(_LIB.MXKVStorePull(
-# pylint: disable=invalid-name, logging-not-lazy, arguments-differ
+# pylint: disable=invalid-name
-        """lr calculation function"""
+    def __call__(self, iteration):
-                    % (iteration, lr))
+            logging.info("At Iteration [%d]: Swith to new learning rate %.5f",
-                    kv.push(index, grad_list)
+                    # push gradient, priority is negative index
-                    kv.pull(index, grad_list)
+                    kv.pull(index, grad_list, priority=-index)
-        _, out_shapes, __ = interals.infer_shape(**shape)
+        _, out_shapes, _ = interals.infer_shape(**shape)
-                 wd=0.0001, rescale_grad=1, lr_scheduler=None):
+                 wd=0.0001, rescale_grad=1, clip_gradient=None,
-            mom[:] += -lr * (grad * self.rescale_grad + self.wd * weight)
+            if self.clip_gradient == None:
-train, val = ilsvrc12_iterator(batch_size=256, input_shape=(3,224,224))
+batch_size = 256
-          epoch_end_callback = mx.callback.Speedometer(100))
+          epoch_end_callback = mx.callback.Speedometer(batch_size=batch_size))
-    symbol        = lenet,
+    symbol        = softmax,
-
+# pylint: disable=unused-variable
-    __next__ = next
+    def __next__(self):
-
+    shape: dict
-        name = "%s_%d" % (op, i)
+        name = node["name"]
-        name = "%s_%d" % (op, i)
+        name = node["name"]
-                input_name = "%s_%d" % (input_node["op"], item[0])
+                input_name = input_node["name"]
-                    # add shape into label
+                    # add shapes
-model.fit(X=train, eval_data=val)
+model.fit(X=train, eval_data=val,
-                                 learning_rate=0.05, momentum=0.9, wd=0.00001)
+                                 learning_rate=0.05, momentum=0.9, wd=0.00001,
-              epoch_end_callback=mx.helper.Speedometer(batch_size))
+              epoch_end_callback=mx.callback.Speedometer(batch_size))
-from . import scheduler
+from . import callback
-# pylint: disable=logging-not-lazy, blacklisted-name
+# pylint: disable=logging-not-lazy, blacklisted-name, invalid-name
-                        epoch_end_callback=None, logger=None):
+                        iter_end_callback=None, epoch_end_callback=None,
-                learning_rate_scheduler(optimizer, nbatch, iteration)
+            if epoch_end_callback != None:
-            iter_end_callback(iteration, symbol, arg_params, aux_params)
+        if iter_end_callback != None:
-            epoch_end_callback=None, logger=None):
+            iter_end_callback=None, epoch_end_callback=None, logger=None):
-# pylint: disable=fixme, invalid-name, unused-argument
+# pylint: disable=fixme, invalid-name, unused-argument, too-many-arguments
-        pass
+        self.iteration = iteration
-                 wd=0.0001, rescale_grad=1):
+                 wd=0.0001, rescale_grad=1, lr_scheduler=None):
-            mom[:] += -self.lr * (grad * self.rescale_grad + self.wd * weight)
+            mom[:] += -lr * (grad * self.rescale_grad + self.wd * weight)
-            weight[:] += -self.lr * (grad * self.rescale_grad + self.wd * weight)
+            weight[:] += -lr * (grad * self.rescale_grad + self.wd * weight)
-        iter_end_callback=mx.model.do_checkpoint(prefix),
+        iter_end_callback=mx.callback.do_checkpoint(prefix),
-
+    """NumpyIter object in mxnet. Taking Numpy Array to get dataiter.
-        """
+        # batching
-        """set loc to 0
+        """set current batch to 0
-        self.loc = 0
+        self.current_batch = -1
-            self.loc += actual_size
+        if self.current_batch < self.batch_num - 1:
-        return array(self.out_data)
+        assert(self.current_batch >= 0)
-        return array(self.out_label)
+        assert(self.current_batch >= 0)
-    """DataIter object in mxnet. List all the needed functions here. """
+    """DataIter built in MXNet. List all the needed functions here.
-    labelcount = [0 for i in range(10)] 
+    labelcount = [0 for i in range(10)]
-lenet = mx.symbol.Softmax(data=relu4)
+lenet = mx.symbol.Softmax(data=fc2)
-    ctx = mx.gpu(), symbol = lenet, num_round = 10,
+    ctx = dev, symbol = lenet, num_round = 20,
-    ctx = mx.cpu(), symbol = mlp, num_round = 10,
+    ctx = mx.cpu(), symbol = mlp, num_round = 20,
-            attr["fillcolor"] = cm[2]
+            attr["fillcolor"] = cm[3]
-            attr["fillcolor"] = cm[3]
+            attr["fillcolor"] = cm[2]
-            dot.node(name=name, label=label, **attr)
+        attr = copy.deepcopy(node_attr)
-            continue
+            if i in heads:
-            dot.node(name=name, label=label, **attr)
+            attr["fillcolor"] = cm[1]
-            dot.node(name=name, label=label, **attr)
+            attr["fillcolor"] = cm[1]
-            dot.node(name=name, label=label, **attr)
+            attr["fillcolor"] = cm[2]
-            dot.node(name=name, label=label, **attr)
+            attr["fillcolor"] = cm[4]
-            dot.node(name=name, label=label, **attr)
+            attr["fillcolor"] = cm[7]
-                    attr = {"dir": "back"}
+                    attr = {"dir": "back", 'arrowtail':'open'}
-    fc1 = internal[nmap['fc1_output']]
+    print internal.list_outputs()
-sys.path.append("../../tests/python/common")
+from data import mnist_iterator
-
+train, val = mnist_iterator(batch_size=100, input_shape = (784,))
-                             wd = 0.00001)
+model = mx.model.FeedForward(
-model.fit(X=train_dataiter, eval_data=val_dataiter)
+model.fit(X=train, eval_data=val)
-    logging.getLogger('').addHandler(console)
+    #console = logging.StreamHandler()
-
+    model.fit(X=train_dataiter, eval_data=test_dataiter,
-        prefetch_buffer=4)
+        prefetch_buffer=4,
-                        iter_end_callback=None, logger=None):
+                        iter_end_callback=None, learning_rate_scheduler=None,
-            outputs.append(self._pred_exec.outputs[0].asnumpy())
+            out_batch = self._pred_exec.outputs[0].asnumpy()
-            iter_end_callback=None, logger=None):
+            iter_end_callback=None, learning_rate_scheduler=None,
-import logging
+import logging
-loss = mx.symbol.Softmax(data=fc, name="loss")
+softmax = mx.symbol.Softmax(data=fc, name="loss")
-epoch = 3
+num_round = 3
-
+logging.basicConfig(level=logging.DEBUG)
-    test_cifar()
+gpus = [mx.gpu(i) for i in range(num_gpus)]
-    Context(device_type=gpu, device_id=2)
+    gpu(2)
-            self.device_type, self.device_id)
+        return '%s(%d)' % (self.device_type, self.device_id)
-    logging.info('Start training with %d devices', num_device)
+    logging.info('Start training with %s', str(ctx))
-        ---------
+        Examples
-        >>>     stored += input * 2
+        ...     print "update on key: %d" % key
-    """DataIter object in mxnet. List all the needed functions here. """
+    """DataIter object in mxnet. """
-        """Initialize with handle
+    def __init__(self):
-        self.handle = handle
+        pass
-        check_call(_LIB.MXDataIterFree(self.handle))
+        """destructor of dataiter
-        """set loc to 0
+        """reset the iter
-        check_call(_LIB.MXDataIterBeforeFirst(self.handle))
+        pass
-            raise StopIteration
+        pass
-        return next_res.value
+        pass
-        return NDArray(hdl, False)
+        pass
-        return DataIter(iter_handle)
+        return MXDataIter(iter_handle)
-    labelcount = [0 for i in range(10)]
+    labelcount = [0 for i in range(10)] 
-    test_Cifar10Rec()
+    #test_Cifar10Rec()
-epoch = 3
+epoch = 10
-        raise ValueError('Cannot find metric %s' % name)
+        raise ValueError('Cannot find metric %s' % metric)
-def create(name):
+class CustomMetric(EvalMetric):
-        The name of the metric
+    metric : str or callable
-    if name == 'acc' or name == 'accuracy':
+    if callable(metric):
-            Evaluation metric function.
+        eval_metric : metric.EvalMetric or str or callable
-        if isinstance(eval_metric, str):
+        if not isinstance(eval_metric, metric.EvalMetric):
-            Evaluation metric function.
+        eval_metric : metric.EvalMetric or str or callable
-num_gpus = 4
+num_gpus = 1
-
+
-num_gpus = 1
+num_gpus = 4
-        """fit the model
+        """Fit the model.
-                             momentum=0.9)
+
-    logging.info('Finish fit...')
+    model = mx.model.FeedForward.create(
-def plot_network(title, symbol, shape=None):
+def plot_network(symbol, title="plot", shape=None):
-            kv.init(index, arg_list[0])
+        if grad_list[0] is not None:
-model = mx.model.FeedForward(softmax, [mx.cpu()] * 2,
+model = mx.model.FeedForward(softmax,
-    gpus = [mx.gpu(i) for i in range(2)]
+    gpus = [mx.gpu(i) for i in range(num_gpus)]
-    model = mx.model.FeedForward(ctx=mx.gpu(), symbol=loss, num_round = epoch,
+    gpus = [mx.gpu(i) for i in range(2)]
-# pylint: skip-file
+# coding: utf-8
-    def _init_zero(self, name, arr):
+    # pylint: disable=no-self-use, missing-docstring
-    def _init_bias(self, name, arr):
+    def _init_bias(self, _, arr):
-    def _init_gamma(self, name, arr):
+    def _init_gamma(self, _, arr):
-    def _init_beta(self, name, arr):
+    def _init_beta(self, _, arr):
-
+    # pylint: enable=no-self-use, missing-docstring
-        random.uniform(-scale, scale, out=arr)
+    def _init_weight(self, _, arr):
-        super().__init__(sigma = sigma)
+        self.sigma = sigma
-        random.normal(0, sigma, out=arr)
+    def _init_weight(self, _, arr):
-        random.uniform(-s, s, out=arr)
+        scale = np.sqrt(6. / (fan_in + fan_out))
-
+
-            kv.init(index, arg[0])
+        arg_list, grad_list = pair
-                    optimizer.update(index, w, g)
+                opt_list = opt_state_blocks[index]
-# pylint: disable=fixme, invalid-name
+# pylint: disable=fixme, invalid-name, unused-argument
-    def update(self, index, weight, grad):
+    def create_state(self, index, weight):
-        weight[:] += mom
+        if state:
-model = mx.model.FeedForward(softmax, mx.cpu(),
+model = mx.model.FeedForward(softmax, [mx.cpu()] * 2,
-    """Context representing device and device id in mxnet"""
+    """Constructing a context.
-        """
+# coding: utf-8
-        os.system("gunzip data/t10k-labels-idx1-ubyte.gz")
+    if (not os.path.exists('data/train-images-idx3-ubyte')) or \
-        os.system("unzip cifar10.zip")
+        os.system("unzip -u cifar10.zip")
-    model.fit(X=train_dataiter, eval_set=test_dataiter, eval_metric=CalAcc)
+    logging.basicConfig(level=logging.DEBUG)
-        end = min((k+1) * step, batch_size)
+        begin = int(min(k * step, batch_size))
-softmax = mx.symbol.Softmax(data = fc3, name = 'sm')
+fc1 = mx.symbol.FullyConnected(data, name='fc1', num_hidden=128)
-    """Inernal training function.
+def _split_input_slice(input_shape, num_split):
-    train_exec = symbol.simple_bind(ctx[0], data=input_shape, grad_req='write')
+    # preparation
-    # setup helper data structures
+    # data structure
-        # training phase
+    merged_shape = list(train_execs[0].outputs[0].shape)
-        optimizer.begin_round(i)
+        optimizer.begin_round(iteration)
-
+        # Iterate over training data.
-            train_exec.backward()
+            # Copy data into the target
-                    optimizer.update(index, weight, grad)
+            for index, pair in enumerate(zip(arg_blocks, grad_blocks)):
-        logger.info('Iteration[%d] Train-%s=%f', i, name, value)
+        logger.info('Iteration[%d] Train-%s=%f', iteration, name, value)
-        if eval_data is not None:
+        logger.info('Iteration[%d] Time cost=%.3f', iteration, (toc - tic))
-
+                # Copy data into the target
-            logger.info('Iteration[%d] Validation-%s=%f', i, name, value)
+            logger.info('Iteration[%d] Validation-%s=%f', iteration, name, value)
-        if iter_end_callback or i + 1 == end_round:
+        if iter_end_callback or iteration + 1 == end_round:
-                arr.copyto(aux_params[key])
+            for name, block in zip(arg_names, arg_blocks):
-    # end of the function
+            iter_end_callback(iteration, symbol, arg_params, aux_params)
-               logger=logger)
+        _train_multi_device(self.symbol, self.ctx, input_shape,
-        input_shape=(3,28,28),
+        data_shape=(3,28,28),
-        input_shape=(3,28,28),
+        data_shape=(3,28,28),
-# pylint: skip-file
+# Pylint: skip-file
-        input_shape=(3,28,28),
+        data_shape=(3,28,28),
-        input_shape=(3,28,28),
+        data_shape=(3,28,28),
-            raise Exception("Set NDArray should use empty index array[:] = target_array")
+        if not isinstance(in_slice, slice) or in_slice.step is not None:
-        return self
+        if not isinstance(in_slice, slice) or in_slice.step is not None:
-        """Block until all pending writes operations on current NDArray are finished.
+    def _slice(self, start, stop):
-        function returns.
+        Parameters
-        check_call(_LIB.MXNDArrayWaitToRead(self.handle))
+        handle = NDArrayHandle()
-        """Block until all pending read/write operations on current NDArray are finished.
+    def wait_to_read(self):
-        check_call(_LIB.MXNDArrayWaitToWrite(self.handle))
+        check_call(_LIB.MXNDArrayWaitToRead(self.handle))
-    narray_arg = []
+    ndarray_arg = []
-        narray_arg.append(narr)
+        ndarray_arg.append(narr)
-    out1 = uf(*narray_arg)
+    out1 = uf(*ndarray_arg)
-def random_narray(dim):
+def random_ndarray(dim):
-    data= mx.nd.array(np.random.uniform(-10, 10, shape))
+    data = mx.nd.array(np.random.uniform(-10, 10, shape))
-def test_narray_elementwise():
+def test_ndarray_elementwise():
-def test_narray_copy():
+def test_ndarray_copy():
-def test_narray_scalar():
+def test_ndarray_scalar():
-def test_narray_pickle():
+def test_ndarray_pickle():
-            a = random_narray(dim)
+            a = random_ndarray(dim)
-def test_narray_saveload():
+def test_ndarray_saveload():
-            data.append(random_narray(np.random.randint(1, 5)))
+            data.append(random_ndarray(np.random.randint(1, 5)))
-        dmap = {'narray xx %s' % i : x for i, x in enumerate(data)}
+        dmap = {'ndarray xx %s' % i : x for i, x in enumerate(data)}
-    test_narray_scalar()
+    test_ndarray_slice()
-import atexit
+from .base import check_call, c_array, c_str, string_types
-    Set a push updater into the store
+class KVStore(object):
-     [ 6.  6.  6.]]
+    name : {'local'}
-atexit.register(_cleanup)
+    if not isinstance(name, string_types):
-    mx.kv.start()
+    kv = mx.kv.create()
-    mx.kv.init(3, mx.nd.zeros(shape))
+    kv.init(3, mx.nd.zeros(shape))
-    mx.kv.init(keys, [mx.nd.zeros(shape)] * len(keys))
+    kv.init(keys, [mx.nd.zeros(shape)] * len(keys))
-    mx.kv.push(3, mx.nd.ones(shape))
+    kv = init_kv()
-    mx.kv.pull(3, out = val)
+    kv.pull(3, out = val)
-    init_kv()
+    kv = init_kv()
-    mx.kv.push(keys, [mx.nd.ones(shape)*4] * len(keys))
+    kv.push(keys, [mx.nd.ones(shape)*4] * len(keys))
-    mx.kv.pull(keys, out = val)
+    kv.pull(keys, out = val)
-    init_kv()
+    kv = init_kv()
-    mx.kv.pull(3, out = vals)
+    kv.push(3, vals)
-    mx.kv.pull(keys, out = vals)
+    kv.push(keys, vals)
-    mx.kv.set_updater(updater)
+    kv = init_kv()
-    mx.kv.pull(3, out = vals)
+    kv.push(3, vals)
-        mx.kv.push(keys, vals)
+        kv.push(keys, vals)
-    mx.kv.pull(keys, out = vals)
+    kv.pull(keys, out = vals)
-    # test_updater('gpu')
+
-        arr[:] = 0
+        arr[:] = 0.0
-    arg_blocks = zip(arg_arrays, grad_arrays)
+    arg_blocks = list(zip(arg_arrays, grad_arrays))
-                train_exec.forward()
+                train_exec.forward(is_train=False)
-            self._pred_exec.forward()
+            self._pred_exec.forward(is_train=False)
-
+
-block = list(zip(grad_narrays, arg_narrays))
+num_round = 1
-    assert(acc_val > 0.96)
+    # print logging by default
-        input_shape=(3,28,28),
+        data_shape=(3,28,28),
-        batch_size=100,       
+        batch_size=100,
-        # randomly crop a patch of the input_shape from the original image
+        # randomly crop a patch of the data_shape from the original image
-        prefetch_buffer=1)
+        prefetch_buffer=4)
-        input_shape=(1, 28, 28),
+        data_shape=(1, 28, 28),
-        input_shape=(1, 28, 28),
+        data_shape=(1, 28, 28),
-        input_shape=(784,),
+        data_shape=(784,),
-        input_shape=(784,),
+        data_shape=(784,),
-            input_shape=(784,),
+            data_shape=(784,),
-            input_shape=(3,28,28),
+            data_shape=(3,28,28),
-    labelcount = [0 for i in range(10)] 
+    labelcount = [0 for i in range(10)]
-def network2dot(title, symbol, shape=None):
+def plot_network(title, symbol, shape=None):
-import atexit
+# use viz as short for mx.ndarray
-from .base import check_call
+from .base import check_call, ctypes2docstring
-        param_str.append(ret)
+    param_str = ctypes2docstring(num_args, arg_names, arg_types, arg_descs)
-               'iterator: Iterator\n'+
+               'iterator: DataIter\n'+
-    doc_str = doc_str % (desc.value, '\n'.join(param_str))
+    doc_str = doc_str % (desc.value, param_str)
-        self.sum_metric += np.sum(y == label)
+        py = np.argmax(pred, axis=1)
-# pylint: disable=too-many-branches, too-many-statements, unused-argument, unused-variable
+# pylint: disable=fixme, invalid-name, too-many-arguments, too-many-locals
-from .context import Context
+from .context import Context, cpu
-           iter_end_callback=None, verbose=True):
+           iter_end_callback=None, logger=None):
-    iter_end_callback : callable(iteration, arg_params, aux_states)
+    iter_end_callback : callable(iteration, symbol, arg_params, aux_states)
-        Whether print message during training.
+    logger : logging logger
-    for key, weight in list(zip(arg_names, arg_arrays)):
+    for key, weight in zip(arg_names, arg_arrays):
-    for key, weight in list(zip(aux_names, aux_arrays)):
+    for key, weight in zip(aux_names, aux_arrays):
-
+    data_index, label_index = _check_arguments(symbol)
-    arg_blocks = list(zip(arg_names, arg_arrays, grad_arrays))
+    arg_blocks = zip(arg_arrays, grad_arrays)
-            for key, weight, grad in arg_blocks:
+            for index, block in enumerate(arg_blocks):
-                    optimizer.update(key, weight, grad)
+                    optimizer.update(index, weight, grad)
-
+        logger.info('Iteration[%d] Train-%s=%f', i, name, value)
-            print("Time: %.3f" % (toc - tic))
+        logger.info('Iteration[%d] Time cost=%.3f', i, (toc - tic))
-            print ('Validation %s:\t%f' % (name, value))
+            logger.info('Iteration[%d] Validation-%s=%f', i, name, value)
-            for key, weight, gard in arg_blocks:
+            for key, weight in zip(arg_names, arg_arrays):
-            for key, arr in list(zip(aux_names, aux_arrays)):
+            for key, arr in zip(aux_names, aux_arrays):
-            iter_end_callback(i, arg_params, aux_arrays)
+            iter_end_callback(i, symbol, arg_params, aux_params)
-    ctx : Context or list of Context
+    ctx : Context or list of Context, optional
-    aux_states : dict of str to NDArray, optional
+    aux_params : dict of str to NDArray, optional
-    def __init__(self, symbol, ctx, input_shape,
+    def __init__(self, symbol, ctx=None,
-                 arg_params=None, aux_states=None,
+                 arg_params=None, aux_params=None,
-        if isinstance(ctx, Context):
+        if ctx is None:
-            optimizer = opt.create(optimizer, rescale_grad=(1.0/batch_size), **kwargs)
+        self.kwargs = kwargs.copy()
-        self.aux_states = aux_states
+        self.aux_params = aux_params
-    def _init_params(self):
+    @staticmethod
-        arg_shapes, _, aux_shapes = self.symbol.infer_shape(data=self.input_shape)
+        arg_shapes, _, aux_shapes = self.symbol.infer_shape(data=input_shape)
-        if self.aux_states is None:
+                               if not self._is_data_arg(k)}
-            self.aux_states = {k : nd.zeros(s) for k, s in list(zip(aux_names, aux_shapes))}
+            self.aux_params = {k : nd.zeros(s) for k, s in list(zip(aux_names, aux_shapes))}
-        for k, v in self.aux_states.items():
+        for k, v in self.aux_params.items():
-    def _init_predictor(self):
+    def __getstate__(self):
-            self.ctx[0], grad_req='null', data=self.input_shape)
+            self.ctx[0], grad_req='null', data=input_shape)
-            if name not in self.arg_datas:
+            if not self._is_data_arg(name):
-                self._pred_exec_input = value
+        data_index, _ = _check_arguments(self.symbol)
-        self._init_predictor()
+        self._init_predictor(self._get_input_shape(X))
-            data.copyto(self.pred_exec_input)
+
-            outputs.extend(self._pred_exec.outputs[0].asnumpy())
+            outputs.append(self._pred_exec.outputs[0].asnumpy())
-    def fit(self, X, y=None, eval_data=None, eval_metric='acc', verbose=True):
+    def fit(self, X, y=None, eval_data=None, eval_metric='acc',
-            Whether print information during training.
+        iter_end_callback : callable(iteration, symbol, arg_params, aux_states)
-            self._init_params()
+            self._init_params(input_shape)
-               self.arg_params, self.aux_states,
+        # setup optimizer
-               optimizer=self.optimizer,
+               optimizer=optimizer,
-               verbose=verbose)
+               iter_end_callback=iter_end_callback,
-from .base import check_call
+from .base import check_call, ctypes2docstring
-
+    param_str = ctypes2docstring(num_args, arg_names, arg_types, arg_descs)
-    doc_str = doc_str % (py_str(desc.value), '\n'.join(param_str))
+    doc_str = doc_str % (py_str(desc.value), param_str)
-    def update(self, key, weight, grad):
+    def update(self, index, weight, grad):
-        weight: NDArray
+        index : int
-        grad: NDArray
+
-        mom = self.momentums[key]
+        if index not in self.momentums:
-from .base import check_call
+from .base import check_call, ctypes2docstring
-
+    param_str = ctypes2docstring(num_args, arg_names, arg_types, arg_descs)
-
+        desc += '\nThis function support variable length of positional input.'
-    doc_str = doc_str % (desc, '\n'.join(param_str))
+    doc_str = doc_str % (desc, param_str)
-import os, gzip
+import os, sys
-                             momentum=0)
+num_round = 4
-              eval_data=val_dataiter)
+              eval_data=val_dataiter,
-        nthread=1)
+        preprocess_threads=1)
-        nthread=1)
+        preprocess_threads=1)
-        prefetch_capacity=6)
+        preprocess_threads=4,
-        prefetch_capacity=6)
+        preprocess_threads=4,
-            prefetch_capacity=1)
+            preprocess_threads=4,
-    #test_Cifar10Rec()
+    #test_MNISTIter()
-    def update(pred, label):
+    def update(self, pred, label):
-# pylint: skip-file
+# pylint: disable=fixme, invalid-name, too-many-arguments, too-many-locals, no-member
-           arg_params, aux_states,
+           arg_params, aux_params,
-    aux_states : dict of str to NDArray
+    aux_params : dict of str to NDArray
-    for key, weight in zip(arg_names, arg_arrays):
+    for key, weight in list(zip(arg_names, arg_arrays)):
-    for key, weight in zip(aux_names, aux_arrays):
+    for key, weight in list(zip(aux_names, aux_arrays)):
-    for name, arr in zip(symbol.list_arguments(),  arg_arrays):
+    for name, arr in list(zip(symbol.list_arguments(), arg_arrays)):
-                arr.copyto(aux_states[key])
+            for key, arr in list(zip(aux_names, aux_arrays)):
-            iter_end_callback(i, arg_params, aux_states)
+            iter_end_callback(i, arg_params, aux_arrays)
-            self.arg_params = {k : nd.zeros(s) for k, s in zip(arg_names, arg_shapes)
+            self.arg_params = {k : nd.zeros(s) for k, s in list(zip(arg_names, arg_shapes))
-            self.aux_states = {k : nd.zeros(s) for k, s in zip(aux_names, aux_shapes)}
+            self.aux_states = {k : nd.zeros(s) for k, s in list(zip(aux_names, aux_shapes))}
-        for name, value in zip(self.symbol.list_arguments(), pred_exec.arg_arrays):
+        for name, value in list(zip(self.symbol.list_arguments(), pred_exec.arg_arrays)):
-# pylint: skip-file
+# pylint: disable=fixme, invalid-name
-            req = {}
+# coding: utf-8
-    return re.findall("\d+", string)
+    """convert shape string to list, internal use only
-    node_attr = {"shape":"box", "fixedsize":"true", "width":"1.3", "height":"0.8034", "style":"filled"}
+    heads = set(conf["heads"][0])  # TODO(xxx): check careful
-            label=node["name"]
+            label = node["name"]
-                                                 node["param"]["num_filter"])
+            label = "Convolution\n%sx%s/%s, %s" % (_str2tuple(node["param"]["kernel"])[0],
-            label="FullyConnected\n%s" % node["param"]["num_hidden"]
+            label = "FullyConnected\n%s" % node["param"]["num_hidden"]
-                    attr = {"dir":"back"}
+                    attr = {"dir": "back"}
-
+from . import visualization
-        return self.auxiliary_states
+        self.arg_arrays = []
-    def outputs(self):
+    def _get_outputs(self):
-        # if user set the content of the head, the backward behavior can be incorrect.
+from .base import string_types
-        """Constructor
+    """Base class for Initializer."""
-            potential parameters for Initializer implmentation
+        name : str
-        self.args = kwargs
+        if not isinstance(name, string_types):
-    def init_weight(self):
+    def _init_gamma(self, name, arr):
-        """Override () function to do Initialization
+    def _init_default(self, name, _):
-    """Uniform Initializer"""
+    """Initialize the weight with uniform [-scale, scale]
-        """Constructor
+        self.scale = scale
-        super().__init__(scale = scale)
+    def _init_weight(self, name, arr):
-    """Gaussian Initializer"""
+    """Initialize the weight with normal(0, sigma)
-            raise TypeError("Input array must be NDArray")
+    def _init_weight(self, name, arr):
-        """Implmentation of abs method
+    """Initialize the weight with Xavier initialization scheme."""
-            raise TypeError("Input array must be NDArray")
+"""Online evaluation metric module."""
-from .io import DataIter
+from . import io
-Base = object
+
-    Base = BaseEstimator
+    BASE_ESTIMATOR = BaseEstimator
-            raise ValueError("num_round must be greater than 0")
+def _train(symbol, ctx, input_shape,
-        self.optimizer = get_optimizer(name=optimizer, batch_size=batch_size, **kwargs)
+        self.input_shape = input_shape
-            raise ValueError("input shape is incomplete")
+        # model parameters
-        """fit the model
+    def _init_params(self):
-            print("Time: %.3f" % (toc - tic))
+    def _init_predictor(self):
-        raise NotImplementedError("TODO")
+    def predict(self, X):
-        """load model
+        Parameters
-            saving path
+        Returns
-        raise NotImplementedError("TODO")
+        assert isinstance(X, io.DataIter)
-        """draw model
+    def fit(self, X, y=None, eval_data=None, eval_metric='acc', verbose=True):
-            saving path
+        eval_data : DataIter or numpy.ndarray pair
-"""
+        if self.arg_params is None:
-            raise TypeError('copyto do not support type ' + type(other))
+            raise TypeError('copyto do not support type ' + str(type(other)))
-    """Optimizer factory
+class Optimizer(object):
-        Parameters for optimizer
+    learning_rate : float, optional
-    A required optimizer object
+    momentum : float, optional
-        raise Exception("Not implemented")
+    wd : float, optional
-        """
+    rescale_grad : float, optional
-        self.batch_size = batch_size
+        self.wd = wd
-        ---------
+    def update(self, key, weight, grad):
-            name of weight
+        # TODO(bing) implement wd_bias, wd_gamma, wd_beta
-        mom = self.momentums[states]
+
-        mom[:] += -self.lr * (grad / self.batch_size + self.wd * weight)
+        mom[:] += -self.lr * (grad * self.rescale_grad + self.wd * weight)
-        """Helper function to get ndarray handles from various inputs.
+    def _get_ndarray_inputs(arg_key, args, arg_names, allow_missing):
-                if name in arg_names:
+                if name in args:
-        return c_array(NDArrayHandle, arg_handles)
+        return c_array(NDArrayHandle, arg_handles), arg_arrays
-        """Simply bind current symbol to get an executor.
+        """Bind current symbol to get an executor, allocate all the ndarrays needed.
-        # pylint: enable=unused-variable
+        arg_shapes, _, aux_shapes = self.infer_shape(**kwargs)
-        grad_ndarrays = [zeros(shape, ctx) for shape in arg_shapes]
+
-        executor = self.bind(ctx, arg_ndarrays, grad_ndarrays, req, aux_ndarrays)
+        executor = self.bind(ctx, arg_ndarrays, grad_ndarrays, grad_req, aux_ndarrays)
-        args_handle = self._get_ndarray_handle('args', args, self.list_arguments(), False)
+        args_handle, args = self._get_ndarray_inputs('args', args, self.list_arguments(), False)
-                                                        self.list_arguments(), True)
+            args_grad_handle, args_grad = self._get_ndarray_inputs(
-                                                   self.list_auxiliary_states(), False)
+        aux_args_handle, aux_states = self._get_ndarray_inputs(
-        executor.auxiliary_states = aux_states
+
-args_list = softmax.list_arguments()
+
-block = list(zip(grad_narrays, arg_narrays))
+model = mx.model.FeedForward(softmax, mx.cpu(), data_shape,
-    assert(acc_val > 0.97)
+    model.fit(X=train_dataiter,
-                                       mx_uint(ctx.device_id),
+                                       ctx.device_typeid,
-    devtype2mask = {'cpu': 1, 'gpu': 2}
+    devtype2str = {1: 'cpu', 2: 'gpu'}
-            self.device_mask = device_type.device_mask
+            self.device_typeid = device_type.device_typeid
-            self.device_mask = Context.devtype2mask[device_type]
+            self.device_typeid = Context.devstr2type[device_type]
-        return Context.devmask2type[self.device_mask]
+        return Context.devtype2str[self.device_typeid]
-        ctx.device_mask,
+        ctx.device_typeid,
-        dev_mask = ctypes.c_int()
+        dev_typeid = ctypes.c_int()
-        return Context(Context.devmask2type[dev_mask.value], dev_id.value)
+            self.handle, ctypes.byref(dev_typeid), ctypes.byref(dev_id)))
-            optimizer=sgd, num_round = epoch, batch_size = batch_size)
+    model = mx.model.MXNetModel(ctx=mx.gpu(),
-            self.init_factory(arr)
+            self.init_weight(arr)
-
+        """Get input/output from shape
-    def uniform(self, arr, scale=0.07):
+        Parameter
-    def normal(self, arr, sigma=0.07):
+class Normal(Initializer):
-    def xavier(self, arr):
+class Xavier(Initializer):
-            self.uniform(arr, s)
+            arr[:] = random.uniform(-s, s, arr.shape)
-from .initializer import Initializer
+from .initializer import Xavier
-    def __init__(self, ctx, symbol, optimizer, num_round, batch_size, initializer=Initializer(init_type="xavier"), **kwargs):
+    """MXNet model"""
-        self.optimizer = optimizer
+        self.optimizer = get_optimizer(name=optimizer, batch_size=batch_size, **kwargs)
-        self.optimizer.batch_size = batch_size
+        """fit the model
-                train_acc += eval_metric(out_ndarray.asnumpy(), label)
+                pred[:] = out_ndarray
-                    self.optimizer.update(weight, grad, state)
+                    self.optimizer(weight, grad, state)
-                # eval
+            # eval
-    def save(self):
+    def save(self, path):
-    def load(self):
+    def load(self, path):
-    def draw(self):
+    def draw(self, path):
-def get_optimizer(name, **kwargs):
+
-        return SGD(**kwargs)
+        return SGD(batch_size=batch_size, **kwargs)
-    """A very simple SGD optimizer with Nesterov method
+    """A very simple SGD optimizer with Nesterov method"""
-        self.batch_size = 0
+        self.lr = learning_rate
-    def update(self, weight, grad, states):
+    def __call__(self, weight, grad, states):
-                indptr.append(len(sdata))
+                if isinstance(v, tuple):
-        # TODO(bing): specail treat input data grad
+        req = {}
-        executor = self.bind(ctx, arg_ndarrays, grad_ndarrays, grad_req, aux_ndarrays)
+        executor = self.bind(ctx, arg_ndarrays, grad_ndarrays, req, aux_ndarrays)
-            req_array = c_array(mx_uint, req_array)
+            reqs_array = c_array(mx_uint, req_array)
-x = mx.random.uniform(-1, 1, (10,),mx.gpu())
+
-import sys
+import sys, os
-print executor.debug_str()
+
-
+x = mx.random.uniform(-1, 1, (10,),mx.gpu())
-atexit.register(finalize)
+import atexit
-        raise TypeError("Input array must be NDArray")
+class Initializer(object):
-from .initializer import xavier
+from .initializer import Initializer
-    def __init__(self, ctx, symbol, optimizer, num_round, batch_size, initializer=xavier, **kwargs):
+    def __init__(self, ctx, symbol, optimizer, num_round, batch_size, initializer=Initializer(init_type="xavier"), **kwargs):
-        arg_blocks = list(zip(arg_narrays, grad_narrays, momentum_narrays))
+        arg_blocks = list(zip(arg_narrays, grad_narrays, self.symbol.list_arguments()))
-                narray[:] = 0.0
+        for state, narray in inputs.items():
-                    self.optimizer.update(weight, grad, mom)
+                for weight, grad, state in arg_blocks:
-
+"""
-
+"""
-
+from .ndarray import NDArray, zeros
-    def update(self, weight, grad, mom):
+    def update(self, weight, grad, states):
-        assert(isinstance(mom, NDArray))
+        if states not in self.momentums:
-"""Symbol support of mxnet"""
+"""Symbolic support of mxnet.
-        args : list of NDArray or dict of str->NDArray
+        args : list of NDArray or dict of str to NDArray
-            If type is dict of str->NDArray, then it maps the name of arguments
+            If type is dict of str to NDArray, then it maps the name of arguments
-                    raise TypeError('Only Accept list of NDArrays or dict of str->NDArray')
+                    raise TypeError('Only Accept list of NDArrays or dict of str to NDArray')
-                        raise TypeError('Only Accept list of NDArrays or dict of str->NDArray')
+                        raise TypeError('Only Accept list of NDArrays or dict of str to NDArray')
-            raise TypeError('Only Accept list of NDArrays or dict of str->NDArray')
+            raise TypeError('Only Accept list of NDArrays or dict of str to NDArray')
-            {'write', 'add', 'null'}, or list of str or dict of str->str, optional
+            {'write', 'add', 'null'}, or list of str or dict of str to str, optional
-        kwargs : dict of str->NDArray
+        kwargs : dict of str to NDArray
-        args : list of NDArray or dict of str->NDArray
+        args : list of NDArray or dict of str to NDArray
-              to the corresponding NDArray,
+            - If type is dict of str to NDArray, then it maps the name of arguments
-        args_grad : list of NDArray or dict of str->NDArray, optional
+        args_grad : list of NDArray or dict of str to NDArray, optional
-            - If type is dict of str->NDArray, then it maps the name of arguments
+            - If type is dict of str to NDArray, then it maps the name of arguments
-            - When the type is dict of str->NDArray, users only need to provide the dict
+            - When the type is dict of str to NDArray, users only need to provide the dict
-        grad_req : {'write', 'add', 'null'}, or list of str or dict of str->str, optional
+        grad_req : {'write', 'add', 'null'}, or list of str or dict of str to str, optional
-        aux_states : list of NDArray, or dict of str->NDArray, optional
+        aux_states : list of NDArray, or dict of str to NDArray, optional
-            - If type is dict of str->NDArray, then it maps the name of auxiliary_states
+            - If type is dict of str to NDArray, then it maps the name of auxiliary_states
-    """Create a symbolic variable that groups several symbols together.
+    """Create a symbol that groups symbols together.
-        - /path-to/my-local-symbol
+        The name of the file, examples:
-#check data
+#########################################################
-
+batch_size = 128
-        prefetch_capacity=6)
+        nthread=1)
-
+        nthread=1)
-            progress(train_nbatch, all_train_bacth, i, toc)
+    sgd = mx.optimizer.get_optimizer(name="sgd", learning_rate=0.05, momentum=0.9,
-        test_dataiter.reset()
+from . import optimizer
-        kwargs : dict of str->NDArray
+        kwargs : dict of str->shape
-        arg_shapes, out_shapes, aux_shapes = self.infer_shape(**input_shapes)
+        arg_shapes, out_shapes, aux_shapes = self.infer_shape(**kwargs)
-                arg_ndarrays.append(zeros(shape, ctx))
+        arg_ndarrays = [zeros(shape, ctx) for shape in arg_shapes]
-        run_doxygen('..')
+    run_doxygen('..')
-        subprocess.call('cd ..; rm -rf build', shell = True)
+        if READTHEDOCS_BUILD:
-if os.environ.get('READTHEDOCS', None) == 'True':
+if READTHEDOCS_BUILD or not os.path.exists('../recommonmark'):
-        batch_size=batch_size, shuffle=True, flat=True, silent=False, seed=10)
+        input_shape=(1, 28, 28),
-        batch_size=batch_size, shuffle=True, flat=True, silent=False)
+        input_shape=(1, 28, 28),
-    test_Cifar10Rec()
+    test_MNISTIter()
-            if out.writable == False:
+            if not out.writable:
-            if out.writable == False:
+            if not out.writable:
-
+        path_imgrec="data/cifar/train.rec",
-
+        input_shape=(784,),
-        prefetch_capacity=4)
+        prefetch_capacity=6)
-        prefetch_capacity=4)
+        prefetch_capacity=6)
-        batch_size=batch_size, shuffle=True, silent=False, seed=10)
+        input_shape=(784,),
-        batch_size=batch_size, shuffle=True, silent=False)
+        input_shape=(784,),
-
+        assert(labelcount[i] == 5000)
-
+    You also get the benefit being able to directly load/save from cloud storage(S3, HDFS)
-        The name of the file
+        The name of the file.Can be S3 or HDFS address (remember built with S3 support).
-                                      ctypes.byref(names)))
+    check_call(_LIB.MXNDArrayLoad(c_str(fname),
-        The name of the file
+        The name of the file.Can be S3 or HDFS address (remember built with S3 support).
-                                      keys))
+    check_call(_LIB.MXNDArraySave(c_str(fname),
-def test_compose():
+def test_symbol_compose():
-            prefetch_capacity=4)
+            prefetch_capacity=1)
-        print "Batch: ", batchcount
+        #print label.asnumpy().flatten() 
-    #test_Cifar10Rec()
+    #test_MNISTIter()
-__all__ = ['start', 'init', 'push', 'pull', 'stop', 'set_updater']
+__all__ = ['start', 'init', 'push', 'pull', 'set_updater']
-    """ Stop the kvstore """
+def _cleanup():
-    stop_kv()
+import atexit
-    check_call(_LIB.MXKVStoreStop())
+
-        capacity=4)
+        prefetch_capacity=4)
-        capacity=4)
+        prefetch_capacity=4)
-            capacity=6)
+            prefetch_capacity=4)
-        assert(labelcount[i] == 5000)
+        print labelcount[i]
-    test_Cifar10Rec()
+    test_MNISTIter()
-        nthread=1,
+        nthread=4,
-        nthread=1,
+        nthread=4,
-            if out.writable == True:
+            if out.writable == False:
-            if out.writable == True:
+            if out.writable == False:
-
+    get_data.GetCifar10()
-            mean_img="data/cifar/cifar10_mean_1.bin",
+            mean_img="data/cifar/cifar10_mean.bin",
-            rand_mirror=False,
+            and_mirror=False,
-            capacity=1)
+            nthread=4,
-        assert(labelcount[i] == 1000)
+        assert(labelcount[i] == 5000)
-    #test_Cifar10Rec()
+    #test_MNISTIter()
-print executor.debug_str()
+print(executor.debug_str())
-from .base import check_call
+from .base import mx_uint, NDArrayHandle, ExecutorHandle
-# update
+executor = softmax.bind(mx.cpu(), arg_narrays, grad_narrays, 'write', aux_narrays)
-        self.device_id = device_id
+        if isinstance(device_type, Context):
-    ctx : Context, optional
+    ctx : Context, optional.
-
+# coding: utf-8
-        nthread=1)
+        nthread=1,
-        nthread=1)
+        nthread=1,
-        return NDArray(hdl)
+        return NDArray(hdl, False)
-        return NDArray(hdl)
+        return NDArray(hdl, False)
-    def __init__(self, handle):
+    def __init__(self, handle, writable=True):
-#from PIL import Image
+from PIL import Image
-            rand_mirror=True,
+            path_imgrec="data/cifar/train.rec",
-            nthread=1)
+            nthread=1,
-        exit(0)
+        npdata = data.asnumpy().flatten().sum()
-        nplabel = label.numpy
+        nplabel = label.asnumpy()
-    test_MNISTIter()
+    CheckEqual()
-sys.path.append("../../tests/python")
+sys.path.append("../../tests/python/common/")
-executor = softmax.bind(mx.Context('gpu'), arg_narrays, grad_narrays)
+executor = softmax.bind(mx.gpu(), arg_narrays, grad_narrays)
-    # init a single key-value pair
+    >>> # init a single key-value pair
-    # init a list of key-value pairs
+    >>> # init a list of key-value pairs
-    # push a single key-value pair
+    >>> # push a single key-value pair
-    # aggregate the value and the push
+    >>> # aggregate the value and the push
-    # single device
+    >>> # push a list of keys.
-    # multiple devices:
+    >>> # multiple devices:
-    # pull a single key-value pair
+    >>> # pull a single key-value pair
-    # pull into multiple devices
+    >>> # pull into multiple devices
-    # On single device
+    >>> # pull a list of key-value pairs.
-    # On multiple devices
+    >>> # On multiple devices
-mx.kvstore.start()
+mx.kv.start()
-mx.kvstore.set_updater(updater)
+mx.kv.set_updater(updater)
-    mx.kvstore.init(idx, val)
+    mx.kv.init(idx, val)
-            mx.kvstore.pull(sync_indices, out = sync_weights)
+            mx.kv.pull(sync_indices, out = sync_weights)
-            mx.kvstore.push(sync_indices, sync_grads)
+            mx.kv.push(sync_indices, sync_grads)
-sys.path.append("../../tests/python")
+sys.path.append("../../tests/python/common")
-mx.kvstore.start()
+devs = [mx.Context('cpu', i) for i in range(num_devs)]
-mx.kvstore.set_updater(updater)
+mx.kv.set_updater(updater)
-# init param in the kvstore
+# init param in the kv
-    mx.kvstore.init(idx, val)
+    mx.kv.init(idx, val)
-                mx.kvstore.pull(idx, out = [p[idx] for p in params])
+                mx.kv.pull(idx, out = [p[idx] for p in params])
-                mx.kvstore.push(idx, [g[idx] for g in grads])
+                mx.kv.push(idx, [g[idx] for g in grads])
-from . import kvstore
+from . import kvstore as kv
-    mx.kvstore.start()
+def init_kv():
-    mx.kvstore.init(3, mx.nd.zeros(shape))
+    mx.kv.init(3, mx.nd.zeros(shape))
-    mx.kvstore.init(keys, [mx.nd.zeros(shape)] * len(keys))
+    mx.kv.init(keys, [mx.nd.zeros(shape)] * len(keys))
-    mx.kvstore.stop()
+def stop_kv():
-    init_kvstore()
+    init_kv()
-    mx.kvstore.push(3, mx.nd.ones(shape))
+    mx.kv.push(3, mx.nd.ones(shape))
-    mx.kvstore.pull(3, out = val)
+    mx.kv.pull(3, out = val)
-    stop_kvstore()
+    stop_kv()
-    init_kvstore()
+    init_kv()
-    mx.kvstore.push(keys, [mx.nd.ones(shape)*4] * len(keys))
+    mx.kv.push(keys, [mx.nd.ones(shape)*4] * len(keys))
-    mx.kvstore.pull(keys, out = val)
+    mx.kv.pull(keys, out = val)
-    stop_kvstore()
+    stop_kv()
-    init_kvstore()
+    init_kv()
-    mx.kvstore.pull(3, out = vals)
+    mx.kv.push(3, vals)
-    mx.kvstore.pull(keys, out = vals)
+    mx.kv.push(keys, vals)
-    stop_kvstore()
+    stop_kv()
-    mx.kvstore.set_updater(updater)
+    init_kv()
-    mx.kvstore.pull(3, out = vals)
+    mx.kv.push(3, vals)
-        mx.kvstore.push(keys, vals)
+        mx.kv.push(keys, vals)
-    mx.kvstore.pull(keys, out = vals)
+    mx.kv.pull(keys, out = vals)
-    stop_kvstore()
+    stop_kv()
-    """parse key-value args into ctype"""
+    """
-    """start kvstore"""
+    """
-    """ Initialize a list of key-value pairs
+    """ Initialize a single or a sequence of key-value pairs into the store.
-        A single value of a list of values
+    key : int or sequence of int
-    """ Push a value into the store
+    """ Push a single or a sequence of key-value pairs into the store
-        A single value of a list of value
+        Keys
-    """Pull value from the store
+    """ Pull a single value or a sequence of values from the store
-        A single value of a list of value
+    key : int or list of int
-    kvstore.set_updater(updater)
+    """
-    updater: functon
+    updater: function
-    """ Stop kvstore """
+    """ Stop the kvstore """
-# html_theme = 'alabaster'
+html_theme = 'sphinx_rtd_theme'
-devs = [mx.cpu(i) for i in range(num_devs)]
+num_devs = 4
-devs = [mx.gpu(i) for i in range(num_devs)]
+num_devs = 1
-updater = mx.updater.momentum(
+
-    epoch = 1
+    epoch = 7
-devs = [mx.cpu(i) for i in range(num_devs)]
+num_devs = 4
-# mx.kvstore.set_updater(updater)
+mx.kvstore.set_updater(updater)
-batch_size = 128
+batch_size = 196
-data_shape = (batch_size, 3, 28, 28)
+data_shape = (batch_size / num_devs, 3, 28, 28)
-
+sync_prefix = ["weight", "bias", "beta", "gamma"]
-                if "weight" in name or "bias" in name]
+                if any(prefix in name for prefix in sync_prefix)]
-# init global shared model
+# init model
-
+
-        nthread=1)
+    path_imgrec="data/cifar/train.rec",
-        nthread=1)
+    path_imgrec="data/cifar/test.rec",
-def progress(count, total, epoch, toc):
+def progress(count, total, epoch, tic):
-    speed = batch_size / float(tic - toc)
+    toc = time.time()
-    epoch = 10
+    epoch = 1
-        all_train_bacth = round(50000 / float(batch_size) + 1)
+        train_count = 0
-                label_in[d] = label[rows]
+                data_in[d][:] = data[rows, :]
-            progress(train_count, all_train_bacth, i, toc)
+            progress(train_count, all_train_bacth, i, tic)
-        for data, label in test_dataiter:
+        for data, label in val_dataiter:
-                data_in[d] = data[rows,:]
+                data_in[d][:] = data[rows,:]
-        print("Train Acc: %g, Valid Acc: %g" % (
+        print("Train Acc: %g, Valid Acc: %g, Time: %g sec" % (
-            val_acc / val_count))
+            val_acc / val_count,
-        test_dataiter.reset()
+        val_dataiter.reset()
-    mx.kvstore.stop()
+import atexit
-
+
-
+from __future__ import absolute_import
-
+    momentums = {}
-
+                executors[d].backward()
-                train_acc += cal_acc(forward_out[d].asnumpy(),
+                train_acc += cal_acc(executors[d].outputs[0].asnumpy(),
-loss = mx.symbol.Softmax(data=fc, name="sm")
+loss = mx.symbol.Softmax(data=fc, name="loss")
-    executors.append(loss.simple_bind(d, data = mx.nd.empty(data_shape, d)))
+executors = [loss.simple_bind(d, data = mx.nd.empty(data_shape, d)) for d in devs]
-    shape = sync_weights[0][idx].shape
+    shape = weights[idx].shape
-sys.path.append("../../tests/python")
+sys.path.append("../../tests/python/common")
-mx.kvstore.start()
+devs = [mx.cpu(i) for i in range(num_devs)]
-#check data
+# mx.kvstore.set_updater(updater)
-test_dataiter = mx.io.ImageRecordIter(
+val_dataiter = mx.io.ImageRecordIter(
-            toc = time.time()
+            # pull weight
-            progress(train_nbatch, all_train_bacth, i, toc)
+            for d in range(num_devs):
-        acc_val = val_acc / val_nbatch
+            for d in range(num_devs):
-        print("Valid Acc: ", val_acc / val_nbatch)
+
-sys.path.append("../../tests/python")
+sys.path.append("../../tests/python/common")
-batch_size = 100
+        start = time.time()
-        print("Valid Acc: ", val_acc / val_count)
+        print("Train Acc: %g, Valid Acc: %g, time: %g" % (
-forward_out = [mx.nd.zeros(e.heads()[0].shape) for e in executors]
+forward_out = [mx.nd.zeros(e.outputs[0].shape) for e in executors]
-                executors[d].heads()[0].copyto(forward_out[d])
+                executors[d].outputs[0].copyto(forward_out[d])
-                val_acc += cal_acc(executors[d].heads()[0].asnumpy(),
+                val_acc += cal_acc(executors[d].outputs[0].asnumpy(),
-        check_call(_LIB.MXExecutorOutputs(self.handle, ctypes.byref(out_size), ctypes.byref(handles)))
+        check_call(_LIB.MXExecutorOutputs(self.handle,
-out_narray = executor.heads()[0]
+out_narray = executor.outputs[0]
-grad_narrays = [mx.nd.zeros(shape, ctx=mx.Context("gpu")) for shape in arg_shapes]
+arg_narrays = [mx.nd.zeros(shape, ctx=mx.gpu()) for shape in arg_shapes]
-grad_narray = mx.nd.zeros(out_narray.shape, ctx=mx.Context("gpu"))
+out_narray = executor.outputs[0]
-    def heads(self):
+    @property
-        check_call(_LIB.MXExecutorHeads(self.handle, ctypes.byref(out_size), ctypes.byref(handles)))
+        check_call(_LIB.MXExecutorOutputs(self.handle, ctypes.byref(out_size), ctypes.byref(handles)))
-        """List all returns in the symbol.
+    def list_outputs(self):
-            List of all the returns.
+            List of all the outputs.
-        check_call(_LIB.MXSymbolListReturns(
+        check_call(_LIB.MXSymbolListOutputs(
-            The order is in the same order as list_returns()
+            The order is in the same order as list_outputs()
-out_narray = executor.heads()[0]
+out_narray = executor.outputs[0]
-out_narray = executor.heads()[0]
+out_narray = executor.outputs[0]
-    out2 = executor.heads()[0].asnumpy()
+    out2 = executor.outputs[0].asnumpy()
-    out4 = exec4.heads()[0].asnumpy()
+    out3 = exec3.outputs[0].asnumpy()
-    out1 = exec1.heads()[0].asnumpy()
+    out1 = exec1.outputs[0].asnumpy()
-    out1 = exec1.heads()[0].asnumpy()
+    out1 = exec1.outputs[0].asnumpy()
-    out1 = exec1.heads()[0]
+    out1 = exec1.outputs[0]
-        m.list_returns()
+        m.list_outputs()
-    assert len(multi_out.list_returns()) == 2
+    assert len(multi_out.list_outputs()) == 2
-                                                 ctypes.byref(cptr)))
+                                                  ctypes.byref(length),
-                                     ctypes.byref(names)))
+                                      ctypes.byref(out_size),
-                                     keys))
+                                      len(handles),
-                                                       self.list_arguments(), True)
+                                                        self.list_arguments(), True)
-                                                  self.list_auxiliary_states(), False)
+                                                   self.list_auxiliary_states(), False)
-    tmp = mx.narray.array(np.random.uniform(-a, a, narray.shape))
+    tmp = mx.nd.array(np.random.uniform(-a, a, narray.shape))
-in_data = mx.narray.empty(data_shape, mx.gpu())
+in_data = mx.nd.empty(data_shape, mx.gpu())
-pred = mx.narray.zeros(out_narray.shape, mx.cpu())
+pred = mx.nd.zeros(out_narray.shape, mx.cpu())
-momentum_narrays = [mx.narray.zeros(item.shape, mx.gpu()) for item in grad_narrays]
+tmp_label = mx.nd.zeros(inputs["sm_label"].shape)
-grad_narrays = [mx.narray.zeros(shape, ctx=mx.Context("gpu")) for shape in arg_shapes]
+arg_narrays = [mx.nd.zeros(shape, ctx=mx.Context("gpu")) for shape in arg_shapes]
-pred = mx.narray.zeros(out_shapes[0])
+pred = mx.nd.zeros(out_shapes[0])
-        tmp = mx.narray.array(np.random.uniform(-0.07, 0.07, name2shape[name]))
+        tmp = mx.nd.array(np.random.uniform(-0.07, 0.07, name2shape[name]))
-grad_narray = mx.narray.zeros(out_narray.shape, ctx=mx.Context("gpu"))
+grad_narray = mx.nd.zeros(out_narray.shape, ctx=mx.Context("gpu"))
-tmp_label = mx.narray.zeros(name2shape["sm_label"])
+tmp_label = mx.nd.zeros(name2shape["sm_label"])
-    val = mx.narray.zeros(shape)
+    val = mx.nd.zeros(shape)
-grads = [[mx.narray.zeros(s, d) for s in param_shapes] for d in devs]
+params = [[mx.nd.zeros(s, d) for s in param_shapes] for d in devs]
-forward_out = [mx.narray.zeros(e.heads()[0].shape) for e in executors]
+forward_out = [mx.nd.zeros(e.heads()[0].shape) for e in executors]
-from . import narray
+from . import ndarray
-NArrayHandle = ctypes.c_void_p
+NDArrayHandle = ctypes.c_void_p
-        shape of target narray
+        shape of target ndarray
-from .base import c_array, mx_uint, NArrayHandle, ExecutorHandle
+from .base import c_array, mx_uint, NDArrayHandle, ExecutorHandle
-from .narray import NArray
+from .ndarray import NDArray
-        self.grad_narrays = []
+        self.arg_ndarrays = []
-            return self.arg_narrays, self.grad_narrays
+            return self.arg_ndarrays, self.grad_ndarrays
-            return self.arg_narrays
+            return self.arg_ndarrays
-        head_grads : NArray or list of NArray, optional
+        head_grads : NDArray or list of NDArray, optional
-        elif isinstance(head_grads, NArray):
+        elif isinstance(head_grads, NDArray):
-        check_call(_LIB.MXExecutorBackward(self.handle, len(head_grads), narray))
+            if not isinstance(obj, NDArray):
-        """list all heads' output narray
+        """list all heads' output ndarray
-        A list of narray binded to the heads of executor.
+        A list of ndarray binded to the heads of executor.
-        # (consider support read only NArray(NArrayView))
+        # (consider support read only NDArray(NDArrayView))
-        handles = ctypes.POINTER(NArrayHandle)()
+        handles = ctypes.POINTER(NDArrayHandle)()
-        return [NArray(NArrayHandle(handles[i])) for i in range(out_size.value)]
+        return [NDArray(NDArrayHandle(handles[i])) for i in range(out_size.value)]
-"""NArray interface of mxnet"""
+"""NDArray interface of mxnet"""
-from .base import DataIterHandle, NArrayHandle
+from .base import DataIterHandle, NDArrayHandle
-from .narray import NArray
+from .ndarray import NDArray
-        hdl = NArrayHandle()
+        hdl = NDArrayHandle()
-        return NArray(hdl)
+        return NDArray(hdl)
-        hdl = NArrayHandle()
+        hdl = NDArrayHandle()
-        return NArray(hdl)
+        return NDArray(hdl)
-from .narray import NArray
+from .ndarray import NDArray
-from .base import check_call, c_array, NArrayHandle
+from .base import check_call, c_array, NDArrayHandle
-        if isinstance(vals, NArray):
+        if isinstance(vals, NDArray):
-                    c_array(NArrayHandle, [vals.handle]))
+                    c_array(NDArrayHandle, [vals.handle]))
-                assert(isinstance(v, NArray))
+                assert(isinstance(v, NDArray))
-                    c_array(NArrayHandle, [v.handle for v in vals]))
+                    c_array(NDArrayHandle, [v.handle for v in vals]))
-        return (c_array(ctypes.c_int, c_keys), c_array(NArrayHandle, c_vals))
+        return (c_array(ctypes.c_int, c_keys), c_array(NDArrayHandle, c_vals))
-    values: NArray or list of NArray
+    values: NDArray or list of NDArray
-    value: list of NArray or list of list of NArray
+    value: list of NDArray or list of list of NDArray
-    out: NArray or list of NArray
+    out: NDArray or list of NDArray
-        rhs = NArray(NArrayHandle(rhs_handle))
+        lhs = NDArray(NDArrayHandle(lhs_handle))
-        None, ctypes.c_int, NArrayHandle, NArrayHandle)
+        None, ctypes.c_int, NDArrayHandle, NDArrayHandle)
-"""NArray interface of mxnet"""
+"""NDArray interface of mxnet"""
-from .base import mx_uint, mx_float, NArrayHandle, FunctionHandle
+from .base import mx_uint, mx_float, NDArrayHandle, FunctionHandle
-    a new empty narray handle
+    a new empty ndarray handle
-    check_call(_LIB.MXNArrayCreateNone(ctypes.byref(hdl)))
+    hdl = NDArrayHandle()
-    a new empty narray handle
+    a new empty ndarray handle
-    check_call(_LIB.MXNArrayCreate(
+    hdl = NDArrayHandle()
-    """NArray object in mxnet.
+class NDArray(object):
-    NArray is basic ndarray/Tensor like data structure in mxnet.
+    NDArray is basic ndarray/Tensor like data structure in mxnet.
-        """initialize a new NArray
+        """initialize a new NDArray
-            NArray handle of C API
+        handle : NDArrayHandle
-        assert isinstance(handle, NArrayHandle)
+        assert isinstance(handle, NDArrayHandle)
-        check_call(_LIB.MXNArrayFree(self.handle))
+        check_call(_LIB.MXNDArrayFree(self.handle))
-            return NArray._plus(self, other)
+        if isinstance(other, NDArray):
-            return NArray._plus_scalar(self, float(other))
+            return NDArray._plus_scalar(self, float(other))
-            return NArray._plus(self, other, out=self)
+        if isinstance(other, NDArray):
-            return NArray._plus_scalar(self, float(other), out=self)
+            return NDArray._plus_scalar(self, float(other), out=self)
-            return NArray._minus(self, other)
+        if isinstance(other, NDArray):
-            return NArray._minus_scalar(self, float(other))
+            return NDArray._minus_scalar(self, float(other))
-            return NArray._minus(self, other, out=self)
+        if isinstance(other, NDArray):
-            return NArray._minus_scalar(self, float(other), out=self)
+            return NDArray._minus_scalar(self, float(other), out=self)
-            return NArray._rminus_scalar(self, float(other))
+            return NDArray._rminus_scalar(self, float(other))
-            return NArray._mul(self, other)
+        if isinstance(other, NDArray):
-            return NArray._mul_scalar(self, float(other))
+            return NDArray._mul_scalar(self, float(other))
-        return NArray._mul_scalar(self, -1.0, out=self)
+        return NDArray._mul_scalar(self, -1.0, out=self)
-            return NArray._mul(self, other, out=self)
+        if isinstance(other, NDArray):
-            return NArray._mul_scalar(self, float(other), out=self)
+            return NDArray._mul_scalar(self, float(other), out=self)
-            return NArray._div(self, other)
+        if isinstance(other, NDArray):
-            return NArray._div_scalar(self, float(other))
+            return NDArray._div_scalar(self, float(other))
-            return NArray._rdiv_scalar(self, float(other))
+            return NDArray._rdiv_scalar(self, float(other))
-            return NArray._div(self, other, out=self)
+        if isinstance(other, NDArray):
-            return NArray._div_scalar(self, float(other), out=self)
+            return NDArray._div_scalar(self, float(other), out=self)
-            check_call(_LIB.MXNArraySaveRawBytes(self.handle,
+            check_call(_LIB.MXNDArraySaveRawBytes(self.handle,
-            handle = NArrayHandle()
+            handle = NDArrayHandle()
-            check_call(_LIB.MXNArrayLoadFromRawBytes(ptr, length, ctypes.byref(handle)))
+            check_call(_LIB.MXNDArrayLoadFromRawBytes(ptr, length, ctypes.byref(handle)))
-        """Set narray value"""
+        """Set ndarray value"""
-        if isinstance(value, NArray):
+            raise Exception("Set NDArray should use empty index array[:] = target_array")
-            NArray._set_value(float(value), out=self)
+            NDArray._set_value(float(value), out=self)
-        """Get narray"""
+        """Get ndarray"""
-            raise Exception("Set NArray should use empty index array[:] += value")
+            raise Exception("Set NDArray should use empty index array[:] += value")
-            raise ValueError('array shape do not match the shape of NArray')
+            raise ValueError('array shape do not match the shape of NDArray')
-        check_call(_LIB.MXNArraySyncCopyFromCPU(
+        check_call(_LIB.MXNDArraySyncCopyFromCPU(
-        """Block until all pending writes operations on current NArray are finished.
+        """Block until all pending writes operations on current NDArray are finished.
-        NArray finishes. There can still be pending read going on when the
+        NDArray finishes. There can still be pending read going on when the
-        check_call(_LIB.MXNArrayWaitToRead(self.handle))
+        check_call(_LIB.MXNDArrayWaitToRead(self.handle))
-        """Block until all pending read/write operations on current NArray are finished.
+        """Block until all pending read/write operations on current NDArray are finished.
-        NArray finishes. There can still be pending read going on when the
+        NDArray finishes. There can still be pending read going on when the
-        check_call(_LIB.MXNArrayWaitToWrite(self.handle))
+        check_call(_LIB.MXNDArrayWaitToWrite(self.handle))
-        """Get shape of current NArray.
+        """Get shape of current NDArray.
-        a tuple representing shape of current narray
+        a tuple representing shape of current ndarray
-        check_call(_LIB.MXNArrayGetShape(
+        check_call(_LIB.MXNDArrayGetShape(
-        """Get context of current NArray.
+        """Get context of current NDArray.
-            The context of current NArray.
+            The context of current NDArray.
-        check_call(_LIB.MXNArrayGetContext(
+        check_call(_LIB.MXNDArrayGetContext(
-        check_call(_LIB.MXNArraySyncCopyToCPU(
+        check_call(_LIB.MXNDArraySyncCopyToCPU(
-        When other is a Context, a new NArray in the context
+        When other is NDArray, the content is copied over.
-            Target Narray or context we want to copy data to.
+        other : NDArray or Context
-            The copy target NArray
+        dst : NDArray
-        if isinstance(other, NArray):
+        if isinstance(other, NDArray):
-            return NArray._copyto(self, out=other)
+            return NDArray._copyto(self, out=other)
-            return NArray._copyto(self, out=hret)
+            hret = NDArray(_new_alloc_handle(self.shape, other, True))
-    """Create an empty uninitialized new NArray, with specified shape.
+    """Create an empty uninitialized new NDArray, with specified shape.
-        shape of the NArray.
+        shape of the NDArray.
-        The context of the NArray, default to current default context.
+        The context of the NDArray, default to current default context.
-        The created NArray.
+        The created NDArray.
-    return NArray(handle=_new_alloc_handle(shape, ctx, False))
+    return NDArray(handle=_new_alloc_handle(shape, ctx, False))
-    """Create a new NArray filled with 0, with specified shape.
+    """Create a new NDArray filled with 0, with specified shape.
-        shape of the NArray.
+        shape of the NDArray.
-        The context of the NArray, default to current default context.
+        The context of the NDArray, default to current default context.
-        The created NArray.
+        The created NDArray.
-    """Create a new NArray filled with 1, with specified shape.
+    """Create a new NDArray filled with 1, with specified shape.
-        shape of the NArray.
+        shape of the NDArray.
-        The context of the NArray, default to current default context.
+        The context of the NDArray, default to current default context.
-        The created NArray.
+        The created NDArray.
-    """Create a new NArray that copies content from source_array.
+    """Create a new NDArray that copies content from source_array.
-        Source data to create NArray from.
+        Source data to create NDArray from.
-        The context of the NArray, default to current default context.
+        The context of the NDArray, default to current default context.
-        The created NArray.
+        The created NDArray.
-    """Load narray from binary file.
+    """Load ndarray from binary file.
-        List of NArray or dict of str->NArray, depending on what was saved.
+    out : list of NDArray or dict of str to NDArray
-    handles = ctypes.POINTER(NArrayHandle)()
+    handles = ctypes.POINTER(NDArrayHandle)()
-    check_call(_LIB.MXNArrayListLoad(c_str(fname),
+    check_call(_LIB.MXNDArrayListLoad(c_str(fname),
-        return [NArray(NArrayHandle(handles[i])) for i in range(out_size.value)]
+        return [NDArray(NDArrayHandle(handles[i])) for i in range(out_size.value)]
-            (py_str(names[i]), NArray(NArrayHandle(handles[i]))) for i in range(out_size.value))
+            (py_str(names[i]), NDArray(NDArrayHandle(handles[i]))) for i in range(out_size.value))
-    """Save list of NArray or dict of str->NArray to binary file.
+    """Save list of NDArray or dict of str->NDArray to binary file.
-    data : list of NArray or dict of str to NArray
+    data : list of NDArray or dict of str to NDArray
-                raise TypeError('save only accept dict str->NArray or list of NArray')
+                raise TypeError('save only accept dict str->NDArray or list of NDArray')
-                raise TypeError('save only accept dict str->NArray or list of NArray')
+            if not isinstance(val, NDArray):
-    check_call(_LIB.MXNArrayListSave(c_str(fname),
+    check_call(_LIB.MXNDArrayListSave(c_str(fname),
-                                     c_array(NArrayHandle, handles),
+                                     c_array(NDArrayHandle, handles),
-    NARRAY_ARG_BEFORE_SCALAR = 1
+def _make_ndarray_function(handle):
-    # Get the property of NArray
+    # Get the property of NDArray
-    if (type_mask & NARRAY_ARG_BEFORE_SCALAR) != 0:
+    if (type_mask & NDARRAY_ARG_BEFORE_SCALAR) != 0:
-               '    The output NArray to hold the result.\n\n'+
+               'out : NDArray, optional\n' +
-               'out : NArray\n'+
+               'out : NDArray\n'+
-    def binary_narray_function(lhs, rhs, out=None):
+    def binary_ndarray_function(lhs, rhs, out=None):
-                raise TypeError('out must be NArray')
+            if isinstance(out, NDArray) == False:
-            out = NArray(_new_empty_handle())
+            out = NDArray(_new_empty_handle())
-                                     c_array(NArrayHandle, (lhs.handle, rhs.handle)),
+                                     c_array(NDArrayHandle, (lhs.handle, rhs.handle)),
-                                     c_array(NArrayHandle, (out.handle,))))
+                                     c_array(NDArrayHandle, (out.handle,))))
-        """internal NArray function"""
+    def unary_ndarray_function(src, out=None):
-                raise TypeError('out must be NArray')
+            if isinstance(out, NDArray) == False:
-            out = NArray(_new_empty_handle())
+            out = NDArray(_new_empty_handle())
-                c_array(NArrayHandle, (src.handle)), \
+                c_array(NDArrayHandle, (src.handle)), \
-                c_array(NArrayHandle, (out.handle,))))
+                c_array(NDArrayHandle, (out.handle,))))
-    def generic_narray_function(*args, **kwargs):
+    def generic_ndarray_function(*args, **kwargs):
-            Output NArray, used to hold the output result.
+            Positional arguments of input scalars and NDArray
-            The result NArray(tuple) of result of computation.
+        out : NDArray
-            if isinstance(mutate_vars, NArray):
+            if isinstance(mutate_vars, NDArray):
-                    NArray(_new_empty_handle()) for i in range(n_mutate_vars))
+                    NDArray(_new_empty_handle()) for i in range(n_mutate_vars))
-                c_array(NArrayHandle, [args[i].handle for i in use_vars_range]), \
+                c_array(NDArrayHandle, [args[i].handle for i in use_vars_range]), \
-                c_array(NArrayHandle, [v.handle for v in mutate_vars])))
+                c_array(NDArrayHandle, [v.handle for v in mutate_vars])))
-        ret_function = binary_narray_function
+        ret_function = binary_ndarray_function
-        ret_function = unary_narray_function
+        ret_function = unary_ndarray_function
-        ret_function = generic_narray_function
+        ret_function = generic_ndarray_function
-    """List and add all the narray functions to current module."""
+def _init_ndarray_module():
-        # if function name starts with underscore, register as static method of NArray
+        function = _make_ndarray_function(hdl)
-            setattr(NArray, function.__name__, staticmethod(function))
+            setattr(NDArray, function.__name__, staticmethod(function))
-_init_narray_module()
+# Initialize the NDArray module
-from .base import NArrayHandle, ExecutorHandle, SymbolHandle
+from .base import NDArrayHandle, ExecutorHandle, SymbolHandle
-from .narray import NArray, zeros
+from .ndarray import NDArray, zeros
-        """Helper function to get narray handles from various inputs.
+    def _get_ndarray_handle(arg_key, args, arg_names, allow_missing):
-        args : list of NArray or dict of str->NArray
+        args : list of NDArray or dict of str->NDArray
-            to the corresponding NArray,
+            If type is list of NDArray, the position is in the same order of arg_names.
-            The positional list of NArrayHandles generated from input.
+        handles : list of NDArrayHandle
-                    raise TypeError('Only Accept list of NArrays or dict of str->NArray')
+                if not isinstance(narr, NDArray):
-                        raise TypeError('Only Accept list of NArrays or dict of str->NArray')
+                    if not isinstance(narr, NDArray):
-        return c_array(NArrayHandle, arg_handles)
+            raise TypeError('Only Accept list of NDArrays or dict of str->NDArray')
-        """Simply bind current symbol to get an executor
+        """Simply bind current symbol to get an executor.
-            - 'add' means everytime gradient is add to the specified NArray.
+            - 'write' means everytime gradient is write to specified args_grad NDArray.
-            - Not all the arguments must be provided.
+        kwargs : dict of str->NDArray
-        arg_narrays = []
+        arg_ndarrays = []
-                arg_narrays.append(kwargs[name])
+                arg_ndarrays.append(kwargs[name])
-                arg_narrays.append(zeros(shape, ctx))
+                arg_ndarrays.append(zeros(shape, ctx))
-        executor = self.bind(ctx, arg_narrays, grad_narrays, grad_req, aux_narrays)
+        grad_ndarrays = [zeros(shape, ctx) for shape in arg_shapes]
-        args : list of NArray or dict of str->NArray
+        args : list of NDArray or dict of str->NDArray
-              to the corresponding NArray,
+            - If type is list of NDArray, the position is in the same order of list_arguments.
-            When specified, args_grad provide NArrays to hold
+        args_grad : list of NDArray or dict of str->NDArray, optional
-            - When the type is dict of str->NArray, users only need to provide the dict
+            - If type is list of NDArray, the position is in the same order of list_arguments.
-            - 'add' means everytime gradient is add to the specified NArray.
+            - 'write' means everytime gradient is write to specified args_grad NDArray.
-        aux_states : list of NArray, or dict of str->NArray, optional
+        aux_states : list of NDArray, or dict of str->NDArray, optional
-              to the corresponding NArray,
+            - If type is list of NDArray, the position is in the same order of list_auxiliary_states
-        args_handle = self._get_narray_handle('args', args, self.list_arguments(), False)
+        args_handle = self._get_ndarray_handle('args', args, self.list_arguments(), False)
-            args_grad_handle = c_array(NArrayHandle, [None] * len(args))
+            args_grad_handle = c_array(NDArrayHandle, [None] * len(args))
-            args_grad_handle = self._get_narray_handle('args_grad', args_grad,
+            args_grad_handle = self._get_ndarray_handle('args_grad', args_grad,
-        aux_args_handle = self._get_narray_handle('aux_states', aux_states,
+        aux_args_handle = self._get_ndarray_handle('aux_states', aux_states,
-        executor.grad_narrays = args_grad
+        executor.arg_ndarrays = args
-aux_narrays = [mx.narray.empty(shape) for shape in aux_shapes]
+arg_narrays = [mx.nd.empty(shape) for shape in arg_shapes]
-grad_narray = mx.narray.empty(out_narray.shape)
+grad_narray = mx.nd.empty(out_narray.shape)
-                
+
-grad_narrays = [mx.narray.empty(shape) for shape in arg_shapes]
+arg_narrays = [mx.nd.empty(shape) for shape in arg_shapes]
-grad_narray = mx.narray.empty(out_narray.shape)
+grad_narray = mx.nd.empty(out_narray.shape)
-    rhs_grad = mx.narray.empty(shape)
+    lhs_arr = mx.nd.array(np.random.uniform(-10, 10, shape))
-    out_grad = mx.narray.array(np.ones(shape))
+    out_grad = mx.nd.array(np.ones(shape))
-    test_bind()
+    test_bind()
-    mx.kvstore.init(3, mx.narray.zeros(shape))
+    mx.kvstore.init(3, mx.nd.zeros(shape))
-    mx.kvstore.init(keys, [mx.narray.zeros(shape)] * len(keys))
+    mx.kvstore.init(keys, [mx.nd.zeros(shape)] * len(keys))
-    val = mx.narray.empty(shape)
+    mx.kvstore.push(3, mx.nd.ones(shape))
-    val = [mx.narray.empty(shape)] * len(keys)
+    mx.kvstore.push(keys, [mx.nd.ones(shape)*4] * len(keys))
-    vals = [mx.narray.ones(shape, d) for d in devs]
+    vals = [mx.nd.ones(shape, d) for d in devs]
-    vals = [[mx.narray.ones(shape, d)*2.0 for d in devs]] * len(keys)
+    vals = [[mx.nd.ones(shape, d)*2.0 for d in devs]] * len(keys)
-    vals = [mx.narray.ones(shape, d) for d in devs]
+    vals = [mx.nd.ones(shape, d) for d in devs]
-    vals = [[mx.narray.ones(shape, d) for d in devs]] * len(keys)
+    vals = [[mx.nd.ones(shape, d) for d in devs]] * len(keys)
-        narr = mx.narray.array(npy)
+        narr = mx.nd.array(npy)
-    data= mx.narray.array(np.random.uniform(-10, 10, shape))
+    data= mx.nd.array(np.random.uniform(-10, 10, shape))
-    c = mx.narray.array(np.random.uniform(-10, 10, (10, 10)))
+    c = mx.nd.array(np.random.uniform(-10, 10, (10, 10)))
-    d = mx.narray.empty((10,10))
+    c = mx.nd.empty((10,10))
-            b = mx.narray.empty(a.shape)
+            b = mx.nd.empty(a.shape)
-        data2 = mx.narray.load(fname)
+        mx.nd.save(fname, data)
-        dmap2 = mx.narray.load(fname)
+        mx.nd.save(fname, dmap)
-    arr_grad = [mx.narray.empty(shape) for i in range(n)]
+    arr = [mx.nd.empty(shape) for i in range(n)]
-    out_grad = mx.narray.empty(shape)
+    out_grad = mx.nd.empty(shape)
-    arr = [mx.narray.empty(shape) for shape in shapes]
+    arr = [mx.nd.empty(shape) for shape in shapes]
-    arr_grad = [mx.narray.empty(shape) for shape in shapes]
+    arr_grad = [mx.nd.empty(shape) for shape in shapes]
-    out_grad = mx.narray.empty(out_shapes[0])
+    out_grad = mx.nd.empty(out_shapes[0])
-executor = loss.simple_bind(mx.gpu(), {"data": in_data})
+executor = loss.simple_bind(mx.gpu(), data = in_data)
-    def simple_bind(self, ctx, args, grad_req='write'):
+    def simple_bind(self, ctx, grad_req='write', **kwargs):
-        args : list of NArray or dict of str->NArray
+        grad_req: string
-        input_shapes = dict((name, arr.shape) for name, arr in args.items())
+        input_shapes = dict((name, arr.shape) for name, arr in kwargs.items())
-                arg_narrays.append(args[name])
+            if name in kwargs:
-import get_data
+from common import get_data
-
+from common import get_data
-import models
+from common import models
-import get_data
+from common import get_data
-    test_MNISTIter()
+    test_MNISTIter()
-import models
+from common import models
-        os.chidr("..")
+        os.chdir("..")
-pred = mx.narray.zeros(out_narray.shape)
+pred = mx.narray.zeros(out_narray.shape, mx.cpu())
-block = zip(grad_narrays, arg_narrays, momentum_narrays)
+block = list(zip(grad_narrays, arg_narrays, momentum_narrays))
-            executor.backward([out_narray])
+            #executor.backward([out_narray])
-    Return CPU context
+    """Return a CPU context.
-            the device id of the device, needed for GPU
+    device_id : int, optional
-    A cpu context
+    -------
-    Return CPU context
+    """Return a GPU context.
-            the device id of the device, needed for GPU
+    device_id : int, optional
-    A cpu context
+    -------
-    def list_auxiliary_states():
+    def list_auxiliary_states(self):
-    def backward(self, grads):
+    def backward(self, head_grads=None):
-            heads' gradient
+        head_grads : NArray or list of NArray, optional
-        for obj in grads:
+        if head_grads is None:
-        check_call(_LIB.MXExecutorBackward(self.handle, len(grads), narray))
+        narray = c_array(NArrayHandle, [item.handle for item in head_grads])
-
+        # pylint: disable=too-many-locals
-        return Executor(handle)
+        executor = Executor(handle)
-def test_updater(dev):
+def test_updater(dev = 'cpu'):
-    test_updater('cpu')
+    test_updater()
-# pylint: disable=invalid-name, global-variable-undefined,
+# pylint: disable=invalid-name, global-statement
-def test_updater():
+def test_updater(dev):
-    devs = [mx.Context('cpu', i) for i in range(num_devs)]
+    devs = [mx.Context(dev, i) for i in range(num_devs)]
-    test_updater()
+    test_updater('cpu')
-    """ parse key-value args into ctype"""
+    """parse key-value args into ctype"""
-                    c_array(ctypes.c_int, [keys]),
+            return (c_array(ctypes.c_int, [keys]),
-                    c_array(ctypes.c_int, [keys] * len(vals)),
+            return (c_array(ctypes.c_int, [keys] * len(vals)),
-                c_array(NArrayHandle, [v.handle for v in vals]))
+        c_keys = []
-def init(keys, values):
+def init(key, value):
-    check_call(_LIB.MXKVStoreInit(num, ckeys, cvals))
+    ckeys, cvals = _ctype_key_value(key, value)
-def push(keys, values):
+def push(key, value):
-        A single value of a list of values
+    key : int or list of int
-    check_call(_LIB.MXKVStorePush(num, ckeys, cvals))
+    ckeys, cvals = _ctype_key_value(key, value)
-    """ Pull the value from the store
+def pull(key, out=None):
-        A single value of a list of values
+    key: int or list of int
-    # check_call(_LIB.MXKVStorePull(num, ckeys, cvals))
+    assert(out is not None)
-    def updater_handle(lhs_handle, rhs_handle):
+    def updater_handle(key, lhs_handle, rhs_handle):
-        updater(lhs, rhs)
+        updater(key, lhs, rhs)
-    _updater_proto = ctypes.CFUNCTYPE(None, NArrayHandle, NArrayHandle)
+    _updater_proto = ctypes.CFUNCTYPE(
-def test_aggregator():
+def test_single_kv_pair():
-    mx.kvstore.start()
+    init_kvstore()
-    # mx.kvstore.init_devices(devs)
+    mx.kvstore.push(3, mx.narray.ones(shape))
-    keys = (5, 9)
+    stop_kvstore()
-    mx.kvstore.init(keys, [mx.narray.zeros(shape) for k in keys])
+def test_list_kv_pair():
-    mx.kvstore.push(keys[0], vals)
+    mx.kvstore.push(keys, [mx.narray.ones(shape)*4] * len(keys))
-    # mx.kvstore.pull(keys[0], out)
+def test_aggregator():
-    # check_diff_to_scalar(out, num_devs)
+    init_kvstore()
-    #     mx.kvstore.pull(keys, vals[-1])
+    # devices
-    #         check_diff_to_scalar(d, num_devs)
+    # single
-    mx.kvstore.pull(keys[0], output=None, shape=shape, ctx=devs)
+    mx.kvstore.push(3, vals)
-    mx.kvstore.stop()
+    for v in vals:
-def updater(recv, local):
+def updater(key, recv, local):
-    mx.kvstore.init_devices(devs)
+    """updater"""
-    mx.kvstore.init(key, mx.narray.zeros(shape))
+    init_kvstore()
-    mx.kvstore.pull(key, vals)
+    mx.kvstore.push(3, vals)
-        check_diff_to_scalar(v, num_devs*3)
+        check_diff_to_scalar(v, num_devs)
-    mx.kvstore.stop()
+    # list
-    # test_updater()
+    test_updater()
-def pull(keys, values):
+def pull(keys, output=None, shape=None, ctx=None):
-    values: NArray or list of NArray
+    output: NArray or list of NArray
-    check_call(_LIB.MXKVStorePull(num, ckeys, cvals))
+    if output is None:
-    mx.kvstore.init_devices(devs)
+    # mx.kvstore.init_devices(devs)
-    check_diff_to_scalar(out, num_devs)
+    # mx.kvstore.pull(keys[0], ctx=)
-        mx.kvstore.pull(keys, vals[-1])
+    # vals = []
-            check_diff_to_scalar(d, num_devs)
+    # for v in vals:
-    test_updater()
+    # test_updater()
-    check_call(_LIB.MXKVStoreInitDevices(len(contexts), masks, ids))
+def start():
-    param["workspace"] = 512
+    param["workspace"] = 256
-for name, narray in zip(loss.list_arguments(), arg_narrays):
+for name, narray in inputs.items():
-    bar_len = 60
+    bar_len = 50
-        all_train_bacth = 50000 / float(batch_size)
+        all_train_bacth = round(50000 / float(batch_size) + 1)
-        input_shapes = dict((arr[0], arr[1].shape) for arr in args.items())
+        input_shapes = dict((name, arr.shape) for name, arr in args.items())
-# pylint: disable=invalid-name,
+# pylint: disable=invalid-name, global-variable-undefined,
-    """ stop kvstore """
+    """ Stop kvstore """
-        key is
+    keys: int or list of int
-        The value
+    keys: int or list of int
-    """ Register a updater into the store
+    """ set a updater into the store
-        weight[:] -= lr * grad  / batch_size
+
-
+    updater: functon
-#     return updater_handle
+def _updater_wrapper(updater):
-#     pass
+def set_updater(updater):
-# _updater_func = _updater_proto(_updater_wrapper(_void_updater))
+    Example:
-#     ----------
+    Parameters
-#     check_call(_LIB.MXKVStoreRegister(updater_func))
+    """
-            return parse_key_value(keys[0], vals)
+            return _ctype_key_value(keys[0], vals)
-    return updater_handle
+# def _updater_wrapper(updater):
-    pass
+# def _void_updater(lhs, rhs):
-_updater_func = _updater_proto(_updater_wrapper(_void_updater))
+# _updater_proto = ctypes.CFUNCTYPE(None, NArrayHandle, NArrayHandle)
-    """ Register a updater into the store
+# def register(updater):
-        weight[:] -= lr * grad  / batch_size
+#     Example:
-    ----------
+#     Parameters
-    check_call(_LIB.MXKVStoreRegister(updater_func))
+#     """
-def init(kv_list):
+
-            check_call(_LIB.MXKVStoreInit(kv[0], kv[1].handle))
+    num, ckeys, cvals = _ctype_key_value(keys, values)
-def push(kv_list):
+def push(keys, values):
-            check_call(_LIB.MXKVStorePush(kv[0], kv[1].handle))
+    num, ckeys, cvals = _ctype_key_value(keys, values)
-def pull(kv_list):
+def pull(keys, values):
-
+    num, ckeys, cvals = _ctype_key_value(keys, values)
-#     return updater_handle
+def _updater_wrapper(updater):
-#     pass
+def _void_updater(lhs, rhs):
-# updater_func = updater_proto(updater_wrapper(void_updater))
+_updater_proto = ctypes.CFUNCTYPE(None, NArrayHandle, NArrayHandle)
-#     """ Register a updater into the store
+def register(updater):
-#         weight[:] -= lr * grad  / batch_size
+    Example:
-#     ----------
+    Parameters
-#     check_call(_LIB.MXKVStoreRegister(updater_func))
+    """
-mx.kvstore.stop()
+def check_diff_to_scalar(A, x):
-    pass
+# def updater_wrapper(updater):
-updater_func = updater_proto(updater_wrapper(void_updater))
+# def void_updater(lhs, rhs):
-    """ Register a updater into the store
+# updater_proto = ctypes.CFUNCTYPE(None, NArrayHandle, NArrayHandle)
-        weight[:] -= lr * grad  / batch_size
+# def register(updater):
-    ----------
+#     Example:
-    check_call(_LIB.MXKVStoreRegister(updater_func))
+#     Parameters
-    mx.kvstore.push((3, b))
+# B = [mx.narray.empty(s,d) for d in devs]
-
+import time
-in4a = SimpleFactory(in3c, 112, 38)
+in4a = SimpleFactory(in3c, 112, 48)
-args_list = loss.list_arguments()
+epoch = 9
-aux_narrays = [mx.narray.zeros(shape, ctx=mx.Context("gpu")) for shape in aux_shapes]
+in_data = mx.narray.empty(data_shape, mx.gpu())
-inputs = dict(zip(args_list, arg_narrays))
+arg_narrays, grad_narrays = executor.list_arguments()
-pred = mx.narray.zeros(out_shapes[0])
+inputs = dict(zip(loss.list_arguments(), arg_narrays))
-for name, narray in inputs.items():
+for name, narray in zip(loss.list_arguments(), arg_narrays):
-tmp_label = mx.narray.zeros(name2shape["sm_label"])
+tmp_label = mx.narray.zeros(inputs["sm_label"].shape)
-    bar_len = 80
+def progress(count, total, epoch, toc):
-
+    tic = time.time()
-            progress(train_nbatch, all_train_bacth, "Epoch %d" % i)
+            toc = time.time()
-from .context import Context, current_context
+from .context import Context, current_context, cpu, gpu
-    arr = NArray(handle=_new_alloc_handle(shape, ctx, False))
+    arr = empty(shape, ctx)
-    arr = NArray(handle=_new_alloc_handle(shape, ctx, False))
+    arr = empty(shape, ctx)
-from .narray import NArray
+from .narray import NArray, zeros
-conv1= mx.symbol.Convolution(data = data, name='conv1', num_filter=32, kernel=(3,3), stride=(2,2), nstep=100)
+conv1= mx.symbol.Convolution(data = data, name='conv1', num_filter=32, kernel=(3,3), stride=(2,2))
-conv2= mx.symbol.Convolution(data = mp1, name='conv2', num_filter=32, kernel=(3,3), stride=(2,2), nstep=100)
+conv2= mx.symbol.Convolution(data = mp1, name='conv2', num_filter=32, kernel=(3,3), stride=(2,2))
-from .base import check_call, c_array
+from .base import check_call, c_array, NArrayHandle
-
+from . import kvstore
-
+import ctypes
-        a key-value tuple or a list of key-value tuples
+        a key-value tuple or a list of key-value tuples, where key is int and
-    param["nstep"] = 100
+    param["nstep"] = 128
-    out_num = narray.numpy.shape[0]
+    in_num = narray.shape[1]
-    tmp.copyto(narray)
+    tmp = mx.narray.array(np.random.uniform(-a, a, narray.shape))
-loss = mx.symbol.Softmax(data=fc, name="softmax")
+loss = mx.symbol.Softmax(data=fc, name="sm")
-data_shape = (128, 3, 28, 28)
+
-grad_narrays = [mx.narray.create(shape, ctx=mx.Context("gpu")) for shape in arg_shapes]
+arg_narrays = [mx.narray.zeros(shape, ctx=mx.Context("gpu")) for shape in arg_shapes]
-pred = mx.narray.create(out_shapes[0])
+pred = mx.narray.zeros(out_shapes[0])
-        tmp.copyto(narray)
+        narray[:] = np.random.uniform(-0.1, 0.1, narray.shape)
-executor = loss.bind(mx.Context('gpu'), arg_narrays, grad_narrays)
+executor = loss.bind(mx.Context('gpu'), arg_narrays, grad_narrays, 'write', aux_narrays)
-wd = 0.0004
+lr = 0.05
-    weight[:] -= lr * grad  / batch_size
+def Update(grad, weight, mom):
-block = list(zip(grad_narrays, arg_narrays))
+block = list(zip(grad_narrays, arg_narrays, mom_narrays))
-        batch_size=128,
+        batch_size=batch_size,
-        rand_mirror=True,
+        rand_crop=False,
-        batch_size=100,
+        batch_size=batch_size,
-tmp_label = mx.narray.create(name2shape["sm_label"])
+tmp_label = mx.narray.zeros(name2shape["sm_label"])
-        print("Epoch %d" % i)
+        all_train_bacth = 50000 / float(batch_size)
-            tmp_label.copyto(inputs["sm_label"])
+            progress(train_nbatch, all_train_bacth, "Epoch %d" % i)
-            train_acc += CalAcc(pred.numpy, label.numpy.flatten())
+            pred[:] = out_narray
-            executor.backward([grad_narray])
+            executor.backward([out_narray])
-                Update(grad, weight)
+            for grad, weight, mom in block:
-            data.copyto(inputs["data"])
+        for data, label in test_dataiter:
-            val_acc += CalAcc(pred.numpy, label)
+            pred[:] = out_narray
-    assert(acc_val > 0.97)
+        test_dataiter.reset()
-
+# create GPU NArray for data
-pred = mx.narray.create(out_shapes[0])
+pred = mx.narray.zeros(out_shapes[0])
-np.random.seed(0)
+np.random.seed(0)
-        tmp.numpy[:] = np.random.uniform(-0.07, 0.07, name2shape[name])
+        tmp = mx.narray.array(np.random.uniform(-0.07, 0.07, name2shape[name]))
-
+# create gradient NArray
-grad_narray = mx.narray.create(out_narray.shape)
+grad_narray = mx.narray.zeros(out_narray.shape, ctx=mx.Context("gpu"))
-tmp_label = mx.narray.create(name2shape["sm_label"])
+tmp_label = mx.narray.zeros(name2shape["sm_label"])
-            tmp_label.copyto(inputs["sm_label"])
+            label = label.asnumpy().reshape(tmp_label.shape)
-            train_acc += CalAcc(pred.numpy, label.numpy.flatten())
+            pred[:] = out_narray
-            out_narray.copyto(grad_narray)
+            grad_narray[:] = out_narray
-            data.copyto(inputs["data"])
+            label = label.asnumpy().flatten()
-            val_acc += CalAcc(pred.numpy, label)
+            pred[:] = out_narray
-        os.system("unzip data/cifar10.zip")
+        os.chdir("./data")
-    rhs_arr.numpy[:] = np.random.uniform(-10, 10, shape)
+    lhs_arr = mx.narray.array(np.random.uniform(-10, 10, shape))
-    out4 = exec4.heads()[0].numpy
+    out2 = executor.heads()[0].asnumpy()
-                              rhs_arr.numpy)
+    out_grad = mx.narray.array(np.ones(shape))
-    assert reldiff(rhs_grad.numpy, rhs_grad2) < 1e-6
+    assert reldiff(lhs_grad.asnumpy(), lhs_grad2) < 1e-6
-aux_narrays = [mx.narray.create(shape) for shape in aux_shapes]
+arg_narrays = [mx.narray.empty(shape) for shape in arg_shapes]
-        narray.numpy[:] = np.random.uniform(-0.07, 0.07, narray.numpy.shape)
+        narray[:] = np.random.uniform(-0.07, 0.07, narray.shape)
-        narray.numpy[:] = 0.0
+        narray[:] = 0.0
-        narray.numpy[:] = 1.0
+        narray[:] = 1.0
-        narray.numpy[:] = 0.0
+        narray[:] = 0.0
-grad_narray = mx.narray.create(out_narray.shape)
+grad_narray = mx.narray.empty(out_narray.shape)
-            inputs["sm_label"].numpy[:] = label
+            label = label.asnumpy().flatten()
-            train_acc += CalAcc(out_narray.numpy, label)
+            train_acc += CalAcc(out_narray.asnumpy(), label)
-            grad_narray.numpy[:] = out_narray.numpy
+            grad_narray[:] = out_narray
-            inputs["data"].numpy[:] = data
+            label = label.asnumpy().flatten()
-            val_acc += CalAcc(out_narray.numpy, label)
+            val_acc += CalAcc(out_narray.asnumpy(), label)
-    label_0 = train_dataiter.getlabel().numpy.flatten()
+    label_0 = train_dataiter.getlabel().asnumpy().flatten()
-    label_1 = train_dataiter.getlabel().numpy.flatten()
+    label_1 = train_dataiter.getlabel().asnumpy().flatten()
-grad_narrays = [mx.narray.create(shape) for shape in arg_shapes]
+arg_narrays = [mx.narray.empty(shape) for shape in arg_shapes]
-        narray.numpy[:, :] = np.random.uniform(-0.07, 0.07, narray.numpy.shape)
+        narray[:] = np.random.uniform(-0.07, 0.07, narray.shape)
-        narray.numpy[:] = 0.0
+        narray[:] = 0.0
-grad_narray = mx.narray.create(out_narray.shape)
+grad_narray = mx.narray.empty(out_narray.shape)
-            inputs["sm_label"].numpy[:] = label
+            label = label.asnumpy().flatten()
-            train_acc += CalAcc(out_narray.numpy, label)
+            train_acc += CalAcc(out_narray.asnumpy(), label)
-            grad_narray.numpy[:] = out_narray.numpy
+            grad_narray[:] = out_narray
-            inputs["data"].numpy[:] = data
+            label = label.asnumpy().flatten()
-            val_acc += CalAcc(out_narray.numpy, label)
+            val_acc += CalAcc(out_narray.asnumpy(), label)
-    arr_grad = [mx.narray.create(shape) for i in range(n)]
+    arr = [mx.narray.empty(shape) for i in range(n)]
-        arr[i].numpy[:] = np.random.uniform(-10, 10, shape)
+        arr[i][:] = np.random.uniform(-10, 10, shape)
-    out1 = exec1.heads()[0].numpy
+    out1 = exec1.heads()[0].asnumpy()
-    out = sum(a.numpy for a  in arr)
+    out1 = exec1.heads()[0].asnumpy()
-    out_grad.numpy[:] = np.random.uniform(-10, 10, shape)
+    out_grad = mx.narray.empty(shape)
-        assert same(a.numpy, out_grad.numpy)
+        assert same(a.asnumpy(), out_grad.asnumpy())
-    arr = [mx.narray.create(shape) for shape in shapes]
+    arr = [mx.narray.empty(shape) for shape in shapes]
-    arr_grad = [mx.narray.create(shape) for shape in shapes]
+    arr_np = [np.copy(narray.asnumpy()) for narray in arr]
-    out_grad = mx.narray.create(out_shapes[0])
+    out_grad = mx.narray.empty(out_shapes[0])
-    assert same(out1.numpy, ret)
+    ret = np.concatenate([narray.asnumpy() for narray in arr], axis=1)
-        assert same(grad.numpy, np_grad + 1)
+        assert same(grad.asnumpy(), np_grad + 1)
-    """ Init key-value store with a list of context
+def init_devices(contexts):
-    """ Insert a key-value pair into the store
+def init(kv_list):
-        The value
+    kv_list : tuple or list/generator of tuples
-def push(key, value):
+def push(kv_list):
-def pull(key, value):
+def pull(kv_list):
-mx.kvstore.init([(k,params[0][k]) for k in sync_keys])
+mx.kvstore.init((k,params[0][k]) for k in sync_keys)
-                mx.kvstore.pull([(k,params[d][k]) for k in sync_keys])
+                mx.kvstore.pull((k,params[d][k]) for k in sync_keys)
-                mx.kvstore.push([(k, grads[d][k]) for k in sync_keys])
+                mx.kvstore.push((k, grads[d][k]) for k in sync_keys)
-from .base import ctypes2numpy_shared, ctypes2buffer
+from .base import ctypes2buffer
-            return NArray._set_value(float(value), out=self)
+            NArray._set_value(float(value), out=self)
-        check_call(_LIB.MXNArrayWait(self.handle))
+    def _sync_copyfrom(self, source_array):
-        the context of current NArray
+        context : mxnet.Context
-        This array have to sit on CPU
+    def asnumpy(self):
-        a numpy array view
+        array : numpy.ndarray
-        return ctypes2numpy_shared(pdata, self.shape)
+        data = np.empty(self.shape, dtype=np.float32)
-    """Create a new NArray, with specified shape.
+def empty(shape, ctx=None):
-        narr.numpy[:] = npy
+        narr = mx.narray.array(npy)
-    assert reldiff(out1.numpy, out2) < 1e-6
+    assert reldiff(out1.asnumpy(), out2) < 1e-6
-    data.numpy[:] = np.random.uniform(-10, 10, data.shape)
+    data= mx.narray.array(np.random.uniform(-10, 10, shape))
-    c.numpy[:] = np.random.uniform(-10, 10, c.shape)
+    c = mx.narray.array(np.random.uniform(-10, 10, (10, 10)))
-    assert np.sum(np.abs(c.numpy != d.numpy)) == 0.0
+    assert np.sum(np.abs(c.asnumpy() != d.asnumpy())) == 0.0
-    d.numpy[:] = 1.0
+    c = mx.narray.empty((10,10))
-    assert(np.sum(d.numpy) + 100 < 1e-5)
+    assert(np.sum(c.asnumpy()) - 100 < 1e-5)
-    assert(np.sum(c.numpy) - 200 < 1e-5)
+    assert(np.sum(c.asnumpy()) - 200 < 1e-5)
-    assert(np.sum(c.numpy) < 1e-5)
+    assert(np.sum(c.asnumpy()) < 1e-5)
-            b.numpy[:] = np.random.uniform(-10, 10, a.shape)
+            b = mx.narray.empty(a.shape)
-            assert np.sum(a.numpy != a2.numpy) == 0
+            assert np.sum(a.asnumpy() != a2.asnumpy()) == 0
-            assert np.sum(x.numpy != y.numpy) == 0
+            assert np.sum(x.asnumpy() != y.asnumpy()) == 0
-            assert np.sum(x.numpy != y.numpy) == 0
+            assert np.sum(x.asnumpy() != y.asnumpy()) == 0
-
+mx.kvstore.init_devices(devs)
-        mx.kvstore.insert(i, v)
+for k in sync_keys:
-                        mx.kvstore.pull(i, params[d][j])
+                mx.kvstore.pull([(k,params[d][k]) for k in sync_keys])
-                        mx.kvstore.pull(i, grads[d][j])
+                mx.kvstore.push([(k, grads[d][k]) for k in sync_keys])
-            # be paralleled?
+            # evaluate. cannot put into the above for loop since it is blocked
-        mean_img="data/cifar10/cifar_mean.bin",
+        mean_img="data/cifar/cifar_mean.bin",
-from PIL import Image
+#from PIL import Image
-    test_Cifar10Rec()
+'''
-            #mean_img="data/smallset/image_net_mean.bin",
+            mean_img="data/smallset/image_net_mean.bin",
-    # Test image
+def test_Cifar10Rec():
-    test_ImageRecIter()
+    test_Cifar10Rec()
-        batch_size=batch_size, shuffle=0, flat=1, silent=0)
+def test_MNISTIter():
-def test_MNISTIter_loop():
+    batch_size = 100
-def test_MNISTIter_reset():
+    # test_reset
-            image_mean="data/val_cxxnet_mean.bin",
+    dataiter = mx.io.ImageRecordIter(
-
+            mirror=True,
-        batch_size=batch_size, shuffle=0, flat=1, silent=0)
+def test_MNISTIter():
-def test_MNISTIter_loop():
+    batch_size = 100
-def test_MNISTIter_reset():
+    # test_reset
-            image_mean="data/val_cxxnet_mean.bin",
+    dataiter = mx.io.ImageRecordIter(
-
+            mirror=True,
-    out = mx.symbol.FullyConnected(data=data, name='fc1', nb_hidden=1000)
+    out = mx.symbol.FullyConnected(data=data, name='fc1', num_hidden=1000)
-    out = mx.symbol.FullyConnected(data=out, name='fc2', nb_hidden=10)
+    out = mx.symbol.FullyConnected(data=out, name='fc2', num_hidden=10)
-conv1= mx.symbol.Convolution(data = data, name='conv1', nb_filter=32, kernel=(3,3), stride=(2,2), nstep=100)
+conv1= mx.symbol.Convolution(data = data, name='conv1', num_filter=32, kernel=(3,3), stride=(2,2), nstep=100)
-conv2= mx.symbol.Convolution(data = mp1, name='conv2', nb_filter=32, kernel=(3,3), stride=(2,2), nstep=100)
+conv2= mx.symbol.Convolution(data = mp1, name='conv2', num_filter=32, kernel=(3,3), stride=(2,2), nstep=100)
-fc2 = mx.symbol.FullyConnected(data = fl, name='fc2', nb_hidden=10)
+fc2 = mx.symbol.FullyConnected(data = fl, name='fc2', num_hidden=10)
-fc1 = mx.symbol.FullyConnected(data = data, name='fc1', nb_hidden=128)
+fc1 = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=128)
-fc2 = mx.symbol.FullyConnected(data = act1, name = 'fc2', nb_hidden = 64)
+fc2 = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 64)
-fc3 = mx.symbol.FullyConnected(data = act2, name='fc3', nb_hidden=10)
+fc3 = mx.symbol.FullyConnected(data = act2, name='fc3', num_hidden=10)
-    net1 = mx.symbol.FullyConnected(data=net1, name='fc2', nb_hidden=100)
+    net1 = mx.symbol.FullyConnected(data=data, name='fc1', num_hidden=10)
-    net2 = mx.symbol.FullyConnected(name='fc3', nb_hidden=10)
+    net2 = mx.symbol.FullyConnected(name='fc3', num_hidden=10)
-    net2 = mx.symbol.FullyConnected(data=net2, name='fc4', nb_hidden=20)
+    net2 = mx.symbol.FullyConnected(data=net2, name='fc4', num_hidden=20)
-        ctypes.byref(arg_descs)))
+        ctypes.byref(arg_descs),
-        ret = '%s : %s' % (py_str(arg_names[i]), py_str(arg_types[i]))
+        key = py_str(arg_names[i])
-    doc_str = doc_str % (py_str(desc.value), '\n'.join(param_str))
+    doc_str = doc_str % (desc, '\n'.join(param_str))
-        for _, val in kwargs.items():
+        for val in kwargs.values():
-        args: list of string
+        returns : list of string
-            List of all the auxiliary
+        aux_states : list of string
-        Pair of Nones is returned if there is not enough information passed in.
+        Tuple of Nones is returned if there is not enough information passed in.
-            return (None, None)
+            return (None, None, None)
-    def bind(self, ctx, args, args_grad, reqs, aux_states=None):
+    @staticmethod
-            input auxiliary states to the symbol
+            The device context the generated executor to run on.
-        if aux_states == None:
+
-        aux_args_handle = c_array(NArrayHandle, [item.handle for item in aux_states])
+        aux_args_handle = self._get_narray_handle('aux_states', aux_states,
-                        reqs=['write_to'] * 2)
+                        args_grad=[lhs_grad, rhs_grad])
-executor = softmax.bind(mx.Context('cpu'), arg_narrays, grad_narrays, req, aux_narrays)
+executor = softmax.bind(mx.Context('cpu'), arg_narrays, grad_narrays, 'write', aux_narrays)
-executor = softmax.bind(mx.Context('cpu'), arg_narrays, grad_narrays, req)
+executor = softmax.bind(mx.Context('cpu'), arg_narrays, grad_narrays)
-    
+
-        if isinstance(value, NArray) == True:
+        if isinstance(value, NArray):
-            return NArray._set_value(self, float(value), out=self)
+            return NArray._set_value(float(value), out=self)
-        
+    def __neg__(self):
-        if isinstance(value, NArray) == False:
+        if isinstance(value, NArray) == True:
-            value.copyto(self)
+        
-            # check_with_uniform(lambda x, y: x / y, 2, dim)
+            check_with_uniform(lambda x, y: x / y, 2, dim)
-    assert(np.sum(d.numpy) == -100)
+    assert(np.sum(c.numpy) - 100 < 1e-5)
-        setattr(module_obj, function.__name__, function)
+        if function.__name__.startswith('_'):
-from .base import _LIB, string_types
+from .base import _LIB, string_types, numeric_types
-        elif isinstance(other, float) or isinstance(other, int):
+        elif isinstance(other, numeric_types):
-        elif isinstance(other, float) or isinstance(other, int):
+        elif isinstance(other, numeric_types):
-        elif isinstance(other, float) or isinstance(other, int):
+        elif isinstance(other, numeric_types):
-        elif isinstance(other, float) or isinstance(other, int):
+        elif isinstance(other, numeric_types):
-        elif isinstance(other, float) or isinstance(other, int):
+        elif isinstance(other, numeric_types):
-        elif isinstance(other, float) or isinstance(other, int):
+        elif isinstance(other, numeric_types):
-        elif isinstance(other, float) or isinstance(other, int):
+        elif isinstance(other, numeric_types):
-        elif isinstance(other, float) or isinstance(other, int):
+        elif isinstance(other, numeric_types):
-        value.copyto(self)
+        if value.handle is not self.handle:
-        """bind current symbol to get an executor.
+        """Bind current symbol to get an executor.
-        ctx: Context
+        ctx : Context
-        args: Array of NArray
+        args : Array of NArray
-        args_grad: Array of NArray
+        args_grad : Array of NArray
-        reqs: Array of enum
+        reqs : Array of enum
-        aux_states: Array of NArray
+        aux_states : Array of NArray
-        """get the autodiff of current symbol.
+        """Get the autodiff of current symbol.
-        wrt: Array of String
+        wrt : Array of String
-            if isinstance(out, NArray):
+            if isinstance(out, NArray) == False:
-            if isinstance(out, NArray):
+            if isinstance(out, NArray) == False:
-        return self.__div__(other)
+        if isinstance(other, NArray):
-    weight.numpy[:] -= lr * grad.numpy[:] / batch_size
+    weight[:] -= lr * grad / batch_size
-    weight.numpy[:] -= lr * grad.numpy[:]  / batch_size
+    weight[:] -= lr * grad  / batch_size
-        check_call(_LIB.MXExecutorForward(self.handle))
+    def forward(self, is_train=True):
-# pylint: disable=invalid-name, protected-access, fixme
+# pylint: disable=invalid-name, protected-access, fixme, too-many-arguments
-        aux_args_handle = c_array(NArrayHandle, [item.handle for item in aux_args])
+        aux_args_handle = c_array(NArrayHandle, [item.handle for item in aux_states])
-                                       len(aux_args),
+                                       len(aux_states),
-#pylint: skip-file
+# pylint: skip-file
-import os
+import os, gzip
-        batch_size=100, shuffle=1, silent=1, input_flat="flat")
+# prepare data
-dataiter.beforefirst()
+batch_size = 100
-    '''
+def test_MNISTIter_loop():
-    out = mx.symbol.FullyConnected(data=data, name='fc1', num_hidden=1000)
+    out = mx.symbol.FullyConnected(data=data, name='fc1', nb_hidden=1000)
-    out = mx.symbol.FullyConnected(data=out, name='fc2', num_hidden=10)
+    out = mx.symbol.FullyConnected(data=out, name='fc2', nb_hidden=10)
-act1 = mx.symbol.Activation(data = conv1, name='relu1', act_type="relu")
+conv1= mx.symbol.Convolution(data = data, name='conv1', nb_filter=32, kernel=(3,3), stride=(2,2), nstep=100)
-act2 = mx.symbol.Activation(data = conv2, name='relu2', act_type="relu")
+conv2= mx.symbol.Convolution(data = mp1, name='conv2', nb_filter=32, kernel=(3,3), stride=(2,2), nstep=100)
-fc2 = mx.symbol.FullyConnected(data = fl, name='fc2', num_hidden=10)
+fc2 = mx.symbol.FullyConnected(data = fl, name='fc2', nb_hidden=10)
-arg_shapes, out_shapes = softmax.infer_shape(data=data_shape)
+arg_shapes, out_shapes, aux_shapes = softmax.infer_shape(data=data_shape)
-        narray.numpy[:, :] = np.random.uniform(-0.07, 0.07, narray.numpy.shape)
+        narray.numpy[:] = np.random.uniform(-0.07, 0.07, narray.numpy.shape)
-executor = softmax.bind(mx.Context('cpu'), arg_narrays, grad_narrays, req)
+executor = softmax.bind(mx.Context('cpu'), arg_narrays, grad_narrays, req, aux_narrays)
-            executor.forward()
+            executor.forward(is_train = True)
-            executor.forward()
+            executor.forward(is_train = False)
-    arg_shapes, out_shapes = out.infer_shape(data=data_shape)
+    arg_shapes, out_shapes, aux_shapes = out.infer_shape(data=data_shape)
-
+    print(len(aux_shapes))
-    arg_shapes, out_shapes = out.infer_shape(data=data_shape, fc1_weight=weight_shape)
+    arg_shapes, out_shapes, aux_shapes = out.infer_shape(data=data_shape, fc1_weight=weight_shape)
-fc1 = mx.symbol.FullyConnected(data = data, name='fc1', num_hidden=128)
+fc1 = mx.symbol.FullyConnected(data = data, name='fc1', nb_hidden=128)
-fc2 = mx.symbol.FullyConnected(data = act1, name = 'fc2', num_hidden = 64)
+fc2 = mx.symbol.FullyConnected(data = act1, name = 'fc2', nb_hidden = 64)
-fc3 = mx.symbol.FullyConnected(data = act2, name='fc3', num_hidden=10)
+fc3 = mx.symbol.FullyConnected(data = act2, name='fc3', nb_hidden=10)
-arg_shapes, out_shapes = softmax.infer_shape(data=data_shape)
+arg_shapes, out_shapes, aux_shapes = softmax.infer_shape(data=data_shape)
-    net1 = mx.symbol.FullyConnected(data=net1, name='fc2', num_hidden=100)
+    net1 = mx.symbol.FullyConnected(data=data, name='fc1', nb_hidden=10)
-    net2 = mx.symbol.FullyConnected(name='fc3', num_hidden=10)
+    net2 = mx.symbol.FullyConnected(name='fc3', nb_hidden=10)
-    net2 = mx.symbol.FullyConnected(data=net2, name='fc4', num_hidden=20)
+    net2 = mx.symbol.FullyConnected(data=net2, name='fc4', nb_hidden=20)
-        """List all auxiliary data in the symbool.
+    def list_auxiliary_states(self):
-        check_call(_LIB.MXSymbolListAuxiliaryArgs(
+        check_call(_LIB.MXSymbolListAuxiliaryStates(
-    def bind(self, ctx, args, args_grad, reqs, aux_args=[]):
+    def bind(self, ctx, args, args_grad, reqs, aux_states=None):
-            input auxiliary args to the symbol
+        aux_states: Array of NArray
-from .base import c_array, py_str
+from .base import _LIB, string_types
-from .base import ctypes2numpy_shared
+from .base import ctypes2numpy_shared, ctypes2buffer
-def create(shape, ctx=Context.default_ctx):
+
-        shape of the NArray
+        shape of the NArray.
-    a new NArray
+    out: Array
-
+
-            return (arg_shapes, out_shapes)
+            aux_shapes = [
-    def bind(self, ctx, args, args_grad, reqs):
+    def bind(self, ctx, args, args_grad, reqs, aux_args=[]):
-block = zip(grad_narrays, arg_narrays)
+block = list(zip(grad_narrays, arg_narrays))
-        batch_size=batch_size, shuffle=1, silent=0, seed=10)
+        batch_size=batch_size, shuffle=True, silent=False, seed=10)
-        batch_size=batch_size, shuffle=1, silent=0)
+        batch_size=batch_size, shuffle=True, silent=False)
-block = zip(grad_narrays, arg_narrays)
+block = list(zip(grad_narrays, arg_narrays))
-        batch_size=batch_size, shuffle=1, flat=1, silent=0, seed=10)
+        batch_size=batch_size, shuffle=True, flat=True, silent=False, seed=10)
-        batch_size=batch_size, shuffle=1, flat=1, silent=0)
+        batch_size=batch_size, shuffle=True, flat=True, silent=False)
-
+if sys.version_info[0] >= 3:
-
+if sys.version_info[0] >= 3:
-    for i in xrange(epoch):
+    for i in range(epoch):
-        label="/home/tianjun/data/mnist/train-labels-idx1-ubyte",
+        image="data/train-images-idx3-ubyte",
-        label="/home/tianjun/data/mnist/t10k-labels-idx1-ubyte",
+        image="data/t10k-images-idx3-ubyte",
-        label="/home/tianjun/data/mnist/train-labels-idx1-ubyte",
+        image="data/train-images-idx3-ubyte",
-        label="/home/tianjun/data/mnist/t10k-labels-idx1-ubyte",
+        image="data/t10k-images-idx3-ubyte",
-from .base import c_array, c_str, mx_uint
+from .base import c_array, c_str, mx_uint, py_str
-    iter_name = name.value
+    iter_name = py_str(name.value)
-            ret += '\n    ' + arg_descs[i]
+            ret += '\n    ' + py_str(arg_descs[i])
-valid = MNISTIter("valid", batch_size, False)
+train_dataiter = mx.io.MNISTIter(
-            data, label = train.Get()
+        train_nbatch = 0
-            data, label = valid.Get()
+        for data, label in val_dataiter:
-        valid.BeforeFirst()
+            val_nbatch += 1
-valid = MNISTIter("valid", batch_size, True)
+train_dataiter = mx.io.MNISTIter(
-            data, label = train.Get()
+        train_nbatch = 0
-            data, label = valid.Get()
+        for data, label in val_dataiter:
-        valid.BeforeFirst()
+            val_nbatch += 1
-    def beforefirst(self):
+    def reset(self):
-        """get next data from iterator
+        """get next data batch from iterator
-    return np.sum(pred == label.transpose()) * 1.0 / out.shape[0]
+    return np.sum(pred == label) * 1.0 / out.shape[0]
-arg_shapes, out_shapes = fc2.infer_shape(data=data_shape)
+#data_shape = (batch_size, 784)
-        batch_size=100, shuffle=1, silent=0, flat=1)
+train = MNISTIter("train", batch_size, False)
-        label = label.numpy.astype(np.int32)
+    while train.Next():
-        label = label.numpy.astype(np.int32)
+    while valid.Next():
-    print "Valid Acc: ", val_acc / val_nbatch
+    print "Train Acc: ", train_acc / train.nbatch
-        
+
-    
+
-        """Invoke iterator as function on inputs. Init params.
+    def __iter__(self):
-                self.handle, num_args, keys, vals))
+        return self
-        """init dataiter
+        """get next data from iterator
-
+    
-            the resulting symbol
+        dataiter: Dataiter
-val_dataiter.beforefirst()
+train_dataiter = mx.io.MNISTIter(
-        inputs["data"].numpy[:] = data.numpy
+    
-        inputs["data"].numpy[:] = data.numpy
+    for data, label in val_dataiter:
-        
+
-
+        
-        
+
-        for k, v in kwargs.items():
+        for k, val in kwargs.items():
-            param_vals.append(c_str(str(v)))
+            param_vals.append(c_str(str(val)))
-        
+
-from .base import c_array, c_str, mx_uint, string_types
+from .base import c_array, c_str, mx_uint
-        
+
-                    keyword arguments')     
+                    keyword arguments')
-        keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()]) 
+        keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()])
-
+    check_call(_LIB.MXListDataIters(ctypes.byref(size), ctypes.byref(plist)))
-        batch_size=100, shuffle=1, silent=1, input_flat="flat")
+        batch_size=100, shuffle=1, silent=1, input_flat="flat", seed_data=1)
-
+from . import io
-
+        Parameters
-        check_call(_LIB.MXIOInit(self._datahandle))
+        if len(args) != 0:
-        check_call(_LIB.MXIOBeforeFirst(self._datahandle))
+        check_call(_LIB.MXDataIterBeforeFirst(self.handle))
-        check_call(_LIB.MXIONext(self._datahandle, ctypes.byref(next_res)))
+        check_call(_LIB.MXDataIterNext(self.handle, ctypes.byref(next_res)))
-        check_call(_LIB.MXIOGetData(self._datahandle, ctypes.byref(hdl)))
+        check_call(_LIB.MXDataIterGetData(self.handle, ctypes.byref(hdl)))
-        check_call(_LIB.MXIOGetLabel(self._datahandle, ctypes.byref(hdl)))
+        check_call(_LIB.MXDataIterGetLabel(self.handle, ctypes.byref(hdl)))
-dataiter.init()
+dataiter = mx.io.MNISTIterator(path_img="/home/tianjun/data/mnist/train-images-idx3-ubyte",
-    info = "Batch %d" % (i)
+idx = 0
-    label = dataiter.getdata()
+    '''
-    """DataIter object in mxnet
+    """DataIter object in mxnet. List all the needed functions here. """
-    """
+    def __init__(self, handle):
-
+DataIterHandle = ctypes.c_void_p
-import os, cPickle, gzip
+import os, pickle, gzip
-        train_set, valid_set, test_set = cPickle.load(f)
+        IgnorePython3()
-fl = mx.symbol.Flatten(data = mp, name="flatten")
+conv1= mx.symbol.Convolution(data = data, name='conv1', nb_filter=32, kernel=(3,3), stride=(1,1), nstep=10)
-        narray.numpy[:, :] = np.random.uniform(-0.001, 0.001, narray.numpy.shape)
+        narray.numpy[:, :] = np.random.uniform(-0.07, 0.07, narray.numpy.shape)
-epoch = 10
+epoch = 1
-lr = 0.001
+lr = 0.1
-    weight.numpy[:] -= lr * grad.numpy[:]
+def Update(grad, weight):
-block = zip(mom_narrays, grad_narrays, arg_narrays)
+block = zip(grad_narrays, arg_narrays)
-
+def test_mnist():
-    def beforefirst(self):
+        return self
-        """get next data from iterator
+        """get next data batch from iterator
-    return np.sum(pred == label.transpose()) * 1.0 / out.shape[0]
+    return np.sum(pred == label) * 1.0 / out.shape[0]
-arg_shapes, out_shapes = fc2.infer_shape(data=data_shape)
+#data_shape = (batch_size, 784)
-        batch_size=100, shuffle=1, silent=0, flat=1)
+train = MNISTIter("train", batch_size, False)
-        label = label.numpy.astype(np.int32)
+    while train.Next():
-        label = label.numpy.astype(np.int32)
+    while valid.Next():
-    print "Valid Acc: ", val_acc / val_nbatch
+    print "Train Acc: ", train_acc / train.nbatch
-        
+
-    
+
-        """Invoke iterator as function on inputs. Init params.
+    def __iter__(self):
-        """init dataiter
+        """get next data from iterator
-
+    
-            the resulting symbol
+        dataiter: Dataiter
-val_dataiter.beforefirst()
+train_dataiter = mx.io.MNISTIter(
-        inputs["data"].numpy[:] = data.numpy
+    
-        inputs["data"].numpy[:] = data.numpy
+    for data, label in val_dataiter:
-        for k, v in kwargs.items():
+        for k, val in kwargs.items():
-            param_vals.append(c_str(str(v)))
+            param_vals.append(c_str(str(val)))
-        
+
-
+        
-arg_shapes, out_shapes = softmax.infer_shape(data=data_shape)
+data_shape = (batch_size, 1, 1, 784)
-from .base import c_array, c_str, mx_uint, string_types
+from .base import c_array, c_str, mx_uint
-        
+
-                    keyword arguments')     
+                    keyword arguments')
-        keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()]) 
+        keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()])
-
+    check_call(_LIB.MXListDataIters(ctypes.byref(size), ctypes.byref(plist)))
-        batch_size=100, shuffle=1, silent=1, input_flat="flat")
+        batch_size=100, shuffle=1, silent=1, input_flat="flat", seed_data=1)
-
+from . import io
-
+        Parameters
-        check_call(_LIB.MXIOInit(self._datahandle))
+        if len(args) != 0:
-        check_call(_LIB.MXIOBeforeFirst(self._datahandle))
+        check_call(_LIB.MXDataIterBeforeFirst(self.handle))
-        check_call(_LIB.MXIONext(self._datahandle, ctypes.byref(next_res)))
+        check_call(_LIB.MXDataIterNext(self.handle, ctypes.byref(next_res)))
-        check_call(_LIB.MXIOGetData(self._datahandle, ctypes.byref(hdl)))
+        check_call(_LIB.MXDataIterGetData(self.handle, ctypes.byref(hdl)))
-        check_call(_LIB.MXIOGetLabel(self._datahandle, ctypes.byref(hdl)))
+        check_call(_LIB.MXDataIterGetLabel(self.handle, ctypes.byref(hdl)))
-dataiter.init()
+dataiter = mx.io.MNISTIterator(path_img="/home/tianjun/data/mnist/train-images-idx3-ubyte",
-    info = "Batch %d" % (i)
+idx = 0
-    label = dataiter.getdata()
+    '''
-
+    return np.sum(pred == label.transpose()) * 1.0 / out.shape[0]
-valid = MNISTIter("valid", batch_size, False)
+train_dataiter = mx.io.MNISTIterator(path_img="/home/tianjun/data/mnist/train-images-idx3-ubyte",
-        inputs["sm_label"].numpy[:] = label
+    train_nbatch = 0
-        inputs["data"].numpy[:] = data
+    while val_dataiter.next():
-    valid.BeforeFirst()
+        val_nbatch += 1
-    """DataIter object in mxnet
+    """DataIter object in mxnet. List all the needed functions here. """
-    """
+    def __init__(self, handle):
-
+DataIterHandle = ctypes.c_void_p
-    def __init__(self, which_set, batch_size=100):
+    def __init__(self, which_set, batch_size=100, flatten=True):
-        return (self.data[start:end, :], self.label[start:end])
+        if self.flatten:
-fc1 = mx.symbol.FullyConnected(data=data, name='fc1', num_hidden=160)
+fc1 = mx.symbol.Convolution(data = data, name='conv1', nb_filter=32, kernel=(7,7), stride=(2,2), nstep=10, no_bias=1)
-args_list = fc2.list_arguments()
+mp = mx.symbol.Pooling(data = act1, name = 'mp', kernel=(2,2), stride=(2,2), pool_type='avg')
-arg_shapes, out_shapes = fc2.infer_shape(data=data_shape)
+#data_shape = (batch_size, 784)
-
+print zip(args_list, arg_shapes)
-executor = fc2.bind(mx.Context('cpu'), arg_narrays, grad_narrays, req)
+executor = softmax.bind(mx.Context('cpu'), arg_narrays, grad_narrays, req)
-valid = MNISTIter("valid", batch_size)
+train = MNISTIter("train", batch_size, False)
-    check_call(_LIB.MXSymbolCreateVariable(name, ctypes.byref(handle)))
+    check_call(_LIB.MXSymbolCreateVariable(c_str(name), ctypes.byref(handle)))
-    return
+    return reldiff
-            check_with_uniform(lambda x, y: x / y, 2, dim)
+            # check_with_uniform(lambda x, y: x / y, 2, dim)
-    print net2.debug_str()
+    print(net2.debug_str())
-    print composed.debug_str()
+    print(composed.debug_str())
-        for k, v in kwargs.items():
+        for k, val in kwargs.items():
-            param_vals.append(c_str(str(v)))
+            param_vals.append(c_str(str(val)))
-from .base import c_array, c_str, mx_uint, string_types
+from .base import c_array, c_str, mx_uint
-        
+
-                    keyword arguments')     
+                    keyword arguments')
-        keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()]) 
+        keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()])
-
+    check_call(_LIB.MXListDataIters(ctypes.byref(size), ctypes.byref(plist)))
-        batch_size=100, shuffle=1, silent=1, input_flat="flat")
+        batch_size=100, shuffle=1, silent=1, input_flat="flat", seed_data=1)
-
+from . import io
-
+        Parameters
-        check_call(_LIB.MXIOInit(self._datahandle))
+        if len(args) != 0:
-        check_call(_LIB.MXIOBeforeFirst(self._datahandle))
+        check_call(_LIB.MXDataIterBeforeFirst(self.handle))
-        check_call(_LIB.MXIONext(self._datahandle, ctypes.byref(next_res)))
+        check_call(_LIB.MXDataIterNext(self.handle, ctypes.byref(next_res)))
-        check_call(_LIB.MXIOGetData(self._datahandle, ctypes.byref(hdl)))
+        check_call(_LIB.MXDataIterGetData(self.handle, ctypes.byref(hdl)))
-        check_call(_LIB.MXIOGetLabel(self._datahandle, ctypes.byref(hdl)))
+        check_call(_LIB.MXDataIterGetLabel(self.handle, ctypes.byref(hdl)))
-dataiter.init()
+dataiter = mx.io.MNISTIterator(path_img="/home/tianjun/data/mnist/train-images-idx3-ubyte",
-    info = "Batch %d" % (i)
+idx = 0
-    label = dataiter.getdata()
+    '''
-    return np.sum(pred == label) * 1.0 / out.shape[0]
+    return np.sum(pred == label.transpose()) * 1.0 / out.shape[0]
-data_shape = (batch_size, 784)
+data_shape = (batch_size, 1, 1, 784)
-valid = MNISTIter("valid", batch_size)
+train_dataiter = mx.io.MNISTIterator(path_img="/home/tianjun/data/mnist/train-images-idx3-ubyte",
-        inputs["data"].numpy[:] = data
+    train_nbatch = 0
-        inputs["data"].numpy[:] = data
+    while val_dataiter.next():
-    valid.BeforeFirst()
+        val_nbatch += 1
-    """DataIter object in mxnet
+    """DataIter object in mxnet. List all the needed functions here. """
-    """
+    def __init__(self, handle):
-
+DataIterHandle = ctypes.c_void_p
-from .base import c_array, c_str, mx_uint, string_types
+from .base import c_array, c_str, mx_uint
-        
+
-                    keyword arguments')     
+                    keyword arguments')
-        keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()]) 
+        keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()])
-
+    check_call(_LIB.MXListDataIters(ctypes.byref(size), ctypes.byref(plist)))
-        batch_size=100, shuffle=1, silent=1, input_flat="flat")
+        batch_size=100, shuffle=1, silent=1, input_flat="flat", seed_data=1)
-
+from . import io
-
+        Parameters
-        check_call(_LIB.MXIOInit(self._datahandle))
+        if len(args) != 0:
-        check_call(_LIB.MXIOBeforeFirst(self._datahandle))
+        check_call(_LIB.MXDataIterBeforeFirst(self.handle))
-        check_call(_LIB.MXIONext(self._datahandle, ctypes.byref(next_res)))
+        check_call(_LIB.MXDataIterNext(self.handle, ctypes.byref(next_res)))
-        check_call(_LIB.MXIOGetData(self._datahandle, ctypes.byref(hdl)))
+        check_call(_LIB.MXDataIterGetData(self.handle, ctypes.byref(hdl)))
-        check_call(_LIB.MXIOGetLabel(self._datahandle, ctypes.byref(hdl)))
+        check_call(_LIB.MXDataIterGetLabel(self.handle, ctypes.byref(hdl)))
-dataiter.init()
+dataiter = mx.io.MNISTIterator(path_img="/home/tianjun/data/mnist/train-images-idx3-ubyte",
-    info = "Batch %d" % (i)
+idx = 0
-    label = dataiter.getdata()
+    '''
-    return np.sum(pred == label) * 1.0 / out.shape[0]
+    return np.sum(pred == label.transpose()) * 1.0 / out.shape[0]
-data_shape = (batch_size, 784)
+data_shape = (batch_size, 1, 1, 784)
-valid = MNISTIter("valid", batch_size)
+train_dataiter = mx.io.MNISTIterator(path_img="/home/tianjun/data/mnist/train-images-idx3-ubyte",
-        inputs["data"].numpy[:] = data
+    train_nbatch = 0
-        inputs["data"].numpy[:] = data
+    while val_dataiter.next():
-    valid.BeforeFirst()
+        val_nbatch += 1
-# pylint: disable=invalid-name, protected-access
+from .base import MXNetError
-
+        raise MXNetError(py_str(_LIB.MXGetLastError()))
-    a char pointer that can be passed to C API
+    str : c_char_p
-    created ctypes array
+    out : ctypes array
-    a numpy array : numpy array
+    out : numpy_array
-from .base import c_array
+from .base import c_array, py_str
-
+    # pylint: disable= no-member
-
+    # pylint: enable= no-member
-
+# pylint: disable=too-many-locals, invalid-name
-            ctypes.byref(type_mask)))
+    check_call(_LIB.MXFuncDescribe(
-    func_name = name.value
+    check_call(_LIB.MXFuncGetInfo(
-        ret = '%s : %s' % (arg_names[i], arg_types[i])
+        ret = '%s : %s' % (py_str(arg_names[i]), py_str(arg_types[i]))
-            ret += '\n    ' + arg_descs[i]
+            ret += '\n    ' + py_str(arg_descs[i])
-    doc_str = doc_str % (desc.value, '\n'.join(param_str))
+    doc_str = doc_str % (py_str(desc.value), '\n'.join(param_str))
-                c_array(NArrayHandle, (out.handle,))))
+        check_call(_LIB.MXFuncInvoke(handle,
-
+# pylint: enable=too-many-locals, invalid-name
-# pylint: disable=invalid-name, protected-access, too-many-locals, fixme
+# pylint: disable=invalid-name, protected-access, fixme
-from .base import c_array, c_str, mx_uint, string_types
+from .base import c_array, c_str, mx_uint, py_str, string_types
-                self.handle, name, num_args, keys, args))
+        check_call(_LIB.MXSymbolCompose(
-        return [sarr[i] for i in range(size.value)]
+        check_call(_LIB.MXSymbolListArguments(
-        return [sarr[i] for i in range(size.value)]
+        check_call(_LIB.MXSymbolListReturns(
-                ctypes.byref(complete)))
+        check_call(_LIB.MXSymbolInferShape(
-                    for i in range(out_shape_size.value)]
+            arg_shapes = [
-        return debug_str.value
+        check_call(_LIB.MXSymbolPrint(
-    func_name = name.value
+    check_call(_LIB.MXSymbolGetAtomicSymbolInfo(
-        ret = '%s : %s' % (arg_names[i], arg_types[i])
+        ret = '%s : %s' % (py_str(arg_names[i]), py_str(arg_types[i]))
-            ret += '\n    ' + arg_descs[i]
+            ret += '\n    ' + py_str(arg_descs[i])
-    doc_str = doc_str % (desc.value, '\n'.join(param_str))
+    doc_str = doc_str % (py_str(desc.value), '\n'.join(param_str))
-                Symbols either as positional or keyword arguments, not both' % func_name)
+            raise TypeError(
-        hdl = ctypes.c_void_p(plist[i])
+        hdl = SymbolHandle(plist[i])
-print multi_out.list_returns()
+"""This file defines various models used in the test"""
-    """DataIter object in mxnet
+    """DataIter object in mxnet. List all the needed functions here. """
-    """
+    def __init__(self, handle):
-
+DataIterHandle = ctypes.c_void_p
-        subprocess.call('cd ..; cp/make/readthedocs.mk config.mk', shell = True)
+        subprocess.call('cd ..; cp make/readthedocs.mk config.mk', shell = True)
-        run_build_mxnet('..')
+
-                    'git clone https://github.com/tqchen/recommonmark', shell=True)
+                    'git clone https://github.com/tqchen/recommonmark', shell = True)
-                        'git clone https://github.com/dmlc/dmlc-core')
+                        'git clone https://github.com/dmlc/dmlc-core', shell = True)
-        subprocess.call('cd ..; cp/make/readthedocs.mk config.mk')
+                        'git clone https://github.com/dmlc/mshadow', shell = True)
-    # app.connect("builder-inited", generate_doxygen_xml)
+    app.connect("builder-inited", generate_doxygen_xml)
-    subprocess.call('cd ..; sh ./scripts/build_dmlc.sh; make clean; make;', shell = True)
+import sys
-    SCALAR_ARG_BEFORE_NARRAY = 1 << 1
+
-            ctypes.byref(n_mutate_vars),
+    check_call(_LIB.MXFuncDescribe( \
-            ctypes.byref(arg_types),
+    check_call(_LIB.MXFuncGetInfo( \
-                c_array(mx_float, ()),
+        check_call(_LIB.MXFuncInvoke( \
-                c_array(mx_float, ()),
+        check_call(_LIB.MXFuncInvoke( \
-                c_array(mx_float, [args[i] for i in scalar_range]),
+        check_call(_LIB.MXFuncInvoke( \
-    if n_mutate_vars == 1 and n_used_vars ==2 and n_scalars == 0:
+    if n_mutate_vars == 1 and n_used_vars == 2 and n_scalars == 0:
-    elif n_mutate_vars == 1 and n_used_vars ==2 and n_scalars == 0:
+    elif n_mutate_vars == 1 and n_used_vars == 2 and n_scalars == 0:
-            ctypes.byref(arg_types),
+    check_call(_LIB.MXSymbolGetAtomicSymbolInfo( \
-from .function import _FunctionRegistry
+from . import narray
-op = NArray._init_function_registry(_FunctionRegistry())
+
-from .base import mx_uint, mx_float, NArrayHandle
+from .base import mx_uint, mx_float, NArrayHandle, FunctionHandle
-    """NArray object in mxnet
+    """NArray object in mxnet.
-    NArray is basic ndarray like data structure in mxnet
+    NArray is basic ndarray/Tensor like data structure in mxnet.
-            NArray._op.plus.invoke_with_handle_((other.handle, self.handle), (), (hret,))
+            return NArray._plus(self, other)
-        return NArray(handle=hret)
+            raise TypeError('type %s not supported' % str(type(other)))
-            NArray._op.minus.invoke_with_handle_((other.handle, self.handle), (), (hret,))
+            return NArray._minus(self, other)
-        return NArray(handle=hret)
+            raise TypeError('type %s not supported' % str(type(other)))
-            NArray._op.mul.invoke_with_handle_((other.handle, self.handle), (), (hret,))
+            return NArray._mul(self, other)
-        return NArray(handle=hret)
+            raise TypeError('type %s not supported' % str(type(other)))
-            NArray._op.div.invoke_with_handle_((other.handle, self.handle), (), (hret,))
+            return NArray._div(self, other)
-        return NArray(handle=hret)
+            raise TypeError('type %s not supported' % str(type(other)))
-        """copy the content of current array to other.
+        """Copy the content of current array to other.
-            or target context we want copy the data to
+            Target Narray or context we want to copy data to.
-        the copy target NArray
+        dst : NArray
-            return other
+            return NArray._copyto(self, out=other)
-            return NArray(handle=hret)
+            hret = NArray(_new_alloc_handle(self.shape, other, True))
-            raise MXNetError('copyto do not support type ' + type(other))
+            raise TypeError('copyto do not support type ' + type(other))
-def _init_module_functions():
+def _init_symbol_module():
-
+_init_symbol_module()
-cc = mx.op.mul(b, a)
+cc = mx.narray.NArray._mul(b, a)
-    func_name = name.value;
+    desc = ctypes.c_char_p()
-    creator.__doc__ = docs.value
+    creator.__doc__ = doc_str
-data = mx.sym.Variable('data')
+data = mx.symbol.Variable('data')
-fc3 = mx.sym.FullyConnected( name='fc2', num_hidden=10)
+fc1 = mx.symbol.FullyConnected(data=data, name='fc1', num_hidden=1000)
-data = mx.sym.Variable('data')
+data = mx.symbol.Variable('data')
-fc2 = mx.sym.FullyConnected(data=fc1, name='fc2', no_bias=0)
+fc1 = mx.symbol.FullyConnected(data=data, name='fc1', no_bias=0)
-fc4 = mx.sym.FullyConnected(data=fc3, name='fc4')
+fc3 = mx.symbol.FullyConnected(name='fc3')
-multi_out = mx.sym.Group([composed_fc4, fc2])
+multi_out = mx.symbol.Group([composed_fc4, fc2])
-def _make_atomic_symbol_function(handle, func_name):
+def _make_atomic_symbol_function(handle):
-        function = _make_atomic_symbol_function(hdl, name.value)
+        function = _make_atomic_symbol_function(hdl)
-from .symbol_creator import _SymbolCreatorRegistry
+from . import symbol
-sym = Symbol._init_symbol_creator_registry(_SymbolCreatorRegistry())
+
-from .base import c_array, c_str, mx_uint, NArrayHandle, ExecutorHandle, SymbolHandle
+from .base import c_array, c_str, mx_uint, string_types
-        return _registry
+
-fc2 = mx.sym.FullyConnected(data = act1, name='fc2', num_hidden=10)
+data = mx.symbol.Variable('data')
-from .symbol_creator import _SymbolCreatorRegistry
+from . import symbol
-sym = Symbol._init_symbol_creator_registry(_SymbolCreatorRegistry())
+
-from .base import c_array, c_str, mx_uint, NArrayHandle, ExecutorHandle, SymbolHandle
+from .base import c_array, c_str, mx_uint, string_types
-        return _registry
+
-fc2 = mx.sym.FullyConnected(data = act1, name='fc2', num_hidden=10)
+data = mx.symbol.Variable('data')
-from .base import c_array, c_str, mx_uint, NArrayHandle, ExecutorHandle
+from .base import c_array, mx_uint, NArrayHandle, ExecutorHandle
-# pylint: disable=invalid-name, protected-access, too-many-locals
+# pylint: disable=invalid-name, protected-access, too-many-locals, fixme
-            hdl = plist[i]
+            hdl = ctypes.c_void_p(plist[i])
-            hmap[name.value] = _SymbolCreator(name, plist[i])
+            check_call(_LIB.MXSymbolGetAtomicSymbolName(hdl, ctypes.byref(name)))
-from .base import SymbolHandle
+from .base import c_array, c_str, mx_uint, NArrayHandle, ExecutorHandle, SymbolHandle
-    """SymbolCreator is a function that takes Param and return symbol"""
+    """Symbol is symbolic graph of the mxnet."""
-        """Init an executor from handle
+    def bind(self, ctx, args, args_grad, reqs):
-        a list of narray binded to the heads of executor
+        ctx: Context
-    return Executor(handle)
+        # TODO(bing): consider a more friendly interface
-    return e / np.sum(e, axis=1)
+    x -= maxes.reshape(batch, 1)
-fc2 = mx.sym.FullyConnected(data=act1, name='fc2', num_hidden=10)
+fc2 = mx.sym.FullyConnected(data = act1, name='fc2', num_hidden=10)
-out_narray = mx.narray.create(out_shapes[0])
+np.random.seed(0)
-
+        narray.numpy[:, :] = np.random.uniform(-0.001, 0.001, narray.numpy.shape)
-# exec = bind(fc2, args_narray, grad_narray, req)
+# TODO(bing): think of a better bind interface
-lr = 0.01
+lr = 0.001
-    weight += mom
+    weight.numpy[:] -= lr * grad.numpy[:]
-valid = MNISTIter("valid")
+train = MNISTIter("train", batch_size)
-        # exec.Forward(args_narray)
+        inputs["data"].numpy[:] = data
-        # exec.Backward(out_narray)
+        grad_narray.numpy[:] = out_narray.numpy
-        # exec.Forward([ inputs["data"] ])
+        inputs["data"].numpy[:] = data
-        check_call(_LIB.MXExecutorForward (self.hanlde, mx_uint(len(inputs), narray))
+        check_call(_LIB.MXExecutorForward (self.hanlde, mx_uint(len(inputs), narray)))
-        check_call(_LIB.MXExecutorForward (self.hanlde, mx_uint(len(grads), narray))
+        check_call(_LIB.MXExecutorForward (self.hanlde, mx_uint(len(grads), narray)))
-    reqs_array = c_array(mx_uint, mx_uint(enum[item]) for item in req)
+    reqs_array = c_array(mx_uint, [mx_uint(enum[item]) for item in req])
-    return Executor(handle);
+        mx_uint(len(args), args_handle, args_grad_handle, reqs_array)))
-
+ExecutorHandle = ctypes.c_void_p
-# pylint: disable=invalid-name, protected-access
+# pylint: disable=invalid-name, protected-access, too-many-locals
-from .base import c_array, c_str, mx_uint
+from .base import c_array, c_str, mx_uint, NArrayHandle, ExecutorHandle
-            raise ValueError('Can only specify known argument shapes either by positional or kwargs way.')
+            raise ValueError('Can only specify known argument \
-                ctypes.byref(out_shape_data),
+        check_call(_LIB.MXSymbolInferShape( \
-            out_shapes = [tuple(out_shape_data[i][:out_shape_ndim[i]]) for i in range(out_shape_size.value)]
+            arg_shapes = [tuple(arg_shape_data[i][:arg_shape_ndim[i]]) \
-        check_call(_LIB.MXSymbolPrint(
+        check_call(_LIB.MXSymbolPrint( \
-        """Compose Symbols
+        """Invoke symbol as function on inputs.
-        assert (len(args) == 0 or len(kwargs) == 0)
+        name = kwargs.pop('name', None)
-            assert isinstance(val, Symbol)
+            if not isinstance(arg, Symbol):
-            args = c_array(SymbolHandle, kwargs.values())
+            args = c_array(SymbolHandle, [s.handle for s in kwargs.values()])
-        return Symbol(out)
+            args = c_array(SymbolHandle, [s.handle for s in args])
-from .base import mx_uint, SymbolHandle
+from .base import c_array, c_str, string_types
-    def __call__(self, **kwargs):
+
-            provide the params necessary for the symbol creation
+            Provide the params necessary for the symbol creation.
-        vals = c_array(ctypes.c_char_p, [c_str(str(val)) for val in kwargs.values()])
+        param_keys = []
-            vals,
+        check_call(_LIB.MXSymbolCreateAtomicSymbol(
-        return Symbol(sym_handle)
+
-            hmap[name] = _SymbolCreator(name, plist[i])
+            name = ctypes.c_char_p()
-#pylint: skip-file
+# pylint: skip-file
-        return tuple(pdata[i] for i in range(ndim.value))
+        return tuple(pdata[:ndim.value])
-from .base import c_array, c_str
+from .base import c_array, c_str, mx_uint
-        check_call(_LIB.MXSymbolPrint( \
+        check_call(_LIB.MXSymbolPrint(
-                param_keys.append(k)
+                param_keys.append(c_str(k))
-                ctypes.byref(sym_handle)))
+        check_call(_LIB.MXSymbolCreateAtomicSymbol(
-        """Compose Symbols
+        """Invoke symbol as function on inputs.
-        assert (len(args) == 0 or len(kwargs) == 0)
+        name = kwargs.pop('name', None)
-            assert isinstance(val, Symbol)
+            if not isinstance(arg, Symbol):
-            args = c_array(SymbolHandle, kwargs.values())
+            args = c_array(SymbolHandle, [s.handle for s in kwargs.values()])
-        return Symbol(out)
+            args = c_array(SymbolHandle, [s.handle for s in args])
-from .base import mx_uint, SymbolHandle
+from .base import c_array, c_str, string_types
-    def __call__(self, **kwargs):
+
-            provide the params necessary for the symbol creation
+            Provide the params necessary for the symbol creation.
-        vals = c_array(ctypes.c_char_p, [c_str(str(val)) for val in kwargs.values()])
+        param_keys = []
-        return Symbol(sym_handle)
+        check_call(_LIB.MXSymbolCreateFromAtomicSymbol( \
-            hmap[name] = _SymbolCreator(name, plist[i])
+            name = ctypes.c_char_p()
-#pylint: skip-file
+# pylint: skip-file
-Version : 0.10
+__version__ = "0.1.0"
-    """load libary by searching possible path."""
+def find_lib_path():
-    dll_path = [api_path, curr_path]
+    dll_path = [curr_path, api_path]
-    lib = ctypes.cdll.LoadLibrary(lib_path[0])
+    if len(lib_path) == 0:
-            hmap[name.value] = _SymbolCreator(name.value, plist[i])
+            hmap[name] = _SymbolCreator(name, plist[i])
-    api_path = os.path.join(curr_path, '../../')
+    api_path = os.path.join(curr_path, '../../lib/')
-        singleton_ = ctypes.c_void_p()
+        singleton_ = SymbolHandle()
-        self.singleton = Symbol(singleton_)
+        if singleton_:
-        params : **kwargs
+        **kwargs
-    def __init__(self, name):
+    def __init__(self, name, handle):
-        self.use_param = use_param.value
+        self.handle = handle
-        params : kwargs
+        params : **kwargs
-            c_str(self.name),
+        check_call(_LIB.MXSymbolCreateFromAtomicSymbol(
-        plist = ctypes.POINTER(ctypes.c_char_p)()
+        plist = ctypes.POINTER(ctypes.c_void_p)()
-                                   ctypes.byref(plist)))
+        check_call(_LIB.MXSymbolListAtomicSymbolCreators(ctypes.byref(size),
-            hmap[name.value] = _SymbolCreator(name.value)
+            name = _LIB.MXSymbolGetAtomicSymbolName(plist[i], ctypes.byref(name))
-    def __init__(self, handle, name):
+    def __init__(self, name):
-            self.handle,
+        check_call(_LIB.MXSymDescribe(
-            self.handle,
+        check_call(_LIB.MXSymCreate(
-        plist = ctypes.POINTER(ctypes.c_void_p)()
+        plist = ctypes.POINTER(ctypes.c_char_p)()
-                                          ctypes.byref(plist)))
+        check_call(_LIB.MXListSyms(ctypes.byref(size),
-            hmap[name.value] = _SymbolCreator(hdl, name.value)
+            name = plist[i]
-
+# coding: utf-8
-from .narray import NArray, _new_empty_handle
+from .base import c_array, c_str
-        for key, val in kwargs:
+        for _, val in kwargs:
-            keys = c_array(ctypes.c_char_p, map(c_str, kwargs.keys()))
+            keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()])
-from .narray import NArray, _new_empty_handle
+from .base import c_array, c_str
-        vals = c_array(ctypes.c_char_p, map(c_str, map(str, kwargs.values())))
+        keys = c_array(ctypes.c_char_p, [c_str(key) for key in kwargs.keys()])
-
+# coding: utf-8
-# pylint: disable=invalid-name
+# pylint: disable=invalid-name, protected-access
-from .narray import NArray, _init_function_registry
+from .narray import NArray
-op = _init_function_registry(_FunctionRegistry())
+op = NArray._init_function_registry(_FunctionRegistry())
-    """load libary by searching possible path"""
+    """load libary by searching possible path."""
-lib = _load_lib()
+_LIB = _load_lib()
-        raise MXNetError(lib.MXGetLastError())
+        raise MXNetError(_LIB.MXGetLastError())
-        """Constructing a context
+        """Constructing a context.
-    def __exit__(self, type, value, trace):
+    def __exit__(self, ptype, value, trace):
-    """Return the current context"""
+    """Return the current context.
-from .base import lib
+from .base import _LIB
-    """Function Object"""
+    """Function Object."""
-        check_call(lib.MXFuncDescribe(
+        check_call(_LIB.MXFuncDescribe(
-        check_call(lib.MXFuncInvoke(
+        check_call(_LIB.MXFuncInvoke(
-                                       ctypes.byref(plist)))
+        check_call(_LIB.MXListFunctions(ctypes.byref(size),
-            h = plist[i]
+            hdl = plist[i]
-            hmap[name.value] = _Function(h, name.value)
+            check_call(_LIB.MXFuncGetName(hdl, ctypes.byref(name)))
-# pylint: disable=invalid-name
+
-from .base import lib
+from .base import _LIB
-    """Return a new empty handle
+    """Return a new empty handle.
-    return h
+    hdl = NArrayHandle()
-    """Return a new handle with specified shape, context
+    """Return a new handle with specified shape and context.
-    check_call(lib.MXNArrayCreate(
+    hdl = NArrayHandle()
-    return h
+        ctypes.byref(hdl)))
-        check_call(lib.MXNArrayFree(self.handle))
+        check_call(_LIB.MXNArrayFree(self.handle))
-            op.plus.invoke_with_handle_((other.handle, self.handle), (), (hret,))
+            NArray._op.plus.invoke_with_handle_((other.handle, self.handle), (), (hret,))
-            op.minus.invoke_with_handle_((other.handle, self.handle), (), (hret,))
+            NArray._op.minus.invoke_with_handle_((other.handle, self.handle), (), (hret,))
-            op.mul.invoke_with_handle_((other.handle, self.handle), (), (hret,))
+            NArray._op.mul.invoke_with_handle_((other.handle, self.handle), (), (hret,))
-            op.div.invoke_with_handle_((other.handle, self.handle), (), (hret,))
+            NArray._op.div.invoke_with_handle_((other.handle, self.handle), (), (hret,))
-        check_call(lib.MXNArrayWait(self.handle))
+        """Wait until the data on current NArray is available."""
-        """Get shape of current NArray
+        """Get shape of current NArray.
-        check_call(lib.MXNArrayGetShape(
+        check_call(_LIB.MXNArrayGetShape(
-        """Get context of current NArray
+        """Get context of current NArray.
-        check_call(lib.MXNArrayGetContext(
+        check_call(_LIB.MXNArrayGetContext(
-        """Return a numpy representation of current array
+        """Return a numpy representation of current array.
-        check_call(lib.MXNArrayGetData(self.handle, ctypes.byref(pdata)))
+        check_call(_LIB.MXNArrayGetData(self.handle, ctypes.byref(pdata)))
-        """copy the content of current array to othe
+        """copy the content of current array to other.
-            op.copy.invoke_with_handle_((self.handle,), (), (other.handle,))
+            NArray._op.copy.invoke_with_handle_((self.handle,), (), (other.handle,))
-            op.copy.invoke_with_handle_((self.handle,), (), (hret,))
+            NArray._op.copy.invoke_with_handle_((self.handle,), (), (hret,))
-    """Create a new NArray, with specified shape
+    """Create a new NArray, with specified shape.
-    return op
+# pylint: disable=invalid-name
-        raise MXNetError(lib.MXGetLastError());
+        raise MXNetError(lib.MXGetLastError())
-    
+
-    """Convert a ctypes pointer to a numpy array 
+    """Convert a ctypes pointer to a numpy array
-    
+
-    return np.frombuffer(dbuffer, dtype = np.float32).reshape(shape)
+    return np.frombuffer(dbuffer, dtype=np.float32).reshape(shape)
-class Context:
+class Context(object):
-    # static class variable 
+    # static class variable
-    def __init__(self, device_type, device_id = 0):
+    devmask2type = {1: 'cpu', 2: 'gpu'}
-        Context.default_ctx= self._old_ctx
+        Context.default_ctx = self._old_ctx
-# initialize the default context in Context        
+# initialize the default context in Context
-from .base import mx_uint, mx_float, NArrayHandle, FunctionHandle
+from .base import mx_uint, mx_float, NArrayHandle
-class _Function:
+class _Function(object):
-                                      self.n_used_vars + self.n_scalars) 
+                                      self.n_used_vars + self.n_scalars)
-        
+            provide the NArray to store the result of the operation
-                
+
-        
+
-        
+
-        
+
-class _FunctionRegistry:    
+class _FunctionRegistry(object):
-    
+
-    NArray is basic ndarray like data structure in mxnet    
+
-            NArray handle of C API        
+            NArray handle of C API
-        return NArray(handle = hret)
+        return NArray(handle=hret)
-        return NArray(handle = hret)
+        return NArray(handle=hret)
-        return NArray(handle = hret)
+        return NArray(handle=hret)
-    
+        return NArray(handle=hret)
-        
+
-        
+
-        
+
-        
+
-        check_call(lib.MXNArrayGetData(self.handle, ctypes.byref(pdata)))        
+        check_call(lib.MXNArrayGetData(self.handle, ctypes.byref(pdata)))
-    
+
-        
+        """copy the content of current array to othe
-        
+
-        
+
-            return NArray(handle = hret)
+            return NArray(handle=hret)
-def create(shape, ctx = Context.default_ctx):
+def create(shape, ctx=Context.default_ctx):
-    
+
-    return NArray(handle = _new_alloc_handle(shape, ctx, False))
+    return NArray(handle=_new_alloc_handle(shape, ctx, False))
-    
+
-    
+
-b = mx.narray.create((3000,4000))
+a = mx.narray.create((3000, 4000))
-from .base import check_call
+from .base import check_call, MXNetError
-    ACCEPT_EMPTY_MUTATE_TARGET = 3
+    SCALAR_ARG_BEFORE_NARRAY = 1 << 1
-                raise MXNetError('expect %d mutate_vars in function %s', self.n_mutate_vars, self.name)
+                raise MXNetError('expect %d mutate_vars in op.%s', self.n_mutate_vars, self.name)
-                raise MXNetError('mutate_vars argument is required to call this function')
+            else:
-from .narray import NArray
+
-        c_array(NArrayHandle, mutate_vars)))
+# coding: utf-8
-from .base import invoke
+global op
-            invoke(op.plus, (other.handle, self.handle), (), (hret,))
+            op.plus.invoke_with_handle_((other.handle, self.handle), (), (hret,))
-            invoke(op.minus, (other.handle, self.handle), (), (hret,))
+            op.minus.invoke_with_handle_((other.handle, self.handle), (), (hret,))
-            invoke(op.mul, (other.handle, self.handle), (), (hret,))
+            op.mul.invoke_with_handle_((other.handle, self.handle), (), (hret,))
-            invoke(op.div, (other.handle, self.handle), (), (hret,))
+            op.div.invoke_with_handle_((other.handle, self.handle), (), (hret,))
-            invoke(op.copy, (self.handle,), (), (other.handle,))
+            op.copy.invoke_with_handle_((self.handle,), (), (other.handle,))
-            invoke(op.copy, (self.handle,), (), (hret,))
+            op.copy.invoke_with_handle_((self.handle,), (), (hret,))
-from .narray import zeros_shared
+from .context import Context, current_context
-    """Convert a ctypes pointer to a numpy array.
+def ctypes2numpy_shared(cptr, shape):
-    a copy of nupy array : numpy array
+    a numpy array : numpy array
-    return res
+    size = 1
-from .base import ctypes2numpy
+from .base import ctypes2numpy_shared
-    Empty handle is only used to hold results
+def _new_alloc_handle(shape, ctx, delay_alloc):
-        """Return a copy of numpy NArray
+    @property
-        a tuple representing shape of current narray
+        the context of current NArray
-
+        check_call(lib.MXNArrayGetData(self.handle, ctypes.byref(pdata)))        
-    """Create a new CPU based narray that shares memory content with a numpy array
+def create(shape, ctx = Context.default_ctx):
-    a new NArray that shares memory with numpy.narray
+    a new NArray
-    return ret
+    return NArray(handle = _new_alloc_handle(shape, ctx, False))
-b = mx.zeros_shared((3,4))
+a = mx.narray.create((3000,4000))
-print(c.to_numpy())
+c = b * a
-    dll_path = [api_path]
+    """load libary by searching possible path"""
-    """check the return value of C API call
+    """Check the return value of C API call
-    this function will raise exception when error occurs    
+    This function will raise exception when error occurs.
-    """get ctypes array 
+    """Create ctypes array from a python array
-    values : tuple like
+    values : tuple or list
-    """convert a ctypes pointer array to a numpy array.
+    """Convert a ctypes pointer to a numpy array.
-    if not isinstance(cptr, ctypes.POINTER(ctypes.c_float)):
+    if not isinstance(cptr, ctypes.POINTER(mx_float)):
-class FunctionRegistry:
+class _FunctionRegistry:    
-op = FunctionRegistry()
+op = _FunctionRegistry()
-    """invoke a function handle by passing in arguments as tuples
+    """Invoke a function handle by passing in arguments as tuples
-    fhandle : ctypes.c_void_p
+    fhandle : FunctionHandle
-        c_array(ctypes.c_void_p, mutate_vars)))
+        c_array(NArrayHandle, used_vars),
-_h_div = op.div
+def _new_empty_handle():
-        handle : ctypes.c_void_p
+        handle : NArrayHandle
-        assert isinstance(handle, ctypes.c_void_p)
+        assert isinstance(handle, NArrayHandle)
-    def __lbinary__(self, handle, other):
+    def __add__(self, other):
-            return NArray(handle = hret)
+            invoke(op.plus, (other.handle, self.handle), (), (hret,))
-            raise MXNetError('type ' + str(other) + 'not supported')            
+            raise MXNetError('type %s not supported' % str(type(other)))
-        return self.__lbinary__(_h_plus, other)
+    def __radd__(self, other):
-        return self.__lbinary__(_h_plus, other)
+        hret = _new_empty_handle()
-    def get_shape(self):
+    @property
-        pdata = ctypes.POINTER(ctypes.c_uint)()
+        ndim = mx_uint()
-    
+
-        return ctypes2numpy(pdata, shape)
+        pdata = ctypes.POINTER(mx_float)()
-    h = ctypes.c_void_p()
+    h = NArrayHandle()
-        c_array(ctypes.c_uint, shape), 
+        c_array(mx_uint, shape), 
-c = b + a
+c = b / a
-from narray import zeros_shared
+from __future__ import absolute_import
-from base import MXNetError
+from .base import lib
-print a.numpy
+print(a.numpy)
-print c.to_numpy()
+print(c.to_numpy())
-        print shape
+#!/usr/bin/env python
