-        raise GAE_Exception(603, "appid not support GAE")
+        raise GAE_Exception(603, "appid not exist %s" % appid)
-        sni = host
+        if not host:
-
+                stream.check_timeout(now)
-            stream.check_timeout(now)
+        if len(self.streams) > 0 and\
-        self.logger.debug("%s %s", self.url, err_text)
+        self.logger.warn("%s %s", self.url, err_text)
-            if response and response.status==200:
+            if response and response.status == 200:
-        self.set_var("dispather_max_workers", 2)
+        self.set_var("dispather_max_workers", 3)
-        self.set_var("http2_max_concurrent", 30)
+        self.set_var("http2_max_concurrent", 40)
-        best_score = 9999999
+        best_score = 999999999
-        ca_certs = None
+        ca_certs = os.path.join(current_path, "cacert.pem")
-        ca_certs = None
+        ca_certs = os.path.join(current_path, "cacert.pem")
-            "accounts.google.com"
+            "appengine.google.com"
-            "mail.google.com"
+            "mail.google.com",
-            "..googleapis.com",
+            ".googleapis.com",
-        self.set_var("min_intermediate_CA", 3)
+        self.set_var("min_intermediate_CA", 2)
-        self.set_var("ipv6_scan_ratio", 90) # 0 - 100
+        self.set_var("ipv6_scan_ratio", 50) # 0 - 100
-        ca_certs = os.path.join(current_path, "cacert.pem")
+        #ca_certs = os.path.join(current_path, "cacert.pem")
-        ca_certs = os.path.join(current_path, "cacert.pem")
+        #ca_certs = os.path.join(current_path, "cacert.pem")
-        sni = self.sni_manager.get()
+        #sni = self.sni_manager.get()
-        self.set_var("front_continue_fail_block", 180)
+        self.set_var("front_continue_fail_block", 20 * 60)
-        self.set_var("hosts_gae", [
+            "appengine.google.com",
-            ".google.com",
+            #".google.com",
-        self.set_var("hosts_gae_endswith", [])
+
-                self.load_old_config(fn)
+                need_save += self.load_old_config(fn)
-            return
+            return 0
-                  response.status, host, path, (time.time()-time_request)*1000, e)
+        xlog.exception("DIRECT %s %d %s %s, t:%d send to client except:%r",
-        out_list.append(response.text)
+        content = response.text
-
+        except Exception as e:
-                time_to_ping = max(self.config.http1_ping_interval - (time.time() - self.last_active_time), 0.2)
+                time_to_ping = max(self.config.http1_ping_interval - (time.time() - self.last_recv_time), 0.2)
-                        time.time() - self.last_active_time > self.config.http1_ping_interval - 1:
+                        time.time() - self.last_recv_time > self.config.http1_ping_interval - 1:
-                time_to_sleep = max(self.config.http1_idle_time - (time.time() - self.last_active_time), 0.2)
+                time_to_sleep = max(self.config.http1_idle_time - (time.time() - self.last_recv_time), 0.2)
-                if not self.request_onway and time.time() - self.last_active_time > self.config.http1_idle_time:
+                if not self.request_onway and time.time() - self.last_recv_time > self.config.http1_idle_time:
-            elif task == "ping":
+
-                    continue
+
-                self.logger.warn("get task but inactive time:%d", time_now - self.last_active_time)
+            if time_now - self.last_recv_time > self.config.http1_idle_time:
-                self.close("inactive timeout %d" % (time_now - self.last_active_time))
+                self.close("inactive timeout %d" % (time_now - self.last_recv_time))
-            self.last_active_time = time_now
+            self.last_send_time = time_now
-                           self.ip, e, time.time()-self.last_active_time, task.timeout)
+                             self.ip, e, time.time() - self.last_recv_time, task.timeout)
-        self.last_active_time = time.time()
+        self.last_recv_time = time.time()
-        self.last_active_time = self.ssl_sock.create_time - 1
+
-            self.logger.debug("%s _consume_single_frame:%r, inactive time:%d", self.ip, e, time.time()-self.last_active_time)
+            self.logger.debug("%s _consume_single_frame:%r, inactive time:%d", self.ip, e, time.time() - self.last_recv_time)
-        self.last_active_time = time.time()
+            self.last_recv_time = time.time()
-            time_cost = time.time() - self.last_active_time
+            time_cost = time.time() - self.last_recv_time
-            inactive_time = time.time() - self.connection.last_active_time
+            inactive_time = time.time() - self.connection.last_recv_time
-                return
+    def check_timeout(self, now):
-            self.connection.close("down fail")
+        #if self.connection.continue_timeout >= self.connection.config.http2_max_timeout_tasks and \
-        self.last_active_time = ssl_sock.create_time
+        self.last_recv_time = self.ssl_sock.create_time
-        self.logger.debug("%s worker close:%s", self.ip, reason)
+        if reason not in ["idle timeout"]:
-        inactive_time = now - self.last_active_time
+        inactive_time = now - self.last_recv_time
-                    (now - best_worker.last_active_time) < self.config.dispather_work_min_idle_time or
+                    (now - best_worker.last_recv_time) < self.config.dispather_work_min_idle_time or
-                    (best_worker and (now - best_worker.last_active_time) >= self.config.dispather_work_min_idle_time):
+                    (best_worker and (now - best_worker.last_recv_time) >= self.config.dispather_work_min_idle_time):
-                        (now-w.ssl_sock.create_time), (now-w.last_active_time), w.processed_tasks)
+                        (now-w.ssl_sock.create_time), (now-w.last_recv_time), w.processed_tasks)
-        self.logger.debug("%s close:%s", ip, reason)
+        # if reason not in ["idle timeout"]:
-        return slice
+        self.source = random_get_slice.RandomGetSlice(list_fn, 200)
-        line = self.get_slice()
+        line = self.source.get()
-
+class DontFakeCA(Exception):
-            ".appspot.com"
+            ".appspot.com",
-import struct
+import xstruct as struct
-            raise socket.error('certficate is none')
+            raise socket.error('certificate is none, sni:%s' % ssl_sock.sni)
-            raise socket.error(' certficate is issued by %r' % (issuer_commonname))
+            raise socket.error(' certificate is issued by %r' % (issuer_commonname))
-            raise socket.error('certficate is none')
+            raise socket.error('certificate is none')
-import struct
+import xstruct as struct
-import struct
+import xstruct as struct
-            raise socket.error(' certficate is none')
+            raise socket.error(' certificate is none, sni:%s' % ssl_sock.sni)
-import struct
+import xstruct as struct
-
+        start_time = time.time()
-          len(front.connect_manager.gae_conn_pool.pool) + \
+    self_check_response_data = "HTTP/1.1 200 OK\r\n" \
-                proxy_type = PROXY_TYPE_SOCKS5
+                    rdns = True
-            proxy_type="socks5", proxy_addr="127.0.0.1", proxy_port=1080, timeout=15
+            proxy_type="socks5h", proxy_addr="127.0.0.1", proxy_port=1080, timeout=15
-    if req.path.startswith("https"):
+    if isinstance(req.connection, ssl.SSLSocket):
-            ssl_sock = ssl.wrap_socket(remote_sock)
+            # TODO: send SNI
-        ssl_sock = remote_sock
+        remote_ssl_sock = remote_sock
-        sock = SocketWrap(sock, "x-tunnel", port, host)
+        sock = SocketWrap(sock, client_address[0], client_address[1])
-    sw = SocketWrap(ssl_sock, client_address[0], client_address[1])
+    remote_ssl_sock.send(left_buf)
-                ssl_sock = g.gae_proxy.proxy_handler.wrap_ssl(sock._sock, host, port, client_address)
+                sock._sock = g.gae_proxy.proxy_handler.wrap_ssl(sock._sock, host, port, client_address)
-    ssl_sock.setblocking(1)
+    sock.setblocking(1)
-    req = g.gae_proxy.proxy_handler.GAEProxyHandler(ssl_sock, client_address, None, xlog)
+    req = g.gae_proxy.proxy_handler.GAEProxyHandler(sock._sock, client_address, None, xlog)
-        req.path = '%s://%s%s' % (schema, req.headers['Host'], req.path)
+        url = '%s://%s%s' % (schema, req.headers['Host'], req.path)
-    if req.path in ["http://www.twitter.com/xxnet",
+    if url in ["http://www.twitter.com/xxnet",
-        raise NotSupported(req, ssl_sock)
+        raise NotSupported(req, sock)
-    #    raise NotSupported(req, ssl_sock)
+    if len(req.path) >= 2048:
-    if req.command not in ["GET", "PUT", "POST", "DELETE", "PATCH", "HEAD"]:
+    if req.command not in ["GET", "PUT", "POST", "DELETE", "PATCH", "HEAD", "OPTIONS"]:
-        raise NotSupported(req, ssl_sock)
+        raise NotSupported(req, sock)
-                    xlog.info("%s %s:%d try gae", scense, host, port)
+                    # xlog.info("%s %s:%d try gae", scense, host, port)
-        self.set_var("dispather_min_idle_workers", 10)
+        self.set_var("dispather_min_idle_workers", 5)
-        self.set_var("dispather_work_max_score", 200000)
+        self.set_var("dispather_work_max_score", 1000)
-        self.set_var("target_handshake_time", 600)
+        self.set_var("target_handshake_time", 200)
-def pack_request(method, url, headers, body):
+def pack_request(method, url, headers, body, timeout):
-    kwargs['timeout'] = '19'
+    kwargs['timeout'] = str(timeout)
-def request_gae_proxy(method, url, headers, body, timeout=60, retry=True):
+def request_gae_proxy(method, url, headers, body, timeout=None):
-            raise GAE_Exception(600, b"".join(error_msg))
+    if not timeout:
-    max_retry = 3
+    bufsize = 65535
-        self.close_connection = 0
+        #self.close_connection = 0
-        if host == self.fake_host:
+        elif host == self.fake_host:
-        self.parsed_url = urlparse.urlparse(self.path)
+        if isinstance(self.connection, ssl.SSLSocket):
-            return self.do_AGENT()
+        if self.path[0] == '/':
-                return self.do_DIRECT()
+                return self.go_DIRECT()
-            return self.do_AGENT()
+            return self.go_AGENT()
-                return self.do_DIRECT()
+            if method == "https":
-        return self.do_AGENT()
+        return self.go_AGENT()
-    def do_AGENT(self):
+    def go_AGENT(self):
-                path = self.parsed_url[2]
+        gae_handler.handler(self.command, self.url, request_headers, payload, self.wfile)
-        xlog.debug('DIRECT %s %s', self.command, url)
+    def go_DIRECT(self):
-                xlog.error('Direct %s read payload failed:%s', url, e)
+                xlog.error('Direct %s read payload failed:%s', self.url, e)
-            direct_handler.handler(self.command, host, path, request_headers, payload, self.wfile)
+            direct_handler.handler(self.command, self.host, self.path, request_headers, payload, self.wfile)
-            xlog.warn('DIRECT %s %s except:%r', self.command, url, e)
+            xlog.warn('DIRECT %s %s except:%r', self.command, self.url, e)
-        time.sleep(1)
+        sys_tray.serve_forever()
-            return len(self.pool)
+    def qsize(self):
-    def get(self, block=True, timeout=None, only_h1=False):
+    def get(self, block=True, timeout=None):
-                if self.qsize(only_h1=only_h1) == 0:
+                if self.qsize() == 0:
-                while self.qsize(only_h1=only_h1) == 0:
+                while self.qsize() == 0:
-                while not self.qsize(only_h1=only_h1):
+                while not self.qsize():
-            item = self._get(only_h1=only_h1)
+            item = self._get()
-        return self.get(block=False, only_h1=only_h1)
+    def get_nowait(self):
-    def _get(self, only_h1=False):
+    def _get(self):
-                out_str += "%d \t %s handshake:%d not_active_time:%d h2:%d\r\n" % (i, sock.ip, t, time.time() - sock.last_use_time, sock.h2)
+                out_str += "%d \t %s handshake:%d not_active_time:%d \r\n" % (i, sock.ip, t, time.time() - sock.last_use_time)
-        self.class_name = "ConnectFactory"
+        self.class_name = "ConnectManager"
-        self.http1_get_num = 0
+        self.no_ip_lock = threading.Lock()
-        if self.http1_get_num or self.https_get_num:
+        if self.https_get_num:
-                #self.logger.warning("no enough ip")
+                with self.no_ip_lock:
-            ssl_sock = self._create_ssl_connection( (ip_str, 443) )
+            ssl_sock = self._create_ssl_connection(ip_str)
-            pass
+            self.logger.exception("connect_process except:%r", e)
-        sock = None
+    def _create_ssl_connection(self, ip):
-            if not self.check_local_network.IPv4.is_ok():
+            if not self.check_local_network.is_ok(ip):
-    def get_ssl_connection(self, host=''):
+    def get_ssl_connection(self):
-        ssl_sock = None
+        start_time = time.time()
-                        break
+            while self.running:
-                            return None
+                else:
-            elif len(self.read_buffers[0]) >= size:
+                self.read_buffers.pop(0)
-            if idle_num < 10 or \
+            if idle_num < self.config.dispather_max_idle_workers or \
-            self.check_ip_thread.start()
+            self.scan_all_exist_ip()
-            self.logger.error("set_ip input")
+            self.logger.error("update_ip input error:%s", ip)
-            if not self.check_local_network.is_ok():
+            if not self.check_local_network.is_ok(ip):
-                        self.good_ip_num -= 1
+    elif isinstance(element, memoryview):
-        raise ValueError("Non string type.")
+        raise ValueError("Non string type:%s" % type(element))
-        raise NotSupported(req, ssl_sock)
+    #if len(req.path) >= 2084:
-        response = gae_proxy.gae_handler.request_gae_proxy(method, url, headers, data, timeout=timeout, retry=False)
+        response = gae_proxy.gae_handler.request_gae_proxy(method, url, headers, data, timeout=timeout)
-    xlog.info('Using PyGTK as the GUI Backend.')
+            xlog.info('Gtk.StatusIcon used.')
-    if X_is_running() and (has_gi() or has_pygtk()):
+    if X_is_running() and (has_pygtk() or has_gi()):
-            length += len(data)
+            data_len = len(data)
-                    wfile.write('0\r\n\r\n')
+                if not data_len:
-                wfile.write('\r\n')
+                wfile._sock.sendall('%x\r\n' % data_len)
-                if not data:
+                if not data_len:
-        xlog.info("DIRECT %d %s %s, t:%d send to client except:%r",
+        xlog.warn("DIRECT %d %s %s, t:%d send to client except:%r",
-                return self.Direct()
+                return self.do_DIRECT()
-                return self.Direct()
+                return self.do_DIRECT()
-                host = urlparse.urlparse(self.path).netloc
+        self.do_DIRECT()
-            request_headers = dict((k.title(), v) for k, v in self.headers.items())
+        xlog.debug('DIRECT %s %s', self.command, url)
-                    return
+        try:
-            time.sleep(100)
+        threading.Thread(target=sys_tray.serve_forever).start()
-    sys.exit()
+        sys.exit()
-                       (w.ip, w.rtt, w.keep_running,  w.accept_task,
+            out_str += "%s score:%d rtt:%d running:%d accept:%d live:%d inactive:%d processed:%d" % \
-        black, white = g.gfwlist.get_pac_string()
+        black = '",\n"'.join(g.gfwlist.gfw_black_list
-            "Connection: Keep-Alive, Persist"\
+            "Connection: Keep-Alive, Persist\r\n"\
-#from front_base.connect_creator import ConnectCreator
+from front_base.random_get_slice import RandomGetSlice
-    def __init__(self, config, logger,):
+    def __init__(self, config, logger):
-        self.public_appids_fsize = os.path.getsize(fn)
+        self.public_appid = RandomGetSlice(fn, 60)
-                appid = self.get_public_appid()
+                appid = self.public_appid.get()
-
+            #"www.google.com"
-        self.set_var("max_good_ip_num", 1000)
+        self.set_var("max_good_ip_num", 500)
-def handler(method, host, path, headers, body, wfile, timeout=30):
+def handler(method, host, path, headers, body, wfile, timeout=60):
-                if response.status > 400:
+                if response.status > 600:
-                        response.close()
+                        #response.close()
-        for key, value in response.headers.items():
+        for key in response_headers:
-        self.host_manager = host_manager.HostManager(self.config, logger)
+    def start(self):
-            logger, self.config, self.openssl_context, self.host_manager)
+        self.host_manager = host_manager.HostManager(self.config, logger)
-        self.host_manager = HostManagerBase()
+        self.host_manager = host_manager.HostManager(front.config, logger)
-            logger, front.config, self.connect_creator, front.ip_manager, check_local_network)
+            logger, front.config, self.connect_creator, self.ip_manager, check_local_network)
-                          'Cache-Control'
+                          #'Connection',
-def request_gae_proxy(method, url, headers, body, timeout=30, retry=True):
+def request_gae_proxy(method, url, headers, body, timeout=60, retry=True):
-                xlog.debug("send to browser wfile.write ret:%d", ret)
+                #xlog.debug("send to browser wfile.write ret:%d", ret)
-                              response.ssl_sock.ip, self.url)
+                    xlog.warn("RangeFetch [%s] get body fail, begin:%d %s",
-    def get_sni_host(self, ip):
+    def get_host(self):
-            raise Exception()
+            return ""
-        return sni, top_domain
+        return appid + ".appspot.com"
-from front import front
+from front import front, direct_front
-    xlog.info("start to terminate GAE_Proxy")
+    direct_front.stop()
-               "Content-Length: 2\r\n\r\nOK"
+            "Access-Control-Allow-Origin: *\r\n"\
-
+        self.close_connection = 0
-            self.path = 'http://%s%s' % (host, self.path)
+            self.path = '%s://%s%s' % (method, host, self.path)
-            xlog.debug("%s %s", self.command, self.path)
+            # xlog.debug("%s %s", self.command, self.path)
-            return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\nContent-Length: 0\r\n\r\n' % self.path.replace('http://', 'https://', 1)).encode())
+            if isinstance(self.connection, ssl.SSLSocket):
-            return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\nContent-Length: 0\r\n\r\n' % self.path.replace('http://', 'https://', 1)).encode())
+            if isinstance(self.connection, ssl.SSLSocket):
-        xlog.debug("GAE %s %s", self.command, self.path)
+        xlog.debug("GAE %s %s from:%s", self.command, self.path, self.address_string())
-
+        self.close_connection = 0
-        self.__realconnection = self.connection
+        self.Direct()
-        xlog.debug('GAE CONNECT Direct %s %s', self.command, self.path)
+        xlog.debug('Direct %s %s', self.command, self.path)
-
+from front_base.random_get_slice import RandomGetSlice
-        return slice
+        self.slice = RandomGetSlice(fn, 20, '|')
-            w = self.get_slice()
+            w = self.slice.get()
-            raise socket.error(' certficate is none')
+            raise socket.error('certficate is none')
-
+            # get_subj_alt_name cost near 100ms. be careful.
-            self.close("disconnect:%r" % e)
+            self.close("ConnectionReset:%r" % e)
-        data = self._recv_payload(length)
+        try:
-                    buff_view[p:] = data
+                    buff_view[p:p+len(data)] = data
-            rtt += len(self.streams) * 100
+            rtt += len(self.streams) * 500
-        self.request_queue.put(task)
+        if self.task_count > self.config.max_task_num:
-            self.last_fail_time = time.time()
+        with self.task_count_lock:
-        return response
+        try:
-                last_ip = self.ip_list[-1]
+                if len(self.ip_list) > 100:
-                self.logger.exception("google_ip.runJob fail:%r", e)
+                self.logger.exception("scan_ip_worker except:%r", e)
-#openssl_version = OpenSSL.version.__version__
+# This is a throwaway variable to deal with a python bug
-                continue
+            except OpenSSL.SSL.ZeroReturnError as e:
-                #xlog.exception("set_alpn_protos:%r", e)
+                # self.logger.exception("set_alpn_protos:%r", e)
-        return line
+import os
-        sock.settimeout(300)
+        sock.settimeout(60)
-        self.logger.warn("unhandler WebSocket from %s", self.address_string())
+        self.logger.warn("unhandled WebSocket from %s", self.address_string())
-            time.sleep(1)
+        self.server_close()
-            dest_addr = socket.gethostbyname(dest_addr)
+            if self.resolve_dest:
-from gae_proxy.local import check_local_network
+import socks
-        leaddata = left_buf + sock.recv(1024, socket.MSG_PEEK)
+        leaddata = left_buf + sock.recv(65535, socket.MSG_PEEK)
-    if req.path in ["http://www.twitter.com/xxnet", "https://www.twitter.com/xxnet"]:
+    if req.path in ["http://www.twitter.com/xxnet",
-                    xlog.info("%s %s:%d gae", scense, host, port)
+                    # sni_host = get_sni(sock, left_buf)
-                    xlog.warn("%s domain:%s sni:%s fail:%r", scense, host, sni_host, e)
+                    xlog.warn("%s domain:%s fail:%r", scense, host, e)
-    config.set_var("min_on_road", 2)
+    config.set_var("min_on_road", 1)
-    config.set_var("send_delay", 100)
+    config.set_var("send_delay", 30)
-    config.set_var("roundtrip_timeout", 15)
+    config.set_var("roundtrip_timeout", 25)
-                res = self.check_ip.check_ip(ip, host=host)
+                res = self.check_ip.check_ip(ip, sni=host, host=host)
-    host = "cloudflare.com"
+    host = "xx-net.net"
-    exit(0)
+    #check_all_ip(check_ip)
-    res = check_ip.check_ip(ip, host=host, wait_time=wait_time)
+    res = check_ip.check_ip(ip, sni=host, host=host, wait_time=wait_time)
-        self.set_var("target_handshake_time", 550)
+        self.set_var("max_good_ip_num", 50)
-        self.set_var("dispather_score_factor", 100)
+        self.set_var("dispather_max_workers", 2)
-        #self.set_var("http2_status_to_close", [])
+        self.set_var("http2_max_concurrent", 30)
-from front_base.connect_creator import ConnectCreator
+#from front_base.connect_creator import ConnectCreator
-from front_base.random_get_line import RandomGetLine
+from front_base.random_get_slice import RandomGetSlice
-class HostManager(RandomGetLine):
+class HostManager(RandomGetSlice):
-        sni = self.get_line()
+        sni = self.get()
-        self.set_var("appids", ["xxnet4.herokuapp.com"])
+        self.set_var("appids", []) # "xxnet4.herokuapp.com"
-        stat["version"] = g.xxnet_version()
+        stat["version"] = g.xxnet_version
-                self.server_send_buf_size -= g.config.max_payload
+                self.server_send_buf_size -= g.config.max_payload /4
-                    ('restart gae_proxy', self.on_restart_gae_proxy),
+                    (u'Reset Each Module', self.on_restart_each_module),
-        module_init.start("gae_proxy")
+    def on_restart_each_module(self, widget=None, data=None):
-        self.set_var("max_good_ip_num", 3000)
+        self.set_var("max_good_ip_num", 1000)
-        self.set_var("dispather_score_factor", 10)
+        self.set_var("dispather_score_factor", 100)
-        self.check_ip = CheckIp(xlog.null, self.config, self.connect_creator)
+        self.ip_checker = CheckIp(xlog.null, self.config, self.connect_creator)
-            self.check_ip.check_ip,
+            self.check_ip,
-        self.appid_manager.check_api = self.check_ip.check_ip
+        self.appid_manager.check_api = self.ip_checker.check_ip
-    raise "get_version_fail:" % readme_file
+    raise "get_version_fail:%s" % readme_file
-    def check_ip(self, ip, host=None, wait_time=0):
+    def check_ip(self, ip, sni=None, host=None, wait_time=0):
-            ssl_sock = self.connect_creator.connect_ssl(ip, sni=host)
+            ssl_sock = self.connect_creator.connect_ssl(ip, sni=sni)
-            raise socket.error(' certficate is issued by %r, not Google' % (issuer_commonname))
+            raise socket.error(' certficate is issued by %r' % (issuer_commonname))
-
+import random
-            # self.logger.debug("%s Send:%s", self.ip, str(frame))
+            if self.config.http2_show_debug:
-        # self.logger.debug("%s Recv:%s", self.ip, str(frame))
+        if self.config.http2_show_debug:
-        self.add_header(":Path", self.task.path)
+        self.add_header(":method", self.task.method)
-        self.send_left_body()
+        if self.request_body_left > 0:
-
+import json
-    default_ip = "172.64.50.59"
+    default_ip = "141.101.120.131"
-    host = None
+    host = "cloudflare.com"
-        self.set_var("dispather_min_idle_workers", 1)
+        self.set_var("dispather_min_idle_workers", 0)
-        self.set_var("dispather_max_workers", 2)
+        self.set_var("dispather_max_workers", 1)
-            self.logger.debug("%s %s%s status:%d trace:%s", method, response.worker.host, path, status,
+            self.logger.debug("%s %s%s status:%d trace:%s", method, response.worker.ssl_sock.host, path, status,
-            self.logger.warn("%s %s%s status:%d trace:%s", method, response.worker.host, path, status,
+            self.logger.warn("%s %s%s status:%d trace:%s", method, response.worker.ssl_sock.host, path, status,
-    notify.init('XX-Net Notify')
+    import gi
-    notify = None
+    import pygtk
-    appindicator = None
+if use_gi:
-        trayicon.set_status(appindicator.IndicatorStatus.ACTIVE)
+        trayicon = new_appindicator('XX-Net', 'indicator-messages', appind_category)
-        trayicon.connect('popup-menu', lambda i, b, t: self.make_menu().popup(None, None, trayicon.position_menu, trayicon, b, t))
+        trayicon.connect('popup-menu', lambda i, b, t: popup_trayicon_menu(self.make_menu(), trayicon, b, t))
-        n = notify.Notification.new('Test', msg)
+        n = new_notification('Test', msg)
-    if X_is_running():
+    def has_gi():
-gdk.threads_init()
+pygtk.require('2.0')
-    notify.init('XX-Net Notify')
+    import pynotify
-    notify = None
+    xlog.warn("import pynotify fail, please install python-notify if possiable.")
-    from gi.repository import AppIndicator3 as appindicator
+    import platform
-            self.trayicon = self.appind_trayicon(logo_filename)
+        if platform and appindicator and platform.dist()[0].lower() == 'ubuntu':
-        trayicon.set_status(appindicator.IndicatorStatus.ACTIVE)
+    def ubuntu_trayicon(self, logo_filename):
-        trayicon.connect('popup-menu', lambda i, b, t: self.make_menu().popup(None, None, trayicon.position_menu, trayicon, b, t))
+        trayicon.connect('popup-menu', lambda i, b, t: self.make_menu().popup(None, None, gtk.status_icon_position_menu, b, t, self.trayicon))
-        trayicon.set_tooltip_text('XX-Net')
+        trayicon.set_tooltip('XX-Net')
-        if not notify:
+        if not pynotify:
-        n = notify.Notification.new('Test', msg)
+        n = pynotify.Notification('Test', msg)
-        gdk.threads_enter()
+        gtk.gdk.threads_enter()
-        gdk.threads_leave()
+        gtk.gdk.threads_leave()
-            from gi.repository import Gtk as gtk
+            import pygtk
-gtk.gdk.threads_init()
+import gi
-    pynotify.init('XX-Net Notify')
+    gi.require_version('Notify', '0.7')
-    pynotify = None
+    xlog.warn("import Notify fail, please install libnotify if possible.")
-    import appindicator
+    gi.require_version('AppIndicator3', '0.1')
-            self.trayicon = self.ubuntu_trayicon(logo_filename)
+        if appindicator:
-        trayicon.set_status(appindicator.STATUS_ACTIVE)
+    def appind_trayicon(self, logo_filename):
-        trayicon.connect('popup-menu', lambda i, b, t: self.make_menu().popup(None, None, gtk.status_icon_position_menu, b, t, self.trayicon))
+        trayicon.connect('popup-menu', lambda i, b, t: self.make_menu().popup(None, None, trayicon.position_menu, trayicon, b, t))
-        trayicon.set_tooltip('XX-Net')
+        trayicon.set_tooltip_text('XX-Net')
-        if not pynotify:
+        if not notify:
-        n = pynotify.Notification('Test', msg)
+        n = notify.Notification.new('Test', msg)
-        gtk.gdk.threads_enter()
+        gdk.threads_enter()
-        gtk.gdk.threads_leave()
+        gdk.threads_leave()
-            import gtk
+            import gi
-        self.set_var("https_new_connect_num", 0)
+        self.set_var("connection_pool_min", 1)
-        self.set_var("support_http2", 0)
+        self.set_var("support_http2", 1)
-        self.set_var("max_good_ip_num", 500)
+        self.set_var("max_scan_ip_thread_num", 10)
-        raise GAE_Exception(600, "unpack protocol:%r" % e)
+        raise GAE_Exception(600, "unpack protocol:%r at:%s" % (e, traceback.format_exc()))
-        self.init_rtt = ssl_sock.handshake_time / 2
+        self.init_rtt = ssl_sock.handshake_time / 3
-            rtt = self.rtt = 200
+        if inactive_time > 30:
-            rtt += len(self.streams) * 500
+        if self.version == "1.1":
-            score = rtt + 5000
+        if inactive_time > 1:
-            score = rtt + (240 - inactive_time)*1
+            # inactive_time < 2
-        self.set_var("max_good_ip_num", 100)
+        self.set_var("max_scan_ip_thread_num", 5)
-            logger, ca_certs=ca_certs,
+            logger, ca_certs=ca_certs, support_http2=config.support_http2,
-    wfile.write(body)
+    try:
-def request_gae_proxy(method, url, headers, body, timeout=120, retry=True):
+def request_gae_proxy(method, url, headers, body, timeout=30, retry=True):
-            self.ip_manager.update_ip(self.ip, self.rtt)
+            #self.ip_manager.update_ip(self.ip, self.rtt)
-        self.network_buffer_size = 128 * 1024
+        self.network_buffer_size = 65535
-            rtt += len(self.streams) * 100
+            rtt += len(self.streams) * 500
-            score = rtt + (240 - inactive_time)*10
+            score = rtt + (240 - inactive_time)*1
-                    best_score > self.config.dispather_work_max_score):
+                    best_score > self.config.dispather_work_max_score or
-        max_slice_len = 120
+        max_slice_len = 200
-        slice = self.fd.read(max_slice_len)
+        slice = self.fd.read(max_slice_len * 2)
-                            d = d[sended:]
+                            d_view = memoryview(d)
-                        self.read_set.remove(s1)
+                        self.try_remove(self.read_set, s1)
-                        self.write_set.remove(s1)
+                        self.try_remove(self.write_set, s1)
-                        continue
+                    while True:
-                        continue
+                        try:
-                        continue
+                        if len(dat) - sended > 0:
-                        self.write_set.remove(s1)
+                        self.try_remove(self.write_set, s1)
-    g.xxnet_version = xxnet_version
+    g.xxnet_version = xxnet_version()
-        light_fronts.append(gae_front)
+        if gae_front.get_dispatcher():
-        self.host_manager = host_manager.HostManager(self.config.appids)
+        self.host_manager = host_manager.HostManager(self.logger, self.config.appids)
-            self.logger.debug("%s %s%s trace:%s", method, host, path, response.task.get_trace())
+
-                    pass
+            self.logger.debug("%s %s%s status:%d", method, host, path, status)
-            self.logger.exception("decode %s response except:%r", content, e)
+            self.logger.warn("decode %s response except:%r", content, e)
-    def __init__(self, appids=[]):
+    def __init__(self, logger, appids=[]):
-        if not g.config.login_account or not self.running:
+        if not g.config.login_account or not self.running or time.time() - self.last_send_time > 60:
-        self.set_var("AUTORANGE_MAXSIZE", 2097152)
+        self.set_var("AUTORANGE_MAXSIZE", 548576)
-    kwargs['maxsize'] = front.config.AUTORANGE_MAXSIZE
+    if url.endswith(".js"):
-            return False
+            return False
-    ips = g.dns_srv.query(host)
+    if g.config.block_advertisement and g.gfwlist.is_advertisement(host):
-                "auto_gae": g.config.auto_gae
+                "auto_gae": g.config.auto_gae,
-        max_id_len = 32
+        max_id_len = 50
-        slice = self.public_appids_fd.read(max_id_len)
+        slice = self.public_appids_fd.read(max_id_len * 2)
-        ns = slice.split("|")
+        ns = slice.split("\n")
-import socket
+import threading
-    if "." in ip:
+def is_ok(ip=None):
-        self.set_var("front_continue_fail_block", 180)
+        self.set_var("front_continue_fail_block", 0)
-        self.set_var("dispather_min_idle_workers", 3)
+        self.set_var("dispather_min_idle_workers", 10)
-        self.set_var("dispather_max_workers", 60)
+        self.set_var("dispather_work_max_score", 200000)
-        # self.set_var("http2_max_concurrent", 50)
+        self.set_var("http2_max_concurrent", 20)
-        self.set_var("max_scan_ip_thread_num", 10)
+        self.set_var("max_scan_ip_thread_num", 1)
-                wfile.write(data)
+                wfile._sock.sendall(data)
-            ret = wfile.write(data)
+            ret = wfile._sock.sendall(data)
-                ret = wfile.write(data)
+                #ret = wfile.write(data)
-                ret = self.wfile.write(data)
+                ret = self.wfile._sock.sendall(data)
-                    ret = self.wfile.write(data)
+                    ret = self.wfile._sock.sendall(data)
-        slice = self.fd.read(max_slice_len)
+        slice = self.fd.read(max_slice_len * 2)
-                front.ip_manager.adjust_scan_thread_num(thread_num_for_scan_ip)
+            front.ip_manager.adjust_scan_thread_num()
-            self.logger.exception("test_xtunnel_ip %s e:%r", ip, e)
+            self.logger.exception("check_ip:%s create_ssl except:%r", ip, e)
-        if self.config.check_ip_host:
+        if host:
-        content = response.read(timeout=1)
+        content = response.read()
-        self.set_var("max_good_ip_num", 3000)
+        self.set_var("max_good_ip_num", 100)
-        ssl_sock = openssl_wrap.SSLConnection(self.openssl_context, sock, ip, on_close=close_cb)
+        ssl_sock = openssl_wrap.SSLConnection(self.openssl_context.context, sock, ip, on_close=close_cb)
-
+        self.get_ssl_cert_domain(ssl_sock)
-                if ssl_sock.sni not in alt_names:
+                alt_names = tuple(alt_names)
-        super(Http1Worker, self).__init__(logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb)
+    def __init__(self, logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-        super(Http2Worker, self).__init__(logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb)
+    def __init__(self, logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-        frame, length = Frame.parse_frame_header(header.tobytes())
+        frame, length = Frame.parse_frame_header(header)
-        self._consume_frame_payload(frame, data.tobytes())
+        self._consume_frame_payload(frame, data)
-            header_data = b''.join(self.response_header_datas)
+            if len(self.response_header_datas) == 1:
-            self._handle_header_block(headers)
+            self.response_headers = HTTPHeaderMap(headers)
-                time.time() - self.connection.last_active_time > 60:
+        if self.connection.continue_timeout >= self.connection.config.http2_max_timeout_tasks and \
-        self.read_buffer = ""
+        self.read_buffers = []
-            return ""
+            return memoryview(b'')
-            while len(self.read_buffer) < size:
+            while self.read_buffer_len < size:
-                self.read_buffer += data
+                    return memoryview(b'')
-                self.read_buffer = ""
+            if self.read_buffers:
-                    return ""
+                    return memoryview(b'')
-            out_list.append(data)
+        if self.content_length:
-        return "".join(out_list)
+            return buff_view[:p]
-    def __init__(self, logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb):
+    def __init__(self, logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-import os
+        # for statistic
-            worker = self.http2worker(self.logger, self.ip_manager, self.config, ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = self.http2worker(
-            worker = self.http1worker(self.logger, self.ip_manager, self.config, ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = self.http1worker(
-        # self.working_tasks[task.unique_id] = task
+
-        # del self.working_tasks[task.unique_id]
+        self.fail_num += 1
-                           (len(w.streams), w.ping_on_way, w.remote_window_size, w.send_queue.qsize())
+                out_str += " continue_timeout:%d streams:%d ping_on_way:%d remote_win:%d send_queue:%d\r\n" % \
-        self.scan_ip_thread_num = self.max_scan_ip_thread_num
+        self.scan_ip_thread_num = self.config.max_scan_ip_thread_num
-
+    def adjust_scan_thread_num(self):
-            scan_ip_thread_num = self.max_scan_ip_thread_num
+            scan_ip_thread_num = self.config.max_scan_ip_thread_num
-                scan_ip_thread_num = int( (the_100th_handshake_time - 200)/2 * self.max_scan_ip_thread_num/50 )
+                last_ip = self.ip_list[-1]
-                scan_ip_thread_num = self.max_scan_ip_thread_num
+            if scan_ip_thread_num > self.config.max_scan_ip_thread_num:
-            return True
+            if 'gws' not in server:
-        return False
+
-            if not self.check_local_network.is_ok(ip):
+            if not self.check_local_network.is_ok():
-                    self.logger.info("scan_ip add ip:%s time:%d", ip, result.request_time)
+                    self.logger.info("scan_ip add ip:%s time:%d h2:%d", ip, result.request_time, result.h2)
-        self.adjust_scan_thread_num()
+        # stop all scan ip threads
-        self.load_ipv6()
+        self.fd = open(list_fn, "r")
-                    continue
+    def get_slice(self):
-        return random.choice(self.ipv6_list)
+        line = self.get_slice()
-class SSLContext(OpenSSL.SSL.Context):
+class SSLContext(object):
-        self._ssl_context = OpenSSL.SSL.Context(protocol_version)
+        self.context = OpenSSL.SSL.Context(protocol_version)
-            self._ssl_context.set_verify(OpenSSL.SSL.VERIFY_PEER, lambda c, x, e, d, ok: ok)
+            self.context.load_verify_locations(os.path.abspath(ca_certs))
-            self._ssl_context.set_verify(OpenSSL.SSL.VERIFY_NONE, lambda c, x,    e, d, ok: ok)
+            self.context.set_verify(OpenSSL.SSL.VERIFY_NONE, lambda c, x, e, d, ok: ok)
-            self.set_cipher_list(':'.join(cipher_suites))
+            self.context.set_cipher_list(':'.join(cipher_suites))
-                self._ssl_context.set_alpn_protos([b'h2', b'http/1.1'])
+                self.context.set_alpn_protos([b'h2', b'http/1.1'])
-                self._ssl_context.set_npn_select_callback(SSLContext.npn_select_callback)
+                self.context.set_npn_select_callback(SSLContext.npn_select_callback)
-            self._ssl_context.set_verify(OpenSSL.SSL.VERIFY_PEER, lambda c, x, e, d, ok: ok)
+            self.context.load_verify_locations(fn)
-            self.data.append(frame.data)
+            self.data.append(frame.data.tobytes())
-            headers = self._decoder.decode(b''.join(self.header_data))
+            if len(self.header_data) == 1:
-        data_mem = memoryview(data)
+        if not isinstance(data, memoryview):
-        self.data = data[padding_data_length:len(data)-self.total_padding].tobytes()
+        self.data = data[padding_data_length:len(data)-self.total_padding]
-        self.data = data[priority_data_length:len(data)-self.total_padding].tobytes()
+        self.data = data[priority_data_length:len(data)-self.total_padding]
-        self.data = data.tobytes()
+        self.data = data
-        self.read_buffer = buffer
+        if isinstance(buffer, memoryview):
-        self.body = self.read_buffer[self.buffer_start:]
+        self.body = self.view[self.buffer_start:]
-            return self._read_plain(read_len, timeout)
+            data = self._read_plain(read_len, timeout)
-            return self._read_chunked(timeout)
+            data = self._read_chunked(timeout)
-    xlog.info("Proxy server listen:%s:%d.", g.config.proxy_bind_ip, g.config.proxy_port)
+    xlog.info("Proxy server listen:%s:%d.", listen_ip, g.config.proxy_port)
-                                   ttl=g.config.dns_ttl)
+    g.dns_srv = dns_server.DnsServer(
-        return rs["ip"]
+        ips = rs["ip"]
-                continue
+                try:
-                continue
+                if not response:
-                continue
+                try:
-            id = p.header.id
+                if len(p.questions) == 0:
-                continue
+                id = p.header.id
-                    self.sock.sendto(req_pack, (server, 53))
+                if id not in self.waiters:
-            que.notify_all()
+                que = self.waiters[id]
-            xlog.warn("request dns except:%r", e)
+            xlog.warn("send_request except:%r", e)
-    def query(self, domain, timeout=3):
+    def query(self, domain, timeout=3, use_local=False):
-        id = random.randint(0, 65535)
+        while True:
-        if "." not in domain:
+        if use_local:
-    def __init__(self, bind_ip="127.0.0.1", port=53, ttl=24*3600):
+    def __init__(self, bind_ip="127.0.0.1", port=53, backup_port=5353, ttl=24*3600):
-            self.serverSock.bind((bind_ip, self.port))
+            self.serverSock.bind((self.bind_ip, self.port))
-            xlog.warn("bind DNS %s:%d fail", bind_ip, port)
+            xlog.warn("bind DNS %s:%d fail", self.bind_ip, self.backup_port)
-            if "x86" in value or "i686" in value or "amd64" in value:
+            if sys.platform.startswith("linux"):
-            ips = g.dns_client.query(domain, timeout=1)
+        if g.gfwlist.check(domain) or rule in ["gae", "socks"]:
-                g.domain_cache.set_ips(domain, ips, type)
+            return ips
-            else:
+        if "." not in domain:
-                xlog.warn("query:%s type:%d", domain, type)
+                xlog.info("direct_query:%s type:%d", domain, type)
-                self.cache[domain] = record
+
-                pass
+                record = None
-        if len(record["ip"]) == 0 or time_now - record["update"] > self.ttl:
+        if len(record["ip"]) == 0:
-        if len(record["ip"]) == 0 or time_now - record["update"] > self.ttl:
+        if len(record["ip"]) == 0:
-            record["r"] = rule
+        record["r"] = rule
-            record["g"] = 0
+        record["g"] = 0
-            return record["g"]
+        return record["g"]
-    elif record:
+    #if check_local_network.IPv6.is_ok() and have_ipv6(ips) and port == 443:
-        if rule == "gae" or not g.ip_region.check_ips(record["ip"]):
+        if rule == "gae":
-            api.set_proxy(args)
+            front.set_proxy(args)
-            score = front.front.get_score()
+            score = front.get_dispatcher().get_score()
-                self.xlog.debug("Conn session:%s conn:%d Peer Close:%s", self.session.session_id, self.conn_id, data.get())
+                dat = data.get()
-    xlog.info("xxnet_version:%s", xxnet_version())
+    xlog.info("xxnet_version:%s", g.xxnet_version)
-    front.set_proxy(args)
+import utils
-        ip = "104.28.77.8"
+        ip = default_ip
-    print("test ip:%s" % ip)
+    xlog.info("test ip:%s", ip)
-        top_domain = None
+        host = sys.argv[2]
-    connect_creator = ConnectCreator(logger, config, openssl_context)
+    host_manager = HostManagerBase()
-    res = check_ip.check_ip(ip, top_domain=top_domain, wait_time=wait_time)
+    res = check_ip.check_ip(ip, host=host, wait_time=wait_time)
-        print("connect fail")
+        xlog.warn("connect fail")
-        print("success, domain:%s handshake:%d" % (res.top_domain, res.handshake_time))
+        xlog.info("success, domain:%s handshake:%d", res.host, res.handshake_time)
-        print("not support")
+        xlog.warn("not support")
-        self.set_var("dispather_max_workers", 5)
+        self.set_var("dispather_max_workers", 2)
-        self.set_var("check_ip_host", "xxnet4.herokuapp.com")
+        self.set_var("target_handshake_time", 550)
-            time.sleep(1)
+    def get_dispatcher(self, host=None):
-        return worker.get_score() * 10
+        return dispatcher
-        dispatcher = self.dispatchs[host]
+        dispatcher = self.get_dispatcher(host)
-            fn = self.default_fn
+        for fn in [self.fn, self.default_fn]:
-            self.logger.warn("load %s for host fail.", fn)
+            lns = []
-                    raise Exception("status:%r", response.status)
+                if not response or response.status != 200:
-                        if response.text == old_content:
+                        if content == old_content:
-                        fd.write(response.text)
+                        fd.write(content)
-                self.logger.debug("updated cloudflare front domains from github fail:%r", e)
+                self.logger.exception("updated cloudflare front domains from github fail:%r", e)
-        super(CloudflareHttp2Worker, self).__init__(logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb)
+    def __init__(self, logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-    content, status, response = front.request("GET", "center.xx-net.net", "/", timeout=10)
+    #content, status, response = front.request("GET", "scan1.xx-net.net", "/", timeout=10)
-    xlog.info("status:%d content:%s", status, content)
+    xlog.info("status:%d content:%s", status, content.tobytes())
-        data += "time:%d  pointer:%d<br>\r\n" % (time_now, front.ip_manager.gws_ip_pointer)
+        data += "time:%d  pointer:%d<br>\r\n" % (time_now, front.ip_manager.ip_pointer)
-        for ip in front.ip_manager.gws_ip_list:
+        for ip in front.ip_manager.ip_list:
-class FakeWorker():
+class FakeWorker(object):
-    return 1
+class FakeDispatcher(object):
-    return 1
+def get_dispatcher(host=None):
-            score = front.get_score(host)
+            dispatcher = front.get_dispatcher(host)
-    global last_success_time, last_fail_time, continue_fail_num, gae_proxy
+    global gae_proxy
-        return None
+def get_dispatcher(host=None):
-    return len(gae_proxy.front.front.http_dispatcher.workers)
+    return gae_proxy.front.front.http_dispatcher
-    success_num += 1
+def set_proxy(args):
-
+import threading
-        ip = "207.246.89.177"
+        ip = "54.225.129.54"
-        print("not support")
+    check_one(ip, top_domain, wait_time)
-        self.set_var("check_ip_content", "OK")
+        self.set_var("check_ip_content", "We are building new site.")
-        return len(self.http_dispatcher.workers)
+    def get_dispatcher(self, host=None):
-                xlog.warn("front request %s %s%s fail, status:%d", method, host, path, status)
+                self.logger.warn("front request %s %s%s fail, status:%d", method, host, path, status)
-            # xlog.debug("%s %s%s trace:%s", method, response.ssl_sock.host, path, response.task.get_trace())
+            # self.logger.debug("%s %s%s trace:%s", method, response.ssl_sock.host, path, response.task.get_trace())
-            xlog.exception("front request %s %s%s fail:%r", method, host, path, e)
+            self.logger.exception("front request %s %s%s fail:%s", method, host, path, e)
-        # xlog.debug("status:%d", status)
+        # self.logger.info('%s "PHP %s %s %s" %s %s', handler.address_string(), handler.command, url, handler.protocol_version, response.status, response.getheader('Content-Length', '-'))
-            self.success_num += 1
+            self.logger.debug("%s %s%s trace:%s", method, host, path, response.task.get_trace())
-                xlog.warn("heroku:%s fail", heroku_host)
+                self.logger.warn("heroku:%s fail", heroku_host)
-        except:
+        except Exception as e:
-    xlog.info("status:%d content:%s", status, content)
+    xlog.info("status:%d content:%s", status, content.tobytes())
-import simple_queue
+        if isinstance(data, memoryview):
-        if not g.config.login_account:
+        if not g.config.login_account or not self.running:
-        stat = self.get_stat()
+        stat = self.get_stat("minute")
-    def get_stat(self):
+    def get_stat(self, type="second"):
-        rtts = []
+        rtt = 0
-            score = front.get_score()
+            dispatcher = front.get_dispatcher()
-            total_received += front.total_received
+
-                "total_traffics": "Up: %s / Down: %s" % (convert(front.total_sent), convert(front.total_received))
+                "rtt": stat["rtt"],
-            "rtt": int(max(rtts)) or 9999,
+            "rtt": int(rtt),
-            "speed": "Up: %s/s / Down: %s/s" % (convert(recent_sent / 5.0), convert(recent_received / 5.0)),
+            "speed": "Up: %s/s / Down: %s/s" % (convert(recent_sent), convert(recent_received)),
-        threading.Thread(target=self.debug_data_clearup_thread).start()
+    def get_dispatcher(self, host=None):
-from config import config
+
-    check_ip.load_proxy_config()
+    front.set_proxy(args)
-    if http_dispatch.is_idle():
+    if front.http_dispatcher.is_idle():
-          http_dispatch.h2_num
+    num = len(front.connect_manager.new_conn_pool.pool) +\
-    user_config.save()
+    front.config.listen_ip = args["ip"]
-
+import datetime
-        config.cert_import_ready = True
+        # config.cert_import_ready = True
-import time
+import OpenSSL
-import binascii
+root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir))
-    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
+    sys.path.append(root_path)
-    xlog = getLogger("gae_proxy")
+    elif sys.platform == "darwin":
-            pass
+import xlog
-import check_local_network
+from front_base.openssl_wrap import SSLContext
-        return False
+class CheckIp(front_base.check_ip.CheckIp):
-            xlog.warn("503 but server type:%s", server_type)
+        if response.status not in self.config.check_ip_accept_status:
-                return ssl_sock
+        if response.status == 503:
-        return ssl_sock
+                return True
-        return ssl_sock
+        content = response.read()
-    return ssl_sock
+        return True
-        xlog.info("check_ip <ip>")
+        ip = "46.134.208.94"
-    from config import config
+#try:
-xlog = getLogger("gae_proxy")
+from front_base.config import ConfigBase
-    current_path = os.path.dirname(os.path.abspath(__file__))
+        self.br_sites = tuple(self.BR_SITES)
-config.load()
+config_path = os.path.join(module_data_path, "config.json")
-from config import config
+from front import direct_front
-def handler(method, host, url, headers, body, wfile):
+def handler(method, host, path, headers, body, wfile, timeout=30):
-        if time.time() - time_request > 30:
+        time_left = time_request + timeout - time.time()
-            response = fetch(method, host, url, headers, body)
+            response = direct_front.request(method, host, path, headers, body, timeout=time_left)
-                    server_type = response.getheader('Server', "")
+                    server_type = response.headers.get('Server', "")
-                        google_ip.report_connect_fail(response.ssl_sock.ip)
+                        xlog.warn("IP:%s host:%s not support GAE, server type:%s status:%d",
-            xlog.warn("direct_handler.handler err:%r %s/%s", e, host, url)
+            xlog.warn("direct_handler.handler err:%r %s/%s", e, host, path)
-            xlog.exception('direct_handler.handler %r %s %s , retry...', e, host, url)
+            xlog.exception('direct_handler.handler %r %s %s , retry...', e, host, path)
-        time_last_read = time.time()
+        wfile.write("HTTP/1.1 %d %s\r\n" % (response.status, response.reason))
-            xlog.exception("direct_handler except:%r %s %s", e, host, url)
+            data = response.task.read()
-        xlog.exception("direct_handler except:%r %s %s", e, host, url)
+        xlog.info("DIRECT %d %s %s, t:%d send to client except:%r",
-
+from front import front
-from http_common import *
+class GAE_Exception(Exception):
-        kwargs['password'] = config.GAE_PASSWORD
+    if front.config.GAE_PASSWORD:
-    kwargs['maxsize'] = config.AUTORANGE_MAXSIZE
+    kwargs['validate'] = front.config.GAE_VALIDATE
-        google_ip.recheck_ip(response.ssl_sock.ip)
+        front.ip_manager.recheck_ip(response.ssl_sock.ip)
-def request_gae_proxy(method, url, headers, body, timeout=60, retry=True):
+def request_gae_server(headers, body, url, timeout):
-            if host in config.br_sites or host.endswith(config.br_endswith):
+            if host in front.config.br_sites or host.endswith(front.config.br_endswith):
-                        config.br_sites = tuple(br_sites)
+                    if host not in front.config.br_sites:
-                    appid_manager.report_out_of_quota(response.ssl_sock.appid)
+                    appid = response.ssl_sock.host.split(".")[0]
-                        "appid out of quota:%s" % response.ssl_sock.appid)
+                        "appid out of quota:%s" % appid)
-                req_range_begin, req_range_begin + config.AUTORANGE_MAXSIZE - 1)
+                req_range_begin, req_range_begin + front.config.AUTORANGE_MAXSIZE - 1)
-            if req_range_end - req_range_begin + 1 > config.AUTORANGE_MAXSIZE:
+            if req_range_end - req_range_begin + 1 > front.config.AUTORANGE_MAXSIZE:
-                    req_range_begin, req_range_begin + config.AUTORANGE_MAXSIZE - 1)
+                    req_range_begin, req_range_begin + front.config.AUTORANGE_MAXSIZE - 1)
-                          config.AUTORANGE_THREADS * 1.3)
+    max_buffer_size = int(front.config.AUTORANGE_MAXSIZE *
-        thread_num = min(config.AUTORANGE_THREADS, fetch_times)
+            (data_left_to_fetch + front.config.AUTORANGE_MAXSIZE - 1) / front.config.AUTORANGE_MAXSIZE)
-                end = min(begin + config.AUTORANGE_MAXSIZE - 1, self.req_end)
+                end = min(begin + front.config.AUTORANGE_MAXSIZE - 1, self.req_end)
-    print (ip)
+import time
-import struct
+from front_base.http2_connection import Http2Worker
-from http_common import *
+class GaeHttp2Worker(Http2Worker):
-        return ",".join(out_list)
+    def get_host(self, task_host):
-def create_data_path():
+def check_create_data_path():
-import connect_manager
+from front import front
-    xlog.info('GAE APPID          : %s', '|'.join(config.GAE_APPIDS))
+    xlog.info('Listen Address     : %s:%d', front.config.listen_ip, front.config.listen_port)
-    #xlog.basicConfig(level=xlog.DEBUG if config.LISTEN_DEBUGINFO else xlog.INFO, format='%(levelname)s - %(asctime)s %(message)s', datefmt='[%b %d %H:%M:%S]')
+    global ready, proxy_server
-        listen_ip = config.LISTEN_IP
+        listen_ip = front.config.listen_ip
-    proxy_thread.start()
+    proxy_server = simple_http_server.HTTPServer(
-        pr.print_stats()
+    
-from connect_control import touch_active
+from front import front
-            return False
+        return False
-            xlog.info("Browse localhost by proxy")
+            xlog.debug("Browse localhost by proxy")
-        if host in config.HOSTS_GAE:
+        if host in front.config.HOSTS_GAE:
-        if host in config.HOSTS_FWD or host in config.HOSTS_DIRECT:
+        if host in front.config.HOSTS_DIRECT:
-        if host.endswith(config.HOSTS_GAE_ENDSWITH):
+        if host.endswith(front.config.HOSTS_GAE_ENDSWITH):
-        if host.endswith(config.HOSTS_FWD_ENDSWITH) or host.endswith(config.HOSTS_DIRECT_ENDSWITH):
+        if host.endswith(front.config.HOSTS_DIRECT_ENDSWITH):
-        if host in config.HOSTS_GAE:
+        if host in front.config.HOSTS_GAE:
-        if host in config.HOSTS_DIRECT:
+        if host in front.config.HOSTS_DIRECT:
-        if host.endswith(config.HOSTS_GAE_ENDSWITH):
+        if host.endswith(front.config.HOSTS_GAE_ENDSWITH):
-        if host.endswith(config.HOSTS_DIRECT_ENDSWITH):
+        if host.endswith(front.config.HOSTS_DIRECT_ENDSWITH):
-    print(n)
+import random
-
+import OpenSSL
-
+from front import front, direct_front
-                pass
+def test_appid_exist(ssl_sock, appid):
-                self.user_special.scan_ip_thread_num = self.DEFAULT_CONFIG.getint('google_ip', 'max_scan_ip_thread_num')
+    response.begin()
-                pass
+    if response.status == 503:
-                pass
+    if response.status != 200:
-            self.user_special.proxy_passwd = self.USER_CONFIG.get('proxy', 'passwd')
+    content = response.read()
-                pass
+    return True
-            xlog.warn("User_config.load except:%s", e)
+def test_appid(appid):
-            xlog.info("save config to %s", CONFIG_USER_FILENAME)
+            return test_appid_exist(ssl_sock, appid)
-            xlog.exception("launcher.config save user config fail:%s %r", CONFIG_USER_FILENAME, e)
+            xlog.warn("check_appid %s %r", appid, e)
-#    return user_config.user_special.fake_host
+def test_appids(appids):
-                           openssl_wrap.support_alpn_npn)
+    return "%s %s h2:%s" % (OpenSSL.version.__version__,
-            connect_control.keep_running = False
+            front.stop()
-            "use_ipv6": config.USE_IPV6,
+            "proxy_listen": config.listen_ip + ":" + str(config.listen_port),
-            "not_exist_appids": "|".join(appid_manager.not_exist_appids),
+            "working_appid": "|".join(front.appid_manager.working_appid_list),
-            "low_prior_lock": len(connect_control.low_prior_lock),
+            "ip_num": len(front.ip_manager.ip_list),
-                data = json.dumps(user_config.user_special, default=lambda o: o.__dict__)
+                ret_config = {
-                        fail_appid_list = test_appid.test_appids(appids)
+                if appids != "|".join(config.GAE_APPIDS):
-                    user_config.user_special.appid = appids
+                    if appids:
-                user_config.save()
+                config.save()
-                connect_manager.https_manager.load_config()
+                front.appid_manager.reset_appid()
-                    http_dispatch.close_all_worker()
+                    front.http_dispatcher.close_all_worker()
-                google_ip.reset()
+                front.ip_manager.reset()
-                    if not ip_utils.check_ip_valid(ip):
+                    if not utils.check_ip_valid(ip):
-                    if google_ip.add_ip(ip, 100, "google.com", "gws"):
+                    if front.ip_manager.add_ip(ip, 100, "google.com", "gws"):
-            google_ip.save_ip_list(force=True)
+            front.ip_manager.save(force=True)
-                if google_ip.ip_dict[ip]['fail_times'] > 0:
+            for ip in front.ip_manager.ip_list:
-        if not result or not result.support_gae:
+        result = front.check_ip.check_ip(ip)
-        data += "time:%d  pointer:%d<br>\r\n" % (time_now, google_ip.gws_ip_pointer)
+        data += "time:%d  pointer:%d<br>\r\n" % (time_now, front.ip_manager.ip_pointer)
-            handshake_time = google_ip.ip_dict[ip]["handshake_time"]
+        for ip in front.ip_manager.gws_ip_list:
-            links = google_ip.ip_dict[ip]["links"]
+            fail_times = front.ip_manager.ip_dict[ip]["fail_times"]
-            get_time = google_ip.ip_dict[ip]["get_time"]
+            get_time = front.ip_manager.ip_dict[ip]["get_time"]
-            success_time = google_ip.ip_dict[ip]["success_time"]
+            success_time = front.ip_manager.ip_dict[ip]["success_time"]
-            fail_time = google_ip.ip_dict[ip]["fail_time"]
+            fail_time = front.ip_manager.ip_dict[ip]["fail_time"]
-            down_fail_time = google_ip.ip_dict[ip]["down_fail_time"]
+            down_fail_time = front.ip_manager.ip_dict[ip]["down_fail_time"]
-            data_active = google_ip.ip_dict[ip]["data_active"]
+            data_active = front.ip_manager.ip_dict[ip]["data_active"]
-            history = google_ip.ip_dict[ip]["history"]
+            history = front.ip_manager.ip_dict[ip]["history"]
-
+            data = front.ipv4_source.load_range_content()
-            old_digest = hashlib.md5(ip_range.load_range_content()).hexdigest()
+            default_digest = hashlib.md5(front.ipv4_source.load_range_content(default=True)).hexdigest()
-                ip_range.remove_user_range()
+                front.ipv4_source.remove_user_range()
-                    ip_range.update_range_content(content)
+                    front.ipv4_source.update_range_content(content)
-                ip_range.load_ip_range()
+                front.ipv4_source.load_ip_range()
-            if user_config.user_special.use_ipv6 != use_ipv6:
+            if config.use_ipv6 != use_ipv6:
-                user_config.user_special.use_ipv6 = use_ipv6
+                config.use_ipv6 = use_ipv6
-            user_config.save()
+            config.auto_adjust_scan_ip_thread_num = should_auto_adjust_scan_ip
-                google_ip.adjust_scan_thread_num(thread_num_for_scan_ip)
+            if front.ip_manager.max_scan_ip_thread_num != thread_num_for_scan_ip:
-        data += https_manager.new_conn_pool.to_string()
+        data += front.connect_manager.new_conn_pool.to_string()
-        data += https_manager.gae_conn_pool.to_string()
+        data += front.connect_manager.gae_conn_pool.to_string()
-        for host in https_manager.host_conn_pool:
+        for host in front.connect_manager.host_conn_pool:
-            data += https_manager.host_conn_pool[host].to_string()
+            data += front.connect_manager.host_conn_pool[host].to_string()
-        data = http_dispatch.to_string()
+        data = front.http_dispatcher.to_string()
-            good_num = (google_ip.good_ipv4_num + google_ip.good_ipv6_num)
+            all_ip_num = len(front.ip_manager.ip_dict)
-            left_num = google_ip.scan_exist_ip_queue.qsize()
+            left_num = front.ip_manager.scan_exist_ip_queue.qsize()
-                google_ip.start_scan_all_exist_ip()
+                front.ip_manager.start_scan_all_exist_ip()
-            left_num = google_ip.scan_exist_ip_queue.qsize()
+            left_num = front.ip_manager.scan_exist_ip_queue.qsize()
-                google_ip.stop_scan_all_exist_ip()
+                front.ip_manager.stop_scan_all_exist_ip()
-        for obj in [https_manager, http_dispatch]:
+        for obj in [front.connect_manager, front.http_dispatcher]:
-            request_data = 'GET / HTTP/1.1\r\nHost: %s\r\n\r\n' % host
+            request_data = 'GET %s HTTP/1.1\r\nHost: %s\r\n\r\n' % (self.config.check_ip_path, host)
-                return ssl_sock
+            return response
-        return ssl_sock
+            return False
-        start_time = time.time()
+        self.logger.debug("ip:%s use http/2", ssl_sock.ip)
-        try:
+            conn.request('GET', self.config.check_ip_path)
-        return ssl_sock
+            return False
-            ssl_sock = self.connect_creator.connect_ssl(ip, host=host)
+            ssl_sock = self.connect_creator.connect_ssl(ip, sni=host)
-            return self.check_http1(ssl_sock, host)
+            response = self.check_http1(ssl_sock, host)
-            return self.check_http2(ssl_sock, host)
+            response = self.check_http2(ssl_sock, host)
-        self.set_var("ipv6_scan_ratio", 0.5)
+        self.set_var("use_ipv6", "auto") #force_ipv4/force_ipv6
-import openssl_wrap
+import openssl_wrap
-    def __init__(self, logger, config, openssl_context, host_manager, timeout=5, debug=False, check_cert=None):
+    def __init__(self, logger, config, openssl_context, host_manager,
-        self.check_cert = check_cert
+        if check_cert:
-        if self.config.connect_force_http2:
+        if self.connect_force_http1:
-        #    raise socket.error(' certficate is issued by %r, not COMODO' % (issuer_commonname))
+        ssl_sock.sni = sni
-        ssl_sock.sni = sni
+
-            task.headers['Host'] = self.ssl_sock.host
+        task.headers['Host'] = self.get_host(task.host)
-            self.logger.exception("%s h1_request:%r inactive_time:%d task.timeout:%d",
+            self.logger.warn("%s h1_request:%r inactive_time:%d task.timeout:%d",
-            return
+        if task.method == "HEAD" or response.status in [204, 304]:
-        task.put_data(data)
+        response.task = task
-        self.ssl_sock.received_size += length
+        self.ssl_sock.received_size += data_len
-            speed = length / time_cost
+            speed = data_len / time_cost
-        self.transfered_size += len(request_data) + length
+        self.transfered_size += len(request_data) + data_len
-            self.logger.warn("try head but no host set")
+            # self.logger.warn("try head but no host set")
-        self.logger.debug("head request %s", self.ip)
+        # self.logger.debug("head request %s", self.ip)
-from hyper.packages.hpack.hpack_compat import Encoder, Decoder
+from hyper.packages.hpack import Encoder, Decoder
-            return self.encoder.encode(headers)
+        return self.encoder.encode(headers)
-        self.streams[stream_id] = stream
+            stream = Stream(self.logger, self.config, self, self.ip, stream_id, task,
-                if frame.type != WindowUpdateFrame.type:
+                if frame.type not in [WindowUpdateFrame.type]:
-            self.logger.debug("WindowUpdateFrame %d", frame.window_increment)
+            # self.logger.debug("WindowUpdateFrame %d", frame.window_increment)
-        self.task.set_state("end send left body")
+        threading.Thread(target=self.left_work).start()
-            data = self.body_queue.get(self.timeout)
+            data = self.read()
-        return score
+        return score
-        self.triger_create_worker_cv = SimpleCondition()
+        self.trigger_create_worker_cv = SimpleCondition()
-            self.triger_create_worker_cv.wait()
+            self.trigger_create_worker_cv.wait()
-                    best_score > self.config.dispather_work_max_score:
+            if len(self.workers) < self.config.dispather_max_workers and \
-                self.triger_create_worker_cv.notify()
+                self.trigger_create_worker_cv.notify()
-        self.triger_create_worker_cv.notify()
+        self.trigger_create_worker_cv.notify()
-        elif os.path.isfile(self.default_ip_list_fn):
+        elif self.default_ip_list_fn and os.path.isfile(self.default_ip_list_fn):
-            self.logger.error("save good_ip.txt fail %s", e)
+            self.logger.error("save %s fail %s", self.ip_list_fn, e)
-        if not self.auto_adjust_scan_ip_thread_num:
+        if not self.config.auto_adjust_scan_ip_thread_num:
-                self.add_ip(ip, result.request_time, result.top_domain)
+                self.add_ip(ip, result.request_time, result.domain)
-        if not self.check_local_network.is_ok():
+        if not self.check_local_network.is_ok(ip):
-            self.add_ip(ip, result.request_time, result.top_domain)
+            self.add_ip(ip, result.request_time, result.domain)
-                if self.add_ip(ip, result.request_time, result.top_domain):
+                if self.add_ip(ip, result.request_time, result.domain):
-                    #     result.top_domain, result.server_type, result.handshake_time, len(self.ip_list))
+                    #     result.domain, result.server_type, result.handshake_time, len(self.ip_list))
-                        self.scan_ip_log.info("Add %s time:%d CN:%s ", ip, result.request_time, result.top_domain)
+                        self.scan_ip_log.info("Add %s time:%d CN:%s ", ip, result.request_time, result.domain)
-                self.add_ip(ip, result.request_time, result.top_domain)
+                self.add_ip(ip, result.request_time, result.domain)
-    def get_ipv6(self):
+    def get_ip(self):
-
+
-    def recv_into(self, buf):
+    def recv_into(self, buf, nbytes=None):
-            ret = self._connection.recv_into(buf)
+            ret = self._connection.recv_into(buf, nbytes)
-                ret = self.__iowait(self._connection.recv_into, buf)
+                ret = self.__iowait(self._connection.recv_into, buf, nbytes)
-__version__ = '1.0.1'
+from .hpack import Encoder, Decoder
-    unicode = unicode
+    def to_bytes(b):
-class HTTP20Error(Exception):
+
-    The base class for all of ``hyper``'s HTTP/2-related exceptions.
+    The base class for all ``hpack`` exceptions.
-class HPACKEncodingError(HTTP20Error):
+class HPACKDecodingError(HPACKError):
-    An error has been encountered while performing HPACK encoding.
+    An error has been encountered while performing HPACK decoding.
-class HPACKDecodingError(HTTP20Error):
+class InvalidTableIndex(HPACKDecodingError):
-    An error has been encountered while performing HPACK decoding.
+    An invalid table index was received.
-class ConnectionError(HTTP20Error):
+class OversizedHeaderListError(HPACKDecodingError):
-    connection, and the connection has been closed.
+    A header list that was larger than we allow has been received. This may be
-class ProtocolError(HTTP20Error):
+class InvalidTableSizeError(HPACKDecodingError):
-    The remote party violated the HTTP/2 protocol.
+    An attempt was made to change the decoder table size to a value larger than
-import collections
+import logging
-from .huffman import HuffmanDecoder, HuffmanEncoder
+from .table import HeaderTable, table_entry_size
-    # log.debug("Encoding %d with %d bits", integer, prefix_bits)
+    log.debug("Encoding %d with %d bits", integer, prefix_bits)
-    max_number = (2 ** prefix_bits) - 1
+    max_number = _PREFIX_BIT_MAX_NUMBERS[prefix_bits]
-    if (integer < max_number):
+    if integer < max_number:
-        integer = integer - max_number
+        integer -= max_number
-            integer = integer // 128  # We need integer division
+            elements.append((integer & 127) + 128)
-    index = 0
+    if prefix_bits < 1 or prefix_bits > 8:
-    number = to_byte(data[index]) & mask
+    max_number = _PREFIX_BIT_MAX_NUMBERS[prefix_bits]
-    if (number == max_number):
+    log.debug("Decoded %d, consumed %d bytes", number, index)
-            next_byte = to_byte(data[index])
+    return number, index
-    return (number, index + 1)
+def _dict_to_iterable(header_dict):
-    if not isinstance(string, (str, bytes)):  # pragma: no cover
+    if not isinstance(string, basestring):  # pragma: no cover
-        self._header_table_size = 4096  # This value set by the standard.
+        self.header_table = HeaderTable()
-        self._table_size_changed = False
+        self.table_size_changes = []
-        return self._header_table_size
+        """
-        self._header_table_size = value
+        self.header_table.maxsize = value
-        Otherwise, a literal representation will be used.
+        :param headers: The headers to encode. Must be either an iterable of
-        # log.debug("HPACK encoding %s", headers)
+        # Transforming the headers into a header block is a procedure that can
-        # natural way to interact with them in HPACK.
+        # natural way to interact with them in HPACK. Because dictionaries are
-        headers = [(_to_bytes(n), _to_bytes(v)) for n, v in headers]
+            headers = _dict_to_iterable(headers)
-        if self._table_size_changed:
+        # to signal all changes since last emission appropriately.
-            self._table_size_changed = False
+            self.header_table.resized = False
-        )
+        # Add each header to the header block
-        # log.debug("Encoded header block to %s", header_block)
+        log.debug("Encoded header block to %s", header_block)
-    def add(self, to_add, huffman=False):
+    def add(self, to_add, sensitive, huffman=False):
-        # log.debug("Adding %s to the header table", to_add)
+        log.debug("Adding %s to the header table", to_add)
-        match = self.matching_header(name, value)
+        match = self.header_table.search(name, value)
-            self._add_to_header_table(to_add)
+            encoded = self._encode_literal(name, value, indexbit, huffman)
-        index, perfect = match
+        index, name, perfect = match
-                32 + len(n) + len(v)
+            encoded = self._encode_indexed_literal(
-            # log.debug("Evicted %s: %s from the header table", n, v)
+        return encoded
-        field[0] = field[0] | 0x80  # we set the top bit
+        field[0] |= 0x80  # we set the top bit
-    def _encode_literal(self, name, value, indexing, huffman=False):
+    def _encode_literal(self, name, value, indexbit, huffman=False):
-        return b''.join([prefix, bytes(name_len), name, bytes(value_len), value])
+        return b''.join(
-    def _encode_indexed_literal(self, index, value, huffman=False):
+    def _encode_indexed_literal(self, index, value, indexbit, huffman=False):
-        prefix[0] |= 0x40
+        if indexbit != INDEX_INCREMENTAL:
-        Produces the encoded form of a header table size change context update.
+        Produces the encoded form of all header table size change context
-        return bytes(size_bytes)
+        block = b''
-        )
+    .. versionchanged:: 2.3.0
-        return self._header_table_size
+        """
-                # log.debug("Evicting %s: %s from the header table", n, v)
+        self.header_table.maxsize = value
-    def decode(self, data):
+    def decode(self, data, raw=False):
-        # log.debug("Decoding %s", data)
+        log.debug("Decoding %s", data)
-            indexed = bool(current & 0x80)
+            indexed = True if current & 0x80 else False
-            literal_index = bool(current & 0x40)
+            literal_index = True if current & 0x40 else False
-            encoding_update = bool(current & 0x20)
+            encoding_update = True if current & 0x20 else False
-                header, consumed = self._decode_indexed(data[current_index:])
+                header, consumed = self._decode_indexed(
-                    data[current_index:]
+                    data_mem[current_index:]
-                consumed = self._update_encoding_context(data)
+                # It's an update to the encoding context. These are forbidden
-                    data[current_index:]
+                    data_mem[current_index:]
-        return [(n.decode('utf-8'), v.decode('utf-8')) for n, v in headers]
+        # Confirm that the table size is lower than the maximum. We do this
-    def _add_to_header_table(self, new_header):
+        try:
-        Adds a header to the header table, evicting old ones if necessary.
+        Check that the table size set by the encoder is lower than the maximum
-                32 + len(n) + len(v)
+        if self.header_table_size > self.max_allowed_table_size:
-
+        if new_size > self.max_allowed_table_size:
-        # log.debug("Decoded %s, consumed %d", header, consumed)
+        header = HeaderTuple(*self.header_table.get_by_index(index))
-            indexed_name = to_byte(data[0]) & 0x0F
+            high_byte = to_byte(data[0])
-                name = Decoder.static_table[index][0]
+            name = self.header_table.get_by_index(index)[0]
-                name = self.huffman_coder.decode(name)
+                name = decode_huffman(name)
-            value = self.huffman_coder.decode(value)
+            value = decode_huffman(value)
-            self._add_to_header_table(header)
+            self.header_table.add(name, value)
-        # log.debug(  "Decoded %s, total consumed %d bytes, indexed %s", header, total_consumed, should_index)
+        log.debug(
-    # log.debug("Using nghttp2's HPACK implementation.")
+    log.debug("Using nghttp2's HPACK implementation.")
-    # log.debug("Using our pure-Python HPACK implementation.")
+    log.debug("Using our pure-Python HPACK implementation.")
-if USE_NGHTTP2:
+if USE_NGHTTP2:  # noqa
-            # log.debug("Setting header table size to %d", value)
+            log.debug("Setting header table size to %d", value)
-            # log.debug("HPACK encoding %s", headers)
+            log.debug("HPACK encoding %s", headers)
-            # log.debug("Setting header table size to %d", value)
+            log.debug("Setting header table size to %d", value)
-            # log.debug("Decoding %s", data)
+            log.debug("Decoding %s", data)
-    from .hpack import Encoder, Decoder
+    from .hpack import Encoder, Decoder  # noqa
-            bin_int = self.huffman_code_list[byte] & (2 ** (bin_int_len + 1) - 1)
+            bin_int = self.huffman_code_list[byte] & (
-        final_num |= (1 << (bits_to_be_padded)) - 1
+        final_num |= (1 << bits_to_be_padded) - 1
-        out_list = [ self.read_buffer[self.buffer_start:] ]
+        out_list = [ self.read_buffer[self.buffer_start:] ]
-            self.notify()
+        try:
-import json
+
-        self.buf = buf
+        self.buf = memoryview(buf)
-        self.set_var("dispather_min_idle_workers", 0)
+        self.set_var("dispather_min_idle_workers", 1)
-        self.set_var("dispather_max_workers", 60)
+        self.set_var("dispather_max_workers", 5)
-    gae_proxy.http_dispatcher.http_dispatch.close_all_worker()
+    gae_proxy.front.front.http_dispatcher.log_debug_data = log_debug_data
-    worker = gae_proxy.http_dispatcher.http_dispatch.get_worker(nowait=True)
+    worker = gae_proxy.front.front.http_dispatcher.get_worker(nowait=True)
-    return len(gae_proxy.http_dispatcher.http_dispatch.workers)
+    return len(gae_proxy.front.front.http_dispatcher.workers)
-#from heroku_front import web_control as heroku_web
+from heroku_front import web_control as heroku_web
-        frame, length = Frame.parse_frame_header(header)
+        frame, length = Frame.parse_frame_header(header.tobytes())
-        self._consume_frame_payload(frame, data)
+        self._consume_frame_payload(frame, data.tobytes())
-            self.logger.debug("%s %s%s status:%d trace:%s", method, response.worker.ssl_sock.host, path, status,
+            self.logger.debug("%s %s%s status:%d trace:%s", method, response.worker.host, path, status,
-            self.logger.warn("%s %s%s status:%d trace:%s", method, response.worker.ssl_sock.host, path, status,
+            self.logger.warn("%s %s%s status:%d trace:%s", method, response.worker.host, path, status,
-        host = host_sub + ".xx-net.net"
+        #p = host.find(".")
-    content, status, response = front.request("GET", "dns.xx-net.com", path="/query?domain=www.google.com")
+    content, status, response = front.request("GET", "dns.xx-net.net", path="/query?domain=www.google.com")
-        xlog.warn("start report_stat")
+        xlog.debug("start report_stat")
-    #print("del response")
+
-
+
-                    self._send_cb, self._close_stream_cb,
+                    self._send_cb, self._close_stream_cb, self.encode_header, self.decoder,
-        return ",".join(out_list)
+        return ",".join(out_list)
-from hyper.packages.hpack.hpack_compat import Encoder, Decoder
+                 encoder,
-        self._decoder = Decoder()
+        self._encoder = encoder
-        self.add_header(":Authority", self.task.host)
+        self.add_header(":Authority", host)
-        encoded_headers = self._encoder.encode(self.request_headers)
+        encoded_headers = self._encoder(self.request_headers)
-            self.response_header_datas = [frame.data]
+            #self.response_header_datas = [frame.data]
-            headers = self._decoder.decode(b''.join(self.response_header_datas))
+            header_data = b''.join(self.response_header_datas)
-    def __init__(self, logger, config, ip_manager, connection_manager):
+    def __init__(self, logger, config, ip_manager, connection_manager,
-            worker = Http2Worker(self.logger, self.ip_manager, self.config, ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = self.http2worker(self.logger, self.ip_manager, self.config, ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
-            worker = Http1Worker(self.logger, self.ip_manager, self.config, ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = self.http1worker(self.logger, self.ip_manager, self.config, ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
-from front import front
+from front import front
-xlog = getLogger("cloudflare_front")
+from front import front
-    check_ip.load_proxy_config()
+def set_proxy(args):
-import struct
+
-import json
+import os
-
+sys.path.append(root_path)
-    response.begin(timeout=5)
+noarch_lib = os.path.abspath( os.path.join(python_path, 'lib', 'noarch'))
-        return False
+if sys.platform == "win32":
-        return False
+import xlog
-    xlog.info("check_xtunnel ok, time:%d", time_cost)
+from front_base.openssl_wrap import SSLContext
-        return check_xtunnel_http2(ssl_sock, host)
+from config import Config
-    #    connect use domain, print altNames
+    #    connect use domain
-        ip = "104.28.100.89"
+        ip = "104.28.77.8"
-    xlog.info("test ip:%s", ip)
+    print("test ip:%s" % ip)
-        xlog.info("test top domain:%s", top_domain)
+    else:
-    res = test_xtunnel_ip2(ip, top_domain=top_domain)
+    connect_creator = ConnectCreator(logger, config, openssl_context)
-        print("success, domain:%s handshake:%d" % (res.domain, res.handshake_time))
+    elif res.ok:
-
+        print("not support")
-# coding:utf-8
+from front_base.config import ConfigBase
-import io
+class Config(ConfigBase):
-xlog = getLogger("cloudflare_front")
+        # front
-    current_path = os.path.dirname(os.path.abspath(__file__))
+        # connect_manager
-        self.CONFIG_FILENAME = os.path.abspath( os.path.join(current_path, 'default_config.ini'))
+        # check_ip
-            self.DATA_PATH = current_path
+        # host_manager
-config.load()
+        # ip_manager
-import time
+import time
-import check_ip
+import xlog
-        self.continue_fail_num = 0
+
-        self.last_host = "center.xx-net.net"
+        self.continue_fail_num = 0
-        while True:
+        while self.running:
-            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host, self.log_debug_data)
+        self.init_host_dispatcher(host)
-            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host, self.log_debug_data)
+        self.init_host_dispatcher(host)
-            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host, self.log_debug_data)
+        self.init_host_dispatcher(host)
-            xlog.warn("req %s get response timeout", path)
+            self.logger.warn("req %s get response timeout", path)
-            # xlog.warn("front request %s %s%s fail, status:%d", method, host, path, status)
+            # self.logger.warn("front request %s %s%s fail, status:%d", method, host, path, status)
-            xlog.debug("%s %s%s status:%d trace:%s", method, response.worker.ssl_sock.host, path, status,
+            self.logger.debug("%s %s%s status:%d trace:%s", method, response.worker.ssl_sock.host, path, status,
-            xlog.warn("%s %s%s status:%d trace:%s", method, response.worker.ssl_sock.host, path, status,
+            self.logger.warn("%s %s%s status:%d trace:%s", method, response.worker.ssl_sock.host, path, status,
-        connect_control.keep_running = False
+        logger.info("terminate")
-front = Front()
+front = Front()
-        self.task_queue.put(None)
+import utils
-import struct
+from front_base.http2_connection import Http2Worker
-from http_common import *
+class CloudflareHttp2Worker(Http2Worker):
-from hyper.common.bufsocket import BufferedSocket
+    def get_host(self, task_host):
-        return ",".join(out_list)
+        return self.host
-    scan_ip_log.info("ADD ab3")
+import threading
-def main():
+def get():
-        time.sleep(1)
+    time_cost = time.time() - start_time
-        main()
+        get()
-os.environ['HTTPS_PROXY'] = ''
+
-            self.postvars = {}
+            front.logger.warn('Control Req %s %s %s ', self.address_string(), self.command, self.path)
-            xlog.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
+        self.wfile.write(b'HTTP/1.1 404\r\nContent-Type: text/plain\r\nConnection: close\r\n\r\n404 Not Found')
-            data = xlog.get_last_lines(max_line)
+            data = front.logger.get_last_lines(max_line)
-            data = xlog.get_new_lines(last_no)
+            data = front.logger.get_new_lines(last_no)
-            xlog.error('PAC %s %s %s ', self.address_string(), self.command, self.path)
+            front.logger.error('PAC %s %s %s ', self.address_string(), self.command, self.path)
-        data += "time:%d  pointer:%d<br>\r\n" % (time_now, ip_manager.gws_ip_pointer)
+        data += "time:%d  pointer:%d<br>\r\n" % (time_now, front.ip_manager.gws_ip_pointer)
-            handshake_time = ip_manager.ip_dict[ip]["handshake_time"]
+        for ip in front.ip_manager.gws_ip_list:
-            links = ip_manager.ip_dict[ip]["links"]
+            fail_times = front.ip_manager.ip_dict[ip]["fail_times"]
-            get_time = ip_manager.ip_dict[ip]["get_time"]
+            get_time = front.ip_manager.ip_dict[ip]["get_time"]
-            success_time = ip_manager.ip_dict[ip]["success_time"]
+            success_time = front.ip_manager.ip_dict[ip]["success_time"]
-            fail_time = ip_manager.ip_dict[ip]["fail_time"]
+            fail_time = front.ip_manager.ip_dict[ip]["fail_time"]
-            down_fail_time = ip_manager.ip_dict[ip]["down_fail_time"]
+            down_fail_time = front.ip_manager.ip_dict[ip]["down_fail_time"]
-            data_active = ip_manager.ip_dict[ip]["data_active"]
+            data_active = front.ip_manager.ip_dict[ip]["data_active"]
-            history = ip_manager.ip_dict[ip]["history"]
+            history = front.ip_manager.ip_dict[ip]["history"]
-
+        objs = [front.connect_manager] + front.dispatchs.values()
-    config_path = os.path.join(module_data_path, "tls_relay.json")
+    config_path = os.path.join(module_data_path, "heroku_front.json")
-    content, status, response = front.request("GET", "scan1.xx-net.com", path="/wait?time=5")
+    content, status, response = front.request("GET", "dns.xx-net.com", path="/query?domain=www.google.com")
-os.environ['HTTPS_PROXY'] = ''
+
-            xlog.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
+            front.logger.warn('Control Req %s %s %s ', self.address_string(), self.command, self.path)
-            xlog.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
+        self.wfile.write(b'HTTP/1.1 404\r\nContent-Type: text/plain\r\nConnection: close\r\n\r\n404 Not Found')
-            data = xlog.get_last_lines(max_line)
+            data = front.logger.get_last_lines(max_line)
-            data = xlog.get_new_lines(last_no)
+            data = front.logger.get_new_lines(last_no)
-            xlog.error('PAC %s %s %s ', self.address_string(), self.command, self.path)
+            front.logger.error('PAC %s %s %s ', self.address_string(), self.command, self.path)
-        data += "time:%d  pointer:%d<br>\r\n" % (time_now, ip_manager.gws_ip_pointer)
+        data += "time:%d  pointer:%d<br>\r\n" % (time_now, front.ip_manager.gws_ip_pointer)
-            handshake_time = ip_manager.ip_dict[ip]["handshake_time"]
+        for ip in front.ip_manager.gws_ip_list:
-            links = ip_manager.ip_dict[ip]["links"]
+            fail_times = front.ip_manager.ip_dict[ip]["fail_times"]
-            get_time = ip_manager.ip_dict[ip]["get_time"]
+            get_time = front.ip_manager.ip_dict[ip]["get_time"]
-            success_time = ip_manager.ip_dict[ip]["success_time"]
+            success_time = front.ip_manager.ip_dict[ip]["success_time"]
-            fail_time = ip_manager.ip_dict[ip]["fail_time"]
+            fail_time = front.ip_manager.ip_dict[ip]["fail_time"]
-            down_fail_time = ip_manager.ip_dict[ip]["down_fail_time"]
+            down_fail_time = front.ip_manager.ip_dict[ip]["down_fail_time"]
-            data_active = ip_manager.ip_dict[ip]["data_active"]
+            data_active = front.ip_manager.ip_dict[ip]["data_active"]
-            history = ip_manager.ip_dict[ip]["history"]
+            history = front.ip_manager.ip_dict[ip]["history"]
-        for obj in [https_manager, http_dispatch]:
+        for obj in [front.connect_manager, front.http_dispatcher]:
-                data += "    %s = %s\r\n" % (attr, getattr(obj, attr))
+                sub_obj = getattr(obj, attr)
-        threading.Thread(target=self.reporter).start()
+        if g.config.enable_tls_relay:
-            sleep(30)
+            sleep(60)
-            self.last_state["timeout"] = g.stat["timeout_roundtrip"]
+        if not g.config.login_account:
-    config.set_var("timeout_threshold", 5)
+    config.set_var("timeout_threshold", 2)
-            sleep(10)
+            sleep(30)
-            raise
+            raise Exception()
-        self.server_list = self.get_dns_server()
+        self.local_list = self.get_dns_server()
-        return self.server_list[self.i]
+    def get_local(self):
-            self.i = self.i % len(self.server_list)
+        if self.i >= len(self.local_list):
-        xlog.debug("next dns server:%s", self.get())
+        xlog.debug("next dns server:%s", self.get_local())
-        return random.choice(self.local_server_list)
+    def get_public(self):
-        for i in xrange(0, len(self.dns_server.server_list)):
+        if "." not in domain:
-            self.send_request(id, domain, server)
+            self.send_request(id, domain, ip)
-    if record:
+    ips = g.dns_srv.query(host)
-from http_dispatcher import http_dispatch
+    from web_control import user_config
-    user_config.LISTEN_IP = args["ip"]
+    user_config.user_special.LISTEN_IP = args["ip"]
-            raise
+            raise Exception()
-from config import config
+try:
-        q = Queue.Queue()
+        q = simple_queue.Queue()
-        response = q.get(True)
+        response = q.get(timeout=timeout)
-            xlog.exception("download %s to %s fail:%r", url, filename, e)
+            xlog.warn("download %s to %s fail:%r", url, filename, e)
-class HTTP1_worker(HTTP_worker):
+class Http1Worker(HttpWorker):
-        super(HTTP1_worker, self).__init__(ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data)
+    def __init__(self, logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb):
-
+        self.task_queue = Queue.Queue()
-        threading.Thread(target=self.keep_alive_thread).start()
+        self.idle_cb()
-        # xlog.debug("%s stat:%s", self.ip, active)
+        # self.logger.debug("%s stat:%s", self.ip, active)
-            if not connect_control.keep_running or not self.keep_running:
+        while time.time() - self.ssl_sock.create_time < self.config.http1_first_ping_wait:
-        if self.processed_tasks == 0:
+        if self.config.http1_first_ping_wait and self.processed_tasks == 0:
-            time.sleep(time_to_ping)
+        if self.config.http1_ping_interval:
-                return
+                if not self.request_onway and \
-            task = self.task_queue.get(99999999)
+        while self.keep_running:
-                    ip_manager.recheck_ip(self.ssl_sock.ip)
+                    self.ip_manager.recheck_ip(self.ssl_sock.ip)
-            # xlog.debug("http1 get task")
+            # self.logger.debug("http1 get task")
-                xlog.warn("get task but inactive time:%d", time_now - self.last_active_time)
+            if time_now - self.last_active_time > self.config.http1_idle_time:
-            if self.processed_tasks > 35:
+            if self.processed_tasks > self.config.http1_max_process_tasks:
-        task.headers['Host'] = task.host
+        if task.host:
-            xlog.warn("%s h1_request:%r inactive_time:%d task.timeout:%d",
+            self.logger.exception("%s h1_request:%r inactive_time:%d task.timeout:%d",
-            xlog.warn('%s trace:%s', self.ip, self.get_trace())
+            self.logger.warn('%s trace:%s', self.ip, self.get_trace())
-            self.close("request fail")
+            self.retry_task_cb(task)
-            xlog.warn("read fail, ip:%s, chunk:%d url:%s task.timeout:%d e:%r",
+            self.logger.exception("read fail, ip:%s, chunk:%d url:%s task.timeout:%d e:%r",
-            self.close("read fail")
+            self.logger.warn('%s trace:%s', self.ip, self.get_trace())
-        # xlog.debug("head request %s", self.ip)
+        self.logger.debug("head request %s", self.ip)
-                xlog.warn('%s trace:%s', self.ip, self.get_trace())
+                self.logger.warn("h1 head send len:%r %d %s", ret, len(data), self.ip)
-            response.begin(timeout=30)
+            response.begin(timeout=5)
-                xlog.warn("%s host:%s head fail status:%d", self.ip, self.ssl_sock.host, status)
+                self.logger.warn("%s host:%s head fail status:%d", self.ip, self.ssl_sock.host, status)
-            self.accept_task = True
+            self.rtt = (time.time() - start_time) * 1000
-            self.close("head fail")
+            self.logger.warn("h1 %s HEAD keep alive request fail:%r", self.ssl_sock.ip, e)
-
+        self.task_queue.put(None)
-        self.task_queue.put(None)
+        super(Http1Worker, self).close(reason)
-class HTTP2_worker(HTTP_worker):
+class Http2Worker(HttpWorker):
-        super(HTTP2_worker, self).__init__(ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data)
+    def __init__(self, logger, ip_manager, config, ssl_sock, close_cb, retry_task_cb, idle_cb):
-        self.last_active_time = self.ssl_sock.create_time
+        self.last_active_time = self.ssl_sock.create_time - 1
-            SettingsFrame.INITIAL_WINDOW_SIZE: 1 * 1024 * 1024,
+            SettingsFrame.INITIAL_WINDOW_SIZE: 16 * 1024 * 1024,
-        self.local_connection_initial_windows = 2 * 1024 * 1024
+        self.local_connection_initial_windows = 32 * 1024 * 1024
-        self.remote_window_size = DEFAULT_WINDOW_SIZE
+        #self.remote_window_size = DEFAULT_WINDOW_SIZE
-        if len(self.streams) > self.max_concurrent:
+        if len(self.streams) > self.config.http2_max_concurrent:
-                    self._send_cb, self._close_stream_cb, self.encoder, self.decoder,
+        stream = Stream(self.logger, self.config, self, self.ip, stream_id, task,
-        while connect_control.keep_running and self.keep_running:
+        while self.keep_running:
-            # xlog.debug("%s Send:%s", self.ip, str(frame))
+            # self.logger.debug("%s Send:%s", self.ip, str(frame))
-                    xlog.warn("%s http2 send fail:%r", self.ip, e)
+                    self.logger.warn("%s http2 send fail:%r", self.ip, e)
-                    xlog.exception("send error:%r", e)
+                    self.logger.exception("send error:%r", e)
-                xlog.debug("http2 %s send error:%r", self.ip, e)
+                self.logger.debug("http2 %s send error:%r", self.ip, e)
-        while connect_control.keep_running and self.keep_running:
+        while self.keep_running:
-                xlog.exception("recv fail:%r", e)
+                self.logger.exception("recv fail:%r", e)
-    def close(self, reason=""):
+    def get_rtt_rate(self):
-        super(HTTP2_worker, self).close(reason)
+        super(Http2Worker, self).close(reason)
-        #xlog.debug("%s increase send win:%d result:%d", self.ip, inc_size, self.remote_window_size)
+        #self.logger.debug("%s increase send win:%d result:%d", self.ip, inc_size, self.remote_window_size)
-        # xlog.debug("%s close stream:%d %s", self.ssl_sock.ip, stream_id, reason)
+        # self.logger.debug("%s close stream:%d %s", self.ssl_sock.ip, stream_id, reason)
-        if self.keep_running and len(self.streams) < self.max_concurrent and self.remote_window_size > 10000:
+        if self.keep_running and \
-            xlog.debug("%s _consume_single_frame:%r, inactive time:%d", self.ip, e, time.time()-self.last_active_time)
+            self.logger.debug("%s _consume_single_frame:%r, inactive time:%d", self.ip, e, time.time()-self.last_active_time)
-            xlog.error("%s Frame size exceeded on stream %d (received: %d, max: %d)",
+        if length > FRAME_MAX_ALLOWED_LEN:
-        # xlog.debug("%s Recv:%s", self.ip, str(frame))
+        # self.logger.debug("%s Recv:%s", self.ip, str(frame))
-                #xlog.debug("%s frame size:%d increase win:%d", self.ip, size, increment)
+            if increment < 0:
-            xlog.error("%s receive push frame", self.ip,)
+            self.logger.error("%s receive push frame", self.ip,)
-                self.last_active_time = time.time()
+                stream = self.streams[frame.stream_id]
-                xlog.exception("%s Unexpected stream identifier %d, e:%r", self.ip, frame.stream_id, e)
+                if frame.type != WindowUpdateFrame.type:
-                    xlog.error("rtt:%f ping_time:%f now:%f", rtt, ping_time, time_now)
+                    self.logger.error("rtt:%f ping_time:%f now:%f", rtt, ping_time, time_now)
-                #xlog.debug("RTT:%d, on_way:%d", self.rtt, self.ping_on_way)
+                #self.logger.debug("RTT:%d, on_way:%d", self.rtt, self.ping_on_way)
-            self.last_active_time = time.time()
+            # self.last_active_time = time.time()
-                xlog.warn("goaway:%s, t:%d", error_string, time_cost)
+                self.logger.warn("goaway:%s, t:%d", error_string, time_cost)
-            xlog.warn("%s get BlockedFrame", self.ip)
+            self.logger.warn("%s get BlockedFrame", self.ip)
-            xlog.error("%s Unexpected frame %s.", self.ip, frame)
+            self.logger.error("%s Unexpected frame %s.", self.ip, frame)
-            xlog.error("%s Received unknown frame, type %d", self.ip, frame.type)
+            self.logger.error("%s Received unknown frame, type %d", self.ip, frame.type)
-            self.encoder.header_table_size = new_size
+            #self.encoder.header_table_size = new_size
-                xlog.error("%s Frame size %d is outside of allowed range", self.ip, new_size)
+                self.logger.error("%s Frame size %d is outside of allowed range", self.ip, new_size)
-        out_list.append(" sni:%s" % self.ssl_sock.sni)
+        out_list.append(" sni:%s, host:%s" % (self.ssl_sock.sni, self.ssl_sock.host))
-xlog = getLogger("heroku_front")
+                 logger,
-                 header_decoder,
+        self.logger = logger
-        self._decoder = header_decoder
+        self._encoder = Encoder()
-        threading.Thread(target=self.timeout_response).start()
+        threading.Thread(target=self.start_request).start()
-    def start(self):
+    def start_request(self):
-        self.add_header(":path", self.task.path)
+        self.add_header(":Method", self.task.method)
-        # xlog.debug("stream %d recved frame %r", self.stream_id, frame)
+        # self.logger.debug("stream %d recved frame %r", self.stream_id, frame)
-            xlog.error("%s receive PushPromiseFrame:%d", self.ip, frame.stream_id)
+            self.logger.error("%s receive PushPromiseFrame:%d", self.ip, frame.stream_id)
-                #    xlog.debug("stream:%d frame size:%d increase win:%d", self.stream_id, size, increment)
+                #    self.logger.debug("stream:%d frame size:%d increase win:%d", self.stream_id, size, increment)
-                #xlog.debug("%s get:%d s:%d", self.ip, self.response_body_len, size)
+                #self.logger.debug("%s get:%d s:%d", self.ip, self.response_body_len, size)
-            xlog.debug("%s Stream %d Rest by server, inactive:%d. error code:%d",
+            self.logger.debug("%s Stream %d Rest by server, inactive:%d. error code:%d",
-            xlog.error("%s Unexpected frame %s.", self.ip, frame)
+            self.logger.error("%s Unexpected frame %s.", self.ip, frame)
-            xlog.error("%s Received unknown frame, type %d", self.ip, frame.type)
+            self.logger.error("%s Received unknown frame, type %d", self.ip, frame.type)
-            #xlog.debug("%s Closing remote side of stream:%d", self.ip, self.stream_id)
+            #self.logger.debug("%s Closing remote side of stream:%d", self.ip, self.stream_id)
-            xlog.warn("http2_stream send_response but responsed.%s", self.task.url)
+            self.logger.error("http2_stream send_response but responsed.%s", self.task.url)
-        response = BaseResponse(status=status, headers=self.response_headers)
+        response = simple_http_client.BaseResponse(status=status, headers=self.response_headers)
-
+    def close(self, reason="close"):
-            self.connection.retry_task_cb(self.task)
+            self.connection.retry_task_cb(self.task, reason)
-            self.task.put_data("")
+            self.task.finish()
-        while time.time() - self.task.start_time < self.task.timeout:
+        start_time = time.time()
-        xlog.warn("h2 %s %s timeout %s",
+        self.logger.warn("h2 timeout %s task_trace:%s worker_trace:%s",
-                  self.task.unique_id,
+                  self.task.get_trace(),
-        self.headers = headers
+import simple_queue
-    def __init__(self, method, host, path, headers, body, queue, url, timeout):
+    def __init__(self, logger, config, method, host, path, headers, body, queue, url, timeout):
-        out_list = [self.read_buffer]
+        out_list = []
-        # xlog.debug("%s stat:%s", self.unique_id, stat)
+        if self.config.show_state_debug:
-            xlog.error("http_common responsed_fail but responed.%s", self.url)
+            self.logger.error("http_common responsed_fail but responed.%s", self.url)
-        res = BaseResponse(body=err_text)
+        self.logger.debug("%s %s", self.url, err_text)
-    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
+class HttpWorker(object):
-        self.log_debug_data = log_debug_data
+        self.last_active_time = ssl_sock.create_time
-        xlog.debug("%s worker close:%s", self.ip, reason)
+        self.logger.debug("%s worker close:%s", self.ip, reason)
-                rtt = 1000
+        if inactive_time > 30 and rtt > 1000:
-        else:
+        if self.version != "1.1":
-            score = rtt + 50000
+        if inactive_time < 1:
-            score = rtt + (10/inactive_time)*1000
+            score = rtt + (240 - inactive_time)*10
-from xlog import getLogger
+import simple_queue
-g_cacertfile = os.path.join(current_path, "cacert.pem")
+from http1 import Http1Worker
-        self.log_debug_data = log_debug_data
+    def __init__(self, logger, config, ip_manager, connection_manager):
-        self.create_worker_th = None
+        self.running = True
-        self.wait_a_worker_cv = SimpleCondition()
+        self.wait_a_worker_cv = simple_queue.Queue()
-        self.https_manager = connect_manager.Https_connection_manager(host, self.on_ssl_created_cb)
+    def stop(self):
-            worker = HTTP2_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb, self.log_debug_data)
+            worker = Http2Worker(self.logger, self.ip_manager, self.config, ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
-            worker = HTTP1_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb, self.log_debug_data)
+            worker = Http1Worker(self.logger, self.ip_manager, self.config, ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
-        while connect_control.keep_running:
+        while self.running:
-                ssl_sock = self.https_manager.get_ssl_connection()
+                ssl_sock = self.connection_manager.get_ssl_connection()
-                # xlog.warn("create_worker_thread get ssl_sock fail")
+                # self.logger.warn("create_worker_thread get ssl_sock fail")
-                xlog.exception("on_ssl_created_cb e:%r", e)
+            except:
-        while connect_control.keep_running:
+        while self.running:
-            if best_worker is None or idle_num < 5 or (now - best_worker.last_active_time) < 2:
+            if best_worker is None or \
-            if best_worker or nowait:
+            if nowait or \
-            self.wait_a_worker_cv.wait()
+            self.wait_a_worker_cv.wait(time.time() + 1)
-            if idle_num < 10 or idle_num < int(len(self.workers) * 0.3) or len(self.workers) < 50:
+            if idle_num < 10 or \
-        # xlog.debug("task start request")
+        # self.logger.debug("task start request")
-        task = http_common.Task(method, host, path, headers, body, q, url, timeout)
+        q = simple_queue.Queue()
-        response = q.get(True)
+        response = q.get(timeout=timeout)
-    def retry_task_cb(self, task):
+    def retry_task_cb(self, task, reason=""):
-            xlog.warn("retry but responsed. %s", task.url)
+            self.logger.warn("retry but responsed. %s", task.url)
-            task.put_data("")
+            self.logger.warn("stack:%r", repr(stl))
-        if time.time() - task.start_time > 240:
+        if time.time() - task.start_time > task.timeout:
-        task.set_state("retry")
+        if not self.running:
-        while connect_control.keep_running:
+        while self.running:
-                xlog.exception("http_dispatcher dispatcher request_queue.get fail:%r", e)
+                self.logger.exception("http_dispatcher dispatcher request_queue.get fail:%r", e)
-                xlog.warn("get worker fail:%r", e)
+                self.logger.warn("get worker fail:%r", e)
-                xlog.warn("http_dispatcher get None worker")
+                self.logger.warn("http_dispatcher get None worker")
-            task.set_state("get_worker:%s" % worker.ip)
+            get_worker_time = time.time()
-                xlog.exception("dispatch request:%r", e)
+                self.logger.exception("dispatch request:%r", e)
-            w.close("close all worker")
+    def close_all_worker(self, reason="close all worker"):
-            worker_rate[w] = w.get_score()
+            worker_rate[w] = w.get_rtt_rate()
-                       (w.ip, w.rtt, w.accept_task, (time.time()-w.ssl_sock.create_time), w.processed_tasks)
+            out_str += "%s rtt:%d running:%d accept:%d live:%d inactive:%d processed:%d" % \
-                out_str += " streams:%d ping_on_way:%d\r\n" % (len(w.streams), w.ping_on_way)
+                out_str += " streams:%d ping_on_way:%d remote_win:%d send_queue:%d\r\n" % \
-
+import random
-current_path = os.path.dirname(os.path.abspath(__file__))
+class IpManagerBase():
-    python_path = os.path.join(root, 'python27', '1.0')
+    def load_config(self):
-    sys.path.append(noarch_lib)
+    def set_ips(self, ips):
-        sys.path.append(linux_lib)
+    def get_ip(self):
-import ip_range
+    def update_ip(self, ip, handshake_time):
-xlog = getLogger("heroku_front")
+    def report_connect_fail(self, ip, reason=""):
-from scan_ip_log import scan_ip_log
+    def report_connect_closed(self, ip, reason=""):
-    def __init__(self):
+    def __init__(self, logger, config, ip_source, check_local_network, check_ip,
-        if config.getint("ip_manager", "check_exist_ip_on_startup"):
+        if config.check_exist_ip_on_startup:
-        self.gws_ip_pointer_reset_time = 0
+        self.ip_pointer = 0
-        self.last_sort_time_for_gws = 0 # keep status for avoid wast too many cpu
+        self.last_sort_time = 0 # keep status for avoid wast too many cpu
-        self.gws_ip_list = []
+        self.ip_list = []
-        self.search_more_google_ip()
+        self.search_more_ip()
-        if len(self.gws_ip_list) >= self.max_good_ip_num:
+        if len(self.ip_list) >= self.max_good_ip_num:
-        self.default_good_ip_file = os.path.join(current_path, default_good_ip_file_name)
+        self.max_scan_ip_thread_num = self.config.max_scan_ip_thread_num
-        self.record_ip_history = config.CONFIG.getint("ip_manager", "record_ip_history")
+        self.max_links_per_ip = self.config.max_links_per_ip
-            file_path = self.good_ip_file
+        if os.path.isfile(self.ip_list_fn):
-            file_path = self.default_good_ip_file
+            return
-                    xlog.warning("line err: %s", line)
+                    self.logger.warning("line err: %s", line)
-                #xlog.info("load ip: %s time:%d domain:%s server:%s", ip, handshake_time, domain, server)
+                #self.logger.info("load ip: %s time:%d domain:%s server:%s", ip, handshake_time, domain, server)
-                xlog.exception("load_ip line:%s err:%s", line, e)
+                self.logger.exception("load_ip line:%s err:%s", line, e)
-        self.try_sort_gws_ip(force=True)
+        self.logger.info("load ip_list num:%d, target num:%d", len(self.ip_dict), len(self.ip_list))
-        #    xlog.info("first run, rescan all exist ip")
+        #    self.logger.info("first run, rescan all exist ip")
-    def save_ip_list(self, force=False):
+    def save(self, force=False):
-            with open(self.good_ip_file, "w") as fd:
+            with open(self.ip_list_fn, "w") as fd:
-            xlog.error("save good_ip.txt fail %s", e)
+            self.logger.error("save good_ip.txt fail %s", e)
-        if time.time() - self.last_sort_time_for_gws < 10 and not force:
+    def try_sort_ip(self, force=False):
-        self.last_sort_time_for_gws = time.time()
+        self.last_sort_time = time.time()
-            self.gws_ip_list = [ip for ip,rate in ip_time]
+            self.ip_list = [ip for ip, rate in ip_time]
-            xlog.error("try_sort_ip_by_handshake_time:%s", e)
+            self.logger.error("try_sort_ip_by_handshake_time:%s", e)
-        time_cost = (( time.time() - self.last_sort_time_for_gws) * 1000)
+        time_cost = ((time.time() - self.last_sort_time) * 1000)
-            xlog.debug("sort ip time:%dms", time_cost) # 5ms for 1000 ip. 70~150ms for 30000 ip.
+            self.logger.debug("sort ip time:%dms", time_cost) # 5ms for 1000 ip. 70~150ms for 30000 ip.
-        elif len(self.gws_ip_list) < 100:
+        elif len(self.ip_list) < 100:
-                the_100th_ip = self.gws_ip_list[99]
+                the_100th_ip = self.ip_list[99]
-                xlog.warn("adjust_scan_thread_num fail:%r", e)
+                self.logger.warn("adjust_scan_thread_num fail:%r", e)
-            xlog.info("Adjust scan thread num from %d to %d", self.scan_ip_thread_num, scan_ip_thread_num)
+            self.logger.info("Adjust scan thread num from %d to %d", self.scan_ip_thread_num, scan_ip_thread_num)
-            self.search_more_google_ip()
+            self.search_more_ip()
-            iplist_length = len(self.gws_ip_list)
+            iplist_length = len(self.ip_list)
-                last_ip = self.gws_ip_list[i]
+                last_ip = self.ip_list[i]
-        self.try_sort_gws_ip()
+    def get_ip(self):
-            ip_num = len(self.gws_ip_list)
+            ip_num = len(self.ip_list)
-                #time.sleep(10)
+                #self.logger.warning("no ip")
-                    if time_now - self.gws_ip_pointer_reset_time < 1:
+                if self.ip_pointer >= ip_num:
-                    self.gws_ip_pointer_reset_time = time_now
+                        self.ip_pointer = 0
-                ip = self.gws_ip_list[self.gws_ip_pointer]
+                ip = self.ip_list[self.ip_pointer]
-                    self.gws_ip_pointer += 1
+                    self.ip_pointer += 1
-                    fail_connect_interval = 3 * 60
+                if time_now - self.ip_dict[ip]['success_time'] > self.config.long_fail_threshold: # 5 min
-                    fail_connect_interval = 10
+                    fail_connect_interval = self.config.short_fail_connect_interval # 10
-                    self.gws_ip_pointer += 1
+                    self.ip_pointer += 1
-                    self.gws_ip_pointer += 1
+                if time_now - down_fail_time < self.config.down_fail_connect_interval:
-                    self.gws_ip_pointer += 1
+                    self.ip_pointer += 1
-                # xlog.debug("get ip:%s t:%d", ip, handshake_time)
+                # self.logger.debug("get ip:%s t:%d", ip, handshake_time)
-                self.gws_ip_pointer += 1
+                self.ip_pointer += 1
-            xlog.exception("get_gws_ip fail:%r", e)
+            self.logger.exception("get_ip fail:%r", e)
-    def add_ip(self, ip, handshake_time, domain=None, server='', fail_times=0, down_fail=0):
+    def add_ip(self, ip, handshake_time=100, domain=None, server='gws', fail_times=0, down_fail=0):
-            xlog.error("add_ip input")
+            self.logger.error("add_ip input")
-            return
+        ip = str(ip)
-                self.gws_ip_list.append(ip)
+                self.ip_list.append(ip)
-            xlog.exception("add_ip err:%s", e)
+            self.logger.exception("add_ip err:%s", e)
-            xlog.error("set_ip input")
+            self.logger.error("set_ip input")
-            xlog.warn("%s handshake:%d impossible", ip, 1000 * handshake_time)
+            self.logger.warn("%s handshake:%d impossible", ip, 1000 * handshake_time)
-        check_ip.continue_fail_count = 0
+        self.check_local_network.report_ok(ip)
-            #xlog.debug("update ip:%s not exist", ip)
+            #self.logger.debug("update ip:%s not exist", ip)
-            xlog.error("update_ip err:%s", e)
+            self.logger.error("update_ip err:%s", e)
-        self.save_ip_list()
+        self.save()
-    def report_connect_fail(self, ip, force_remove=False):
+    def report_connect_fail(self, ip, reason="", force_remove=False):
-                xlog.debug("report_connect_fail %s not exist", ip)
+                self.logger.debug("report_connect_fail %s not exist", ip)
-                    self.gws_ip_list.remove(ip)
+                if ip in self.ip_list:
-                xlog.info("remove ip:%s left amount:%d gws_num:%d", ip, len(self.ip_dict), len(self.gws_ip_list))
+                self.logger.info("remove ip:%s left amount:%d target_num:%d", ip, len(self.ip_dict), len(self.ip_list))
-            if not check_local_network.is_ok():
+            if not self.check_local_network.is_ok(ip):
-                xlog.debug("fail time too near %s", ip)
+                self.logger.debug("fail time too near %s", ip)
-            xlog.debug("report_connect_fail:%s", ip)
+            self.logger.debug("report_connect_fail:%s", ip)
-            xlog.exception("report_connect_fail err:%s", e)
+            self.logger.exception("report_connect_fail err:%s", e)
-            self.search_more_google_ip()
+            self.search_more_ip()
-        xlog.debug("%s close:%s", ip, reason)
+        self.logger.debug("%s close:%s", ip, reason)
-            # xlog.debug("ssl_closed %s", ip)
+            # self.logger.debug("ssl_closed %s", ip)
-            xlog.error("ssl_closed %s err:%s", ip, e)
+            self.logger.error("ssl_closed %s err:%s", ip, e)
-        #xlog.debug("%s ssl_closed:%s", ip, reason)
+        #self.logger.debug("%s ssl_closed:%s", ip, reason)
-                    # xlog.debug("ssl_closed %s", ip)
+                    # self.logger.debug("ssl_closed %s", ip)
-            xlog.error("ssl_closed %s err:%s", ip, e)
+            self.logger.error("ssl_closed %s err:%s", ip, e)
-        while connect_control.keep_running:
+        while self.running:
-            if not check_local_network.is_ok():
+            if not self.check_local_network.is_ok(ip):
-                xlog.debug("restore ip:%s", ip)
+            result = self.check_ip(ip)
-            xlog.debug("ip:%s real fail", ip)
+            self.logger.debug("ip:%s real fail", ip)
-        if len(self.gws_ip_list) <= self.max_good_ip_num:
+        if len(self.ip_list) <= self.max_good_ip_num:
-        self.try_sort_gws_ip(force=True)
+        self.try_sort_ip(force=True)
-            ip_num = len(self.gws_ip_list)
+            ip_num = len(self.ip_list)
-                ip = self.gws_ip_list[ip_num - 1]
+                ip = self.ip_list[ip_num - 1]
-                xlog.info("remove_slowest_ip:%s handshake_time:%d, fails:%d", ip, handshake_time, fails)
+                self.logger.info("remove_slowest_ip:%s handshake_time:%d, fails:%d", ip, handshake_time, fails)
-                    self.gws_ip_list.remove(ip)
+                if ip in self.ip_list:
-            xlog.exception("remove_slowest_ip err:%s", e)
+            self.logger.exception("remove_slowest_ip err:%s", e)
-            xlog.debug("recheck_ip:%s network is fail", ip)
+        if not self.check_local_network.is_ok():
-        connect_control.end_connect_register()
+        result = self.check_ip(ip)
-        if not result.support_xtunnel:
+        if not result.ok:
-            xlog.debug("recheck_ip:%s real fail, removed.", ip)
+            self.logger.debug("recheck_ip:%s real fail, removed.", ip)
-            xlog.debug("recheck_ip:%s restore okl", ip)
+            self.add_ip(ip, result.request_time, result.top_domain)
-                continue
+        while self.scan_thread_count <= self.scan_ip_thread_num and self.running:
-                # xlog.debug("check ip:%s", ip)
+                ip = self.ip_source.get_ip()
-                if ip in self.ip_dict:
+                if not ip or ip in self.ip_dict:
-                if not result or not result.support_xtunnel:
+                result = self.check_ip(ip)
-                    scan_ip_log.info("Add %s time:%d CN:%s ", ip, result.request_time, result.domain)
+                if self.add_ip(ip, result.request_time, result.top_domain):
-                    self.save_ip_list()
+                    self.save()
-                xlog.exception("google_ip.runJob fail:%r", e)
+                self.logger.exception("google_ip.runJob fail:%r", e)
-        #xlog.info("scan_ip_worker exit")
+        #self.logger.info("scan_ip_worker exit")
-        if self.use_ipv6:
+    def search_more_ip(self):
-        xlog.debug("start scan all exist ip, num:%d", self.scan_exist_ip_queue.qsize())
+        self.logger.debug("start scan all exist ip, num:%d", self.scan_exist_ip_queue.qsize())
-        self.save_ip_list(force=True)
+        self.try_sort_ip()
-            xlog.warn("scan all exist ip is running")
+            self.logger.warn("scan all exist ip is running")
-        while connect_control.keep_running and self.keep_scan_all_exist_ip:
+        while self.running and self.keep_scan_all_exist_ip:
-            connect_control.end_connect_register()
+            result = self.check_ip(ip)
-                self.add_ip(ip, result.request_time, result.domain, "gws")
+            elif result.ok:
-ip_manager = IpManager()
+        self.try_sort_ip(True)
-        connect_control.keep_running = False
+    def stop(self):
-support_alpn_npn = "no"
+#openssl_version = OpenSSL.version.__version__
-    """OpenSSL Connection Wrapper"""
+        global socks_num
-                #xlog.exception("e:%r", e)
+                #self.logger.exception("e:%r", e)
-            #xlog.exception("ssl send:%r", e)
+            #self.logger.exception("ssl send:%r", e)
-                # xlog.debug("recv_into 0")
+                # self.logger.debug("recv_into 0")
-                    # xlog.debug("recv_into 0")
+                    # self.logger.debug("recv_into 0")
-                #xlog.exception("recv_into:%r", e)
+                #self.logger.exception("recv_into:%r", e)
-            return b"h2"
+
-            return b"http/1.1"
+            ssl_version = "SSLv23"
-            xlog.info("SSL use version:%s", ssl_version)
+        if sys.platform == "darwin":
-        ssl_context = OpenSSL.SSL.Context(protocol_version)
+        self._ssl_context = OpenSSL.SSL.Context(protocol_version)
-            ssl_context.set_verify(OpenSSL.SSL.VERIFY_PEER, lambda c, x, e, d, ok: ok)
+            self._ssl_context.load_verify_locations(os.path.abspath(ca_certs))
-        if True:
+            self._ssl_context.set_verify(OpenSSL.SSL.VERIFY_NONE, lambda c, x,    e, d, ok: ok)
-                return ssl_context
+                self._ssl_context.set_alpn_protos([b'h2', b'http/1.1'])
-                support_alpn_npn = "npn"
+                self._ssl_context.set_npn_select_callback(SSLContext.npn_select_callback)
-    return dns_name
+                self.logger.info("OpenSSL dont't support npn/alpn, no HTTP/2 supported.")
-        if (length > FRAME_MAX_LEN):
+        if (length > FRAME_MAX_ALLOWED_LEN):
-                FRAME_MAX_LEN
+                FRAME_MAX_ALLOWED_LEN
-
+import select
-                        time.sleep(0.1)
+                        #time.sleep(0.1)
-                    time.sleep(0.1)
+                    #time.sleep(0.1)
-    def __init__(self, sock, client, args, logger=logging):
+    def __init__(self, sock, client, args, logger=None):
-        self.logger = logger
+        if logger:
-    def __init__(self, address, handler, args=(), use_https=False, cert="", logger=logging):
+    def __init__(self, address, handler, args=(), use_https=False, cert="", logger=xlog):
-                events = p.poll(timeout=1)
+                try:
-    while len(queue_list):
+    while True:
-                q.check()
+                wait_count += q.check()
-                # print(c)
+                # print(c, id(q))
-    timer_th = None
+    with th_lock:
-        timer_th.start()
+
-                return
+        if not self.waiters:
-                self.notify()
+        return 1
-    config.set_var("pip_cache_size", 16*1024)
+    config.set_var("pip_cache_size", 32*1024)
-def remote_query_dns(domain, type):
+def remote_query_dns(domain, type=None):
-    def send_request(self, id, domain):
+    def send_request(self, id, domain, server):
-            self.send_request(id, domain)
+        for i in xrange(0, len(self.dns_server.server_list)):
-            que.wait(time.time() + 0.5)
+            que.wait(time.time() + 1)
-                self.dns_server.next_server()
+                continue
-            xlog.warn("Or run XX-Net as root")
+
-        if g.user_rules.check_host(domain, 0) == "direct" or \
+        rule = g.user_rules.check_host(domain, 0)
-        os._exit(0)
+    r = remote_query_dns("apple.com")
-                   s1==remote_sock, left1, e)
+                   s1==remote_sock, s1.buf_size, e)
-        if s2 in self.send_buf and self.send_buf[s2].size:
+        if s2.buf_size:
-            self.send_buf[s2].add("")
+                       s1, e, s2, s2.buf_size)
-                            self.send_buf[s2].add(d2)
+                            s2.add_dat(d2)
-                        self.send_buf[s2].add(d)
+                        s2.add_dat(d)
-                    if self.send_buf[s2].size > self.buf_size:
+                    if s2.buf_size > self.buf_size:
-                    if s1 not in self.send_buf or self.send_buf[s1].num == 0:
+                    if s1.buf_num == 0:
-                    dat = self.send_buf[s1].get()
+                    dat = s1.get_dat()
-                        self.send_buf[s1].restore(dat[sended:])
+                        s1.restore_dat(dat[sended:])
-                    if self.send_buf[s1].size == 0:
+                    if s1.buf_size == 0:
-                    if self.send_buf[s1].size < self.buf_size:
+                    if s1.buf_size < self.buf_size:
-import heroku_front
+import front_dispatcher
-    for front in all_fronts:
+    for front in front_dispatcher.all_fronts:
-        for front in all_fronts:
+        for front in front_dispatcher.session_fronts:
-
+    front_dispatcher.init()
-    start()
+    g.running = True
-            raise
+            raise Exception()
-        return worker.get_score()
+        return worker.get_score() * 10
-# all_fronts = [direct_front]
+all_fronts = []
-current_front = running_front_list.pop(0)
+
-        fronts = dns_fronts
+    if host in ["dns.xx-net.net", g.config.api_server]:
-
+    g.stat["timeout_roundtrip"] += 5
-            xlog.warn("front retry %s", path)
+            xlog.warn("front retry %s%s", host, path)
-xlog = getLogger("heroku_front")
+from front import front
-    check_ip.load_proxy_config()
+def set_proxy(args):
-import struct
+
-import time
+import os
-        ssl_sock.send(request_data.encode())
+sys.path.append(root_path)
-        response.begin(timeout=5)
+noarch_lib = os.path.abspath( os.path.join(python_path, 'lib', 'noarch'))
-            return ssl_sock
+if sys.platform == "win32":
-        return ssl_sock
+import xlog
-    return ssl_sock
+from front_base.openssl_wrap import SSLContext
-        return check_xtunnel_http2(ssl_sock, host)
+from config import Config
-        ip = "107.21.125.200"
+        ip = "207.246.89.177"
-    xlog.info("test ip:%s", ip)
+    print("test ip:%s" % ip)
-    res = test_xtunnel_ip2(ip, top_domain=top_domain, wait_time=wait_time)
+    config_path = os.path.join(module_data_path, "tls_relay.json")
-        print("success, domain:%s handshake:%d" % (res.domain, res.handshake_time))
+    elif res.ok:
-# coding:utf-8
+from front_base.config import ConfigBase
-import io
+class Config(ConfigBase):
-xlog = getLogger("heroku_front")
+        # front
-    current_path = os.path.dirname(os.path.abspath(__file__))
+        # connect_manager
-        self.CONFIG_FILENAME = os.path.abspath( os.path.join(current_path, 'default_config.ini'))
+        # check_ip
-            self.DATA_PATH = current_path
+        # host_manager
-config.load()
+        # ip_manager
-            self.getter_num -= 1
+import os
-import random
+import struct
-xlog.set_buffer(500)
+import xlog
-    return zlib.decompress(data, -zlib.MAX_WBITS)
+from config import Config
-    return zlib.compress(data)[2:-4]
+current_path = os.path.dirname(os.path.abspath(__file__))
-        self.host = str(random.choice(self.hosts))
+        self.logger = logger
-        self.continue_fail_num = 0
+        self.continue_fail_num = 0
-        while True:
+        while self.running:
-            time.sleep(0.01)
+            time.sleep(1)
-                self.continue_fail_num > 10:
+        if not self.host_manager.appids:
-        if len(self.hosts) == 0:
+        now = time.time()
-        worker = dispatcher.get_worker(nowait=True)
+        worker = self.http_dispatcher.get_worker(nowait=True)
-        timeout = 40
+        return worker.get_score()
-            response = self.dispatcher.request(method, host, path, header, data, timeout=timeout)
+            response = self.http_dispatcher.request(method, host, path, dict(headers), data, timeout=timeout)
-        heroku_host = str(random.choice(self.hosts))
+        heroku_host = ""
-        #xlog.info('%s "PHP %s %s %s" %s %s', handler.address_string(), handler.command, url, handler.protocol_version, response.status, response.getheader('Content-Length', '-'))
+        # xlog.info('%s "PHP %s %s %s" %s %s', handler.address_string(), handler.command, url, handler.protocol_version, response.status, response.getheader('Content-Length', '-'))
-                    self.hosts.remove(heroku_host)
+                    self.host_manager.remove(heroku_host)
-        connect_control.keep_running = False
+        logger.info("terminate")
-front = Front()
+front = Front()
-    python_path = os.path.abspath( os.path.join(root_path, 'python27', '1.0'))
+current_path = os.path.dirname(os.path.abspath(__file__))
-    sys.path.append(noarch_lib)
+if sys.platform == "win32":
-    xlog.info("Exiting heroku_front module...")
+from gae_proxy.local import check_local_network
-            self.timer_th.start()
+            threading.Thread(target=self.timer).start()
-            #xlog.debug("end join roundtrip_thread")
+    def reporter(self):
-        elif update_server:
+        elif update_server or not g.server_host:
-
+
-        }
+        res = g.session.get_stat()
-                    pass
+            try:
-        self.PUBLIC_APPIDS = [x.strip() for x in self.CONFIG.get('gae', 'public_appid').split("|")]
+        def appids_init(appids):
-            self.GAE_APPIDS = [x.strip() for x in self.CONFIG.get('gae', 'appid').split("|")]
+            self.GAE_APPIDS = appids_init(self.CONFIG.get('gae', 'appid'))
-    config.set_var("server_port", 0)
+    config.set_var("server_port", 443)
-        if g.config.server_host and g.config.server_port:
+        if g.config.server_host and g.config.server_port == 443:
-            if g.selectable and server == 'auto':
+            if g.selectable and server == '':
-                'selected': g.config.server_host, # "auto" as default
+                'selected': 'auto' if g.config.server_host == '' else g.config.server_host,  # "auto" as default
-                    g.config.server_host = server
+                    g.server_host = g.config.server_host = server
-    return user_config.user_special.fake_host
+#def get_fake_host():
-                   }
+            "sys_platform": "%s, %s" % (platform.machine(), platform.platform()),
-top_path = os.path.abspath( os.path.join(root_path, os.pardir, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir))
-        _winreg.KEY_ALL_ACCESS)
+        return _winreg.OpenKey(_registry, r"Software\Microsoft\Windows\CurrentVersion\Run", 0, _winreg.KEY_ALL_ACCESS)
-        except : #WindowsError
+        except:  # WindowsError
-            "X-GNOME-Autostart-enabled=true" % (name, application)
+        desktop_entry = "[Desktop Entry]\n" \
-        if(exists(name)):
+        if (exists(name)):
-            
+
-        if(os.path.isfile(plist_file_path)):
+        if (os.path.isfile(plist_file_path)):
-proc_handler = {}
+proc_handler = {}
-root_path = os.path.abspath( os.path.join(current_path, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir))
-root_path = os.path.abspath( os.path.join(current_path, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir))
-        v2 = map(int, map(m2.group, [1,2,3]))
+        v1 = map(int, map(m1.group, [1, 2, 3]))
-        return False # is not older
+        xlog.warn("older_or_equal fail: %s, %s" % (version, reference_version))  # e.g. "get_version_fail" when post_update.run(last_run_version), "last_run_version" in \data\launcher\config.yaml
-        shutil.rmtree(os.path.join(top_path, 'launcher')) # launcher is for auto-update from 2.X
+        shutil.rmtree(os.path.join(top_path, 'launcher'))  # launcher is for auto-update from 2.X
-                                             'data', 'downloads'))
+download_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, 'data', 'downloads'))
-
+class SimpleI18N:
-    web_control.confirm_xxnet_exit()
+    web_control.confirm_xxnet_not_running()
-        config.save()
+    import post_update
-    
+    update_from_github.cleanup()
-    reduce         # Python 2 
+    reduce         # Python 2
-root_path = os.path.abspath( os.path.join(current_path, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir))
-
+
-    new_module_version_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, module, new_version))
+    new_module_version_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, module, new_version))
-    #check path exist
+    # check path exist
-    #call setup.py
+    # call setup.py
-
+
-    download_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'data', 'downloads'))
+    download_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, 'data', 'downloads'))
-            module_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, module))
+            module_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, module))
-            msg = "Module %s new version %s downloaded, Install?" % (module,  new_version)
+            msg = "Module %s new version %s downloaded, Install?" % (module, new_version)
-                           2: {"data":data_ignore, "label":"Ignore", 'callback':general_gtk_callback}}
+                buttons = {1: {"data": data_install, "label": "Install", 'callback': general_gtk_callback},
-            + "&platform=" + platform.platform()
+                  + "&version=" + update_from_github.current_version() \
-                    msg = "Module %s new version: %s, Download?\nNew:%s" % (module,  new_version, describe)
+                    msg = "Module %s new version: %s, Download?\nNew:%s" % (module, new_version, describe)
-                               2: {"data":data_ignore, "label":"Ignore", 'callback':general_gtk_callback}}
+                    buttons = {1: {"data": data_download, "label": "Download", 'callback': general_gtk_callback},
-                    msg = "Module %s new version: %s, Download?" % (module,  new_version)
+                    msg = "Module %s new version: %s, Download?" % (module, new_version)
-                    msg = "Module %s new version: %s, Download?" % (module,  new_version)
+                    msg = "Module %s new version: %s, Download?" % (module, new_version)
-        if os.getenv("DESKTOP_SESSION","unknown") != "unknown" :  # make sure this is desktop linux
+        if os.getenv("DESKTOP_SESSION", "unknown") != "unknown":  # make sure this is desktop linux
-            cmd='env XXNETPATH="' + xxnet_path + '" "' + work_path + '/create_shortcut_linux.sh"'
+            cmd = 'env XXNETPATH="' + xxnet_path + '" "' + work_path + '/create_shortcut_linux.sh"'
-    #update need gae_proxy as proxy
+    # wait gae_proxy to start
-root_path = os.path.abspath( os.path.join(current_path, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir))
-progress = {} # link => {"size", 'downloaded', status:downloading|canceled|finished:failed}
+progress = {}  # link => {"size", 'downloaded', status:downloading|canceled|finished:failed}
-    progress["update_status"] = "Over writing"
+    progress["update_status"] = "Overwriting"
-            relate_path = root[len(xxnet_unzip_path)+1:]
+            relate_path = root[len(xxnet_unzip_path) + 1:]
-                    if sys.platform != 'win32' and os.path.isfile("/system/bin/dalvikvm")==False and os.path.isfile("/system/bin/dalvikvm64")==False and os.path.isfile(dst_file):
+                    # modify by outofmemo, files in '/sdcard' are not allowed to chmod for Android
-        progress["update_status"] = "Over write Fail:%r" % e
+        xlog.warn("update overwrite fail:%r", e)
-def download_overwrite_new_version(xxnet_version,checkhash=1):
+def download_overwrite_new_version(xxnet_version, checkhash=1):
-        if os.path.isdir(os.path.join(code_path, name)) :
+        if os.path.isdir(os.path.join(code_path, name)):
-    local_versions.sort(key= lambda s: map(int, s[0].split('.')) , reverse=True)
+            if v:
-        shutil.rmtree( os.path.join(top_path, "code", version) )
+        shutil.rmtree(os.path.join(top_path, "code", version))
-def update_version(version,checkhash=1):
+def update_version(version, checkhash=1):
-        download_overwrite_new_version(version,checkhash)
+        download_overwrite_new_version(version, checkhash)
-    th = threading.Thread(target=update_version, args=(version,checkhash))
+    th = threading.Thread(target=update_version, args=(version, checkhash))
-def delete_to_save_disk():
+
-            
+
-                        os.remove(pt) 
+                        os.remove(pt)
-    
+
-    if keep_old_num < 99 and keep_old_num >=0 :  # 99 means don't delete any old version
+    if keep_old_num < 99 and keep_old_num >= 0:  # 99 means don't delete any old version
-                    del_paths.append( "code/" + local_vs[u][1] + "/" )
+                for u in range(i + keep_old_num + 1, len(local_vs)):
-        if del_paths :
+        if del_paths:
-    
+
-    if config.get(["savedisk", "clear_cache"], 0) :
+    if config.get(["savedisk", "clear_cache"], 0):
-    if config.get(["savedisk", "del_win"], 0) :
+
-            "code/*/python27/1.0/Microsoft.VC90.CRT.manifest", 
+            "code/*/python27/1.0/WinSxS/",
-            "code/*/python27/1.0/lib/darwin/" 
+    if config.get(["savedisk", "del_mac"], 0):
-            "code/*/python27/1.0/lib/linux/" 
+    if config.get(["savedisk", "del_linux"], 0):
-            "code/*/gae_proxy/" 
+    if config.get(["savedisk", "del_gae"], 0):
-            "code/*/gae_proxy/server/" 
+    if config.get(["savedisk", "del_gae_server"], 0):
-            "code/*/x_tunnel/" 
+    if config.get(["savedisk", "del_xtunnel"], 0):
-            "code/*/smart_router/" 
+    if config.get(["savedisk", "del_smartroute"], 0):
-        
+
-    noarch_lib = os.path.abspath( os.path.join(python_path, 'lib', 'noarch'))
+    python_path = os.path.abspath(os.path.join(current_path, os.pardir, 'python27', '1.0'))
-            if module != "launcher" and config.get(["modules", module, "auto_start"], 0) != 1: # skip php_proxy module
+            if module != "launcher" and config.get(["modules", module, "auto_start"], 0) != 1:  # skip php_proxy module
-            menu_path = os.path.join(root_path, module, "web_ui", "menu.yaml") # launcher & gae_proxy modules
+            menu_path = os.path.join(root_path, module, "web_ui", "menu.yaml")  # launcher & gae_proxy modules
-                              key=lambda k_and_v: (k_and_v[1]['menu_sort_id']))
+        module_menus = sorted(new_module_menus.iteritems(), key=lambda k_and_v: (k_and_v[1]['menu_sort_id']))
-            xlog.warn('%s %s %s haking', self.address_string(), self.command, self.path )
+            xlog.warn('%s %s %s haking', self.address_string(), self.command, self.path)
-                target_menu = 'config'
+            # elif config.get(['modules', 'smart_router', 'auto_start'], 0) == 1:
-        for module,v in module_menus:
+        for module, v in module_menus:
-                "check_update": check_update,
+                "check_update": config.get(["update", "check_update"], "notice-stable"),
-                "keep_old_ver_num": config.get(["modules", "launcher", "keep_old_ver_num"], -1) # -1 means not set yet
+                "keep_old_ver_num": config.get(["modules", "launcher", "keep_old_ver_num"], -1),  # -1 means not set yet
-                    
+
-            elif 'gae_proxy_enable' in reqs :
+            elif 'gae_proxy_enable' in reqs:
-            elif 'x_tunnel_enable' in reqs :
+            elif 'x_tunnel_enable' in reqs:
-            elif 'smart_router_enable' in reqs :
+            elif 'smart_router_enable' in reqs:
-            
+
-            data = '[  %s  ]' %(s)
+                s += ' { "v":"%s" , "folder":"%s" } ' % (v[0], v[1])
-            if update_from_github.del_version( reqs['version'][0] ):
+            if update_from_github.del_version(reqs['version'][0]):
-            
+
-
+
-    """
+def confirm_xxnet_not_running():
-        host_port = config.get(["modules", "launcher", "control_port"], 8085)
+        host_port = config.get(["modules", "launcher", "control_port"], 8085)  # web_control(default port:8085)
-        xlog.error("confirm_module_ready with port 0")
+        xlog.error("confirm_module_ready with port: 0")
-    noarch_lib = os.path.abspath( os.path.join(python_path, 'lib', 'noarch'))
+    python_path = os.path.abspath(os.path.join(current_path, os.pardir, 'python27', '1.0'))
-    win32_lib = os.path.abspath( os.path.join(python_path, 'lib', 'win32'))
+    win32_lib = os.path.abspath(os.path.join(python_path, 'lib', 'win32'))
-            self.make_menu(), self.on_quit, left_click=self.on_show, right_click=self.on_right_click)
+        self.systray = SysTrayIcon(icon_path, "XX-Net", self.make_menu(), self.on_quit, left_click=self.on_show, right_click=self.on_right_click)
-        
+
-        disable_checked = win32_adapter.fState.MFS_CHECKED if proxy_stat=="disable" else 0
+        gae_proxy_checked = win32_adapter.fState.MFS_CHECKED if proxy_stat == "gae" else 0
-                        (u'éåº', None, SysTrayIcon.QUIT, False)]
+                (u"å¨å±PACæºè½ä»£ç", None, self.on_enable_pac, pac_checked),
-    if content_range:
+    if content_range and 'bytes */' not in content_range:
-        return record
+        with self.lock:
-                self.cache.popitem(last=False)
+        with self.lock:
-        self.last_update_time = time.time()
+            self.cache[domain] = record
-                    continue
+        with self.lock:
-                        xlog.warn("rule line:%s fail", line)
+                    try:
-                self.cache[domain] = record
+                    self.cache[domain] = record
-        return record
+        with self.lock:
-                self.cache.popitem(last=False)
+        with self.lock:
-        self.last_update_time = time.time()
+            self.cache[ip] = record
-                    continue
+        with self.lock:
-                    continue
+                    lp = line.split()
-                self.cache[ip] = {"r": rule, "c": connect_time, "update": update_time}
+                    ip = lp[0]
-        self.__class__.handle_num += 1
+    def try_redirect(self):
-                return handle_ip_proxy(self.conn, ip_str, dst_port, self.client_address)
+                handle_ip_proxy(self.conn, ip_str, dst_port, self.client_address)
-    config.set_var("roundtrip_timeout", 25)
+    config.set_var("roundtrip_timeout", 15)
-                    except:
+                    except Exception as e:
-        self.task_queue = Queue.Queue()
+        self.task_queue = simple_queue.Queue()
-            task = self.task_queue.get(True)
+            task = self.task_queue.get(99999999)
-import Queue
+import simple_queue
-        self.body_queue = Queue.Queue()
+        self.body_queue = simple_queue.Queue()
-                data = self.body_queue.get(block=True)
+                data = self.body_queue.get(self.timeout)
-                data = self.body_queue.get(block=True)
+                data = self.body_queue.get(self.timeout)
-            data = self.body_queue.get(block=True)
+            data = self.body_queue.get(self.timeout)
-
+socks_num = 0
-        self.fake_host = sni_generater.get()
+        self.fake_host = ""
-                self.save()
+            #try:
-        data = ""
+        data = "ssl_socket num:%d \n" % openssl_wrap.socks_num
-            raise
+            raise Exception()
-    xlog.info("This is Android or IOS.")
+if "arm" in platform.machine() or "mips" in platform.machine() or "aarch64" in platform.machine():
-        webbrowser.open("http://127.0.0.1:%s/" % host_port)
+        webbrowser.open("http://localhost:%s/" % host_port)
-import threading
+        global mem_stat
-            dat = mem_top(limit=50, width=150, sep='\n')
+            import tracemalloc
-        except:
+        except Exception as e:
-        self.raw_requestline = self.rfile.readline(65537)
+        try:
-
+            if time.time() > end_time:
-import proxy_handler
+from . import proxy_handler
-    g.pipe_socks = pipe_socks.PipeSocks()
+    g.pipe_socks = pipe_socks.PipeSocks(g.config.pip_cache_size)
-            ips = g.dns_client.query(domain, timeout=0.5)
+        if ips:
-        if not ips:
+        if not ips or not self.in_country(ips):
-        self.gfwlist = self.load()
+        self.gfw_black_list = self.load("gfw_black_list.txt")
-        user_file = os.path.join(data_path, "gfw_black_list.txt")
+    def load(self, name):
-            list_file = os.path.join(current_path, "gfw_black_list.txt")
+            list_file = os.path.join(current_path, name)
-        xlog.info("Load GFW black list file:%s", list_file)
+        xlog.info("Load file:%s", list_file)
-        if not host.endswith(self.gfwlist):
+        if host.endswith(self.gfw_white_list):
-            if h in self.gfwlist:
+            if h in self.gfw_black_list:
-        return s
+        black = '",\n"'.join(self.gfw_black_list)
-        keeprange = ('0.0.0.0/8',  # æ¬å°ç½ç»
+        keeprange = (
-        content = content.replace("BLACK_LIST", g.gfwlist.get_pac_string())
+        black, white = g.gfwlist.get_pac_string()
-    def __init__(self, buf_size=256*1024):
+    def __init__(self, buf_size=16*1024):
-        s1.close()
+            left1 = self.send_buf[s1].size
-        xlog.debug("pipe close %s->%s run_time:%d upload:%d,%d download:%d,%d, by remote:%d, e:%r",
+        xlog.debug("pipe close %s->%s run_time:%d upload:%d,%d download:%d,%d, by remote:%d, left:%d e:%r",
-                   s1==remote_sock, e)
+                   s1==remote_sock, left1, e)
-                ((s1 == local_sock and create_time > 10) or (s1 == remote_sock)):
+        if local_sock.recved_data > 0 and local_sock.recved_times == 1 and remote_sock.port == 443 and \
-            g.domain_cache.update_rule(host, 443, "gae")
+            xlog.debug("SNI:%s fail.", host)
-                    elif self.send_buf[s1].size < self.buf_size:
+
-                        if s2 not in self.read_set:
+                        if s2 not in self.read_set and s2 in self.sock_dict:
-            pass
+        except Exception as e:
-                return handle_domain_proxy(sock, host, port, client_address)
+                if host:
-        if rule == "gae":
+        if rule == "gae" or not g.ip_region.check_ips(record["ip"]):
-        rule_list = ["direct", "gae", "socks", "redirect_https"]
+        ips = g.dns_srv.query(host)
-            return self.send_response('text/html', data)
+        elif path == "/status":
-            return self.response_json({"res": "success"})
+            return self.response_json({"res": "success"})
-    config.set_var("min_on_road", 5)
+    config.set_var("min_on_road", 2)
-    config.set_var("roundtrip_timeout", 15)
+    config.set_var("roundtrip_timeout", 25)
-                if self.getter_num == 0 or self.new_conn_pool.qsize() > self.connection_pool_min:
+                if self.new_conn_pool.qsize() > self.connection_pool_min:
-
+            ip_manager.update_ip(ip, ssl_sock.handshake_time)
-import Queue
+import simple_queue
-        self.body_queue = Queue.Queue()
+        self.body_queue = simple_queue.Queue()
-                data = self.body_queue.get(block=True)
+                data = self.body_queue.get(self.timeout)
-                data = self.body_queue.get(block=True)
+                data = self.body_queue.get(self.timeout)
-            data = self.body_queue.get(block=True)
+            data = self.body_queue.get(self.timeout)
-                rtt = 1000
+        if inactive_time > 30 and rtt > 1000:
-            rtt += len(self.streams) * 1
+        if self.version != "1.1":
-            score = rtt + 500
+        if inactive_time < 1:
-            score = rtt + (5/inactive_time)*100
+            score = rtt + (240 - inactive_time)*10
-                # xlog.debug("trigger get more worker")
+            if best_worker is None or idle_num < 1 or (now - best_worker.last_active_time) < min_idle_time or best_score>20000:
-            if best_worker and (now - best_worker.last_active_time) > 1:
+            if best_worker and (now - best_worker.last_active_time) > min_idle_time:
-                down_fail_connect_interval = 600
+                down_fail_connect_interval = 120
-all_fronts = [gae_front, cloudflare_front]
+all_fronts = [gae_front, cloudflare_front, heroku_front]
-        fronts = all_fronts
+        fronts = session_fronts
-                if self.new_conn_pool.qsize() > self.connection_pool_min:
+                #if self.new_conn_pool.qsize() > self.connection_pool_min:
-                    continue
+        try:
-
+                    if time.time() - start_time > max_timeout:
-        self.task_queue = Queue.Queue()
+        self.task_queue = simple_queue.Queue()
-            task = self.task_queue.get(True)
+            task = self.task_queue.get(99999999)
-import Queue
+import simple_queue
-        self.body_queue = Queue.Queue()
+        self.body_queue = simple_queue.Queue()
-                data = self.body_queue.get(block=True)
+                data = self.body_queue.get(self.timeout)
-                data = self.body_queue.get(block=True)
+                data = self.body_queue.get(self.timeout)
-            data = self.body_queue.get(block=True)
+            data = self.body_queue.get(self.timeout)
-            except:
+            except Exception as e:
-                time.sleep(0.2)
+                time.sleep(0.01)
-            for i in self.roundtrip_thread:
+            #for i in self.roundtrip_thread:
-                    if rthead is threading.current_thread():
+                #try:
-                    pass
+                        #continue
-            threading.Thread(target=proxy_session.login_process).start()
+            threading.Thread(target=proxy_session.request_balance, args=(None,None,False,False)).start()
-
+import check_local_network
-        touch_active()
+        if self.path != "%s:443" % self.fake_host:
-        if self.path == "http://www.twitter.com/xxnet":
+        if host == self.fake_host:
-        if self.path != "www.twitter.com:443":
+        if self.path != "%s:443" % self.fake_host:
-        if self.path == "https://www.twitter.com/xxnet":
+        if self.path == "https://%s/xxnet" % self.fake_host:
-                self.LISTEN_IP = self.USER_CONFIG.get('listen', 'ip')
+                self.user_special.LISTEN_IP = self.USER_CONFIG.get('listen', 'ip')
-            if self.LISTEN_IP != "127.0.0.1":
+            if self.user_special.LISTEN_IP != "127.0.0.1":
-                f.write("ip = %s\n\n" % self.LISTEN_IP)
+                f.write("ip = %s\n\n" % self.user_special.LISTEN_IP)
-            xlog.warn("launcher.config save user config fail:%s %r", CONFIG_USER_FILENAME, e)
+            xlog.exception("launcher.config save user config fail:%s %r", CONFIG_USER_FILENAME, e)
-            self.send_response("text/html", "no pympler")
+            self.send_response("text/html", "no mem_top")
-            self.notify()
+        self.notify_all()
-    g.ip_cache.save()
+    g.domain_cache.save(True)
-            xlog.warn("bind DNS %s:%d fail", bind_ip, port)
+class DnsServerList(object):
-        return False
+    def get(self):
-        start_time = time.time()
+    def next_server(self):
-            return []
+        xlog.debug("next dns server:%s", self.get())
-        req4_pack = d4.pack()
+    def get_local_server(self):
-                        continue
+class DnsClient(object):
-                    break
+            except Exception as e:
-            sock.close()
+            xlog.warn("request dns except:%r", e)
-            ip_list.append(ip + "|XX")
+        self.serverSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
-        return ip_list
+    def in_country(self, ips):
-            g.domain_cache.set_ips(domain, ips, type)
+                g.domain_cache.set_ips(domain, ips, type)
-            # xlog.debug("DNS query:%s type:%d from %s", domain, type, addr)
+            xlog.debug("DNS query:%s type:%d from %s", domain, type, addr)
-    def get_ips(self, domain, type):
+    def get_ips(self, domain, type=None):
-    def get_ordered_ips(self, domain, type):
+    def get_ordered_ips(self, domain, type=None):
-        ordered_ips = [ip for ip, rate in ip_time]
+
-    def set_ips(self, domain, ips, type, rule="direct"):
+    def set_ips(self, domain, ips, type=None, rule="direct"):
-        req_d = req_d.replace(url, path)
+        #req_d = self.conn.recv(len(req_line))
-        handle_domain_proxy(sock, host, port, self.client_address, req_d)
+        handle_domain_proxy(sock, host, port, self.client_address)
-    leadbyte = sock.recv(1, socket.MSG_PEEK)
+def get_sni(sock, left_buf=""):
-                leaddata = sock.recv(1024, socket.MSG_PEEK)
+                leaddata = left_buf + sock.recv(1024, socket.MSG_PEEK)
-        if not leaddata:
+        leaddata = left_buf + sock.recv(1024, socket.MSG_PEEK)
-    req.do_AGENT()
+    req.do_METHOD()
-                if not is_gae_workable() and host != "www.twitter.com":
+                if not is_gae_workable() and host != fake_host:
-                    do_gae(sock, host, port, client_address, left_buf)
+                    sni_host = get_sni(sock, left_buf)
-                    xlog.warn("%s host:%s:%d try gae, GetReqTimeout:%d", scense, host, port,
+                    xlog.debug("%s host:%s:%d try gae, GetReqTimeout:%d", scense, host, port,
-                    xlog.warn("%s host:%s:%d cache rule:%s except:%r", scense, host, port, rule, e)
+                    xlog.exception("%s host:%s:%d rule:%s except:%r", scense, host, port, rule, e)
-                do_socks(sock, host, port, client_address, left_buf)
+                do_socks(sock, host, port, client_address, left_buf)
-        rule_list = ["socks"]
+    if g.config.auto_direct and g.ip_region.check_ip(ip):
-        rule_list = ["direct", "socks"]
+        if g.config.auto_direct or g.config.auto_gae:
-        if host == "www.twitter.com":
+        if host == fake_host:
-            g.ip_cache.save()
+            g.domain_cache.save(True)
-
+import check_local_network
-                if self.new_conn_pool.qsize() > self.connection_pool_min:
+                #if self.new_conn_pool.qsize() > self.connection_pool_min:
-                    time.sleep(60)
+                    time.sleep(10)
-                    continue
+        try:
-                    return None
+                    if time.time() - start_time > max_timeout:
-    def close(self, reason=""):
+    def close(self, reason="conn close"):
-
+    def close(self, reason="close"):
-            self.task.put_data("")
+            self.task.finish()
-            rtt += len(self.streams) * 100
+            rtt += len(self.streams) * 1
-            score = rtt + 50000
+            score = rtt + 500
-            score = rtt + (5/inactive_time)*1000
+            score = rtt + (5/inactive_time)*100
-        self.wait_a_worker_cv = SimpleCondition()
+        self.wait_a_worker_cv = simple_queue.Queue()
-            if best_worker is None or idle_num < 5 or (now - best_worker.last_active_time) < 2 or best_score>1000:
+            if best_worker is None or idle_num < 1: # or (now - best_worker.last_active_time) < 2 or best_score>1000:
-            self.wait_a_worker_cv.wait()
+            self.triger_create_worker_cv.notify()
-        q = Queue.Queue()
+        q = simple_queue.Queue()
-        response = q.get(True)
+        response = q.get(timeout=timeout)
-            task.put_data("")
+            task.finish()
-import utils
+import check_local_network
-                    time.sleep(60)
+                    time.sleep(10)
-            self.lock.acquire()
+        with self.lock:
-        xlog.debug("session stopped.")
+        with self.lock:
-                    continue
+                    g.server_host = None
-                return None
+            return None
-                                                                  path="/data", data=upload_post_data,
+                                                                  path="/data?tid=%d" % transfer_no,
-                self.reset()
+                g.server_host = None
-
+def login_process():
-                return None
+            return g.session.start()
-                last_login_center_time = time.time()
+def create_conn(sock, host, port):
-            g.session.start()
+    update_from_github.delete_to_save_disk()
-                "no_mess_system": config.get(["no_mess_system"], 0)
+                "no_mess_system": config.get(["no_mess_system"], 0),
-
+            elif 'keep_old_ver_num' in reqs:
-code_path = os.path.join(root_path, os.pardir)
+code_path = os.path.abspath(os.path.join(root_path, os.pardir))
-            xlog.info("use DNS server:%s", ip)
+            xlog.debug("use local DNS server:%s", ip)
-gae_proxy_listen = "PROXY_LISTEN"
+
-    return serving_pacfile
+
-        self.send_response('text/plain', data)
+
-                r, w, e = select.select(self.read_set, self.write_set, self.error_set, 1)
+                r, w, e = select.select(self.read_set, self.write_set, self.error_set, 0.1)
-                path = url[7+p:]
+            url_prex_len = url[7:].find("/")
-        # xlog.debug("http %r connect to %s:%d", self.client_address, host, port)
+        req_d = self.conn.recv(len(req_line))
-            return
+        sock = SocketWrap(self.conn, self.client_address[0], self.client_address[1])
-        handle_domain_proxy(self.conn, host, port, self.client_address, new_req_line)
+        xlog.debug("http %r connect to %s:%d %s %s", self.client_address, host, port, method, path)
-        return
+        raise XTunnelNotRunning()
-        return
+        raise XTunnelNotRunning()
-        return
+        raise XTunnelNotRunning()
-    sock = SocketWrap(sock, client_address[0], client_address[1])
+    if not isinstance(sock, SocketWrap):
-            return
+        return try_loop("ip user", [rule], sock, ip, port, client_address)
-        rule = record["r"]
+    if record and record["r"] == "socks":
-        rule = "direct"
+        rule_list = ["direct", "socks"]
-    return
+    try_loop("ip", rule_list, sock, ip, port, client_address)
-            return
+        return try_loop("domain user", [rule], sock, host, port, client_address, left_buf)
-    else:
+    if record:
-                rule = "gae"
+        if rule == "gae":
-            rule = "gae"
+            rule_list = ["direct", "gae", "socks", "redirect_https"]
-        return
+        if not g.domain_cache.accept_gae(host):
-        return
+        rule_list = ["direct", "gae", "socks", "redirect_https"]
-        if now - self.last_fail_time < 5*60 and \
+        if now - self.last_fail_time < 60 and \
-    code_path = os.path.join(root_path, os.pardir)
+    
-            local_versions.append(name)
+        if os.path.isdir(os.path.join(code_path, name)) :
-                s += ' { "v":"%s" } ' % (v)
+                s += ' { "v":"%s" , "folder":"%s" } ' % (v[0],v[1])
-                "gae_proxy_enable": config.get(["modules", "gae_proxy", "show_detail"], 0),
+                "gae_proxy_enable": config.get(["modules", "gae_proxy", "auto_start"], 0),
-        self.last_active_time = self.ssl_sock.create_time
+        self.last_active_time = self.ssl_sock.create_time - 1
-    check_ip.load_proxy_config()
+    check_ip.load_proxy_config()
-            xlog.warning('please install *libnss3-tools* package to import GoAgent root ca')
+            xlog.warn('please install *libnss3-tools* package to import GoAgent root ca')
-            xlog.warning('please install *libnss3-tools* package to import GoAgent root ca')
+            xlog.info('please install *libnss3-tools* package to import GoAgent root ca')
-    if config.PROXY_ENABLE:
+    if int(config.PROXY_ENABLE):
-    if config.PROXY_ENABLE:
+    if int(config.PROXY_ENABLE):
-            br_sites.append(k)
+            if k.startswith("."):
-        xlog.debug("accept_br_encoding for %s", url)
+        # xlog.debug("accept_br_encoding for %s", url)
-            accept_codes.remove("gzip")
+        if "gzip" in accept_encoding:
-    return reduce(lambda a, b: a << 8 | b, map(int, s.split(".")))
+from utils import *
-        if check_ip_valid(ip) and (0 <= int(bits) <= 32):
+        if check_ip_valid4(ip) and (0 <= int(bits) <= 32):
-def main():
+def main(args):
-    proxy_daemon = simple_http_server.HTTPServer((config.LISTEN_IP, config.LISTEN_PORT), proxy_handler.GAEProxyHandler)
+    allow_remote = args.get("allow_remote", 0)
-        main()
+        main({})
-            self.local_names.append(socket.gethostname().lower());
+            self.local_names.append(socket.gethostname().lower())
-            return
+        self.parse_request()
-                raise
+        self.parse_request()
-
+            try:
-            xlog.error('PAC %s %s %s ', self.address_string(), self.command, self.path)
+            xlog.error('WebUI log from:%s unknown cmd:%s path:%s ', self.address_string(), self.command, self.path)
-def main():
+def main(args):
-        client.main()
+        client.main(args)
-    main()
+    main({})
-        set(["modules", "launcher", "proxy"], "pac")
+        set(["modules", "launcher", "proxy"], "smart_router")
-        if proxyState == 'xtunnel':
+        if proxyState == 'x_tunnel':
-        elif proxyState == 'xtunnel':
+        elif proxyState == 'x_tunnel':
-        config.set(["modules", "launcher", "proxy"], "xtunnel")
+        config.set(["modules", "launcher", "proxy"], "x_tunnel")
-        return "xtunnel"
+        return "x_tunnel"
-    proxy_setting = config.get(["modules", "launcher", "proxy"], "pac")
+    proxy_setting = config.get(["modules", "launcher", "proxy"], "smart_router")
-        elif proxy_setting == "xtunnel":
+        elif proxy_setting == "x_tunnel":
-            p = threading.Thread(target=_start.main)
+            p = threading.Thread(target=_start.main, args=([xargs]))
-    module_init.start_all_auto()
+    allow_remote = 0
-    web_control.start()
+    module_init.start_all_auto()
-                    return
+                    return self.send_not_found()
-                return
+                return controler.do_POST()
-            elif config.get(['modules', 'x_tunnel', 'auto_start'], 0) == 1:
+            if config.get(['modules', 'x_tunnel', 'auto_start'], 0) == 1:
-                    )
+            if module_init.xargs.get("allow_remote", 0):
-                    data = '{"res":"fail, allow_remote_connect:%s"}' % allow_remote_connect
+            elif 'allow_remote_switch' in reqs:
-                    config.set(["modules", "launcher", "allow_remote_connect"], allow_remote_connect)
+                    config.set(["modules", "launcher", "allow_remote_connect"], allow_remote_switch)
-            enable = reqs['enable'][0]
+            enable = int(reqs['enable'][0])
-            port = reqs['port'][0]
+            port = int(reqs['port'][0])
-    global process, server
+    def req_debug_handler(self):
-    allow_remote = config.get(["modules", "launcher", "allow_remote_connect"], 0)
+    if not allow_remote:
-    process.start()
+    server = simple_http_server.HTTPServer((host_addr, host_port), Http_Handler, logger=xlog)
-        return
+def stop():
-        proxy_setting = config.get(["modules", "launcher", "proxy"], "pac")
+        proxy_setting = config.get(["modules", "launcher", "proxy"], "smart_router")
-
+        self.raw_requestline = ""
-                return False
+                raise ParseReqFail("Req command format fail:%s" % requestline)
-                    raise ValueError
+                    raise ParseReqFail("Req command format fail:%s" % requestline)
-                return False
+                raise ParseReqFail("Req command format fail:%s" % requestline)
-                return False
+                raise ParseReqFail("Req command format fail:%s" % requestline)
-                return False
+                raise ParseReqFail("Req command format HTTP/0.9 line:%s" % requestline)
-            return False
+            raise ParseReqFail("Req command format fail:%s" % requestline)
-            return False
+            raise ParseReqFail("Req command format fail:%s" % requestline)
-
+
-import xconfig
+import host_records
-
+
-    g.config = config
+    config.set_var("proxy_bind_ip", "127.0.0.1")
-        return
+    config.set_var("country_code", "CN")
-    global proc_handler, ready, dns_srv, g, redirect_srv
+
-    redirect_srv.start()
+    load_config()
-                                   cache_size=g.config.cache_size, ttl=g.config.ttl)
+    g.user_rules = user_rules.Config()
-    dns_srv.server_forever()
+    g.dns_srv.server_forever()
-    redirect_srv.stop()
+
-
+import global_var as g
-    xlog.info("set_proxy:%s", args)
+    xlog.info("set_proxy:%s", args)
-
+import json
-
+import re
-from dnslib import DNSRecord, DNSHeader, A, AAAA, RR
+import global_var as g
-        ips6 = []
+def remote_query_dns(domain, type):
-            self.cache[domain] = record
+    content, status, response = g.x_tunnel.front_dispatcher.request(
-            pass
+    if status != 200:
-            return
+    try:
-    def __init__(self, bind_ip="127.0.0.1", port=53, query_cb=None, cache_size=200, ttl=24*3600):
+    def __init__(self, bind_ip="127.0.0.1", port=53, ttl=24*3600):
-        self.cache = DnsCache(cache_size, ttl)
+        self.ttl = ttl
-        self.running = False
+    def get_dns_server(self):
-            xlog.warn("no query_cb")
+        ips = {}
-        ips = self.cache.get(domain)
+        d4 = DNSRecord()
-            self.cache.set(domain, ips)
+            if "." in domain:
-            threading.Thread(target=self.on_udp_query, args=(data, addr)).start()
+            r, w, e = select.select(self.sockets, [], [], 1)
-config = None
+
-        self.sock.close()
+import time
-def main():
+def main(args):
-        client.run()
+        client.run(args)
-    main()
+    main({})
-#import heroku_front.apis as heroku_apis
+import global_var as g
-apis = [cloudflare_apis]
+all_fronts = [cloudflare_front, heroku_front]
-    for api in apis:
+    for front in all_fronts:
-            xlog.exception("set_proxy except:%r", e)
+            xlog.exception("set_proxy except:%r", e)
-                self.xlog.info("Conn session:%s conn:%d Peer Close:%s", self.session.session_id, self.conn_id, data.get())
+                self.xlog.debug("Conn session:%s conn:%d Peer Close:%s", self.session.session_id, self.conn_id, data.get())
-def main():
+def main(args):
-    g.socks5_server = simple_http_server.HTTPServer((g.config.socks_host, g.config.socks_port), Socks5Server)
+    allow_remote = args.get("allow_remote", 0)
-        main()
+        main({})
-
+import apis
-    if config.PROXY_ENABLE:
+    if int(config.PROXY_ENABLE):
-    if config.PROXY_ENABLE:
+    if int(config.PROXY_ENABLE):
-        raise e
+        raise socket.error('conn fail, sni:%s, top:%s e:%r' % (sni, top_domain, e))
-        raise socket.error('certficate is none, sni:%s', sni)
+        raise socket.error('certficate is none, sni:%s, top:%s' % (sni, top_domain))
-            time.sleep(0.01)
+            time.sleep(1)
-            self.last_active_time = time.time()
+            # self.last_active_time = time.time()
-#from heroku_front.front import front as heroku_front
+from heroku_front.front import front as heroku_front
-        for front in all_fronts:
+        for front in fronts:
-        time.sleep(0.01)
+        time.sleep(1)
-    if config.PROXY_ENABLE:
+    if int(config.PROXY_ENABLE):
-    if config.PROXY_ENABLE:
+    if int(config.PROXY_ENABLE):
-            if self.new_conn_pool.qsize() < self.connection_pool_min:
+            if not self.new_conn_pool.qsize():
-        self.hosts = []
+        self.hosts = ["xxnet4.herokuapp.com"]
-import threading
+import select
-    buffer_start = 0
+        self.read_buffer = ""
-
+            r, w, e = select.select([self.connection], [], [])
-            xlog.warn('socks handler read error %r', e)
+            xlog.debug('socks handler read error %r', e)
-def main():
+def main(args):
-        client.main()
+        client.main(args)
-    main()
+    main({})
-
+        elif reqs['cmd'] == ['get_localversions']:
-    xxnet_port = get(["modules", "gae_proxy", "LISTEN_PORT"], 8087)
+
-        set(["modules", "launcher", "xxnet_port"], xxnet_port)
+
-        time.sleep(1)
+        time.sleep(1)
-                context = ssl.SSLContext(ssl.PROTOCOL_TLS)
+                context = OpenSSL.SSL.Context(OpenSSL.SSL.TLSv1_METHOD)
-                except ssl.SSLError:
+                    context.use_certificate(cert)
-from ip_utils import check_ip_valid
+from utils import check_ip_valid
-        elif utils.check_ip_valid(host):
+        elif utils.check_ip_valid4(host):
-def check_ip_valid(ip):
+
-        self.log_fd = open(file_name, "w")
+        self.log_fd = open(file_name, "a+")
-            elif utils.check_ip_valid(host):
+            elif utils.check_ip_valid4(host):
-        subj = req.get_subject()
+        ca = OpenSSL.crypto.X509()
-        subj.organizationalUnitName = '%s Root' % CertUtil.ca_vendor
+        # Log generated time.
-        ca.set_pubkey(req.get_pubkey())
+        ca.gmtime_adj_notBefore(- 3600 * 24)
-        cert.set_pubkey(CertUtil.cert_publickey)
+        if CertUtil.cert_publickey:
-        return True
+        if hasattr(OpenSSL.crypto, "X509StoreContext"):
-                os.rename(CertUtil.ca_certfile, CertUtil.ca_keyfile)
+                xlog.info("update CA file storage format")
-                CertUtil.cert_publickey = OpenSSL.crypto.load_publickey(OpenSSL.crypto.FILETYPE_PEM, fp.read())
+        if hasattr(OpenSSL.crypto, "load_publickey"):
-        cert_publickey_str = OpenSSL.crypto.dump_publickey(OpenSSL.crypto.FILETYPE_PEM, CertUtil.cert_publickey)
+            CertUtil.cert_keyfile = None
-            if not CertUtil.verify_certificate(ca, cert) or cert_publickey_str != OpenSSL.crypto.dump_publickey(OpenSSL.crypto.FILETYPE_PEM, cert.get_pubkey()):
+            remove_certs = False
-            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, certfile=certfile, server_side=True)
+            ssl_sock = ssl.wrap_socket(self.connection, keyfile=CertUtil.cert_keyfile, certfile=certfile, server_side=True)
-            ssl_sock = ssl.wrap_socket(sock, keyfile=certfile,
+            ssl_sock = ssl.wrap_socket(sock, keyfile=CertUtil.cert_keyfile,
-                #xlog.warn('app_msg:%s', cgi.escape(response.app_msg))
+                # xlog.warn('app_msg:%s', cgi.escape(response.app_msg))
-                xlog.info('gae_handler send to browser return %r %r', e_b, url)
+                xlog.info('gae_handler send to browser return %r %r, len:%d, sended:%d', e_b, url, body_length, body_sended)
-                response.worker.close("range status:%s", response.status)
+                response.worker.close("range status:%s" % response.status)
-        if config.get(["proxy", "enable"], 0):
+        if int(config.get(["proxy", "enable"], 0)):
-                    reply.add_answer(RR(domain, ttl=60, rdata=AAAA(ip)))
+                    reply.add_answer(RR(domain, rtype=type, ttl=60, rdata=AAAA(ip)))
-        self.running = False
+        self.running = False
-    g.cert = os.path.abspath(os.path.join(data_path, "CA.crt"))
+    xlog.info("xxnet_version:%s", xxnet_version())
-    ssl_sock.do_handshake()
+    try:
-        raise socket.error(' certficate is none')
+        raise socket.error('certficate is none, sni:%s', sni)
-        out_list.append(" sni:%s" % self.ssl_sock.sni)
+        out_list.append(" sni:%s, domain:%s" % (self.ssl_sock.sni, self.ssl_sock.top_domain))
-                                                     data=upload_post_data, timeout=10)
+                                                     data=upload_post_data, timeout=5)
-    ssl_sock.set_tlsext_host_name(sni)
+    if hasattr(ssl_sock, 'set_tlsext_host_name'):
-    else:
+    else:
-    ca_keyfile = os.path.join(data_path, 'CA.crt')
+    ca_certfile = os.path.join(data_path, 'CA.crt')
-
+    def generate_cert_keyfile():
-        subj = req.get_subject()
+    @staticmethod
-            cert.set_serial_number(int(time.time()*1000))
+        subj.commonName = commonname
-            sans = [commonname] + [s for s in sans if s != commonname]
+        cert.set_issuer(CertUtil.ca_subject)
-        cert.sign(key, CertUtil.ca_digest)
+
-            yield '.' + commonname.partition('.')[-1]
+    def _get_old_cert(commonname):
-                        # well, have to use the old one
+    def get_cert(commonname, sans=None, full_name=False):
-                certfile = CertUtil._get_old_cert(commonname, full_name)
+            # some site need full name cert
-                return CertUtil._get_cert(commonname, sans)
+
-            xlog.info("no GAE CA file exist in XX-Net data dir")
+            if os.path.exists(CertUtil.ca_certfile):
-            any(os.remove(x) for x in glob.glob(CertUtil.ca_certdir+'/*.crt')+glob.glob(CertUtil.ca_certdir+'/.*.crt'))
+                xlog.info("clean old site certs in XX-Net cert dir")
-            CertUtil.generate_ca_file()
+                CertUtil.generate_ca_file()
-            CertUtil.ca_thumbprint = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, fp.read()).digest('sha1')
+            content = fp.read()
-        certfiles = glob.glob(CertUtil.ca_certdir+'/*.crt')+glob.glob(CertUtil.ca_certdir+'/.*.crt')
+        # Check exist site cert buffer with CA
-            if serial_number != CertUtil.get_cert_serial_number(commonname):
+                cert = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, fp.read())
-def check_ip_valid(ip):
+def check_ip_valid4(ip):
-            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, certfile=certfile, server_side=True)
+            ssl_sock = ssl.wrap_socket(self.connection, keyfile=CertUtil.cert_keyfile, certfile=certfile, server_side=True)
-                    if not ip_utils.check_ip_valid(ip) and not ip_utils.check_ip_valid6(ip):
+                    if not ip_utils.check_ip_valid(ip):
-    if check_local_network.is_ok(ip):
+    if not check_local_network.is_ok(ip):
-
+import io
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Reload GAEProxy', 'resetGoagent:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Reset Each Module', 'restartEachModule:', '')
-        module_init.start("gae_proxy")
+    def restartEachModule_(self, _):
-        if module == "launcher":
+    for module in config.modules:
-        except:
+        except Exception as e:
-             "no_mess_system": %d }' %\
+             "smart_router_enable": %d, "no_mess_system": %d }' %\
-                        (u"éå¯ GAEProxy", None, self.on_restart_gae_proxy, 0),
+                        (u"éå¯åæ¨¡å", None, self.on_restart_each_module, 0),
-                (u"Reset GAEProxy", None, self.on_restart_gae_proxy, 0),
+                (u"Reset Each module", None, self.on_restart_each_module, 0),
-    def on_restart_gae_proxy(self, widget=None, data=None):
+    def on_restart_each_module(self, widget=None, data=None):
-
+
-        if status in [400, 403]:
+        if status in [400, 403, 405]:
-                data = sock.recv(256)
+
-                data = sock.recv(256)
+
-                data = sock.recv(256)
+
-                data = sock.recv(need)
+
-def redirect_process(sock, host, port, client_address=""):
+def redirect_process(sock, host, port, client_address):
-    xlog.info("redirect connect to %s:%d conn_id:%d", client_address, host, port, conn_id)
+    xlog.info("redirect connect from:%s to %s:%d conn_id:%d", client_address, host, port, conn_id)
-def redirect_handler(sock, host, port, client_address=""):
+def redirect_handler(sock, host, port, client_address):
-        if url.startswith("http://"):
+        if url.startswith("http://") or url.startswith("HTTP://"):
-    threading.Thread(target=redirect_process, args=(sock, host, port, client_address)).start()
+    threading.Thread(target=redirect_process, args=(sock, host, port, client_address)).start()
-    if not url.startswith("http"):
+    if not url.startswith("http") and not url.startswith("HTTP"):
-        xlog.info("check_ip <ip>")
+        xlog.info("check_ip <ip>")
-        self.http_client = simple_http_client.Client(self.proxy, timeout=30)
+        self.http_client = simple_http_client.Client(self.proxy, timeout=10)
-            response = self.http_client.request("HEAD", url, header, "")
+            response = self.http_client.request("HEAD", url, header, "", read_payload=False)
-
+            else:
-IPv6.urls = ["http://[2001:470:1:18::125]", "http://[2001:41d0:8:e8ad::1]", "http://[2001:260:401:372::5f]"]
+IPv6.urls = ["http://[2001:41d0:8:e8ad::1]",
-    print IPv6._test_host("http://[2001:470:1:18::125]")
+    print IPv6._test_host("http://[2804:10:4068::202:82]")
-        fd = open(self.range_file, "r")
+            fd = open(self.user_range_file, "r")
-            xlog.error("load ip range %s fail", self.range_file)
+            xlog.error("load ip range %s fail", self.default_range_file)
-                raise
+        except Exception as e:
-                    left -= len(chunk)
+            start_time = time.time()
-def update_current_version(xxnet_version):
+def update_current_version(version):
-        fd.write(xxnet_version)
+        fd.write(version)
-            update_from_github.update_current_version(version)
+            if update_from_github.update_current_version(version):
-                    if e.errno in [2, 11]:
+                    if e.errno in [2, 11, 10035]:
-                    if e.errno in [2, 11]:
+                    if e.errno in [2, 11, 10035]:
-                if e.errno in [2, 11]:
+                if e.errno in [2, 11, 10035]:
-            request_data = '%s %s HTTP/1.1\r\n' % (method, path)
+        request_data = '%s %s HTTP/1.1\r\n' % (method, path)
-            request_data += '\r\n'
+        request_data += ''.join('%s: %s\r\n' % (k, v) for k, v in headers.items())
-                sock.send(request_data.encode())
+        if len(request_data) + len(body) < 1300:
-                start += sended
+        payload_len = len(body)
-            response = Response(sock)
+        sock.settimeout(self.timeout)
-                        data_buffer.append(data)
+        response.begin(timeout=self.timeout)
-                    response.text = response.read(content_length)
+        if response.status != 200:
-            logging.warn("request e:%r", e)
+        if not read_payload:
-        return False
+            return response
-def request(method="GET", url=None, headers={}, body="", proxy=None, timeout=60):
+def request(method="GET", url=None, headers={}, body="", proxy=None, timeout=60, read_payload=True):
-    return client.request(method, url, headers, body)
+    return client.request(method, url, headers, body, read_payload)
-        proxy_type, proxy_addr, proxy_port, rdns, username, password = self.proxy
+        proxy_type, proxy_host, proxy_port, rdns, username, password = self.proxy
-        proxy_addr = self._proxy_addr()
+        proxy_port = proxy_port or DEFAULT_PORTS.get(proxy_type)
-            _BaseSocket.connect(self, proxy_addr)
+            proxy_ip = socket.gethostbyname(proxy_host)
-            proxy_server = "{0}:{1}".format(proxy_addr, proxy_port)
+            proxy_server = "{0}:{1}".format(proxy_host, proxy_port)
-
+                        if i >= len(self.waiters):
-import socks
+import simple_http_client
-default_socket = socket.socket
+if config.PROXY_ENABLE:
-def _check_one_host(host):
+def _check_one_host(url):
-        if response.status:
+        }
-        return False
+        if __name__ == "__main__":
-        if _check_one_host(host):
+
-            self.connection.retry_task_cb(self.task)
+            self.connection.retry_task_cb(self.task, reason)
-    def retry_task_cb(self, task):
+    def retry_task_cb(self, task, reason=""):
-        task.set_state("retry")
+        task.set_state("retry(%s)" % reason)
-                n1 = self.read_buffer.find("\r", self.buffer_start)
+                n1 = self.read_buffer.find("\r\n", self.buffer_start)
-                    self.buffer_start = n1 + 1
+                    self.buffer_start = n1 + 2
-                    self.buffer_start = n1 + 1
+                    self.buffer_start = n1 + 4
-            xlog.exception("socks5 protocol error:%r", e)
+            xlog.warn("socks5 protocol error:%r", e)
-            xlog.debug("upload sn:%d len:%d", sn, plen)
+            # xlog.debug("upload sn:%d len:%d", sn, plen)
-import heroku_front.apis as heroku_apis
+#import heroku_front.apis as heroku_apis
-apis = [cloudflare_apis, heroku_apis]
+apis = [cloudflare_apis]
-                if config.get("proxy", "enable", 0):
+                if config.getint("proxy", "enable", 0):
-        task.headers['Host'] = self.task.host
+        task.headers['Host'] = self.ssl_sock.host
-        # Use less for GAE server.
+import simple_http_client
-        response = BaseResponse(status=status, headers=self.response_headers)
+        response = simple_http_client.BaseResponse(status=status, headers=self.response_headers)
-import collections
+import simple_http_client
-        self.headers = headers
+show_state_debug = config.getint("system", "show_state_debug", 0)
-        # xlog.debug("%s stat:%s", self.unique_id, stat)
+        if show_state_debug:
-        res = BaseResponse(body=err_text)
+        res = simple_http_client.BaseResponse(body=err_text)
-        if time.time() - task.start_time > 240:
+        if time.time() - task.start_time > task.timeout:
-
+from config import config
-        if True:
+        if config.getint("connect_manager", "use_http2", 1):
-    print(response)
+    content, status, response = front.request("GET", "center.xx-net.net", "/", timeout=10)
-
+        self.load()
-            xlog.debug('GAEProxy Web_control %s %s %s ', self.address_string(), self.command, self.path)
+            xlog.debug('cloudflare Web_control %s %s %s ', self.address_string(), self.command, self.path)
-        xlog.debug ('GAEProxy web_control %s %s %s ', self.address_string(), self.command, self.path)
+        xlog.debug ('cloudflare web_control %s %s %s ', self.address_string(), self.command, self.path)
-all_fronts = [gae_front, cloudflare_front, heroku_front]
+#from heroku_front.front import front as heroku_front
-        self.hosts = ["xxnet4.herokuapp.com"]
+        self.hosts = []
-        self.load()
+        self.load()
-    def read_line(self):
+    def read_null_end_line(self):
-                    n1 = self.read_buffer.find("\r", self.buffer_start)
+                if n1 > -1:
-        user_id = self.read_line()
+        user_id = self.read_null_end_line()
-            addr = self.read_line()
+            addr = self.read_null_end_line()
-        line = self.read_line()
+        line = self.read_crlf_line()
-        req_line = self.read_line()
+        req_line = self.read_crlf_line()
-            xlog.warn("https req line fail:%s", req_line)
+            xlog.warn("http req line fail:%s", req_line)
-from heroku_front import web_control as heroku_web
+#from heroku_front import web_control as heroku_web
-            controler.do_GET()
+        elif reqs['cmd'] == ['set_localversion']:
-            "port": 8087
+            "port": 8087,
-                           password=self.proxy["pass"])
+        import socks
-            sock.settimeout(connect_timeout)
+        sock = socks.socksocket(socket.AF_INET)
-            sock.connect((host, port))
+        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
-            # xlog.debug("proxy:%s tcp conn:%s time:%d", proxy["host"], host, conn_time * 1000)
+        sock.connect((host, port))
-            raise e
+        # conn_time = time.time() - start_time
-            # xlog.info("Block_send_pool stop finished")
+        self.running = False
-                pass
+        # xlog.debug("notify")
-        response = dispatcher.request(method, host, path, headers, data, timeout=timeout)
+        response = dispatcher.request(method, host, path, dict(headers), data, timeout=timeout)
-        self.start_request()
+        threading.Thread(target=self.start_request).start()
-        while time.time() - self.task.start_time < self.task.timeout:
+        start_time = time.time()
-import direct_front
+# import direct_front
-            method, host=host, path=path, headers=headers, data=data, timeout=timeout)
+            method, host=host, path=path, headers=dict(headers), data=data, timeout=timeout)
-
+running = True
-                self.user_special.scan_ip_thread_num = self.DEFAULT_CONFIG.getint('google_ip', 'max_scan_ip_thread_num')
+        self.wait_queue.stop()
-                       transfer_no, send_data_len, send_ack_len, server_timeout)
+            #xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d timeout:%d",
-    while time.time() - start_time < 10 * 60:
+    while g.running and  time.time() - start_time < 10 * 60:
-import socks
+            import socks
-            xlog.warn("%s h1_request:%r", self.ip, e)
+            xlog.warn("%s h1_request:%s %r time_cost:%d inactive:%d", self.ip, task.url, e,
-            xlog.warn("%s h1_request:%r", self.ip, e)
+            xlog.warn("%s h1 get data:%r", self.ip, e)
-        # xlog.debug("task start request")
+        # xlog.debug("task start request:%s timeout:%d", url, timeout)
-xlog.set_buffer(2000)
+xlog.set_buffer(1000)
-        elif cmd == "get_last":
+        if cmd == "get_last":
-                check_ip.load_proxy_config()
+
-import urllib2
+import urllib2
-import ssl
+import simple_http_client
-def get_opener(retry=0):
+
-        return opener
+        if config.get(["proxy", "enable"], 0):
-        return update.get_opener()
+        cert = os.path.join(data_root, "gae_proxy", "CA.crt")
-            progress[url]["size"] = int(req.headers.get('content-length') or 0)
+            req = request(url, i, timeout=120)
-            chunk_len = 65536
+            left = file_size
-            continue
+import simple_http_client
-    def __init__(self, proxy, timeout):
+    def __init__(self, proxy=None, timeout=60, cert=""):
-        if proxy:
+        if isinstance(proxy, str):
-    def request(self, method, url, headers, body):
+    def request(self, method, url, headers={}, body="", read_payload=True):
-            sock = ssl.wrap_socket(sock)
+            if os.path.isfile(self.cert):
-        
+        with self.buffer_lock:
-__all__ = ["local", "start"]
+__all__ = ["local", "start"]
-        # xlog.info("Block_send_pool stop finished")
+        with self.lock:
-            pass
+        with self.lock:
-        lock.acquire()
+        with self.lock:
-            if is_max:
+            if len(self.waiters) == 0:
-                self.waiters.insert(i, (end_time, lock))
+                is_max = True
-        out_string = "waiters:<br>\n"
+        out_string = "waiters[%d]:<br>\n" % len(self.waiters)
-xlog = getLogger("x_tunnel", buffer_size=500, file_name=log_file)
+xlog = getLogger("x_tunnel")
-    config.set_var("roundtrip_timeout", 28)
+    config.set_var("roundtrip_timeout", 15)
-
+                timeout = 30
-                            check_ip.update_front_domains()
+                        if response.text == old_content:
-            if best_worker is None or idle_num < 5 or (now - best_worker.last_active_time) < 2:
+            if best_worker is None or idle_num < 5 or (now - best_worker.last_active_time) < 2 or best_score>1000:
-            if best_worker or nowait:
+            if nowait:
-            task.set_state("get_worker:%s" % worker.ip)
+            get_worker_time = time.time()
-top_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir))
+top_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, os.pardir))
-        DEFAULT_CONFIG_FILENAME = os.path.abspath( os.path.join(current_path, 'proxy.ini'))
+        DEFAULT_CONFIG_FILENAME = os.path.abspath( os.path.join(current_path, 'default_config.ini'))
-                self.USER_CONFIG.read(CONFIG_USER_FILENAME)
+            if os.path.isfile(self.CONFIG_USER_FILENAME):
-                f.write("password = %s\n\n" % self.user_special.password)
+            f = open(self.CONFIG_USER_FILENAME, 'w')
-
+            xlog.info("save config to %s", self.CONFIG_USER_FILENAME)
-            xlog.warn("launcher.config save user config fail:%s", CONFIG_USER_FILENAME)
+            xlog.exception("launcher.config save user config fail:%s", self.CONFIG_USER_FILENAME)
-        elif cmd == "get_last":
+        if cmd == "get_last":
-    timeout = 30
+from web_control import user_config
-                self.hosts.remove(heroku_host)
+                try:
-top_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir))
+top_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, os.pardir))
-        DEFAULT_CONFIG_FILENAME = os.path.abspath( os.path.join(current_path, 'proxy.ini'))
+        DEFAULT_CONFIG_FILENAME = os.path.abspath( os.path.join(current_path, 'default_config.ini'))
-                self.USER_CONFIG.read(CONFIG_USER_FILENAME)
+            if os.path.isfile(self.CONFIG_USER_FILENAME):
-                f.write("password = %s\n\n" % self.user_special.password)
+            f = open(self.CONFIG_USER_FILENAME, 'w')
-
+
-            xlog.warn("launcher.config save user config fail:%s", CONFIG_USER_FILENAME)
+            xlog.exception("launcher.config save user config fail:%s", self.CONFIG_USER_FILENAME)
-        elif cmd == "get_last":
+        if cmd == "get_last":
-        content, status, response = g.http_client.request(method="POST", host=g.config.api_server, path=path,
+        while time.time() - start_time < 30:
-                                                     data=upload_post_data, timeout=g.config.network_timeout)
+                                                     data=upload_post_data, timeout=10)
-        elif cmd == "get_last":
+        if cmd == "get_last":
-                    g.session.start()
+
-        elif 'en' in lang_code:
+        elif 'fa' in lang_code:
-            rtt = self.rtt + 100
+            rtt += 100
-            rtt = self.rtt + len(self.streams) * 100
+            rtt += len(self.streams) * 100
-        elif inactive_time < 0.001:
+        elif inactive_time < 0.1:
-            score = rtt + (1/inactive_time)*100
+            score = rtt + (1/inactive_time)*1000
-            if idle_num < 5:
+            if idle_num < 5 or best_rtt > 1000:
-            self.last_notify_time = 0
+class WaitQueue():
-                self.notify()
+        for end_time, lock in self.waiters:
-        self.last_notify_time = time.time()
+        try:
-        end_time = begin_time + timeout
+    def status(self):
-                    self.tail_sn += 1
+            self.head_sn = 1
-                    break
+    def put(self, data):
-                        self.tail_sn += 1
+        # xlog.debug("SendBuffer put len:%d", len(data))
-                    break
+            if len(self.last_block) > self.max_payload:
-                    # xlog.debug("send_pool get wake after no data, sn:%d tail:%d", sn, self.tail_sn)
+    def get(self):
-        return data, sn
+        #xlog.debug("Get:%s", utils.str2hex(data))
-        out_string = "Block_send_pool:<br>\n"
+        out_string = "SendBuffer:<br>\n"
-        out_string += " block_list: %d<br>\n" % len(self.block_list)
+        out_string += "block_list:[%d]<br>\n" % len(self.block_list)
-            out_string += "  %d<br>\r\n" % ((end_time - time.time()))
+            out_string += "[%d] len:%d<br>\r\n" % (sn, len(data))
-    config.set_var("send_delay", 10)
+    # min roundtrip on road if connectoin exist
-    config.set_var("roundtrip_timeout", 10)
+    config.set_var("max_payload", 128 * 1024)
-    config.set_var("windows_size", 32 * 1024 * 1024)
+    config.set_var("windows_size", 16 * 1024 * 1024)
-            rtt = self.rtt + 100
+            rtt += 100
-            rtt = self.rtt + len(self.streams) * 3000
+            rtt += len(self.streams) * 100
-        elif inactive_time < 0.001:
+        elif inactive_time < 0.01:
-                       (w.ip, w.rtt, w.accept_task, (time.time()-w.ssl_sock.create_time), w.processed_tasks)
+            out_str += "%s rtt:%d a:%d live:%d inactive:%d processed:%d" % \
-protocol_version = 1
+protocol_version = 2
-    "timeout_roundtrip": 0
+    "timeout_roundtrip": 0,
-            rtt = self.rtt + 100
+            rtt += 100
-            rtt = self.rtt + len(self.streams) * 3000
+            rtt += len(self.streams) * 100
-        self.download_order_queue = base_container.BlockReceivePool(process_callback=self.download_data_processor)
+        self.wait_queue = base_container.WaitQueue()
-        self.last_download_data_time = 0
+        self.last_receive_time = 0
-            self.mutex.acquire()
+            self.lock.acquire()
-            self.last_roundtrip_time = 0
+            self.last_send_time = time.time()
-            self.last_download_data_time = 0
+            self.last_receive_time = 0
-                self.roundtrip_thread[i] = threading.Thread(target=self.normal_roundtrip_worker, args=(server_address,))
+                self.roundtrip_thread[i] = threading.Thread(target=self.normal_roundtrip_worker)
-            self.mutex.release()
+            self.lock.release()
-        self.upload_task_queue.stop()
+
-        out_string += "last_download_data_time:%d<br>\n" % (time.time() - self.last_download_data_time)
+        out_string += "last_send_time:%f<br>\n" % (time.time() - self.last_send_time)
-        out_string += "<br>\n" + self.download_order_queue.status()
+        out_string += "<br>\n" + self.wait_queue.status()
-                                               g.config.windows_ack)
+                upload_data_head = struct.pack("<cBB8sIHIIHH", magic, g.protocol_version, pack_type, str(self.session_id),
-                                                               timeout=g.config.roundtrip_timeout)
+                                                               timeout=g.config.network_timeout)
-                    return False
+                    continue
-                    return False
+                    continue
-                if magic != "P" or protocol_version != 1 or pack_type != 1:
+                if magic != "P" or protocol_version != g.protocol_version or pack_type != 1:
-        elif time.time() - self.last_roundtrip_time > 5 * 60 - 5:
+        elif time.time() - self.last_send_time > 5 * 60 - 5:
-        self.mutex.acquire()
+        self.lock.acquire()
-        self.mutex.release()
+        self.lock.release()
-        buf.append(struct.pack("<BII", 2, 4 + len(data), conn_id))
+        buf.append(struct.pack("<II", conn_id, len(data)))
-        self.upload_task_queue.put(buf, no_delay)
+        self.send_buffer.put(buf)
-                    raise Exception("process_block, unknown type:%d" % data_type)
+                conn_id, payload_len = struct.unpack("<II", data.get(8))
-        self.upload_task_queue.put("")
+    def roundtrip_process(self, data, ack):
-        with self.mutex:
+        with self.lock:
-        while self.running:
+    def trigger_more(self, server_pool_size):
-                # xlog.debug("block for none")
+        xlog.debug("trigger %d, server_pool_size:%d", action_num, server_pool_size)
-                get_timeout = 0
+    def normal_roundtrip_worker(self):
-            upload_data, send_sn = self.upload_task_queue.get(get_timeout)
+            send_data_len = len(data)
-            send_ack_len = len(upload_ack_data)
+
-            if self.on_road_num > g.config.concurent_thread_num * 0.8:
+            if self.on_road_num > g.config.concurent_thread_num * 0.6:
-                    server_timeout = g.config.roundtrip_timeout / 2
+                server_timeout = g.config.roundtrip_timeout
-                                           send_sn, server_timeout, send_data_len, send_ack_len)
+            upload_data_head = struct.pack("<cBB8sIBIH", magic, g.protocol_version, pack_type,
-            upload_post_buf.append(upload_ack_data)
+            upload_post_buf.append(data)
-                sleep_time = min(try_no, 30)
+            self.last_send_time = time.time()
-                start_time = time.time()
+            sleep_time = 1
-                    self.on_road_num += 1
+            start_time = time.time()
-                                                                    timeout=server_timeout + g.config.network_timeout)
+            with self.lock:
-                g.stat["roundtrip_num"] += 1
+            xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d timeout:%d",
-                    return
+                traffic = len(upload_post_data) + len(content) + 645
-                            self.reset()
+            g.stat["roundtrip_num"] += 1
-                    break
+                elif error_code == 2:
-                              (time.time() - start_time) * 1000, transfer_no, send_sn, send_data_len, status, try_no)
+                    xlog.error("unknown error code:%d, message:%s", error_code, message)
-                    del self.transfer_list[transfer_no]
+                data = payload.get_buf(data_len)
-        xlog.info("roundtrip port:%d thread exit", server_address[1])
+                xlog.debug("data not enough")
-                                                     data=upload_post_data, timeout=g.config.roundtrip_timeout)
+                                                     data=upload_post_data, timeout=g.config.network_timeout)
-    req_info = {"account": account, "password": password}
+    req_info = {"account": account, "password": password, "protocol_version": "2"}
-                    self.transfer_list[transfer_no]["start"] = time.time()
+                    try:
-def download_overwrite_new_version(xxnet_version):
+def download_overwrite_new_version(xxnet_version,checkhash=1):
-        raise Exception("download xxnet zip checksum fail:%s" % xxnet_zip_file)
+    if checkhash:
-def update_version(version):
+def update_version(version,checkhash=1):
-        download_overwrite_new_version(version)
+        download_overwrite_new_version(version,checkhash)
-def start_update_version(version):
+def start_update_version(version, checkhash=1):
-    th = threading.Thread(target=update_version, args=(version,))
+    th = threading.Thread(target=update_version, args=(version,checkhash))
-            update_from_github.start_update_version(version)
+
-            if ran < 20:
+            if ran < self.ipv6_scan_ratio:
-                   "ipv6_tunnel": ipv6_tunnel.state(),
+                   # "ipv6_tunnel": ipv6_tunnel.state(),
-                    count += 1
+            lines = ip_list.split("\n")
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global X-Tunne;', 'enableGlobalXTunnel:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global X-Tunnel', 'enableGlobalXTunnel:', '')
-
+current_path = os.path.dirname(os.path.abspath(__file__))
-            addr_bytes = socket.inet_pton(socket.AF_INET6, host)
+            addr_bytes = inet_pton(socket.AF_INET6, host)
-            addr_bytes = socket.inet_pton(socket.AF_INET, host)
+            addr_bytes = socket.inet_aton(host)
-            addr = socket.inet_ntop(socket.AF_INET6, self._readall(file, 16))
+            addr = inet_ntop(socket.AF_INET6, self._readall(file, 16))
-from pyasn1.codec.der import decoder as der_decoder
+from pyasn1.codec.der import decoder as der_decoder
-    content, status, response = front.request("GET", "center6.xx-net.net", "/")
+    content, status, response = front.request("GET", "center.xx-net.net", "/")
-    host = "xxnet3.herokuapp.com"
+    host = "xxnet4.herokuapp.com"
-        ip = "50.19.244.243"
+        ip = "107.21.125.200"
-        return worker.get_score()
+        return worker.get_score() * 5
-        password_hash = str(hashlib.sha256(password).hexdigest())
+        if password == "_HiddenPassword":
-            out_list.append("%s: %s\r\n" % (key, value))
+            out_list.append("%s: %s\r\n" % (key, response.headers[key]))
-                if not ip_utils.check_ip_valid(ip):
+                if not ip_utils.check_ip_valid(ip) and not ip_utils.check_ip_valid6(ip):
-        time.sleep(1)
+    #for i in range(30):
-                (config.PROXY_TYPE, config.PROXY_USER, config.PROXY_PASSWD, config.PROXY_HOST, config.PROXY_PORT)
+        if config.PROXY_ENABLE:
-                (config.PROXY_TYPE, config.PROXY_HOST, config.PROXY_PORT)
+            self.proxy = None
-            self._triger_check_network(True)
+            self.triger_check_network(True)
-    def _triger_check_network(self, fail=False, force=False):
+    def triger_check_network(self, fail=False, force=False):
-IPv4._triger_check_network()
+IPv4.triger_check_network()
-IPv6._triger_check_network()
+IPv6.triger_check_network()
-        out = subprocess.check_output(cmd, startupinfo=startupinfo)
+        #out = subprocess.check_output(cmd, startupinfo=startupinfo)
-        if self.path != "https://www.twitter.com/xxnet":
+        if self.path != "www.twitter.com:443":
-import httplib
+            if __name__ == "__main__":
-            if ran < 10:
+            if ran < 20:
-from subprocess import check_output
+import subprocess
-    out = check_output(cmd)
+    cmd = shlex.split(cmd)
-            out = "Exception:%r" % e
+        out = run(cmd)
-                   # "ipv6_tunnel": ipv6_tunnel.state(),
+                   "ipv6_tunnel": ipv6_tunnel.state(),
-                connect_manager.load_proxy_config()
+from base64 import b64encode
-                     b" HTTP/1.1\r\n" + b"Host: " + dest_addr.encode('idna') + b"\r\n\r\n")
+        http_headers = [
-                   "ipv6_tunnel": ipv6_tunnel.state(),
+                   # "ipv6_tunnel": ipv6_tunnel.state(),
-                self.last_check_time = time_now
+        self.last_check_time = time_now
-        IPv4.is_ok()
+        return IPv4.is_ok()
-        IPv6.is_ok()
+        return IPv6.is_ok()
-                inactive_time = time.time() -sock.last_use_time
+                inactive_time = time.time() - sock.last_use_time
-                out_str += "%d \t %s handshake:%d not_active_time:%d h2:%d\r\n" % (i, sock.ip, t, time.time() -sock.last_use_time, sock.h2)
+                out_str += "%d \t %s handshake:%d not_active_time:%d h2:%d\r\n" % (i, sock.ip, t, time.time() - sock.last_use_time, sock.h2)
-                # xlog.warning("no enough ip")
+                xlog.warning("no enough ip")
-            xlog.debug("%s fail:%r", ip, e)
+            xlog.debug("connect %s fail:%r", ip, e)
-
+
-    if response.status >= 500:
+    if response.status >= 600:
-
+import yaml
-import simple_http_server
+import ipv6_tunnel
-                   "ipv6_tunnel": ipv6_tunnel_status(),
+                   "ipv6_tunnel": ipv6_tunnel.state(),
-        global ipv6_tunnel_proc
+            cmd = reqs['cmd'][0]
-                cmd = reqs['cmd'][0]
+            if os.path.isfile(log_path):
-                    data = '{"res":"success", "time":"%s"}' % time_now
+                    os.remove(log_path)
-                    data = '{"res":"%s", "time":"%s"}' % (e, time_now)
+                    xlog.warn("remove %s fail:%r", log_path, e)
-                data = '{"res":"ipv6_tunnel is killed", "time":"%s"}' % time_now
+            if cmd == "enable":
-                data = '{"res":"ipv6_tunnel is not running", "time":"%s"}' % time_now
+                xlog.warn("unknown cmd:%s", cmd)
-            if ipv6_tunnel_proc and os.path.isfile(log_path):
+            if os.path.isfile(log_path):
-                status = ipv6_tunnel_status()
+            status = ipv6_tunnel.state()
-            data = json.dumps({'status': status, 'log': content, 'time': time_now})
+            data = json.dumps({'status': status, 'log': content.decode("GBK"), 'time': time_now})
-        self.send_response_nc('text/html', data)
+
-        xlog.warn("IP:%s not support GAE, server:%s status:%d", response.ssl_sock.ip, server_type,
+        xlog.warn("IP:%s not support GAE, headers:%s status:%d", response.ssl_sock.ip, response.headers,
-            # tear down the entire connection. TODO: actually do that.
+            # tear down the entire connection.
-        response = BaseResponse(status=status, headers=self.response_headers)
+        response = simple_http_client.BaseResponse(status=status, headers=self.response_headers)
-import collections
+import simple_http_client
-        res = BaseResponse(body=err_text)
+        res = simple_http_client.BaseResponse(body=err_text)
-import urllib2
+data_path = os.path.abspath( os.path.join(top_path, 'data', 'gae_proxy'))
-        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(top_path, 'data', 'gae_proxy', 'config.ini'))
+        CONFIG_USER_FILENAME = os.path.join(data_path, 'config.ini')
-        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(top_path, 'data', 'gae_proxy', 'config.ini'))
+        CONFIG_USER_FILENAME = os.path.join(data_path, 'config.ini')
-    return
+ipv6_tunnel_proc = None
-        self.send_response_nc(mimetype, data)
+        self.send_response_nc(mimetype, data)
-class TxtResponse(object):
+class BaseResponse(object):
-            key = str(key.lower())
+            key = str(key.title())
-class Response(object):
+class Response(BaseResponse):
-
+        if self.USE_IPV6 not in ["auto", "force_ipv4", "force_ipv6"]:
-                    if appids and google_ip.good_ip_num:
+                    if appids and (google_ip.good_ipv4_num + google_ip.good_ipv6_num):
-            good_num = google_ip.good_ip_num
+            good_num = (google_ip.good_ipv4_num + google_ip.good_ipv6_num)
-    if check_local_network.network_stat != "OK":
+    if check_local_network.is_ok(ip):
-    check_local_network.continue_fail_count = 0
+    check_local_network.report_ok(ip)
-
+import simple_http_client
-    _checking_lock.release()
+class CheckNetwork(object):
-        last_check_time = time.time()
+        if config.PROXY_USER:
-    _checking_lock.release()
+        return False
-        xlog.debug("restore socket")
+    def _simple_check_worker(self):
-            return
+        network_ok = False
-                return
+        if network_ok:
-            if time_now - last_check_time < 10:
+            xlog.warn("network %s fail", self.type)
-
+            if fail or self.network_stat != "OK":
-#===========================================
+def report_fail(ip):
-        return False
+def is_ok(ip):
-    return False
+if __name__ == "__main__":
-        self.USE_IPV6 = self.CONFIG.getint('google_ip', 'use_ipv6')
+        self.USE_IPV6 = self.CONFIG.get('google_ip', 'use_ipv6')
-    if response.status >= 600:
+    if response.status >= 500:
-    content_type = response.headers.get("content-type", "")
+    server_type = response.getheader("server", "")
-        self.good_ip_num = 0 # only success ip num
+        self.good_ipv4_num = 0
-            self.auto_adjust_scan_ip_thread_num = config.CONFIG.getint("google_ip", "auto_adjust_scan_ip_thread_num")
+        good_ip_file_name = "good_ip.txt"
-
+    def _add_ip_num(self, ip, num):
-            self.good_ip_num = 0
+            self.good_ipv4_num = 0
-                    self.good_ip_num += 1
+                    self._add_ip_num(ip, 1)
-                    self.good_ip_num += 1
+                    self._add_ip_num(ip, 1)
-            self.good_ip_num += 1
+            self._add_ip_num(ip, 1)
-        check_local_network.report_network_ok()
+        check_local_network.report_ok(ip)
-                    self.good_ip_num += 1
+                    self._add_ip_num(ip, 1)
-                    self.good_ip_num -= 1
+                    self._add_ip_num(ip, -1)
-            if not check_local_network.is_ok():
+            if not check_local_network.is_ok(ip):
-            if not check_local_network.is_ok():
+            check_local_network.report_fail(ip)
-                self.good_ip_num -= 1
+                self._add_ip_num(ip, -1)
-                self.good_ip_num -= 1
+                self._add_ip_num(ip, -1)
-            if not check_local_network.is_ok():
+            if not check_local_network.is_ok(ip):
-                        self.good_ip_num += 1
+                        self._add_ip_num(ip, 1)
-        if not check_local_network.is_ok():
+        if not check_local_network.is_ok(ip):
-                ip = self.get_ip_to_scan()
+                ip = google_ip_range.ip_range.get_ip()
-                        self.good_ip_num -= 1
+                        self._add_ip_num(ip, -1)
-        self.load_ip_range()
+
-                continue
+    def get_ip(self, use_ipv6=None):
-            return ip_int
+        if use_ipv6 == "force_ipv4":
-        #return self.get_real_random_ip()
+            ran = random.randint(0, 100)
-        print(ip)
+    ip = ip_range.get_ip()
-                status, self.command, path, request_headers, payload, response)
+        response = simple_http_client.request(self.command, self.path, request_headers, payload)
-        for key, value in response.getheaders():
+        out_list.append("HTTP/1.1 %d\r\n" % response.status)
-        out_list.append(content)
+        out_list.append(response.text)
-        self.use_ipv6 = 0
+        self.use_ipv6 = "auto"
-                self.user_special.use_ipv6 = config.CONFIG.getint('google_ip', 'use_ipv6')
+                self.user_special.use_ipv6 = config.CONFIG.get('google_ip', 'use_ipv6')
-                f.write("use_ipv6 = %d\n\n" % int(self.user_special.use_ipv6))
+            if self.user_special.use_ipv6 != self.DEFAULT_CONFIG.get('google_ip', 'use_ipv6'):
-            xlog.warn("launcher.config save user config fail:%s", CONFIG_USER_FILENAME)
+        except Exception as e:
-                   "use_ipv6": config.CONFIG.getint("google_ip", "use_ipv6"),
+                   "use_ipv6": config.USE_IPV6,
-                   "network_state": check_local_network.network_stat,
+                   "ipv4_state": check_local_network.IPv4.get_stat(),
-                   "good_ip_num": good_ip_num,
+                   "good_ipv4_num": google_ip.good_ipv4_num,
-
+            use_ipv6 = self.postvars['use_ipv6'][0]
-                data = sock.recv(8192)
+                try:
-                data = sock.recv(8192)
+                try:
-            key = str(key.lower())
+            key = str(key.title())
-        key = key.lower()
+        key = key.title()
-            data = self.connection.recv(to_read)
+
-        dest_pair - 2-tuple of (IP/hostname, port).
+        dest_pair
-        dest_addr, dest_port = dest_pair
+        if len(dest_pair) == 2:
-                or not isinstance(dest_port, int)):
+        if not dest_addr or not isinstance(dest_port, int):
-        self.hosts = ["xxnet2.herokuapp.com", "xxnet3.herokuapp.com", "xxnet4.herokuapp.com", "xxnet5.herokuapp.com"]
+        self.hosts = ["xxnet4.herokuapp.com"]
-             "show_systray": %d, "auto_start": %d, "show_detail": %d, "gae_proxy_enable": %d, "x_tunnel_enable": %d}' %\
+             "show_systray": %d, "auto_start": %d, "show_detail": %d, "gae_proxy_enable": %d, "x_tunnel_enable": %d, \
-                        (u"å¨å±éè¿X-Tunnelä»£ç", None, self.on_enable_x_tunnel, x_tunnel_checked),
+            menu_options = [(u"è®¾ç½®", None, self.on_show, 0)]
-                        (u'éåº', None, SysTrayIcon.QUIT, False))
+                        (u'éåº', None, SysTrayIcon.QUIT, False)]
-        return menu_options
+            menu_options = [(u"Config", None, self.on_show, 0)]
-
+import sni_generater
-def connect_ssl(ip, port=443, timeout=5, check_cert=True):
+def connect_ssl(ip, port=443, timeout=5, check_cert=True, close_cb=None):
-    ssl_sock = openssl_wrap.SSLConnection(openssl_context, sock, ip)
+    ssl_sock = openssl_wrap.SSLConnection(openssl_context, sock, ip, close_cb)
-        raise socket.error(' certficate is none')
+    def verify_SSL_certificate_issuer(ssl_sock):
-            raise socket.error(' certficate is issued by %r, not Google' % ( issuer_commonname))
+        verify_SSL_certificate_issuer(ssl_sock)
-                    ssl_sock.h2 = False
+            ssl_sock = check_ip.connect_ssl(ip, port=443, timeout=self.timeout, check_cert=True,
-            ssl_sock.host = ''
+
-                xlog.debug("%s fail:%r", ip, e)
+            xlog.debug("%s fail:%r", ip, e)
-            return False
+
-                self.file_config = json.loads(f.read())
+                content = f.read()
-    global recent_sent, recent_received
+    global recent_sent, recent_received, total_sent, total_received
-                if num > 1023:
+                if num >= 1024:
-        super(HTTP1_worker, self).__init__(ssl_sock, close_cb, retry_task_cb, idle_cb)
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-        super(HTTP2_worker, self).__init__(ssl_sock, close_cb, retry_task_cb, idle_cb)
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb):
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-    def update_rtt_speed(self, rtt, speed):
+    def update_debug_data(self, rtt, sent, received, speed):
-            worker = HTTP2_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = HTTP2_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb, self.log_debug_data)
-            worker = HTTP1_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = HTTP1_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb, self.log_debug_data)
-            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host)
+            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host, self.log_debug_data)
-            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host)
+            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host, self.log_debug_data)
-            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host)
+            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host, self.log_debug_data)
-        super(HTTP1_worker, self).__init__(ssl_sock, close_cb, retry_task_cb, idle_cb)
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-        super(HTTP2_worker, self).__init__(ssl_sock, close_cb, retry_task_cb, idle_cb)
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb):
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-    def update_rtt_speed(self, rtt, speed):
+    def update_debug_data(self, rtt, sent, received, speed):
-    def __init__(self, host):
+    def __init__(self, host, log_debug_data):
-            worker = HTTP2_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = HTTP2_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb, self.log_debug_data)
-            worker = HTTP1_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = HTTP1_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb, self.log_debug_data)
-        self.dispatcher = http_dispatcher.HttpsDispatcher(self.host)
+        self.dispatcher = http_dispatcher.HttpsDispatcher(self.host, self.log_debug_data)
-        super(HTTP1_worker, self).__init__(ssl_sock, close_cb, retry_task_cb, idle_cb)
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-        super(HTTP2_worker, self).__init__(ssl_sock, close_cb, retry_task_cb, idle_cb)
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb):
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb, log_debug_data):
-    def update_rtt_speed(self, rtt, speed):
+    def update_debug_data(self, rtt, sent, received, speed):
-    def __init__(self, host):
+    def __init__(self, host, log_debug_data):
-            worker = HTTP2_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = HTTP2_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb, self.log_debug_data)
-            worker = HTTP1_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
+            worker = HTTP1_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb, self.log_debug_data)
-                    response.worker.update_rtt_speed(rtt, speed)
+                    response.worker.update_debug_data(rtt, send_data_len, len(content), speed)
-                "worker_num": front.worker_num()
+                "worker_num": front.worker_num(),
-        xlog.debug("Initializing CA")
+        #xlog.debug("Initializing CA")
-            xlog.info("no XX-Net GAE CA file exist")
+            xlog.info("no GAE CA file exist in XX-Net data dir")
-        pass
+        if os.getenv("DESKTOP_SESSION","unknown") != "unknown" :  # make sure this is desktop linux
-        if os.getenv("XXNET_NO_MESS_SYSTEM", "0") != "0":
+        if os.getenv("XXNET_NO_MESS_SYSTEM", "0") == "0":
-
+need_save_config = False
-        need_save_config = True
+        xlog.debug("Begin to import Windows CA")
-    def import_debian_ca(common_name, ca_file):
+    def import_linux_ca(common_name, ca_file):
-        def get_debian_ca_sha1(nss_path):
+        def get_linux_ca_sha1(nss_path):
-        sha1 = get_debian_ca_sha1(nss_path)
+        sha1 = get_linux_ca_sha1(nss_path)
-            xlog.info("system cert exist")
+            xlog.info("Database $HOME/.pki/nssdb cert exist")
-            CertUtil.import_debian_ca(commonname, certfile)
+            CertUtil.import_linux_ca(commonname, certfile)
-            xlog.info("no CA file exist")
+            xlog.info("no XX-Net GAE CA file exist")
-            xlog.info("clean old site certs")
+            xlog.info("clean old site certs in XX-Net cert dir")
-        CertUtil.import_ca(CertUtil.ca_keyfile)
+        if os.getenv("XXNET_NO_MESS_SYSTEM", "0") == "0" :
-
+    if get(["no_mess_system"], 0) == 1 or os.getenv("XXNET_NO_MESS_SYSTEM","0") != "0" :
-        create_desktop_shortcut()
+        if os.getenv("XXNET_NO_MESS_SYSTEM", "0") != "0":
-                
+
-                        (u"éå¯ GAEProxy", None, self.on_restart_gae_proxy, 0),
+                        (u"Set Global X-Tunnel Proxy", None, self.on_enable_x_tunnel, x_tunnel_checked),
-                        (u"Reset GAEProxy", None, self.on_restart_gae_proxy, 0),
+    def on_enable_x_tunnel(self, widget=None, data=None):
-
+        self.last_active_time = time.time()
-        xlog.warn("h2 %s %s timeout %s",
+        xlog.warn("h2 timeout %s task_trace:%s worker_trace:%s",
-                  self.task.unique_id,
+                  self.task.get_trace(),
-        return "", 602, {}
+    start_time = time.time()
-    return front.request(method, host=host, path=path, headers=headers, data=data, timeout=timeout)
+    return content, status, response
-        return "", 500, {}
+        try:
-            start_time = time.time()
+        start_time = time.time()
-                return False
+                magic = "P"
-                return False
+                upload_post_data = encrypt_data(upload_data_head)
-                return False
+                content, status, response = g.http_client.request(method="POST", host=g.server_host, path="/data",
-                return False
+                time_cost = time.time() - start_time
-            return False
+                if status == 521:
-def request_balance(account, password, is_register=False, update_server=True):
+def request_balance(account=None, password=None, is_register=False, update_server=True):
-        if not retry and err_msg:
+        if not retry and error_msg:
-
+    now = time.time()
-def request_gae_proxy(method, url, headers, body, timeout=60):
+def request_gae_proxy(method, url, headers, body, timeout=60, retry=True):
-                task.set_state("read body:%d" % len(data))
+                # task.set_state("read body:%d" % len(data))
-            top_domain = None
+        if not ip_utils.check_ip_valid(ip):
-            print("not support")
+    xlog.info("test ip:%s", ip)
-        xlog.info("check_ip <ip>")
+        top_domain = None
-        response = gae_proxy.gae_handler.request_gae_proxy(method, url, headers, data, timeout=timeout)
+        response = gae_proxy.gae_handler.request_gae_proxy(method, url, headers, data, timeout=timeout, retry=False)
-        if "X_Tunnel OK" not in content:
+        if content != "OK":
-    if "X_Tunnel OK" not in content:
+    if content != "OK":
-    host = "xxnet10.herokuapp.com"
+    host = "xxnet3.herokuapp.com"
-        ip = "50.17.207.130"
+        ip = "50.19.244.243"
-        self.hosts = ["xxnet3.herokuapp.com"]
+        self.hosts = ["xxnet2.herokuapp.com", "xxnet3.herokuapp.com", "xxnet4.herokuapp.com", "xxnet5.herokuapp.com"]
-                if status not in [200, 405]:
+                if status not in [200, 404]:
-        pall()
+        get()
-        if len(g.server_host) == 0 or g.server_port == 0:
+        if len(g.server_host) == 0:
-            ssl_sock.last_use_time = time.time()
+            ssl_sock.last_use_time = time_begin
-            raise GAE_Exception(
+            raise GAE_Exception(600,
-        raise GAE_Exception("unpack protocol:%r", e)
+        raise GAE_Exception(600, "unpack protocol:%r" % e)
-import Queue
+import simple_http_client
-        ping_interval = 55.0
+        while time.time() - self.ssl_sock.create_time < 5:
-            if time_now - self.last_request_time > self.idle_time:
+            if not self.task and time.time() - self.last_active_time > ping_interval:
-
+        self.ssl_sock.last_use_time = time.time()
-            response.begin()
+            response = simple_http_client.Response(self.ssl_sock)
-                data = response.read(to_read)
+                data = response.read()
-        request_data = 'HEAD /_gh/ HTTP/1.1\r\nHost: %s\r\n\r\n' % self.ssl_sock.host
+        request_data = 'GET /_gh/ HTTP/1.1\r\nHost: %s\r\n\r\n' % self.ssl_sock.host
-            response.begin()
+
-            xlog.debug("h1 %s appid:%s HEAD keep alive request fail:%r", self.ssl_sock.ip, self.ssl_sock.appid, e)
+            inactive = time.time() - self.ssl_sock.last_use_time
-        # This function may be call by out side http2
+        # This function may be call by out side http
-        self.working_tasks[task.unique_id] = task
+        # self.working_tasks[task.unique_id] = task
-            xlog.error("http_dispatcher request unique_id %s, %s not found.", unique_id, task.unique_id)
+        # del self.working_tasks[task.unique_id]
-        self.info = " ".join(words[2:])
+        self.reason = " ".join(words[2:])
-            read_len = int(self.content_length)
+        #if not read_len and self.content_length is not None:
-
+        # self.working_tasks[task.unique_id] = task
-        del self.working_tasks[task.unique_id]
+        # del self.working_tasks[task.unique_id]
-        return worker.get_score() * 5
+        return worker.get_score()
-        self.working_tasks[task.unique_id] = task
+        # self.working_tasks[task.unique_id] = task
-        del self.working_tasks[task.unique_id]
+        # del self.working_tasks[task.unique_id]
-            xlog.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
+            xlog.debug('launcher web_control %s %s %s ', self.address_string(), self.command, self.path)
-import ip_utils
+
-        xlog.debug("content:%s", content)
+    content = response.read()
-
+        if len(sys.argv) > 2:
-        res = test_xtunnel_ip2(ip, sub=sub, top_domain=top_domain)
+        res = test_xtunnel_ip2(ip, top_domain=top_domain)
-                if inactive_time > self.ssl_first_use_timeout or not self.ssl_timeout_cb:
+                if inactive_time > self.ssl_first_use_timeout + 5 or not self.ssl_timeout_cb:
-            ssl_sock.last_use_time = time.time()
+            ssl_sock.last_use_time = ssl_sock.create_time
-                        check_ip.update_front_domains()
+                        if content != old_content:
-        self.processed_reqs += 1
+
-        return "", 500, {}
+        response = dispatcher.request(method, host, path, headers, data, timeout=timeout)
-        # stream.start()
+    def get_rtt_rate(self):
-                                   self.ip, frame.type, frame.stream_id, e)
+                    xlog.exception("%s Unexpected stream identifier %d, frame.type:%s e:%r",
-        self.start()
+        self.start_request()
-    def start(self):
+    def start_request(self):
-            xlog.warn("http2_stream send_response but responsed.%s", self.task.url)
+            xlog.error("http2_stream send_response but responsed.%s", self.task.url)
-
+        res.worker = self.worker
-        self.last_request_time = self.ssl_sock.create_time
+        self.last_active_time = ssl_sock.create_time
-        return score
+        return score
-            if best_worker is None or idle_num < 5 or (now - best_worker.last_active_time) < 2 or best_score > 1000:
+            if best_worker is None or idle_num < 5 or (now - best_worker.last_active_time) < 2:
-            slowest_rtt = 9999
+            slowest_score = 9999
-                rtt = worker.get_score()
+                score = worker.get_score()
-                    slowest_rtt = rtt
+                if score > slowest_score:
-            if idle_num < 30 or idle_num < int(len(self.workers) * 0.3):
+            if idle_num < 10 or idle_num < int(len(self.workers) * 0.3) or len(self.workers) < 50:
-
+
-            task.worker = worker
+            task.worker = worker
-            worker_rate[w] = w.get_score()
+            worker_rate[w] = w.get_rtt_rate()
-                    fail_connect_interval = 5 * 60
+                    fail_connect_interval = 1800 # 30 min
-                    fail_connect_interval = 60
+                    fail_connect_interval = 120 # 2 min
-                down_fail_connect_interval = 60 * 3
+                down_fail_connect_interval = 600
-                if inactive_time > self.ssl_first_use_timeout or not self.ssl_timeout_cb:
+                if inactive_time > self.ssl_first_use_timeout + 3 or not self.ssl_timeout_cb:
-            ssl_sock.last_use_time = time.time()
+            ssl_sock.last_use_time = ssl_sock.create_time
-                if time.time() - ssl_sock.last_use_time > self.ssl_first_use_timeout + 1:
+                if time.time() - ssl_sock.last_use_time > self.ssl_first_use_timeout + 3:
-        self.hosts = ["xxnet10.herokuapp.com"]
+        self.hosts = ["xxnet3.herokuapp.com"]
-                xlog.warn("front request %s %s%s fail:%r", method, host, path, e)
+                xlog.exception("front request %s %s%s fail:%r", method, host, path, e)
-        ping_interval = 300
+        ping_interval = 50
-            if not self.request_onway and time_now - self.last_active_time > self.idle_time:
+            if not self.request_onway and time.time() - self.last_active_time > ping_interval:
-        self.last_active_time = time.time()
+
-            self.rtt = (time.time() - start_time) * 1000
+            self.rtt = (time_now - start_time) * 1000
-            slowest_rtt = 9999
+            slowest_score = 9999
-                rtt = worker.get_score()
+                score = worker.get_score()
-                    slowest_rtt = rtt
+                if score > slowest_score:
-            if idle_num < 30 or idle_num < int(len(self.workers) * 0.3):
+            if idle_num < 10 or idle_num < int(len(self.workers) * 0.3) or len(self.workers) < 50:
-        threading.Thread(target=get).start()
+    for _ in range(10):
-                    if rtt > 3000:
+                    if rtt > 8000:
-                "score": front.get_score(),
+                "score": score,
-            if best_worker is None or idle_num < 5 or (now - best_worker.last_active_time) < 2:
+            if best_worker is None or idle_num < 5 or (now - best_worker.last_active_time) < 2 or best_score > 1000:
-
+import ip_utils
-    content = response.read()
+        sub = "scan1"
-            top_domain = sys.argv[2]
+        if not ip_utils.check_ip_valid(ip):
-            top_domain = None
+            if len(sys.argv) > 2:
-        res = test_xtunnel_ip2(ip, top_domain=top_domain)
+        res = test_xtunnel_ip2(ip, sub=sub, top_domain=top_domain)
-                    xlog.debug("new_conn_pool.get:%s handshake:%d", ssl_sock.ip, handshake_time)
+                    # xlog.debug("new_conn_pool.get:%s handshake:%d", ssl_sock.ip, handshake_time)
-    def get_score(self, host):
+    def get_score(self, host=None):
-        stream.start()
+        # stream.start()
-                self.streams[frame.stream_id].receive_frame(frame)
+                stream = self.streams[frame.stream_id]
-                    xlog.exception("%s Unexpected stream identifier %d, e:%r", self.ip, frame.stream_id, e)
+                    xlog.exception("%s Unexpected stream identifier %d frame.type:%d, e:%r",
-            xlog.warn("status:%d", status)
+        if status in [400, 403]:
-import heroku_front.front as heroku_front
+from cloudflare_front.front import front as cloudflare_front
-all_fronts = [gae_front, cloudflare_front.front, heroku_front.front]
+all_fronts = [gae_front, cloudflare_front, heroku_front]
-def get_score(host):
+def get_score(host=""):
-    global last_success_time, last_fail_time, continue_fail_num, gae_proxy
+    global last_success_time, last_fail_time, continue_fail_num, gae_proxy, success_num, fail_num
-    def get_score(self, host):
+    def get_score(self, host=None):
-            request_headers, request_body, timeout)
+                                            "POST", heroku_host, "/2/",
-    content, status, response = front.request("GET", "v11.xx-net.com")
+    content, status, response = front.request("GET", "scan1.xx-net.com", path="/wait?time=5")
-def main():
+def pall():
-        main()
+        pall()
-            self.task = None
+            self.task = task
-        task.put_data("")
+        task.finish()
-                self.task.put_data("")
+                self.task.finish()
-            self.task.put_data(frame.data)
+
-            self.send_response()
+            if not self.task.responsed:
-            # empty block means fail or closed.
+            self.task.set_state("h2 finish")
-            if self._remote_closed:
+            if self.task.finished:
-        xlog.warn("h2 %s %s timeout %s",
+        self.task.set_state("h2 timeout")
-                  self.connection.get_trace())
+                  self.task.get_trace())
-            self.task.response_fail("timeout")
+            self.task.response_fail("h2 timeout")
-            self.put_data("")
+            self.finish()
-                rtt = worker.get_rtt_rate()
+                rtt = worker.get_score()
-            worker_rate[w] = w.get_rtt_rate()
+            worker_rate[w] = w.get_score()
-from instances import xlog
+
-from instances import xlog
+from xlog import getLogger
-from instances import xlog
+from xlog import getLogger
-xlog = Logger(file_name=log_file)
+from xlog import getLogger
-from instances import xlog
+from xlog import getLogger
-from instances import xlog
+from xlog import getLogger
-from instances import xlog
+
-from instances import xlog
+
-from instances import xlog
+from xlog import getLogger
-    # sys.exit(1)#åºç°å¼å¸¸æ¶ç¨åºæ¯å¦éåº
+    # sys.exit(1)
-from instances import xlog
+from xlog import getLogger
-from instances import xlog
+from xlog import getLogger
-from instances import xlog
+from xlog import getLogger
-from instances import xlog
+from xlog import getLogger
-    def __init__(self, buffer_size=0, file_name=None, roll_num=1):
+    def __init__(self, name, buffer_size=0, file_name=None, roll_num=1):
-                sys.stderr.write(string)
+                console_string = '%s [%s][%s] %s\n' % (time_str, self.name, level, fmt % args)
-    global loggerDict
+    global loggerDict, default_log
-        logger_instance = Logger(buffer_size, file_name, roll_num)
+        logger_instance = Logger(name, buffer_size, file_name, roll_num)
-default_log = getLogger("default")
+default_log = getLogger()
-    config.set_var("api_server", "center6.xx-net.net")
+    config.set_var("api_server", "center.xx-net.net")
-def connect_ssl(ip, port=443, timeout=5, top_domain=None):
+def connect_ssl(ip, port=443, timeout=5, top_domain=None, on_close=None):
-    ssl_sock = openssl_wrap.SSLConnection(openssl_context, sock, ip)
+    ssl_sock = openssl_wrap.SSLConnection(openssl_context, sock, ip, on_close=on_close)
-            ssl_sock = check_ip.connect_ssl(ip, port=port, timeout=self.connect_timeout)
+            ssl_sock = check_ip.connect_ssl(ip, port=port, timeout=self.connect_timeout, on_close=ip_manager.ssl_closed)
-                            check_ip.update_front_domains()
+                    if content != old_content:
-                if status not in [200, 405]:
+                content = response.task.read_all()
-                content = response.task.read_all()
+        out_list.append(" host:%s" % self.ssl_sock.top_domain)
-        if inactive_time > 3:
+        if inactive_time > 5:
-            score = rtt + (3/inactive_time)*1000
+            score = rtt + (5/inactive_time)*1000
-                rtt = worker.get_rtt_rate()
+                rtt = worker.get_score()
-            worker_rate[w] = w.get_rtt_rate()
+            worker_rate[w] = w.get_score()
-                    fail_connect_interval = 1800 # 30 min
+                    fail_connect_interval = 5 * 60
-                    fail_connect_interval = 120 # 2 min
+                    fail_connect_interval = 60
-                down_fail_connect_interval = 600
+                down_fail_connect_interval = 60 * 3
-def request(method, host, path, headers={}, data="", timeout=10):
+def request(method, host, path="/", headers={}, data="", timeout=100):
-        front.stop()
+        front.stop()
-def request(method, host, schema="https", path="/", headers={}, data="", timeout=10):
+def request(method, host, schema="https", path="/", headers={}, data="", timeout=60):
-    url = schema + "://" + host + path
+    timeout = 30
-def connect_ssl(ip, port=443, timeout=5, top_domain=None):
+def connect_ssl(ip, port=443, timeout=5, top_domain=None, on_close=None):
-    ssl_sock = openssl_wrap.SSLConnection(openssl_context, sock, ip)
+    ssl_sock = openssl_wrap.SSLConnection(openssl_context, sock, ip, on_close=on_close)
-    response.begin(timeout=5)
+    try:
-        return False
+        response = simple_http_client.Response(ssl_sock)
-        return False
+        server_type = response.getheader('Server', "")
-        return False
+        if response.status != 200:
-    return True
+    ssl_sock.support_xtunnel = True
-def test_xtunnel_ip2(ip, top_domain=None):
+def test_xtunnel_ip2(ip, top_domain=None, wait_time=0):
-    host = "obscure-everglades-64338.herokuapp.com"
+    ssl_sock.support_xtunnel = False
-    ssl_sock.support_xtunnel = False
+    time.sleep(wait_time)
-            return False
+        return check_xtunnel_http1(ssl_sock, host)
-            top_domain = None
+    else:
-            print("not support")
+    if len(sys.argv) > 2:
-        xlog.info("check_ip <ip>")
+        print("not support")
-        self.thread_num_lock = threading.Lock()
+        self.thread_num_lock = threading.Lock()
-            ssl_sock = check_ip.connect_ssl(ip, port=port, timeout=self.connect_timeout)
+            ssl_sock = check_ip.connect_ssl(ip, port=port, timeout=self.connect_timeout, on_close=ip_manager.ssl_closed)
-                if time.time() - ssl_sock.last_use_time > self.ssl_first_use_timeout:
+                if time.time() - ssl_sock.last_use_time > self.ssl_first_use_timeout + 1:
-        self.dispatchs = {}
+        self.hosts = ["xxnet10.herokuapp.com"]
-            self.dispatchs[host] = http_dispatcher.HttpsDispatcher(host)
+        now = time.time()
-        dispatcher = self.dispatchs[host]
+        dispatcher = self.dispatcher
-                return True
+        return worker.get_score() * 5
-        return False
+    def _request(self, method, host, path="/", header={}, data="", timeout=30):
-                response = dispatcher.request(method, host, path, header, data, timeout=timeout)
+                response = self.dispatcher.request(method, host, path, header, data, timeout=timeout)
-                xlog.debug("%s %s%s trace:%s", method, response.ssl_sock.host, path, response.task.get_trace())
+                # xlog.debug("%s %s%s trace:%s", method, response.ssl_sock.host, path, response.task.get_trace())
-        connect_control.keep_running = False
+    def request(self, method, host, schema="http", path="/", headers={}, data="", timeout=40):
-    def request(self, method, host, schema="https", path="/", headers={}, data="", timeout=10):
+        heroku_host = str(random.choice(self.hosts))
-            "POST", "xxnet10.herokuapp.com", "/2/index.php",
+            "POST", heroku_host, "/2/",
-        self.task_queue.put("ping")
+        while time.time() - self.ssl_sock.create_time < 10:
-        task.headers['Host'] = self.task.host
+        task.headers['Host'] = task.host
-            self.task = None
+            self.task = task
-        xlog.debug("head request %s", self.ip)
+        # xlog.debug("head request %s", self.ip)
-            response.begin(timeout=5)
+            response.begin(timeout=30)
-        if inactive_time > 3:
+        if inactive_time > 10:
-            score = rtt + (3/inactive_time)*1000
+            score = rtt + (10/inactive_time)*1000
-        return score * 5
+        return score
-                rtt = worker.get_rtt_rate()
+                rtt = worker.get_score()
-            worker_rate[w] = w.get_rtt_rate()
+            worker_rate[w] = w.get_score()
-                    fail_connect_interval = 1800 # 30 min
+                    fail_connect_interval = 3 * 60
-                    fail_connect_interval = 120 # 2 min
+                    fail_connect_interval = 10
-                down_fail_connect_interval = 600
+                down_fail_connect_interval = 60
-            self.ip_dict[ip]['links'] -= 1
+            if self.ip_dict[ip]['links'] > 0:
-            self.to_check_ip_queue.put((ip, time_now + 10))
+            # self.to_check_ip_queue.put((ip, time_now + 10))
-    print(response)
+def get():
-        time.sleep(1)
+        get()
-        if time.time() - self.last_roundtrip_time > 5 * 60 - 5:
+
-            quota_left += quota_list["current"]["quota"]
+    try:
-                continue
+                quota_left += b_q_quota
-            quota_left += b_q_quota
+    except Exception as e:
-    return True, "success"
+    try:
-            try:
+            if time.time() - last_login_center_time > 60:
-                g.login_process = False
+                last_login_center_time = time.time()
-        return worker.get_score()
+        return worker.get_score() * 10
-def request_gae_server(headers, body, url):
+def request_gae_server(headers, body, url, timeout):
-    response = http_dispatch.request(headers, body, url)
+    response = http_dispatch.request(headers, body, url, timeout)
-            raise GAE_Exception("get protocol head fail")
+            raise GAE_Exception(600, "get protocol head fail")
-def request_gae_proxy(method, url, headers, body):
+def request_gae_proxy(method, url, headers, body, timeout=60):
-        if time.time() - time_request > 60:  # time out
+        if time.time() - time_request > timeout:
-            response = request_gae_server(request_headers, request_body, url)
+            response = request_gae_server(request_headers, request_body, url, timeout)
-        self.last_request_time = self.ssl_sock.create_time
+        self.transfered_size = 0
-        return self.rtt + 100
+    def record_active(self, active=""):
-            xlog.debug("http1 get task")
+            # xlog.debug("http1 get task")
-
+
-
+import threading
-    def __init__(self, headers, body, queue, url):
+    def __init__(self, headers, body, queue, url, timeout):
-    def get_worker(self):
+    def get_worker(self, nowait=False):
-                rtt = worker.get_rtt_rate()
+                rtt = worker.get_score()
-            if best_worker:
+            if best_worker or nowait:
-    def request(self, headers, body, url):
+    def request(self, headers, body, url, timeout):
-        task = http_common.Task(headers, body, q, url)
+        task = http_common.Task(headers, body, q, url, timeout)
-                               seq, self.next_cmd_seq)
+                # xlog.warn("put_send_data %s conn:%d seq:%d next:%d",
-from cloudflare_front import front
+import front_dispatcher
-    g.http_client = front.front
+    g.http_client = front_dispatcher
-    def request(self, method, host, path="/", header={}, data="", timeout=120):
+    def request(self, method, host, path="/", headers={}, data="", timeout=120):
-                response = dispatcher.request(method, host, path, header, data, timeout=timeout)
+                response = dispatcher.request(method, host, path, headers, data, timeout=timeout)
-                    xlog.warn("front request %s %s%s fail, status:%d", method, host, path, status)
+                    xlog.warn("front request %s %s%s fail, status:%d trace:%s",
-
+        self.last_active_time = self.ssl_sock.create_time
-    def get_worker(self):
+    def get_worker(self, nowait=False):
-                    score = rtt + (3/inactive_time)*1000
+                score = worker.get_score()
-            if best_worker:
+            if best_worker or nowait:
-                        xlog.debug("DATA conn_id %d not in list", conn_id)
+                        #xlog.debug("DATA conn_id %d not in list", conn_id)
-                        self.download_order_queue.put(sn, data)
+                        # self.download_order_queue.put(sn, data)
-                                                     header={"Content-Type": "application/json"},
+                                                     headers={"Content-Type": "application/json"},
-                xlog.exception("%s Unexpected stream identifier %d, e:%r", self.ip, frame.stream_id, e)
+                if frame.type != WindowUpdateFrame.type:
-    # the following is confirm exit
+        # xlog.debug("%s create stream %d", self.ssl_sock.ip, stream_id)
-    def start_request(self):
+    def start(self):
-def restart_xxnet(version):
+def restart_xxnet(version=None):
-    start_script = os.path.join(top_path, "code", version, "launcher", "start.py")
+    if version is None:
-    #os._exit(0)
+    # new process will call http://127.0.0.1:8085/quit
-    if sni is None:
+def connect_ssl(ip, port=443, timeout=5, top_domain=None):
-        xlog.debug("sni:%s", sni)
+    else:
-        top_domain = sni
+    top_domain = str(top_domain)
-def test_xtunnel_ip2(ip, sni=None, sub="scan1"):
+def test_xtunnel_ip2(ip, sub="scan1", top_domain=None):
-        ssl_sock = connect_ssl(ip, sni=sni, timeout=max_timeout)
+        ssl_sock = connect_ssl(ip, timeout=max_timeout, top_domain=top_domain)
-            sni = sys.argv[2]
+            top_domain = sys.argv[2]
-        res = test_xtunnel_ip2(ip, sni=sni)
+            top_domain = None
-                       ip, handshake_time, ssl_sock.h2, ssl_sock.sni, ssl_sock.top_domain)
+                       ip, ssl_sock.handshake_time, ssl_sock.h2, ssl_sock.sni, ssl_sock.top_domain)
-                xlog.debug("connect %s fail:%s cost:%d h:%d", ip, e, time_cost * 1000, handshake_time)
+                xlog.debug("connect %s fail:%s cost:%d ", ip, e, time_cost * 1000)
-            return
+    @staticmethod
-            fd.write(content)
+                front_domains_fn = os.path.join(config.DATA_PATH, "front_domains.json")
-        check_ip.update_front_domains()
+                next_update_time = time.time() + (4 * 3600)
-                        dns_name.append(str(component.getComponent()))
+                        n = str(component.getComponent())
-def test_xtunnel_ip2(ip, sub="scan1"):
+def test_xtunnel_ip2(ip, sni=None, sub="scan1"):
-        ssl_sock = connect_ssl(ip, timeout=max_timeout)
+        ssl_sock = connect_ssl(ip, sni=sni, timeout=max_timeout)
-        res = test_xtunnel_ip2(ip)
+        if len(sys.argv) > 2:
-        if test_version != skip_version:
+        if test_version != config.get(["update", "skip_test_version"]):
-        if stable_version != skip_version:
+        if stable_version != config.get(["update", "skip_stable_version"]):
-                data = '{"res":"success"}'
+                skip_version_type = reqs['skip_version_type'][0]
-        self.address = address
+        if isinstance(address, str):
-      ]
+
-    handshake_time = int((time_handshaked - time_connected) * 1000)
+    handshake_time = int((time_handshaked - time_begin) * 1000)
-                xlog.exception("%s Unexpected stream identifier %d", self.ip, frame.stream_id)
+            except KeyError as e:
-                    scan_ip_log.info("Add %s time:%d CN:%s ", ip, result.handshake_time, result.domain)
+                    xlog.info("scan_ip add ip:%s time:%d", ip, result.request_time)
-
+from front import front
-    response = front.get("GET", "center6.xx-net.net", "/", {}, "")
+    content, status, response = front.request("GET", "center6.xx-net.net", "/")
-            #opener.addheaders += [('Range', 'bytes=0-')]
+
-#from ndg.httpsclient.subj_alt_name import SubjectAltName
+# http://docs.python.org/dev/library/ssl.html
-ns = ['alouc.com', 'alouc.net', 'baonhat.com', 'baouc.us', 'bellsmarden.co.uk', 'bitshares.com.ua',
+ns =[ ["xx-net.net", ['alouc.com', 'alouc.net', 'baonhat.com', 'baouc.us', 'bellsmarden.co.uk', 'bitshares.com.ua',
-      'yeunuocnhat.com']
+      'yeunuocnhat.com']],
-    ip_port = (ip, port)
+def connect_ssl(ip, sni=None, port=443, timeout=5, top_domain=None):
-    ssl_sock.set_tlsext_host_name(host)
+    ssl_sock.set_tlsext_host_name(sni)
-        ssl_sock.h2 = False
+    try:
-            raise socket.error(' certficate is issued by %r, not COMODO' % ( issuer_commonname))
+    issuer_commonname = next((v for k, v in cert.get_issuer().get_components() if k == 'CN'), '')
-    xlog.debug("alt names:%s", alt_names)
+    if __name__ == "__main__":
-
+def test_xtunnel_ip2(ip, sub="scan1"):
-        ssl_sock = connect_ssl(ip, sni, timeout=max_timeout)
+        ssl_sock = connect_ssl(ip, timeout=max_timeout)
-        res = test_xtunnel_ip2(ip, host)
+        res = test_xtunnel_ip2(ip)
-g_cacertfile = os.path.join(current_path, "cacert.pem")
+from config import config
-        self.host = host
+        sub, top = utils.split_domain(host)
-
+        port = ip_port[1]
-                    ssl_sock.h2 = False
+            ssl_sock = check_ip.connect_ssl(ip, port=port, timeout=self.connect_timeout)
-            ssl_sock.create_time = time_begin
+            xlog.debug("create_ssl update ip:%s time:%d h2:%d sni:%s top:%s",
-            ssl_sock.sni = sni
+            ssl_sock.host = self.sub + "." + ssl_sock.top_domain
-                xlog.debug("%s %s%s trace:%s", method, host, path, response.task.get_trace())
+                xlog.debug("%s %s%s trace:%s", method, response.ssl_sock.host, path, response.task.get_trace())
-#update_info = '{"type":"stable", "version":"3.3.3"}'
+            #opener.addheaders += [('Range', 'bytes=0-')]
-                content = response.task.read(size=length)
+                content = response.task.read_all()
-        conn = hyper.HTTP20Connection(ssl_sock, host=host, ip=ip, port=443)
+        conn = hyper.HTTP20Connection(ssl_sock, host=host, ip=ssl_sock.ip, port=443)
-        xlog.debug("ip:%s http/1.1:%r", ip, e )
+        xlog.debug("ip:%s http/1.1:%r", ssl_sock.ip, e )
-    xlog.debug("ip:%s http/2", ip)
+    xlog.debug("ip:%s http/2", ssl_sock.ip)
-        xlog.warn("app check ip:%s status:%d", ip, response.status)
+        xlog.warn("app check ip:%s status:%d", ssl_sock.ip, response.status)
-                        if not check_local_network.check_ipv6():
+                        if not check_local_network.check_ipv6() and False:
-    if False:
+    if True:
-            xlog.debug("create_ssl update ip:%s time:%d h2:%d", ip, handshake_time, ssl_sock.h2)
+            xlog.debug("create_ssl update ip:%s time:%d h2:%d sni:%s", ip, handshake_time, ssl_sock.h2, sni)
-                self.task.put_data("")
+                self.task.finish()
-                xlog.debug("recv fail:%r", e)
+                xlog.exception("recv fail:%r", e)
-        return self.rtt + len(self.streams) * 100
+        return self.rtt + len(self.streams) * 3000
-        #xlog.debug("%s close stream:%d %s", self.ssl_sock.ip, stream_id, reason)
+        # xlog.debug("%s close stream:%d %s", self.ssl_sock.ip, stream_id, reason)
-                xlog.error("%s Unexpected stream identifier %d", self.ip, frame.stream_id)
+                xlog.exception("%s Unexpected stream identifier %d", self.ip, frame.stream_id)
-
+
-
+import time
-        while self.remote_window_size and self.request_body_left:
+        while self.remote_window_size and not self.request_body_sended:
-
+        # xlog.debug("stream %d recved frame %r", self.stream_id, frame)
-            self.task.put_data(frame.data)
+            if not self.task.finished:
-            # tear down the entire connection. TODO: actually do that.
+            # tear down the entire connection.
-            self.send_response()
+
-            if time_cost > 0:
+            if time_cost > 0 and \
-            xlog.error("http2_stream send_response but responsed.%s", self.task.url)
+            xlog.warn("http2_stream send_response but responsed.%s", self.task.url)
-        if False:
+        if True:
-                xlog.debug("block for none")
+                # xlog.debug("block for none")
-                xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d", transfer_no, send_data_len, send_ack_len)
+                # xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d", transfer_no, send_data_len, send_ack_len)
-                        xlog.error("roundtrip time:%d transfer_no:%d sn:%d send:%d len:%d status:%r retry:%d",
+                        xlog.warn("roundtrip time:%d transfer_no:%d sn:%d send:%d len:%d status:%r retry:%d",
-        if update_rule == "dont-check":
+        if update_from_github.update_info == "dont-check":
-        if update_rule != "stable" and update_rule != "test":
+        update_rule = config.get(["update", "check_update"], "notice-stable")
-                update_from_github.update_version(versions[1][1])
+        skip_version = config.get(["update", "skip_version"])
-    global update_progress
+    global update_progress, update_info
-            else:
+            elif config.get(['modules', 'x_tunnel', 'auto_start'], 0) == 1:
-                check_update = "stable"
+            check_update = config.get(["update", "check_update"], "notice-stable")
-            if 'check_update' in reqs:
+            if 'skip_version' in reqs:
-                if check_update not in ["dont-check", "stable", "test"]:
+                if check_update not in ["dont-check", "stable", "notice-stable", "test", "notice-test"]:
-                    config.save()
+                    if config.get(["update", "check_update"]) != check_update:
-        if reqs['cmd'] == ['get_progress']:
+        if reqs['cmd'] == ['get_info']:
-                    raise socket.error('The intermediate CA is mismatching.')
+                if hasattr(OpenSSL.crypto, "dump_publickey"):
-    host = random.choice(ns)
+    sni = random.choice(ns)
-        ssl_sock = connect_ssl(ip, host, timeout=max_timeout)
+        ssl_sock = connect_ssl(ip, sni, timeout=max_timeout)
-            ssl_sock.set_tlsext_host_name(host)
+            sni = random.choice(ns)
-                xlog.warn("%s appid:%s head fail status:%d", self.ip, self.ssl_sock.appid, status)
+                xlog.warn("%s host:%s head fail status:%d", self.ip, self.ssl_sock.host, status)
-                target_menu = 'about'
+                target_module = 'x_tunnel'
-                    self.process_connect(sock, address)
+                        continue
-        xlog.warn("ip:%s status:%d", ip, response.status)
+        xlog.warn("ip:%s status:%d", ssl_sock.ip, response.status)
-                                                                    timeout=server_timeout + 4)
+                                                                    timeout=server_timeout + g.config.network_timeout)
-                if not cert:
+                #cert = ssl_sock.get_peer_certificate()
-                if not issuer_commonname.startswith('Google'):
+                if len(certs) < 3:
-                    raise socket.error(' certficate is issued by %r, not Google' % ( issuer_commonname))
+                    raise socket.error('The intermediate CA is mismatching.')
-    main()
+        if config is None:
-        return logger_instance
+        return logger_instance
-import httplib
+import random
-
+ns = ['alouc.com', 'alouc.net', 'baonhat.com', 'baouc.us', 'bellsmarden.co.uk', 'bitshares.com.ua',
-            ssl_sock.set_tlsext_host_name(self.host)
+
-        # sock.setblocking(0)
+        sock.setblocking(0)
-        # sock.setblocking(0)
+        sock.setblocking(0)
-        #self.connection.setblocking(0)
+        self.connection.setblocking(0)
-                response = dispatcher.request(method, host, path, header, data)
+                response = dispatcher.request(method, host, path, header, data, timeout=timeout)
-        timeout = 8
+        timeout = task.timeout
-            xlog.exception("%s h1_request:%r inactive_time:%d", self.ip, e, time.time()-self.last_active_time)
+            xlog.exception("%s h1_request:%r inactive_time:%d task.timeout:%d",
-            xlog.exception("read fail, ip:%s, chunk:%d url:%s e:%r", self.ip, response.chunked, task.url, e)
+            xlog.exception("read fail, ip:%s, chunk:%d url:%s task.timeout:%d e:%r",
-    def __init__(self, method, host, path, headers, body, queue, url):
+    def __init__(self, method, host, path, headers, body, queue, url, timeout):
-                if inactive_time > 2:
+                if inactive_time > 3:
-                    score = rtt + (2/inactive_time)*1000
+                    score = rtt + (3/inactive_time)*1000
-    def request(self, method, host, path, headers, body, url=""):
+    def request(self, method, host, path, headers, body, url="", timeout=60):
-        task = http_common.Task(method, host, path, headers, body, q, url)
+        task = http_common.Task(method, host, path, headers, body, q, url, timeout)
-                server_timeout = g.config.roundtrip_timeout / 2
+                if last_roundtrip_download_size >= g.config.block_max_size:
-                                                                    timeout=g.config.roundtrip_timeout)
+                                                                    timeout=server_timeout + 4)
-#monkey.patch_all()
+
-                        xlog.warn("Transfer-Encoding e:%r ", e)
+                        # xlog.warn("Transfer-Encoding e:%r ", e)
-            if self.command == "GET":
+            if self.upgrade == "websocket":
-        logging.debug("GET %s from %s:%d", self.path, self.client_address[0], self.client_address[1])
+        self.logger.debug("GET %s from %s:%d", self.path, self.client_address[0], self.client_address[1])
-    config.set_var("concurent_thread_num", 20)
+    config.set_var("concurent_thread_num", 50)
-    config.set_var("send_delay", 100)
+    config.set_var("send_delay", 10)
-    config.set_var("roundtrip_timeout", 25)
+    config.set_var("roundtrip_timeout", 10)
-    config.windows_ack = 0.2 * config.windows_size
+    config.windows_ack = 0.05 * config.windows_size
-    g.http_client = front.Front()
+    g.http_client = front.front
-        #terminate()
+        terminate()
-
+import simple_http_client
-    #xlog.debug("conn: %d  handshake:%d", conncet_time, handshake_time)
+    xlog.debug("conn: %d  handshake:%d", connect_time, handshake_time)
-    response.begin()
+    response = simple_http_client.Response(ssl_sock)
-    content = response.read()
+    content = response.read(timeout=1)
-            xlog.warn("check fail:%r", e)
+            xlog.exception("check fail:%r", e)
-                return content, status, heads
+                return content, status, response
-        connect_control.keep_running = False
+        connect_control.keep_running = False
-import Queue
+import simple_http_client
-        threading.Thread(target=self.keep_alive_thread).start()
+
-            self.task_queue.put("ping")
+        self.task_queue.put("ping")
-            if time_now - self.last_active_time > self.idle_time:
+            if not self.request_onway and time_now - self.last_active_time > self.idle_time:
-
+            self.request_task(task)
-            self.request_task(task)
+
-            response.begin()
+            task.set_state("h1_req_sended")
-            ip_manager.report_connect_closed(self.ssl_sock.ip, "request_fail")
+            xlog.exception("%s h1_request:%r inactive_time:%d", self.ip, e, time.time()-self.last_active_time)
-            self.last_active_time = time.time()
+        time_left = timeout - (time.time() - start_time)
-            self.close("request body fail")
+        response.worker = self
-
+        self.request_onway = True
-        # xlog.debug("head request %s", self.ssl_sock.ip)
+        xlog.debug("head request %s", self.ip)
-            response.begin()
+            response = simple_http_client.Response(self.ssl_sock)
-                xlog.debug("%s appid:%s head fail status:%d", self.ip, self.ssl_sock.appid, status)
+                xlog.warn("%s appid:%s head fail status:%d", self.ip, self.ssl_sock.appid, status)
-
+            content = response.readall(timeout=5)
-            xlog.debug("%s keep alive fail, inactive_time:%d head_timeout:%d",
+            xlog.warn("%s keep alive fail, inactive_time:%d head_timeout:%d",
-            xlog.debug("h1 %s appid:%s HEAD keep alive request fail:%r", self.ssl_sock.ip, self.ssl_sock.appid, e)
+            xlog.warn("h1 %s HEAD keep alive request fail:%r", self.ssl_sock.ip, e)
-        # xlog.debug("%s stat:%s", self.url, stat)
+        # xlog.debug("%s stat:%s", self.unique_id, stat)
-        out_list.append(":%d" % (time.time()-last_time))
+        out_list.append(":%d" % ((time.time()-last_time)*1000))
-        self.init_rtt = ssl_sock.handshake_time / 3
+        self.init_rtt = ssl_sock.handshake_time / 2
-            best_rtt = 9999
+            best_score = 99999999
-                    best_rtt = rtt
+                if best_score > score:
-            if idle_num < 5:
+            if best_worker is None or idle_num < 5 or (now - best_worker.last_active_time) < 2:
-            out_str += " Speed:"
+            elif w.version == "1.1":
-                out_str += "%d," % speed
+               out_str += "%d," % speed
-        out_str += "\r\n<br> working_tasks:\r\n"
+        out_str += "\r\n working_tasks:\r\n"
-
+from front import front
-        elif path == "/config":
+        if path == "/config":
-        elif path == "/config":
+        if path == "/config":
-            good_ip_num = len(google_ip.gws_ip_list)
+        good_ip_num = ip_manager.good_ip_num
-                   "use_ipv6": config.CONFIG.getint("google_ip", "use_ipv6"),
+                   "use_ipv6": config.CONFIG.getint("ip_manager", "use_ipv6"),
-                   "ip_num": len(google_ip.gws_ip_list),
+                   "ip_num": len(ip_manager.gws_ip_list),
-                   "ip_quality": google_ip.ip_quality(),
+                   "scan_ip_thread_num": ip_manager.scan_thread_count,
-                    if appids and google_ip.good_ip_num:
+                    if appids and ip_manager.good_ip_num:
-                google_ip.reset()
+                ip_manager.reset()
-        data += "time:%d  pointer:%d<br>\r\n" % (time_now, google_ip.gws_ip_pointer)
+        data += "time:%d  pointer:%d<br>\r\n" % (time_now, ip_manager.gws_ip_pointer)
-            handshake_time = google_ip.ip_dict[ip]["handshake_time"]
+        for ip in ip_manager.gws_ip_list:
-            links = google_ip.ip_dict[ip]["links"]
+            fail_times = ip_manager.ip_dict[ip]["fail_times"]
-            get_time = google_ip.ip_dict[ip]["get_time"]
+            get_time = ip_manager.ip_dict[ip]["get_time"]
-            success_time = google_ip.ip_dict[ip]["success_time"]
+            success_time = ip_manager.ip_dict[ip]["success_time"]
-            fail_time = google_ip.ip_dict[ip]["fail_time"]
+            fail_time = ip_manager.ip_dict[ip]["fail_time"]
-            down_fail_time = google_ip.ip_dict[ip]["down_fail_time"]
+            down_fail_time = ip_manager.ip_dict[ip]["down_fail_time"]
-            data_active = google_ip.ip_dict[ip]["data_active"]
+            data_active = ip_manager.ip_dict[ip]["data_active"]
-            history = google_ip.ip_dict[ip]["history"]
+            history = ip_manager.ip_dict[ip]["history"]
-        data = http_dispatch.to_string()
+        out_str = ""
-            return self.send_not_exist()
+        self.send_response_nc(mimetype, out_str)
-        self.download_order_queue.reset()
+        try:
-        self.roundtrip_thread = {}
+            self.ack_pool.reset()
-        self.traffic = 0
+            self.roundtrip_thread = {}
-            return False
+            self.session_id = utils.generate_random_lowercase(8)
-        return True
+            self.last_roundtrip_time = time.time()
-            content, status, heads = g.http_client.request(method="POST", host=g.server_host, path="/data",
+            content, status, response = g.http_client.request(method="POST", host=g.server_host, path="/data",
-            if self.on_road_num > g.config.concurent_thread_num * 0.8:
+            if self.on_road_num > g.config.concurent_thread_num * 0.9:
-            elif last_roundtrip_download_size > g.config.block_max_size:
+                xlog.debug("block for too many on road")
-                # xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d", transfer_no, send_data_len, send_ack_len)
+                xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d", transfer_no, send_data_len, send_ack_len)
-                        (time.time() - start_time) * 1000, time_cost, transfer_no, send_sn, send_data_len, sn,
+                        roundtrip_time, time_cost, transfer_no, send_sn, send_data_len, sn,
-        content, status, heads = g.http_client.request(method="POST", host=g.config.api_server, path=path,
+        content, status, response = g.http_client.request(method="POST", host=g.config.api_server, path=path,
-    if update_server:
+    if g.config.server_host:
-        time.sleep(1)
+login_lock = threading.Lock()
-                xlog.warn("x-tunnel request_balance fail when create_conn:%s", reason)
+def create_conn(sock, host, port):
-                if not g.session.start():
+            g.login_process = True
-            g.login_process = False
+
-        del self.working_tasks[task.unique_id]
+        try:
-import socket, struct
+import socket
-                xlog.warn("http proxy protocal is not supported now, please use Socks5.")
+            elif socks_version in ["G", "P", "D", "O", "H", "T"]:
-                if filename in ['start.sh', 'start.command', 'start.lnk', 'LICENSE.txt', 'download.md', 'version.txt', 'xxnet', 'xxnet.bat', 'xxnet.vbs', 'xx_net.sh']:
+                if filename in ['start.sh', 'start.command', 'start.lnk', 'LICENSE.txt', 'download.md', 'version.txt', 'xxnet', 'xxnet.bat', 'xxnet.vbs']:
-        for i in range(0, 50):
+        scan_thread_num = config.CONFIG.getint("google_ip", "max_scan_ip_thread_num")
-if sys.platform.startswith("linux"):
+
-    config.set_var("api_server", "center.xx-net.online")
+    config.set_var("api_server", "center6.xx-net.net")
-def test_xtunnel_ip2(ip, host="scan1.xx-net.online"):
+def test_xtunnel_ip2(ip, host="scan1.xx-net.net"):
-        host = "scan1.xx-net.online"
+        host = "scan1.xx-net.net"
-        # self.start_scan_all_exist_ip()
+
-            self.start_scan_all_exist_ip()
+        # if file_path == self.default_good_ip_file:
-    response = front.get("GET", "center.xx-net.online", "/", {}, "")
+    response = front.get("GET", "center6.xx-net.net", "/", {}, "")
-
+        for _ in range(5):
-                        xlog.debug("new_conn_pool.get:%s handshake:%d", ssl_sock.ip, handshake_time)
+                        # xlog.debug("new_conn_pool.get:%s handshake:%d", ssl_sock.ip, handshake_time)
-            if not ip_utils.check_ip_valid(ip):
+            try:
-            ip_num = ip_utils.ip_string_to_num(ip)
+        while self.bin_fd is None:
-        self.ip_range = google_ip_range.ip_range
+        self.ip_pool = None
-                ip = self.ip_range.get_ip()
+                ip = self.get_ip_to_scan()
-current_path = os.path.dirname(os.path.abspath(__file__))
+class IpPool(object):
-ip_range = IpRange()
+
-            print "merge:", ip_utils.ip_num_to_string(last_begin), ip_utils.ip_num_to_string(last_end), ip_utils.ip_num_to_string(begin), ip_utils.ip_num_to_string(end)
+            # print "merge:", ip_utils.ip_num_to_string(last_begin), ip_utils.ip_num_to_string(last_end), ip_utils.ip_num_to_string(begin), ip_utils.ip_num_to_string(end)
-    print_range_list(ip_range_list)
+    # PRINT("Good ip range:\n")
-        print_range_list(ip_range_list)
+        # PRINT("Bad ip range:\n")
-        print_range_list(ip_range_list)
+        # PRINT("Output ip range:\n")
-        print ip_utils.ip_num_to_string(nbegin), ip_utils.ip_num_to_string(nend), num
+        # print ip_utils.ip_num_to_string(nbegin), ip_utils.ip_num_to_string(nend), num
-                xlog.debug("get ip:%s t:%d", ip, handshake_time)
+                # xlog.debug("get ip:%s t:%d", ip, handshake_time)
-            xlog.debug("ssl_closed %s", ip)
+            # xlog.debug("ssl_closed %s", ip)
-                    xlog.debug("ssl_closed %s", ip)
+                    # xlog.debug("ssl_closed %s", ip)
-    def __init__(self, sock, client, args):
+    def __init__(self, sock, client, args, logger=logging):
-        self.wfile = socket._fileobject(self.connection, "wb", self.wbufsize)
+        sock.settimeout(300)
-        #logging.info('Connected from %r', self.client_address)
+        #self.logger.info('Connected from %r', self.client_address)
-                #logging.warn("handle err:%r close", e)
+                #self.logger.warn("handle err:%r close", e)
-        #logging.debug("closed from %s:%d", self.client_address[0], self.client_address[1])
+        #self.logger.debug("closed from %s:%d", self.client_address[0], self.client_address[1])
-                #logging.warn("simple server handle except %r", e)
+                #self.logger.warn("simple server handle except %r", e)
-                #logging.warn("recv command line too large")
+                #self.logger.warn("recv command line too large")
-                #logging.warn("closed")
+                #self.logger.warn("closed")
-                logging.warn("unhandler cmd:%s path:%s from:%s", self.command, self.path, self.address_string())
+                self.logger.warn("unhandler cmd:%s path:%s from:%s", self.command, self.path, self.address_string())
-            
+
-            #logging.warn("socket error:%r", e)
+            #self.logger.warn("socket error:%r", e)
-                logging.warn("PIPE error:%r", e)
+                self.logger.warn("PIPE error:%r", e)
-                logging.warn("IOError:%r", e)
+                self.logger.warn("IOError:%r", e)
-        #    logging.warn("socket error:%r", e)
+        #    self.logger.warn("socket error:%r", e)
-            logging.exception("handler:%r cmd:%s path:%s from:%s", e,  self.command, self.path, self.address_string())
+            self.logger.exception("handler:%r cmd:%s path:%s from:%s", e,  self.command, self.path, self.address_string())
-        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
+        self.logger.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
+        self.logger.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
+        self.logger.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
+        self.logger.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
+        self.logger.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
+        self.logger.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
+        self.logger.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-            #logging.warn("download broken")
+            #self.logger.warn("download broken")
-    def __init__(self, address, handler, args=(), use_https=False, cert=""):
+    def __init__(self, address, handler, args=(), use_https=False, cert="", logger=logging):
-            #server can listen multi-port
+            # server can listen multi-port
-        #logging.info("server %s:%d started.", address[0], address[1])
+        #self.logger.info("server %s:%d started.", address[0], address[1])
-            logging.error(err_string)
+            self.logger.error(err_string)
-            #server.pem's location (containing the server private key and the server certificate).
+            # server.pem's location (containing the server private key and the server certificate).
-        logging.info("server %s:%d started.", addr[0], addr[1])
+        self.logger.info("server %s:%d started.", addr[0], addr[1])
-                self.process_connect(sock, address)
+        if hasattr(select, 'epoll'):
-        #logging.debug("connect from %s:%d", address[0], address[1])
+        #self.logger.debug("connect from %s:%d", address[0], address[1])
-    http_thread.start()
+    httpd.start()
-        
+
-    
+
-        out_string += "block_list:<br>\n"
+        out_string += " block_list: %d<br>\n" % len(self.block_list)
-            out_string += "[%d] len:%d<br>\r\n" % (sn, len(data))
+            out_string += "  [%d] len:%d<br>\r\n" % (sn, len(data))
-        out_string += "waiters:<br>\n"
+        out_string += " waiters: %d<br>\n" % len(self.waiters)
-            out_string += "%d<br>\r\n" % ((end_time - time.time()))
+            out_string += "  %d<br>\r\n" % ((end_time - time.time()))
-        self.create_more_connection()
+
-        return "", 500, {}
+        return "", 500, {}
-    idle_time = 370
+    idle_time = 360
-            if time_now - self.last_request_time > self.idle_time:
+            if time_now - self.last_active_time > self.idle_time:
-
+                # not work now.
-        # for keep alive
+        # for keep alive, not work now.
-            task.set_state("get_task")
+            task.set_state("get_task(%d)" % get_cost)
-                xlog.debug("get ip:%s t:%d", ip, handshake_time)
+                # xlog.debug("get ip:%s t:%d", ip, handshake_time)
-            xlog.debug("ssl_closed %s", ip)
+            # xlog.debug("ssl_closed %s", ip)
-                    xlog.debug("ssl_closed %s", ip)
+                    # xlog.debug("ssl_closed %s", ip)
-import urlparse
+        self.last_roundtrip_time = time.time()
-        self.start()
+        return self.start()
-        out_string += "transfer_list:<br>\r\n"
+        out_string += "transfer_list: %d<br>\r\n" % len(self.transfer_list)
-                xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d", transfer_no, send_data_len, send_ack_len)
+                # xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d", transfer_no, send_data_len, send_ack_len)
-                        self.reset()
+                        if self.session_id == request_session_id:
-    255.255.255.255/32      #for algorithm
+    special_bad_ip_range_lines = """
-    if False:
+    if True:
-        if url_path == '/':
+        if url_path == "/test":
-        host = "scan5.xx-net.online"
+        host = "scan1.xx-net.online"
-        xlog.debug("head request %s", self.ssl_sock.ip)
+        # xlog.debug("head request %s", self.ssl_sock.ip)
-        self.start_scan_all_exist_ip()
+        # self.start_scan_all_exist_ip()
-            xlog.exception('socks handler read error %r', e)
+            xlog.warn('socks handler read error %r', e)
-            xlog.info("login_session time:%d msg:%s", 1000 * time_cost, message)
+            xlog.info("login_session %s time:%d msg:%s", self.session_id, 1000 * time_cost, message)
-                # xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d", transfer_no, send_data_len, send_ack_len)
+                xlog.debug("start roundtrip transfer_no:%d send_data_len:%d ack_len:%d", transfer_no, send_data_len, send_ack_len)
-            self.close_connection = 0
+            
-            pass
+            self.close_connection = 1
-            pass
+            self.close_connection = 1
-                self.close("keep alive")
+                self.close("inactive timeout %d" % (time_now - self.last_active_time))
-        res, info = proxy_session.call_api("order", {
+        res, info = proxy_session.call_api("/order", {
-        res, info = proxy_session.call_api("transfer", req_info)
+        res, info = proxy_session.call_api("/transfer", req_info)
-        res, info = proxy_session.call_api("get_history", req_info)
+        res, info = proxy_session.call_api("/get_history", req_info)
-        proxy_session.request_balance(g.config.login_account, g.config.login_password)
+    #if g.config.login_account:
-        return False
+    time_cost = (time.time() - start_time) * 1000
-        xlog.warn("ip:%s not support http/2", ip)
+    return True
-            return False
+def check_xtunnel_http2(ssl_sock, host):
-    xlog.info("check_xtunnel ok")
+    time_cost = (time.time() - start_time) * 1000
-        host = "scan1.xx-net.online"
+        host = "scan5.xx-net.online"
-                (self.new_conn_pool.qsize() == 0):
+                (self.new_conn_pool.qsize() < self.connection_pool_min):
-            ip_manager.update_ip(ip, handshake_time)
+            # ip_manager.update_ip(ip, handshake_time)
-
+xlog.set_buffer(500)
-                if status != 200:
+                if status not in [200, 405]:
-        ping_interval = 360
+        ping_interval = 10
-        if time.time() - self.ssl_sock.create_time > 9:
+        time_now = time.time()
-            xlog.warn("%s h1_request:%r", self.ip, e)
+            xlog.warn("%s h1_request:%r inactive_time:%d", self.ip, e, time.time()-self.last_active_time)
-        self.close("request body fail")
+            task.put_data("")
-        # xlog.debug("head request %s", host)
+        xlog.debug("head request %s", self.ssl_sock.ip)
-    def on_ssl_created_cb(self, ssl_sock, check_free_worke=True):
+    def on_ssl_created_cb(self, ssl_sock, check_free_work=True):
-        if check_free_worke:
+        if check_free_work:
-                self.on_ssl_created_cb(ssl_sock, check_free_worke=False)
+                self.on_ssl_created_cb(ssl_sock, check_free_work=False)
-
+        self.start_scan_all_exist_ip()
-                self.add_ip(ip, result.handshake_time, result.domain, "gws")
+            if result and result.support_xtunnel:
-            self.add_ip(ip, result.handshake_time, result.domain, "gws")
+            self.add_ip(ip, result.request_time, result.domain, "gws")
-                if self.add_ip(ip, result.handshake_time, result.domain, "gws"):
+                if self.add_ip(ip, result.request_time, result.domain, "gws"):
-                self.add_ip(ip, result.handshake_time, result.domain, "gws")
+                self.add_ip(ip, result.request_time, result.domain, "gws")
-            path = path[17:]
+            path = self.path[17:]
-
+https_max_connect_thread = config.getint("connect_manager", "https_max_connect_thread")
-        if high_prior_connecting_num + low_prior_connecting_num > config.https_max_connect_thread:
+        if high_prior_connecting_num + low_prior_connecting_num > https_max_connect_thread:
-        if high_prior_connecting_num + low_prior_connecting_num < config.https_max_connect_thread:
+        if high_prior_connecting_num + low_prior_connecting_num < https_max_connect_thread:
-from connect_manager import https_manager
+
-from http_dispatcher import http_dispatch
+from cloudflare_front import web_control as cloudflare_web
-        sock = socket.socket(socket.AF_INET)
+    def create_sock(self, host):
-    def get_conn(self):
+    def get_conn(self, host):
-            sock = self.create_sock()
+            sock = self.create_sock(host)
-        conn = self.get_conn()
+        conn = self.get_conn(host)
-        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+        if ":" in addr[0]:
-                                                                             seq, self.next_cmd_seq))
+                xlog.warn("put_send_data %s conn:%d seq:%d next:%d",
-    config.set_var("api_server", "http://center.xx-net.net:8888/")
+    config.set_var("api_server", "center.xx-net.online")
-    config.set_var("use_https", 0)
+    config.set_var("use_https", 1)
-
+    g.http_client = front.Front()
-proxy = None
+http_client = None
-        sock.send(b'HTTP/1.1 200 OK\r\n\r\n')
+        try:
-            rthead.join()
+            try:
-                                                         timeout=g.config.roundtrip_timeout)
+            content, status, heads = g.http_client.request(method="POST", host=g.server_host, path="/data",
-                    content, status, response = http_client.request(method="POST", path="data", data=upload_post_data,
+                    content, status, response = g.http_client.request(method="POST", host=g.server_host,
-        content, status, heads = http_client.request(method="POST", path=path,
+        content, status, heads = g.http_client.request(method="POST", host=g.config.api_server, path=path,
-        login_path = "register"
+        login_path = "/register"
-        login_path = "login"
+        login_path = "/login"
-        return altnames
+    cert_validity_years = 2
-        cert.gmtime_adj_notAfter(CertUtil.ca_validity)
+        cert.gmtime_adj_notAfter(CertUtil.cert_validity)
-
+    def _get_cert_cn(commonname, full_name=False):
-        if os.path.exists(certfile):
+            yield '.' + commonname.partition('.')[-1]
-        elif OpenSSL is None:
+
-                if os.path.exists(certfile):
+                certfile = CertUtil._get_old_cert(commonname, full_name)
-            thread.start_new_thread(PacUtil.update_pacfile, (user_pacfile,))
+        # STOP update PAC
-    data = 'HTTP/1.1 %d %s\r\n%s\r\n\r\n%s' % 
+    data = 'HTTP/1.1 %d %s\r\n%s\r\n\r\n%s' % \
-
+from instances import xlog
-        self.PROXY_HOSTS_ONLY = [x.strip() for x in self.CONFIG.get('switch_rule', 'proxy_hosts_only').split("|")]
+        self.PROXY_HOSTS_ONLY = []
-                if not self.qsize(only_h1=only_h1):
+                if self.qsize(only_h1=only_h1) == 0:
-                while not self.qsize(only_h1=only_h1):
+                while self.qsize(only_h1=only_h1) == 0:
-                #xlog.debug("create ssl conn %s", ip_str)
+            start_time = time.time()
-                ret = self.new_conn_pool.get(True, self.max_timeout, only_h1=only_h1)
+                ret = self.new_conn_pool.get(True, 1, only_h1=only_h1)
-                    return None
+                    if time.time() - start_time > self.max_timeout:
-from instances import xlog
+from instances import xlog
-        module_menus = sorted(new_module_menus.iteritems(), key=lambda (k,v): (v['menu_sort_id']))
+        module_menus = sorted(new_module_menus.iteritems(),
-    create_desktop_shortcut()
+    create_desktop_shortcut()
-        xlog.info("Only these hosts will proxy: %s", self.ONLYHOSTS)
+        self.PROXY_HOSTS_ONLY = [x.strip() for x in self.CONFIG.get('switch_rule', 'proxy_hosts_only').split("|")]
-            for h in config.ONLYHOSTS:
+
-        if len(config.ONLYHOSTS) > 0:
+
-                self.CONFIG.readfp(io.BytesIO(content))
+        # load ../../../data/gae_proxy/config.ini, set by web_ui
-
+        self.do_profile = config.CONFIG.getint("system", "do_profile")
-                return None
+            while True:
-def request_gae_server(headers, body):
+def request_gae_server(headers, body, url):
-    response = http_dispatch.request(headers, body)
+    response = http_dispatch.request(headers, body, url)
-            response = request_gae_server(request_headers, request_body)
+            response = request_gae_server(request_headers, request_body, url)
-        send_response(wfile, e.type, body=e.message)
+        send_response(wfile, e.error_code, body=e.message)
-        super(HTTP1_worker, self).__init__(ssl_sock, close_cb, retry_task_cb)
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb):
-        ping_interval = 55
+        ping_interval = 55.0
-            time_to_ping = max(ping_interval - (time.time() - self.last_active_time), 0)
+            time_to_ping = max(ping_interval - (time.time() - self.last_active_time), 0.2)
-                # None task to exit
+                # None task means exit
-                self.last_active_time = time.time()
+                    self.accept_task = False
-            # xlog.debug("http1 get task")
+            xlog.debug("http1 get task")
-            self.last_active_time = time_now
+            self.task = None
-                xlog.warn("head send len:%r %d", ret, len(data))
+                xlog.warn("h1 head send len:%r %d %s", ret, len(data), self.ip)
-                xlog.debug("appid:%s head fail status:%d", self.ssl_sock.appid, status)
+                xlog.debug("%s appid:%s head fail status:%d", self.ip, self.ssl_sock.appid, status)
-            inactive_time = time_now - self.ssl_sock.last_use_time
+            inactive_time = time_now - self.last_active_time
-            xlog.warn("%s head appid:%s request fail:%r", self.ssl_sock.ip, self.ssl_sock.appid, e)
+            xlog.debug("h1 %s appid:%s HEAD keep alive request fail:%r", self.ssl_sock.ip, self.ssl_sock.appid, e)
-        super(HTTP2_worker, self).__init__(ssl_sock, close_cb, retry_task_cb)
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb):
-        ssl_sock.settimeout(240)
+        self.ssl_sock.settimeout(240)
-        # this is the export api
+
-                # None frame to exist
+                # None frame means exist
-                time.sleep(0.001)
+                time.sleep(0.01)
-                    xlog.exceptiong("send error:%r", e)
+                    xlog.exception("send error:%r", e)
-                self.close("send fail:%r", e)
+                self.close("send fail:%r" % e)
-                # after get header,
+            if stream.task.responsed:
-        self._sock.send(b'PRI * HTTP/2.0\r\n\r\nSM\r\n\r\n')
+        self.send_queue.put(RawFrame(b'PRI * HTTP/2.0\r\n\r\nSM\r\n\r\n'))
-            xlog.warn("%s _consume_single_frame:%r", self.ip, e)
+            xlog.debug("%s _consume_single_frame:%r, inactive time:%d", self.ip, e, time.time()-self.last_active_time)
-                frame.stream_id, length, FRAME_MAX_LEN)
+            xlog.error("%s Frame size exceeded on stream %d (received: %d, max: %d)",
-                xlog.warn("goaway:%s", error_string)
+                xlog.warn("goaway:%s, t:%d", error_string, time_cost)
-            self.close("GoAway:%s" % error_string)
+            self.close("GoAway:%s inactive time:%d" % (error_string, time_cost))
-            self.close("RESET")
+            # Rest Frame send from server is not define in RFC
-            self.get_head_time = time_now
+            self.task.set_state("h2_get_head")
-                speed = self.task.content_length / time_cose
+            time_cost = time_now - self.get_head_time
-
+        if not self.task.responsed:
-        self.message = message
+    def __init__(self, error_code, message):
-    def __init__(self, headers, body, queue):
+    def __init__(self, headers, body, queue, url):
-        self.trace_time = {}
+        self.unique_id = "%s:%f" % (url, self.start_time)
-        self.trace_time[time_now] = stat
+        self.trace_time.append((time_now, stat))
-        for t, stat in tr_list.items():
+        for t, stat in self.trace_time:
-                out_list.append("%s:%d" % (stat, time_diff))
+            out_list.append("%d:%s" % (time_diff, stat))
-        xlog.debug(err_text)
+        xlog.debug("%s %s", self.url, err_text)
-    def __init__(self, ssl_sock, close_cb, retry_task_cb):
+    def __init__(self, ssl_sock, close_cb, retry_task_cb, idle_cb):
-        self.close_cb(self)
+        self.close_cb(self)
-from config import config
+from utils import SimpleCondition
-    min_worker_num = 20
+        self.working_tasks = {}
-        th.start()
+
-    def on_ssl_created_cb(self, ssl_sock):
+    def on_ssl_created_cb(self, ssl_sock, check_free_worke=True):
-            worker = HTTP2_worker(ssl_sock, self.close_cb, self.retry_task_cb)
+            worker = HTTP2_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
-            worker = HTTP1_worker(ssl_sock, self.close_cb, self.retry_task_cb)
+            worker = HTTP1_worker(ssl_sock, self.close_cb, self.retry_task_cb, self._on_worker_idle_cb)
-        return worker
+        self.wait_a_worker_cv.notify()
-            while True:
+        while connect_control.keep_running:
-                    continue
+            except Exception as e:
-                    time.sleep(10)
+            if not ssl_sock:
-                    break
+            try:
-            self.create_worker_th = None
+            idle_num = 0
-            return
+                if worker.version == "1.1":
-        self.create_worker_th.start()
+            if idle_num > 5 and acceptable_num > 20:
-                continue
+        while connect_control.keep_running:
-                if len(worker.streams) == 0:
+                if worker.version == "1.1":
-            self.create_more_worker()
+                rtt = worker.get_rtt_rate()
-            return best_worker
+                if rtt < best_rtt:
-            raise http_common.GAE_Exception(1, "no ssl_sock")
+            if idle_num < 5:
-        worker = self.on_ssl_created_cb(ssl_sock)
+            if best_worker:
-        return worker
+            self.wait_a_worker_cv.wait()
-        while len(self.workers) > config.max_worker_num:
+    def check_free_worker(self):
-            if idle_num < 3 or slowest_worker is None:
+            if idle_num < 30 or idle_num < int(len(self.workers) * 0.3):
-    def request(self, headers, body):
+    def request(self, headers, body, url):
-        task = http_common.Task(headers, body, q)
+        task = http_common.Task(headers, body, q, url)
-        # xlog.debug("task get response")
+        del self.working_tasks[task.unique_id]
-            except:
+                if task is None:
-            worker.request(task)
+            if worker is None:
-        return time.time() - self.last_request_time > 20 * 60
+        return time.time() - self.last_request_time > self.idle_time
-        for w,r in w_r:
+        for w, r in w_r:
-        if self._sock:
+        if not self.socket_closed:
-            self._sock = None
+            self.socket_closed = True
-        while self._connection:
+        while self.running:
-                _, _, errors = select.select([fd], [], [fd], timeout)
+                _, _, errors = select.select([fd], [], [fd], wait_timeout)
-                if time_now - time_start > timeout:
+                if time_now - time_start > self.timeout:
-                _, _, errors = select.select([], [fd], [fd], timeout)
+                _, _, errors = select.select([], [fd], [fd], wait_timeout)
-                if time_now - time_start > timeout:
+                if time_now - time_start > self.timeout:
-                        _, _, errors = select.select([], [fd], [fd], timeout)
+                        _, _, errors = select.select([], [fd], [fd], wait_timeout)
-                        _, _, errors = select.select([fd], [], [fd], timeout)
+                        _, _, errors = select.select([fd], [], [fd], wait_timeout)
-                    if time_now - time_start > timeout:
+                    if time_now - time_start > self.timeout:
-        while True:
+        while self.running:
-            if self._sock:
+            self.running = False
-                self._sock = None
+                self.socket_closed = True
-    do_profile = False
+    do_profile = config.do_profile
-            return True
+            continue
-            xlog.exception("check_appid %s %r", appid, e)
+            xlog.warn("check_appid %s %r", appid, e)
-
+import threading
-    return ba
+    return ba
-import time
+import os
-    threading.stack_size(128*1024)
+    threading.stack_size(128 * 1024)
-root_path = os.path.abspath( os.path.join(current_path, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir))
-noarch_lib = os.path.abspath( os.path.join(python_path, 'lib', 'noarch'))
+noarch_lib = os.path.abspath(os.path.join(python_path, 'lib', 'noarch'))
-from instances import xlog
+create_data_path()
-    platform_lib = os.path.abspath( os.path.join(python_path, 'lib', 'darwin'))
+    platform_lib = os.path.abspath(os.path.join(python_path, 'lib', 'darwin'))
-        if m == module or m.startswith(module+"."):
+        if m == module or m.startswith(module + "."):
-    print 'Stopping all modules before exit!'
+    print('Stopping all modules before exit!')
-    except KeyboardInterrupt: # Ctrl + C on console
+    except KeyboardInterrupt:  # Ctrl + C on console
-import sys
+import sys
-            #('en_GB', 'cp1252'), en_US,
+            # ('en_GB', 'cp1252'), en_US,
-            #Mac fail to run this
+            # Mac fail to run this
-        if lang_code.find('zh') != -1:
+        if 'zh' in lang_code:
-        elif lang_code.find('en') != -1:
+        elif 'en' in lang_code:
-        elif lang_code.find('fa') != -1:
+        elif 'en' in lang_code:
-                print content[bp:]
+                print(content[bp:])
-                print content[bp:]
+                print(content[bp:])
-            b2p = content.find("\"", b1p+2, b1p + 4)
+            b2p = content.find("\"", b1p + 2, b1p + 4)
-                print content[bp:]
+                print(content[bp:])
-                print content[bp:]
+                print(content[bp:])
-                print content[bp:]
+                print(content[bp:])
-            if key not in po_dict or po_dict[key] == "":
+            key = content[b2p + 1:e2p]
-from instances import xlog
+import sys
-python_path = os.path.abspath( os.path.join(current_path, os.pardir, 'python27', '1.0'))
+python_path = os.path.abspath(os.path.join(current_path, os.pardir, 'python27', '1.0'))
-            #print relate_path
+            # print(relate_path)
-            #print "file:", file
+            # print("root:", path)
-def check_setup(): #40ms
+
-    import uuid
+
-    #config.load()
+    # config.load()
-    print "cost time:", t_c
+    print("cost time:", t_c)
-noarch_lib = os.path.abspath( os.path.join(python_path, 'lib', 'noarch'))
+python_path = os.path.abspath(os.path.join(current_path, os.pardir, 'python27', '1.0'))
-download_path = os.path.abspath( os.path.join(root_path, os.pardir, os.pardir, 'data', 'downloads'))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir))
-                    if not chunk: break
+                    if not chunk:
-    data_path = os.path.abspath( os.path.join(xxnet_unzip_path, 'data', 'launcher', 'config.yaml'))
+    data_path = os.path.abspath(os.path.join(xxnet_unzip_path, 'data',
-        new_config = yaml.load(file(data_path, 'r'))
+        new_config = yaml.load(open(data_path, 'r'))
-        print "Error in configuration file:", exc
+    except yaml.YAMLError as exc:
-    #config.config["modules"]["launcher"]["current_version"] = new_config["modules"]["launcher"]["current_version"]
+    # TODO: fix bug
-def install_xxnet_files():
+def install_xxnet_files():
-        relate_path = root[len(xxnet_unzip_path)+1:]
+        # print("root:", root)
-            #logging.exception("web_control http_request:%s fail:%s", url, e)
+            # logging.exception("web_control http_request:%s fail:%s", url, e)
-        if http_request(req_url) == False:
+        if http_request(req_url) is False:
-def run_new_start_script():
+def run_new_start_script():
-    start_sript = os.path.abspath( os.path.join(current_path, "start.py"))
+    start_sript = os.path.abspath(os.path.join(current_path, "start.py"))
-from distutils.version import LooseVersion
+from instances import xlog
-root_path = os.path.abspath( os.path.join(current_path, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir))
-        print "Error in configuration file:", exc
+        config = yaml.load(open(config_path, 'r'))
-        yaml.dump(config, file(config_path, "w"))
+        yaml.dump(config, open(config_path, "w"))
-    #    set(["modules", "gae_proxy", "control_port"], 8084)
+    # if get(["modules", "gae_proxy", "control_port"], 0) == 0:
-            os.mkdir(launch_path, 0755)
+            os.mkdir(launch_path, 0o755)
-    pack_req_head_len: 2 bytes,
+    pack_req_head_len: 2 bytes,ï¼ï¼°ï¼¯ï¼³ï¼´ãæ¶ä½¿ç¨
-    return string.Template(MESSAGE_TEMPLATE).substitute(title=title, banner=banner, detail=detail)
+    return string.Template(MESSAGE_TEMPLATE).substitute(
-        except:
+        except BaseException:
-    html = generate_message_html('504 GAEProxy Proxy Time out', u'è¿æ¥è¶æ¶ï¼åä¼æ¯ä¸ä¼åæ¥ï¼')
+    html = generate_message_html(
-        raise GAE_Exception(response.status, "fetch gae fail:%d" % response.status)
+        raise GAE_Exception(
-        response.status == 403 or response.status == 405:
+            response.status == 403 or response.status == 405:
-                   response.status)
+                  response.status)
-        appid_manager.report_not_exist(response.ssl_sock.appid, response.ssl_sock.ip)
+        appid_manager.report_not_exist(
-        xlog.warning('APPID %r out of Quota, remove it. %s', appid, response.ssl_sock.ip)
+        xlog.warning('APPID %r out of Quota, remove it. %s',
-        xlog.warn("GAE %s appid:%s status:%d", response.ssl_sock.ip, response.ssl_sock.appid, response.status)
+        xlog.warn("GAE %s appid:%s status:%d", response.ssl_sock.ip,
-    #kwargs['validate'] =
+    # kwargs['options'] =
-    #for k, v in headers.items():
+    payload += ''.join('%s: %s\r\n' % (k, v)
-    payload += ''.join('X-URLFETCH-%s: %s\r\n' % (k, v) for k, v in kwargs.items() if v)
+    payload += ''.join('X-URLFETCH-%s: %s\r\n' % (k, v)
-    request_headers = {}
+    request_headers = {}
-            raise GAE_Exception("get protocol head fail, len:%d" % headers_length)
+            raise GAE_Exception(
-        if time.time() - time_request > 60: #time out
+        if time.time() - time_request > 60:  # time out
-                    #xlog.warn('app_msg:%s', cgi.escape(response.app_msg))
+                xlog.warn("server app return fail, status:%d",
-                    response.worker.close("appid out of quota:%s" % response.ssl_sock.appid)
+                    response.worker.close(
-            err_msg = 'gae_handler.handler %r %s , retry...' % ( e, url)
+            err_msg = 'gae_handler.handler %r %s , retry...' % (e, url)
-            req_range_begin, req_range_end = tuple(x for x in re.search(r'bytes=(\d*)-(\d*)', v).group(1, 2))
+            req_range_begin, req_range_end = tuple(
-            xlog.debug("change Range %s => %s %s", req_range, headers["Range"], url)
+            headers["Range"] = "bytes=%d-%d" % (
-                xlog.debug("change Range %s => %s %s", req_range, headers["Range"], url)
+                headers["Range"] = "bytes=%d-%d" % (
-        return RangeFetch2(method, url, org_headers, body, response, wfile).run()
+        # éè¦åç
-            #http://en.wikipedia.org/wiki/Chunked_transfer_encoding
+            # http://en.wikipedia.org/wiki/Chunked_transfer_encoding
-        start, end, length = tuple(int(x) for x in re.search(r'bytes (\d+)-(\d+)/(\d+)', content_range).group(1, 2, 3))
+        start, end, length = tuple(int(x) for x in re.search(
-        start, end, length = 0, content_length-1, content_length
+        start, end, length = 0, content_length - 1, content_length
-            xlog.warn("get body fail, left:%d %s", body_length - body_sended, url)
+            xlog.warn("get body fail, until:%d %s",
-            if e_b[0] in (errno.ECONNABORTED, errno.EPIPE, errno.ECONNRESET) or 'bad write retry' in repr(e_b):
+            if e_b[0] in (errno.ECONNABORTED, errno.EPIPE,
-    xlog.info("GAE t:%d s:%d %s %s %s", (time.time()-request_time)*1000, content_length, method, url,
+    # å®æ´ä¸æ¬¡httpsè¯·æ±
-    max_buffer_size = int(config.AUTORANGE_MAXSIZE * config.AUTORANGE_THREADS * 1.3)
+    max_buffer_size = int(config.AUTORANGE_MAXSIZE *
-                raise Exception("range_begin:%d expect:%d" % (range_begin, self.wait_begin))
+                raise Exception("range_begin:%d expect:%d" %
-                req_range_begin, req_range_end = tuple(x for x in re.search(r'bytes=(\d*)-(\d*)', v).group(1, 2))
+                req_range_begin, req_range_end = tuple(
-        response_headers = dict((k.title(), v) for k, v in self.response.headers.items())
+        response_headers = dict((k.title(), v)
-        res_begin, res_end, res_length = tuple(int(x) for x in re.search(r'bytes (\d+)-(\d+)/(\d+)', content_range).group(1, 2, 3))
+        res_begin, res_end, res_length = tuple(int(x) for x in re.search(
-            response_headers['Content-Length'] = str(self.req_end-res_begin+1)
+            response_headers['Content-Range'] = 'bytes %s-%s/%s' % (
-        xlog.info('RangeFetch %d-%d started(%r) ', res_begin, self.req_end, self.url)
+        xlog.info('RangeFetch %d-%d started(%r) ',
-        fetch_times = int((data_left_to_fetch + config.AUTORANGE_MAXSIZE - 1)/config.AUTORANGE_MAXSIZE)
+        fetch_times = int(
-        threading.Thread(target=self.fetch, args=(res_begin, res_end, self.response)).start()
+        threading.Thread(target=self.fetch, args=(
-                    xlog.debug("send to browser wfile.write ret:%d, retry", ret)
+                    xlog.debug(
-                    xlog.debug("fetch_worker blocked, buffer:%d %s", self.data_size, self.url)
+                    xlog.debug("fetch_worker blocked, buffer:%d %s",
-                    response = request_gae_proxy(self.method, self.url, headers, self.body)
+                    response = request_gae_proxy(
-                    xlog.warning('RangeFetch %s request fail:%r', headers['Range'], e)
+                    xlog.warning('RangeFetch %s request fail:%r',
-                response.worker.close("range get gae status:%d" % response.app_status)
+                response.worker.close(
-                xlog.warn('RangeFetch Redirect(%r) status:%s', self.url, response.status)
+                self.url = urlparse.urljoin(
-                    #xlog.warn('body:%s', cgi.escape(response.body))
+                             self.method, self.url, response.headers, begin, end)
-                    xlog.warn("RangeFetch [%s] get body fail %s", response.ssl_sock.ip, self.url)
+                    xlog.warn("RangeFetch [%s] get body fail %s",
-                    xlog.warn("RangeFetch expect:%d, get:%d", expect_len, data_len)
+                    xlog.warn("RangeFetch expect:%d, get:%d",
-            self.waiter.notify()
+            self.waiter.notify()
-URLFETCH_DEFLATE_MAXSIZE = 4*1024*1024
+URLFETCH_MAXSIZE = 4 * 1024 * 1024
-    return string.Template(MESSAGE_TEMPLATE).substitute(title=title, banner=banner, detail=detail)
+    return string.Template(MESSAGE_TEMPLATE).substitute(
-    data = 'HTTP/1.1 %d %s\r\n%s\r\n\r\n%s' % (status, httplib.responses.get(status, 'Unknown'), '\r\n'.join('%s: %s' % (k.title(), v) for k, v in headers.items()), content)
+    data = 'HTTP/1.1 %d %s\r\n%s\r\n\r\n%s' % (status,
-        ctime = time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(timestamp+8*3600))
+        # xxnet èªç¨
-            yield 'Password: %s%s%s' % (__password__[0], '*'*(len(__password__)-2), __password__[-1])
+            yield 'Password: %s%s%s' % (__password__[0], '*' * (len(__password__) - 2), __password__[-1])
-            body = inflate(base64.b64decode(environ['HTTP_X_URLFETCH_PS2'])) if 'HTTP_X_URLFETCH_PS2' in environ else ''
+            body = inflate(
-            input_data = wsgi_input.read(int(environ.get('CONTENT_LENGTH', '0')))
+            input_data = wsgi_input.read(
-            body = input_data[2+payload_length:]
+            payload_length, = struct.unpack('!h', input_data[:2])  # è·åé¿åº¦
-    any(kwargs.__setitem__(x[len('x-urlfetch-'):].lower(), headers.pop(x)) for x in headers.keys() if x.lower().startswith('x-urlfetch-'))
+    any(kwargs.__setitem__(x[len('x-urlfetch-'):].lower(), headers.pop(x))
-            except:
+            except BaseException:
-    logging.info('%s "%s %s %s" - -', environ['REMOTE_ADDR'], method, url, 'HTTP/1.1')
+    logging.info(
-    accept_encoding = headers.get('Accept-Encoding', '') or headers.get('Bccept-Encoding', '')
+    accept_encoding = headers.get(
-            response = urlfetch.fetch(url, body, fetchmethod, headers, allow_truncated=allow_truncated, follow_redirects=False, deadline=timeout, validate_certificate=validate_certificate)
+            response = urlfetch.fetch(
-            logging.error('DeadlineExceededError(timeout=%s, url=%r)', timeout, url)
+            logging.error(
-            m = re.search(r'=\s*(\d+)-', headers.get('Range') or headers.get('range') or '')
+            m = re.search(r'=\s*(\d+)-', headers.get('Range')
-                headers['Range'] = 'bytes=%s-%d' % (start, start+(maxsize or URLFETCH_MAXSIZE))
+                headers['Range'] = 'bytes=%s-%d' % (start,
-            logging.error('ResponseTooLargeError(timeout=%s, url=%r) response(%r)', timeout, url, response)
+            logging.error(
-            m = re.search(r'=\s*(\d+)-', headers.get('Range') or headers.get('range') or '')
+            m = re.search(r'=\s*(\d+)-', headers.get('Range')
-                headers['Range'] = 'bytes=%s-%d' % (start, start+(maxsize or URLFETCH_MAXSIZE))
+                headers['Range'] = 'bytes=%s-%d' % (start,
-    #for k in response_headers:
+    response_headers['X-Head-Content-Length'] = response_headers.get(
-    if status_code == 200 and maxsize and len(data) > maxsize and response_headers.get('accept-ranges', '').lower() == 'bytes' and int(response_headers.get('content-length', 0)):
+    # ä¹æ¯åçåå¹¶ä¹ç±»çç»è
-        response_headers['Content-Range'] = 'bytes 0-%d/%d' % (maxsize-1, len(data))
+        response_headers['Content-Range'] = 'bytes 0-%d/%d' % (
-    if status_code == 200 and 'content-encoding' not in response_headers and 512 < len(data) < URLFETCH_DEFLATE_MAXSIZE and content_type.startswith(('text/', 'application/json', 'application/javascript')):
+    if status_code == 200 and 'content-encoding' not in response_headers and 512 < len(
-            compressobj = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION, zlib.DEFLATED, -zlib.MAX_WBITS, zlib.DEF_MEM_LEVEL, 0)
+            compressobj = zlib.compressobj(
-            dataio.write(struct.pack('<LL', zlib.crc32(data) & 0xFFFFFFFFL, len(data) & 0xFFFFFFFFL))
+            dataio.write(
-    #last_check_time = time.time()
+    # don't record last_check_time here, it's not a real check
-    global network_stat, last_check_time, continue_fail_count
+    global network_stat, continue_fail_count
-    last_check_time = time.time()
+    #last_check_time = time.time()
-def triger_check_network(force=False):
+def triger_check_network(fail=False, force=False):
-        else:
+        if fail or network_stat != "OK":
-        network_stat = "unknown"
+        # network_stat = "unknown"
-        triger_check_network()
+        triger_check_network(True)
-                    if sys.platform != 'win32' and os.path.isfile(dst_file):
+                    #modify by outofmemo, files in '/sdcard' are not allowed to chmod for Android
-                        shutil.copy(src_file, dst_file)
+                        shutil.copyfile(src_file, dst_file)
-            metadata_length, = struct.unpack('!h', input_data[:2])
+            metadata_length = struct.unpack('!h', input_data[:2])
-        self.GAE_VALIDATE = self.CONFIG.getboolean('gae', 'validate')
+        self.GAE_VALIDATE = self.CONFIG.getint('gae', 'validate')
-    #kwargs['validate'] =
+    kwargs['validate'] = config.GAE_VALIDATE
-        fd.write(ip_utils.ip_num_to_string(begin)+ "-" + ip_utils.ip_num_to_string(nend)+"\n")
+        fd.write(ip_utils.ip_num_to_string(begin)+ "-" + ip_utils.ip_num_to_string(end)+"\n")
-        fd.write(ip_utils.ip_num_to_string(begin)+ "-" + ip_utils.ip_num_to_string(geneend)+"\n")
+        fd.write(ip_utils.ip_num_to_string(begin)+ "-" + ip_utils.ip_num_to_string(nend)+"\n")
-def generage_range_from_apnic(input):
+def generate_range_from_apnic(input):
-    bad_ip_range_lines = generage_range_from_apnic(apnic_lines)
+    bad_ip_range_lines = generate_range_from_apnic(apnic_lines)
-        fd.write(ip_utils.ip_num_to_string(begin)+ "-" + ip_utils.ip_num_to_string(end)+"\n")
+        fd.write(ip_utils.ip_num_to_string(begin)+ "-" + ip_utils.ip_num_to_string(geneend)+"\n")
-    sepcial_bad_ip_range_lines  = """
+    special_bad_ip_range_lines  = """
-    return bad_ip_range_lines + sepcial_bad_ip_range_lines
+    return bad_ip_range_lines + special_bad_ip_range_lines
-                stream.max_frame_size += new_size
+                stream.max_frame_size += new_size
-                    target_relate_path = "code/" + xxnet_version + relate_path[12:]
+            
-    raise "get_version_fail:" % readme_file
+    raise "get_version_fail: %s" % readme_file
-        #cert.add_extensions([OpenSSL.crypto.X509Extension(b'subjectAltName', True, ', '.join('DNS: %s' % x for x in sans))])
+        cert.add_extensions([OpenSSL.crypto.X509Extension(b'subjectAltName', True, ', '.join('DNS: %s' % x for x in sans))])
-    config.set_var("roundtrip_timeout", 120)
+    config.set_var("roundtrip_timeout", 25)
-                    xlog.warn("request_balance fail when start:%s", reason)
+            pass
-import socket, SocketServer, struct
+import socket, struct
-
+import proxy_session
-        conn_id = g.session.create_conn(sock, addr, port)
+        conn_id = proxy_session.create_conn(sock, addr, port)
-        conn_id = g.session.create_conn(sock, addr, port)
+        conn_id = proxy_session.create_conn(sock, addr, port)
-        conn_id = g.session.create_conn(sock, host, port)
+        conn_id = proxy_session.create_conn(sock, host, port)
-        self.start()
+        self.roundtrip_thread = {}
-            xlog.warn("x-tunnel session not start")
+            xlog.warn("x-tunnel login_session fail, session not start")
-            del self.transfer_list[transfer_no]
+            try:
-    config.set_var("block_max_size", 512 * 1024)
+    config.set_var("block_max_size", 256 * 1024)
-    config.set_var("roundtrip_timeout", 20)
+    config.set_var("roundtrip_timeout", 120)
-            return
+            #xlog.warn("session not running, can't connect")
-                    continue
+                    if transfer_no not in self.transfer_list:
-                        xlog.error("roundtrip time:%d transfer_no:%d sn:%d send:%d status:%r retry:%d",
+                        xlog.error("roundtrip time:%d transfer_no:%d sn:%d send:%d len:%d status:%r retry:%d",
-    hosts = ["www.6rank.edu.cn", "v6.testmyipv6.com", ]
+    hosts = ["bt.neu6.edu.cn", "v6.ipv6-test.com", "ipv6.test-ipv6.jp"]
-        xlog.warn("here we go")
+        self.ONLYHOSTS = [x.strip() for x in self.CONFIG.get('proxy', 'hosts').split("|")]
-        return True
+            for h in config.ONLYHOSTS:
-        http_client = simple_http_client.HTTP_client((host_ip, int(port)))
+        self.parsed_url = urlparse.urlparse(self.path)
-        self.parsed_url = urlparse.urlparse(self.path)
+        xlog.warn("here we go")
-            xlog.warn("forward_local fail")
+            xlog.warn("forward_local fail: %d, command: %s, path: %s, headers: %s, payload: %s, response: %s",
-        return False
+            if s.startswith('gcr.io') \
-        executeCommand = 'do shell script "%s;%s;%s;%s" with administrator privileges' % (rmCommand, cpCommand, chmodCommand, chownCommand)
+        chmodCommand   = "chmod 4755 \\\"%s\\\"" % helper_path
-        # network_stat = "unknown"
+        network_stat = "unknown"
-    global network_stat, last_check_time, continue_fail_count
+    global network_stat
-        if host_ip in socket.gethostbyname_ex(socket.gethostname())[-1]:
+        if self.is_local([host, host_ip]):
-
+    ca_validity_years = 10
-        ca.gmtime_adj_notAfter(24 * 60 * 60 * 3652)
+        ca.gmtime_adj_notAfter(CertUtil.ca_validity)
-        cert.gmtime_adj_notAfter(60 * 60 * 24 * 365)
+        cert.gmtime_adj_notAfter(CertUtil.ca_validity)
-            version = fd.read()
+            with open(version_file, "r") as fd:
-    modules = ["gae_proxy", "launcher", "php_proxy", "x_tunnel"]
+    modules = ["gae_proxy", "launcher", "x_tunnel"]
-        if module not in ["launcher", "php_proxy"]:
+        if module not in ["launcher"]:
-             "x_tunnel_enable": %d}' %\
+             "show_systray": %d, "auto_start": %d, "show_detail": %d, "gae_proxy_enable": %d, "x_tunnel_enable": %d}' %\
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Config'), 'config:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Config', 'config:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Enable Auto GAEProxy'), 'enableAutoProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Auto GAEProxy', 'enableAutoProxy:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Enable Global GAEProxy'), 'enableGlobalProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global GAEProxy', 'enableGlobalProxy:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Disable GAEProxy'), 'disableProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Disable GAEProxy', 'disableProxy:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Reload GAEProxy'), 'resetGoagent:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Reload GAEProxy', 'resetGoagent:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Quit'), 'windowWillClose:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Quit', 'windowWillClose:', '')
-        return lang_text('Connection: %s') % currentService
+        return 'Connection: %s' % currentService
-        return lang_text('Connection: None')
+        return 'Connection: None'
-                target_relate_path = "code/" + xxnet_version + relate_path[12:]
+            if sys.platform == 'win32':
-                target_relate_path = "code\\" + xxnet_version + relate_path[12:]
+            if sys.platform == 'win32':
-                target_relate_path = "code/" + xxnet_version + relate_path[12:]
+            if target_relate_path.startswith("code\\default"):
-
+# Launuage dictionary
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'éç½®' if lang_code == "zh_CN" else 'Config', 'config:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Config'), 'config:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'å¯ç¨èªå¨ä»£ç' if lang_code == "zh_CN" else 'Enable Auto GAEProxy', 'enableAutoProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Enable Auto GAEProxy'), 'enableAutoProxy:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'å¯ç¨å¨å±ä»£ç' if lang_code == "zh_CN" else 'Enable Global GAEProxy', 'enableGlobalProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Enable Global GAEProxy'), 'enableGlobalProxy:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'ç¦ç¨ä»£ç' if lang_code == "zh_CN" else 'Disable GAEProxy', 'disableProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Disable GAEProxy'), 'disableProxy:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'éå¯ä»£ç' if lang_code == "zh_CN" else 'Reload GAEProxy', 'resetGoagent:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Reload GAEProxy'), 'resetGoagent:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'éåº' if lang_code == "zh_CN" else 'Quit', 'windowWillClose:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(lang_text('Quit'), 'windowWillClose:', '')
-        return (u'è¿æ¥: %s' if lang_code == "zh_CN" else 'Connection: %s') % currentService
+        return lang_text('Connection: %s') % currentService
-        return u'è¿æ¥: æ ' if lang_code == "zh_CN" else 'Connection: None'
+        return lang_text('Connection: None')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'éè½½ä»£ç' if lang_code == "zh_CN" else 'Reload GAEProxy', 'resetGoagent:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'éå¯ä»£ç' if lang_code == "zh_CN" else 'Reload GAEProxy', 'resetGoagent:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Config', 'config:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'éç½®' if lang_code == "zh_CN" else 'Config', 'config:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Auto GAEProxy', 'enableAutoProxy:', '')
+        # Separator
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global GAEProxy', 'enableGlobalProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'å¯ç¨å¨å±ä»£ç' if lang_code == "zh_CN" else 'Enable Global GAEProxy', 'enableGlobalProxy:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Disable GAEProxy', 'disableProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'ç¦ç¨ä»£ç' if lang_code == "zh_CN" else 'Disable GAEProxy', 'disableProxy:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Reload GAEProxy', 'resetGoagent:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'éè½½ä»£ç' if lang_code == "zh_CN" else 'Reload GAEProxy', 'resetGoagent:', '')
-        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Quit', 'windowWillClose:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(u'éåº' if lang_code == "zh_CN" else 'Quit', 'windowWillClose:', '')
-        return 'Connection: %s' % currentService
+        return (u'è¿æ¥: %s' if lang_code == "zh_CN" else 'Connection: %s') % currentService
-        return 'Connection: None'
+        return u'è¿æ¥: æ ' if lang_code == "zh_CN" else 'Connection: None'
-from instances import xlog
+from instances import xlog
-        self.systray = SysTrayIcon(icon_path, "XX-Net", self.make_menu(), self.on_quit, left_click=self.on_show, right_click=self.on_right_click)
+        self.systray = SysTrayIcon(icon_path, "XX-Net", 
-            0, winreg.KEY_ALL_ACCESS)
+        self.INTERNET_SETTINGS = winreg.OpenKey(winreg.HKEY_CURRENT_USER, reg_path, 0, winreg.KEY_ALL_ACCESS)
-            AutoConfigURL, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'AutoConfigURL')
+            AutoConfigURL, reg_type = winreg.QueryValueEx(self.INTERNET_SETTINGS, 'AutoConfigURL')
-        except Exception as e:
+        except:
-            ProxyEnable, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'ProxyEnable')
+            ProxyEnable, reg_type = winreg.QueryValueEx(self.INTERNET_SETTINGS, 'ProxyEnable')
-                ProxyServer, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'ProxyServer')
+                ProxyServer, reg_type = winreg.QueryValueEx(self.INTERNET_SETTINGS, 'ProxyServer')
-        except Exception as e:
+        except:
-        win32_proxy_manager.set_proxy_server("127.0.0.1", 8087)
+        win32_proxy_manager.set_proxy("127.0.0.1:8087")
-        win32_proxy_manager.set_proxy_auto("http://127.0.0.1:8086/proxy.pac")
+        win32_proxy_manager.set_proxy("http://127.0.0.1:8086/proxy.pac")
-    nSize = c_ulong(sizeof(INTERNET_PER_CONN_OPTION_LIST))
+    _,values_num,_ = winreg.QueryInfoKey(CONNECTIONS)
-    Option[0].Value.dwValue = PROXY_TYPE_DIRECT
+        List = INTERNET_PER_CONN_OPTION_LIST()
-    List.pOptions = Option
+        Option[0].dwOption = INTERNET_PER_CONN_FLAGS
-    InternetSetOption(None, INTERNET_OPTION_PER_CONNECTION_OPTION, byref(List), nSize)
+        List.dwSize = sizeof(INTERNET_PER_CONN_OPTION_LIST)
-    setting = create_unicode_buffer(pac_url)
+def set_proxy_auto(proxy_addr, conn_name='DefaultConnectionSettings'):
-    List.pszConnection = None
+    List.pszConnection = create_unicode_buffer(conn_name)
-    setting = create_unicode_buffer(ip+":"+str(port))
+def set_proxy_server(proxy_addr, conn_name='DefaultConnectionSettings'):
-    List.pszConnection = None
+    List.pszConnection = create_unicode_buffer(conn_name)
-    set_proxy_server("127.0.0.1", 8087)
+    set_proxy("127.0.0.1:8087")
-        # network_stat = "unkown"
+        # network_stat = "unknown"
-    return v1 <= v2
+    try:
-        xlog.info("scan_ip_worker exit")
+        #xlog.info("scan_ip_worker exit")
-                self.send_response(mimetype, data)
+                self.send_response_nc(mimetype, data)
-        self.send_response(mimetype, data)
+        self.send_response_nc(mimetype, data)
-        self.send_response('text/html', data)
+        self.send_response_nc('text/html', data)
-                            return self.send_response('text/html', '{"res":"fail", "reason":"appid fail:%s"}' % fail_appid)
+                            return self.send_response_nc('text/html', '{"res":"fail", "reason":"appid fail:%s"}' % fail_appid)
-                            return self.send_response('text/html', '{"res":"fail", "reason":"IPv6 fail"}')
+                            return self.send_response_nc('text/html', '{"res":"fail", "reason":"IPv6 fail"}')
-                self.send_response('text/html', data)
+                self.send_response_nc('text/html', data)
-        self.send_response('text/html', data)
+        self.send_response_nc('text/html', data)
-        self.send_response('text/html', data)
+        self.send_response_nc('text/html', data)
-        self.send_response('text/html', data)
+        self.send_response_nc('text/html', data)
-        self.send_response('text/html', data)
+        self.send_response_nc('text/html', data)
-        self.send_response(mimetype, data)
+        self.send_response_nc(mimetype, data)
-        self.send_response(mimetype, data)
+        self.send_response_nc(mimetype, data)
-        self.send_response(mimetype, data)
+        self.send_response_nc(mimetype, data)
-        self.send_response(mimetype, data)
+        self.send_response_nc(mimetype, data)
-        self.send_response(mimetype, data)
+        self.send_response_nc(mimetype, data)
-            self.send_response('text/plain', data)
+            self.send_response_nc('text/plain', data)
-                self.send_response('text/plain', '{"res":"fail", "reason":"running"}')
+                self.send_response_nc('text/plain', '{"res":"fail", "reason":"running"}')
-                self.send_response('text/plain', '{"res":"success"}')
+                self.send_response_nc('text/plain', '{"res":"success"}')
-                self.send_response('text/plain', '{"res":"fail", "reason":"not running"}')
+                self.send_response_nc('text/plain', '{"res":"fail", "reason":"not running"}')
-                self.send_response('text/plain', '{"res":"success"}')
+                self.send_response_nc('text/plain', '{"res":"success"}')
-						
+                        (u'éåº', None, SysTrayIcon.QUIT, False))
-			(u'Quit', None, SysTrayIcon.QUIT, False))
+                        (u'Quit', None, SysTrayIcon.QUIT, False))
-						     (u'éåº', None, SysTrayIcon.QUIT, False))
+			(u'éåº', None, SysTrayIcon.QUIT, False))
-						     (u'Quit', None, SysTrayIcon.QUIT, False))
+			(u'Quit', None, SysTrayIcon.QUIT, False))
-from systray import SysTrayIcon, win32_adapter
+import config
-            self.make_menu(), self.on_quit, left_click=self.on_show, right_click=self.on_right_click)
+        self.systray = SysTrayIcon(icon_path, "XX-Net", self.make_menu(), self.on_quit, left_click=self.on_show, right_click=self.on_right_click)
-        self.INTERNET_SETTINGS = winreg.OpenKey(winreg.HKEY_CURRENT_USER, reg_path, 0, winreg.KEY_ALL_ACCESS)
+        self.INTERNET_SETTINGS = winreg.OpenKey(winreg.HKEY_CURRENT_USER,
-            AutoConfigURL, reg_type = winreg.QueryValueEx(self.INTERNET_SETTINGS, 'AutoConfigURL')
+            AutoConfigURL, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'AutoConfigURL')
-        except:
+        except Exception as e:
-            ProxyEnable, reg_type = winreg.QueryValueEx(self.INTERNET_SETTINGS, 'ProxyEnable')
+            ProxyEnable, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'ProxyEnable')
-                ProxyServer, reg_type = winreg.QueryValueEx(self.INTERNET_SETTINGS, 'ProxyServer')
+                ProxyServer, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'ProxyServer')
-        except:
+        except Exception as e:
-                        (u'éåº', None, SysTrayIcon.QUIT, False))
+						     (u'éåº', None, SysTrayIcon.QUIT, False))
-                        (u'Quit', None, SysTrayIcon.QUIT, False))
+						     (u'Quit', None, SysTrayIcon.QUIT, False))
-        win32_proxy_manager.set_proxy("127.0.0.1:8087")
+        win32_proxy_manager.set_proxy_server("127.0.0.1", 8087)
-        win32_proxy_manager.set_proxy("http://127.0.0.1:8086/proxy.pac")
+        win32_proxy_manager.set_proxy_auto("http://127.0.0.1:8086/proxy.pac")
-        if type != socket.SOCK_STREAM and type != socket.SOCK_DGRAM:
+        if type not in {socket.SOCK_STREAM, socket.SOCK_DGRAM}:
-    print name2
+    print name2
-        nSize = c_ulong(sizeof(INTERNET_PER_CONN_OPTION_LIST))
+def disable_proxy():
-        Option[0].Value.dwValue = PROXY_TYPE_DIRECT
+    Option[0].dwOption = INTERNET_PER_CONN_FLAGS
-        List.pOptions = Option
+    List.dwSize = sizeof(INTERNET_PER_CONN_OPTION_LIST)
-        InternetSetOption(None, INTERNET_OPTION_PER_CONNECTION_OPTION, byref(List), nSize)
+    InternetSetOption(None, INTERNET_OPTION_PER_CONNECTION_OPTION, byref(List), nSize)
-    setting = create_unicode_buffer(proxy_addr)
+def set_proxy_auto(pac_url):
-    List.pszConnection = create_unicode_buffer(conn_name)
+    List.pszConnection = None
-    setting = create_unicode_buffer(proxy_addr)
+
-    List.pszConnection = create_unicode_buffer(conn_name)
+    List.pszConnection = None
-    set_proxy("127.0.0.1:8087")
+    set_proxy_server("127.0.0.1", 8087)
-        if type not in {socket.SOCK_STREAM, socket.SOCK_DGRAM}:
+        if type != socket.SOCK_STREAM and type != socket.SOCK_DGRAM:
-    print name2
+    print name2
-def handler(method, url, headers, body, wfile):
+def handler(method, host, url, headers, body, wfile):
-        if time.time() - time_request > 120:
+        if time.time() - time_request > 30:
-            response = fetch(method, host, path, headers, body)
+            response = fetch(method, host, url, headers, body)
-            xlog.warn("direct_handler.handler err:%r %s", e, url)
+            xlog.warn("direct_handler.handler err:%r %s/%s", e, host, url)
-            xlog.exception('direct_handler.handler %r %s , retry...', e,url)
+            xlog.exception('direct_handler.handler %r %s %s , retry...', e, host, url)
-            xlog.info("direct_handler.handler send response fail. t:%d e:%r %s", wait_time, e, url)
+            xlog.info("direct_handler.handler send response fail. t:%d e:%r %s%s", wait_time, e, host, url)
-            xlog.info("DIRECT t:%d %d %s", (time.time()-time_request)*1000, response.status, url)
+            xlog.info("DIRECT t:%d %d %s %s", (time.time()-time_request)*1000, response.status, host, url)
-                    xlog.warn("direct_handler.handler send Transfer-Encoding t:%d e:%r %s", time.time()-time_request, e, url)
+                    xlog.warn("direct_handler.handler send Transfer-Encoding t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
-                        xlog.info("direct_handler.handler send Transfer-Encoding t:%d e:%r %s", time.time()-time_request, e,  url)
+                        xlog.info("direct_handler.handler send Transfer-Encoding t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
-            xlog.info("DIRECT chucked t:%d s:%d %d %s", (time.time()-time_request)*1000, length, response.status, url)
+            xlog.info("DIRECT chucked t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
-                xlog.info("DIRECT t:%d s:%d %d %s", (time.time()-time_request)*1000, length, response.status, url)
+                xlog.info("DIRECT t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
-                    xlog.warn("read timeout t:%d len:%d left:%d %s", (time.time()-time_request)*1000, length, (end-start), url)
+                    xlog.warn("read timeout t:%d len:%d left:%d %s %s", (time.time()-time_request)*1000, length, (end-start), host, url)
-                        xlog.info('direct_handler send to browser return %r %r', e_b, url)
+                        xlog.info('direct_handler send to browser return %r %s %r', e_b, host, url)
-                        xlog.info('direct_handler send to browser return %r %r', e_b, url)
+                        xlog.info('direct_handler send to browser return %r %s %r', e_b, host, url)
-            xlog.exception("direct_handler err:%r %s time:%d", e, url, time_cost)
+            xlog.exception("direct_handler err:%r %s %s time:%d", e, host, url, time_cost)
-            xlog.exception("direct_handler except:%r %s", e, url)
+            xlog.exception("direct_handler except:%r %s %s", e, host, url)
-        xlog.exception("direct_handler except:%r %s", e, url)
+        xlog.exception("direct_handler except:%r %s %s", e, host, url)
-    # GAE don't support command like OPTIONS, PROFILE
+    # GAE don't support command like OPTION
-    bufsize = 64*1024
+    bufsize = 256*1024
-        elif 'Transfer-Encoding' in self.headers:
+    def forward_local(self):
-                chunk_size = int("0x" + chunk_size_list[0], 0)
+                chunk_size = int("0x"+chunk_size_list[0], 0)
-                self.payload += self.rfile.read(chunk_size)
+                payload += self.rfile.read(chunk_size)
-            self.port = 80
+        if self.command == "OPTIONS":
-        self.read_payload()
+        if self.command not in self.gae_support_methods:
-        self.dispatch_request()
+        xlog.debug("GAE %s %s", self.command, self.path)
-
+        if self.path != "https://www.twitter.com/xxnet":
-            certfile = CertUtil.get_cert(self.host, full_name=True)
+            xlog.info('ssl error: %s, create full domain cert for host:%s', e, host)
-                               self.connection, e, self.path, e.args[0])
+                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
-                #xlog.warn("read request line len:%d", len(self.raw_requestline))
+            if len(self.raw_requestline) > 65536:
-                               e, self.path, e.args[0])
+                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
-        self.url = 'https://%s%s' % (self.host_port, self.path)
+        if self.path[0] == '/' and host:
-            xlog.debug("%s %s", self.command, self.url)
+        if self.path == "https://www.twitter.com/xxnet":
-                return self.use_DIRECT()
+        try:
-        return self.use_GAE()
+            self.parsed_url = urlparse.urlparse(self.path)
-            return self.send_method_allows(self.headers, self.payload)
+            return self.do_AGENT()
-        direct_handler.handler(self.command, self.url, self.headers, self.payload, self.wfile)
+        except NetWorkIOError as e:
-        http_client = simple_http_client.HTTP_client((self.host, int(self.port)))
+        certfile = CertUtil.get_cert(host)
-            xlog.warn("forward_local fail")
+        try:
-        out_list.append(content)
+        self.__realconnection = self.connection
-        self.wfile.write("".join(out_list))
+        try:
-        # Refer: https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS#Preflighted_requests
+        xlog.debug('GAE CONNECT Direct %s %s', self.command, self.path)
-                "Content-Length: 0\r\n"
+        try:
-            response += "Access-Control-Allow-Headers: %s\r\n" % req_header
+            self.parsed_url = urlparse.urlparse(self.path)
-            response += "Access-Control-Allow-Origin: *\r\n"
+            direct_handler.handler(self.command, host, path, request_headers, payload, self.wfile)
-        response += "\r\n"
+        except NetWorkIOError as e:
-        self.wfile.write(response)
+                fd.flush()
-        proxy_rdns=None, proxy_user=None, proxy_pass=None):
+        proxy_rdns=True, proxy_user=None, proxy_pass=None):
-        if use_proxy and proxy_rdns:
+        if use_proxy:
-        self_host.append(host)
+try:
-        sock = socks.socksocket(socket.AF_INET)
+        sock = socks.socksocket(socket.AF_INET if ':' not in ip else socket.AF_INET6)
-        sock = socket.socket(socket.AF_INET)
+        sock = socket.socket(socket.AF_INET if ':' not in ip else socket.AF_INET6)
-    conn = hyper.HTTP20Connection(ssl_sock, host='%s.appspot.com'%appid, ip=ip, port=443)
+        conn = hyper.HTTP20Connection(ssl_sock, host='%s.appspot.com'%appid, ip=ip, port=443)
-def handler(method, host, url, headers, body, wfile):
+def handler(method, url, headers, body, wfile):
-        if time.time() - time_request > 30:
+        if time.time() - time_request > 120:
-            response = fetch(method, host, url, headers, body)
+            response = fetch(method, host, path, headers, body)
-            xlog.warn("direct_handler.handler err:%r %s/%s", e, host, url)
+            xlog.warn("direct_handler.handler err:%r %s", e, url)
-            xlog.exception('direct_handler.handler %r %s %s , retry...', e, host, url)
+            xlog.exception('direct_handler.handler %r %s , retry...', e,url)
-            xlog.info("direct_handler.handler send response fail. t:%d e:%r %s%s", wait_time, e, host, url)
+            xlog.info("direct_handler.handler send response fail. t:%d e:%r %s", wait_time, e, url)
-            xlog.info("DIRECT t:%d %d %s %s", (time.time()-time_request)*1000, response.status, host, url)
+            xlog.info("DIRECT t:%d %d %s", (time.time()-time_request)*1000, response.status, url)
-                    xlog.warn("direct_handler.handler send Transfer-Encoding t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
+                    xlog.warn("direct_handler.handler send Transfer-Encoding t:%d e:%r %s", time.time()-time_request, e, url)
-                        xlog.info("direct_handler.handler send Transfer-Encoding t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
+                        xlog.info("direct_handler.handler send Transfer-Encoding t:%d e:%r %s", time.time()-time_request, e,  url)
-            xlog.info("DIRECT chucked t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
+            xlog.info("DIRECT chucked t:%d s:%d %d %s", (time.time()-time_request)*1000, length, response.status, url)
-                xlog.info("DIRECT t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
+                xlog.info("DIRECT t:%d s:%d %d %s", (time.time()-time_request)*1000, length, response.status, url)
-                    xlog.warn("read timeout t:%d len:%d left:%d %s %s", (time.time()-time_request)*1000, length, (end-start), host, url)
+                    xlog.warn("read timeout t:%d len:%d left:%d %s", (time.time()-time_request)*1000, length, (end-start), url)
-                        xlog.info('direct_handler send to browser return %r %s %r', e_b, host, url)
+                        xlog.info('direct_handler send to browser return %r %r', e_b, url)
-                        xlog.info('direct_handler send to browser return %r %s %r', e_b, host, url)
+                        xlog.info('direct_handler send to browser return %r %r', e_b, url)
-            xlog.exception("direct_handler err:%r %s %s time:%d", e, host, url, time_cost)
+            xlog.exception("direct_handler err:%r %s time:%d", e, url, time_cost)
-            xlog.exception("direct_handler except:%r %s %s", e, host, url)
+            xlog.exception("direct_handler except:%r %s", e, url)
-        xlog.exception("direct_handler except:%r %s %s", e, host, url)
+        xlog.exception("direct_handler except:%r %s", e, url)
-                xlog.debug("fetch_worker blocked, buffer:%d %s", self.data_size, self.url)
+                if not self.blocked:
-import re
+def get_crlf(rfile):
-    # GAE don't support command like OPTION
+    # GAE don't support command like OPTIONS, PROFILE
-    bufsize = 256*1024
+    bufsize = 64*1024
-        elif 'Transfer-Encoding' in request_headers:
+    def read_payload(self):
-                chunk_size = int("0x"+chunk_size_list[0], 0)
+                chunk_size = int("0x" + chunk_size_list[0], 0)
-                payload += self.rfile.read(chunk_size)
+                self.payload += self.rfile.read(chunk_size)
-            return self.send_method_allows(request_headers, payload)
+    def do_METHOD(self):
-            return self.wfile.write(('HTTP/1.1 404 Not Found\r\n\r\n').encode())
+        self.read_payload()
-        gae_handler.handler(self.command, self.path, request_headers, payload, self.wfile)
+        self.dispatch_request()
-        # xlog.info('https GAE %s %s:%d ', self.command, host, port)
+        self.method = "https"
-            certfile = CertUtil.get_cert(host, full_name=True)
+            xlog.info('ssl error: %s, create full domain cert for host:%s', e, self.host)
-                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
+                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s',
-                # xlog.warn("read request line empty")
+            if len(self.raw_requestline) > 65535 or not self.raw_requestline:
-                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
+                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection,
-            self.path = 'https://%s%s' % (self.headers['Host'], self.path)
+            return
-        if self.path == "https://www.twitter.com/xxnet":
+        if self.__realconnection:
-                host = urlparse.urlparse(self.path).netloc
+        if self.host in self_host:
-            self.parsed_url = urlparse.urlparse(self.path)
+        touch_active()
-            return self.do_AGENT()
+        if self.host == "www.google.com" and \
-            return
+        if self.host in config.HOSTS_GAE or self.host.endswith(config.HOSTS_GAE_ENDSWITH):
-        self.wfile.write(b'HTTP/1.1 200 OK\r\n\r\n')
+        # redirect http request to https request
-                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
+        return self.use_GAE()
-        self.wfile = self.connection.makefile('wb', 0)
+        out_list = []
-            self.path = 'https://%s%s' % (self.headers['Host'], self.path)
+        self.wfile.write("".join(out_list))
-        xlog.debug('GAE CONNECT Direct %s %s', self.command, self.path)
+    def send_method_allows(self, headers, payload):
-                host = urlparse.urlparse(self.path).netloc
+        response = \
-                    return
+        req_header = headers.get("Access-Control-Request-Headers", "")
-            direct_handler.handler(self.command, host, path, request_headers, payload, self.wfile)
+        origin = headers.get("Origin", "")
-                    self.__realconnection = None
+        response += "\r\n"
-def uploads(appids, rc4_password=""):
+def uploads(appids, rc4_password):
-        logging.info("Usage: uploader.py <appids> [-debug]")
+        logging.info("Usage: uploader.py <appids> [-debug] [-password rc4_password]")
-        logging.info("enable debug logging")
+    rc4_password = "";
-    uploads(appids)
+    uploads(appids, rc4_password)
-        network_stat = "unkown"
+        # network_stat = "unkown"
-    _checking_lock.release()
+def _check_one_host(host):
-                  }
+        conn = httplib.HTTPSConnection(host, 443, timeout=30)
-            xlog.debug("restore socket")
+    network_ok = False
-threading.Thread(target=_simple_check_worker).start()
+    _checking_lock.acquire()
-            xlog.warn("direct_handler.handler send response fail. t:%d e:%r %s%s", wait_time, e, host, url)
+            xlog.info("direct_handler.handler send response fail. t:%d e:%r %s%s", wait_time, e, host, url)
-                        xlog.warn("direct_handler.handler send Transfer-Encoding t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
+                        xlog.info("direct_handler.handler send Transfer-Encoding t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
-                        xlog.warn('direct_handler send to browser return %r %s %r', e_b, host, url)
+                        xlog.info('direct_handler send to browser return %r %s %r', e_b, host, url)
-                        xlog.warn('direct_handler send to browser return %r %s %r', e_b, host, url)
+                        xlog.info('direct_handler send to browser return %r %s %r', e_b, host, url)
-        xlog.warn("gae_handler.handler send response fail. e:%r %s", e, url)
+        xlog.info("gae_handler.handler send response fail. e:%r %s", e, url)
-                xlog.warn('gae_handler send to browser return %r %r', e_b, url)
+                xlog.info('gae_handler send to browser return %r %r', e_b, url)
-                xlog.warn('gae_handler send to browser return %r %r', e_b, url)
+                xlog.info('gae_handler send to browser return %r %r', e_b, url)
-            xlog.warn("RangeFetch send response fail:%r %s", e, self.url)
+            self.keep_running = False
-                xlog.warn('RangeFetch client closed(%s). %s', e, self.url)
+                xlog.info('RangeFetch client closed(%s). %s', e, self.url)
-                xlog.warn('RangeFetch Redirect(%r)', self.url)
+                xlog.warn('RangeFetch Redirect(%r) status:%s', self.url, response.status)
-                response.worker.close("no range")
+                # response.worker.close("no range")
-            sys.stderr.write(string)
+            try:
-            string = '%s - [%s]LOG_EXCEPT: %s, Except:%s<br>' % (time.ctime()[4:-5], level, fmt % args, e)
+            string = '%s - [%s]LOG_EXCEPT: %s, Except:%s<br> %s' % (time.ctime()[4:-5], level, fmt % args, e, traceback.format_exc())
-openssl_context.set_session_id(binascii.b2a_hex(os.urandom(10)))
+try:
-        self.openssl_context.set_session_id(binascii.b2a_hex(os.urandom(10)))
+        try:
-        if self.wait_begin == 0 and self.req_end == res_length:
+        if self.wait_begin == 0 and self.req_end == res_length - 1:
-                xlog.debug("fetch_worker blocked %s", self.url)
+                xlog.debug("fetch_worker blocked, buffer:%d %s", self.data_size, self.url)
-        return self.rtt
+        return self.rtt + 100
-                xlog.exception("recv fail:%r", e)
+                xlog.debug("recv fail:%r", e)
-                continue
+        while len(self.workers) > config.max_worker_num:
-                continue
+                if worker.version == "2" and len(worker.streams) > 0:
-            idle_num += 1
+                idle_num += 1
-            rtt = worker.get_rtt_rate()
+                rtt = worker.get_rtt_rate()
-                slowest_worker = worker
+                if rtt > slowest_rtt:
-            return
+            if idle_num < 3 or slowest_worker is None:
-        self.close_cb(slowest_worker)
+            self.close_cb(slowest_worker)
-__version__ = '0.13'
+# This file is dual licensed under the terms of the Apache License, Version
-from six import text_type as _text_type
+    UNSPECIFIED as _UNSPECIFIED,
-    exception_from_error_queue as _exception_from_error_queue,
+    make_assert as _make_assert,
-    UNSPECIFIED as _UNSPECIFIED,
+    text_to_bytes_and_warn as _text_to_bytes_and_warn,
-OP_NETSCAPE_REUSE_CIPHER_CHANGE_BUG = _lib.SSL_OP_NETSCAPE_REUSE_CIPHER_CHANGE_BUG
+OP_NETSCAPE_REUSE_CIPHER_CHANGE_BUG = (
-OP_NETSCAPE_DEMO_CIPHER_CHANGE_BUG= _lib.SSL_OP_NETSCAPE_DEMO_CIPHER_CHANGE_BUG
+OP_NETSCAPE_DEMO_CIPHER_CHANGE_BUG = (
-OP_ALL   = _lib.SSL_OP_ALL
+OP_ALL = _lib.SSL_OP_ALL
-
+_openssl_assert = _make_assert(Error)
-
+
-
+
-                result = callback(connection, cert, error_number, error_depth, ok)
+                result = callback(
-                    proto = instr[1:l+1]
+                    proto = instr[1:l + 1]
-                    instr = instr[l+1:]
+                    instr = instr[l + 1:]
-                    "const unsigned char *, unsigned int, void *)",
+            ("int (*)(SSL *, unsigned char **, unsigned char *, "
-                    "const unsigned char *, unsigned int, void *)",
+            ("int (*)(SSL *, unsigned char **, unsigned char *, "
-    new SSL connections.
+    :class:`OpenSSL.SSL.Context` instances define the parameters for setting
-        }
+    }
-        load_result = _lib.SSL_CTX_load_verify_locations(self._context, cafile, capath)
+        load_result = _lib.SSL_CTX_load_verify_locations(
-        result = _lib.SSL_CTX_use_certificate_chain_file(self._context, certfile)
+        result = _lib.SSL_CTX_use_certificate_chain_file(
-        use_result = _lib.SSL_CTX_use_certificate_file(self._context, certfile, filetype)
+        use_result = _lib.SSL_CTX_use_certificate_file(
-        separately.
+        Load the trusted certificates that will be sent to the client.  Does
-        :param cafile: The name of the certificates file
+        :param bytes cafile: The path to a certificates file in PEM format.
-        resumption.
+        Set the session id to *buf* within which a session can be reused for
-        :param buf: A Python object that can be safely converted to a string
+        buf = _text_to_bytes_and_warn("buf", buf)
-        Change the cipher list
+        Set the list of ciphers to be used in this context.
-        :param cipher_list: A cipher list, see ciphers(1)
+        See the OpenSSL manual for more information (e.g.
-            cipher_list = cipher_list.encode("ascii")
+        cipher_list = _text_to_bytes_and_warn("cipher_list", cipher_list)
-            _raise_current_error()
+            raise TypeError("cipher_list must be a byte string.")
-        Set the list of preferred client certificate signers for this server context.
+        Set the list of preferred client certificate signers for this server
-        server requests a client certificate.
+        This list of certificate authorities will be sent to the client when
-                            type(ca_name).__name__,))
+                        "client CAs must be X509Name objects, not %s "
-        Add the CA certificate to the list of preferred signers for this context.
+        Add the CA certificate to the list of preferred signers for this
-        Specify a callback function to be called when clients specify a server name.
+        Specify a callback function to be called when clients specify a server
-
+        self._app_data = None
-            set_result = _lib.SSL_set_fd(self._ssl, _asFileDescriptor(self._socket))
+            set_result = _lib.SSL_set_fd(
-        the Connection object.
+        Look up attributes on the wrapped socket object if they are not found
-
+        if self._socket is None:
-        name = _lib.SSL_get_servername(self._ssl, _lib.TLSEXT_NAMETYPE_host_name)
+        name = _lib.SSL_get_servername(
-                      API, the value is ignored
+        :param flags: (optional) The only supported flag is ``MSG_PEEK``,
-        result = _lib.SSL_read(self._ssl, buf, bufsiz)
+        if flags is not None and flags & socket.MSG_PEEK:
-            API, the value is ignored.
+        :param flags: (optional) The only supported flag is ``MSG_PEEK``,
-        result = _lib.SSL_read(self._ssl, buf, nbytes)
+        if flags is not None and flags & socket.MSG_PEEK:
-        Renegotiate the session
+        Renegotiate the session.
-        :return: True if the renegotiation can be started, false otherwise
+        :return: True if the renegotiation can be started, False otherwise
-        Check if there's a renegotiation in progress, it will return false once
+        Check if there's a renegotiation in progress, it will return False once
-        connect_ex method doesn't return 0, SSL won't be initialized.
+        Connect to remote host and set up client-side SSL. Note that if the
-        Get the session cipher list
+        Retrieve the list of ciphers used by the Connection object.
-        :return: A list of cipher strings
+        :return: A list of native cipher strings.
-            has not yet happened.
+        :return: If this is a server connection, a list of X509Names
-        for SSL connections
+        The makefile() method is not implemented, since there is no dup
-
+        raise NotImplementedError(
-        :return: The shutdown state, a bitvector of SENT_SHUTDOWN, RECEIVED_SHUTDOWN.
+        :return: The shutdown state, a bitvector of SENT_SHUTDOWN,
-    def state_string(self):
+    def get_state_string(self):
-        Get a verbose state description
+        Retrieve a verbose string detailing the state of the Connection.
-        operation.
+        Checks if more data has to be read from the transport layer to complete
-        automatically by read/write.
+        Set the connection to work in server mode. The handshake will be
-        automatically by read/write.
+        Set the connection to work in client mode. The handshake will be
-            no session exists.
+        @return: An instance of :py:class:`OpenSSL.SSL.Session` or
-            version =_ffi.string(_lib.SSL_CIPHER_get_version(cipher))
+            version = _ffi.string(_lib.SSL_CIPHER_get_version(cipher))
-
+    @_requires_alpn
-from OpenSSL.version import __version__
+from OpenSSL.version import (
-    'rand', 'crypto', 'SSL', 'tsafe', '__version__']
+    "SSL", "crypto", "rand",
-from warnings import warn
+import warnings
-                text(lib.ERR_reason_error_string(error))))
+            text(lib.ERR_lib_error_string(error)),
-
+
-        warn(
+        warnings.warn(
-from time import time
+import datetime
-
+
-    elif _lib.ASN1_STRING_type(string_timestamp) == _lib.V_ASN1_GENERALIZEDTIME:
+    elif (
-
+    """
-        Generate a key of a given type, with a given number of a bits
+        Generate a key pair of the given type, with the given number of bits.
-        :param bits: The number of bits
+        This generates a key "into" the this object.
-        :return: None
+        :param type: The key type.
-                bits, _ffi.NULL, 0, _ffi.NULL, _ffi.NULL, _ffi.NULL, _ffi.NULL)
+            dsa = _lib.DSA_new()
-            if not _lib.EVP_PKEY_assign_DSA(self._pkey, dsa):
+            if not _lib.EVP_PKEY_set1_DSA(self._pkey, dsa):
-
+        This is the Python equivalent of OpenSSL's ``RSA_check_key``.
-        if _lib.EVP_PKEY_type(self._pkey.type) != _lib.EVP_PKEY_RSA:
+        if _lib.EVP_PKEY_type(self.type()) != _lib.EVP_PKEY_RSA:
-
+        return _lib.Cryptography_EVP_PKEY_id(self._pkey)
-            # Abort the whole process, I suppose...?  -exarkun
+            # The return value on this call should be num_curves again.  We
-
+    """
-        :param name: An X509Name object to copy
+        :param name: The name to copy.
-                    type(value).__name__,))
+                type(value).__name__,))
-        CN) and more...
+        organization (alias O), organizationalUnit (alias OU), commonName
-            result = _ffi.buffer(result_buffer[0], data_length)[:].decode('utf-8')
+            result = _ffi.buffer(
-        result_buffer = _ffi.new("char[]", 512);
+        result_buffer = _ffi.new("char[]", 512)
-        Return the hash value of this name
+        Return an integer representation of the first four bytes of the
-        :return: None
+        This is the Python equivalent of OpenSSL's ``X509_NAME_hash``.
-        Return the DER encoding of this name
+        Return the DER encoding of this name.
-            this name.
+        :return: The DER encoded form of this name.
-        Returns the split-up components of this name.
+        Returns the components of this name, as a sequence of 2-tuples.
-        :return: List of tuples (name, value).
+        :return: The components of this name.
-                        _lib.ASN1_STRING_length(fval))))
+                _ffi.string(name),
-        :type typename: :py:data:`str`
+        Initializes an X509 extension.
-        :param critical: A flag indicating whether this is a critical extension.
+        :param bool critical: A flag indicating whether this is a critical
-        :type value: :py:data:`str`
+        :type value: :py:data:`bytes`
-        :param subject: Optional X509 cert to use as subject.
+        :param subject: Optional X509 certificate to use as subject.
-        :param issuer: Optional X509 cert to use as issuer.
+        :param issuer: Optional X509 certificate to use as issuer.
-        :return: The X509Extension object
+        .. _extension: https://openssl.org/docs/manmaster/apps/
-        # Start off by initializing most of the fields to NULL.
+        # A context is necessary for any extension which uses the r2i
-        # any references, so no need to mess with reference counts or duplicates.
+        # any references, so no need to mess with reference counts or
-            # ext_struc it desires for its last parameter, though.)
+            # with strings? (However, X509V3_EXT_i2d in particular seems like
-        return _lib.OBJ_obj2nid(self._extension.object)
+        return _lib.OBJ_obj2nid(
-        }
+    }
-        length = self._extension.value.length
+        ext_data = _lib.X509_EXTENSION_get_data(self._extension)
-        Returns the critical field of the X509Extension
+        Returns the critical field of this X.509 extension.
-        Returns the short version of the type name of the X509Extension
+        Returns the short type name of this X.509 extension.
-        Returns the data of the X509Extension
+        Returns the data of the X509 extension, encoded as ASN.1.
-        :return: A :py:data:`str` giving the X509Extension's ASN.1 encoded data.
+        .. versionadded:: 0.12
-        Set the public key of the certificate request
+        Set the public key of the certificate signing request.
-        :return: None
+        :param pkey: The public key to use.
-        Get the public key from the certificate request
+        Get the public key of the certificate signing request.
-        :return: The public key
+        :return: The public key.
-        :return: None
+        :param int version: The version number.
-        :return: an integer giving the value of the version subfield
+        :return: The value of the version subfield.
-        Create an X509Name object for the subject of the certificate request
+        Return the subject of this certificate signing request.
-        :return: An X509Name object
+        :return: The subject of this certificate signing request.
-        Add extensions to the request.
+        Add extensions to the certificate signing request.
-        :return: None
+        :param extensions: The X.509 extensions to add.
-        Get extensions to the request.
+        Get X.509 extensions in the certificate signing request.
-        :return: A :py:class:`list` of :py:class:`X509Extension` objects.
+        .. versionadded:: 0.15
-        Sign the certificate request using the supplied key and digest
+        Sign the certificate signing request with this key and digest type.
-        :return: None
+        :param pkey: The key pair to sign with.
-        Verifies a certificate request using the supplied public key
+        Verifies the signature on this certificate signing request.
-        :raise OpenSSL.crypto.Error: If the signature is invalid or there is a
+        :param key: A public key.
-
+    """
-        Set version number of the certificate
+        Set the version number of the certificate.
-        :param version: The version number
+        :param version: The version number of the certificate.
-        :return: None
+        :return: :py:const:`None`
-        Return version number of the certificate
+        Return the version number of the certificate.
-        :return: Version number as a Python integer
+        :return: The version number of the certificate.
-        Get the public key of the certificate
+        Get the public key of the certificate.
-        :return: The public key
+        :return: The public key.
-        Set the public key of the certificate
+        Set the public key of the certificate.
-        :param pkey: The public key
+        :param pkey: The public key.
-        :return: None
+        :return: :py:data:`None`
-        Sign the certificate using the supplied key and digest
+        Sign the certificate with this key and digest type.
-        :return: None
+        :param digest: The name of the message digest to use.
-        Retrieve the signature algorithm used in the certificate
+        Return the signature algorithm used in the certificate.
-        :raise ValueError: If the signature algorithm is undefined.
+        :return: The name of the algorithm.
-        :return: The digest of the object
+        :return: The digest of the object, formatted as
-
+            b16encode(ch).upper() for ch
-        Set serial number of the certificate
+        Set the serial number of the certificate.
-        :param serial: The serial number
+        :param serial: The new serial number.
-        :return: None
+        :return: :py:data`None`
-        # actually the result.  I hope.  -exarkun
+        # it.  If bignum is still NULL after this call, then the return value
-        Return serial number of the certificate
+        Return the serial number of this certificate.
-        :return: Serial number as a Python integer
+        :return: The serial number.
-        Adjust the time stamp for when the certificate stops being valid
+        Adjust the time stamp on which the certificate stops being valid.
-                       validity time.
+        :param amount: The number of seconds by which to adjust the timestamp.
-        :return: None
+        :return: :py:const:`None`
-        time plus an offset.
+        Adjust the timestamp on which the certificate starts being valid.
-        :return: None
+        :param amount: The number of seconds by which to adjust the timestamp.
-        :return: True if the certificate has expired, false otherwise
+        :return: :py:const:`True` if the certificate has expired,
-            _ffi.cast('ASN1_UTCTIME*', notAfter), now) < 0
+        time_string = _native(self.get_notAfter())
-        Retrieve the time stamp for when the certificate starts being valid
+        Get the timestamp at which the certificate starts being valid.
-        :return: A string giving the timestamp, in the format::
+        The timestamp is formatted as an ASN.1 GENERALIZEDTIME::
-                         YYYYMMDDhhmmss-hhmm
+            YYYYMMDDhhmmssZ
-                 or None if there is no value set.
+        :return: A timestamp string, or :py:const:`None` if there is none.
-        Set the time stamp for when the certificate starts being valid
+        Set the timestamp at which the certificate starts being valid.
-        :param when: A string giving the timestamp, in the format:
+        The timestamp is formatted as an ASN.1 GENERALIZEDTIME::
-                         YYYYMMDDhhmmss-hhmm
+            YYYYMMDDhhmmssZ
-        :return: None
+        :return: :py:const:`None`
-        Retrieve the time stamp for when the certificate stops being valid
+        Get the timestamp at which the certificate stops being valid.
-        :return: A string giving the timestamp, in the format::
+        The timestamp is formatted as an ASN.1 GENERALIZEDTIME::
-                         YYYYMMDDhhmmss-hhmm
+            YYYYMMDDhhmmssZ
-                 or None if there is no value set.
+        :return: A timestamp string, or :py:const:`None` if there is none.
-        Set the time stamp for when the certificate stops being valid
+        Set the timestamp at which the certificate stops being valid.
-        :param when: A string giving the timestamp, in the format:
+            YYYYMMDDhhmmssZ
-                         YYYYMMDDhhmmss-hhmm
+        :param when: A timestamp string.
-        :return: None
+        :return: :py:const:`None`
-        Create an X509Name object for the issuer of the certificate
+        Return the issuer of this certificate.
-        :return: An X509Name object
+        This creates a new :class:`X509Name` that wraps the underlying issuer
-        Set the issuer of the certificate
+        Set the issuer of this certificate.
-        :param issuer: The issuer name
+        :param issuer: The issuer.
-        :return: None
+        :return: :py:const:`None`
-        Create an X509Name object for the subject of the certificate
+        Return the subject of this certificate.
-        :return: An X509Name object
+        This creates a new :class:`X509Name` that wraps the underlying subject
-        Set the subject of the certificate
+        Set the subject of this certificate.
-        :param subject: The subject name
+        :param subject: The subject.
-        :return: None
+
-        Get the number of extensions on the certificate.
+        Get the number of extensions on this certificate.
-        :return: The number of extensions as an integer.
+        .. versionadded:: 0.12
-        :return: None
+        :param extensions: The extensions to add.
-        :return: The X509Extension object at the specified index.
+        Extensions on a certificate are kept in order. The index
-X509Type = X509
+X509Type = X509
-
+        """
-    `OpenSSL.X509StoreContext.verify_certificate`.
+    An exception raised when an error occurred while verifying a certificate
-
+    :type certificate: :class:`X509`
-    Of these, only the set of trusted certificates is currently exposed.
+    .. note::
-        ret = _lib.X509_STORE_CTX_init(self._store_ctx, self._store._store, self._cert._x509, _ffi.NULL)
+        ret = _lib.X509_STORE_CTX_init(
-        about the failure can be obtained from the store context.
+        When a call to native OpenSSL X509_verify_cert fails, additional
-                        _lib.X509_STORE_CTX_get_error(self._store_ctx)))),
+                _lib.X509_STORE_CTX_get_error(self._store_ctx)))),
-
+        .. versionadded:: 0.15
-
+        .. versionadded:: 0.15
-        :raises: Error
+
-        x509 = _lib.d2i_X509_bio(bio, _ffi.NULL);
+        x509 = _lib.d2i_X509_bio(bio, _ffi.NULL)
-    :rtype: :py:data:`str`
+    :rtype: :py:data:`bytes`
-
+    """
-        ]
+    ]
-        Set the serial number of a revoked Revoked structure
+        Set the serial number.
-        :return: None
+        :type hex_str: :py:class:`bytes`
-        Return the serial number of a Revoked structure
+        Get the serial number.
-        :return: The serial number as a string
+        The serial number is formatted as a hexadecimal number encoded in
-            if _lib.OBJ_obj2nid(ext.object) == _lib.NID_crl_reason:
+            obj = _lib.X509_EXTENSION_get_object(ext)
-        Set the reason of a Revoked object.
+        Set the reason of this revocation.
-        If :py:data:`reason` is :py:data:`None`, delete the reason instead.
+        If :py:data:`reason` is :py:const:`None`, delete the reason instead.
-        :return: None
+        :type reason: :py:class:`bytes` or :py:class:`NoneType`
-        Return the reason of a Revoked object.
+        Set the reason of this revocation.
-        :return: The reason as a string
+        .. seealso::
-            if _lib.OBJ_obj2nid(ext.object) == _lib.NID_crl_reason:
+            obj = _lib.X509_EXTENSION_get_object(ext)
-                    print_result = _lib.M_ASN1_OCTET_STRING_print(bio, ext.value)
+                    print_result = _lib.M_ASN1_OCTET_STRING_print(
-
+        This list is a copy; modifying it does not change the supported reason
-        :param when: A string giving the timestamp, in the format:
+        Set the revocation timestamp.
-        :return: None
+        :param when: The timestamp of the revocation, as ASN.1 GENERALIZEDTIME.
-        :return: A string giving the timestamp, in the format:
+        Get the revocation timestamp.
-                         YYYYMMDDhhmmss-hhmm
+        :return: The timestamp of the revocation, as ASN.1 GENERALIZEDTIME.
-
+    """
-        Create a new empty CRL object.
+        Create a new empty certificate revocation list.
-        Return revoked portion of the CRL structure (by value not reference).
+        Return the revocations in this certificate revocation list.
-        :return: A tuple of Revoked objects.
+        :return: The revocations in this CRL.
-            revoked_copy = _X509_REVOKED_dup(revoked)
+            revoked_copy = _lib.Cryptography_X509_REVOKED_dup(revoked)
-        :type revoked: :class:`X509`
+        This revocation will be added by value, not by reference. That
-        :return: None
+        :return: :py:const:`None`
-        copy = _X509_REVOKED_dup(revoked._revoked)
+        copy = _lib.Cryptography_X509_REVOKED_dup(revoked._revoked)
-        export a CRL as a string
+        Export a CRL as a string.
-        :type cert: :class:`X509`
+        :param cert: The certificate used to sign the CRL.
-        :type key: :class:`PKey`
+        :param key: The key used to sign the CRL.
-        # A scratch time object to give different values to different CRL fields
+        # A scratch time object to give different values to different CRL
-        _lib.X509_CRL_set_issuer_name(self._crl, _lib.X509_get_subject_name(cert._x509))
+        _lib.X509_CRL_set_issuer_name(
-                "type argument must be FILETYPE_PEM, FILETYPE_ASN1, or FILETYPE_TEXT")
+        return dump_crl(type, self)
-
+    """
-        Return certificate portion of the PKCS12 structure
+        Get the certificate in the PKCS #12 structure.
-        :return: X509 object containing the certificate
+        :return: The certificate, or :py:const:`None` if there is none.
-        Replace the certificate portion of the PKCS12 structure
+        Set the certificate in the PKCS #12 structure.
-        :return: None
+        :return: :py:const:`None`
-        Return private key portion of the PKCS12 structure
+        Get the private key in the PKCS #12 structure.
-        :returns: PKey object containing the private key
+        :return: The private key, or :py:const:`None` if there is none.
-        Replace or set the certificate portion of the PKCS12 structure
+        Set the certificate portion of the PKCS #12 structure.
-        :return: None
+        :param pkey: The new private key, or :py:const:`None` to unset it.
-        Return CA certificates within of the PKCS12 object
+        Get the CA certificates in the PKCS #12 structure.
-                 if any are present, or None if no CA certificates are present.
+        :return: A tuple with the CA certificates in the chain, or
-        :return: None
+        :param cacerts: The new CA certificates, or :py:const:`None` to unset
-                    raise TypeError("iterable must only contain X509 instances")
+                    raise TypeError(
-        Replace or set the certificate portion of the PKCS12 structure
+        Set the friendly name in the PKCS #12 structure.
-        :return: None
+        :return: :py:const:`None`
-            raise TypeError("name must be a byte string or None (not %r)" % (name,))
+            raise TypeError(
-        Return friendly name portion of the PKCS12 structure
+        Get the friendly name in the PKCS# 12 structure.
-        :returns: String containing the friendlyname
+        :returns: The friendly name,  or :py:const:`None` if there is none.
-        Dump a PKCS12 object as a string.  See also "man PKCS12_create".
+        Dump a PKCS12 object as a string.
-        :param passphrase: used to encrypt the PKCS12
+        :param passphrase: The passphrase used to encrypt the structure. Unlike
-        :param iter: How many times to repeat the encryption
+        :param iter: Number of times to repeat the encryption step.
-        :param maciter: How many times to repeat the MAC
+        :param maciter: Number of times to repeat the MAC step.
-        :return: The string containing the PKCS12
+        :return: The string representation of the PKCS #12 structure.
-PKCS12Type = PKCS12
+PKCS12Type = PKCS12
-        Sign the certificate request using the supplied key and digest
+        Sign the certificate request with this key and digest type.
-        :return: None
+        :param digest: The message digest to use.
-        sign_result = _lib.NETSCAPE_SPKI_sign(self._spki, pkey._pkey, digest_obj)
+        sign_result = _lib.NETSCAPE_SPKI_sign(
-        Verifies a certificate request using the supplied public key
+        Verifies a signature on a certificate request.
-            problem verifying the signature.
+        :param key: The public key that signature is supposedly from.
-        Generate a base64 encoded string from an SPKI
+        Generate a base64 encoded representation of this SPKI object.
-        :return: The base64 encoded string
+        :return: The base64 encoded string.
-        _lib.CRYPTO_free(encoded)
+        _lib.OPENSSL_free(encoded)
-        Get the public key of the certificate
+        Get the public key of this certificate.
-        :return: The public key
+        :return: The public key.
-        :return: None
+        :return: :py:const:`None`
-            raise ValueError("only FILETYPE_PEM key format supports encryption")
+            raise ValueError(
-                    raise ValueError("passphrase returned by callback is too long")
+                    raise ValueError(
-        raise ValueError("type argument must be FILETYPE_PEM, FILETYPE_ASN1, or FILETYPE_TEXT")
+        raise ValueError(
-    signature_buffer = _ffi.new("unsigned char[]", 512)
+    pkey_length = (PKey.bits(pkey) + 7) // 8
-    Verify a signature
+    Verify a signature.
-    :return: None if the signature is correct, raise exception otherwise
+    :return: :py:const:`None` if the signature is correct, raise exception
-    verify_result = _lib.EVP_VerifyFinal(md_ctx, signature, len(signature), pkey)
+    verify_result = _lib.EVP_VerifyFinal(
-        friendlyname = _ffi.buffer(friendlyname_buffer, friendlyname_length[0])[:]
+        friendlyname_buffer = _lib.X509_alias_get0(
-See the file RATIONALE for a short explanation of why this module was written.
+import os
-    An error occurred in an `OpenSSL.rand` API.
+    An error occurred in an :mod:`OpenSSL.rand` API.
-    Get some random bytes as a string.
+    Get some random bytes from the PRNG as a string.
-    :return: A string of random bytes
+    :return: A string of random bytes.
-    Add data with a given entropy to the PRNG
+    Mix bytes from *string* into the PRNG state.
-    :return: None
+    The *entropy* argument is (the lower bound of) an estimate of how much
-    Alias for rand_add, with entropy equal to length
+    Equivalent to calling :func:`add` with *entropy* as the length of *buffer*.
-    :return: None
+
-    Retrieve the status of the PRNG
+    Check whether the PRNG has been seeded with enough data.
-    :return: True if the PRNG is seeded enough, false otherwise
+    :return: :obj:`True` if the PRNG is seeded enough, :obj:`False` otherwise.
-    just returns 0.
+    Query the system random source and seed the PRNG.
-              error, check rand.status())
+    Does *not* actually query the EGD.
-
+    seed(os.urandom(bytes))
-    :return: None
+    This is a wrapper for the C function ``RAND_cleanup``.
-    Seed the PRNG with data from a file
+    Read *maxbytes* of data from *filename* and seed the PRNG with it.
-        the entire file
+    :param maxbytes: (optional) The number of bytes to read.    Default is to
-    Save PRNG state to a file
+    Write a number of random bytes (currently 1024) to the file *path*.  This
-    :return: The number of bytes written
+    :return: The number of bytes written.
-    Windows.
+    Add the current contents of the screen to the PRNG state.
-del SSL
+from threading import RLock as _RLock
-              'connect_ex', 'sendall'):
+              'sock_shutdown', 'get_peer_certificate', 'get_peer_cert_chain',
-__version__ = '0.15.1'
+__all__ = [
-            if len(self.data_list) * config.AUTORANGE_MAXSIZE > self.max_buffer_size:
+            if self.data_size > self.max_buffer_size:
-            self.connection.report_speed(speed, self.task.content_length)
+            time_cose = time_now - self.get_head_time
-                    xlog.warn('app_msg:%s', cgi.escape(response.app_msg))
+                #if len(response.app_msg) < 2048:
-                                      (response.app_status, cgi.escape(response.app_msg)))
+                response.worker.close("range get gae status:%d" % response.app_status)
-                xlog.error('RangeFetch %r return %s :%s', self.url, response.status, cgi.escape(response.body))
+                #xlog.error('RangeFetch %r return %s :%s', self.url, response.status, cgi.escape(response.body))
-                    xlog.warn('body:%s', cgi.escape(response.body))
+                #if len(response.body) < 2048:
-            p = threading.Thread(target=self.create_connection_daemon)
+            p = threading.Thread(target=self.keep_connection_daemon)
-
+    def keep_connection_daemon(self):
-                time.sleep(1)
+            if self.new_conn_pool.qsize() > self.connection_pool_min_num:
-                connect_start_num = 0
+            self.connect_process()
-                xlog.warning("no enough ip")
+                # xlog.warning("no enough ip")
-                ssl_sock.last_use_time = time.time()
+            ssl_sock = self._create_ssl_connection( (ip_str, 443) )
-            self.thread_num_lock.release()
+            self.new_conn_pool.put((ssl_sock.handshake_time, ssl_sock))
-                self.new_conn_pool.qsize(only_h1=True) < 1):
+                (self.new_conn_pool.qsize() < self.https_new_connect_num or
-                self.new_conn_pool.qsize(only_h1=True) < 1:
+                    self.new_conn_pool.qsize(only_h1=True) < 1:
-                    time.sleep(60)
+                    time.sleep(60)
-                    break
+                ssl_sock = self._create_ssl_connection( (ip_str, 443) )
-                               threading.currentThread().ident, percent,
+                    xlog.debug('RangeFetch [%s] %d%% length:%s range:%s %s %s',
-                    xlog.warn("RangeFetch [%s] get body fail %s", threading.currentThread().ident, self.url)
+                    xlog.warn("RangeFetch [%s] get body fail %s", response.ssl_sock.ip, self.url)
-                #xlog.warn("recv fail:%r", e)
+                xlog.exception("recv fail:%r", e)
-            self.task.content_length = int(self.response_headers.get("Content-Length", "0"))
+            self.task.content_length = int(self.response_headers["Content-Length"][0])
-from http_common import *
+import http_common
-            raise GAE_Exception(1, "no appid can use")
+            raise http_common.GAE_Exception(1, "no appid can use")
-            raise GAE_Exception(1, "no ssl_sock")
+            raise http_common.GAE_Exception(1, "no ssl_sock")
-        task = Task(headers, body, q)
+        task = http_common.Task(headers, body, q)
-                if not filename.startswith('.') and filename not in ['README.md', 'start', 'start.bat', 'start.vbs']:
+                if filename in ['start.sh', 'start.command', 'start.lnk', 'LICENSE.txt', 'download.md', 'version.txt', 'xxnet', 'xxnet.bat', 'xxnet.vbs', 'xx_net.sh']:
-                if not filename.startswith('.') and filename not in ['code', 'data', 'SwitchyOmega']:
+                if filename in ['goagent', 'python27', 'gae_proxy', 'php_proxy', 'x_tunnel', 'python3', 'Python3', 'lib', 'SwitchySharp']:
-                    self.report_speed(speed, body_length)
+                    time_cost = (time.time() - time_response)
-        data = response.body.get(2)
+        data = response.task.read(size=2)
-        data = response.body.get(headers_length)
+        data = response.task.read(size=headers_length)
-    # force the get file range
+    # force to get content range
-            # don't add range, github don't support this it.
+            # don't add range, some host like github don't support Range.
-        return
+    if method == "HEAD":
-            xlog.debug("send to browser wfile.write ret:%d", ret)
+    body_sended = 0
-            xlog.warn('gae_handler send to browser return %r %r', e_b, url)
+            if ret == ssl.SSL_ERROR_WANT_WRITE or ret == ssl.SSL_ERROR_WANT_READ:
-        del self.response
+        threading.Thread(target=self.fetch, args=(res_begin, res_end, self.response)).start()
-            self.fetch(begin, end)
+            self.fetch(begin, end, None)
-    def fetch(self, begin, end):
+    def fetch(self, begin, end, first_response):
-                continue
+            if first_response:
-                data_len = expect_len
+            data_readed = 0
-            self.put_data(begin, data)
+                self.put_data(begin, data)
-                    content_length, content_range, self.url, response.task.get_trace())
+                expect_len -= data_len
-        headers['Host'] = self.ssl_sock.host
+        task.headers['Host'] = self.ssl_sock.host
-        request_data += ''.join('%s: %s\r\n' % (k, v) for k, v in headers.items())
+        request_data += ''.join('%s: %s\r\n' % (k, v) for k, v in task.headers.items())
-            payload_len = len(payload)
+            payload_len = len(task.body)
-                sended = self.ssl_sock.send(payload[start:start+send_size])
+                sended = self.ssl_sock.send(task.body[start:start+send_size])
-            task.set_state("h1_get_head")
+        #except httplib.BadStatusLine as e:
-            body_length = int(response.getheader("Content-Length", "0"))
+        # read response body,
-                    speed = len(response.body) / (time.time() - time_response)
+                    speed = body_length / (time.time() - time_response)
-                    return response
+                    self.report_speed(speed, body_length)
-                to_read = end - start
+                to_read = max(end - start, 65535)
-                        xlog.warn("%s read timeout t:%d len:%d left:%d ",
+                        xlog.warn("%s read timeout t:%d expect:%d left:%d ",
-                        return False
+                        break
-                response_body.append(data)
+                task.put_data(data)
-        return False
+            xlog.warn("%s h1_request:%r", self.ip, e)
-            self.retry_task_cb(stream.task)
+            if stream.get_head_time:
-        if len(self.streams) < self.max_concurrent and self.remote_window_size > 10000:
+        if self.keep_running and len(self.streams) < self.max_concurrent and self.remote_window_size > 10000:
-            self.response_body_len += len(frame.data)
+            self.task.put_data(frame.data)
-            self._close_remote()
+            time_now = time.time()
-            self.send_response()
+            self._close_remote()
-        response = BaseResponse(status=status, headers=self.response_headers, body=body)
+        response = BaseResponse(status=status, headers=self.response_headers)
-        self.connection.report_speed(speed)
+        self.task.put_data("")
-    def report_speed(self, speed):
+    def report_speed(self, speed, body_length):
-    xlog.info("GAE t:%d s:%d %s %s", (time.time()-request_time)*1000, len(response.body), method, url)
+    xlog.info("GAE t:%d s:%d %s %s %s", (time.time()-request_time)*1000, len(response.body), method, url,
-            xlog.debug('RangeFetch [thread %s] %d%% begin:%d end:%d data_len:%d length:%s range:%s %s',
+            xlog.debug('RangeFetch [thread %s] %d%% begin:%d end:%d data_len:%d length:%s range:%s %s %s',
-                    content_length, content_range, self.url)
+                    content_length, content_range, self.url, response.task.get_trace())
-        response = self._request(headers, payload)
+        task.set_state("h1_req")
-    def _request(self, headers, payload):
+    def _request(self, task):
-            time_response = time.time()
+            time_response = last_read_time = time.time()
-            xlog.exception("%s head appid:%s request fail:%r", self.ssl_sock.ip, self.ssl_sock.appid, e)
+            xlog.warn("%s head appid:%s request fail:%r", self.ssl_sock.ip, self.ssl_sock.appid, e)
-            elif e[0] == 10053:
+            elif e[0] == 10053 or e[0] == 10054 or e[0] == 10038:
-                if not filename.startswith('.') and filename not in ['code', 'data']:
+                if not filename.startswith('.') and filename not in ['code', 'data', 'SwitchyOmega']:
-            target_conn_num = (1 - (connect_control.inactive_time()/(10*60))) * self.connection_pool_min_num
+            target_conn_num = self.connection_pool_min_num
-            data = response.read(config.AUTORANGE_BUFSIZE)
+            to_read = end - start + 1
-import urllib
+import collections
-    headers = clean_empty_header(headers)
+
-        return RangeFetch(method, url, headers, body, response, wfile).fetch()
+        # use org_headers
-    if body_length != len(response.body):
+    if body_length != len(response.body) and method != "HEAD":
-    bufsize = config.AUTORANGE_BUFSIZE
+class RangeFetch2(object):
-        self.expect_begin = 0
+        self.keep_running = True
-            response_headers['Content-Length'] = str(length)
+        res_begin, res_end, res_length = tuple(int(x) for x in re.search(r'bytes (\d+)-(\d+)/(\d+)', content_range).group(1, 2, 3))
-            response_headers['Content-Length'] = str(length-start)
+            response_headers['Content-Range'] = 'bytes %s-%s/%s' % (res_begin, self.req_end, res_length)
-        xlog.info('>>>>>>>>>>>>>>> RangeFetch started(%r) %d-%d', self.url, start, end)
+        xlog.info('RangeFetch %d-%d started(%r) ', res_begin, self.req_end, self.url)
-            self.wfile.write("HTTP/1.1 200 OK\r\n")
+            self.wfile.write("HTTP/1.1 %d OK\r\n" % state_code)
-        thread_num = min(self.threads, range_queue.qsize())
+        data_left_to_fetch = self.req_end - self.req_begin + 1
-            spawn_later(i*0.1, self.__fetchlet, range_queue, data_queue, range_delay_size)
+            threading.Thread(target=self.fetch_worker).start()
-                        break
+        self.put_data(res_begin, data)
-                break
+                    data = self.data_list[self.wait_begin]
-        self._stopped = True
+        self.keep_running = False
-            except Queue.Empty:
+    def fetch_worker(self):
-                time.sleep(2)
+            with self.lock:
-                range_queue.put((start, end, None))
+                response.worker.close("range get gae status:%d app_msg:%s" % \
-                response.status = response.app_status
+            response.status = response.app_status
-                    self.method, self.url, response.headers, start, end)
+                    self.method, self.url, response.headers, begin, end)
-            start += data_len
+            if data_len > expect_len:
-                continue
+            self.put_data(begin, data)
-            xlog.info('>>>>>>>>>>>>>>> Successfully reached %d bytes.', start - 1)
+            begin += data_len
-            xlog.warn("%s keep alive fail, inactive_time:%d head_timeout:%d",
+            xlog.debug("%s keep alive fail, inactive_time:%d head_timeout:%d",
-            elif e[0] == 10054 and e[1] == "WSAECONNRESET":
+            elif e[0] == 10053:
-                xlog.debug("recv_into 0")
+                # xlog.debug("recv_into 0")
-                    xlog.debug("recv_into 0")
+                    # xlog.debug("recv_into 0")
-                xlog.warn("read request line empty")
+                # xlog.warn("read request line empty")
-
+#!/usr/bin/env python2
-            end = body_length - 1
+            end = body_length
-                if start > end:
+                if start >= end:
-                data = response.read(65535)
+                to_read = end - start
-        out_str = ''
+        out_str = 'thread num:%d\r\n' % threading.activeCount()
-        while True:
+        while self._connection:
-    import OpenSSL
+    import OpenSSL as oss_test
-        th.start()
+        threading.Thread(target=self.work_loop).start()
-        last_request_time = time.time()
+    def keep_alive_thread(self):
-                    return
+            time_to_ping = max(ping_interval - (time.time() - self.last_active_time), 0)
-                    return
+            time_now = time.time()
-                last_ssl_active_time = time.time()
+    def work_loop(self):
-            last_request_time = time.time()
+            # xlog.debug("http1 get task")
-            #xlog.debug("%s Send:%s", self.ip, str(frame))
+            # xlog.debug("%s Send:%s", self.ip, str(frame))
-        #xlog.debug("%s Recv:%s", self.ip, str(frame))
+        # xlog.debug("%s Recv:%s", self.ip, str(frame))
-    idle_time = 10 * 60
+    idle_time = 20 * 60
-        xlog.info('GAE %s %s:%d ', self.command, host, port)
+        # xlog.info('https GAE %s %s:%d ', self.command, host, port)
-        xlog.debug('GAE CONNECT %s %s', self.command, self.path)
+        # xlog.debug('GAE CONNECT %s %s', self.command, self.path)
-    sys.path.append(linux_lib)
+    platform_lib = os.path.join(python_path, 'lib', 'linux')
-    sys.path.append(win32_lib)
+    platform_lib = os.path.join(python_path, 'lib', 'win32')
-    sys.path.append(darwin_lib)
+    platform_lib = os.path.abspath( os.path.join(python_path, 'lib', 'darwin'))
-    os._exit(0)
+    xlog.info("use build-in openssl lib")
-logger.setLevel(logging.DEBUG)
+# import logging
-            logger.debug("buffer socket flush:%d", len(data))
+            # logger.debug("buffer socket flush:%d", len(data))
-    log.debug("Encoding %d with %d bits", integer, prefix_bits)
+    # log.debug("Encoding %d with %d bits", integer, prefix_bits)
-    log.debug("Decoded %d, consumed %d bytes", number, index + 1)
+    # log.debug("Decoded %d, consumed %d bytes", number, index + 1)
-        )
+        # log.debug("Setting header table size to %d from %d", value, self._header_table_size)
-                )
+                # log.debug( "Removed %s: %s from the encoder header table", n, v )
-        log.debug("HPACK encoding %s", headers)
+        # log.debug("HPACK encoding %s", headers)
-        log.debug("Encoded header block to %s", header_block)
+        # log.debug("Encoded header block to %s", header_block)
-        log.debug("Adding %s to the header table", to_add)
+        # log.debug("Adding %s to the header table", to_add)
-            log.debug("Evicted %s: %s from the header table", n, v)
+            # log.debug("Evicted %s: %s from the header table", n, v)
-        )
+        # log.debug( "Resizing decoder header table to %d from %d", value, self._header_table_size)
-                log.debug("Evicting %s: %s from the header table", n, v)
+                # log.debug("Evicting %s: %s from the header table", n, v)
-        log.debug("Decoding %s", data)
+        # log.debug("Decoding %s", data)
-            log.debug("Evicting %s: %s from the header table", n, v)
+            # log.debug("Evicting %s: %s from the header table", n, v)
-        log.debug("Decoded %s, consumed %d", header, consumed)
+        # log.debug("Decoded %s, consumed %d", header, consumed)
-        )
+        # log.debug(  "Decoded %s, total consumed %d bytes, indexed %s", header, total_consumed, should_index)
-    log.debug("Using nghttp2's HPACK implementation.")
+    # log.debug("Using nghttp2's HPACK implementation.")
-    log.debug("Using our pure-Python HPACK implementation.")
+    # log.debug("Using our pure-Python HPACK implementation.")
-            log.debug("Setting header table size to %d", value)
+            # log.debug("Setting header table size to %d", value)
-            log.debug("HPACK encoding %s", headers)
+            # log.debug("HPACK encoding %s", headers)
-            log.debug("Setting header table size to %d", value)
+            # log.debug("Setting header table size to %d", value)
-            log.debug("Decoding %s", data)
+            # log.debug("Decoding %s", data)
-import httplib
+import cgi
-                    xlog.warn('app_msg:%s', urllib.urlencode(response.app_msg))
+                    xlog.warn('app_msg:%s', cgi.escape(response.app_msg))
-                xlog.error('RangeFetch %r return %s :%s', self.url, response.status, urllib.urlencode(response.body))
+                xlog.error('RangeFetch %r return %s :%s', self.url, response.status, cgi.escape(response.body))
-                    xlog.warn('body:%s', urllib.urlencode(response.body))
+                    xlog.warn('body:%s', cgi.escape(response.body))
-        """
+	  <string>-hungup</string>
-                fastest_time = time
+            hs_time = self.pool[sock]
-        if fastest_sock and not fastest_sock.h2:
+        if not fastest_sock.h2:
-        return (fastest_time, fastest_sock)
+        return fastest_time, fastest_sock
-            return (slowest_handshake_time, slowest_sock)
+            return slowest_handshake_time, slowest_sock
-                #xlog.debug("inactive_time:%d", inactive_time * 1000)
+                # xlog.debug("inactive_time:%d", inactive_time * 1000)
-                self.new_conn_pool.qsize(only_h1=True) < 1:
+                (self.new_conn_pool.qsize() < self.https_new_connect_num or \
-            while self.new_conn_pool.qsize() < self.https_new_connect_num:
+            while self.new_conn_pool.qsize() < self.https_new_connect_num or \
-        if not fastest_sock.h2:
+        if fastest_sock and not fastest_sock.h2:
-                self.new_conn_pool.qsize() < self.https_new_connect_num:
+                self.new_conn_pool.qsize() < self.https_new_connect_num and \
-            raise Exception(600, b"".join(error_msg))
+            raise GAE_Exception(600, b"".join(error_msg))
-                    xlog.warning('RangeFetch %s return %r', headers['Range'], response)
+                    xlog.warning('RangeFetch %s return %r', headers['Range'], e)
-        if os.path.isfile(self.user_range_file):
+    def load_range_content(self, default=False):
-            xlog.debug("%s Send:%s", self.ip, str(frame))
+            #xlog.debug("%s Send:%s", self.ip, str(frame))
-        xlog.debug("%s increase send win:%d result:%d", self.ip, inc_size, self.remote_window_size)
+        #xlog.debug("%s increase send win:%d result:%d", self.ip, inc_size, self.remote_window_size)
-        xlog.debug("%s close stream:%d %s", self.ssl_sock.ip, stream_id, reason)
+        #xlog.debug("%s close stream:%d %s", self.ssl_sock.ip, stream_id, reason)
-        xlog.debug("%s Recv:%s", self.ip, str(frame))
+        #xlog.debug("%s Recv:%s", self.ip, str(frame))
-                xlog.debug("%s frame size:%d increase win:%d", self.ip, size, increment)
+                #xlog.debug("%s frame size:%d increase win:%d", self.ip, size, increment)
-                    xlog.debug("rtt:%f ping_time:%f now:%f", rtt, ping_time, time_now)
+                    xlog.error("rtt:%f ping_time:%f now:%f", rtt, ping_time, time_now)
-                xlog.debug("RTT:%d, on_way:%d", self.rtt, self.ping_on_way)
+                #xlog.debug("RTT:%d, on_way:%d", self.rtt, self.ping_on_way)
-            xlog.warning("%s Received unknown frame, type %d", self.ip, frame.type)
+            xlog.error("%s Received unknown frame, type %d", self.ip, frame.type)
-                xlog.warning("%s Frame size %d is outside of allowed range", self.ip, new_size)
+                xlog.error("%s Frame size %d is outside of allowed range", self.ip, new_size)
-                    xlog.debug("stream:%d frame size:%d increase win:%d", self.stream_id, size, increment)
+                #if increment:
-            xlog.warning("%s Received unknown frame, type %d", self.ip, frame.type)
+            xlog.error("%s Received unknown frame, type %d", self.ip, frame.type)
-xlog.set_buffer(500)
+xlog.set_buffer(2000)
-                # google_ip.report_connect_closed(response.ssl_sock.ip, "no range")
+                xlog.warning('RangeFetch "%s %s" return headers=%r, retry %s-%s',
-                #raise Exception("app check fail %r" % status)
+                xlog.debug("appid:%s head fail status:%d", self.ssl_sock.appid, status)
-            xlog.exception("%s head %s request fail:%r", self.ssl_sock.ip, self.ssl_sock.appid, e)
+            xlog.exception("%s head appid:%s request fail:%r", self.ssl_sock.ip, self.ssl_sock.appid, e)
-        if check_ip.test_gae_ip2(ip, "xxnet-1"):
+        ret = check_ip.test_gae_ip2(ip, "xxnet-1")
-                self.working_appid_list.remove(appid)
+                try:
-        xlog.exception("alpn:%r", e)
+        #xlog.exception("alpn:%r", e)
-    if use_openssl:
+def test_gae_ip2(ip, appid="xxnet-1"):
-            return False
+            if not check_goagent(ssl_sock, appid):
-            xlog.exception("test_gae_ip %s e:%r",ip, e)
+            xlog.warn("check fail:%r", e)
-        return False
+        return ssl_sock
-        return False
+        return ssl_sock
-        return False
+        return ssl_sock
-            return False
+            return ssl_sock
-        return False
+        return ssl_sock
-        return False
+        return ssl_sock
-        print res
+        if not res:
-            connect_time = int((time_connected - time_begin) * 1000)
+            def verify_SSL_certificate_issuer(ssl_sock):
-def fetch_by_gae(method, url, headers, body):
+def request_gae_server(headers, body):
-    response = http_dispatch.request(request_headers, body)
+    return request_headers, body
-    check_local_network.report_network_ok()
+def unpack_response(response):
-        response.reason = response.reason.strip()
+        _, status, reason = raw_response_line.split(None, 2)
-        headers_pairs = headers_data.split('\r\n')
+        headers_block, app_msg = headers_data.split('\r\n\r\n')
-            return False
+            raise Exception(600, b"".join(error_msg))
-                continue
+            response = request_gae_server(request_headers, request_body)
-                continue
+            check_local_network.report_network_ok()
-        xlog.warn("GAE %s %s request fail", method, url)
+
-        xlog.warn("%s response.body len:%d, expect:%d", url, len(response.body), body_length)
+        xlog.warn("%s %s response.body len:%d, expect:%d", method, url, len(response.body), body_length)
-                continue
+                try:
-            if result:
+            if result and result.support_gae:
-                if not result:
+                if not result or not result.support_gae:
-            else:
+            elif result.support_gae:
-            time_to_ping = min(0, 55 - (time.time() - last_ssl_active_time))
+            time_to_ping0 = 55 - (time.time() - last_ssl_active_time)
-                    self.close("keep alive, maybe not support")
+                    self.close("keep alive")
-                xlog.warn("head send len:%d %d", ret, len(data))
+                xlog.warn("head send len:%r %d", ret, len(data))
-                raise Exception("app check fail %r" % status)
+                #raise Exception("app check fail %r" % status)
-    def __init__(self, status=506, reason="", headers={}, body=""):
+    def __init__(self, status=601, reason="", headers={}, body=""):
-            xlog.exception("ssl send:%r", e)
+            #xlog.exception("ssl send:%r", e)
-                    connect_manager.https_manager.clean_old_connection()
+                    http_dispatch.close_all_worker()
-        if not result:
+        if not result or not result.support_gae:
-                xlog.warn("all appid out of quota, need 10 min to reset")
+            time_to_reset = 600 - (time.time() - self.last_reset_time)
-        xlog.info("migrating to 3.0.5+")
+        xlog.info("migrating to 3.x.x")
-            self.working_appid_list.remove(appid)
+            try:
-                    self.ssl_timeout_cb(ssl_sock)
+                    try:
-        stream = Stream(self.ip, stream_id, self.ssl_sock.host, task,
+        stream = Stream(self, self.ip, stream_id, self.ssl_sock.host, task,
-                self.on_ssl_created_cb(ssl_sock)
+                try:
-                xlog.exception("recv_into:%r", e)
+                #xlog.exception("recv_into:%r", e)
-            cipher_suites = ('ALL:!RC4-SHA:!ECDHE-RSA-RC4-SHA:!ECDHE-RSA-AES128-GCM-SHA256:!AES128-GCM-SHA256',)
+            cipher_suites = ('ALL:!RC4-SHA:!ECDHE-RSA-RC4-SHA:!ECDHE-RSA-AES128-GCM-SHA256:!AES128-GCM-SHA256:!ECDHE-RSA-AES128-SHA:!AES128-SHA',)
-                if sys.platform != 'win32' and filename == 'xxnet':
+                if sys.platform != 'win32' and filename == 'start':
-    xlog = getLogger("gae_proxy")
+    class xlog():
-#    openssl_context.set_session_cache_mode(OpenSSL.SSL.SESS_CACHE_BOTH)
+openssl_context.set_session_id(binascii.b2a_hex(os.urandom(10)))
-    except:
+        xlog.debug("%s alpn h2:%s", ip, h2)
-        xlog.info("%s CN:%s", ip, ssl_cert.cn)
+    xlog.info("%s CN:%s", ssl_sock.ip, ssl_cert.cn)
-            xlog.warn("app check %s status:%d", appid, response.status)
+        xlog.warn("app check %s status:%d", appid, response.status)
-                xlog.warn("503 but server type:%s", server_type)
+            xlog.warn("503 but server type:%s", server_type)
-                xlog.info("503 server type:%s", server_type)
+            xlog.info("503 server type:%s", server_type)
-            xlog.warn("app check %s ip:%s status:%d", appid, ip, response.status)
+        xlog.warn("app check %s ip:%s status:%d", appid, ip, response.status)
-            xlog.warn("app check %s content:%s", appid, content)
+        xlog.warn("app check %s content:%s", appid, content)
-        xlog.info("check_goagent ok")
+    xlog.info("check_goagent ok")
-                xlog.warn("connect timeout")
+            xlog.warn("connect timeout")
-                xlog.exception("test_gae_ip %s e:%r",ip, e)
+            xlog.exception("test_gae_ip %s e:%r",ip, e)
-                xlog.warn("ip:%s not support http/2", ip)
+        if not ssl_sock.h2:
-                    xlog.exception("check fail:%r", e)
+                xlog.warn("check fail:%r", e)
-            xlog.exception("http2 get response fail:%r", e)
+        xlog.exception("http2 get response fail:%r", e)
-            xlog.warn("app check %s status:%d", appid, response.status)
+        xlog.warn("app check %s status:%d", appid, response.status)
-                xlog.warn("503 but server type:%s", server_type)
+            xlog.warn("503 but server type:%s", server_type)
-                xlog.info("503 server type:%s", server_type)
+            xlog.info("503 server type:%s", server_type)
-            xlog.warn("app check %s ip:%s status:%d", appid, ip, response.status)
+        xlog.warn("app check %s ip:%s status:%d", appid, ip, response.status)
-            xlog.warn("app check %s content:%s", appid, content)
+        xlog.warn("app check %s content:%s", appid, content)
-        xlog.info("check_goagent ok")
+    xlog.info("check_goagent ok")
-        if check_ip.test_gae_ip(ip, "xxnet-1"):
+        if check_ip.test_gae_ip2(ip, "xxnet-1"):
-            except:
+            except Exception as e:
-        self.AUTORANGE_WAITSIZE = self.CONFIG.getint('autorange', 'waitsize')
+import check_local_network
-        for i in xrange(0, self.threads):
+
-            spawn_later(float(range_delay_size)/self.waitsize, self.__fetchlet, range_queue, data_queue, range_delay_size)
+            spawn_later(i*0.1, self.__fetchlet, range_queue, data_queue, range_delay_size)
-            if self.expect_begin < start and data_queue.qsize() * self.bufsize + range_delay_size > 30*1024*1024:
+            if start > self.expect_begin and data_queue.qsize() * self.bufsize + range_delay_size > 30*1024*1024:
-                time.sleep(10)
+                time.sleep(2)
-                    (self.ip_dict[ip]['down_fail'] * 500 )
+                ip_rate[ip] = self._ip_rate(self.ip_dict[ip])
-                the_100th_handshake_time = self.ip_dict[the_100th_ip]['handshake_time']
+                the_100th_handshake_time = self._ip_rate(self.ip_dict[the_100th_ip])
-        check_local_network.network_stat = "OK"
+        check_local_network.report_network_ok()
-            if check_local_network.network_stat == "Fail":
+            if not check_local_network.is_ok():
-                check_local_network.triger_check_network()
+            check_local_network.report_network_fail()
-            if check_local_network.network_stat == "Fail":
+            if not check_local_network.is_ok():
-            result = check_ip.test_gae_ip(ip)
+            result = check_ip.test_gae_ip2(ip)
-                    self.close("idle 2 mins")
+                    self.close("idle")
-                    google_ip.report_connect_fail(self.ssl_sock.ip, force_remove=True)
+                    # google_ip.report_connect_fail(self.ssl_sock.ip, force_remove=True)
-import httplib
+from config import config
-SSLError = OpenSSL.SSL.WantReadError
+g_cacertfile = os.path.join(current_path, "cacert.pem")
-
+        if not ssl_sock:
-        result = check_ip.test_gae_ip(ip)
+        result = check_ip.test_gae_ip2(ip)
-subprocess.Popen([sys.executable, start_sript])
+
-        self.max_timeout = 15
+        self.max_timeout = 60
-            time.sleep(1)
+            time.sleep(0.5)
-                if time.time() - last_request_time > 2 * 60:
+                if time.time() - last_request_time > self.idle_time:
-from openssl_wrap import SSLConnection
+
-            xlog.warn("create_worker_thread get ssl_sock fail")
+        try:
-        self.on_ssl_created_cb(ssl_sock)
+                self.on_ssl_created_cb(ssl_sock)
-        threading.Thread(target=self.create_worker_thread).start()
+        if self.create_worker_th:
-        if idle_num == 0:
+        if idle_num == 0 or len(self.workers) < self.min_worker_num:
-                result = check_ip.test_gae_ip(ip)
+                result = check_ip.test_gae_ip2(ip)
-    print_range_list(ip_range_list)
+    if False:
-    print_range_list(ip_range_list)
+        ip_range_list = filter_ip_range(ip_range_list, bad_range_list)
-import random
+
-    input('Press any to exit...')
+    print("Try install python-openssl or cffi\r\n")
-        input('Press any to exit...')
+
-        if len(self.send_buffer) > 1300:
+        if len(self.send_buffer) > 1300 or flush:
-        # xlog.debug("ip:%s http/1.1", ip)
+
-                ssl_sock.h2 = False
+            try:
-        return response
+    try:
-    return response
+    except Exception as e:
-            xlog.warn("goaway:%s", error_string)
+            if frame.additional_data != "session_timed_out":
-            self.close("GoAway")
+            self.close("GoAway:%s" % error_string)
-
+openssl_version = OpenSSL.version.__version__
-    def __iowait2(self, io_func, *args, **kwargs):
+    def __iowait(self, io_func, *args, **kwargs):
-        global  ssl_version
+        global ssl_version, support_alpn_npn
-
+
-
+def get_openssl_version():
-        return "error_string:%s additional_data:%s" % (error_string, self.additional_data)
+        out_str = ""
-            #xlog.warn("ip:%s not support http/2", ip)
+            if __name__ == "__main__":
-    response = conn.get_response()
+    try:
-                self.new_conn_pool.put((ssl_sock.handshake_time, ssl_sock))
+
-                    if self.new_conn_pool.qsize() > self.connection_pool_max_num and self.ssl_timeout_cb:
+                    if self.new_conn_pool.qsize() >= self.connection_pool_max_num and self.ssl_timeout_cb:
-            time_to_ping = min(0, 50 - (time.time() - last_ssl_active_time))
+            time_to_ping = min(0, 55 - (time.time() - last_ssl_active_time))
-                    self.close("idle 4 mins")
+                if time.time() - last_request_time > 2 * 60:
-                    self.close("keep alive")
+                    google_ip.report_connect_fail(self.ssl_sock.ip, force_remove=True)
-                    self.close("send fail")
+                else:
-    def __iowait(self, io_func, *args, **kwargs):
+    def __iowait2(self, io_func, *args, **kwargs):
-                   )"""
+top_path = os.path.abspath( os.path.join(root_path, os.pardir, os.pardir))
-              os.path.abspath( os.path.join(root_path, "launcher", "start.py")) + "\""
+    run_cmd = "\"" + os.path.join(top_path, "start.vbs") + "\""
-    run_cmd = os.path.abspath( os.path.join(root_path, os.pardir, os.pardir, "xxnet"))
+    run_cmd = os.path.join(top_path, "start")
-    run_cmd = os.path.abspath( os.path.join(root_path, "launcher", "start.py"))
+    run_cmd = os.path.join(top_path, "start")
-                if not filename.startswith('.') and filename not in ['README.md', 'xxnet', 'xxnet.bat', 'xxnet.vbs']:
+                if not filename.startswith('.') and filename not in ['README.md', 'start', 'start.bat', 'start.vbs']:
-    os._exit(0)
+    time.sleep(20)
-            return self._sck.send(data)
+            self.flush()
-                raise Exception("send fail")
+
-from openssl_wrap import SSLConnection
+import openssl_wrap
-    openssl_context.set_session_cache_mode(OpenSSL.SSL.SESS_CACHE_BOTH)
+openssl_context = openssl_wrap.SSLConnection.context_builder(ca_certs=g_cacertfile)
-def connect_ssl(ip, port=443, timeout=5, openssl_context=None, check_cert=True):
+def connect_ssl(ip, port=443, timeout=5, check_cert=True):
-    ssl_sock = SSLConnection(openssl_context, sock, ip)
+    ssl_sock = openssl_wrap.SSLConnection(openssl_context, sock, ip)
-        ssl_sock = connect_ssl(ip, timeout=max_timeout, openssl_context=openssl_context)
+        ssl_sock = connect_ssl(ip, timeout=max_timeout)
-    conn = hyper.HTTP20Connection(host='%s.appspot.com'%appid, ip=ip, port=443, ssl_content=content)
+def test_gae_ip2(ip, appid="xxnet-1", use_openssl=True):
-        xlog.exception("gae %r", e)
+        #xlog.exception("gae %r", e)
-        server_type = response.getheader('Server', "")
+        server_type = response.headers.get('Server', "")
-            return True
+            return ssl_sock
-    return True
+    return ssl_sock
-        self.ip_traffic_quota_base = self.CONFIG.getint('google_ip', 'ip_traffic_quota_base')
+        self.ip_connect_interval = self.CONFIG.getint('google_ip', 'ip_connect_interval')
-from appids_manager import appid_manager
+        self.h1_num = 0
-        return len(self.pool)
+    def qsize(self, only_h1=False):
-    def get(self, block=True, timeout=None):
+    def get(self, block=True, timeout=None, only_h1=False):
-                if not self.qsize():
+                if not self.qsize(only_h1=only_h1):
-                while not self.qsize():
+                while not self.qsize(only_h1=only_h1):
-                while not self.qsize():
+                while not self.qsize(only_h1=only_h1):
-            item = self._get()
+            item = self._get(only_h1=only_h1)
-        return self.get(block=False)
+    def get_nowait(self, only_h1=False):
-    def _get(self):
+    def _get(self, only_h1=False):
-            #    continue
+            if only_h1 and sock.h2:
-        str = ''
+        out_str = ''
-                str += "%d \t %s handshake:%d not_active_time:%d\r\n" % (i, sock.ip, t, time.time() -sock.last_use_time)
+                out_str += "%d \t %s handshake:%d not_active_time:%d h2:%d\r\n" % (i, sock.ip, t, time.time() -sock.last_use_time, sock.h2)
-        return str
+        return out_str
-            p = threading.Thread(target = self.keep_alive_thread)
+        p = threading.Thread(target=self.keep_alive_thread)
-        p.start()
+        self.https_new_connect_num = config.CONFIG.getint("connect_manager", "https_new_connect_num")
-        work_thread.start()
+    def set_ssl_time_handler(self, cb):
-        while self.keep_alive and connect_control.keep_running:
+        while connect_control.keep_running:
-            to_keep_live_list = new_list + old_list
+            to_keep_live_list = self.new_conn_pool.get_need_keep_alive(maxtime=self.keep_alive-3)
-                if inactive_time > self.keep_alive:
+                if inactive_time > self.keep_alive or not self.ssl_timeout_cb:
-                    self.start_keep_alive(ssl_sock)
+                    # put ssl to worker
-            #self.create_more_connection()
+        # only used by direct mode now, host ssl.
-
+    def create_more_connection_worker(self):
-                    break
+            while self.new_conn_pool.qsize() < self.https_new_connect_num:
-                    self.new_conn_pool.put((ssl_sock.handshake_time, ssl_sock))
+                    if self.new_conn_pool.qsize() > self.connection_pool_max_num and self.ssl_timeout_cb:
-            sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 32*1024)
+            sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 64*1024)
-            xlog.debug("create_ssl update ip:%s time:%d", ip, handshake_time)
+            xlog.debug("create_ssl update ip:%s time:%d h2:%d", ip, handshake_time, ssl_sock.h2)
-                    ret = self.host_conn_pool[host].get_nowait()
+                    ret = self.host_conn_pool[host].get_nowait(only_h1=True)
-            ret = self.new_conn_pool.get(True, self.max_timeout)
+            ret = self.new_conn_pool.get(True, self.max_timeout, only_h1=only_h1)
-    def get_new_ssl(self):
+    def get_new_ssl(self, only_h1=True):
-        ret = self.new_conn_pool.get(True, self.max_timeout)
+        ret = self.new_conn_pool.get(True, self.max_timeout, only_h1=only_h1)
-def fetch(method, host, path, headers, payload, bufsize=8192):
+def fetch(method, host, path, headers, payload):
-import socket
+import zlib
-
+from http_dispatcher import http_dispatch
-        self.message = message
+def send_response(wfile, status=404, headers={}, body=''):
-                continue
+def return_fail_message(wfile):
-    raise GAE_Exception(2, "try max times")
+def clean_empty_header(headers):
-    return zlib.decompress(data, -zlib.MAX_WBITS)
+    return headers
-def fetch(method, url, headers, body):
+def fetch_by_gae(method, url, headers, body):
-    # GAE donot allow set `Host` header
+    # GAE don't allow set `Host` header
-    response = request(request_headers, body)
+    response = http_dispatch.request(request_headers, body)
-    response.app_msg = ''
+    response.app_headers = response.headers
-    data = response.read(2)
+    data = response.body.get(2)
-    data = response.read(headers_length)
+    data = response.body.get(headers_length)
-        del headers[key]
+    headers_pairs = headers_data.split('\r\n')
-    return headers
+    return response
-def handler(method, url, headers, body, wfile):
+def request_gae_proxy(method, url, headers, body):
-            return return_fail_message(wfile)
+        if time.time() - time_request > 60: #time out
-                    continue
+            response = fetch_by_gae(method, url, headers, body)
-                #xlog.warning('APPID %r not exists, remove it.', response.ssl_sock.appid)
+                # xlog.warning('APPID %r not exists, remove it.', response.ssl_sock.appid)
-                    continue
+                # google_ip.report_connect_closed(response.ssl_sock.ip, "appid not exist")
-                response.close()
+                response.worker.close("ip not support GAE")
-                break
+                # google_ip.report_connect_closed(response.ssl_sock.ip, "out of quota")
-            errors.append(e)
+def handler(method, url, headers, body, wfile):
-            xlog.warn("gae_handler.handler send response fail. t:%d e:%r %s", time.time()-time_request, e, url)
+    xlog.info("GAE t:%d s:%d %s %s", (time.time()-request_time)*1000, len(response.body), method, url)
-        body_length = end - start + 1
+    if 'X-Head-Content-Length' in response_headers:
-                    speed = body_length / (time_finished - time_response)
+    try:
-                    continue
+    if body_length != len(response.body):
-            google_ip.report_connect_closed(response.ssl_sock.ip, "Net")
+    try:
-        xlog.exception("gae_handler except:%r %s", e, url)
+            xlog.warn('gae_handler send to browser return %r %r', e_b, url)
-        response_headers = dict((k.title(), v) for k, v in self.response.getheaders())
+        response_headers = dict((k.title(), v) for k, v in self.response.headers.items())
-                    continue
+                start, end, response = range_queue.get(timeout=1)
-                    continue
+            if self.expect_begin < start and data_queue.qsize() * self.bufsize + range_delay_size > 30*1024*1024:
-                    continue
+            if not response:
-                        continue
+            if not response:
-import os
+import os, sys
-            result = check_ip.test_gae_ip(ip)
+            result = check_ip.test_gae_ip2(ip)
-                    break
+                    raise
-                    break
+                    raise
-                return ''
+                # remote closed
-        # Google video ip can act as Google FrontEnd if cipher suits not include RC4-SHA:ECDHE-RSA-RC4-SHA:ECDHE-RSA-AES128-GCM-SHA256:AES128-GCM-SHA256
+    def npn_select_callback(conn, protocols):
-            ssl_context.set_verify(OpenSSL.SSL.VERIFY_NONE, lambda c, x, e, d, ok: ok)
+            ssl_context.set_verify(OpenSSL.SSL.VERIFY_NONE, lambda c, x,    e, d, ok: ok)
-            controler = web_control.ControlHandler(self.client_address, self.headers, self.command, self.path, self.rfile, self.wfile)
+            controller = web_control.ControlHandler(self.client_address, self.headers, self.command, self.path, self.rfile, self.wfile)
-                return controler.do_GET()
+                return controller.do_GET()
-                return controler.do_POST()
+                return controller.do_POST()
-                    "<td>%d</td><td>%d</td><td>%d</td><td>%d</td><td>%s</td></tr>\n" % \
+                    "<td>%d</td><td>%d</td><td>%s</td></tr>\n" % \
-                    active_time, transfered_data, transfered_quota, str_out)
+                    active_time, str_out)
-python_path = os.path.abspath( os.path.join(root_path, 'python27', '1.0'))
+python_path = os.path.join(root_path, 'python27', '1.0')
-        input('Press any to exit...')
+        input('Press any to exit...')
-    'rand', 'crypto', 'SSL', 'tsafe', '__version__']
+# This file is dual licensed under the terms of the Apache License, Version
-        self._ssl_conn = apply(_ssl.Connection, args)
+        self._ssl_conn = _ssl.Connection(*args)
-__version__ = '0.13'
+__version__ = '0.15.1'
-            (should_read)):
+        if ((self._remaining_capacity > self._bytes_in_buffer) and (should_read)):
-    def __init__(self, host, ip=None, port=None, secure=None, window_manager=None, enable_push=False,
+    def __init__(self, ssl_sock, host=None, ip=None, port=None, secure=None, window_manager=None, enable_push=False,
-        ).format(type=type(self).__name__, stream=self.stream_id, flags=flags, body=body)
+        out_str = "{type}".format(type=type(self).__name__)
-        return (frame, length)
+        return frame, length
-    defined_flags = [Flag('END_HEADERS', 0x04),]
+    defined_flags = [Flag('END_HEADERS', 0x04), ]
-__version__ = '0.13.1'
+__all__ = ['FFI', 'VerificationError', 'VerificationMissing', 'CDefError',
-        # xlog.debug("client version:%d, auth num:%d, list:%s", 5, auth_mode_num, utils.str2hex(data))
+        try:
-    shutil.rmtree(os.path.join(top_path, 'launcher')) # launcher is for auto-update from 2.X
+    if os.path.isdir(os.path.join(top_path, 'launcher')):
-    
+
-        xlog.info("migrating to 3.0.5")
+        xlog.info("migrating to 3.0.5+")
-        elif reqs['cmd'] == ['set_range']:
+
-            user_config.user_special.scan_ip_thread_num = int(self.postvars['scan_ip_thread_num'][0])
+            #check ip_range checksums, update if needed
-            data = '{"res":"success"}'
+            #update google_ip settings
-        data_path = os.path.abspath( os.path.join(root_path, 'data', 'launcher', 'config.yaml'))
+        data_path = os.path.abspath( os.path.join(top_path, 'data', 'launcher', 'config.yaml'))
-    run_cmd = os.path.abspath( os.path.join(root_path, "start.sh"))
+    run_cmd = os.path.abspath( os.path.join(root_path, os.pardir, os.pardir, "xxnet"))
-    run_cmd = os.path.abspath( os.path.join(root_path, "start.sh"))
+    run_cmd = os.path.abspath( os.path.join(root_path, "launcher", "start.py"))
-download_path = os.path.abspath( os.path.join(root_path, 'data', 'downloads'))
+download_path = os.path.abspath( os.path.join(root_path, os.pardir, os.pardir, 'data', 'downloads'))
-    xlog.info("start XX-Net %s", update_from_github.current_version())
+    current_version = update_from_github.current_version()
-                    os.chmod(dst_file, st.st_mode)
+                    if sys.platform != 'win32' and os.path.isfile(dst_file):
-        p = re.compile(r'https://codeload.github.com/XX-net/XX-Net/zip/([0-9]+)\.([0-9]+)\.([0-9]+)')
+        p = re.compile(r'https://codeload.github.com/XX-net/XX-Net/zip/([0-9]+)\.([0-9]+)\.([0-9]+) ([0-9a-f]*)')
-                versions.append([m.group(0), version])
+                hashsum = m.group(4)
-        raise "get_version_fail:" % readme_file
+
-def sha1_file(filename):
+def get_hash_sum(version):
-    hasher = hashlib.sha1()
+    hasher = hashlib.sha256()
-                if not os.path.isfile(dst_file) or sha1_file(src_file) != sha1_file(dst_file):
+                if not os.path.isfile(dst_file) or hash_file_sum(src_file) != hash_file_sum(dst_file):
-        raise Exception("download xxnet zip fail:%s" % download_path)
+        raise Exception("download xxnet zip fail:%s" % xxnet_zip_file)
-        import_command = 'security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ../../data/gae_proxy/CA.crt'# % certfile.decode('utf-8')
+        import_command = 'security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ../../../../data/gae_proxy/CA.crt'# % certfile.decode('utf-8')
-        raise
+        raise e
-helper_path = os.path.join(current_path, os.pardir, 'data', 'launcher', 'helper')
+helper_path = os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'data', 'launcher', 'helper')
-    run_cmd = os.path.abspath( os.path.join(root_path, "start.command"))
+    run_cmd = os.path.abspath( os.path.join(root_path, "start.sh"))
-                    deploy_proc = subprocess.Popen([sys.executable, script_path, appid])
+                    args = [sys.executable, script_path, appid]
-        logging.info("Usage: uploader.py <appids> ")
+        logging.info("Usage: uploader.py <appids> [-debug]")
-        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(root_path, 'data', 'gae_proxy', 'config.ini'))
+        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(top_path, 'data', 'gae_proxy', 'config.ini'))
-        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(root_path, 'data', 'gae_proxy', 'config.ini'))
+        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(top_path, 'data', 'gae_proxy', 'config.ini'))
-def restart_xxnet():
+def restart_xxnet(version):
-    start_script = os.path.join(current_path, "start.py")
+    start_script = os.path.join(top_path, "code", version, "launcher", "start.py")
-        restart_xxnet()
+        restart_xxnet(version)
-from openssl_wrap import SSLConnection, init_context
+from openssl_wrap import SSLConnection
-data_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, 'data', 'gae_proxy'))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir))
-from openssl_wrap import SSLConnection
+from openssl_wrap import SSLConnection, init_context
-        res = test_gae_ip(ip)
+        res = test_gae_ip2(ip)
-_simple_check_worker()
+
-        self.DATA_PATH = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'data', 'gae_proxy'))
+        self.DATA_PATH = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir, os.pardir, 'data', 'gae_proxy'))
-data_root = os.path.join(root_path, 'data')
+data_root = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, 'data'))
-data_path = os.path.join(root_path, 'data')
+data_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, 'data'))
-        version_file = os.path.join(root_path, "launcher", "version.txt")
+        version_file = os.path.join(root_path, "version.txt")
-data_path = os.path.join(root_path, 'data')
+data_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, 'data'))
-data_path = os.path.join(root_path, 'data')
+data_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, 'data'))
-data_path = os.path.join(root_path, 'data')
+data_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, 'data'))
-data_root = os.path.join(root_path, 'data')
+data_root = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, 'data'))
-    readme_file = os.path.join(root_path, "launcher", "version.txt")
+    readme_file = os.path.join(root_path, "version.txt")
-        return versions[0][1]
+        with open(readme_file) as fd:
-        return "get_version_fail"
+        xlog.warn("get_version_fail in update_from_github")
-    readme_url = "https://raw.githubusercontent.com/XX-net/XX-Net/master/launcher/update_version.txt"
+    readme_url = "https://raw.githubusercontent.com/XX-net/XX-Net/master/code/default/update_version.txt"
-    xlog.info("update file finished.")
+    overwrite(xxnet_version, xxnet_unzip_path)
-data_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, 'data', 'php_proxy'))
+data_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, os.pardir, os.pardir, 'data', 'php_proxy'))
-        self.DATA_PATH = os.path.join(root_path, "data", "php_proxy")
+        self.DATA_PATH = os.path.join(root_path, os.pardir, os.pardir, "data", "php_proxy")
-    CONFIG_USER_FILENAME = os.path.abspath( os.path.join(root_path, 'data', 'php_proxy', 'config.ini'))
+    CONFIG_USER_FILENAME = os.path.abspath( os.path.join(root_path, os.pardir, os.pardir, 'data', 'php_proxy', 'config.ini'))
-            raise e
+            err_string = "bind to %s:%d fail:%r" % (addr[0], addr[1], e)
-data_path = os.path.join(root_path, 'data')
+data_path = os.path.abspath(os.path.join(root_path, os.pardir, os.pardir, 'data'))
-        conn = httplib.HTTPConnection("www.baidu.com", 80, timeout=3)
+        conn = httplib.HTTPSConnection("www.microsoft.com", 443, timeout=30)
-        version_file = os.path.join(root_path, "version.txt")
+        version_file = os.path.join(root_path, "launcher", "version.txt")
-    readme_file = os.path.join(root_path, "README.md")
+    readme_file = os.path.join(root_path, "launcher", "version.txt")
-    readme_target = os.path.join(download_path, "README.md")
+    readme_url = "https://raw.githubusercontent.com/XX-net/XX-Net/master/launcher/update_version.txt"
-clean_old_file()
+clean_old_file()
-from SystemConfiguration import *
+import AppKit
-class MacTrayObject(NSObject):
+class MacTrayObject(AppKit.NSObject):
-        self.statusitem = self.statusbar.statusItemWithLength_(NSSquareStatusItemLength) #NSSquareStatusItemLength #NSVariableStatusItemLength
+        self.statusbar = AppKit.NSStatusBar.systemStatusBar()
-        image = NSImage.alloc().initByReferencingFile_(icon_path.decode('utf-8'))
+        image = AppKit.NSImage.alloc().initByReferencingFile_(icon_path.decode('utf-8'))
-        self.menu = NSMenu.alloc().initWithTitle_('XX-Net')
+        self.menu = AppKit.NSMenu.alloc().initWithTitle_('XX-Net')
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Config', 'config:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Config', 'config:', '')
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(getCurrentServiceMenuItemTitle(), None, '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(getCurrentServiceMenuItemTitle(), None, '')
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Auto GAEProxy', 'enableAutoProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Auto GAEProxy', 'enableAutoProxy:', '')
-            menuitem.setState_(NSOnState)
+            menuitem.setState_(AppKit.NSOnState)
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global GAEProxy', 'enableGlobalProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global GAEProxy', 'enableGlobalProxy:', '')
-            menuitem.setState_(NSOnState)
+            menuitem.setState_(AppKit.NSOnState)
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Disable GAEProxy', 'disableProxy:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Disable GAEProxy', 'disableProxy:', '')
-            menuitem.setState_(NSOnState)
+            menuitem.setState_(AppKit.NSOnState)
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Reload GAEProxy', 'resetGoagent:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Reload GAEProxy', 'resetGoagent:', '')
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Quit', 'windowWillClose:', '')
+        menuitem = AppKit.NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Quit', 'windowWillClose:', '')
-        NSApp.setActivationPolicy_(NSApplicationActivationPolicyProhibited)
+        AppKit.NSApp.setActivationPolicy_(AppKit.NSApplicationActivationPolicyProhibited)
-        self.disableGaeProxyMenuItem.setState_(NSOffState)
+        self.autoGaeProxyMenuItem.setState_(AppKit.NSOffState)
-            self.autoGaeProxyMenuItem.setState_(NSOnState)
+            self.autoGaeProxyMenuItem.setState_(AppKit.NSOnState)
-            self.globalGaeProxyMenuItem.setState_(NSOnState)
+            self.globalGaeProxyMenuItem.setState_(AppKit.NSOnState)
-            self.disableGaeProxyMenuItem.setState_(NSOnState)
+            self.disableGaeProxyMenuItem.setState_(AppKit.NSOnState)
-        alert = NSAlert.alloc().init()
+        alert = AppKit.NSAlert.alloc().init()
-        self.alertReturn = alert.runModal() == NSAlertFirstButtonReturn
+        self.alertReturn = alert.runModal() == AppKit.NSAlertFirstButtonReturn
-        nc.addObserver_selector_name_object_(self, 'windowWillClose:', NSWorkspaceWillPowerOffNotification, None)
+        nc = AppKit.NSWorkspace.sharedWorkspace().notificationCenter()
-        NSApp.terminate_(self)
+        AppKit.NSApp.terminate_(self)
-    status = SCDynamicStoreCopyValue(None, "State:/Network/Global/" + protocol)
+    status = SystemConfiguration.SCDynamicStoreCopyValue(None, "State:/Network/Global/" + protocol)
-    service = SCDynamicStoreCopyValue(None, "Setup:/Network/Service/" + serviceID)
+    service = SystemConfiguration.SCDynamicStoreCopyValue(None, "Setup:/Network/Service/" + serviceID)
-@objc.callbackFor(CFNotificationCenterAddObserver)
+@AppKit.objc.callbackFor(AppKit.CFNotificationCenterAddObserver)
-    app = NSApplication.sharedApplication()
+    app = AppKit.NSApplication.sharedApplication()
-    CFNotificationCenterAddObserver(nc, None, networkChanged, "com.apple.system.config.network_change", None, CFNotificationSuspensionBehaviorDeliverImmediately)
+    nc = AppKit.CFNotificationCenterGetDarwinNotifyCenter()
-            if socks_version == 4:
+            socks_version = self.read_bytes(1)
-            elif socks_version == 5:
+            elif socks_version == "\x05":
-                xlog.warn("socks version:%d not supported", socks_version)
+                xlog.warn("socks version:%s not supported",  utils.str2hex(socks_version))
-            
+
-        data += "<th>data_active</th><th>transfered_data</th><th>Trans</th><th>history</th></tr>\n"
+        data += "<table><tr><th>N</th><th>IP</th><th>HS</th><th>Fails</th>"
-                    (i, ip, handshake_time, fail_times, links, get_time, success_time, fail_time,
+            data += "<tr><td>%d</td><td>%s</td><td>%d</td><td>%d</td><td>%d</td><td>%d</td><td>%d</td><td>%d</td><td>%d</td>" \
-#
+                 # 'down_fail' => times of fails when download content data
-                    
+
-                self.add_ip(ip, handshake_time, domain, server, fail_times)
+                self.add_ip(ip, handshake_time, domain, server, fail_times, down_fail)
-                        (ip, property['domain'], property['server'], property['handshake_time'], property['fail_times']) )
+                    fd.write( "%s %s %s %d %d %d\n" %
-                ip_rate[ip] = self.ip_dict[ip]['handshake_time'] + (self.ip_dict[ip]['fail_times'] * 1000)
+                ip_rate[ip] = self.ip_dict[ip]['handshake_time'] + \
-    def add_ip(self, ip, handshake_time, domain=None, server='', fail_times=0):
+    def add_ip(self, ip, handshake_time, domain=None, server='', fail_times=0, down_fail=0):
-                                    "success_time":0, "get_time":0, "links":0}
+                                    "success_time":0, "get_time":0, "links":0,
-                
+
-        
+
-        for i in range(0, 10):
+        for i in range(0, 50):
-google_ip = IpManager()
+google_ip = IpManager()
-                        (u"éå¯ GAEProxy", None, self.on_restart_gae_proxy, 0))
+                        (u"éå¯ GAEProxy", None, self.on_restart_gae_proxy, 0),
-                        (u"Reset GAEProxy", None, self.on_restart_gae_proxy, 0))
+                        (u"Reset GAEProxy", None, self.on_restart_gae_proxy, 0),
-                win32elevate.elevateAdminRun(os.path.abspath(__file__))
+                try:
-        image = NSImage.alloc().initByReferencingFile_(icon_path)
+        image = NSImage.alloc().initByReferencingFile_(icon_path.decode('utf-8'))
-                executeCommand            = rootCommand.encode('utf-8')
+                executeCommand            = 'do shell script "%s;%s" with administrator privileges' % (disableAutoProxyCommand, disableGlobalProxyCommand)
-                os.system(executeCommand)
+                subprocess.call(['osascript', '-e', executeCommand])
-            executeCommand            = rootCommand.encode('utf-8')
+            executeCommand            = 'do shell script "%s;%s" with administrator privileges' % (disableGlobalProxyCommand, enableAutoProxyCommand)
-            os.system(executeCommand)
+            subprocess.call(['osascript', '-e', executeCommand])
-            executeCommand            = rootCommand.encode('utf-8')
+            executeCommand            = 'do shell script "%s;%s" with administrator privileges' % (disableAutoProxyCommand, enableGlobalProxyCommand)
-            os.system(executeCommand)
+            subprocess.call(['osascript', '-e', executeCommand])
-            executeCommand            = rootCommand.encode('utf-8')
+            executeCommand            = 'do shell script "%s;%s" with administrator privileges' % (disableAutoProxyCommand, disableGlobalProxyCommand)
-            os.system(executeCommand)
+            subprocess.call(['osascript', '-e', executeCommand])
-        chmodCommand   = "chmod 4777 \\\"%s\\\"" % helper_path
+        chmodCommand   = "chmod 4755 \\\"%s\\\"" % helper_path
-        executeCommand = rootCommand.encode('utf-8')
+        executeCommand = 'do shell script "%s;%s;%s;%s" with administrator privileges' % (rmCommand, cpCommand, chmodCommand, chownCommand)
-        os.system(executeCommand)
+        subprocess.call(['osascript', '-e', executeCommand])
-    # should use config.yaml to bing ip
+    # should use config.yaml to bind ip
-helper_path = os.path.join(current_path, '..', 'data', 'launcher', 'helper')
+helper_path = os.path.join(current_path, os.pardir, 'data', 'launcher', 'helper')
-        self.setupHelper()
+        setupHelper()
-        proxyState = self.getProxyState(currentService)
+        proxyState = getProxyState(currentService)
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(self.getCurrentServiceMenuItemTitle(), None, '')
+        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_(getCurrentServiceMenuItemTitle(), None, '')
-        self.currentServiceMenuItem.setTitle_(self.getCurrentServiceMenuItemTitle())
+        self.currentServiceMenuItem.setTitle_(getCurrentServiceMenuItemTitle())
-        proxyState = self.getProxyState(currentService)
+        proxyState = getProxyState(currentService)
-        services = filter(lambda service : service and service.find('*') == -1 and self.getProxyState(service) != 'disable', services) # Remove disabled services and empty lines
+        services = filter(lambda service : service and service.find('*') == -1 and getProxyState(service) != 'disable', services) # Remove disabled services and empty lines
-                map(self.helperDisableGlobalProxy, services)
+                map(helperDisableAutoProxy, services)
-                disableGlobalProxyCommand = ';'.join(map(self.getDisableGlobalProxyCommand, services))
+                disableAutoProxyCommand   = ';'.join(map(getDisableAutoProxyCommand, services))
-            self.helperEnableAutoProxy(currentService)
+            helperDisableGlobalProxy(currentService)
-            enableAutoProxyCommand    = self.getEnableAutoProxyCommand(currentService)
+            disableGlobalProxyCommand = getDisableGlobalProxyCommand(currentService)
-            self.helperEnableGlobalProxy(currentService)
+            helperDisableAutoProxy(currentService)
-            enableGlobalProxyCommand  = self.getEnableGlobalProxyCommand(currentService)
+            disableAutoProxyCommand   = getDisableAutoProxyCommand(currentService)
-            self.helperDisableGlobalProxy(currentService)
+            helperDisableAutoProxy(currentService)
-            disableGlobalProxyCommand = self.getDisableGlobalProxyCommand(currentService)
+            disableAutoProxyCommand   = getDisableAutoProxyCommand(currentService)
-        return "networksetup -setautoproxystate \\\"%s\\\" off" % service
+def setupHelper():
-        return "%s;%s" % (enableHttpProxyCommand, enableHttpsProxyCommand)
+def getEnableGlobalProxyCommand(service):
-        return "%s;%s" % (disableHttpProxyCommand, disableHttpsProxyCommand)
+def getDisableGlobalProxyCommand(service):
-        subprocess.check_call([helper_path, 'enableauto', service, 'http://127.0.0.1:8086/proxy.pac'])
+# Call helper
-        subprocess.check_call([helper_path, 'disableauto', service])
+def helperDisableAutoProxy(service):
-        subprocess.check_call([helper_path, 'enablehttps', service, '127.0.0.1', '8087'])
+def helperEnableGlobalProxy(service):
-        subprocess.check_call([helper_path, 'disablehttps', service])
+def helperDisableGlobalProxy(service):
-        executeResult = subprocess.check_output(checkAutoProxyUrlCommand, shell=True)
+        executeResult = subprocess.check_output(['networksetup', '-getautoproxyurl', service])
-        executeResult = subprocess.check_output(checkGlobalProxyUrlCommand, shell=True)
+        executeResult = subprocess.check_output(['networksetup', '-getwebproxy', service])
-        executeResult = subprocess.check_output(listNetworkServicesCommand, shell=True)
+        executeResult = subprocess.check_output(['networksetup', '-listallnetworkservices'])
-            executeCommand            = rootCommand.encode('utf-8')
+            try:
-        executeCommand            = rootCommand.encode('utf-8')
+        try:
-        os.system(executeCommand)
+            xlog.info("try enable auto proxy:%s", executeCommand)
-        executeCommand            = rootCommand.encode('utf-8')
+        try:
-        os.system(executeCommand)
+            xlog.info("try enable global proxy:%s", executeCommand)
-        os.system(executeCommand)
+        try:
-        self.updateConfig('disable')
+    # Call helper
-                if sys_tray.dialog_yes_no(msg, u"Install", None, None) == 1:
+                if sys_tray.presentAlert_withTitle_(msg, "Install"):
-                    if sys_tray.dialog_yes_no(msg, u"Download", None, None) == 1:
+                    if sys_tray.presentAlert_withTitle_(msg, "Download"):
-        executeResult  = subprocess.check_output(executeCommand, shell=True)
+    def getProxyState(self, service):
-        executeResult  = subprocess.check_output(executeCommand, shell=True)
+        checkGlobalProxyUrlCommand = 'networksetup -getwebproxy "%s"' % service
-        proxyState = self.getProxyState()
+        proxyState = self.getProxyState(currentService)
-        disableGaeProxyMenuItem = self.menu.itemWithTitle_('Disable GAEProxy')
+        self.currentServiceMenuItem.setTitle_(self.getCurrentServiceMenuItemTitle())
-        disableGaeProxyMenuItem.setState_(NSOffState)
+        self.autoGaeProxyMenuItem.setState_(NSOffState)
-        proxyState = self.getProxyState()
+        proxyState = self.getProxyState(currentService)
-            autoGaeProxyMenuItem.setState_(NSOnState)
+            self.autoGaeProxyMenuItem.setState_(NSOnState)
-            globalGaeProxyMenuItem.setState_(NSOnState)
+            self.globalGaeProxyMenuItem.setState_(NSOnState)
-            disableGaeProxyMenuItem.setState_(NSOnState)
+            self.disableGaeProxyMenuItem.setState_(NSOnState)
-        self.disableProxy_(None)
+        listNetworkServicesCommand = 'networksetup -listallnetworkservices'
-        executeCommand                      = rootCommand.encode('utf-8')
+        disableGlobalProxyCommand = self.getDisableGlobalProxyCommand(currentService)
-        xlog.info("try enable proxy:%s", executeCommand)
+        xlog.info("try enable auto proxy:%s", executeCommand)
-        executeCommand                      = rootCommand.encode('utf-8')
+        disableAutoProxyCommand   = self.getDisableAutoProxyCommand(currentService)
-        xlog.info("try enable proxy:%s", executeCommand)
+        xlog.info("try enable global proxy:%s", executeCommand)
-        executeCommand                      = rootCommand.encode('utf-8')
+        disableAutoProxyCommand   = self.getDisableAutoProxyCommand(currentService)
-sys_tray = Mac_tray()
+    # Generate commands for Apple Script
-    app.setDelegate_(delegate)
+    app.setDelegate_(sys_tray)
-threading.stack_size(128*1024)
+try:
-        logging.info("å¦ææ²¡æå¯ç¨ä¸¤é¶æ®µç»å½ï¼è¯·åè®¸å¼±å®å¨åºç¨: https://www.google.com/settings/security/lesssecureapps")
+        logging.info("å¦æå·²å¯ç¨ä¸¤æ­¥éªè¯ï¼è¯·ç³è¯·åºç¨ä¸ç¨å¯ç : https://security.google.com/settings/security/apppasswords")
-        xlog.info("create ca")
+        xlog.info("create CA")
-        xlog.info("detect Win10, enable connect concurent control, interval:%d", config.connect_interval)
+        xlog.info("detected Win10, enable connect concurrency control, interval:%d", config.connect_interval)
-            xlog.info("start %s time cost %d", module, (finished_time - start_time) * 1000)
+            xlog.info("start %s time cost:%d ms", module, (finished_time - start_time) * 1000)
-        #logging.debug("CA key:%s", key)
+        #xlog.debug("CA key:%s", key)
-                #logging.debug("inactive_time:%d", inactive_time * 1000)
+                #xlog.debug("inactive_time:%d", inactive_time * 1000)
-            #logging.info("public appid don't keep alive")
+            #xlog.info("public appid don't keep alive")
-        #logging.debug("head request %s", host)
+        #xlog.debug("head request %s", host)
-            #logging.debug("create ssl conn %s", ip_str)
+            #xlog.debug("create ssl conn %s", ip_str)
-                #logging.debug("create ssl conn %s", ip_str)
+                #xlog.debug("create ssl conn %s", ip_str)
-            #logging.debug("Head1 %s: %s", keyword, cookie)
+            #xlog.debug("Head1 %s: %s", keyword, cookie)
-        #logging.debug("Head1 %s: %s", keyword, value)
+        #xlog.debug("Head1 %s: %s", keyword, value)
-        #logging.debug("Head1 %s: %s", keyword, value)
+        #xlog.debug("Head1 %s: %s", keyword, value)
-            #logging.debug("Head1 %s: %s", keyword, cookie)
+            #xlog.debug("Head1 %s: %s", keyword, cookie)
-        #logging.debug("Head1 %s: %s", keyword, value)
+        #xlog.debug("Head1 %s: %s", keyword, value)
-        #logging.debug("Head1 %s: %s", keyword, value)
+        #xlog.debug("Head1 %s: %s", keyword, value)
-        #logging.warn("_request bad status line:%r", e)
+        #xlog.warn("_request bad status line:%r", e)
-    #    logging.debug("Send %s: %s", k, v)
+    #    xlog.debug("Send %s: %s", k, v)
-                #logging.debug("Head- %s: %s", key, value)
+                #xlog.debug("Head- %s: %s", key, value)
-                #logging.debug("Head %s: %s", key.title(), value)
+                #xlog.debug("Head %s: %s", key.title(), value)
-                #logging.info("load ip: %s time:%d domain:%s server:%s", ip, handshake_time, domain, server)
+                #xlog.info("load ip: %s time:%d domain:%s server:%s", ip, handshake_time, domain, server)
-                #logging.warning("no gws ip")
+                #xlog.warning("no gws ip")
-            #logging.debug("update ip:%s not exist", ip)
+            #xlog.debug("update ip:%s not exist", ip)
-                    #logging.info("add  %s  CN:%s  type:%s  time:%d  gws:%d ", ip,
+                    #xlog.info("add  %s  CN:%s  type:%s  time:%d  gws:%d ", ip,
-            #logging.debug("random.randint %d - %d", ip_range[0], ip_range[1])
+            #xlog.debug("random.randint %d - %d", ip_range[0], ip_range[1])
-                #logging.debug("payload_len:%d %s %s", payload_len, self.command, self.path)
+                #xlog.debug("payload_len:%d %s %s", payload_len, self.command, self.path)
-                    #logging.debug("payload_len:%d %s %s", payload_len, self.command, self.path)
+                    #xlog.debug("payload_len:%d %s %s", payload_len, self.command, self.path)
-        #    logging.debug("m:%s id:%d", k, v['menu_sort_id'])
+        #    xlog.debug("m:%s id:%d", k, v['menu_sort_id'])
-            #logging.debug("m:%s id:%d", module, v['menu_sort_id'])
+            #xlog.debug("m:%s id:%d", module, v['menu_sort_id'])
-        #logging.exception("web_control http_request:%s fail:%s", url, e)
+        #xlog.exception("web_control http_request:%s fail:%s", url, e)
-        #logging.debug("cert_import_ready return:%s", content)
+        #xlog.debug("cert_import_ready return:%s", content)
-        if ssl_sock.appid.startswith("xxnet-") and ssl_sock.appid[7:].isdigit():
+        if ssl_sock.appid in config.PUBLIC_APPIDS:
-        readme_file = os.path.join(root_path, "README.md")
+        version_file = os.path.join(root_path, "version.txt")
-                    return version
+            fd = open(version_file, "r")
-        mimetype = 'application/octet-stream'
+        mimetype = 'application/x-x509-ca-cert'
-        self.wfile.write(('HTTP/1.1 200\r\nContent-Disposition: attachment; filename=CA.crt\r\nContent-Type: %s\r\nContent-Length: %s\r\n\r\n' % (mimetype, len(data))).encode())
+        self.wfile.write(('HTTP/1.1 200\r\nContent-Disposition: inline; filename=CA.crt\r\nContent-Type: %s\r\nContent-Length: %s\r\n\r\n' % (mimetype, len(data))).encode())
-import re
+    # GAE don't support command like OPTION
-        """deploy fake cert to client"""
+        """send fake cert to client"""
-__password__ = ''
+__password__ = ''
-        mimetype = "text/plain"
+        mimetype = 'application/octet-stream'
-                            xlog.warn("Enable Ipv6 but check failed.")
+                            xlog.warn("IPv6 was enabled, but check failed.")
-    autoproxy = '127.0.0.1:8087'
+    autoproxy = '127.0.0.1:%s' % config.LISTEN_PORT
-                return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\nContent-Length: 0\r\n\r\n' % self.path).encode())
+                content_length = 'Content-Length: 0\r\n'
-            xlog.debug('socks handler read error %r', e)
+            xlog.exception('socks handler read error %r', e)
-appengine_rpc.HttpRpcServer.DEFAULT_COOKIE_FILE_PATH = './.appcfg_cookies'
+
-        os.remove(appengine_rpc.HttpRpcServer.DEFAULT_COOKIE_FILE_PATH)
+        os.remove(cookie_file)
-                    deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd, rc4_passwd])
+                    deploy_proc = subprocess.Popen([sys.executable, script_path, appid])
-ch.setLevel(logging.DEBUG)
+logger = logging.getLogger()
-import socket
+import logging
-    global defined_password
+def upload(appid):
-    my_stdout.write("============  Begin upload  ============\r\nappid:%s \r\n\r\n" % (appid))
+    logging.info("============  Begin upload  ============")
-                result = appcfg.AppCfgApp(['appcfg', 'rollback', dirname], password_input_fn=getpass_getpass, raw_input_fn = my_input, error_fh = my_stdout).Run()
+                #"--noauth_local_webserver"
-                result = appcfg.AppCfgApp(['appcfg', 'update', dirname], password_input_fn=getpass_getpass, raw_input_fn = my_input, error_fh = my_stdout).Run()
+                result = appcfg.AppCfgApp(['appcfg', 'update', dirname]).Run()
-                my_stdout.write("upload  fail: %s\n\n" % e)
+                logging.info("upload  fail: %s" % e)
-                my_stdout.write("upload  fail: %s\n\n" % e)
+                logging.exception("upload fail:%r", e)
-                    my_stdout.write("Retry %d time...\n\n" % (i + 1))
+                    logging.info("Retry %d time..." % (i + 1))
-                    my_stdout.write("Retry max time, failed.\n\n" )
+                    logging.info("Retry max time, failed." )
-
+
-        my_stdout.write("appid wrong:%s\n" % appid)
+        logging.info("appid wrong:%s" % appid)
-        my_stdout.write(u'appid:%s format err, check http://appengine.google.com !' % appid)
+        logging.info(u'appid:%s format err, check http://appengine.google.com !' % appid)
-        my_stdout.write(u'appid ä¸è½åå« ios/android/mobile ç­å­æ ·ã')
+        logging.info(u'appid:%s format err, check http://appengine.google.com !' % appid)
-            my_stdout.write('Setting in the %s password failed!\n' % file_name)
+            logging.info('Setting in the %s password failed!' % file_name)
-def uploads(appids, email, password, rc4_password):
+def uploads(appids, rc4_password=""):
-            if upload(appid, email, password):
+            if upload(appid):
-        my_stdout.write("å¦ææ²¡æå¯ç¨ä¸¤é¶æ®µç»å½ï¼è¯·åè®¸å¼±å®å¨åºç¨: https://www.google.com/settings/security/lesssecureapps\n")
+        logging.info("Auth fail. Please check you password.")
-    my_stdout.write("=======================\n")
+    logging.info("=======================")
-        my_stdout.write("Deploy %d appid successed.\n" % len(success_appid_list))
+        logging.info("Deploy %d appid successed." % len(success_appid_list))
-        my_stdout.write("Deploy failed appid list:\n")
+        logging.info("Deploy failed appid list:")
-            my_stdout.write("- %s\n" % appid)
+            logging.info("- %s" % appid)
-    do_clean_up()
+    logging.info("== END ==")
-        my_stdout.write("Usage: uploader.py <appids> <email> [password] [rc4_password] [proxy]\r\n")
+    if len(sys.argv) < 2:
-        my_stdout.write("== END ==\n")
+        logging.info("input err: %s " % input_line)
-    uploads(appids, email, password, rc4_password)
+
-                p = subprocess.Popen(["/usr/bin/defaults", 'read', 'NSGlobalDomain', 'AppleLanguages'],stdout=oot[1])
+                p = subprocess.Popen(["/usr/bin/defaults", 'read', 'NSGlobalDomain', 'AppleLanguages'], stdout=oot[1])
-                lang_code = os.read(oot[0],10000)
+                lang_code = self.get_default_language_code_for_mac(os.read(oot[0], 10000))
-                    , config.get(["language"])
+                    , config.get(["language"], i18n_translator.lang)
-                    , config.get(["language"], i18n_translator.lang)
+                    , config.get(["language"])
-        if module not in ["launcher", "php_proxy", "x_tunnel"]:
+        if module not in ["launcher", "php_proxy"]:
-            return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\n\r\n' % self.path.replace('http://', 'https://', 1)).encode())
+            return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\nContent-Length: 0\r\n\r\n' % self.path.replace('http://', 'https://', 1)).encode())
-            return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\n\r\n' % self.path.replace('http://', 'https://', 1)).encode())
+            return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\nContent-Length: 0\r\n\r\n' % self.path.replace('http://', 'https://', 1)).encode())
-                return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\n\r\n' % self.path).encode())
+                return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\nContent-Length: 0\r\n\r\n' % self.path).encode())
-        key,value,_ = winreg.EnumValue(CONNECTIONS, i)
+        try:
-            key,value,_ = winreg.EnumValue(CONNECTIONS, i)
+            try:
-                else:
+                try:
-        my_stdout.write("Usage: uploader.py <appids> <email> [password] [rc4_password]\r\n")
+        my_stdout.write("Usage: uploader.py <appids> <email> [password] [rc4_password] [proxy]\r\n")
-        self.last_reset_time = 0
+        self.reset_appid()
-            self.working_appid_list = list(config.GAE_APPIDS)
+        with self.lock:
-
+        self.last_reset_time = time.time()
-                self.last_reset_time = time.time()
+                self.reset_appid()
-        try:
+        with self.lock:
-        try:
+        with self.lock:
-        self.GAE_APPIDS = [x.strip() for x in self.CONFIG.get('gae', 'appid').split("|")]
+        self.PUBLIC_APPIDS = [x.strip() for x in self.CONFIG.get('gae', 'public_appid').split("|")]
-        sys.exit(-1)
+                else:
-        if force or time_now - g.last_refresh_time > 3600:
+        if force or time_now - g.last_refresh_time > 3600 or \
-                g.last_api_error = "session server login fail:%d" % status
+                g.last_api_error = "session server login fail:%r" % status
-                        xlog.warn("DATA conn_id %d not in list", conn_id)
+                        xlog.debug("DATA conn_id %d not in list", conn_id)
-
+        current_version = update_from_github.current_version()
-        data = (index_content.decode('utf-8') % (menu_content, right_content.decode('utf-8') )).encode('utf-8')
+        data = (index_content.decode('utf-8') % (current_version, current_version, menu_content, right_content.decode('utf-8') )).encode('utf-8')
-    http_request("http://getbootstrap.com/dist/js/bootstrap.min.js")
+    # http_request("http://getbootstrap.com/dist/js/bootstrap.min.js")
-        if proxy_setting == "pac":
+        pass
-        elif proxy_setting == "gae":
+        elif proxySetting == "gae":
-        elif proxy_setting == "disable":
+        elif proxySetting == "disable":
-        self.registerObserver()
+            xlog.warn("proxy_setting:%r", proxySetting)
-        checkAutoProxyUrlEthernetCommand =   "networksetup -getautoproxyurl Ethernet"
+        checkAutoProxyUrlEthernetCommand    =   "networksetup -getautoproxyurl Ethernet"
-        checkAutoProxyUrlWiFiCommand = "networksetup -getautoproxyurl Wi-Fi"
+        checkAutoProxyUrlWiFiCommand        = "networksetup -getautoproxyurl Wi-Fi"
-                executeResult.find('Enabled: Yes') != -1 ):
+
-        if ( executeResult.find('http://127.0.0.1:8087') != -1 ):
+        if ( executeResult.find('Enabled: Yes\nServer: 127.0.0.1\nPort: 8087') != -1 ):
-        globalGaeProxyMenuItem = self.menu.itemWithTitle_('Enable Global GAEProxy')
+        autoGaeProxyMenuItem    = self.menu.itemWithTitle_('Enable Auto GAEProxy')
-        os.system(cmd)
+        disableProxyCommand                 = self.getDisableProxyCommand()
-        os.system(cmd)
+        disableProxyCommand                 = self.getDisableProxyCommand()
-        os.system(cmd)
+        disableProxyCommand                 = self.getDisableProxyCommand()
-        if proxy_setting == "pac":
+        pass
-        elif proxy_setting == "gae":
+        elif proxySetting == "gae":
-        elif proxy_setting == "disable":
+        elif proxySetting == "disable":
-        self.registerObserver()
+            xlog.warn("proxy_setting:%r", proxySetting)
-        checkAutoProxyUrlEthernetCommand =   "networksetup -getautoproxyurl Ethernet"
+        checkAutoProxyUrlEthernetCommand    =   "networksetup -getautoproxyurl Ethernet"
-        checkAutoProxyUrlWiFiCommand = "networksetup -getautoproxyurl Wi-Fi"
+        checkAutoProxyUrlWiFiCommand        = "networksetup -getautoproxyurl Wi-Fi"
-                executeResult.find('Enabled: Yes') != -1 ):
+
-        if ( executeResult.find('http://127.0.0.1:8087') != -1 ):
+        if ( executeResult.find('Enabled: Yes\nServer: 127.0.0.1\nPort: 8087') != -1 ):
-        globalGaeProxyMenuItem = self.menu.itemWithTitle_('Enable Global GAEProxy')
+        autoGaeProxyMenuItem    = self.menu.itemWithTitle_('Enable Auto GAEProxy')
-        os.system(cmd)
+        disableProxyCommand                 = self.getDisableProxyCommand()
-        os.system(cmd)
+        disableProxyCommand                 = self.getDisableProxyCommand()
-        os.system(cmd)
+        disableProxyCommand                 = self.getDisableProxyCommand()
-import webbrowser
+import config
-from instances import xlog
+import subprocess
-from PyObjCTools import AppHelper
+from instances import xlog
-        pass
+        proxy_setting = config.get(["modules", "launcher", "proxy"], "pac")
-        icon_path = os.path.join(current_path, "web_ui", "favicon_MAC.ico")
+        icon_path = os.path.join(current_path, "web_ui", "favicon-mac.ico")
-
+        # Get current selected mode
-        self.menu = NSMenu.alloc().init()
+        self.menu = NSMenu.alloc().initWithTitle_('XX-Net')
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Auto Goagent Proxy', 'enableAutoProxy:', '')
+        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Auto GAEProxy', 'enableAutoProxy:', '')
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global Goagent Proxy', 'enableGlobalProxy:', '')
+        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global GAEProxy', 'enableGlobalProxy:', '')
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Disable Goagent Proxy', 'disableProxy:', '')
+        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Disable GAEProxy', 'disableProxy:', '')
-        # Rest Menu Item
+        # Reset Menu Item
-    # limited by Mac cocoa
+        self.updateStatusBarMenu()
-
+        self.updateStatusBarMenu()
-        if len(g.last_api_error):
+        if len(g.last_api_error) and g.last_api_error != 'balance not enough':
-        return self.response_json({"res": "ok"})
+        return self.response_json({"res": "success"})
-                "login_account": ""
+                "res": "logout"
-            force = False
+            force = 1
-                g.session.start()
+            if res:
-        }
+        if len(g.last_api_error):
-logging = xlog.Logger()
+logging = xlog.getLogger("simple_http_server")
-        self.sockets = None
+        self.sockets = []
-            logging.info("server %s:%d started.", addr[0], addr[1])
+            self.add_listen(addr)
-            r, w, e = select.select(self.sockets, [], [], 1)
+            r, w, e = select.select(self.sockets, [], [], 3)
-
+        self.sockets = []
-        #logging.debug("GET %s from %s:%d", self.path, self.client_address[0], self.client_address[1])
+        logging.debug("GET %s from %s:%d", self.path, self.client_address[0], self.client_address[1])
-    def __init__(self, session, conn_id, sock, host, port, windows_size, windows_ack, is_client):
+    def __init__(self, session, conn_id, sock, host, port, windows_size, windows_ack, is_client, xlog):
-        # xlog.debug("Conn session:%s conn:%d stop:%s", self.session.session_id, self.conn_id, reason)
+        self.xlog.debug("Conn session:%s conn:%d stop:%s", self.session.session_id, self.conn_id, reason)
-        # xlog.info("session_id:%s create_conn %d %s:%d", self.session.session_id, self.conn_id, host, port)
+        self.xlog.info("session_id:%s create_conn %d %s:%d", self.session.session_id, self.conn_id, host, port)
-                    # xlog.debug("Conn session:%s conn:%d get data len:%d ", self.session.session_id, self.conn_id, len(payload))
+                    #self.xlog.debug("Conn session:%s conn:%d get data len:%d ", self.session.session_id, self.conn_id, len(payload))
-                # xlog.debug("Conn session:%s conn:%d ACK:%d", self.session.session_id, self.conn_id, position)
+                self.xlog.debug("Conn session:%s conn:%d ACK:%d", self.session.session_id, self.conn_id, position)
-                # xlog.info("Conn session:%s conn:%d Peer Close:%s", self.session.session_id, self.conn_id, data.get())
+                self.xlog.info("Conn session:%s conn:%d Peer Close:%s", self.session.session_id, self.conn_id, data.get())
-                    xlog.debug("Conn session:%s conn:%d %s:%d Create fail", self.session.session_id, self.conn_id,
+                    self.xlog.debug("Conn session:%s conn:%d %s:%d Create fail", self.session.session_id, self.conn_id,
-                    xlog.info("Conn session:%s conn:%d %s:%d", self.session.session_id, self.conn_id, self.host,
+                    self.xlog.info("Conn session:%s conn:%d %s:%d", self.session.session_id, self.conn_id, self.host,
-                # xlog.info("%s conn_id:%d send closed", self.session.session_id, self.conn_id)
+                self.xlog.info("%s conn_id:%d send closed", self.session.session_id, self.conn_id)
-import SocketServer
+import threading
-    g.socks5_server.server_activate()
+    g.socks5_server = simple_http_server.HTTPServer((g.config.socks_host, g.config.socks_port), Socks5Server)
-        terminate()
+        #terminate()
-class Socks5Server(SocketServer.StreamRequestHandler):
+class Socks5Server():
-        xlog.info("Socks4:%r to %s:%d", self.client_address, addr, port)
+        xlog.info("Socks4:%r to %s:%d, conn_id:%d", self.client_address, addr, port, conn_id)
-        # xlog.info("%r to %s:%d", self.client_address, addr, port)
+            xlog.warn("create conn fail")
-                                                      g.config.windows_ack, is_client=True)
+                                                      g.config.windows_ack, True, xlog)
-            return self.req_status_handler()
+            return self.req_login_handler()
-    def req_status_handler(self):
+    def req_info_handler(self):
-                   "login_account": ""})
+            return self.response_json({
-            force = int(self.postvars['force'][0])
+        req = urlparse.urlparse(self.path).query
-                                                        is_register=False, update_server=update_server)
+            res, reason = proxy_session.request_balance(
-
+                return self.response_json({
-                   }
+        res_arr = {
-        password = str(self.postvars['password'][0])
+        username    = str(self.postvars['username'][0])
-        pa = check_email(account)
+        pa = check_email(username)
-            return self.response_json({"res": "fail", "reason": reason})
+            return self.response_json({
-            return self.response_json({"res": "fail", "reason": reason})
+            return self.response_json({
-        res, reason = proxy_session.request_balance(account, password_hash, is_register, update_server=True)
+        res, reason = proxy_session.request_balance(username, password_hash, is_register, update_server=True)
-            g.config.login_account = account
+            g.config.login_account  = username
-                       "balance": float(g.balance)}
+            res_arr = {
-            res_arr = {"res": "fail", "reason": reason}
+            res_arr = {
-            return self.response_json({"res": "fail", 'reason': 'product %s not support' % product})
+            return self.response_json({
-                                            "plan": plan})
+            return self.response_json({
-                    "amount": amount}
+        req_info = {
-            return self.response_json({"res": "fail", "reason": info})
+            return self.response_json({
-                    "limit": int(self.postvars['limit'][0]), }
+        req = urlparse.urlparse(self.path).query
-        self.response_json({"res": "success", "history": info["history"]})
+            return self.response_json({
-    main()
+        self.file_max_size = 1024 * 1024
-            self.roll_log()
+        if os.path.isfile(file_name):
-            self.info("roll_log %s -> %s", old_name, new_name)
+            #self.info("roll_log %s -> %s", old_name, new_name)
-        shutil.move(self.log_filename, self.log_filename + ".0")
+        shutil.move(self.log_filename, self.log_filename + ".1")
-                logging.warn("unhandler cmd:%s", self.command)
+                logging.warn("unhandler cmd:%s path:%s from:%s", self.command, self.path, self.address_string())
-            logging.exception("handler:%r", e)
+            logging.exception("handler:%r cmd:%s path:%s from:%s", e,  self.command, self.path, self.address_string())
-        pass
+        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        pass
+        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        pass
+        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        pass
+        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        pass
+        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        pass
+        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-        pass
+        logging.warn("unhandler cmd:%s from:%s", self.command, self.address_string())
-    modules = ["gae_proxy", "launcher", "php_proxy"]
+    modules = ["gae_proxy", "launcher", "php_proxy", "x_tunnel"]
-        if module not in ["launcher", "php_proxy"]:
+        if module not in ["launcher", "php_proxy", "x_tunnel"]:
-            data = '{ "check_update": "%s", "language": "%s", "popup_webui": %d, "allow_remote_connect": %d, "show_systray": %d, "auto_start": %d, "show_detail": %d, "php_enable": %d, "gae_proxy_enable": %d }' %\
+            data = '{ "check_update": "%s", "language": "%s", "popup_webui": %d, "allow_remote_connect": %d, \
-                    , config.get(["modules", "gae_proxy", "auto_start"], 0))
+                    , config.get(["modules", "gae_proxy", "auto_start"], 0)
-import xlog
+import json
-    def send_response_data(self, mimetype="", content="", heads="", status=200):
+    def send_response(self, mimetype="", content="", headers="", status=200):
-            data.append(heads)
+        if len(headers):
-        if len(content) + len(heads) < 1024:
+        if len(content) < 1024:
-    file_config = None
+    file_config = {}
-google_server_types = ["GAE", "Google Frontend", "GSE"]
+#"GAE", "Google Frontend", "GSE", "GFE/2.0",
-                        google_ip.report_connect_fail(response.ssl_sock.ip, force_remove=True)
+                    if "G" not in server_type and "g" not in server_type and server_type not in google_server_types:
-    def request(self, method="GET", path="", header=None, data="", timeout=60):
+    def request(self, method="GET", path="", header={}, data="", timeout=60):
-            header = {"Content-Length": str(len(data)), "Host": self.address[0]}
+            header["Content-Length"] = str(len(data))
-            response = self.fetch(method, self.address[0], req_path, header, data, timeout=timeout)
+            response = self.fetch(method, host, req_path, header, data, timeout=timeout)
-                        #logging.warn("read timeout t:%d len:%d left:%d %s", (time.time()-time_request)*1000, length, (end-start), url)
+            response_headers = dict((k.title(), v) for k, v in response.getheaders())
-                        continue
+                        data_buffer.append(data)
-                data_buffer.append(data)
+                data_buffer = []
-        return "Except:%s" % e
+        raise
-                    if "gws" not in server_type and "Google Frontend" not in server_type and "GFE" not in server_type:
+
-        for i in range(100):
+        for i in range(3):
-from instances import xlog
+from instances import xlog
-        self.systray = SysTrayIcon(icon_path, "XX-Net", self.make_menu(), self.on_quit, left_click=self.on_show, right_click=self.on_right_click)
+        self.systray = SysTrayIcon(icon_path, "XX-Net", 
-            0, winreg.KEY_ALL_ACCESS)
+        self.INTERNET_SETTINGS = winreg.OpenKey(winreg.HKEY_CURRENT_USER, reg_path, 0, winreg.KEY_ALL_ACCESS)
-            AutoConfigURL, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'AutoConfigURL')
+            AutoConfigURL, reg_type = winreg.QueryValueEx(self.INTERNET_SETTINGS, 'AutoConfigURL')
-        except Exception as e:
+        except:
-            ProxyEnable, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'ProxyEnable')
+            ProxyEnable, reg_type = winreg.QueryValueEx(self.INTERNET_SETTINGS, 'ProxyEnable')
-                ProxyServer, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'ProxyServer')
+                ProxyServer, reg_type = winreg.QueryValueEx(self.INTERNET_SETTINGS, 'ProxyServer')
-        except Exception as e:
+        except:
-        win32_proxy_manager.set_proxy_server("127.0.0.1", 8087)
+        win32_proxy_manager.set_proxy("127.0.0.1:8087")
-        win32_proxy_manager.set_proxy_auto("http://127.0.0.1:8086/proxy.pac")
+        win32_proxy_manager.set_proxy("http://127.0.0.1:8086/proxy.pac")
-    nSize = c_ulong(sizeof(INTERNET_PER_CONN_OPTION_LIST))
+    _,values_num,_ = winreg.QueryInfoKey(CONNECTIONS)
-    Option[0].Value.dwValue = PROXY_TYPE_DIRECT
+        List = INTERNET_PER_CONN_OPTION_LIST()
-    List.pOptions = Option
+        Option[0].dwOption = INTERNET_PER_CONN_FLAGS
-    InternetSetOption(None, INTERNET_OPTION_PER_CONNECTION_OPTION, byref(List), nSize)
+        List.dwSize = sizeof(INTERNET_PER_CONN_OPTION_LIST)
-    setting = create_unicode_buffer(pac_url)
+def set_proxy_auto(proxy_addr, conn_name='DefaultConnectionSettings'):
-    List.pszConnection = None
+    List.pszConnection = create_unicode_buffer(conn_name)
-    setting = create_unicode_buffer(ip+":"+str(port))
+def set_proxy_server(proxy_addr, conn_name='DefaultConnectionSettings'):
-    List.pszConnection = None
+    List.pszConnection = create_unicode_buffer(conn_name)
-    set_proxy_server("127.0.0.1", 8087)
+    set_proxy("127.0.0.1:8087")
-        title = u"XX-Net ååç½"
+        # import ctypes
-        p = subprocess.call(["Wscript.exe", "//E:JScript", "create_shortcut.js"], shell=False)
+        subprocess.call(["Wscript.exe", "//E:JScript", "create_shortcut.js"], shell=False)
-        wfile.write("HTTP/1.1 %d %s\r\n" % (response.status, response.reason))
+            wfile.write("HTTP/1.1 %d %s\r\n" % (response.status, response.reason))
-        touch_active()
+        if self.path != "https://www.twitter.com/xxnet":
-            response = self.fetch(method, self.address[0], req_path, header, data, timeout)
+            response = self.fetch(method, self.address[0], req_path, header, data, timeout=timeout)
-                    return
+                    break
-                    return
+                    break
-        self.iplist_need_save = 0
+        self.iplist_need_save = False
-            if self.iplist_need_save == 0:
+            if not self.iplist_need_save:
-            self.iplist_need_save = 0
+            self.iplist_need_save = False
-            self.iplist_need_save = 1
+            self.iplist_need_save = True
-                self.iplist_need_save = 1
+                self.iplist_need_save = True
-            self.iplist_need_save = 1
+            self.iplist_need_save = True
-
+from xlog import getLogger
-    main()
+    main()
-        xlog.exception("test_gae_ip %s e:%r",ip, e)
+        if __name__ == "__main__":
-                user_config.user_special.password = self.postvars['password'][0]
+
-from proxy import xlog
+from xlog import getLogger
-from proxy import xlog
+from xlog import getLogger
-import socks
+import binascii
-
+import socks
-from proxy import xlog
+from xlog import getLogger
-openssl_context = SSLConnection.context_builder(ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
+openssl_context = SSLConnection.context_builder(ca_certs=g_cacertfile)
-
+    # resize socket recv buffer 8K->32K to improve browser releated application performance
-    continue_fail_count = 0
+    # report network ok
-    ssl_sock.sock = sock
+    ssl_sock._sock = sock
-    #xlog.info("%s CN:%s", ip, ssl_cert.cn)
+    if __name__ == "__main__":
-def check_appid(ssl_sock, appid, ip):
+def check_goagent(ssl_sock, appid):
-        #xlog.warn("app check %s status:%d", appid, response.status)
+        if __name__ == "__main__":
-        #xlog.warn("app check %s ip:%s status:%d", appid, ip, response.status)
+        if __name__ == "__main__":
-        #xlog.warn("app check %s content:%s", appid, content)
+        if __name__ == "__main__":
-        if not check_appid(ssl_sock, appid, ip):
+            appid = "xxnet-1"
-        #xlog.exception("test_gae_ip %s e:%r",ip, e)
+    except socket.timeout:
-            return False
+        xlog.exception("test_gae_ip %s e:%r",ip, e)
-    return False
+if __name__ == "__main__":
-import logging as xlog
+
-from proxy import xlog
+
-import Queue
+
-from proxy import xlog
+
-            ssl_sock.sock.settimeout(10)
+            ssl_sock._sock.settimeout(10)
-from proxy import xlog
+
-from proxy import xlog
+
-from proxy import xlog
+
-        #if check_ip.network_stat == "OK" and not config.USE_IPV6:
+        #if check_local_network.network_stat == "OK" and not config.USE_IPV6:
-        check_ip.network_stat = "OK"
+        check_local_network.network_stat = "OK"
-            if check_ip.network_stat == "Fail":
+            if check_local_network.network_stat == "Fail":
-                check_ip.triger_check_network()
+            check_local_network.continue_fail_count += 1
-            check_ip.triger_check_network()
+            check_local_network.triger_check_network()
-            if check_ip.network_stat == "Fail":
+            if check_local_network.network_stat == "Fail":
-from proxy import xlog
+
-            add_last_byte = ip % 255
+            add_last_byte = ip % 256
-from proxy import xlog
+
-        self.sock = sock
+        self._sock = sock
-            self.sock = None
+        if self._sock:
-        fd = self.sock.fileno()
+        timeout = self._sock.gettimeout() or 0.1
-        sock, addr = self.sock.accept()
+        sock, addr = self._sock.accept()
-                self.sock = None
+            if self._sock:
-from proxy import xlog
+
-from xlog import Logger
+
-xlog = Logger(buffer_size=500, file_name=log_file)
+    xlog.set_file(log_file)
-from proxy import xlog
+from xlog import getLogger
-from proxy import xlog
+
-from proxy import xlog
+
-from proxy import xlog
+
-                   "network_state": check_ip.network_stat,
+                   "network_state": check_local_network.network_stat,
-                        if not check_ip.check_ipv6():
+                        if not check_local_network.check_ipv6():
-            self.log_to_file(file_name)
+            self.set_file(file_name)
-    def log_to_file(self, file_name):
+    def set_file(self, file_name):
-        if max_scan_ip_thread_num:
+        if max_scan_ip_thread_num!=None:
-        self._sock = sock
+        self.sock = sock
-            self._sock = None
+        if self.sock:
-        fd = self._sock.fileno()
+        timeout = self.sock.gettimeout() or 0.1
-        sock, addr = self._sock.accept()
+        sock, addr = self.sock.accept()
-                self._sock = None
+            if self.sock:
-            return False
+            return True
-        #self.search_more_google_ip()
+        #if check_ip.network_stat == "OK" and not config.USE_IPV6:
-                    xlog.warn("ssl_closed %s", ip)
+                    xlog.debug("ssl_closed %s", ip)
-        th.start()
+        if hasattr(self, "scan_all_ip_thread") and self.scan_all_ip_thread:
-                    , config.get(["modules", "launcher", "show_detail"], 0)
+                    , config.get(["modules", "gae_proxy", "show_detail"], 0)
-                    config.set(["modules", "launcher", "show_detail"], show_detail)
+                    config.set(["modules", "gae_proxy", "show_detail"], show_detail)
-                    if appids:
+                    if appids and google_ip.good_ip_num:
-
+progress = {} # link => {"size", 'downloaded', status:downloading|canceled|finished:failed}
-        download_progress[url]["status"] = "downloading"
+def download_file(url, filename):
-        if download_progress[url]["status"] == "downloading":
+        if progress[url]["status"] == "downloading":
-            xlog.info("download %s to %s, retry:%d", url, file, i)
+            xlog.info("download %s to %s, retry:%d", url, filename, i)
-            download_progress[url]["size"] = int(req.headers.get('content-length') or 0)
+            req = opener.open(url, timeout=30)
-            CHUNK = 16 * 1024
+            chunk_len = 65536
-            with open(file, 'wb') as fp:
+            with open(filename, 'wb') as fp:
-                    chunk = req.read(CHUNK)
+                    chunk = req.read(chunk_len)
-                    download_progress[url]["downloaded"] = downloaded
+                    progress[url]["downloaded"] = downloaded
-                xlog.warn("download size:%d, need size:%d, download fail.", downloaded, download_progress[url]["size"])
+            if downloaded != progress[url]["size"]:
-                download_progress[url]["status"] = "finished"
+                progress[url]["status"] = "finished"
-            xlog.warn("download %s to %s URL fail:%r", url, file, e)
+        except (urllib2.URLError, ssl.SSLError) as e:
-            xlog.exception("download %s to %s fail:%r", url, file, e)
+            xlog.exception("download %s to %s fail:%r", url, filename, e)
-    download_progress[url]["status"] = "failed"
+    progress[url]["status"] = "failed"
-def get_xxnet_url_version(readme_file):
+
-    readme_targe = os.path.join(download_path, "README.md")
+    readme_target = os.path.join(download_path, "README.md")
-    if not download_file(readme_url, readme_targe):
+    if not download_file(readme_url, readme_target):
-    versions = get_xxnet_url_version(readme_targe)
+    versions = parse_readme_versions(readme_target)
-        raise "download xxnet zip fail:" % download_path
+        progress["update_status"] = "Download Fail."
-        dz.close()
+    xlog.info("update start unzip")
-    if config.get(["update", "uuid"], '') != 'test':
+    progress["update_status"] = "Over writing"
-            #print "root:", root
+            if config.get(["update", "uuid"], '') == 'test' and "launcher" in relate_path:
-                    shutil.copy(src_file, dst_file)
+                    shutil.copy(src_file, dst_file)
-    start_sript = os.path.abspath( os.path.join(current_path, "start.py"))
+    start_script = os.path.join(current_path, "start.py")
-    subprocess.Popen([sys.executable, start_sript])
+    subprocess.Popen([sys.executable, start_script])
-        update_config(version)
+
-        xlog.exception("update version %s fail:%r", version, e)
+        xlog.warn("update version %s fail:%r", version, e)
-        pass
+def start_update_version(version):
-remove_old_file()
+clean_old_file()
-            self.req_download_handler()
+        elif url_path == '/update':
-    def req_download_handler(self):
+    def req_update_handler(self):
-
+            data = json.dumps(update_from_github.progress)
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global Goagent Proxy', 'enableProxy:', '')
+        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Auto Goagent Proxy', 'enableAutoProxy:', '')
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Disable Global Goagent Proxy', 'disableProxy:', '')
+        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Enable Global Goagent Proxy', 'enableGlobalProxy:', '')
-    def enableProxy_(self, _):
+    def enableAutoProxy_(self, _):
-        cmd5 = "networksetup -setsecurewebproxy Ethernet 127.0.0.1 8087"
+        cmd2 = "networksetup -setwebproxy \\\"Thunderbolt Ethernet\\\" 127.0.0.1 8087"
-        exec_command = "%s;%s;%s;%s;%s;%s;%s;%s" % (cmd1, cmd2, cmd3, cmd4, cmd5, cmd6, cmd7, cmd8)
+        exec_command = "%s;%s;%s;%s;%s;%s" % (cmd1, cmd2, cmd3, cmd4, cmd5, cmd6)
-        exec_command = "%s;%s;%s;%s" % (cmd1, cmd2, cmd3, cmd4)
+        cmd2 = "networksetup -setwebproxystate \\\"Thunderbolt Ethernet\\\" off"
-            data = '{ "check_update": "%s", "language": "%s", "popup_webui": %d, "allow_remote_connect": %d, "show_systray": %d, "auto_start": %d, "php_enable": %d, "gae_proxy_enable": %d }' %\
+            data = '{ "check_update": "%s", "language": "%s", "popup_webui": %d, "allow_remote_connect": %d, "show_systray": %d, "auto_start": %d, "show_detail": %d, "php_enable": %d, "gae_proxy_enable": %d }' %\
-    request_data = 'GET /_gh HTTP/1.1\r\nHost: %s.appspot.com\r\n\r\n' % appid
+    request_data = 'GET /_gh/ HTTP/1.1\r\nHost: %s.appspot.com\r\n\r\n' % appid
-        while connect_control.keep_running:
+        while connect_control.keep_running and self.keep_scan_all_exist_ip:
-    request_data = 'GET / HTTP/1.1\r\nHost: %s.appspot.com\r\n\r\n' % appid
+    request_data = 'GET /_gh/ HTTP/1.1\r\nHost: %s.appspot.com\r\n\r\n' % appid
-        if check_ip.network_stat == "OK":
+        if check_ip.network_stat == "OK" and not config.USE_IPV6:
-    request_data = 'GET / HTTP/1.1\r\nHost: %s.appspot.com\r\n\r\n' % appid
+    request_data = 'GET /_gh HTTP/1.1\r\nHost: %s.appspot.com\r\n\r\n' % appid
-                return "pac"
+                if AutoConfigURL == "http://127.0.0.1:8086/proxy.pac":
-                return "gae"
+                ProxyServer, reg_type = winreg.QueryValueEx(INTERNET_SETTINGS, 'ProxyServer')
-        win32_proxy_manager.disable_proxy()
+        proxy_setting = config.get(["modules", "launcher", "proxy"], "disable")
-            return self.wfile.write('HTTP/1.1 200\r\nAccess-Control-Allow-Origin: *\r\nContent-Length: 2\r\n\r\nOK')
+            return self.wfile.write(self.self_check_response_data)
-            return self.wfile.write('HTTP/1.1 200\r\nAccess-Control-Allow-Origin: *\r\nContent-Length: %d\r\n\r\n%s' %(len(data), data) )
+            xlog.debug("CONNECT %s %s", self.command, self.path)
-                xlog.warning('APPID %r out of Quota, remove it.', response.ssl_sock.appid)
+                xlog.warning('APPID %r out of Quota, remove it. %s', response.ssl_sock.appid, response.ssl_sock.ip)
-                google_ip.report_connect_closed(response.ssl_sock.ip, "get_timeout")
+                google_ip.report_connect_closed(response.ssl_sock.ip, "out of quota")
-        gae_handler.send_response(self.wfile, 200, body=html.encode('utf-8'))
+        host = self.headers.get('Host', '')
-            xlog.warn("Your browser forward localhost to proxy.")
+            #xlog.warn("Your browser forward localhost to proxy.")
-            self.path_base = "/"
+            self.path_base = ""
-                self.path_base = "https://%s:%d/" % self.address
+                self.path_base = "https://%s:%d" % self.address
-                self.path_base = "http://%s:%d/" % self.address
+                self.path_base = "http://%s:%d" % self.address
-            response = self.fetch(method, self.address[0], self.path_base + path, header, data, timeout)
+            if path.startswith("/"):
-                xlog.warn("all appid out of quota, need 1 min to reset")
+            if time.time() - self.last_reset_time < 600:
-                self.working_appid_list.remove(appid)
+            self.working_appid_list.remove(appid)
-            self.on_disable_proxy()
+            # Don't disable proxy setting, just do nothing.
-            xlog.info("report_connect_fail:%s", ip)
+            xlog.debug("report_connect_fail:%s", ip)
-            return self.wfile.write('HTTP/1.1 200\r\nAccess-Control-Allow-Origin: *\r\nContent-Length: %d\r\n\r\n%s' %(len(data), data) )
+            return self.wfile.write('HTTP/1.1 200\r\nAccess-Control-Allow-Origin: *\r\nContent-Length: 2\r\n\r\nOK')
-elif sys.platform == 'linux' or sys.platform == 'linux2':
+elif sys.platform.startswith('linux'):
-            "autostart")
+    home_config_path = os.path.expanduser(_xdg_config_home)
-            xlog.warn("autorun linux config path not found:%s", os.path.expanduser(_xdg_config_home))
+        if not os.path.isdir(home_config_path):
-            os.mkdir(_xdg_user_autostart)
+            try:
-            "Terminal=false\n" % (name, application)
+            "Terminal=false\n"\
-class simpleI18N():
+class SimpleI18N():
-from simple_i18n import simpleI18N
+from simple_i18n import SimpleI18N
-i18n_translator = simpleI18N(config.get(['language'], None))
+i18n_translator = SimpleI18N(config.get(['language'], None))
-            stream = i18n_translator.render(locale_dir, os.path.join(root_path, module, "web_ui", "menu.yaml"))
+            stream = i18n_translator.render(locale_dir, menu_path)
-            xlog.info("ip:%s real fail", ip)
+            xlog.debug("ip:%s real fail", ip)
-                user_config.user_special.use_ipv6 = int(self.postvars['use_ipv6'][0])
+                use_ipv6 = int(self.postvars['use_ipv6'][0])
-        xlog.warn("APPID_manager, report_not_exist %s", appid)
+    def report_not_exist(self, appid, ip):
-            pass
+
-                'basicConstraints', False, 'CA:TRUE', ca, ca)
+                'basicConstraints', False, 'CA:TRUE', subject=ca, issuer=ca)
-network_ok = False
+network_stat = "unknown"
-check_network_interval = 60
+continue_fail_count = 0
-    global checking_lock, checking_num, network_ok, last_check_time, check_network_interval
+    global checking_lock, checking_num, network_stat, last_check_time
-        network_ok = False
+        network_stat = "Fail"
-    global checking_lock, checking_num, network_ok, last_check_time, check_network_interval
+def simple_check_worker():
-            return network_ok
+    if config.PROXY_ENABLE:
-            return network_ok
+            return
-    th = threading.Thread(target=check_worker)
+    last_check_time = time_now
-    ssl_sock = SSLConnection(openssl_context, sock)
+    ssl_sock = SSLConnection(openssl_context, sock, ip)
-def check_appid(ssl_sock, appid):
+def check_appid(ssl_sock, appid, ip):
-        return True
+        server_type = response.getheader('Server', "")
-        xlog.warn("app check %s status:%d", appid, response.status)
+        #xlog.warn("app check %s ip:%s status:%d", appid, ip, response.status)
-# export api for google_ip
+# export api for google_ip, appid_manager
-        check_appid(ssl_sock, appid)
+        if not check_appid(ssl_sock, appid, ip):
-    if config.PROXY_ENABLE:
+
-                raise Exception("app check fail")
+                raise Exception("app check fail %r" % status)
-            xlog.debug("head request fail:%r", e)
+            xlog.warn("%s head %s request fail:%r", ssl_sock.ip, ssl_sock.appid, e)
-                sock = socks.socksocket(socket.AF_INET if ':' not in ip_port[0] else socket.AF_INET6)
+                sock = socks.socksocket(socket.AF_INET if ':' not in ip else socket.AF_INET6)
-                sock = socket.socket(socket.AF_INET if ':' not in ip_port[0] else socket.AF_INET6)
+                sock = socket.socket(socket.AF_INET if ':' not in ip else socket.AF_INET6)
-            ssl_sock = SSLConnection(self.openssl_context, sock)
+            ssl_sock = SSLConnection(self.openssl_context, sock, ip, google_ip.ssl_closed)
-            xlog.warn("forward port %d not supported.", port)
+    def get_new_ssl(self):
-                    if "gws" not in server_type and "Google Frontend" not in server_type:
+                    if "gws" not in server_type and "Google Frontend" not in server_type and "GFE" not in server_type:
-                appid_manager.report_not_exist(response.ssl_sock.appid)
+                #xlog.warning('APPID %r not exists, remove it.', response.ssl_sock.appid)
-                        appid_manager.report_not_exist(response.ssl_sock.appid)
+                        appid_manager.report_not_exist(response.ssl_sock.appid, response.ssl_sock.ip)
-import traceback
+import os
-from google_ip_range import ip_range
+import google_ip_range
-    scan_exist_ip_queue = Queue.Queue()
+class IpManager():
-        self.searching_thread_count = 0
+        self.scan_thread_count = 0
-        # ip_str => {
+        # ip => {
-        self.to_remove_ip_list = Queue.Queue()
+        self.to_check_ip_queue = Queue.Queue()
-        self.search_more_google_ip()
+        if check_ip.network_stat == "OK":
-                ip_str = str_l[0]
+                ip = str_l[0]
-                self.add_ip(ip_str, handshake_time, domain, server, fail_times)
+                #logging.info("load ip: %s time:%d domain:%s server:%s", ip, handshake_time, domain, server)
-                for ip_str, property in ip_dict:
+                for ip, property in ip_dict:
-                        (ip_str, property['domain'], property['server'], property['handshake_time'], property['fail_times']) )
+                        (ip, property['domain'], property['server'], property['handshake_time'], property['fail_times']) )
-                if 'gws' not in self.ip_dict[ip_str]['server']:
+            for ip in self.ip_dict:
-                if self.ip_dict[ip_str]['fail_times'] == 0:
+                ip_rate[ip] = self.ip_dict[ip]['handshake_time'] + (self.ip_dict[ip]['fail_times'] * 1000)
-            self.gws_ip_list = [ip_str for ip_str,rate in ip_time]
+            self.gws_ip_list = [ip for ip,rate in ip_time]
-    def adjust_scan_thread_num(self):
+    def adjust_scan_thread_num(self, max_scan_ip_thread_num=None):
-    def append_ip_history(self, ip_str, info):
+    def append_ip_history(self, ip, info):
-            self.ip_dict[ip_str]['history'].append([time.time(), info])
+            self.ip_dict[ip]['history'].append([time.time(), info])
-                get_time = self.ip_dict[ip_str]["get_time"]
+                ip = self.gws_ip_list[self.gws_ip_pointer]
-                if time_now - self.ip_dict[ip_str]['success_time'] > 300: # 5 min
+                if time_now - self.ip_dict[ip]['success_time'] > 300: # 5 min
-                fail_time = self.ip_dict[ip_str]["fail_time"]
+                fail_time = self.ip_dict[ip]["fail_time"]
-                if self.ip_dict[ip_str]['links'] >= config.max_links_per_ip:
+                if self.ip_dict[ip]['links'] >= config.max_links_per_ip:
-                self.ip_dict[ip_str]['links'] += 1
+                handshake_time = self.ip_dict[ip]["handshake_time"]
-                return ip_str
+                return ip
-            traceback.print_exc()
+            xlog.exception("get_gws_ip fail:%r", e)
-        if not isinstance(ip_str, basestring):
+    def add_ip(self, ip, handshake_time, domain=None, server='', fail_times=0):
-            xlog.warn("add %s but ipv6", ip_str)
+        if config.USE_IPV6 and ":" not in ip:
-                    self.ip_dict[ip_str]['fail_time'] = 0
+            if ip in self.ip_dict:
-                self.append_ip_history(ip_str, handshake_time)
+                self.append_ip_history(ip, handshake_time)
-            self.ip_dict[ip_str] = {'handshake_time':handshake_time, "fail_times":fail_times,
+            self.ip_dict[ip] = {'handshake_time':handshake_time, "fail_times":fail_times,
-                self.gws_ip_list.append(ip_str)
+                self.gws_ip_list.append(ip)
-        if not isinstance(ip_str, basestring):
+    def update_ip(self, ip, handshake_time):
-            xlog.warn("%s handshake:%d impossible", ip_str, 1000 * handshake_time)
+            xlog.warn("%s handshake:%d impossible", ip, 1000 * handshake_time)
-        check_ip.network_ok = True
+        check_ip.network_stat = "OK"
-            if ip_str in self.ip_dict:
+            if ip in self.ip_dict:
-                org_time = self.ip_dict[ip_str]['handshake_time']
+                org_time = self.ip_dict[ip]['handshake_time']
-                    self.ip_dict[ip_str]['handshake_time'] = org_time + 500
+                    self.ip_dict[ip]['handshake_time'] = org_time + 500
-                    self.ip_dict[ip_str]['handshake_time'] = handshake_time
+                    self.ip_dict[ip]['handshake_time'] = handshake_time
-                if self.ip_dict[ip_str]['fail_times'] > 0:
+                self.ip_dict[ip]['success_time'] = time_now
-                self.ip_dict[ip_str]["fail_time"] = 0
+                self.ip_dict[ip]['fail_times'] = 0
-            #logging.debug("update ip:%s not exist", ip_str)
+            #logging.debug("update ip:%s not exist", ip)
-    def report_connect_fail(self, ip_str, force_remove=False):
+    def report_connect_fail(self, ip, force_remove=False):
-            if not ip_str in self.ip_dict:
+            if not ip in self.ip_dict:
-            self.ip_dict[ip_str]['links'] -= 1
+            if force_remove:
-                xlog.debug("fail time too near")
+                if ip in self.gws_ip_list:
-                    self.gws_ip_list.remove(ip_str)
+            self.ip_dict[ip]['links'] -= 1
-                    xlog.info("remove ip:%s left amount:%d gws_num:%d", ip_str, len(self.ip_dict), len(self.gws_ip_list))
+            check_ip.continue_fail_count += 1
-                    self.good_ip_num = len(self.ip_dict)
+            fail_time = self.ip_dict[ip]["fail_time"]
-            self.iplist_need_save = 1
+            if self.ip_dict[ip]['fail_times'] == 0:
-            xlog.exception("set_ip err:%s", e)
+            xlog.exception("report_connect_fail err:%s", e)
-        xlog.debug("%s close:%s", ip_str, reason)
+    def report_connect_closed(self, ip, reason=""):
-                self.append_ip_history(ip_str, "C[%s]"%reason)
+            if ip in self.ip_dict:
-        self.remove_ip_thread_num_lock.release()
+    def ssl_closed(self, ip, reason=""):
-        p.start()
+    def check_ip_process(self):
-            while connect_control.keep_running:
+            time_wait = test_time - time.time()
-                    ip_str = self.to_remove_ip_list.get_nowait()
+                    if self.ip_dict[ip]['fail_times']:
-                    break
+                    pass
-                    continue
+            result = check_ip.test_gae_ip(ip)
-            self.remove_ip_thread_num_lock.release()
+            xlog.info("ip:%s real fail", ip)
-                ip_str = self.gws_ip_list[ip_num - 1]
+                ip = self.gws_ip_list[ip_num - 1]
-                property = self.ip_dict[ip_str]
+                property = self.ip_dict[ip]
-                del self.ip_dict[ip_str]
+                xlog.info("remove_slowest_ip:%s handshake_time:%d, fails:%d", ip, handshake_time, fails)
-                    self.gws_ip_list.remove(ip_str)
+                if 'gws' in server and ip in self.gws_ip_list:
-        while self.searching_thread_count <= self.scan_ip_thread_num and connect_control.keep_running:
+        while self.scan_thread_count <= self.scan_ip_thread_num and connect_control.keep_running:
-                ip_str = ip_utils.ip_num_to_string(ip_int)
+                ip = self.ip_range.get_ip()
-                if ip_str in self.ip_dict:
+                if ip in self.ip_dict:
-                result = check_ip.test_gae_ip(ip_str)
+                result = check_ip.test_gae_ip(ip)
-                    #logging.info("add  %s  CN:%s  type:%s  time:%d  gws:%d ", ip_str,
+                if self.add_ip(ip, result.handshake_time, result.domain, "gws"):
-                    scan_ip_log.info("Add %s time:%d CN:%s ", ip_str, result.handshake_time, result.domain)
+                    xlog.info("scan_ip add ip:%s time:%d", ip, result.handshake_time)
-        self.ncount_lock.release()
+        self.scan_thread_lock.acquire()
-        new_thread_num = self.scan_ip_thread_num - self.searching_thread_count
+        new_thread_num = self.scan_ip_thread_num - self.scan_thread_count
-            self.ncount_lock.release()
+            self.scan_thread_lock.acquire()
-        self.update_scan_thread_num(0)
+        self.max_scan_ip_thread_num = 0
-            th = threading.Thread(target=self.scan_exist_ip_worker)
+            th = threading.Thread(target=self.scan_exist_ip_worker, )
-        self.update_scan_thread_num(max_scan_ip_thread_num)
+
-                ip_str = self.scan_exist_ip_queue.get_nowait()
+                ip = self.scan_exist_ip_queue.get_nowait()
-            result = check_ip.test_gae_ip(ip_str)
+            result = check_ip.test_gae_ip(ip)
-                    if ip_str not in self.ip_dict:
+                    if ip not in self.ip_dict:
-                    if self.ip_dict[ip_str]['fail_times'] == 0:
+                    if self.ip_dict[ip]['fail_times'] == 0:
-                    self.ip_dict[ip_str]["fail_time"] = time.time()
+                    self.ip_dict[ip]['fail_times'] += 1
-    test()
+                self.add_ip(ip, result.handshake_time, result.domain, "gws")
-class Ip_range(object):
+
-            self.range_file = user_range_file
+        
-            self.range_file = default_range_file
+            self.range_file = self.default_range_file
-        with open(user_range_file, "w") as fd:
+        with open(self.user_range_file, "w") as fd:
-                return ip_range[1]
+                return ip_utils.ip_num_to_string(ip_range[1])
-        print index
+            return ip_utils.ip_num_to_string(ip)
-    test_random()
+ip_range = IpRange()
-    def __init__(self, context, sock):
+    def __init__(self, context, sock, ip=None, on_close=None):
-xlog = Logger(buffer_size=500)
+if config.log_file:
-    pass
+import httplib
-                   "xxnet_version":self.xxnet_version(),
+                   "sys_platform": "%s, %s" % (platform.machine(), platform.platform()),
-                   "scan_ip_thread_num":google_ip.searching_thread_count,
+                   "proxy_listen": config.LISTEN_IP + ":" + str(config.LISTEN_PORT),
-                   "block_stat":connect_control.block_stat(),
+                   "block_stat": connect_control.block_stat(),
-                   "low_prior_lock":len(connect_control.low_prior_lock),
+                   "high_prior_connecting_num": connect_control.high_prior_connecting_num,
-                        fail_appid_list = google_ip.test_appids(appids)
+                        fail_appid_list = test_appid.test_appids(appids)
-                connect_manager.forwork_manager.load_config()
+                if appid_updated:
-                data = '{"res":"deploy is running", "time":"%s"}' % (time_now)
+                data = '{"res":"deploy is running", "time":"%s"}' % time_now
-                data = '{"res":"deploy is killed", "time":"%s"}' % (time_now)
+                data = '{"res":"deploy is killed", "time":"%s"}' % time_now
-                data = '{"res":"deploy is not running", "time":"%s"}' % (time_now)
+                data = '{"res":"deploy is not running", "time":"%s"}' % time_now
-            data = json.dumps({'status':status,'log':content, 'time':time_now})
+            data = json.dumps({'status': status, 'log': content, 'time': time_now})
-            data = data[0 : len(data) - 1]
+            data = data[0: len(data) - 1]
-            str = ''
+            str_out = ''
-                str += "%d(%s) " % (time_per, v)
+                str_out += "%d(%s) " % (time_per, v)
-                    active_time, transfered_data, transfered_quota, str)
+                    active_time, transfered_data, transfered_quota, str_out)
-            google_ip.update_scan_thread_num(scan_ip_thread_num)
+            google_ip.adjust_scan_thread_num(scan_ip_thread_num)
-    run_cmd = os.path.abspath( os.path.join(root_path, "start.sh"))
+    run_cmd = os.path.abspath( os.path.join(root_path, "start.command"))
-        module_menus = {}
+        new_module_menus = {}
-            module_menus[module] = module_menu
+            new_module_menus[module] = module_menu
-        module_menus = sorted(module_menus.iteritems(), key=lambda (k,v): (v['menu_sort_id']))
+        module_menus = sorted(new_module_menus.iteritems(), key=lambda (k,v): (v['menu_sort_id']))
-            return self._render(po_dict, template_file)
+            return self._render(po_dict, template_file)
-i18n_translator = simpleI18N()
+i18n_translator = simpleI18N(config.get(['language'], None))
-        if os.path.isfile(right_content_file):            
+        if os.path.isfile(right_content_file):
-            data = '{ "check_update": "%s", "popup_webui": %d, "allow_remote_connect": %d, "show_systray": %d, "auto_start": %d, "php_enable": %d, "gae_proxy_enable": %d }' %\
+            data = '{ "check_update": "%s", "language": "%s", "popup_webui": %d, "allow_remote_connect": %d, "show_systray": %d, "auto_start": %d, "php_enable": %d, "gae_proxy_enable": %d }' %\
-    
+
-                if target_module == title and target_menu == sub_url:
+                if target_module == module and target_menu == sub_url:
-        self.GAE_APPIDS = self.CONFIG.get('gae', 'appid').split("|")
+        self.GAE_APPIDS = [x.strip() for x in self.CONFIG.get('gae', 'appid').split("|")]
-def connect_ssl(ip, port=443, timeout=5, openssl_context=None):
+def connect_ssl(ip, port=443, timeout=5, openssl_context=None, check_cert=True):
-from proxy import xlog
+import logging as xlog
-                time.sleep(10)
+                time.sleep(60)
-    def ip_quality(self, num=20):
+    def ip_quality(self, num=10):
-                content = i18n_translator.render(locale_dir, os.path.join(root_path, module, "web_ui", relate_path))
+                content = i18n_translator.render(locale_dir, file_path)
-
+    def reset_appid(self):
-        xlog.warn("app check %s status:%d", appid, response.status)
+        #xlog.warn("app check %s status:%d", appid, response.status)
-        xlog.warn("app check %s content:%s", appid, content)
+        #xlog.warn("app check %s content:%s", appid, content)
-        self.GAE_APPIDS = re.findall(r'[\w\-\.]+', self.CONFIG.get('gae', 'appid').replace('.appspot.com', ''))
+        self.GAE_APPIDS = self.CONFIG.get('gae', 'appid').split("|")
-        self.reset_appid()
+        if len(config.GAE_APPIDS) == 0:
-            return random.choice(self.working_appid_list)
+            if time.time() - self.last_reset_time < 60:
-            
+        if os.path.isfile(right_content_file):            
-Integral.register(long)
+    if response.status == 404:
-        raise Exception("app check fail")
+        xlog.warn("app check %s status:%d", appid, response.status)
-        raise Exception("content fail")
+        xlog.warn("app check %s content:%s", appid, content)
-    ssl_sock.server_type = response.getheader('Server', "")
+    return True
-                return False
+                return check_ip.check_appid(ssl_sock, appid)
-                        return self.send_response('text/html', '{"res":"fail", "reason":"appid fail:%s"}' % fail_appid)
+                    if appids:
-            #return unicode(line, errors='replace')
+            if type(line) is types.UnicodeType:
-            self.wfile.write('HTTP/1.1 200\r\nContent-Length: %d\r\n\r\n%s' %(len(data), data) )
+            self.wfile.write('HTTP/1.1 200\r\nAccess-Control-Allow-Origin: *\r\nContent-Length: %d\r\n\r\n%s' %(len(data), data) )
-                    if "gws" not in server_type and "Google Frontend" not in server_type:
+                    if "gws" not in server_type and "Google Frontend" not in server_type and "GFE" not in server_type:
-network_ok = True
+network_ok = False
-last_ok_time = 0
+check_network_interval = 60
-def network_is_ok(force=False):
+def check_worker():
-        conn = httplib.HTTPSConnection("github.com", 443, timeout=10)
+        conn = httplib.HTTPSConnection("github.com", 443, timeout=30)
-            last_check_time = time.time()
+def network_is_ok(force=False):
-    last_ok_time = time_handshaked
+
-    def ip_handshake_th(self, num):
+    def ip_quality(self, num=20):
-            return handshake_time
+            ip_th = min(num, iplist_length)
-                time_now = time.time()
+
-        xlog.debug('GAE CONNECT %s %s', self.command, self.path)
+
-                   "good_ip_num":good_ip_num,
+        res_arr = {
-                   "pac_url":config.pac_url,
+
-                   "ip_handshake_10":google_ip.ip_handshake_th(10),
+                   "ip_quality": google_ip.ip_quality(),
-                   "use_ipv6":config.CONFIG.getint("google_ip", "use_ipv6"),
+
-                value = read_msgstr(fp)
+                value = ""
-                return "auto"
+                return "pac"
-                return "enable"
+                return "gae"
-        auto_checked = win32_adapter.fState.MFS_CHECKED if proxy_stat=="auto" else 0
+        gae_proxy_checked = win32_adapter.fState.MFS_CHECKED if proxy_stat=="gae" else 0
-                        (u"å¨å±PACæºè½ä»£ç", None, self.on_enable_pac, auto_checked),
+                        (u"å¨å±éè¿GAEProxyä»£ç", None, self.on_enable_gae_proxy, gae_proxy_checked),
-                        (u"Set Global PAC Proxy", None, self.on_enable_pac, auto_checked),
+                        (u"Set Global GAEProxy Proxy", None, self.on_enable_gae_proxy, gae_proxy_checked),
-    def on_enable_proxy(self, widget=None, data=None):
+    def on_enable_gae_proxy(self, widget=None, data=None):
-def network_is_ok():
+def network_is_ok(force=False):
-        return network_ok
+    time_now = time.time()
-        return True
+        if time_now - last_ok_time < check_network_interval:
-        return network_ok
+        if checking_num > 0:
-        conn = httplib.HTTPSConnection("github.com", 443, timeout=30)
+        conn = httplib.HTTPSConnection("github.com", 443, timeout=10)
-            xlog.debug("network is ok")
+            xlog.debug("network is ok, cost:%d ms", 1000*(time.time() - time_now))
-        pass
+    except Exception as e:
-    return False
+
-                   "network_state":check_ip.network_is_ok(),
+                   "network_state":check_ip.network_ok,
-        req_url = update_url + "?uuid=" + get_uuid() + "&version=" + update_from_github.current_version()
+        req_url = update_url + "?uuid=" + get_uuid() \
-import errno
+import socket, ssl
-import jinja2_i18n_helper
+#import jinja2_i18n_helper
-
+i18n_translator = simpleI18N()
-            stream = jinja2_i18n_helper.ihelper.render("menu.yaml", None)
+            #template_dir = os.path.abspath(os.path.join(root_path, module, 'web_ui'))
-                content = jinja2_i18n_helper.ihelper.render(relate_path, None)
+                #jinja2_i18n_helper.ihelper.refresh_env(locale_dir, template_dir)
-        index_content = jinja2_i18n_helper.ihelper.render("index.html", None)
+        #template_dir = os.path.abspath(os.path.join(current_path, 'web_ui'))
-            right_content = jinja2_i18n_helper.ihelper.render(target_menu + ".html", None)
+            #template_dir = os.path.abspath(os.path.join(root_path, target_module, 'web_ui'))
-import _winreg as winreg
+
-                   "good_ip_num":google_ip.good_ip_num,
+                   "good_ip_num":good_ip_num,
-
+import _winreg as winreg
-
+
-def test_gae_ip(ip):
+def test_gae_ip(ip, appid=None):
-        appid = appid_manager.get_appid()
+        if not appid:
-
+    def test_appid(self, appid):
-                user_config.user_special.appid = self.postvars['appid'][0]
+                appids = self.postvars['appid'][0]
-                   "network_state":check_ip.network_ok,
+                   "network_state":check_ip.network_is_ok(),
-                user_config.user_special.proxy_port = int(self.postvars['proxy_port'][0])
+                user_config.user_special.proxy_port = self.postvars['proxy_port'][0]
-# V. See Language_contry code list: http://www.fincher.org/Utilities/CountryLanguageList.shtml
+# V. See Language_country code list: http://www.fincher.org/Utilities/CountryLanguageList.shtml
-                time.sleep(10)
+                #time.sleep(10)
-        self.wfile.write(data)
+        try:
-            return unicode(line, errors='replace')
+            return line.encode("utf-8")
-noarch_lib = os.path.abspath( os.path.join(python_path, 'lib', 'noarch'))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir))
-
+import struct
-            pass
+from proxy import xlog
-g_handshake_timeout = 2
+openssl_context = SSLConnection.context_builder(ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
-default_socket = None
+default_socket = socket.socket
-        default_socket = socket.socket
+#  Checking network ok
-    xlog.debug("conn: %d  handshake:%d", connct_time, handshake_time)
+    #xlog.debug("conn: %d  handshake:%d", connct_time, handshake_time)
-    def check(self, callback=None, check_ca=True, close_ssl=True):
+    ssl_sock.connct_time = connct_time
-                    raise SSLError("no cert")
+def get_ssl_cert_domain(ssl_sock):
-                issuer_commonname = next((v for k, v in cert.get_issuer().get_components() if k == 'CN'), '')
+    #issuer_commonname = next((v for k, v in cert.get_issuer().get_components() if k == 'CN'), '')
-    appid = appid_manager.get_appid()
+def check_appid(ssl_sock, appid):
-        return False
+    response.begin()
-    check.result.server_type = result
+    content = response.read()
-    return check.result
+    ssl_sock.server_type = response.getheader('Server', "")
-    check = Check_frame(ip_str, check_cert=False)
+        appid = appid_manager.get_appid()
-    if not result or not "gvs" in result:
+        return ssl_sock
-
+    scan_exist_ip_queue = Queue.Queue()
-                self.ip_dict[ip_str]['fail_time'] = 0
+                if self.ip_dict[ip_str]['fail_time'] > 0:
-            self.ip_dict[ip_str]['handshake_time'] += 300
+            #self.ip_dict[ip_str]['handshake_time'] += 300
-                result = check_ip.test_gae(ip_str)
+                result = check_ip.test_gae_ip(ip_str)
-                if self.add_ip(ip_str, result.handshake_time, result.domain, result.server_type):
+                if self.add_ip(ip_str, result.handshake_time, result.domain, "gws"):
-                    scan_ip_log.info("Add %s time:%d CN:%s type:%s", ip_str, result.handshake_time, result.domain, result.server_type)
+                    scan_ip_log.info("Add %s time:%d CN:%s ", ip_str, result.handshake_time, result.domain)
-                user_config.user_special.proxy_port = self.postvars['proxy_port'][0]
+                user_config.user_special.proxy_port = int(self.postvars['proxy_port'][0])
-        result = check_ip.test_gws(ip)
+        result = check_ip.test_gae_ip(ip)
-                file_path = os.path.join(root_path, module, url_path_list[3:].join('/'))
+                relate_path = '/'.join(url_path_list[3:])
-        print("Your desired language is %s" % desired_lang)
+        #print("Your desired language is %s" % desired_lang)
-		    
+		
-        #print("Your desired language is %s" % desired_lang)
+        print("Your desired language is %s" % desired_lang)
-    #desired_lang = "zh_CN" # Simple Chinese
+    desired_lang = "zh_CN" # Simple Chinese
-    desired_lang = "fa_IR" # Iran-Persian
+    #desired_lang = "fa_IR" # Iran-Persian
-    stream = ihelper.render("menu.yaml", None)
+    stream = ihelper.render("menu.yaml", desired_lang)
-            self.out_of_quota_appids.append(appid)
+            if appid not in self.out_of_quota_appids:
-            self.reset_appid()
+            self.working_appid_list = config.GAE_APPIDS
-            self.working_appid_list.remove(appid)
+            if appid not in self.not_exist_appids:
-
+        #xlog.debug("reset_appid")
-            self.out_of_quota_appids = []
+            self.out_of_quota_appids = []
-                   "total_connected_link":(len(https_manager.new_conn_pool.pool)+len(https_manager.gae_conn_pool.pool)),
+                   "connected_link_new":len(https_manager.new_conn_pool.pool),
-                check_update = "long-term-stable"
+                check_update = "stable"
-                if check_update not in ["dont-check", "long-term-stable", "stable", "test"]:
+                if check_update not in ["dont-check", "stable", "test"]:
-        self.trayicon.set_from_file(logo_filename)
+        if platform and appindicator and platform.dist()[0].lower() == 'ubuntu':
-        self.trayicon.set_visible(True)
+        return trayicon
-import random
+from proxy import xlog
-import xlog
+from proxy import xlog
-    import xlog
+    import logging as xlog
-import xlog
+
-import xlog
+from proxy import xlog
-import random
+
-from connect_control import connect_allow_time, connect_fail_time
+from proxy import xlog
-import BaseHTTPServer
+from proxy import xlog
-import xlog
+from proxy import xlog
-import xlog
+from proxy import xlog
-
+import select
-import xlog
+from proxy import xlog
-import simple_http_server
+import simple_http_server
-python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir))
-NetWorkIOError = (socket.error, ssl.SSLError, OSError)
+def create_data_path():
-    xlog.basicConfig(level=xlog.DEBUG if config.LISTEN_DEBUGINFO else xlog.INFO, format='%(levelname)s - %(asctime)s %(message)s', datefmt='[%b %d %H:%M:%S]')
+    #xlog.basicConfig(level=xlog.DEBUG if config.LISTEN_DEBUGINFO else xlog.INFO, format='%(levelname)s - %(asctime)s %(message)s', datefmt='[%b %d %H:%M:%S]')
-
+from proxy import xlog
-from connect_control import connect_allow_time, connect_fail_time, touch_active
+from connect_control import touch_active
-import xlog
+from proxy import xlog
-import xlog
+from proxy import xlog
-class ControlHandler():
+
-
+            
-import launcher_log
+from instances import xlog
-            launcher_log.warn("autorun linux config path not found:%s", os.path.expanduser(_xdg_config_home))
+            xlog.warn("autorun linux config path not found:%s", os.path.expanduser(_xdg_config_home))
-        launcher_log.info("create file:%s", plist_file_path)
+        xlog.info("create file:%s", plist_file_path)
-            launcher_log.info("remove file:%s", plist_file_path)
+            xlog.info("remove file:%s", plist_file_path)
-
+from instances import xlog
-        launcher_log.warn("save config %s fail %s", config_path, e)
+        xlog.warn("save config %s fail %s", config_path, e)
-    create_data_path()
+def init():
-    #eval("conf = {}")
+init()
-import base64
+import sys
-import launcher_log
+from instances import xlog
-    launcher_log.warn("import pynotify fail, please install python-notify if possiable.")
+    xlog.warn("import pynotify fail, please install python-notify if possiable.")
-import launcher_log
+from instances import xlog
-        launcher_log.info("try enable proxy:%s", cmd)
+        xlog.info("try enable proxy:%s", cmd)
-        launcher_log.info("try disable proxy:%s", cmd)
+        xlog.info("try disable proxy:%s", cmd)
-        launcher_log.debug("dialog_yes_no return %d", res)
+        xlog.debug("dialog_yes_no return %d", res)
-        launcher_log.error("Mac notify_general not implemented.")
+        xlog.error("Mac notify_general not implemented.")
-import launcher_log
+from instances import xlog
-            launcher_log.error("module not exist %s", module)
+            xlog.error("module not exist %s", module)
-            launcher_log.error("module %s is running", module)
+            xlog.error("module %s is running", module)
-                launcher_log.warn("start module script not exist:%s", script_path)
+                xlog.warn("start module script not exist:%s", script_path)
-        launcher_log.info("%s started", module)
+        xlog.info("module %s started", module)
-        launcher_log.exception("start module %s fail:%s", module, e)
+        xlog.exception("start module %s fail:%s", module, e)
-            launcher_log.error("module %s not running", module)
+            xlog.error("module %s not running", module)
-            launcher_log.debug("module %s stopping", module)
+            xlog.debug("module %s stopping", module)
-        launcher_log.info("module %s stopped", module)
+        xlog.info("module %s stopped", module)
-        launcher_log.exception("stop module %s fail:%s", module, e)
+        xlog.exception("stop module %s fail:%s", module, e)
-            launcher_log.info("start %s time cost %d", module, (finished_time - start_time) * 1000)
+            xlog.info("start %s time cost %d", module, (finished_time - start_time) * 1000)
-import launcher_log
+from instances import xlog
-            launcher_log.info("download %s to %s", url, file)
+            xlog.info("download %s to %s", url, file)
-            launcher_log.info("download %s to %s fail", url, file)
+            xlog.info("download %s to %s fail", url, file)
-            launcher_log.exception("xxnet_version fail:%s", e)
+            xlog.exception("xxnet_version fail:%s", e)
-                launcher_log.info("mkdir %s", target_path)
+                xlog.info("mkdir %s", target_path)
-                launcher_log.info("copy %s => %s", src_file, dst_file)
+                xlog.info("copy %s => %s", src_file, dst_file)
-    launcher_log.info("setup start run new launcher")
+    xlog.info("setup start run new launcher")
-import launcher_log
+from instances import xlog
-                launcher_log.info("setup win python, mkdir:%s", dest_path)
+                xlog.info("setup win python, mkdir:%s", dest_path)
-                launcher_log.info("setup win python, copy:%s %s", src_path, target_file)
+                xlog.info("setup win python, copy:%s %s", src_path, target_file)
-            launcher_log.exception("setup win python except:%s", e)
+            xlog.exception("setup win python except:%s", e)
-python_path = os.path.abspath( os.path.join(current_path, os.pardir, 'python27', '1.0'))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir))
-    launcher_log.info("start XX-Net %s", update_from_github.current_version())
+    xlog.info("start XX-Net %s", update_from_github.current_version())
-import launcher_log
+from instances import xlog
-        launcher_log.info("download %s to %s", url, file)
+        xlog.info("download %s to %s", url, file)
-        launcher_log.info("download %s to %s fail", url, file)
+        xlog.info("download %s to %s fail", url, file)
-        launcher_log.error("install module %s dir %s not exist", module, new_module_version_path)
+        xlog.error("install module %s dir %s not exist", module, new_module_version_path)
-        launcher_log.warn("update %s fail. setup script %s not exist", module, setup_script)
+        xlog.warn("update %s fail. setup script %s not exist", module, setup_script)
-        launcher_log.info("Setup %s version %s ...", module, new_version)
+        xlog.info("Setup %s version %s ...", module, new_version)
-            launcher_log.info("Finished new version setup.")
+            xlog.info("Finished new version setup.")
-            launcher_log.info("Restarting new version ...")
+            xlog.info("Restarting new version ...")
-            launcher_log.error("install module %s %s fail:%s", module, new_version, e)
+            xlog.error("install module %s %s fail:%s", module, new_version, e)
-                launcher_log.warn("download %s fail", url)
+                xlog.warn("download %s fail", url)
-                launcher_log.warn("download %s sha1 wrong", url)
+                xlog.warn("download %s sha1 wrong", url)
-                launcher_log.error("module dir exist:%s, download exist.", version_path)
+                xlog.error("module dir exist:%s, download exist.", version_path)
-        launcher_log.warn("get gae_proxy source fail, content:%s err:%s", update_content, e)
+        xlog.warn("get gae_proxy source fail, content:%s err:%s", update_content, e)
-        launcher_log.error("general_gtk_callback data:%s", data)
+        xlog.error("general_gtk_callback data:%s", data)
-                launcher_log.info("update to test version %s", versions[0][1])
+                xlog.info("update to test version %s", versions[0][1])
-                launcher_log.info("update to stable version %s", versions[1][1])
+                xlog.info("update to stable version %s", versions[1][1])
-        launcher_log.warn("check update fail:%r", e)
+        xlog.warn("check update fail:%r", e)
-        launcher_log.exception("check_update fail:%r", e)
+        xlog.exception("check_update fail:%r", e)
-            launcher_log.warn("check_update fail:%r", e)
+            xlog.warn("check_update fail:%r", e)
-                launcher_log.info("new %s version:%s", module, new_version)
+                xlog.info("new %s version:%s", module, new_version)
-        launcher_log.exception("check_update except:%s", e)
+        xlog.exception("check_update except:%s", e)
-        launcher_log.info("generate desktop shortcut")
+        xlog.info("generate desktop shortcut")
-        launcher_log.info("need_new_uuid: uuid is empty")
+        xlog.info("need_new_uuid: uuid is empty")
-    launcher_log.info("generate uuid:%s", xx_net_uuid)
+    xlog.info("generate uuid:%s", xx_net_uuid)
-    launcher_log.info("get uuid:%s", xx_net_uuid)
+    xlog.info("get uuid:%s", xx_net_uuid)
-import launcher_log
+from instances import xlog
-        launcher_log.exception("xxnet_version fail")
+        xlog.exception("xxnet_version fail")
-            launcher_log.warn("url in downloading, %s", url)
+            xlog.warn("url in downloading, %s", url)
-            launcher_log.info("download %s to %s, retry:%d", url, file, i)
+            xlog.info("download %s to %s, retry:%d", url, file, i)
-                launcher_log.warn("download size:%d, need size:%d, download fail.", downloaded, download_progress[url]["size"])
+                xlog.warn("download size:%d, need size:%d, download fail.", downloaded, download_progress[url]["size"])
-            launcher_log.warn("download %s to %s URL fail:%r", url, file, e)
+            xlog.warn("download %s to %s URL fail:%r", url, file, e)
-            launcher_log.exception("download %s to %s fail:%r", url, file, e)
+            xlog.exception("download %s to %s fail:%r", url, file, e)
-        launcher_log.exception("xxnet_version fail:%r", e)
+        xlog.exception("xxnet_version fail:%r", e)
-                    launcher_log.info("mkdir %s", target_path)
+                    xlog.info("mkdir %s", target_path)
-                    launcher_log.info("copy %s => %s", src_file, dst_file)
+                    xlog.info("copy %s => %s", src_file, dst_file)
-        launcher_log.exception("update version %s fail:%r", version, e)
+        xlog.exception("update version %s fail:%r", version, e)
-import launcher_log
+from instances import xlog
-class Http_Handler(BaseHTTPServer.BaseHTTPRequestHandler):
+class Http_Handler(simple_http_server.HttpServerHandler):
-                launcher_log.warn("web control ref:%s host:%s", refer_loc, host)
+                xlog.warn("web control ref:%s host:%s", refer_loc, host)
-                    launcher_log.warn("request %s no module in path", self.path)
+                    xlog.warn("request %s no module in path", self.path)
-                launcher_log.warn("web control ref:%s host:%s", refer_loc, host)
+                xlog.warn("web control ref:%s host:%s", refer_loc, host)
-            launcher_log.warn('%s %s %s haking', self.address_string(), self.command, self.path )
+            xlog.warn('%s %s %s haking', self.address_string(), self.command, self.path )
-                    launcher_log.warn("request %s no module in path", url_path)
+                    xlog.warn("request %s no module in path", url_path)
-                    launcher_log.warn("request module:%s start fail", module)
+                    xlog.warn("request module:%s start fail", module)
-        launcher_log.debug ('launcher web_control %s %s %s ', self.address_string(), self.command, self.path)
+        xlog.debug ('launcher web_control %s %s %s ', self.address_string(), self.command, self.path)
-            self.wfile.write(b'HTTP/1.1 404\r\nContent-Type: text/plain\r\nConnection: close\r\n\r\n404 Open file fail')
+            self.send_not_found()
-                    launcher_log.debug("restart web control.")
+                    xlog.debug("restart web control.")
-                    launcher_log.debug("launcher web control restarted.")
+                    xlog.debug("launcher web control restarted.")
-            launcher_log.info("%s", data)
+            xlog.info("%s", data)
-                launcher_log.info("update_test_version fail:%r", e)
+                xlog.info("update_test_version fail:%r", e)
-            launcher_log.exception("init_module except:%s", e)
+            xlog.exception("init_module except:%s", e)
-    server = LocalServer((host_addr, host_port), Http_Handler)
+    xlog.info("begin to start web control")
-    launcher_log.info("launcher web control started.")
+    
-    launcher_log.info("begin to exit web control")
+    xlog.info("begin to exit web control")
-    launcher_log.info("launcher web control exited.")
+    xlog.info("launcher web control exited.")
-    launcher_log.debug("start confirm_xxnet_exit")
+    xlog.debug("start confirm_xxnet_exit")
-            launcher_log.debug("good, xxnet:8087 cleared!")
+            xlog.debug("good, xxnet:8087 cleared!")
-            launcher_log.debug("<%d>: try to terminate xxnet:8087" % i)
+            xlog.debug("<%d>: try to terminate xxnet:8087" % i)
-            launcher_log.debug("good, xxnet:%s clear!" % host_port)
+            xlog.debug("good, xxnet:%s clear!" % host_port)
-            launcher_log.debug("<%d>: try to terminate xxnet:%s" % (i, host_port))
+            xlog.debug("<%d>: try to terminate xxnet:%s" % (i, host_port))
-    launcher_log.debug("finished confirm_xxnet_exit")
+    xlog.debug("finished confirm_xxnet_exit")
-        launcher_log.error("confirm_module_ready with port 0")
+        xlog.error("confirm_module_ready with port 0")
-import launcher_log
+from instances import xlog
-        #xlog.info('Connected from %r', self.client_address)
+        #logging.info('Connected from %r', self.client_address)
-                xlog.warn("handle err:%r close", e)
+                #logging.warn("handle err:%r close", e)
-        #xlog.debug("closed from %s:%d", self.client_address[0], self.client_address[1])
+        #logging.debug("closed from %s:%d", self.client_address[0], self.client_address[1])
-            self.wfile.write(message)
+    def address_string(self):
-                #xlog.warn("simple server handle except %r", e)
+                #logging.warn("simple server handle except %r", e)
-                xlog.warn("recv command line too large")
+                #logging.warn("recv command line too large")
-                #xlog.warn("closed")
+                #logging.warn("closed")
-                xlog.warn("unhandler cmd:%s", self.command)
+                logging.warn("unhandler cmd:%s", self.command)
-            xlog.warn("socket error:%r", e)
+            #logging.warn("socket error:%r", e)
-                xlog.warn("PIPE error:%r", e)
+                logging.warn("PIPE error:%r", e)
-                xlog.warn("IOError:%r", e)
+                logging.warn("IOError:%r", e)
-        #    xlog.warn("socket error:%r", e)
+        #    logging.warn("socket error:%r", e)
-        self.wfile.write(data)
+            logging.exception("handler:%r", e)
-    def do_PUT(self):
+    def do_POST(self):
-    def do_POST(self):
+    def do_PUT(self):
-        self.socket = None
+    def __init__(self, address, handler, args=(), use_https=False, cert=""):
-        self.server_address = address
+        if isinstance(address, tuple):
-        xlog.info("server %s:%d started.", address[0], address[1])
+        #logging.info("server %s:%d started.", address[0], address[1])
-        self.socket.listen(200)
+        self.sockets = []
-            if self.socket in r:
+            r, w, e = select.select(self.sockets, [], [], 1)
-                    (sock, address) = self.socket.accept()
+                    (sock, address) = rsock.accept()
-                    xlog.warn("socket(%s:%s) accept fail(errno: %s).", self.server_address[0], self.server_address[1], e.args[0])
+                    logging.warn("socket accept fail(errno: %s).", e.args[0])
-                        xlog.info("server %s:%d restarted.", self.server_address[0], self.server_address[1])
+                        logging.info("restart socket server.")
-        #xlog.debug("connect from %s:%d", address[0], address[1])
+        #logging.debug("connect from %s:%d", address[0], address[1])
-        self.socket.close()
+        for sock in self.sockets:
-class test_http_server(HttpServerHandler):
+class TestHttpServer(HttpServerHandler):
-
+        #logging.debug("GET %s from %s:%d", self.path, self.client_address[0], self.client_address[1])
-
+def main(data_path="."):
-            xlog.warn("download broken")
+    http_thread = threading.Thread(target=httpd.serve_forever)
-    httpd.serve_forever()
+    while True:
-
+        
-            return -1
+            return 9999
-        res_arr = {"gws_ip_num": "%d,%d" % (len(google_ip.gws_ip_list), google_ip.good_ip_num),
+        res_arr = {"ip_num":len(google_ip.gws_ip_list),
-# they're put in folder python27/1.0/lib/noarc
+# they're put in folder python27/1.0/lib/noarch
-# IV. See ytz-2015.7.tar.gz: https://pypi.python.org/packages/source/p/pytz/pytz-2015.7.tar.gz#md5=252bb731883f37ff9c7f462954e8706d
+# IV. See pytz-2015.7.tar.gz: https://pypi.python.org/packages/source/p/pytz/pytz-2015.7.tar.gz#md5=252bb731883f37ff9c7f462954e8706d
-	
+
-            if module != "launcher" and config.get(["modules", module, "auto_start"], 0) != 1:
+            if module != "launcher" and config.get(["modules", module, "auto_start"], 0) != 1: # skip php_proxy module
-            menu_path = os.path.join(root_path, module, "web_ui", "menu.yaml")
+            menu_path = os.path.join(root_path, module, "web_ui", "menu.yaml") # launcher & gae_proxy modules
-            module_menu = yaml.load(file(menu_path, 'r'))
+            #module_menu = yaml.load(file(menu_path, 'r')) # non-i18n
-            index_content = f.read()
+            
-                right_content = f.read()
+            # Old code without i18n
-        self.wfile.write(('HTTP/1.1 200\r\nAccess-Control-Allow-Origin: *\r\nContent-Type: %s\r\nContent-Length: %s\r\n\r\n' % (mimetype, len(data))).encode())
+        no_cache = "Cache-Control: no-cache, no-store, must-revalidate\r\nPragma: no-cache\r\nExpires: 0\r\n"
-    cnregex = re.compile(r'^apnic\|(?:cn|hk|mo)\|ipv4\|[\d\.]+\|\d+\|\d+\|a\w*$',
+    cnregex = re.compile(r'^apnic\|(?:cn)\|ipv4\|[\d\.]+\|\d+\|\d+\|a\w*$',
-import webbrowser
+        import webbrowser
-        launcher_log.exception("update version %d fail:%r", version, e)
+        launcher_log.exception("update version %s fail:%r", version, e)
-            for sock in pool:
+            for sock in self.pool:
-                xlog.debug("%s fail", ip)
+                xlog.debug("%s fail:%r", ip, e)
-        _BaseSocket.__init__(self, family, type, proto, _sock)
+            proxy_host = self.proxy[1]
-            addr_bytes = socket.inet_aton(host)
+        if ":" in host:
-            # Well it's not an IP number, so it's probably a DNS name.
+        else:
-threading.stack_size(64*1024)
+# reduce resource request for threading
-        sys.exit
+        os._exit(0)
-    plist_file_path = os.path.join(home, "Library/LaunchAgents/com.xxnet.launcher.plist")
+    plist_file_path = os.path.join(launch_path, "com.xxnet.launcher.plist")
-            fgae.writelines(lines)
+    file_names = ["gae.py", "wsgi.py"]
-        my_stdout.write('Setting in the Gae.py RC4 password failed!\n')
+        except IOError as e:
-                (sock, address) = self.socket.accept()
+                try:
-                    if time.time() - ssl_sock.last_use_time < self.keep_alive+1: 
+                    if time.time() - ssl_sock.last_use_time < self.keep_alive+1:
-        
+
-                        response.ssl_sock.fd, response.ssl_sock.ip, response.ssl_sock.received_size, (time_finished-time_request)*1000, 
+                    xlog.info("GAE %d|%s|%d t:%d s:%d hs:%d Spd:%d %d %s",
-                    xlog.info("GAE %d|%s|%d t:%d s:%d hs:%d %d %s", 
+                    xlog.info("GAE %d|%s|%d t:%d s:%d hs:%d %d %s",
-                PRINT("any case? good:%s-%s bad:%s-%s" % (ip_utils.ip_num_to_string(good_begin), ip_utils.ip_num_to_string(good_end), 
+                PRINT("any case? good:%s-%s bad:%s-%s" % (ip_utils.ip_num_to_string(good_begin), ip_utils.ip_num_to_string(good_end),
-    
+
-    
+
-        self.gws_ip_list = [] 
+        self.gws_ip_list = []
-        
+
-                
+
-         |                          #   A v4 address with NO leading zeros 
+         |                          #   A v4 address with NO leading zeros
-            
+
-        
+
-                    "failed to decode base64 data: %s" % exc, node.start_mark) 
+                    "failed to decode base64 data: %s" % exc, node.start_mark)
-                if ch in u'#,[]{}&*!|>\'\"%@`': 
+                if ch in u'#,[]{}&*!|>\'\"%@`':
-            
+
-            
+
-        
+
-        
+
-        
+
-        
+
-        
+
-            
+
-                
+
-                
+
-            p = re.compile(r'https://codeload.github.com/XX-net/XX-Net/zip/([0-9]+)\.([0-9]+)\.([0-9]+)') 
+            p = re.compile(r'https://codeload.github.com/XX-net/XX-Net/zip/([0-9]+)\.([0-9]+)\.([0-9]+)')
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-        
+
-    
+
-    
+
-    
+
-        
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-        
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-    
+
-A library to encode/decode DNS wire-format packets supporting both 
+A library to encode/decode DNS wire-format packets supporting both
-   resolvers (dnslib.server) and a number of example servers 
+ * A server framework allowing the simple creation of custom DNS
- * Python 2.7/3.2+ support (the last version supporting Python 2.6 
+ * Python 2.7/3.2+ support (the last version supporting Python 2.6
-   split forward (value->text) lookups via __getitem__ and 
+ * The 'Bimap' interface was changed significantly to explicitly
-   definitions to make it harder to generate invalid packets 
+   definitions to make it harder to generate invalid packets
-   file format 
+ * Support for encoding/decoding resource records in 'Zone' (BIND)
-   be created by just subclassing the DNSResolver class and 
+   be created by just subclassing the DNSResolver class and
- * A lot of fixes to error detection/handling which should make 
+ * A lot of fixes to error detection/handling which should make
-The key DNS packet handling classes are in dnslib.dns and map to the 
+The key DNS packet handling classes are in dnslib.dns and map to the
-    - Additional section containing zero or more RR objects 
+    - DNSHeader
-(retaining support for Python 2.7+). As part of the Py3 changes a 
+Version 0.9 of the library was a major rewrite to support Python 3.2+
-- Much better error handling (packet decoding errors should be 
+- Much better error handling (packet decoding errors should be
-    >>> q = DNSRecord(q=DNSQuestion("abc.com",QTYPE.ANY)) 
+    >>> q = DNSRecord(q=DNSQuestion("abc.com",QTYPE.ANY))
-    >>> q = DNSRecord(q=DNSQuestion("abc.com",QTYPE.ANY)) 
+    >>> q = DNSRecord(q=DNSQuestion("abc.com",QTYPE.ANY))
-    >>> q = DNSRecord(q=DNSQuestion("abc.com",QTYPE.ANY)) 
+    >>> q = DNSRecord(q=DNSQuestion("abc.com",QTYPE.ANY))
-requires implementing a custom 'resolve' method which receives a question 
+resolvers in dnslib.server (see module docs). In post cases this just
- * dnslib.zoneresolver     - Respond from Zone file 
+ * dnslib.zoneresolver     - Respond from Zone file
- * DiG like client library 
+ * DiG like client library
-        Bi-directional mapping between code/value. 
+        Bi-directional mapping between code/value.
-            * A 'reverse' map (code>value) which is accessed through 
+
-        
+
-    
+
-    left = length // 2 
+    left = length // 2
-        result.append("%s%04x  %s %s %s %s" % (prefix, n, hexa, hexb, 
+        result.append("%s%04x  %s %s %s %s" % (prefix, n, hexa, hexb,
-        
+
-    return (data & mask) >> offset 
+    return (data & mask) >> offset
-    A simple data buffer - supports packing/unpacking in struct format 
+    A simple data buffer - supports packing/unpacking in struct format
-            Modify data at offset `ptr` 
+            Modify data at offset `ptr`
-            raise BufferError("Error unpacking struct '%s' <%s>" % 
+            raise BufferError("Error unpacking struct '%s' <%s>" %
-    
+    DNS Client - DiG-like CLI utility.
-    
+
-    question (if present: +qr flag) & answer sections and returns list 
+    Encode/decode DNS packets from DiG textual representation. Parses
-    Unsupported RR types are skipped (this is different from the packet 
+    Unsupported RR types are skipped (this is different from the packet
-        
+
-    DNS - main dnslib module 
+    DNS - main dnslib module
-QTYPE =  Bimap('QTYPE', 
+QTYPE =  Bimap('QTYPE',
-                 250:'TSIG', 251:'IXFR', 252:'AXFR', 255:'ANY', 257:'TYPE257', 
+                 51:'NSEC3PARAM', 52:'TLSA', 55:'HIP', 99:'SPF', 249:'TKEY',
-                 4:'NOTIMP', 5:'REFUSED', 6:'YXDOMAIN', 7:'YXRRSET', 
+                {0:'NOERROR', 1:'FORMERR', 2:'SERVFAIL', 3:'NXDOMAIN',
-        return (origin if isinstance(origin,DNSLabel) 
+        return (origin if isinstance(origin,DNSLabel)
-        Main DNS class - corresponds to DNS packet & comprises DNSHeader, 
+        Main DNS class - corresponds to DNS packet & comprises DNSHeader,
-        """ 
+        """
-            
+
-        
+
-            Formatted 'DiG' (zone) style output 
+            Formatted 'DiG' (zone) style output
-            Implements parse interface 
+            Implements parse interface
-            self.id = id 
+            self.id = id
-    
+
-        
+
-        
+
-        
+
-              self.ra and 'RA' ] 
+        f = [ self.aa and 'AA',
-                            "rcode='%s' %s=%d %s=%d %s=%d %s=%d>" % ( 
+                            "rcode='%s' %s=%d %s=%d %s=%d %s=%d>" % (
-              self.ra and 'ra' ] 
+              self.aa and 'aa',
-    
+
-        
+
-            
+
-        DNS Resource Record 
+        DNS Resource Record
-                    self.rname, QTYPE.get(self.rtype), CLASS.get(self.rclass), 
+                    self.rname, QTYPE.get(self.rtype), CLASS.get(self.rclass),
-            edns = [ ";OPT PSEUDOSECTION", 
+            edns = [ ";OPT PSEUDOSECTION",
-                             self.edns_ver, 
+                             self.edns_ver,
-            raise DNSError("Error unpacking RD [offset=%d]: %s" % 
+            raise DNSError("Error unpacking RD [offset=%d]: %s" %
-                raise DNSError("Invalid TXT record: len(%d) > RD len(%d)" % 
+                raise DNSError("Invalid TXT record: len(%d) > RD len(%d)" %
-            raise DNSError("Error unpacking TXT [offset=%d]: %s" % 
+            raise DNSError("Error unpacking TXT [offset=%d]: %s" %
-            raise DNSError("Error unpacking A [offset=%d]: %s" % 
+            raise DNSError("Error unpacking A [offset=%d]: %s" %
-        Parse IPv6 address. Ideally we would use the ipaddress module in 
+        Parse IPv6 address. Ideally we would use the ipaddress module in
-    return tuple(l_groups + zeros + r_groups) 
+    return tuple(l_groups + zeros + r_groups)
-    
+
- 
+
-            raise DNSError("Error unpacking AAAA [offset=%d]: %s" % 
+            raise DNSError("Error unpacking AAAA [offset=%d]: %s" %
- 
+
-            raise DNSError("Error unpacking MX [offset=%d]: %s" % 
+            raise DNSError("Error unpacking MX [offset=%d]: %s" %
-        
+
-        
+
-            raise DNSError("Error unpacking CNAME [offset=%d]: %s" % 
+            raise DNSError("Error unpacking CNAME [offset=%d]: %s" %
-        
+
-            raise DNSError("Error unpacking SOA [offset=%d]: %s" % 
+            raise DNSError("Error unpacking SOA [offset=%d]: %s" %
-        
+
-            raise DNSError("Error unpacking SRV [offset=%d]: %s" % 
+            raise DNSError("Error unpacking SRV [offset=%d]: %s" %
-    
+
-            raise DNSError("Error unpacking NAPTR [offset=%d]: %s" % 
+            raise DNSError("Error unpacking NAPTR [offset=%d]: %s" %
-            raise DNSError("Error unpacking DNSKEY [offset=%d]: %s" % 
+            raise DNSError("Error unpacking DNSKEY [offset=%d]: %s" %
-        
+
-            raise DNSError("Error unpacking DNSKEY [offset=%d]: %s" % 
+            raise DNSError("Error unpacking DNSKEY [offset=%d]: %s" %
-        
+
-RDMAP = { 'CNAME':CNAME, 'A':A, 'AAAA':AAAA, 'TXT':TXT, 'MX':MX, 
+RDMAP = { 'CNAME':CNAME, 'A':A, 'AAAA':AAAA, 'TXT':TXT, 'MX':MX,
-          'DNSKEY':DNSKEY, 'RRSIG':RRSIG, 
+          'DNSKEY':DNSKEY, 'RRSIG':RRSIG,
-##         with circular dependencies 
+## TODO  - ideally this would be in a separate file but have to deal
-    
+
-    InterceptResolver - proxy requests to upstream server 
+    InterceptResolver - proxy requests to upstream server
-        
+
-        
+        Intercepting resolver
-            skip            - list of wildcard labels to skip 
+            skip            - list of wildcard labels to skip
-    DNSHandler.log = { 
+    DNSHandler.log = {
-    # Too hard to get unicode doctests to work on Python 3.2  
+    # Too hard to get unicode doctests to work on Python 3.2
-            Create DNS label instance 
+            Create DNS label instance
-            Prepend name to label 
+            Prepend name to label
-            Return True if label suffix matches 
+            Return True if label suffix matches
-                    raise BufferError("Recursive pointer in DNSLabel [offset=%d,pointer=%d,length=%d]" % 
+                    raise BufferError("Recursive pointer in DNSLabel [offset=%d,pointer=%d,length=%d]" %
-                    raise BufferError("Invalid pointer in DNSLabel [offset=%d,pointer=%d,length=%d]" % 
+                    raise BufferError("Invalid pointer in DNSLabel [offset=%d,pointer=%d,length=%d]" %
-            Encode and store label with no compression 
+            Encode and store label with no compression
-        Simple Lexer base class. Provides basic lexer framework and 
+        Simple Lexer base class. Provides basic lexer framework and
-        and if lexYYYY is None or the lexer reaches the end of the 
+        and if lexYYYY is None or the lexer reaches the end of the
-                yield tok 
+                yield tok
-        tok = lambda n : (('COMMENT',''.join(s)),n) if s else (None,n) 
+        tok = lambda n : (('COMMENT',''.join(s)),n) if s else (None,n)
-        tok = lambda n : (('ATOM',''.join(s)),n) if s else (None,n) 
+        tok = lambda n : (('ATOM',''.join(s)),n) if s else (None,n)
-        tok = lambda n : (('ATOM',''.join(s)),n) 
+        tok = lambda n : (('ATOM',''.join(s)),n)
-        Test lexing from infinite stream. 
+        Test lexing from infinite stream.
-        Note that the request/response will be each be decoded/re-encoded 
+        Note that the request/response will be each be decoded/re-encoded
-        b) DNSRecord passed to ProxyResolver, serialised back into packet 
+        a) Request packet received by DNSHandler and parsed into DNSRecord
-        Modify DNSHandler logic (get_reply method) to send directly to 
+        Modify DNSHandler logic (get_reply method) to send directly to
-    integer value range (throws ValueError). 
+    integer value range (throws ValueError).
-    Intended to ensure that values packed with struct are in the 
+    Intended to ensure that values packed with struct are in the
-            raise ValueError("Attribute '%s' must be between %d-%d [%s]" % 
+            raise ValueError("Attribute '%s' must be between %d-%d [%s]" %
-            raise ValueError("Attribute '%s' must be tuple with %d elements [%s]" % 
+            raise ValueError("Attribute '%s' must be tuple with %d elements [%s]" %
-            raise ValueError("Attribute '%s' elements must be between %d-%d [%s]" % 
+            raise ValueError("Attribute '%s' elements must be between %d-%d [%s]" %
-                      
+                      packet, hands off a DNSRecord to the Resolver instance,
-                      class variable. 
+                      class variable.
-        Reply: [...] (udp) / 'abc.def.' (A) / RRs: 
+        Reply: [...] (udp) / 'abc.def.' (A) / RRs:
-        called by DNSHandler with the decode request (DNSRecord instance) 
+        called by DNSHandler with the decode request (DNSRecord instance)
-        Note that a single instance is used by all DNSHandler instances so 
+        Note that a single instance is used by all DNSHandler instances so
-        instance specified in <SocketServer>.resolver 
+        instance specified in <SocketServer>.resolver
-        optionally custom logger (instance), handler (class), and 
+        optionally custom logger (instance), handler (class), and
-    
+
-        Example dynamic resolver. 
+        Example dynamic resolver.
-        
+
-    dump of DNS exchange (packet dump & parse output) and test 
+    Reads test files from dnslib/test (by default) containing
-    using the --new option. 
+    using the --new option.
-    failed tests (--debug) 
+    failed tests (--debug)
-    
+
-        python -m dnslib.client --query --hex --dig <domain> <type> 
+        python -m dnslib.client --query --hex --dig <domain> <type>
-    handled as an opaque blob). 
+    handled as an opaque blob).
-    (matched against the --glob parameter) 
+    (matched against the --glob parameter)
-try: 
+try:
-except NameError: 
+except NameError:
-    # Repack the data 
+    # Repack the data
-    # We occasionally get issues where original packet did not 
+    # Check if repacked question data matches original
-            Initialise resolver from zone file. 
+            Initialise resolver from zone file.
-            If 'glob' is True use glob match against zone file 
+            If 'glob' is True use glob match against zone file
-                                                 qtype == 'ANY' or 
+            if getattr(qname,self.eq)(name) and (qtype == rtype or
-    
+
-        
+
-                                
+
-    
+
-                j = 7                    
+                j = 7
-        
+
-    
+
-                )            
+                )
-            idx = idx + 1                
+            idx = idx + 1
-    protoComponent = univ.SequenceOf()    
+    protoComponent = univ.SequenceOf()
-            idx = idx + 1                
+            idx = idx + 1
-    
+
-    
+
-        
+
-        
+
-                
+
-            #            
+            #
-                else:                    
+                else:
-            
+
-        
+
-        (1, 3, 6, 1, 2): (43, 6, 1, 2),        
+        (1, 3, 6, 1, 2): (43, 6, 1, 2),
-    def encodeValue(self, encodeFun, value, defMode, maxChunkSize):    
+    def encodeValue(self, encodeFun, value, defMode, maxChunkSize):
-                    subid = subid >> 7 
+                    subid = subid >> 7
-                
+
-    useful.UTCTime.tagSet: OctetStringEncoder()        
+    useful.UTCTime.tagSet: OctetStringEncoder()
-        # BER allows any non-zero value as TRUE; cf. sections 8.2.2. and 11.1 
+        # BER allows any non-zero value as TRUE; cf. sections 8.2.2. and 11.1
-                  c2.getEffectiveTagSet() or c2.getTagSet()        
+                  c2.getEffectiveTagSet() or c2.getTagSet()
-        
+
-        
+
-    
+
-            [ '%s%.2X' % (n%16 == 0 and ('\n%.5d: ' % n) or '', x) 
+            [ '%s%.2X' % (n%16 == 0 and ('\n%.5d: ' % n) or '', x)
-    
+
-    
+
-        
+
-    
+
-    
+
-    
+
-class AbstractSimpleAsn1Item(Asn1ItemBase):    
+class AbstractSimpleAsn1Item(Asn1ItemBase):
-        
+
-    
+
-    def clone(self, tagSet=None, subtypeSpec=None, sizeSpec=None, 
+    def clone(self, tagSet=None, subtypeSpec=None, sizeSpec=None,
-    
+
-    
+
-        
+
-# Boolean ops on constraints 
+# Boolean ops on constraints
-    
+
-    
+
-    
+
-    
+
-    
+
-        
+
-        
+
-    
+
-        self.namedValues = ()        
+        self.namedValues = ()
-    
+
-    
+
-        
+
-    
+
-        
+
-    
+
-        
+
-            
+
-        
+
-    def __float__(self): return float(self._value)    
+    def __float__(self): return float(self._value)
-                        )                
+                        )
-   
+
-                        )                
+                        )
-                        
+
-    def fromHexString(self, value):            
+
-                                
+
- 
+
-    
+
-    
+
-    
+
-    
+
-    
+
-            return tuple(value)        
+            return tuple(value)
-    
+
-    
+
-        
+
-    
+
-    
+
-    
+
-        
+
-            self._verifySubtypeSpec(value, idx)            
+            self._verifySubtypeSpec(value, idx)
-        r = self.__class__.__name__ + ':\n'        
+        r = self.__class__.__name__ + ':\n'
-        
+
-            self._verifySubtypeSpec(value, idx)            
+            self._verifySubtypeSpec(value, idx)
-    
+
-    
+
-    
+
-    
+
-        
+
-            
+
-    
+
-            self._verifySubtypeSpec(value, idx)            
+            self._verifySubtypeSpec(value, idx)
-            
+
-                    "failed to decode base64 data: %s" % exc, node.start_mark) 
+                    "failed to decode base64 data: %s" % exc, node.start_mark)
-                if ch in u'#,[]{}&*!|>\'\"%@`': 
+                if ch in u'#,[]{}&*!|>\'\"%@`':
-            
+
-            
+
-        
+
-        
+
-        
+
-        
+
-        
+
-            
+
-                
+
-                
+
-            xlog.warn('request failed:%s', e)
+            xlog.exception('request failed:%s', e)
-        start += sended
+    try:
-    response = httplib.HTTPResponse(ssl_sock, buffering=True)
+        response = httplib.HTTPResponse(ssl_sock, buffering=True)
-    try:
+        google_ip.report_connect_closed(ssl_sock.ip, "request_fail")
-                    if "gws" not in server_type:
+                    if "gws" not in server_type and "Google Frontend" not in server_type:
-                    continue
+                continue
-                xlog.warning("no gws ip")
+                time.sleep(10)
-
+            self.ip_dict[ip_str]['links'] -= 1
-            self.ip_dict[ip_str]['links'] -= 1
+                if self.good_ip_num > len(self.ip_dict):
-                continue
+            try:
-
+        self.lock.acquire()
-        try:
+        finally:
-        self.not_exist_appids = []
+        self.lock.acquire()
-def load_sock():
+def load_proxy_config():
-load_sock()
+load_proxy_config()
-
+def test_alive(ip_str="180.188.250.54", begin=45, end=60, interval=2):
-    time.sleep(begin)
+    time_now = time.time()
-            break
+            xlog.info("time alive fail:%d", time_now - stat["start_time"])
-    #test("216.239.38.123")
+    #result = test_gae('64.15.119.69')
-def load_sock():
+def load_proxy_config():
-load_sock()
+load_proxy_config()
-            ssl_sock.sock.settimeout(3)
+            ssl_sock.settimeout(10)
-            xlog.debug("head request BadStatusLine fail:%r", e)
+            inactive_time = time.time() - ssl_sock.last_use_time
-                    xlog.warning("no gws ip")
+                    xlog.warning("ip not enough")
-                    if time.time() - ssl_sock.last_use_time < self.keep_alive+1: # gws ssl connection can keep for 230s after created
+                    if time.time() - ssl_sock.last_use_time < self.keep_alive+1: 
-                if time.time() - ssl_sock.last_use_time < self.keep_alive+1: # gws ssl connection can keep for 230s after created
+                if time.time() - ssl_sock.last_use_time < self.keep_alive+1:
-        response.status = 502
+        response.app_status = 502
-        response.status = 502
+        response.app_status = 509
-                    xlog.warn('gae_handler.handler %r %s , retry...', e, url):
+                    xlog.warn('gae_handler.handler %r %s , retry...', e, url)
-                    length, response.ssl_sock.handshake_time, response.status, url)
+                        response.ssl_sock.fd, response.ssl_sock.ip, response.ssl_sock.received_size, (time_finished-time_request)*1000,
-                        xlog.warning('RangeFetch "%s %s" return Content-Range=%r: response headers=%r, retry %s-%s', self.method, self.url, content_range, response.getheaders(), start, end)
+                        xlog.warning('RangeFetch "%s %s" return Content-Range=%r: response headers=%r, retry %s-%s',
-                PRINT("any case?")
+                PRINT("any case? good:%s-%s bad:%s-%s" % (ip_utils.ip_num_to_string(good_begin), ip_utils.ip_num_to_string(good_end), 
-    searching_thread_count = 0
+    trafic_control = 0
-    to_remove_ip_list = Queue.Queue()
+    
-    ip_dict = {} # ip_str => {
+    def reset(self):
-    gws_ip_pointer_reset_time = 0
+        # gererate from ip_dict, sort by handshake_time, when get_batch_ip
-    def __init__(self):
+            self.max_scan_ip_thread_num = 0
-        self.max_scan_ip_thread_num = config.CONFIG.getint("google_ip", "max_scan_ip_thread_num") #50
+        
-        self.last_sort_time_for_gws = time.time()
+        self.last_sort_time_for_gws = time.time()
-
+                time_now = time.time()
-                    if time.time() - self.gws_ip_pointer_reset_time < 1:
+                    if time_now - self.gws_ip_pointer_reset_time < 1:
-                elif self.gws_ip_pointer > 0 and time.time() - self.gws_ip_pointer_reset_time > 3:
+                        self.gws_ip_pointer_reset_time = time_now
-                    self.gws_ip_pointer_reset_time = time.time()
+                    self.gws_ip_pointer_reset_time = time_now
-                if time.time() - get_time < self.ip_connect_interval:
+                if time_now - get_time < self.ip_connect_interval:
-                if time.time() - self.ip_dict[ip_str]['success_time'] > 300: # 5 min
+                if time_now - self.ip_dict[ip_str]['success_time'] > 300: # 5 min
-                if time.time() - fail_time < fail_connect_interval:
+                if time_now - fail_time < fail_connect_interval:
-                if transfered_data > config.ip_traffic_quota_base:
+                if self.trafic_control: # not check now
-                self.ip_dict[ip_str]['get_time'] = time.time()
+                self.append_ip_history(ip_str, "get")
-                self.ip_dict[ip_str]['history'].append([time.time(), "get"])
+                self.append_ip_history(ip_str, "get")
-                self.ip_dict[ip_str]['history'].append([time.time(), handshake_time])
+                self.append_ip_history(ip_str, handshake_time)
-                                    "success_time":0, "get_time":0}
+                                    "success_time":0, "get_time":0, "links":0}
-                self.ip_dict[ip_str]['success_time'] = time.time()
+                self.ip_dict[ip_str]['success_time'] = time_now
-                self.ip_dict[ip_str]['history'].append([time.time(), handshake_time])
+                self.append_ip_history(ip_str, handshake_time)
-        if bytes == 0:
+        if bytes == 0 or self.trafic_control == 0:
-                self.ip_dict[ip_str]['history'].append([time.time(), "%d_B" % bytes])
+                self.append_ip_history(ip_str, "%d_B" % bytes)
-            if not force_remove and time.time() - fail_time < 1:
+            if not force_remove and time_now - fail_time < 1:
-            self.ip_dict[ip_str]["fail_time"] = time.time()
+            self.append_ip_history(ip_str, "fail")
-                xlog.info("remove ip:%s left amount:%d gws_num:%d", ip_str, len(self.ip_dict), len(self.gws_ip_list))
+                    xlog.info("remove ip tmp:%s left amount:%d gws_num:%d", ip_str, len(self.ip_dict), len(self.gws_ip_list))
-                xlog.info("remove_slowest_ip:%s handshake_time:%d", ip_str, handshake_time)
+                xlog.info("remove_slowest_ip:%s handshake_time:%d, fails:%d", ip_str, handshake_time, fails)
-                xlog.exception("google_ip.runJob fail:%s", e)
+                xlog.exception("google_ip.runJob fail:%r", e)
-            file_size = 1024 * 1024 * 1024
+            if "size" in reqs:
-            data = ''.join(chr(ord('a')+i) for i in xrange(65535))
+            data = self.generate_random_lowercase(65535)
-    main(data_path=data_path)
+        
-        res_arr = {"gws_ip_num": gws_ip_num,
+        res_arr = {"gws_ip_num": "%d,%d" % (len(google_ip.gws_ip_list), google_ip.good_ip_num),
-                   "ip_handshake_100":google_ip.ip_handshake_th(100),
+                   "ip_handshake_10":google_ip.ip_handshake_th(10),
-                connect_manager.load_sock()
+                connect_manager.load_proxy_config()
-                check_ip.load_sock()
+                google_ip.reset()
-        data += "N \t IP      \t\t Han \t Fail \t Trans \t Tran_t \t his\r\n"
+        data = "<html><body><div  style='float: left; white-space:nowrap;font-family: monospace;'>"
-            transfered_data = google_ip.ip_dict[ip]["transfered_data"]
+            links = google_ip.ip_dict[ip]["links"]
-                data_active = time_now - data_active
+                active_time = time_now - data_active
-                    (i, ip, handshake_time, fail_times, transfered_data, data_active, str)
+            data += "<tr><td>%d</td><td>%s</td><td>%d</td><td>%d</td><td>%d</td><td>%d</td><td>%d</td><td>%d</td>" \
-        mimetype = 'text/plain'
+        data += "</table></div></body></html>"
-        data = https_manager.gae_conn_pool.to_string()
+        data = "New conn:\n"
-                launcher_log.warn("web control ref:%s refuse", netloc)
+        refer = self.headers.getheader('Referer')
-                    response.close()
+                try:
-                speed = body_length / (time_finished - time_response)
+                if body_length > 1024 and time_finished - time_response > 0:
-                    length, response.ssl_sock.handshake_time, int(speed), response.status, url)
+
-
+    else:
-import threading
+
-    test('1.255.22.210', 1)
+    result = test_gae('64.15.119.69')
-
+        call_time = time.time()
-            self.save_ssl_connection_for_reuse(sock)
+            self.save_ssl_connection_for_reuse(sock, call_time=call_time)
-                    #connect_control.fall_into_honeypot()
+                    google_ip.report_connect_fail(ip, force_remove=True)
-                xlog.info("GAE %d|%s|%d t:%d s:%d %d %s", response.ssl_sock.fd, response.ssl_sock.ip, response.ssl_sock.received_size, (time.time()-time_request)*1000, length, response.status, url)
+                xlog.info("GAE %d|%s|%d t:%d s:%d hs:%d Spd:%d %d %s", 
-# # Comment (#åé¢ä¸ºæ³¨éï¼
+# Support format:
-# æ¯ä¸ªèå´å¯ä»¥ç¨ éå·(,) åç«çº¿(|) æåè¡è¿è¡åå²
+# range can seperate by (,) or(|) or new line
-"""
+# Single rang format: ï¼
-            elif good_begin >= bad_begin and good_end >= bad_end:
+            elif good_begin >= bad_begin and good_end == bad_end:
-                #     [    bad  ]
+                #     [      bad       ]
-                PRINT("cut bad ip case 3:%s - %s" % (ip_utils.ip_num_to_string(bad_begin), ip_utils.ip_num_to_string(bad_end)))
+                PRINT("cut bad ip case 4:%s - %s" % (ip_utils.ip_num_to_string(bad_begin), ip_utils.ip_num_to_string(bad_end)))
-                PRINT("cut bad ip case 4:%s - %s" % (ip_utils.ip_num_to_string(good_begin), ip_utils.ip_num_to_string(good_end)))
+                PRINT("cut bad ip case 5:%s - %s" % (ip_utils.ip_num_to_string(good_begin), ip_utils.ip_num_to_string(good_end)))
-    fd = open("ip_range.txt", "w")
+    output_file = os.path.join(config.DATA_PATH, file_name)
-    fd = open("ip_range.txt", "r")
+    file_name = os.path.join(config.DATA_PATH, "ip_range.txt")
-            ip_dict = sorted(self.ip_dict.items(),  key=lambda x: x[1]['handshake_time'])
+            ip_dict = sorted(self.ip_dict.items(),  key=lambda x: (x[1]['handshake_time'] + x[1]['fail_times'] * 1000))
-                ip_rate[ip_str] = self.ip_dict[ip_str]['handshake_time']
+                ip_rate[ip_str] = self.ip_dict[ip_str]['handshake_time'] + (self.ip_dict[ip_str]['fail_times'] * 1000)
-    google_ip.save_ip_list(force=True)
+    test()
-    request_data = 'HEAD /_gh/ HTTP/1.1\r\nHost: %s.appspot.com\r\n\r\n' % appid
+    request_data = 'GET / HTTP/1.1\r\nHost: %s.appspot.com\r\n\r\n' % appid
-
+        content = response.read()
-    test('216.239.38.125', 1)
+    test('1.255.22.210', 1)
-            if host.endswith(".google.com") or host.endswith(config.HOSTS_FWD_ENDSWITH) or host.endswith(config.HOSTS_GAE_ENDSWITH):
+            if host.endswith(".google.com") or host.endswith(config.HOSTS_DIRECT_ENDSWITH) or host.endswith(config.HOSTS_GAE_ENDSWITH):
-                    fwd_set = [s for s in config.HOSTS_FWD]
+                if host not in config.HOSTS_DIRECT:
-                xlog.warn("Method %s not support in GAE, Redirect to FWD for %s", self.command, self.path)
+                    config.HOSTS_DIRECT = tuple(fwd_set)
-            raise e
+            ip = sock.ip
-                    connect_control.fall_into_honeypot()
+                    #google_ip.report_bad_ip(ssl_sock.ip)
-
+                 # 'fail_times' => N   continue timeout num, if connect success, reset to 0
-    bad_ip_pool = set()
+        if config.USE_IPV6:
-                if len(str_l) != 4:
+                if len(str_l) < 4:
-                self.add_ip(ip_str, handshake_time, domain, server)
+                self.add_ip(ip_str, handshake_time, domain, server, fail_times)
-                        xlog.exception("parse bad_ip.txt err:%r", e)
+        self.try_sort_gws_ip(force=True)
-                    fd.write("%s\n" % (ip))
+                    fd.write( "%s %s %s %d %d\n" %
-    def try_sort_ip_by_handshake_time(self, force=False):
+    def try_sort_gws_ip(self, force=False):
-            ip_dict_handshake_time = {}
+            ip_rate = {}
-                ip_dict_handshake_time[ip_str] = self.ip_dict[ip_str]['handshake_time']
+                ip_rate[ip_str] = self.ip_dict[ip_str]['handshake_time']
-            self.gws_ip_list = [ip_str for ip_str,handshake_time in ip_time]
+            ip_time = sorted(ip_rate.items(), key=operator.itemgetter(1))
-        self.try_sort_ip_by_handshake_time()
+        self.try_sort_gws_ip()
-                    continue
+                time_now = time.time()
-        self.try_sort_ip_by_handshake_time()
+        self.try_sort_gws_ip()
-    def add_ip(self, ip_str, handshake_time, domain=None, server=''):
+    def add_ip(self, ip_str, handshake_time, domain=None, server='', fail_times=0):
-                self.ip_dict[ip_str]['timeout'] = 0
+                self.ip_dict[ip_str]['fail_times'] = fail_times
-                                    'timeout':0, "history":[[time.time(), handshake_time]], "fail_time":0,
+            self.ip_dict[ip_str] = {'handshake_time':handshake_time, "fail_times":fail_times,
-            xlog.error("set_ip err:%s", e)
+            xlog.exception("add_ip err:%s", e)
-                self.ip_dict[ip_str]['timeout'] = 0
+                self.ip_dict[ip_str]['success_time'] = time.time()
-        if not ip_utils.check_ip_valid(ip_str):
+    def is_traffic_quota_allow(self, ip_str):
-        return False
+        self.ip_lock.acquire()
-            self.ip_dict[ip_str]['timeout'] += 1
+            self.ip_dict[ip_str]['fail_times'] += 1
-            if force_remove or self.ip_dict[ip_str]['timeout'] >= 50:
+            if force_remove or self.ip_dict[ip_str]['fail_times'] >= 50:
-        self.try_sort_ip_by_handshake_time(force=True)
+        self.try_sort_gws_ip(force=True)
-
+import connect_manager
-            timeout = google_ip.ip_dict[ip]["timeout"]
+            fail_times = google_ip.ip_dict[ip]["fail_times"]
-            data += "%d \t %s      \t %d \t %d \t %s\r\n" % (i, ip, handshake_time, timeout, str)
+            data += "%d \t %s      \t %d \t %d \t %d \t %d \t %s\r\n" % \
-        #logging.error("APPID_manager, report_not_exist %s", appid)
+        xlog.warn("APPID_manager, report_not_exist %s", appid)
-            p = Popen(["xset", "-q"], stdout=PIPE, stderr=PIPE, shell=True)
+            p = Popen(["xset", "-q"], stdout=PIPE, stderr=PIPE)
-                if "gws" not in server_type:
+                if "gws" not in server_type and "Google Frontend" not in server_type:
-        body_length = end - start
+        body_length = end - start + 1
-                response.ssl_sock.received_size += body_length
+import update
-download_progress = {} # link => {"size", 'downloaded', status:downloading|canceled|finished}
+download_progress = {} # link => {"size", 'downloaded', status:downloading|canceled|finished:failed}
-    return opener
+def get_opener(retry=0):
-        return False
+    for i in range(0, 2):
-                https_manager.save_ssl_connection_for_reuse(response.ssl_sock, host)
+                https_manager.save_ssl_connection_for_reuse(response.ssl_sock, host, call_time=time_request)
-                xlog.info("GAE t:%d s:%d %d %s", (time.time()-time_request)*1000, length, response.status, url)
+                https_manager.save_ssl_connection_for_reuse(response.ssl_sock, call_time=time_request)
-        self.keep_alive = config.CONFIG.getint("connect_manager", "https_keep_alive") #1
+        self.max_thread_num = config.CONFIG.getint("connect_manager", "https_max_connect_thread")
-        ssl_sock.last_use_time = time.time()
+    def save_ssl_connection_for_reuse(self, ssl_sock, host=None, call_time=0):
-# check it.
+import ip_utils
-4.3.2.0/24
+# This code functions:
-8.8.8.0/24
+8.8.4.0-8.8.4.255
-62.197.198.193-62.197.198.251
+41.254.36.0-41.254.36.255
-64.15.119.0-64.15.126.255
+63.243.168.0-63.243.168.255
-66.102.0.0-66.102.15.255
+65.39.199.0-65.39.199.255
-81.175.29.128-81.175.29.191
+74.207.242.0-74.207.242.255
-106.162.216.20-106.162.216.123
+86.51.24.0-86.51.24.255
-114.4.41.0/24
+108.177.0.0-108.177.255.255
-139.175.107.88/24
+119.15.80.0-119.15.80.255
-178.60.128.1-178.60.128.
+173.237.115.0-173.237.115.255
-193.120.166.64-193.120.166.127
+192.193.133.0-192.193.133.255
-195.249.20.192-195.249.20.255
+195.12.177.0-195.12.177.255
-199.87.241.32-199.87.241.63
+198.142.187.0-198.142.187.255
-203.165.14.210-203.165.14.251
+200.29.113.0-200.29.113.255
-203.211.0.20-203.211.0.59
+203.210.2.0-203.210.2.255
-208.117.240.0-208.117.255.255
+208.54.64.0-208.54.64.255
-210.245.14.0/24
+210.2.185.0-210.2.185.255
-216.58.208.0/20
+212.188.49.0-212.188.49.255
-223.196.82.0/24
+218.189.25.0-218.189.25.255
-def merge_ip_range():
+def print_range_list(ip_range_list):
-            continue
+    ip_lines_list = re.split("\r|\n", input_lines)
-        ips = re.split(",|\|", iplines)
+        ips = re.split(",|\|", context_line)
-            if len(line) == 0 or line[0] == '#':
+            if len(line) == 0:
-    range_num = len(ip_range_list)
+    return ip_range_list
-        ip_range = ip_range_list[i]
+def merge_range(input_ip_range_list):
-            ip_range_list_2.append([last_begin, last_end])
+            output_ip_range_list.append([last_begin, last_end])
-        print ip_utils.ip_num_to_string(begin), ip_utils.ip_num_to_string(end)
+    output_ip_range_list.append([last_begin, last_end])
-    for ip_range in ip_range_list_2:
+    for ip_range in ip_range_list:
-
+    print("Begin test load ip_range.txt\n")
-
+    print "amount ip:", amount
-
+import SocketServer
-#import OpenSSL
+import select
-            #sock = self.connection
+        self.running = True
-            self.process_connect(sock, address)
+        fdset = [self.socket, ]
-import local.proxy as client
+import local.proxy as client
-import BaseHTTPServer
+import simple_http_server
-class PACServerHandler(BaseHTTPServer.BaseHTTPRequestHandler):
+class PACServerHandler(simple_http_server.HttpServerHandler):
-import SocketServer
+import simple_http_server
-    proxy_daemon = LocalProxyServer((config.LISTEN_IP, config.LISTEN_PORT), proxy_handler.GAEProxyHandler)
+    proxy_daemon = simple_http_server.HTTPServer((config.LISTEN_IP, config.LISTEN_PORT), proxy_handler.GAEProxyHandler)
-        pac_daemon = LocalProxyServer((config.PAC_IP, config.PAC_PORT), pac_server.PACServerHandler)
+        pac_daemon = simple_http_server.HTTPServer((config.PAC_IP, config.PAC_PORT), pac_server.PACServerHandler)
-
+import simple_http_server
-    #protocol_version = 'HTTP/1.1'
+
-
+#from gevent import monkey
-#=============================================
+# =============================================
-#=============================================
+# =============================================
-#==============================================
+# ==============================================
-scan_sleep_time = 600 # Need examination
+block_delay = 5
-#=============================================
+# =============================================
-    ready = True #checked by launcher.module_init
+    ready = True  # checked by launcher.module_init
-    xlog.info("Finished Exiting gae_proxy module...")
+    ready = False  # checked by launcher.module_init
-import os, sys
+import os
-    main()
+    main()
-        if not module in config.config["modules"]:
+        if module not in config.config["modules"]:
-            p = threading.Thread(target = _start.main)
+            p = threading.Thread(target=_start.main)
-
+
-        if not module in proc_handler:
+        if module not in proc_handler:
-            #web_control.confirm_module_ready(config.get(["modules", module, "control_port"], 0))
+            # web_control.confirm_module_ready(config.get(["modules", module, "control_port"], 0))
-                
+
-            data = '{ "check_update": "%s", "popup_webui": %d, "show_systray": %d, "auto_start": %d, "php_enable": %d, "gae_proxy_enable": %d }' %\
+            data = '{ "check_update": "%s", "popup_webui": %d, "allow_remote_connect": %d, "show_systray": %d, "auto_start": %d, "php_enable": %d, "gae_proxy_enable": %d }' %\
-            elif 'popup_webui' in reqs :
+            elif 'popup_webui' in reqs:
-            elif 'show_systray' in reqs :
+
-            elif 'auto_start' in reqs :
+
-        host_port = config.config["modules"]["launcher"].get("control_port", 8085)
+        host_port = config.get(["modules", "launcher", "control_port"], 8085)
-
+import config
-        webbrowser.open_new("http://127.0.0.1:8085/")
+        host_port = config.get(["modules", "launcher", "control_port"], 8085)
-        webbrowser.open_new("http://127.0.0.1:8085/")
+        host_port = config.get(["modules", "launcher", "control_port"], 8085)
-        if http_request("http://127.0.0.1:8085/quit") == False:
+        host_port = config.get(["modules", "launcher", "control_port"], 8085)
-        webbrowser.open("http://127.0.0.1:8085/")
+        host_port = config.config["modules"]["launcher"].get("control_port", 8085)
-    server = LocalServer(("127.0.0.1", 8085), Http_Handler)
+    allow_remote = config.get(["modules", "launcher", "allow_remote_connect"], 0)
-    # suppose xxnet is running, try to close it
+    """suppose xxnet is running, try to close it
-            launcher_log.debug("good, xxnet:8085 clear!")
+        # web_control(default port:8085)
-            launcher_log.debug("<%d>: try to terminate xxnet:8085" % i)
+            launcher_log.debug("<%d>: try to terminate xxnet:%s" % (i, host_port))
-        webbrowser.open("http://127.0.0.1:8085/")
+        host_port = config.get(["modules", "launcher", "control_port"], 8085)
-    server = LocalServer(("0.0.0.0", 8085), Http_Handler)
+    # should use config.yaml to bing ip
-            return True
+            launcher_log.debug("good, xxnet:8087 cleared!")
-            return True
+            launcher_log.debug("good, xxnet:8085 clear!")
-    return False
+    return is_xxnet_exit
-                   "sys_platform":sys.platform,
+                   "sys_platform":"%s, %s" % (platform.machine(), platform.platform()),
-        info += 'PHP proxy Version    : %s (python/%s gevent/%s pyopenssl/%s)\n' % (__version__, sys.version[:5], gevent.__version__, OpenSSL.__version__)
+        info += 'PHP Proxy Version    : %s (python/%s gevent/%s pyopenssl/%s)\n' % (__version__, platform.python_version(), gevent.__version__, OpenSSL.__version__)
-        sys.exit()
+        sys.exit()
-network_ok = False
+network_ok = True
-        conn = httplib.HTTPSConnection("code.jquery.com", 443, timeout=30)
+        conn = httplib.HTTPSConnection("github.com", 443, timeout=30)
-    check.result.server_type = result
+    check.result.server_type = "gws"
-    test('118.98.36.203', 1)
+    test('216.239.38.125', 1)
-from config import config
+from config import config
-def check_win10():
+def get_connect_interval():
-        return False
+        return 0
-        return True
+    win_version = env_info.win32_version()
-    return False
+    return 0
-is_win10 = check_win10() or True
+connect_interval = get_connect_interval()
-    if not is_win10:
+    if not connect_interval:
-            wait_time = config.connect_interval/1000.0 - last_connect_interval
+        if last_connect_interval < connect_interval/1000.0:
-    if not is_win10:
+    if not connect_interval:
-    return time.time() - last_request_time
+    t = time.time() - last_request_time
-block_delay = 10 # (60 * 5)
+block_delay = 5 #
-    #connect_allow_time = time.time() + block_delay
+    connect_allow_time = time.time() + block_delay
-
+            p.daemon = True
-        self.max_thread_num = config.https_max_connect_thread #10
+        self.max_thread_num = config.CONFIG.getint("connect_manager", "https_max_connect_thread") #10
-            self.create_more_connection()
+            #self.create_more_connection()
-                    ssl_sock.close()
+                    self.start_keep_alive(ssl_sock)
-                self.create_more_connection()
+            #self.create_more_connection()
-                    ssl_sock.close()
+                ssl_sock.close()
-    def create_more_connection(self):
+    def create_more_connection_worker(self):
-        for i in range(0, target_thread_num):
+        while self.thread_num < target_thread_num and self.new_conn_pool.qsize() < self.connection_pool_min_num:
-                time.sleep(10)
+                time.sleep(5)
-                break
+                continue
-            p = threading.Thread(target=self.create_connection_worker)
+            p = threading.Thread(target=self.connect_process)
-            time.sleep(1)
+            time.sleep(10)
-                    raise socket.error(' certificate is none')
+                    raise socket.error(' certficate is none')
-                    raise socket.error(' certificate is issued by %r, not Google' % ( issuer_commonname))
+                    connect_control.fall_into_honeypot()
-            xlog.debug("create_ssl %s fail:%s cost:%d h:%d", ip, e, time_cost * 1000, handshake_time)
+            if time_cost < self.timeout - 1:
-    def create_connection_worker(self):
+    def connect_thread(self, sleep_time=0):
-            while connect_control.keep_running:
+            while self.new_conn_pool.qsize() < self.connection_pool_min_num:
-                    time.sleep(10)
+                    break
-                    if time.time() - ssl_sock.last_use_time < self.keep_alive: # gws ssl connection can keep for 60s after created
+                    if time.time() - ssl_sock.last_use_time < self.keep_alive+1: # gws ssl connection can keep for 230s after created
-                if time.time() - ssl_sock.last_use_time < self.keep_alive: # gws ssl connection can keep for 60s after created
+                if time.time() - ssl_sock.last_use_time < self.keep_alive+1: # gws ssl connection can keep for 230s after created
-            connect_control.start_connect_register(True)
+            connect_control.start_connect_register(high_prior=True)
-                connect_control.end_connect_register(True)
+                connect_control.end_connect_register(high_prior=True)
-                        xlog.warn("IP:%s not support GAE, server type:%s", response.ssl_sock.ip, server_type)
+                        xlog.warn("IP:%s not support GAE, server type:%s status:%d", response.ssl_sock.ip, server_type, response.status)
-__author__ = 'debian'
+import platform
-    def add_ip(self, ip_str, handshake_time, domain=None, server=None):
+    def add_ip(self, ip_str, handshake_time, domain=None, server=''):
-                result = check_ip.test_gws(ip_str)
+                result = check_ip.test_gae(ip_str)
-                connect_control.fall_into_honeypot()
+
-
+import platform
-
+import env_info
-        ctypes.windll.kernel32.SetConsoleTitleW(u'GoAgent v%s' % config.__version__)
+        ctypes.windll.kernel32.SetConsoleTitleW(u'GoAgent ')
-    xlog.info(config.info())
+    log_info()
-import BaseHTTPServer
+import env_info
-                   "os_detail":os_detail(),
+                   "os_detail":env_info.os_detail(),
-                   "python_version": config.python_version,
+                   "python_version": platform.python_version(),
-
+    launcher_log.info("start XX-Net %s", update_from_github.current_version())
-                else:
+                    continue
-            need_conn_num = self.connection_pool_min_num - self.new_conn_pool.qsize()
+    def create_more_connection(self):
-            p = threading.Thread(target = self.create_connection_worker, args=(type,))
+            p = threading.Thread(target=self.create_connection_worker)
-                    connect_control.fall_into_honeypot()
+                    #connect_control.fall_into_honeypot()
-    def create_connection_worker(self, type="gae"):
+    def create_connection_worker(self):
-                        break
+                if self.new_conn_pool.qsize() >= self.connection_pool_min_num:
-            self.create_more_connection(type="gae")
+        self.create_more_connection()
-            p = Popen(["xset", "-q"], stdout=PIPE, stderr=PIPE)
+            p = Popen(["xset", "-q"], stdout=PIPE, stderr=PIPE, shell=True)
-    test('216.58.208.112', 1)
+    test('118.98.36.203', 1)
-def is_active(timeout=600):
+def is_active(timeout=60 * 30):
-        self.timeout = 2
+        self.timeout = 4
-                break
+                xlog.warn("create more connect, control not allow")
-                    break
+                    xlog.debug("create_connection_worker, control not allow")
-                if response.status == 404 or response.status == 400:
+                if response.status > 400:
-                continue
+                server_type = response.getheader('Server', "")
-                handshake_time = self.ip_dict[ip_str]["handshake_time"]
+
-                if time.time() - fail_time < 300:
+                if time.time() - fail_time < fail_connect_interval:
-                                    "get_time":0}
+                                    "success_time":0, "get_time":0}
-                self.close_connection = 1
+                xlog.warn("read request line empty")
-            launcher_log.error("url in downloading, %s", url)
+            launcher_log.warn("url in downloading, %s", url)
-
+# remember to remove the following ip range:
-                if response.status == 404:
+                if response.status == 404 or response.status == 400:
-    test('203.210.2.25', 1)
+    test('216.58.208.112', 1)
-is_win10 = check_win10()
+is_win10 = check_win10() or True
-            wait_time = min_connect_interval - last_connect_interval
+        if last_connect_interval < config.connect_interval/1000.0:
-def is_active(timeout=60):
+def is_active(timeout=600):
-    global high_prior_connecting_num, low_prior_connecting_num
+    global high_prior_connecting_num, low_prior_connecting_num, last_connect_time
-    test_alive()
+    #test_alive()
-    #test("74.125.216.36", 10) #gvs
+    #test_gws("216.58.196.176") #gvs
-    #test('208.117.224.213', 10)
+    test('203.210.2.25', 1)
-
+from google_ip import google_ip
-
+import threading
-scan_allow_time = 0
+#=============================================
-scan_sleep_time = 600 # Need examination
+#==============================================
-
+#=============================================
-        self.max_thread_num = config.CONFIG.getint("connect_manager", "https_max_connect_thread") #10
+        self.max_thread_num = config.https_max_connect_thread #10
-        except OpenSSL.SysCallError as e:
+        except OpenSSL.SSL.SysCallError as e:
-        while self.searching_thread_count < self.scan_ip_thread_num:
+        if config.USE_IPV6:
-        self.ip_connect_interval = ""
+        self.ip_connect_interval = 10
-
+
-                self.USER_CONFIG.read(CONFIG_USER_FILENAME)
+            if os.path.isfile(DEFAULT_CONFIG_FILENAME):
-                self.DEFAULT_CONFIG.read(DEFAULT_CONFIG_FILENAME)
+            if os.path.isfile(CONFIG_USER_FILENAME):
-        data = json.dumps(res_arr)
+        data = json.dumps(res_arr, indent=0, sort_keys=True)
-        ca.add_extensions([v3])
+        ca.set_version(2)
-        ca.add_extensions([v3])
+        v3 = OpenSSL.crypto.X509Extension('basicConstraints', False, 'CA:TRUE')
-        self.keep_run = True
+# change to False when exit: system tray exit menu, or Ctrl+C in console
-            slowest_time = 0
+            slowest_handshake_time = 0
-                    slowest_time = time
+                handshake_time = self.pool[sock]
-            return (slowest_time, slowest_sock)
+            return (slowest_handshake_time, slowest_sock)
-                        return_list.append(sock)
+                    return_list.append(sock)
-                str += "%d \t %s handshake:%d create:%d\r\n" % (i, sock.ip, t, time.time() -sock.last_use_time)
+                str += "%d \t %s handshake:%d not_active_time:%d\r\n" % (i, sock.ip, t, time.time() -sock.last_use_time)
-            self.keep_alive = 0
+            #self.keep_alive = 0
-                        continue
+        while self.keep_alive and connect_control.keep_running:
-                        ssl_sock.close()
+            new_list = self.new_conn_pool.get_need_keep_alive(maxtime=self.keep_alive-3)
-                xlog.warn("keep alive except:%r", e)
+            for ssl_sock in to_keep_live_list:
-        ssl_sock.last_use_time = time.time()
+
-        self.gae_conn_pool.put( (ssl_sock.handshake_time, ssl_sock) )
+        else:
-            t, ssl_sock = self.gae_conn_pool.get_slowest()
+            while self.gae_conn_pool.qsize() > self.connection_pool_max_num:
-                ssl_sock.close()
+                if handshake_time < self.keep_alive:
-        #need_conn_num = self.connection_pool_min_num - self.new_conn_pool.qsize()
+    def create_more_connection(self, type="gae"):
-        while self.thread_num < self.max_thread_num and self.new_conn_pool.qsize() < self.connection_pool_min_num:
+        target_thread_num = min(self.max_thread_num, need_conn_num)
-            p.daemon = True
+            p = threading.Thread(target = self.create_connection_worker, args=(type,))
-
+            #time.sleep(0.3)
-    def connect_thread(self):
+    def create_connection_worker(self, type="gae"):
-            while self.new_conn_pool.qsize() < self.connection_pool_min_num:
+            while connect_control.keep_running:
-    def create_ssl_connection(self, host=''):
+    def get_ssl_connection(self, host=''):
-                    if time.time() - ssl_sock.last_use_time < self.keep_alive+1: # gws ssl connection can keep for 230s after created
+                    if time.time() - ssl_sock.last_use_time < self.keep_alive: # gws ssl connection can keep for 60s after created
-                if time.time() - ssl_sock.last_use_time < self.keep_alive+1: # gws ssl connection can keep for 230s after created
+                if time.time() - ssl_sock.last_use_time < self.keep_alive: # gws ssl connection can keep for 60s after created
-        self.create_more_connection()
+        if host:
-    ssl_sock = https_manager.create_ssl_connection(host)
+    ssl_sock = https_manager.get_ssl_connection(host)
-            ssl_sock = https_manager.create_ssl_connection()
+            ssl_sock = https_manager.get_ssl_connection()
-        if handshake_time < 5: # this is impossible
+        if handshake_time < 5: # that's impossible
-            if force_remove or self.ip_dict[ip_str]['timeout'] >= 5:
+            if force_remove or self.ip_dict[ip_str]['timeout'] >= 50:
-            while True:
+            while connect_control.keep_running:
-        while self.searching_thread_count <= self.scan_ip_thread_num:
+        while self.searching_thread_count <= self.scan_ip_thread_num and connect_control.keep_running:
-import web_control
+import connect_control
-    ready = True
+    ready = True #checked by launcher.module_init
-    while config.keep_run:
+    while connect_control.keep_running:
-    ready = False
+    ready = False #checked by launcher.module_init
-from connect_control import connect_allow_time, connect_fail_time
+from connect_control import connect_allow_time, connect_fail_time, touch_active
-            config.keep_run = False
+            connect_control.keep_running = False
-            _start.client.config.keep_run = False
+            _start.client.terminate()
-    php_enable = '0'
+    php_enable = '1'
-         win32_proxy_manager.disable_proxy()
+        win32_proxy_manager.disable_proxy()
-from update import get_opener
+def get_opener():
-    python_version = sys.version[:5]
+    python_version = platform.python_version()
-        info += 'GAEProxy Version    : %s (python/%s )\n' % (self.__version__, sys.version[:5])
+        info += 'GAEProxy Version   : %s (python/%s)\n' % (self.__version__, self.python_version)
-    test()
+    test()
-        pass
+         win32_proxy_manager.disable_proxy()
-                pass
+                self.user_special.scan_ip_thread_num = self.DEFAULT_CONFIG.getint('google_ip', 'max_scan_ip_thread_num')
-        blackhole = '%s:%s' % (listen_ip, config.PAC_PORT)
+        autoproxy = gae_proxy_listen
-        host, _, port = self.path.rpartition(':')
+        host = self.headers.getheader('Host')
-        return p.returncode == 0
+        try:
-    if config.get(["modules", "launcher", "popup_webui"], 1) == 1:
+    if has_desktop and config.get(["modules", "launcher", "popup_webui"], 1) == 1:
-
+import win32_proxy_manager
-        self.set_register(self.INTERNET_SETTINGS, 'ProxyServer', 1, '127.0.0.1:8087')
+        win32_proxy_manager.set_proxy_server("127.0.0.1", 8087)
-        self.set_register(self.INTERNET_SETTINGS, 'AutoConfigURL', 1, "http://127.0.0.1:8086/proxy.pac")
+        win32_proxy_manager.set_proxy_auto("http://127.0.0.1:8086/proxy.pac")
-        self.set_register(self.INTERNET_SETTINGS, 'AutoConfigURL', 1, "") # disable auto proxy
+        win32_proxy_manager.disable_proxy()
-        info += 'GAEProxy Version   : %s (python/%s )\n' % (self.__version__, self.python_version)
+        info += 'GAEProxy Version   : %s (python/%s)\n' % (self.__version__, self.python_version)
-    python_version = sys.version[:5]
+    python_version = platform.python_version()
-        info += 'GAEProxy Version    : %s (python/%s )\n' % (self.__version__, sys.version[:5])
+        info += 'GAEProxy Version   : %s (python/%s )\n' % (self.__version__, self.python_version)
-    test()
+    test()
-        set(["modules", "gae_proxy", "control_port"], 8084)
+    #if get(["modules", "gae_proxy", "control_port"], 0) == 0:
-                    self.ip_dict[ip_str]['handshake_time'] = org_time + 100
+                if handshake_time - org_time > 500:
-            self.ip_dict[ip_str]['handshake_time'] += 200
+            self.ip_dict[ip_str]['handshake_time'] += 300
-            if force_remove or self.ip_dict[ip_str]['timeout'] >= 50:
+            if force_remove or self.ip_dict[ip_str]['timeout'] >= 5:
-        self.timeout = 4
+        self.timeout = 2
-        need_conn_num = self.connection_pool_min_num - self.new_conn_pool.qsize()
+        #need_conn_num = self.connection_pool_min_num - self.new_conn_pool.qsize()
-        while self.thread_num < target_thread_num and self.new_conn_pool.qsize() < self.connection_pool_min_num:
+        #target_thread_num = min(self.max_thread_num, need_conn_num)
-                    raise socket.error(' certficate is issued by %r, not Google' % ( issuer_commonname))
+                    raise socket.error(' certificate is issued by %r, not Google' % ( issuer_commonname))
-                    raise socket.error(' certficate is none')
+                    raise socket.error(' certificate is none')
-    protocol_version = 'HTTP/1.1'
+    #protocol_version = 'HTTP/1.1'
-        import_command = 'security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ../../../data/php_proxy/CA.crt'# % certfile.decode('utf-8')
+        import_command = 'security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ../../data/php_proxy/CA.crt'# % certfile.decode('utf-8')
-        icon_path = os.path.join(current_path, "web_ui", "favicon.ico")
+        icon_path = os.path.join(current_path, "web_ui", "favicon_MAC.ico")
-            proc_handler[module]["imp"] = __import__(module, globals(), locals(), ['local', 'start'], -1)
+        if os.path.isfile(os.path.join(root_path, module, "__init__.py")):
-        proc_handler[module]["proc"] = p
+            _start = proc_handler[module]["imp"].start
-            time.sleep(0.1)
+        if os.path.isfile(os.path.join(root_path, module, "__init__.py")):
-import logging
+import xlog
-            logging.error("No usable appid left, add new appid to continue use GAEProxy")
+            xlog.error("No usable appid left, add new appid to continue use GAEProxy")
-import logging
+import xlog
-        logging.info("create ca")
+        xlog.info("create ca")
-        logging.info("generate CA file:%s", CertUtil.ca_keyfile)
+        xlog.info("generate CA file:%s", CertUtil.ca_keyfile)
-            logging.warning('CertUtil.remove_windows_ca failed: %r', e)
+            xlog.warning('CertUtil.remove_windows_ca failed: %r', e)
-            logging.warning('please install *libnss3-tools* package to import GoAgent root ca')
+            xlog.warning('please install *libnss3-tools* package to import GoAgent root ca')
-            logging.warning('please install *libnss3-tools* package to import GoAgent root ca')
+            xlog.warning('please install *libnss3-tools* package to import GoAgent root ca')
-            logging.info("system cert exist")
+            xlog.info("system cert exist")
-                logging.warning('install root certificate failed, Please run as administrator/root/sudo')
+                xlog.warning('install root certificate failed, Please run as administrator/root/sudo')
-            logging.info("GoAgent CA exist")
+            xlog.info("GoAgent CA exist")
-        logging.info("try auto import CA command:%s", cmd)
+        xlog.info("try auto import CA command:%s", cmd)
-            logging.info("no CA file exist")
+            xlog.info("no CA file exist")
-            logging.info("clean old site certs")
+            xlog.info("clean old site certs")
-    import logging
+    import xlog
-    class logging():
+    class xlog():
-            logging.error("proxy type %s unknown, disable proxy", config.PROXY_TYPE)
+            xlog.error("proxy type %s unknown, disable proxy", config.PROXY_TYPE)
-    logging.debug("conn: %d  handshake:%d", connct_time, handshake_time)
+    xlog.debug("conn: %d  handshake:%d", connct_time, handshake_time)
-                logging.info("%s CN:%s", self.ip, ssl_cert.cn)
+                xlog.info("%s CN:%s", self.ip, ssl_cert.cn)
-            logging.warn("honeypot %s", self.ip)
+            xlog.warn("honeypot %s", self.ip)
-            logging.debug("Check_appengine %s SSLError:%s", self.ip, e)
+            xlog.debug("Check_appengine %s SSLError:%s", self.ip, e)
-            logging.warn("Check %s IOError:%s", self.ip, e)
+            xlog.warn("Check %s IOError:%s", self.ip, e)
-            logging.exception('check_appengine %s %s err:%s', self.ip, errno_str, e)
+            xlog.exception('check_appengine %s %s err:%s', self.ip, errno_str, e)
-        logging.info("server_type:%s time:%d", server_type, time_cost)
+        xlog.info("server_type:%s time:%d", server_type, time_cost)
-            logging.debug("app check %s status:%d", ip, status)
+            xlog.debug("app check %s status:%d", ip, status)
-        logging.exception("test_app_head except:%r", e)
+        xlog.exception("test_app_head except:%r", e)
-    logging.debug("app check time:%d", time_cost)
+    xlog.debug("app check time:%d", time_cost)
-            logging.debug("app check %s status:%d", ip, status)
+            xlog.debug("app check %s status:%d", ip, status)
-            logging.debug("app check %s content:%s", ip, content)
+            xlog.debug("app check %s content:%s", ip, content)
-    logging.debug("app check time:%d", time_cost)
+    xlog.debug("app check time:%d", time_cost)
-        logging.debug("patch socks")
+        xlog.debug("patch socks")
-            logging.debug("network is ok")
+            xlog.debug("network is ok")
-            logging.debug("restore socket")
+            xlog.debug("restore socket")
-    logging.warn("network fail.")
+    xlog.warn("network fail.")
-    logging.info("==>%s", ip_str)
+    xlog.info("==>%s", ip_str)
-    logging.info("==>%s", ip_str)
+    xlog.info("==>%s", ip_str)
-    logging.info("test_with_app %s app %s", ip_str, result)
+    xlog.info("test_with_app %s app %s", ip_str, result)
-    logging.info("==>%s", ip_str)
+    xlog.info("==>%s", ip_str)
-                logging.warn("ip:%s server_type:%s but appengine check fail.", ip_str, check.result.server_type)
+                xlog.warn("ip:%s server_type:%s but appengine check fail.", ip_str, check.result.server_type)
-            logging.warn("check fail")
+            xlog.warn("check fail")
-            logging.debug("=======app check ok: %s", ip_str)
+            xlog.debug("=======app check ok: %s", ip_str)
-            logging.debug("test server type fail")
+            xlog.debug("test server type fail")
-        logging.info("========== %s type:%s domain:%s handshake:%d", ip_str, check.result.server_type,
+        xlog.info("========== %s type:%s domain:%s handshake:%d", ip_str, check.result.server_type,
-                logging.warn("google_ip.runJob fail:%s", e)
+                xlog.warn("google_ip.runJob fail:%s", e)
-                logging.warning("line err: %s", line)
+                xlog.warning("line err: %s", line)
-            logging.info("test ip: %s time:%d domain:%s server:%s", ip_str, handshake_time, domain, server)
+            xlog.info("test ip: %s time:%d domain:%s server:%s", ip_str, handshake_time, domain, server)
-            logging.exception("load_ip line:%s err:%s", line, e)
+            xlog.exception("load_ip line:%s err:%s", line, e)
-    logging.info("==>%s, time:%d", ip_str, interval)
+    xlog.info("==>%s, time:%d", ip_str, interval)
-    logging.info("first:%r", result)
+    xlog.info("first:%r", result)
-    logging.info("result:%r", result)
+    xlog.info("result:%r", result)
-            logging.info("time alive:%d", time_now - stat["start_time"])
+            xlog.info("time alive:%d", time_now - stat["start_time"])
-            logging.info("time alive fail")
+            xlog.info("time alive fail")
-            logging.debug("%s", cipher)
+            xlog.debug("%s", cipher)
-                logging.debug("%s", server_type)
+                xlog.debug("%s", server_type)
-                logging.warn("err:%s", e)
+                xlog.warn("err:%s", e)
-            logging.debug("%s", cipher)
+            xlog.debug("%s", cipher)
-                logging.debug("%s", server_type)
+                xlog.debug("%s", server_type)
-                logging.warn("err:%s", e)
+                xlog.warn("err:%s", e)
-        logging.info("work ciphers:%s", work_str)
+        xlog.info("work ciphers:%s", work_str)
-import logging
+import xlog
-                    logging.info("load manual.ini success")
+                    xlog.info("load manual.ini success")
-                    logging.exception("data/gae_proxy/manual.ini load error:%s", e)
+                    xlog.exception("data/gae_proxy/manual.ini load error:%s", e)
-import logging
+import xlog
-    logging.warn("fall_into_honeypot.")
+    xlog.warn("fall_into_honeypot.")
-    logging.warn("Scan Blocked, due to exceeds Google's frequency limit. Please reduce the number of scan threads.")
+    xlog.warn("Scan Blocked, due to exceeds Google's frequency limit. Please reduce the number of scan threads.")
-import logging
+import xlog
-            logging.error("proxy type %s unknown, disable proxy", config.PROXY_TYPE)
+            xlog.error("proxy type %s unknown, disable proxy", config.PROXY_TYPE)
-                logging.error("no appid can use")
+                xlog.error("no appid can use")
-                logging.warn("head send len:%d %d", ret, len(data))
+                xlog.warn("head send len:%d %d", ret, len(data))
-                logging.debug("app head fail status:%d", status)
+                xlog.debug("app head fail status:%d", status)
-            logging.debug("head request fail:%r", e)
+            xlog.debug("head request fail:%r", e)
-                logging.warn("keep alive except:%r", e)
+                xlog.warn("keep alive except:%r", e)
-            logging.debug("create_ssl update ip:%s time:%d", ip, handshake_time)
+            xlog.debug("create_ssl update ip:%s time:%d", ip, handshake_time)
-            logging.debug("create_ssl %s fail:%s cost:%d h:%d", ip, e, time_cost * 1000, handshake_time)
+            xlog.debug("create_ssl %s fail:%s cost:%d h:%d", ip, e, time_cost * 1000, handshake_time)
-                    logging.warning("no gws ip")
+                    xlog.warning("no gws ip")
-                        logging.debug("host_conn_pool %s get:%s handshake:%d", host, ssl_sock.ip, handshake_time)
+                        xlog.debug("host_conn_pool %s get:%s handshake:%d", host, ssl_sock.ip, handshake_time)
-                    logging.debug("ssl_pool.get:%s handshake:%d", ssl_sock.ip, handshake_time)
+                    xlog.debug("ssl_pool.get:%s handshake:%d", ssl_sock.ip, handshake_time)
-                logging.debug("create ssl timeout fail.")
+                xlog.debug("create ssl timeout fail.")
-            logging.warn("forward port %d not supported.", port)
+            xlog.warn("forward port %d not supported.", port)
-                logging.debug("tcp conn %s time:%d", ip, conn_time * 1000)
+                xlog.debug("tcp conn %s time:%d", ip, conn_time * 1000)
-                logging.debug("tcp conn %s fail t:%d", ip, conn_time)
+                xlog.debug("tcp conn %s fail t:%d", ip, conn_time)
-                    logging.error("no gws ip.")
+                    xlog.error("no gws ip.")
-        logging.warning('create tcp connection fail.')
+        xlog.warning('create tcp connection fail.')
-                            logging.debug("forward remote disconnected.")
+                            xlog.debug("forward remote disconnected.")
-                            logging.debug("forward local disconnected.")
+                            xlog.debug("forward local disconnected.")
-                logging.exception("forward except:%s.", e)
+                xlog.exception("forward except:%s.", e)
-import logging
+import xlog
-        logging.warn("direct_handler.fetch bad status line:%r", e)
+        xlog.warn("direct_handler.fetch bad status line:%r", e)
-        logging.warn("direct_handler.fetch:%r", e)
+        xlog.warn("direct_handler.fetch:%r", e)
-            logging.warn("direct_handler.handler err:%r %s/%s", e, host, url)
+            xlog.warn("direct_handler.handler err:%r %s/%s", e, host, url)
-            logging.exception('direct_handler.handler %r %s %s , retry...', e, host, url)
+            xlog.exception('direct_handler.handler %r %s %s , retry...', e, host, url)
-            logging.warn("direct_handler.handler send response fail. t:%d e:%r %s%s", wait_time, e, host, url)
+            xlog.warn("direct_handler.handler send response fail. t:%d e:%r %s%s", wait_time, e, host, url)
-            logging.info("DIRECT t:%d %d %s %s", (time.time()-time_request)*1000, response.status, host, url)
+            xlog.info("DIRECT t:%d %d %s %s", (time.time()-time_request)*1000, response.status, host, url)
-                        logging.warn("direct_handler.handler send Transfer-Encoding t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
+                        xlog.warn("direct_handler.handler send Transfer-Encoding t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
-            logging.info("DIRECT chucked t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
+            xlog.info("DIRECT chucked t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
-                logging.info("DIRECT t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
+                xlog.info("DIRECT t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
-                    logging.warn("read timeout t:%d len:%d left:%d %s %s", (time.time()-time_request)*1000, length, (end-start), host, url)
+                    xlog.warn("read timeout t:%d len:%d left:%d %s %s", (time.time()-time_request)*1000, length, (end-start), host, url)
-                        logging.debug("send to browser wfile.write ret:%d", ret)
+                        xlog.debug("send to browser wfile.write ret:%d", ret)
-                        logging.warn('direct_handler send to browser return %r %s %r', e_b, host, url)
+                        xlog.warn('direct_handler send to browser return %r %s %r', e_b, host, url)
-                        logging.warn('direct_handler send to browser return %r %s %r', e_b, host, url)
+                        xlog.warn('direct_handler send to browser return %r %s %r', e_b, host, url)
-            logging.exception("direct_handler err:%r %s %s time:%d", e, host, url, time_cost)
+            xlog.exception("direct_handler err:%r %s %s time:%d", e, host, url, time_cost)
-            logging.exception("direct_handler except:%r %s %s", e, host, url)
+            xlog.exception("direct_handler except:%r %s %s", e, host, url)
-        logging.exception("direct_handler except:%r %s %s", e, host, url)
+        xlog.exception("direct_handler except:%r %s %s", e, host, url)
-import logging
+import xlog
-        logging.warn("_request:%r", e)
+        xlog.warn("_request:%r", e)
-        logging.debug("GAE_Exception %r %r", type, message)
+        xlog.debug("GAE_Exception %r %r", type, message)
-                logging.debug('create_ssl_connection fail')
+                xlog.debug('create_ssl_connection fail')
-            logging.warn('request failed:%s', e)
+            xlog.warn('request failed:%s', e)
-            logging.warn("body len:%d %s %s", len(body), method, url)
+            xlog.warn("body len:%d %s %s", len(body), method, url)
-        logging.warn("fetch too short lead byte len:%d %s", len(data), url)
+        xlog.warn("fetch too short lead byte len:%d %s", len(data), url)
-        logging.warn("fetch too short header need:%d get:%d %s", headers_length, len(data), url)
+        xlog.warn("fetch too short header need:%d get:%d %s", headers_length, len(data), url)
-                logging.warn("fetch gae status:%s url:%s", response.app_status, url)
+                xlog.warn("fetch gae status:%s url:%s", response.app_status, url)
-                logging.warning('APPID %r not exists, remove it.', response.ssl_sock.appid)
+                xlog.warning('APPID %r not exists, remove it.', response.ssl_sock.appid)
-                logging.warning('405 Method not allowed. remove %s ', response.ssl_sock.ip)
+                xlog.warning('405 Method not allowed. remove %s ', response.ssl_sock.ip)
-                logging.warning('APPID %r out of Quota, remove it.', response.ssl_sock.appid)
+                xlog.warning('APPID %r out of Quota, remove it.', response.ssl_sock.appid)
-            logging.warn("gae_exception:%r %s", e, url)
+            xlog.warn("gae_exception:%r %s", e, url)
-            logging.exception('gae_handler.handler %r %s , retry...', e, url)
+            xlog.exception('gae_handler.handler %r %s , retry...', e, url)
-            logging.warn("gae_handler.handler send response fail. t:%d e:%r %s", time.time()-time_request, e, url)
+            xlog.warn("gae_handler.handler send response fail. t:%d e:%r %s", time.time()-time_request, e, url)
-            logging.warn("APPID error:%d url:%s", response.status, url)
+            xlog.warn("APPID error:%d url:%s", response.status, url)
-                logging.info("GAE t:%d s:%d %d %s", (time.time()-time_request)*1000, length, response.status, url)
+                xlog.info("GAE t:%d s:%d %d %s", (time.time()-time_request)*1000, length, response.status, url)
-                    logging.warn("read timeout t:%d len:%d left:%d %s", (time.time()-time_request)*1000, length, (end-start), url)
+                    xlog.warn("read timeout t:%d len:%d left:%d %s", (time.time()-time_request)*1000, length, (end-start), url)
-                        logging.debug("send to browser wfile.write ret:%d", ret)
+                        xlog.debug("send to browser wfile.write ret:%d", ret)
-                        logging.warn('gae_handler send to browser return %r %r', e_b, url)
+                        xlog.warn('gae_handler send to browser return %r %r', e_b, url)
-                        logging.warn('gae_handler send to browser return %r %r', e_b, url)
+                        xlog.warn('gae_handler send to browser return %r %r', e_b, url)
-            logging.warn("gae_handler err:%r time:%d %s ", e, time_cost, url)
+            xlog.warn("gae_handler err:%r time:%d %s ", e, time_cost, url)
-            logging.exception("gae_handler except:%r %s", e, url)
+            xlog.exception("gae_handler except:%r %s", e, url)
-        logging.exception("gae_handler except:%r %s", e, url)
+        xlog.exception("gae_handler except:%r %s", e, url)
-        logging.info('>>>>>>>>>>>>>>> RangeFetch started(%r) %d-%d', self.url, start, end)
+        xlog.info('>>>>>>>>>>>>>>> RangeFetch started(%r) %d-%d', self.url, start, end)
-            logging.warn("RangeFetch send response fail:%r %s", e, self.url)
+            xlog.warn("RangeFetch send response fail:%r %s", e, self.url)
-                        logging.error('RangeFetch Error: begin(%r) < expect_begin(%r), quit.', begin, self.expect_begin)
+                        xlog.error('RangeFetch Error: begin(%r) < expect_begin(%r), quit.', begin, self.expect_begin)
-                        logging.error('RangeFetch Error: begin(%r) < expect_begin(%r), quit.', begin, self.expect_begin)
+                        xlog.error('RangeFetch Error: begin(%r) < expect_begin(%r), quit.', begin, self.expect_begin)
-                logging.error('data_queue peek timeout, break')
+                xlog.error('data_queue peek timeout, break')
-                    logging.debug("send to browser wfile.write ret:%d, retry", ret)
+                    xlog.debug("send to browser wfile.write ret:%d, retry", ret)
-                    logging.debug("send to browser wfile.write ret:%d", ret)
+                    xlog.debug("send to browser wfile.write ret:%d", ret)
-                logging.warn('RangeFetch client closed(%s). %s', e, self.url)
+                xlog.warn('RangeFetch client closed(%s). %s', e, self.url)
-                    logging.warning("RangeFetch fetch response %r in __fetchlet", e)
+                    xlog.warning("RangeFetch fetch response %r in __fetchlet", e)
-                    logging.warning('RangeFetch %s return %r', headers['Range'], response)
+                    xlog.warning('RangeFetch %s return %r', headers['Range'], response)
-                    logging.warning('Range Fetch return %s "%s %s" %s ', response.app_status, self.method, self.url, headers['Range'])
+                    xlog.warning('Range Fetch return %s "%s %s" %s ', response.app_status, self.method, self.url, headers['Range'])
-                        logging.warning('APPID %r not exists, remove it.', response.ssl_sock.appid)
+                        xlog.warning('APPID %r not exists, remove it.', response.ssl_sock.appid)
-                            logging.error("no appid left")
+                            xlog.error("no appid left")
-                        logging.warning('APPID %r out of Quota, remove it temporary.', response.ssl_sock.appid)
+                        xlog.warning('APPID %r out of Quota, remove it temporary.', response.ssl_sock.appid)
-                            logging.error("no appid left")
+                            xlog.error("no appid left")
-                    logging.info('RangeFetch Redirect(%r)', self.url)
+                    xlog.info('RangeFetch Redirect(%r)', self.url)
-                        logging.warning('RangeFetch "%s %s" return Content-Range=%r: response headers=%r, retry %s-%s', self.method, self.url, content_range, response.getheaders(), start, end)
+                        xlog.warning('RangeFetch "%s %s" return Content-Range=%r: response headers=%r, retry %s-%s', self.method, self.url, content_range, response.getheaders(), start, end)
-                    logging.info('>>>>>>>>>>>>>>> [thread %s] %s %s', threading.currentThread().ident, content_length, content_range)
+                    xlog.info('>>>>>>>>>>>>>>> [thread %s] %s %s', threading.currentThread().ident, content_length, content_range)
-                            logging.warning('RangeFetch "%s %s" %s failed: %s', self.method, self.url, headers['Range'], e)
+                            xlog.warning('RangeFetch "%s %s" %s failed: %s', self.method, self.url, headers['Range'], e)
-                        logging.warning('RangeFetch "%s %s" retry %s-%s', self.method, self.url, start, end)
+                        xlog.warning('RangeFetch "%s %s" retry %s-%s', self.method, self.url, start, end)
-                    logging.info('>>>>>>>>>>>>>>> Successfully reached %d bytes.', start - 1)
+                    xlog.info('>>>>>>>>>>>>>>> Successfully reached %d bytes.', start - 1)
-                    logging.error('RangeFetch %r return %s', self.url, response.status)
+                    xlog.error('RangeFetch %r return %s', self.url, response.status)
-                logging.exception('RangeFetch._fetchlet error:%s', e)
+                xlog.exception('RangeFetch._fetchlet error:%s', e)
-import logging
+import xlog
-                    logging.warning("line err: %s", line)
+                    xlog.warning("line err: %s", line)
-                logging.exception("load_ip line:%s err:%s", line, e)
+                xlog.exception("load_ip line:%s err:%s", line, e)
-        logging.info("load google ip_list num:%d, gws num:%d", len(self.ip_dict), len(self.gws_ip_list))
+        xlog.info("load google ip_list num:%d, gws num:%d", len(self.ip_dict), len(self.gws_ip_list))
-                            logging.warning("bad_ip line err: %s", line)
+                            xlog.warning("bad_ip line err: %s", line)
-                        logging.exception("parse bad_ip.txt err:%r", e)
+                        xlog.exception("parse bad_ip.txt err:%r", e)
-                    logging.debug("save bad ip:%s", ip)
+                    xlog.debug("save bad ip:%s", ip)
-            logging.error("save good_ip.txt fail %s", e)
+            xlog.error("save good_ip.txt fail %s", e)
-            logging.error("try_sort_ip_by_handshake_time:%s", e)
+            xlog.error("try_sort_ip_by_handshake_time:%s", e)
-            logging.debug("sort ip time:%dms", time_cost) # 5ms for 1000 ip. 70~150ms for 30000 ip.
+            xlog.debug("sort ip time:%dms", time_cost) # 5ms for 1000 ip. 70~150ms for 30000 ip.
-                logging.warn("adjust_scan_thread_num fail:%r", e)
+                xlog.warn("adjust_scan_thread_num fail:%r", e)
-            logging.info("Adjust scan thread num from %d to %d", self.scan_ip_thread_num, scan_ip_thread_num)
+            xlog.info("Adjust scan thread num from %d to %d", self.scan_ip_thread_num, scan_ip_thread_num)
-                logging.debug("get ip:%s t:%d", ip_str, handshake_time)
+                xlog.debug("get ip:%s t:%d", ip_str, handshake_time)
-            logging.error("get_gws_ip fail:%s", e)
+            xlog.error("get_gws_ip fail:%s", e)
-                logging.debug("get host:%s ip:%s t:%d", host, ip_str, handshake_time)
+                xlog.debug("get host:%s ip:%s t:%d", host, ip_str, handshake_time)
-            logging.error("get_gws_ip fail:%s", e)
+            xlog.error("get_gws_ip fail:%s", e)
-            logging.error("add_ip input")
+            xlog.error("add_ip input")
-            logging.error("set_ip err:%s", e)
+            xlog.error("set_ip err:%s", e)
-            logging.error("set_ip input")
+            xlog.error("set_ip input")
-            logging.error("update_ip err:%s", e)
+            xlog.error("update_ip err:%s", e)
-        logging.debug("report_bad_ip %s", ip_str)
+        xlog.debug("report_bad_ip %s", ip_str)
-                logging.debug("report_connect_fail network fail")
+                xlog.debug("report_connect_fail network fail")
-                logging.info("remove ip:%s left amount:%d gws_num:%d", ip_str, len(self.ip_dict), len(self.gws_ip_list))
+                xlog.info("remove ip:%s left amount:%d gws_num:%d", ip_str, len(self.ip_dict), len(self.gws_ip_list))
-            logging.exception("set_ip err:%s", e)
+            xlog.exception("set_ip err:%s", e)
-                    logging.debug("remove ip process, restore ip:%s", ip_str)
+                    xlog.debug("remove ip process, restore ip:%s", ip_str)
-                    logging.warn("network is unreachable. check your network connection.")
+                    xlog.warn("network is unreachable. check your network connection.")
-                logging.info("real remove ip:%s ", ip_str)
+                xlog.info("real remove ip:%s ", ip_str)
-                logging.info("remove_slowest_ip:%s handshake_time:%d", ip_str, handshake_time)
+                xlog.info("remove_slowest_ip:%s handshake_time:%d", ip_str, handshake_time)
-            logging.exception("remove_slowest_ip err:%s", e)
+            xlog.exception("remove_slowest_ip err:%s", e)
-                    logging.info("scan_ip add ip:%s time:%d", ip_str, result.handshake_time)
+                    xlog.info("scan_ip add ip:%s time:%d", ip_str, result.handshake_time)
-                logging.exception("google_ip.runJob fail:%s", e)
+                xlog.exception("google_ip.runJob fail:%s", e)
-        logging.info("scan_ip_worker exit")
+        xlog.info("scan_ip_worker exit")
-import logging
+import xlog
-        logging.info("load ip range file:%s", self.range_file)
+        xlog.info("load ip range file:%s", self.range_file)
-            logging.error("load ip range %s fail", self.range_file)
+            xlog.error("load ip range %s fail", self.range_file)
-                logging.warn("load ip range:%s fail", line)
+                xlog.warn("load ip range:%s fail", line)
-                logging.exception("random.randint:%r %d - %d, %d", e, ip_range[0], ip_range[1], ip_range[1] - ip_range[0])
+                xlog.exception("random.randint:%r %d - %d, %d", e, ip_range[0], ip_range[1], ip_range[1] - ip_range[0])
-import logging
+import xlog
-            logging.info("SSL use version:%s", ssl_version)
+            xlog.info("SSL use version:%s", ssl_version)
-import logging
+import xlog
-                logging.info('try download %r to update_pacfile(%r)', config.PAC_ADBLOCK, filename)
+                xlog.info('try download %r to update_pacfile(%r)', config.PAC_ADBLOCK, filename)
-                logging.warn("pac_update download adblock fail:%r", e)
+                xlog.warn("pac_update download adblock fail:%r", e)
-            logging.info('try download %r to update_pacfile(%r)', config.PAC_GFWLIST, filename)
+            xlog.info('try download %r to update_pacfile(%r)', config.PAC_GFWLIST, filename)
-            logging.warn("pac_update download gfwlist fail:%r", e)
+            xlog.warn("pac_update download gfwlist fail:%r", e)
-                logging.info('%r downloaded, try convert it with adblock2pac', config.PAC_ADBLOCK)
+                xlog.info('%r downloaded, try convert it with adblock2pac', config.PAC_ADBLOCK)
-                logging.info('%r downloaded and parsed', config.PAC_ADBLOCK)
+                xlog.info('%r downloaded and parsed', config.PAC_ADBLOCK)
-            logging.exception('update_pacfile failed: %r', e)
+            xlog.exception('update_pacfile failed: %r', e)
-            logging.info('%r downloaded, try convert it with autoproxy2pac', config.PAC_GFWLIST)
+            xlog.info('%r downloaded, try convert it with autoproxy2pac', config.PAC_GFWLIST)
-            logging.info('%r downloaded and parsed', config.PAC_GFWLIST)
+            xlog.info('%r downloaded and parsed', config.PAC_GFWLIST)
-            logging.exception('update_pacfile failed: %r', e)
+            xlog.exception('update_pacfile failed: %r', e)
-            logging.info('%r successfully updated', user_pacfile)
+            xlog.info('%r successfully updated', user_pacfile)
-        logging.info('PAC from:%s %s %s ', self.address_string(), self.command, self.path)
+        xlog.info('PAC from:%s %s %s ', self.address_string(), self.command, self.path)
-            logging.info('%s "%s %s HTTP/1.1" 200 -', self.address_string(), self.command, self.path)
+            xlog.info('%s "%s %s HTTP/1.1" 200 -', self.address_string(), self.command, self.path)
-            logging.warn('%s %s %s haking', self.address_string(), self.command, self.path )
+            xlog.warn('%s %s %s haking', self.address_string(), self.command, self.path )
-            logging.warn("pac_server GET %s fail", filename)
+            xlog.warn("pac_server GET %s fail", filename)
-import logging
+import xlog
-                logging.exception('psutil.get_windows_running_process_list() failed: %r', e)
+                xlog.exception('psutil.get_windows_running_process_list() failed: %r', e)
-        logging.info('cygwin is not officially supported, please continue at your own risk :)')
+        xlog.info('cygwin is not officially supported, please continue at your own risk :)')
-        logging.critical('please edit %s to add your appid to [gae] !', config.CONFIG_FILENAME)
+        xlog.critical('please edit %s to add your appid to [gae] !', config.CONFIG_FILENAME)
-    logging.basicConfig(level=logging.DEBUG if config.LISTEN_DEBUGINFO else logging.INFO, format='%(levelname)s - %(asctime)s %(message)s', datefmt='[%b %d %H:%M:%S]')
+    xlog.basicConfig(level=xlog.DEBUG if config.LISTEN_DEBUGINFO else xlog.INFO, format='%(levelname)s - %(asctime)s %(message)s', datefmt='[%b %d %H:%M:%S]')
-    logging.info(config.info())
+    xlog.info(config.info())
-    logging.info("Exiting gae_proxy module...")
+    xlog.info("Exiting gae_proxy module...")
-    logging.info("Finished Exiting gae_proxy module...")
+    xlog.info("Finished Exiting gae_proxy module...")
-import logging
+import xlog
-                logging.warn("method not defined: %s", self.command)
+                xlog.warn("method not defined: %s", self.command)
-            logging.warn("Your browser forward localhost to proxy.")
+            xlog.warn("Your browser forward localhost to proxy.")
-                logging.warn("chunk header read fail crlf")
+                xlog.warn("chunk header read fail crlf")
-                logging.error('handle_method_urlfetch read payload failed:%s', e)
+                xlog.error('handle_method_urlfetch read payload failed:%s', e)
-                    logging.warn("chunk ext: %s", chunk_size_str)
+                    xlog.warn("chunk ext: %s", chunk_size_str)
-                            logging.warn("entity header:%s", line)
+                            xlog.warn("entity header:%s", line)
-        logging.info('FWD %s %s:%d ', self.command, host, port)
+        xlog.info('FWD %s %s:%d ', self.command, host, port)
-            logging.exception('do_CONNECT_FWD (%r, %r) Exception:%s', host, port, e)
+            xlog.exception('do_CONNECT_FWD (%r, %r) Exception:%s', host, port, e)
-            logging.warn('FWD %s %s:%d create_connection fail', self.command, host, port)
+            xlog.warn('FWD %s %s:%d create_connection fail', self.command, host, port)
-            logging.exception('do_CONNECT_FWD (%r, %r) Exception:%s', host, port, e)
+            xlog.exception('do_CONNECT_FWD (%r, %r) Exception:%s', host, port, e)
-        logging.debug('FWD %s %s:%d with closed', self.command, host, port)
+        xlog.debug('FWD %s %s:%d with closed', self.command, host, port)
-        logging.info('GAE %s %s:%d ', self.command, host, port)
+        xlog.info('GAE %s %s:%d ', self.command, host, port)
-            logging.info('ssl error: %s, create full domain cert for host:%s', e, host)
+            xlog.info('ssl error: %s, create full domain cert for host:%s', e, host)
-                logging.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
+                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
-                logging.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
+                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
-        logging.debug('GAE CONNECT %s %s', self.command, self.path)
+        xlog.debug('GAE CONNECT %s %s', self.command, self.path)
-                logging.warn("Method %s not support in GAE, Redirect to FWD for %s", self.command, self.path)
+                xlog.warn("Method %s not support in GAE, Redirect to FWD for %s", self.command, self.path)
-                logging.warn("Method %s not support in GAEProxy for %s", self.command, self.path)
+                xlog.warn("Method %s not support in GAEProxy for %s", self.command, self.path)
-            logging.warn("CONNECT %s port:%d not support", host, port)
+            xlog.warn("CONNECT %s port:%d not support", host, port)
-        logging.info('GAE %s %s:%d ', self.command, host, port)
+        xlog.info('GAE %s %s:%d ', self.command, host, port)
-            logging.info('ssl error: %s, create full domain cert for host:%s', e, host)
+            xlog.info('ssl error: %s, create full domain cert for host:%s', e, host)
-                logging.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
+                xlog.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
-        logging.debug('GAE CONNECT %s %s', self.command, self.path)
+        xlog.debug('GAE CONNECT %s %s', self.command, self.path)
-                    logging.error('handle_method_urlfetch read payload failed:%s', e)
+                    xlog.error('handle_method_urlfetch read payload failed:%s', e)
-import logging
+import xlog
-            logging.info("scan_ip_log roll %s -> %s", self.log_path, file_name)
+            xlog.info("scan_ip_log roll %s -> %s", self.log_path, file_name)
-import logging
+import xlog
-            logging.warn("User_config.load except:%s", e)
+            xlog.warn("User_config.load except:%s", e)
-            logging.warn("launcher.config save user config fail:%s", CONFIG_USER_FILENAME)
+            xlog.warn("launcher.config save user config fail:%s", CONFIG_USER_FILENAME)
-        logging.exception("web_control http_request:%s fail:%s", url, e)
+        xlog.exception("web_control http_request:%s fail:%s", url, e)
-                logging.warn("web control ref:%s refuse", netloc)
+                xlog.warn("web control ref:%s refuse", netloc)
-            logging.debug('GAEProxy Web_control %s %s %s ', self.address_string(), self.command, self.path)
+            xlog.debug('GAEProxy Web_control %s %s %s ', self.address_string(), self.command, self.path)
-                logging.warn('%s %s %s wizard file %s not found', self.address_string(), self.command, self.path, file_path)
+                xlog.warn('%s %s %s wizard file %s not found', self.address_string(), self.command, self.path, file_path)
-            logging.warn('Control Req %s %s %s ', self.address_string(), self.command, self.path)
+            xlog.warn('Control Req %s %s %s ', self.address_string(), self.command, self.path)
-            logging.warn('%s %s %s haking', self.address_string(), self.command, self.path )
+            xlog.warn('%s %s %s haking', self.address_string(), self.command, self.path )
-            logging.info('%s "%s %s HTTP/1.1" 200 -', self.address_string(), self.command, self.path)
+            xlog.info('%s "%s %s HTTP/1.1" 200 -', self.address_string(), self.command, self.path)
-            logging.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
+            xlog.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
-                logging.warn("web control ref:%s refuse", netloc)
+                xlog.warn("web control ref:%s refuse", netloc)
-        logging.debug ('GAEProxy web_control %s %s %s ', self.address_string(), self.command, self.path)
+        xlog.debug ('GAEProxy web_control %s %s %s ', self.address_string(), self.command, self.path)
-            logging.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
+            xlog.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
-            logging.set_buffer_size(buffer_size)
+            xlog.set_buffer_size(buffer_size)
-            data = logging.get_last_lines(max_line)
+            data = xlog.get_last_lines(max_line)
-            data = logging.get_new_lines(last_no)
+            data = xlog.get_new_lines(last_no)
-            logging.error('PAC %s %s %s ', self.address_string(), self.command, self.path)
+            xlog.error('PAC %s %s %s ', self.address_string(), self.command, self.path)
-            logging.exception("xxnet_version fail")
+            xlog.exception("xxnet_version fail")
-            logging.exception("req_config_handler except:%s", e)
+            xlog.exception("req_config_handler except:%s", e)
-                logging.warn("deploy is running, request denied.")
+                xlog.warn("deploy is running, request denied.")
-                    logging.info("deploy begin.")
+                    xlog.info("deploy begin.")
-import logging
+import launcher_log
-            logging.warn("autorun linux config path not found:%s", os.path.expanduser(_xdg_config_home))
+            launcher_log.warn("autorun linux config path not found:%s", os.path.expanduser(_xdg_config_home))
-        logging.info("create file:%s", plist_file_path)
+        launcher_log.info("create file:%s", plist_file_path)
-            logging.info("remove file:%s", plist_file_path)
+            launcher_log.info("remove file:%s", plist_file_path)
-import logging
+import launcher_log
-        logging.warn("save config %s fail %s", config_path, e)
+        launcher_log.warn("save config %s fail %s", config_path, e)
-import logging
+import launcher_log
-    logging.warn("import pynotify fail, please install python-notify if possiable.")
+    launcher_log.warn("import pynotify fail, please install python-notify if possiable.")
-import logging
+import launcher_log
-        logging.info("try enable proxy:%s", cmd)
+        launcher_log.info("try enable proxy:%s", cmd)
-        logging.info("try disable proxy:%s", cmd)
+        launcher_log.info("try disable proxy:%s", cmd)
-        logging.debug("dialog_yes_no return %d", res)
+        launcher_log.debug("dialog_yes_no return %d", res)
-        logging.error("Mac notify_general not implemented.")
+        launcher_log.error("Mac notify_general not implemented.")
-import logging
+import launcher_log
-            logging.error("module not exist %s", module)
+            launcher_log.error("module not exist %s", module)
-            logging.error("module %s is running", module)
+            launcher_log.error("module %s is running", module)
-        logging.info("%s started", module)
+        launcher_log.info("%s started", module)
-        logging.exception("start module %s fail:%s", module, e)
+        launcher_log.exception("start module %s fail:%s", module, e)
-            logging.error("module %s not running", module)
+            launcher_log.error("module %s not running", module)
-        logging.debug("module %s stopping", module)
+        launcher_log.debug("module %s stopping", module)
-        logging.info("module %s stopped", module)
+        launcher_log.info("module %s stopped", module)
-        logging.exception("stop module %s fail:%s", module, e)
+        launcher_log.exception("stop module %s fail:%s", module, e)
-            logging.info("start %s time cost %d", module, (finished_time - start_time) * 1000)
+            launcher_log.info("start %s time cost %d", module, (finished_time - start_time) * 1000)
-import logging
+import launcher_log
-            logging.info("download %s to %s", url, file)
+            launcher_log.info("download %s to %s", url, file)
-            logging.info("download %s to %s fail", url, file)
+            launcher_log.info("download %s to %s fail", url, file)
-            logging.exception("xxnet_version fail:%s", e)
+            launcher_log.exception("xxnet_version fail:%s", e)
-                logging.info("mkdir %s", target_path)
+                launcher_log.info("mkdir %s", target_path)
-                logging.info("copy %s => %s", src_file, dst_file)
+                launcher_log.info("copy %s => %s", src_file, dst_file)
-    logging.info("setup start run new launcher")
+    launcher_log.info("setup start run new launcher")
-import logging
+import launcher_log
-                logging.info("setup win python, mkdir:%s", dest_path)
+                launcher_log.info("setup win python, mkdir:%s", dest_path)
-                logging.info("setup win python, copy:%s %s", src_path, target_file)
+                launcher_log.info("setup win python, copy:%s %s", src_path, target_file)
-            logging.exception("setup win python except:%s", e)
+            launcher_log.exception("setup win python except:%s", e)
-import logging
+import launcher_log
-import logging
+import launcher_log
-        logging.info("download %s to %s", url, file)
+        launcher_log.info("download %s to %s", url, file)
-        logging.info("download %s to %s fail", url, file)
+        launcher_log.info("download %s to %s fail", url, file)
-        logging.error("install module %s dir %s not exist", module, new_module_version_path)
+        launcher_log.error("install module %s dir %s not exist", module, new_module_version_path)
-        logging.warn("update %s fail. setup script %s not exist", module, setup_script)
+        launcher_log.warn("update %s fail. setup script %s not exist", module, setup_script)
-        logging.info("Setup %s version %s ...", module, new_version)
+        launcher_log.info("Setup %s version %s ...", module, new_version)
-            logging.info("Finished new version setup.")
+            launcher_log.info("Finished new version setup.")
-            logging.info("Restarting new version ...")
+            launcher_log.info("Restarting new version ...")
-            logging.error("install module %s %s fail:%s", module, new_version, e)
+            launcher_log.error("install module %s %s fail:%s", module, new_version, e)
-                logging.warn("download %s fail", url)
+                launcher_log.warn("download %s fail", url)
-                logging.warn("download %s sha1 wrong", url)
+                launcher_log.warn("download %s sha1 wrong", url)
-                logging.error("module dir exist:%s, download exist.", version_path)
+                launcher_log.error("module dir exist:%s, download exist.", version_path)
-        logging.warn("get gae_proxy source fail, content:%s err:%s", update_content, e)
+        launcher_log.warn("get gae_proxy source fail, content:%s err:%s", update_content, e)
-        logging.error("general_gtk_callback data:%s", data)
+        launcher_log.error("general_gtk_callback data:%s", data)
-                logging.info("update to test version %s", versions[0][1])
+                launcher_log.info("update to test version %s", versions[0][1])
-                logging.info("update to stable version %s", versions[1][1])
+                launcher_log.info("update to stable version %s", versions[1][1])
-        logging.exception("check_update fail:%r", e)
+        launcher_log.exception("check_update fail:%r", e)
-            logging.warn("check_update fail:%r", e)
+            launcher_log.warn("check_update fail:%r", e)
-                logging.info("new %s version:%s", module, new_version)
+                launcher_log.info("new %s version:%s", module, new_version)
-        logging.exception("check_update except:%s", e)
+        launcher_log.exception("check_update except:%s", e)
-        logging.info("generate desktop shortcut")
+        launcher_log.info("generate desktop shortcut")
-        logging.info("need_new_uuid: uuid is empty")
+        launcher_log.info("need_new_uuid: uuid is empty")
-    logging.info("generate uuid:%s", xx_net_uuid)
+    launcher_log.info("generate uuid:%s", xx_net_uuid)
-    logging.info("get uuid:%s", xx_net_uuid)
+    launcher_log.info("get uuid:%s", xx_net_uuid)
-import logging
+import launcher_log
-        logging.exception("xxnet_version fail")
+        launcher_log.exception("xxnet_version fail")
-            logging.error("url in downloading, %s", url)
+            launcher_log.error("url in downloading, %s", url)
-        logging.info("download %s to %s", url, file)
+        launcher_log.info("download %s to %s", url, file)
-        logging.exception("download %s to %s fail:%r", url, file, e)
+        launcher_log.exception("download %s to %s fail:%r", url, file, e)
-        logging.exception("xxnet_version fail:%r", e)
+        launcher_log.exception("xxnet_version fail:%r", e)
-                    logging.info("mkdir %s", target_path)
+                    launcher_log.info("mkdir %s", target_path)
-                    logging.info("copy %s => %s", src_file, dst_file)
+                    launcher_log.info("copy %s => %s", src_file, dst_file)
-        logging.exception("update version %d fail:%r", version, e)
+        launcher_log.exception("update version %d fail:%r", version, e)
-import logging
+import launcher_log
-                    logging.warn("request %s no module in path", self.path)
+                    launcher_log.warn("request %s no module in path", self.path)
-                logging.warn("web control ref:%s refuse", netloc)
+                launcher_log.warn("web control ref:%s refuse", netloc)
-            logging.warn('%s %s %s haking', self.address_string(), self.command, self.path )
+            launcher_log.warn('%s %s %s haking', self.address_string(), self.command, self.path )
-                    logging.warn("request %s no module in path", url_path)
+                    launcher_log.warn("request %s no module in path", url_path)
-        logging.debug ('launcher web_control %s %s %s ', self.address_string(), self.command, self.path)
+        launcher_log.debug ('launcher web_control %s %s %s ', self.address_string(), self.command, self.path)
-            logging.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
+            launcher_log.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
-            logging.info("%s", data)
+            launcher_log.info("%s", data)
-                logging.info("update_test_version fail:%r", e)
+                launcher_log.info("update_test_version fail:%r", e)
-            logging.exception("init_module except:%s", e)
+            launcher_log.exception("init_module except:%s", e)
-    logging.info("begin to exit web control")
+    launcher_log.info("begin to exit web control")
-    logging.info("launcher web control exited.")
+    launcher_log.info("launcher web control exited.")
-    logging.debug("start confirm_xxnet_exit")
+    launcher_log.debug("start confirm_xxnet_exit")
-    logging.debug("finished confirm_xxnet_exit")
+    launcher_log.debug("finished confirm_xxnet_exit")
-        logging.error("confirm_module_ready with port 0")
+        launcher_log.error("confirm_module_ready with port 0")
-import logging
+import launcher_log
-        logging.info("set register path:%r name:%s type:%d value:%s", reg_path, name, reg_type, value)
+        launcher_log.info("set register path:%r name:%s type:%d value:%s", reg_path, name, reg_type, value)
-        import_command = 'security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ../../../data/gae_proxy/CA.crt'# % certfile.decode('utf-8')
+        import_command = 'security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ../../data/gae_proxy/CA.crt'# % certfile.decode('utf-8')
-        # this function have bug, not fixed, and proved to be not a good idea.
+    def get_real_random_ip(self):
-            add_last_byte = ip % 255
+            ip_int = random.randint(0, 4294967294)
-            return ip
+
-            logging.error("No usable appid left, add new appid to continue use GoAgent")
+            logging.error("No usable appid left, add new appid to continue use GAEProxy")
-data_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, 'data', 'goagent'))
+data_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, 'data', 'gae_proxy'))
-        import_command = 'security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ../../../data/goagent/CA.crt'# % certfile.decode('utf-8')
+        import_command = 'security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ../../../data/gae_proxy/CA.crt'# % certfile.decode('utf-8')
-        self.DATA_PATH = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'data', 'goagent'))
+        self.DATA_PATH = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'data', 'gae_proxy'))
-        # load ../../../data/goagent/config.ini, set by web_ui
+        # load ../../../data/gae_proxy/config.ini, set by web_ui
-        # load ../../../data/goagent/manual.ini, set by manual
+        # load ../../../data/gae_proxy/manual.ini, set by manual
-                    logging.exception("data/goagent/manual.ini load error:%s", e)
+                    logging.exception("data/gae_proxy/manual.ini load error:%s", e)
-        info += 'GoAgent Version    : %s (python/%s )\n' % (self.__version__, sys.version[:5])
+        info += 'GAEProxy Version    : %s (python/%s )\n' % (self.__version__, sys.version[:5])
-            time.sleep(2)
+            time.sleep(1)
-                sock_list = self.new_conn_pool.get_need_keep_alive(maxtime=self.keep_alive)
+                sock_list = self.new_conn_pool.get_need_keep_alive(maxtime=self.keep_alive-3)
-                sock_list = self.gae_conn_pool.get_need_keep_alive(maxtime=self.keep_alive)
+                sock_list = self.gae_conn_pool.get_need_keep_alive(maxtime=self.keep_alive-3)
-                    if time.time() - ssl_sock.last_use_time < self.keep_alive: # gws ssl connection can keep for 230s after created
+                    if time.time() - ssl_sock.last_use_time < self.keep_alive+1: # gws ssl connection can keep for 230s after created
-                if time.time() - ssl_sock.last_use_time < self.keep_alive: # gws ssl connection can keep for 230s after created
+                if time.time() - ssl_sock.last_use_time < self.keep_alive+1: # gws ssl connection can keep for 230s after created
-    html = generate_message_html('504 GoAgent Proxy Time out', u'è¿æ¥è¶æ¶ï¼åä¼æ¯ä¸ä¼åæ¥ï¼')
+    html = generate_message_html('504 GAEProxy Proxy Time out', u'è¿æ¥è¶æ¶ï¼åä¼æ¯ä¸ä¼åæ¥ï¼')
-#goagent_listen = config.LISTEN_IP + ":" + str(config.LISTEN_PORT)
+#gae_proxy_listen = config.LISTEN_IP + ":" + str(config.LISTEN_PORT)
-goagent_listen = "GOAGENT_LISTEN"
+gae_proxy_listen = "GOAGENT_LISTEN"
-        cafile = os.path.join(data_root, "goagent", "CA.crt")
+        cafile = os.path.join(data_root, "gae_proxy", "CA.crt")
-    """GoAgent Pac Util"""
+    """GAEProxy Pac Util"""
-    def autoproxy2pac(content, func_name='FindProxyForURLByAutoProxy', proxy=goagent_listen, default='DIRECT', indent=4):
+    def autoproxy2pac(content, func_name='FindProxyForURLByAutoProxy', proxy=gae_proxy_listen, default='DIRECT', indent=4):
-        goagent_proxy = host + ":" + str(config.LISTEN_PORT)
+        gae_proxy_proxy = host + ":" + str(config.LISTEN_PORT)
-        data = data.replace(goagent_listen, goagent_proxy)
+        data = data.replace(gae_proxy_listen, gae_proxy_proxy)
-    if config.GAE_APPIDS[0] == 'goagent':
+    if config.GAE_APPIDS[0] == 'gae_proxy':
-    # to profile goagent, run proxy.py, visit some web by proxy, then visit http://127.0.0.1:8084/quit to quit and print result.
+    # to profile gae_proxy, run proxy.py, visit some web by proxy, then visit http://127.0.0.1:8084/quit to quit and print result.
-    logging.info("Exiting goagent module...")
+    logging.info("Exiting gae_proxy module...")
-    logging.info("Finished Exiting goagent module...")
+    logging.info("Finished Exiting gae_proxy module...")
-            connected_in_s = 5 # goagent upload to appengine is slow, it need more 'fresh' connection.
+            connected_in_s = 5 # gae_proxy upload to appengine is slow, it need more 'fresh' connection.
-                logging.warn("Method %s not support in GoAgent for %s", self.command, self.path)
+                logging.warn("Method %s not support in GAEProxy for %s", self.command, self.path)
-        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(root_path, 'data', 'goagent', 'config.ini'))
+        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(root_path, 'data', 'gae_proxy', 'config.ini'))
-        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(root_path, 'data', 'goagent', 'config.ini'))
+        CONFIG_USER_FILENAME = os.path.abspath( os.path.join(root_path, 'data', 'gae_proxy', 'config.ini'))
-            logging.debug('GoAgent Web_control %s %s %s ', self.address_string(), self.command, self.path)
+            logging.debug('GAEProxy Web_control %s %s %s ', self.address_string(), self.command, self.path)
-        logging.debug ('GoAgent web_control %s %s %s ', self.address_string(), self.command, self.path)
+        logging.debug ('GAEProxy web_control %s %s %s ', self.address_string(), self.command, self.path)
-                #http_request("http://127.0.0.1:8085/init_module?module=goagent&cmd=restart")
+                #http_request("http://127.0.0.1:8085/init_module?module=gae_proxy&cmd=restart")
-__version__ = '3.3.1'
+#!/usr/bin/env python
-        yield cipher.encrypt(data)
+__hostsdeny__ = ()
-    modules = ["goagent", "launcher", "php_proxy"]
+    modules = ["gae_proxy", "launcher", "php_proxy"]
-        set(["modules", "goagent", "control_port"], 8084)
+    if get(["modules", "gae_proxy", "control_port"], 0) == 0:
-        os.mkdir(data_goagent_path)
+    data_gae_proxy_path = os.path.join(data_path, 'gae_proxy')
-                    ('restart goagent', self.on_restart_goagent),
+                    ('restart gae_proxy', self.on_restart_gae_proxy),
-        module_init.start("goagent")
+    def on_restart_gae_proxy(self, widget=None, data=None):
-        os.mkdir(data_goagent_path)
+    data_gae_proxy_path = os.path.join(data_path, 'gae_proxy')
-        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Reload GoAgent', 'resetGoagent:', '')
+        menuitem = NSMenuItem.alloc().initWithTitle_action_keyEquivalent_('Reload GAEProxy', 'resetGoagent:', '')
-        module_init.start("goagent")
+        module_init.stop("gae_proxy")
-    #config.config["modules"]["goagent"]["current_version"] = new_config["modules"]["goagent"]["current_version"]
+    #config.config["modules"]["gae_proxy"]["current_version"] = new_config["modules"]["gae_proxy"]["current_version"]
-            if relate_path == os.path.join("data", "goagent") and filename == "config.ini":
+            if relate_path == os.path.join("data", "gae_proxy") and filename == "config.ini":
-goagent_path = ""
+new_gae_proxy_version = ""
-        cafile = os.path.join(data_root, "goagent", "CA.crt")
+        cafile = os.path.join(data_root, "gae_proxy", "CA.crt")
-        logging.warn("get goagent source fail, content:%s err:%s", update_content, e)
+        logging.warn("get gae_proxy source fail, content:%s err:%s", update_content, e)
-    #update need goagent as proxy
+    #wait gae_proxy to start
-    config.config["modules"]["goagent"]["current_version"] = ""
+    config.config["modules"]["gae_proxy"]["current_version"] = ""
-                target_module = 'goagent'
+            if config.get(['modules', 'gae_proxy', 'auto_start'], 0) == 1:
-            data = '{ "check_update": "%s", "popup_webui": %d, "show_systray": %d, "auto_start": %d, "php_enable": %d, "goagent_enable": %d }' %\
+            data = '{ "check_update": "%s", "popup_webui": %d, "show_systray": %d, "auto_start": %d, "php_enable": %d, "gae_proxy_enable": %d }' %\
-                    , config.get(["modules", "goagent", "auto_start"], 0))
+                    , config.get(["modules", "gae_proxy", "auto_start"], 0))
-                    data = '{"res":"fail, goagent_enable:%s"}' % goagent_enable
+            elif 'gae_proxy_enable' in reqs :
-                    config.set(["modules", "goagent", "auto_start"], goagent_enable)
+                    config.set(["modules", "gae_proxy", "auto_start"], gae_proxy_enable)
-                        module_init.start("goagent")
+                    if gae_proxy_enable:
-                        module_init.stop("goagent")
+                        module_init.stop("gae_proxy")
-                        (u"å¨å±éè¿GoAgentä»£ç", None, self.on_enable_proxy, enable_checked),
+                        (u"å¨å±éè¿GAEProxyä»£ç", None, self.on_enable_proxy, enable_checked),
-                        (u"éå¯ GoAgent", None, self.on_restart_goagent, 0))
+                        (u"éå¯ GAEProxy", None, self.on_restart_gae_proxy, 0))
-                        (u"Set Global GoAgent Proxy", None, self.on_enable_proxy, enable_checked),
+                        (u"Set Global GAEProxy Proxy", None, self.on_enable_proxy, enable_checked),
-                        (u"Reset GoAgent", None, self.on_restart_goagent, 0))
+                        (u"Reset GAEProxy", None, self.on_restart_gae_proxy, 0))
-    def on_restart_goagent(self, widget=None, data=None):
+    def on_restart_gae_proxy(self, widget=None, data=None):
-        self.set_register(self.INTERNET_SETTINGS, 'ProxyEnable', 4, 0) # disable goagent proxy
+        self.set_register(self.INTERNET_SETTINGS, 'ProxyEnable', 4, 0) # disable gae_proxy proxy
-        self.set_register(self.INTERNET_SETTINGS, 'ProxyEnable', 4, 0) # disable goagent proxy
+        self.set_register(self.INTERNET_SETTINGS, 'ProxyEnable', 4, 0) # disable gae_proxy proxy
-       pass
+__all__ = ["local", "start"]
-data_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'data', 'goagent'))
+python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
-        cert.gmtime_adj_notAfter(60 * 60 * 24 * 3652)
+        cert.gmtime_adj_notAfter(60 * 60 * 24 * 365)
-    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'python27', '1.0'))
+    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
-        sys.path.append(win32_lib)
+    elif sys.platform.startswith("linux"):
-        raise
+def load_sock():
-    default_socket = socket.socket
+        if config.PROXY_TYPE == "HTTP":
-        test_keep_alive("64.15.114.41", i)
+def test_keep_alive(ip_str, interval=5):
-    #test_alive()
+    test_alive()
-    test_gws("74.125.216.36")
+    #test_gws("74.125.216.36")
-        self.DATA_PATH = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'data', 'goagent'))
+        self.DATA_PATH = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'data', 'goagent'))
-    socks.set_default_proxy(proxy_type, config.PROXY_HOST, config.PROXY_PORT, config.PROXY_USER, config.PROXY_PASSWD)
+def load_sock():
-    keep_alive = config.CONFIG.getint("connect_manager", "https_keep_alive") #1
+
-            self.keep_alive = False
+            self.keep_alive = 0
-                sock_list = self.new_conn_pool.get_need_keep_alive(maxtime=220)
+                sock_list = self.new_conn_pool.get_need_keep_alive(maxtime=self.keep_alive)
-                sock_list = self.gae_conn_pool.get_need_keep_alive(maxtime=200)
+                sock_list = self.gae_conn_pool.get_need_keep_alive(maxtime=self.keep_alive)
-            if t < 200:
+            if t < self.keep_alive:
-                    if time.time() - ssl_sock.last_use_time < 225: # gws ssl connection can keep for 230s after created
+                    if time.time() - ssl_sock.last_use_time < self.keep_alive: # gws ssl connection can keep for 230s after created
-                if time.time() - ssl_sock.last_use_time < 225: # gws ssl connection can keep for 230s after created
+                if time.time() - ssl_sock.last_use_time < self.keep_alive: # gws ssl connection can keep for 230s after created
-    max_thread_num = config.CONFIG.getint("connect_manager", "forward_max_connect_thread") #10
+
-        logging.warn("_request bad status line:%r", e)
+        #logging.warn("_request bad status line:%r", e)
-    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'python27', '1.0'))
+    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
-    ip_connect_interval = config.CONFIG.getint("google_ip", "ip_connect_interval") #5,10
+
-        self.search_more_google_ip()
+    def load_config(self):
-                the_100th_ip = self.gws_ip_list[100]
+                the_100th_ip = self.gws_ip_list[99]
-            jd[i] = unicode(buffer[i], errors='replace')
+            line = buffer[i]
-root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir))
-pac_listen = config.PAC_IP + ":" + str(config.PAC_PORT)
+#goagent_listen = config.LISTEN_IP + ":" + str(config.LISTEN_PORT)
- 
+
-        self.send_file(get_serving_pacfile(), mimetype)
+
-python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'python27', '1.0'))
+python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
-elif sys.platform == "linux" or sys.platform == "linux2":
+elif sys.platform.startswith("linux"):
-
+ready = False
-        except ValueError:
+        except Exception as e:
-    p.start()
+    proxy_daemon = LocalProxyServer((config.LISTEN_IP, config.LISTEN_PORT), proxy_handler.GAEProxyHandler)
-        p.start()
+        pac_daemon = LocalProxyServer((config.PAC_IP, config.PAC_PORT), pac_server.PACServerHandler)
-    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'python27', '1.0'))
+    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
-root_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir))
-                ('wReserved', ctypes.c_byte)]
+
-    elif sys.platform == "linux" or sys.platform == "linux2":
+    elif sys.platform.startswith("linux"):
-    deploy_proc = None
+deploy_proc = None
-                   "goagent_version": config.__version__,
+                   "xxnet_version":self.xxnet_version(),
-                http_request("http://127.0.0.1:8085/init_module?module=goagent&cmd=restart")
+                #http_request("http://127.0.0.1:8085/init_module?module=goagent&cmd=restart")
-            if RemoteContralServerHandler.deploy_proc and RemoteContralServerHandler.deploy_proc.poll() == None:
+            if deploy_proc and deploy_proc.poll() == None:
-                    RemoteContralServerHandler.deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd, rc4_passwd])
+                    deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd, rc4_passwd])
-                RemoteContralServerHandler.deploy_proc.kill()
+            if deploy_proc and deploy_proc.poll() == None:
-            if self.deploy_proc and os.path.isfile(log_path):
+            if deploy_proc and os.path.isfile(log_path):
-                if RemoteContralServerHandler.deploy_proc.poll() == None:
+            if deploy_proc:
-    print RemoteContralServerHandler.xxnet_version()
+        # fix bug for LinkedIn android client
-            del headers['Content-Encoding']
+            try:
-import local.proxy as goagent
+import local.proxy as client
-    sys.exit()
+if __name__ == "__main__":
-root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir))
-data_path = os.path.join(root_path, 'data', 'launcher', 'config.yaml')
+root_path = os.path.abspath( os.path.join(current_path, os.pardir))
-    global config, data_path
+    global config, config_path
-        config = yaml.load(file(data_path, 'r'))
+        config = yaml.load(file(config_path, 'r'))
-    global config, data_path
+    global config, config_path
-        yaml.dump(config, file(data_path, "w"))
+        yaml.dump(config, file(config_path, "w"))
-        logging.warn("save config %s fail %s", data_path, e)
+        logging.warn("save config %s fail %s", config_path, e)
-
+    if get(["modules", "launcher", "control_port"], 0) == 0:
-
+def create_data_path():
-    if os.path.isfile(data_path):
+    create_data_path()
-    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
+    python_path = os.path.abspath( os.path.join(current_path, os.pardir, 'python27', '1.0'))
-log_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, "data", "launcher", "log.log"))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir))
-    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
+    python_path = os.path.abspath( os.path.join(current_path, os.pardir, 'python27', '1.0'))
-
+root_path = os.path.abspath( os.path.join(current_path, os.pardir))
-        logging.info("%s %s started %s", module, version, script_path)
+        #script_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, module, version, 'start.py'))
-        proc_handler[module].wait()
+        #proc_handler[module].terminate()  # Sends SIGTERM
-            web_control.confirm_module_ready(config.get(["modules", module, "control_port"], 0))
+            #web_control.confirm_module_ready(config.get(["modules", module, "control_port"], 0))
-python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
+python_path = os.path.abspath( os.path.join(current_path, os.pardir, 'python27', '1.0'))
-root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir))
-    start_sript = os.path.abspath( os.path.join(current_path, os.pardir, "start.py"))
+    start_sript = os.path.abspath( os.path.join(current_path, "start.py"))
-python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
+python_path = os.path.abspath( os.path.join(current_path, os.pardir, 'python27', '1.0'))
-#!/usr/bin/env python2
+#!/usr/bin/env python
-import yaml
+if sys.platform.startswith("linux"):
-        return False
+elif sys.platform == "win32":
-        os.mkdir(data_goagent_path)
+        import mac_tray as sys_tray
-    launcher_main()
+    # change path to launcher
-        sys.exit()
+    except KeyboardInterrupt: # Ctrl + C on console
-root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir))
-        logging.warn("check_update fail:%r", e)
+        logging.exception("check_update fail:%r", e)
-        #config.load()
+        opener = get_opener()
-        req_url = update_url + "?uuid=" + get_uuid()
+        req_url = update_url + "?uuid=" + get_uuid() + "&version=" + update_from_github.current_version()
-    if sys.platform == "linux" or sys.platform == "linux2":
+    if sys.platform.startswith("linux"):
-python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
+python_path = os.path.abspath( os.path.join(current_path, os.pardir, 'python27', '1.0'))
-root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir))
-    start_sript = os.path.abspath( os.path.join(current_path, os.pardir, "start.py"))
+    start_sript = os.path.abspath( os.path.join(current_path, "start.py"))
-    subprocess.Popen([sys.executable, start_sript], shell=False)
+    subprocess.Popen([sys.executable, start_sript])
-    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
+    python_path = os.path.abspath( os.path.join(current_path, os.pardir, 'python27', '1.0'))
-root_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir))
-            menu_path = os.path.join(root_path, module, version, "web_ui", "menu.yaml")
+            #version = values["current_version"]
-        logging.debug ('launcher web_control %s %s %s ', self.address_string(), self.command, self.path)
+
-            file_path = os.path.join(root_path, module, modules_version, url_path.split('/')[3:].join('/'))
+        url_path_list = self.path.split('/')
-        right_content_file = os.path.join(root_path, target_module, config.get(["modules", target_module, "current_version"]), "web_ui", target_menu + ".html")
+        right_content_file = os.path.join(root_path, target_module, "web_ui", target_menu + ".html")
-    server = LocalServer(("127.0.0.1", 8085), Http_Handler)
+    server = LocalServer(("0.0.0.0", 8085), Http_Handler)
-        req = opener.open(url)
+        req = opener.open(url, timeout=30)
-            break
+    logging.debug("start confirm_xxnet_exit")
-    for i in range(10):
+    for i in range(30):
-    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
+    python_path = os.path.abspath( os.path.join(current_path, os.pardir, 'python27', '1.0'))
-data_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'data', 'php_proxy'))
+python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
-root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir))
-    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'python27', '1.0'))
+    python_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'python27', '1.0'))
-root_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir, os.pardir))
+root_path = os.path.abspath(os.path.join(current_path, os.pardir, os.pardir))
-                scan_ip_thread_num = int( (the_100th_handshake_time - 200)/10 * self.max_scan_ip_thread_num/50 )
+                scan_ip_thread_num = int( (the_100th_handshake_time - 200)/2 * self.max_scan_ip_thread_num/50 )
-def create_url_opener():
+def get_opener():
-
+ 
-
+        opener = get_opener()
-    def context_builder(ca_certs=None, cipher_suites=('ALL', '!aNULL', '!eNULL')):
+    def context_builder(ca_certs=None, cipher_suites=('ALL:!RC4-SHA:!ECDHE-RSA-RC4-SHA:!ECDHE-RSA-AES128-GCM-SHA256:!AES128-GCM-SHA256',)):
-        ssl_context.set_cipher_list('ECDHE-RSA-AES128-SHA')
+        ssl_context.set_cipher_list(':'.join(cipher_suites))
-def create_url_opener():
+def get_opener():
-opener = create_url_opener()
+        opener = get_opener()
-from update import opener
+from update import get_opener
-            jd[i] = buffer[i]
+            jd[i] = unicode(buffer[i], errors='replace')
-            last_ip = self.gws_ip_list[len(self.gws_ip_list)]
+    def ip_handshake_th(self, num):
-            return the_100th_handshake_time
+        except:
-                   "ip_handshake_100":google_ip.ip_handshake_100(),
+                   "ip_handshake_100":google_ip.ip_handshake_th(100),
-    return openergit
+    return opener
-    opener = urllib2.build_opener(urllib2.ProxyHandler({'http': autoproxy, 'https': autoproxy}), https_handler)
+
-    return opener
+
-    def autoproxy2pac(content, func_name='FindProxyForURLByAutoProxy', proxy='127.0.0.1:8087', default='DIRECT', indent=4):
+    def autoproxy2pac(content, func_name='FindProxyForURLByAutoProxy', proxy=goagent_listen, default='DIRECT', indent=4):
-opener = urllib2.build_opener(urllib2.ProxyHandler({'http': autoproxy, 'https': autoproxy}))
+current_path = os.path.dirname(os.path.abspath(__file__))
-opener = urllib2.build_opener(urllib2.ProxyHandler({'http': autoproxy, 'https': autoproxy}))
+from update import opener
-        logging.warn("download %s to %s fail:%r", url, file, e)
+        logging.exception("download %s to %s fail:%r", url, file, e)
-        return
+    if config.get(["update", "uuid"], '') != 'test':
-        for subdir in subdirs:
+                target_path = os.path.join(root_path, relate_path, subdir)
-                os.mkdir(target_path)
+            for filename in files:
-                logging.info("copy %s => %s", src_file, dst_file)
+    os.remove(xxnet_zip_file)
-
+
-                 left_click=None):
+                 left_click=None,
-        self._menu_actions_by_id = dict(self._menu_actions_by_id)
+        self._set_menu_options(menu_options)
-    def update(self, icon=None, hover_text=None):
+    def update(self, icon=None, hover_text=None, menu=None):
-            self._show_menu()
+            self._right_click()
-        for option_text, option_icon, option_action, option_id in menu_options[::-1]:
+        for option_text, option_icon, option_action, fState, option_id in menu_options[::-1]:
-                InsertMenuItem(menu, 0, 1, ctypes.byref(item))
+                                        wID=option_id,
-                InsertMenuItem(menu, 0, 1,  ctypes.byref(item))
+            InsertMenuItem(menu, 0, 1,  ctypes.byref(item))
-def PackMENUITEMINFO(text=None, hbmpItem=None, wID=None, hSubMenu=None):
+def PackMENUITEMINFO(text=None, hbmpItem=None, wID=None, hSubMenu=None, fState=fState.MFS_ENABLED):
-    res.fMask = 0
+    res.fMask = 1
-        self.systray = SysTrayIcon(icon_path, "XX-Net", self.make_menu(), self.on_quit, left_click=self.on_show)
+        self.systray = SysTrayIcon(icon_path, "XX-Net", self.make_menu(), self.on_quit, left_click=self.on_show, right_click=self.on_right_click)
-                        (u"éå¯ GoAgent", None, self.on_restart_goagent))
+            menu_options = ((u"è®¾ç½®", None, self.on_show, 0),
-                        (u"Reset GoAgent", None, self.on_restart_goagent))
+            menu_options = ((u"Config", None, self.on_show, 0),
-    max_check_ip_thread_num = config.CONFIG.getint("google_ip", "max_check_ip_thread_num") #20
+    auto_adjust_scan_ip_thread_num = config.CONFIG.getint("google_ip", "auto_adjust_scan_ip_thread_num")
-        logging.debug("sort ip time:%dms", time_cost) # 5ms for 1000 ip. 70~150ms for 30000 ip.
+        if time_cost > 30:
-        while self.searching_thread_count <= self.max_check_ip_thread_num:
+        while self.searching_thread_count <= self.scan_ip_thread_num:
-        while self.searching_thread_count < self.max_check_ip_thread_num:
+        while self.searching_thread_count < self.scan_ip_thread_num:
-        self.search_more_google_ip()
+        self.max_scan_ip_thread_num = num
-                self.user_special.scan_ip_thread_num = config.CONFIG.getint('google_ip', 'max_check_ip_thread_num')
+                self.user_special.scan_ip_thread_num = config.CONFIG.getint('google_ip', 'max_scan_ip_thread_num')
-                f.write("max_check_ip_thread_num = %d\n\n" % int(self.user_special.scan_ip_thread_num))
+            if int(self.user_special.auto_adjust_scan_ip_thread_num) != self.DEFAULT_CONFIG.getint('google_ip', 'auto_adjust_scan_ip_thread_num'):
-                    RemoteContralServerHandler.deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd, rc4_passwd], stdout=subprocess.PIPE)
+                    RemoteContralServerHandler.deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd, rc4_passwd])
-            wfile.write("HTTP/1.1 %d %s\r\n" % (response.status, response.reason))
+            wfile.write("HTTP/1.1 %d %s\r\n" % (response.status, response.reason))
-            ssl_sock = self.connect_ssl(self.ip)
+            ssl_sock,self.result.connct_time,self.result.handshake_time = connect_ssl(self.ip, timeout=self.timeout, openssl_context=self.openssl_context)
-    ssl = check.connect_ssl(ip_str)
+    ssl = connect_ssl(ip_str)
-    #test_gws("210.158.146.245")
+    #check_all_exist_ip()
-    def context_builder(ca_certs=None, cipher_suites=('ALL', '!aNULL', '!eNULL')):
+    def context_builder(ca_certs=None, cipher_suites=('ALL:!RC4-SHA:!ECDHE-RSA-RC4-SHA',)):
-        ssl_context.set_cipher_list('ECDHE-RSA-AES128-SHA')
+        ssl_context.set_cipher_list(':'.join(cipher_suites))
-        logging.debug("sort ip time:%d", time_cost) # 5ms for 1000 ip.
+        if time_cost < 30: return
-            logging.debug('GoAgent Web_control %s "%s %s ', self.address_string(), self.command, self.path)
+            logging.debug('GoAgent Web_control %s %s %s ', self.address_string(), self.command, self.path)
-            logging.error('PAC %s "%s %s ', self.address_string(), self.command, self.path)
+            logging.error('PAC %s %s %s ', self.address_string(), self.command, self.path)
-        logging.debug ('launcher web_control %s "%s %s ', self.address_string(), self.command, self.path)
+        logging.debug ('launcher web_control %s %s %s ', self.address_string(), self.command, self.path)
-            logging.debug('PHP Web_control %s "%s %s ', self.address_string(), self.command, self.path)
+            logging.debug('PHP Web_control %s %s %s ', self.address_string(), self.command, self.path)
-            logging.error('PAC %s "%s %s ', self.address_string(), self.command, self.path)
+            logging.error('PAC %s %s %s ', self.address_string(), self.command, self.path)
-    return json.dumps(jd)
+    return json.dumps(jd, sort_keys=True)
-    return json.dumps(jd)
+    return json.dumps(jd, sort_keys=True)
-    if last_no >= from_no:
+    if last_no > from_no:
-def get_last_lines(max_lines):
+def get_last_lines(max_lines): # Unused?
-        if server_type == '':
+        server_type = server_type.replace(" ", "_") # gvs 1.0
-    global connect_allow_time
+    global connect_allow_time, scan_allow_time
-    if wait_time < 0:
+    scan_time = scan_allow_time - time.time()
-        return "Blocked, %d seconds to wait." % wait_time
+    elif wait_time > 0:
-            if not connect_control.allow_connect():
+            if not connect_control.allow_scan():
-            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, ciphers='ECDHE-RSA-AES128-SHA',certfile=certfile, server_side=True)
+            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, certfile=certfile, server_side=True)
-            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, ciphers='ECDHE-RSA-AES128-SHA',certfile=certfile, server_side=True)
+            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, certfile=certfile, server_side=True)
-            logging.debug('check_appengine %s %s err:%s', self.ip, errno_str, e)
+            logging.exception('check_appengine %s %s err:%s', self.ip, errno_str, e)
-    result = test_app_head(ssl, ip_str)
+    result = test_app_check(ssl, ip_str)
-    result = test_app_head(ssl, ip_str)
+    result = test_app_check(ssl, ip_str)
-    request_data = 'HEAD /_gh/ HTTP/1.1\r\nHost: xxnet-100.appspot.com\r\n\r\n'
+    appid = appid_manager.get_appid()
-            #raise Exception("content fail")
+
-    result = check.check(callback=test_app_check, check_ca=True)
+    result = check.check(callback=test_app_head, check_ca=True)
-        result = check.check(callback=test_app_check)
+        result = check.check(callback=test_app_head)
-        check.result.appspot_ok = result
+
-    test_alive()
+    #test_alive()
-    #test("216.58.220.86", 10) #gws
+    #test("74.125.216.36", 10) #gvs
-    #check_all_exist_ip()
+    check_all_exist_ip()
-current_path = os.path.dirname(os.path.abspath(__file__))
+            ip_dict = sorted(self.ip_dict.items(),  key=lambda x: x[1]['handshake_time'])
-                for ip_str, property in self.ip_dict.items():
+                for ip_str, property in ip_dict:
-    pass
+    google_ip.save_ip_list(force=True)
-        req = opener.open(url)
+        req = opener.open(url, cafile="")
-                
+
-        ssl_context.set_cipher_list(':'.join(cipher_suites))
+        #ssl_context.set_cipher_list(':'.join(cipher_suites))
-            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, certfile=certfile, server_side=True)
+            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, ciphers='ECDHE-RSA-AES128-SHA',certfile=certfile, server_side=True)
-            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, certfile=certfile, server_side=True)
+            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, ciphers='ECDHE-RSA-AES128-SHA',certfile=certfile, server_side=True)
-    good_ip_file_name = "good_ip.txt"
+    if config.USE_IPV6:
-    default_good_ip_file = os.path.join(current_path, "good_ip.txt")
+    bad_ip_file = os.path.abspath( os.path.join(config.DATA_PATH, bad_ip_file_name))
-
+google_ip = Check_ip()
-# There for, gvs can't direct connect if GFW block some some of it.
+# There for, gvs can't direct connect if GFW block some some of it.
-        print "Error in configuration file:", exc
+        print( "Error in configuration file:", exc )
-        print "get_launcher_version_from_config:", e
+        print("get_launcher_version_from_config:", e)
-    print "launcher version:", launcher_version
+    print( "launcher version:", launcher_version)
-        set(["modules", module, "current_version"], current_version)
+        set(["modules", module, "current_version"], current_version)
-    sys_tray.serve_forever()
+    if config.get(["modules", "launcher", "show_systray"], 1):
-            data = '{ "check_update": "%s", "popup_webui": %d, "auto_start": %d, "php_enable": %d, "goagent_enable": %d }' %\
+            data = '{ "check_update": "%s", "popup_webui": %d, "show_systray": %d, "auto_start": %d, "php_enable": %d, "goagent_enable": %d }' %\
-            self.wfile.write(('HTTP/1.1 200\r\nAccess-Control-Allow-Origin: *\r\nCache-Control:public, max-age=31536000\r\nexpires: %s\r\nContent-Type: %s\r\nContent-Length: %s\r\n\r\n' % (tme, mimetype, len(data))).encode())
+            tme = (datetime.datetime.today()+datetime.timedelta(minutes=330)).strftime('%a, %d %b %Y %H:%M:%S GMT')
-                return 0
+    """æ£æ¥ipv6å°åçåæ³æ§"""
-                   "block_stat":connect_control.block_stat()
+                   "block_stat":connect_control.block_stat(),
-    keep_alive = config.CONFIG.getint("connect_manager", "https_keep_alive") #0
+    keep_alive = config.CONFIG.getint("connect_manager", "https_keep_alive") #1
-    max_check_ip_thread_num = config.CONFIG.getint("google_ip", "max_check_ip_thread_num") #5
+    max_check_ip_thread_num = config.CONFIG.getint("google_ip", "max_check_ip_thread_num") #20
-        self.not_exist_appids = []
+        self.reset_appid()
-            self.working_appid_list = config.GAE_APPIDS
+            self.reset_appid()
-            self.working_appid_list = config.GAE_APPIDS
+            self.reset_appid()
-        update_rule = config.get(["update", "check_update"], "dont-check")
+        update_rule = config.get(["update", "check_update"], "stable")
-    logging.info("==>%s", ip_str)
+def test_keep_alive(ip_str, interval=5):
-    print result
+    logging.info("first:%r", result)
-    print result
+    #print result
-    network_is_ok()
+    #network_is_ok()
-                sock_list = self.new_conn_pool.get_need_keep_alive(maxtime=230)
+                sock_list = self.new_conn_pool.get_need_keep_alive(maxtime=220)
-            ssl_version = "TLSv1"
+        global  ssl_version
-                break
+                time.sleep(10)
-                break
+                continue
-            #self.check_all_exist_ip()
+                # some ip can connect, and server type is gws
-            file_path = os.path.join(root_path, module, modules_versoin, url_path.split('/')[3:].join('/'))
+            modules_version = config.get(['modules', module, 'current_version'], None)
-    max_good_ip_num = config.CONFIG.getint("google_ip", "max_good_ip_num") #4000  # stop scan ip when enough
+    max_good_ip_num = config.CONFIG.getint("google_ip", "max_good_ip_num") #3000  # stop scan ip when enough
-            #self.check_exist_ip()
+            #self.check_all_exist_ip()
-    sys.path.append(win32_lib)
+    linux_lib = os.path.abspath( os.path.join(python_path, 'lib', 'linux'))
-            update_from_github.update_version(versions[1][1])
+    try:
-        logging.exception("download %s to %s fail:%r", url, file, e)
+        logging.warn("download %s to %s fail:%r", url, file, e)
-            self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1", ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
+            self.openssl_context = SSLConnection.context_builder(ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
-            self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1") #, ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
+            self.openssl_context = SSLConnection.context_builder() #, ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
-        self.timeout = 2
+        self.timeout = 4
-        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1", ca_certs=g_cacertfile)
+        self.openssl_context = SSLConnection.context_builder(ca_certs=g_cacertfile)
-    def context_builder(ssl_version='SSLv23', ca_certs=None, cipher_suites=('ALL', '!aNULL', '!eNULL')):
+    def context_builder(ca_certs=None, cipher_suites=('ALL', '!aNULL', '!eNULL')):
-            if start >= end:
+            if start > end:
-            if start >= end:
+            if start > end:
-pynotify.init('XX-Net Notify')
+try:
-
+    good_ip_file_name = "good_ip.txt"
-            file_path = good_ip_file
+        if os.path.isfile(self.good_ip_file):
-            file_path = default_good_ip_file
+            file_path = self.default_good_ip_file
-            with open(bad_ip_file, "r") as fd:
+        if os.path.isfile(self.bad_ip_file):
-            with open(good_ip_file, "w") as fd:
+            with open(self.good_ip_file, "w") as fd:
-            with open(bad_ip_file, "w") as fd:
+            with open(self.bad_ip_file, "w") as fd:
-google_ip = Check_ip()
+if config.USE_IPV6:
-                user_config.user_special.scan_ip_thread_num = int(self.postvars['scan_ip_thread_num'][0])
+                user_config.user_special.use_ipv6 = int(self.postvars['use_ipv6'][0])
-            google_ip.save_ip_list(force=True)
+            google_ip.save_ip_list(force=True)
-        if time.time() - time_request > 30 or time.time() < connect_allow_time:
+        if time.time() - time_request > 30:
-        send_to_broswer = True
+        send_to_browser = True
-            logging.warn("direct_handler.handler send response fail. t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
+            send_to_browser = False
-                if send_to_broswer:
+                if send_to_browser:
-                        send_to_broswer = False
+                        send_to_browser = False
-        time_start = time.time()
+        time_last_read = time.time()
-                logging.warn("read timeout t:%d len:%d left:%d %s %s", (time.time()-time_request)*1000, length, (end-start), host, url)
+            if start >= end:
-            if send_to_broswer:
+            if send_to_browser:
-                    send_to_broswer = False
+                    send_to_browser = False
-        if time.time() - time_request > 90: #time out
+        if time.time() - time_request > 30: #time out
-        wfile.write("\r\n")
+        send_to_browser = True
-        send_to_broswer = True
+        last_read_time = time.time()
-                logging.warn("read timeout t:%d len:%d left:%d %s", (time.time()-time_request)*1000, length, (end-start), url)
+            if start >= end:
-            if send_to_broswer:
+            if send_to_browser:
-                return
+                    send_to_browser = False
-
+        try:
-                    time_start = time.time()
+                    time_last_read = time.time()
-                                if time.time() - time_start > 20:
+                                if time.time() - time_last_read > 20:
-                #CertUtil.win32_notify(msg=u'Import GoAgent Ca finished, please restart browser.', title=u'Restart browser need.')
+            else:
-            logging.warn("direct_handler.hanler send response fail. t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
+            logging.warn("direct_handler.handler send response fail. t:%d e:%r %s/%s", time.time()-time_request, e, host, url)
-                    wfile.write('\r\n')
+
-ip_connect_interval = config.CONFIG.getint("google_ip", "ip_connect_interval") #5,10
+
-        if len(self.gws_ip_list) >= max_good_ip_num:
+        if len(self.gws_ip_list) >= self.max_good_ip_num:
-                if time.time() - get_time < ip_connect_interval:
+                if time.time() - get_time < self.ip_connect_interval:
-        if len(self.gws_ip_list) <= max_good_ip_num:
+        if len(self.gws_ip_list) <= self.max_good_ip_num:
-            while ip_num > max_good_ip_num:
+            while ip_num > self.max_good_ip_num:
-        while True: #not self.is_ip_enough() and self.searching_thread_count < 2:
+        while self.searching_thread_count <= self.max_check_ip_thread_num:
-        while self.searching_thread_count < max_check_ip_thread_num:
+        while self.searching_thread_count < self.max_check_ip_thread_num:
-
+    def load_range_content(self):
-            exit()
+            logging.error("load ip range %s fail", self.range_file)
-        for line in fd.readlines():
+
-        fd.close()
+        self.log_path = os.path.join(config.DATA_PATH, "scan_ip.log")
-        self.log_path = os.path.join(config.DATA_PATH, "scan_ip.log")
+from google_ip_range import ip_range
-        logging.warn("_request bad status line:%r", e)
+        logging.warn("direct_handler.fetch bad status line:%r", e)
-        logging.warn("_request:%r", e)
+        logging.warn("direct_handler.fetch:%r", e)
-        wfile.write("\r\n")
+        send_to_broswer = True
-                wfile.write('\r\n')
+                if send_to_broswer:
-            logging.warn("direct_handler err:%r %s %s time:%d", e, host, url, time_cost)
+            logging.exception("direct_handler err:%r %s %s time:%d", e, host, url, time_cost)
-
+    os._exit(0)
-opener = urllib2.build_opener()
+autoproxy = '127.0.0.1:8087'
-        logging.info("download %s to %s fail", url, file)
+    except Exception as e:
-
+        except OpenSSL.SysCallError as e:
-                logging.debug("fetch gae status:%s url:%s", response.app_status, url)
+                logging.warn("fetch gae status:%s url:%s", response.app_status, url)
-        time_start = time.time()
+
-                sock_list = self.gae_conn_pool.get_need_keep_alive(maxtime=200)
+                sock_list = self.new_conn_pool.get_need_keep_alive(maxtime=230)
-                    return
+    allow_truncated = False
-            response = urlfetch.fetch(url, body, fetchmethod, headers, allow_truncated=True, follow_redirects=False, deadline=timeout, validate_certificate=validate_certificate)
+            response = urlfetch.fetch(url, body, fetchmethod, headers, allow_truncated=allow_truncated, follow_redirects=False, deadline=timeout, validate_certificate=validate_certificate)
-            #time.sleep(1)
+            time.sleep(1)
-            time.sleep(1)
+            #time.sleep(1)
-    request_data = "GET / HTTP/1.1\r\nAccept: */*\r\nHost: %s\r\nConnection: Keep-Alive\r\n\r\n" % ip
+    request_data = "HEAD / HTTP/1.1\r\nAccept: */*\r\nHost: %s\r\n\r\n" % ip
-    ca_digest = 'sha1' if sys.platform == 'win32' and sys.getwindowsversion() < (6,) else 'sha256'
+    ca_digest = 'sha256'
-            continue
+        try:
-check_network_interval = 10
+check_network_interval = 100
-        conn = httplib.HTTPSConnection("code.jquery.com", 443, timeout=8)
+        conn = httplib.HTTPSConnection("code.jquery.com", 443, timeout=30)
-block_delay = (60 * 5)
+block_delay = 10 # (60 * 5)
-            raise e
+        wfile.write("%s: %s\r\n" % (keyword, value))
-            logging.warn("direct_handler err:%r %s %s", e, host, url)
+            logging.warn("direct_handler err:%r %s %s time:%d", e, host, url, time_cost)
-        sock.settimeout(90)
+        sock.settimeout(100)
-        if time.time() - time_request > 30: #time out
+        if time.time() - time_request > 90: #time out
-        return
+        time_except = time.time()
-            logging.warn("gae_handler err:%r %s ", e, url)
+            logging.warn("gae_handler err:%r time:%d %s ", e, time_cost, url)
-import math
+from scan_ip_log import scan_ip_log
-            if force_remove or self.ip_dict[ip_str]['timeout'] >= 2:
+            if force_remove or self.ip_dict[ip_str]['timeout'] >= 50:
-    def runJob(self):
+    def scan_ip_worker(self):
-                    logging.info("check_ip add ip:%s time:%d", ip_str, result.handshake_time)
+                    logging.info("scan_ip add ip:%s time:%d", ip_str, result.handshake_time)
-            p = threading.Thread(target = self.runJob)
+            p = threading.Thread(target = self.scan_ip_worker)
-    google_ip = Check_ip()
+google_ip = Check_ip()
-    check.search_more_google_ip()
+    google_ip.search_more_google_ip()
-    while not check.is_ip_enough():
+    while not google_ip.is_ip_enough():
-    test_mask()
+    pass
-                logging.error("load ip range:%s", line)
+                logging.warn("load ip range:%s fail", line)
-
+        # this function have bug, not fixed, and proved to be not a good idea.
-        if module not in ["launcher"]:
+        if module not in ["launcher", "php_proxy"]:
-    restart_xxnet()
+    try:
-from distutils.version import LooseVersion, StrictVersion
+from distutils.version import LooseVersion
-    
+
-    return False
+            if LooseVersion(version) < LooseVersion(filename):
-                   (config.get(["update", "check_update"], 1)
+            check_update = config.get(["update", "check_update"], 1)
-                if check_update != 0 and check_update != 1:
+                check_update = reqs['check_update'][0]
-                    config.set(["update", "check_update"], int(check_update))
+                    config.set(["update", "check_update"], check_update)
-def get_server_certificate(addr, ssl_version=PROTOCOL_SSLv3, ca_certs=None):
+def get_server_certificate(addr, ssl_version=PROTOCOL_SSLv23, ca_certs=None):
-    global checking_lock, checking_num
+    global checking_lock, checking_num, network_ok, last_check_time, check_network_interval
-        return False
+        return network_ok
-        conn = httplib.HTTPSConnection("www.baidu.com", 443, timeout=3)
+        conn = httplib.HTTPSConnection("code.jquery.com", 443, timeout=8)
-    logging.warn("network check to github fail.")
+    logging.warn("network fail.")
-        check.result.server_type = result
+        logging.debug("=======app check ok: %s", ip_str)
-import ip_utils
+
-    ip_range_manager = google_ip_range.ip_range()
+    #ip_range_manager = google_ip_range.ip_range()
-        ip_int = ip_range_manager.get_ip()
+        ip_int = ip_range.get_ip()
-from google_ip_range import ip_range
+    #test_main()
-    test("64.233.163.117")
+    #test("216.239.38.123")
-                ip_num = len(self.gws_ip_list)
+            ip_num = len(self.gws_ip_list)
-            if not self.network_is_ok():
+            if not check_ip.network_is_ok():
-                if not self.network_is_ok():
+                if not check_ip.network_is_ok():
-                if not self.network_is_ok():
+                if not check_ip.network_is_ok():
-    pool = {}
+    def __init__(self):
-        self.conn_pool = Connect_pool() #Queue.PriorityQueue()
+        self.new_conn_pool = Connect_pool()
-                sock_list = self.conn_pool.get_need_keep_alive(maxtime=200)
+                sock_list = self.gae_conn_pool.get_need_keep_alive(maxtime=200)
-                    if self.conn_pool.qsize() > max(1, len(appid_manager.working_appid_list)/2):
+                    if self.gae_conn_pool.qsize() > max(1, len(appid_manager.working_appid_list)/2):
-    def save_ssl_connection_for_reuse(self, ssl_sock):
+    def save_ssl_connection_for_reuse(self, ssl_sock, host=None):
-            t, ssl_sock = self.conn_pool.get_slowest()
+        if host:
-                self.conn_pool.put( (ssl_sock.handshake_time, ssl_sock) )
+                self.gae_conn_pool.put( (ssl_sock.handshake_time, ssl_sock) )
-        while self.thread_num < target_thread_num and self.conn_pool.qsize() < self.connection_pool_min_num:
+        need_conn_num = self.connection_pool_min_num - self.new_conn_pool.qsize()
-            while self.conn_pool.qsize() < self.connection_pool_min_num:
+            while self.new_conn_pool.qsize() < self.connection_pool_min_num:
-                    self.conn_pool.put((ssl_sock.handshake_time, ssl_sock))
+                    self.new_conn_pool.put((ssl_sock.handshake_time, ssl_sock))
-                break
+                    if time.time() - ssl_sock.last_use_time < 225: # gws ssl connection can keep for 230s after created
-                continue
+                if time.time() - ssl_sock.last_use_time < 225: # gws ssl connection can keep for 230s after created
-            self.create_more_connection()
+        self.create_more_connection()
-            ret = self.conn_pool.get(True, self.max_timeout)
+            ret = self.new_conn_pool.get(True, self.max_timeout)
-from direct_connect_manager import direct_connect_manager
+from connect_manager import https_manager
-    ssl_sock = direct_connect_manager.create_ssl_connection(host)
+    ssl_sock = https_manager.create_ssl_connection(host)
-            direct_connect_manager.save_ssl_connection_for_reuse(response.ssl_sock, host)
+            https_manager.save_ssl_connection_for_reuse(response.ssl_sock, host)
-                direct_connect_manager.save_ssl_connection_for_reuse(response.ssl_sock, host)
+                https_manager.save_ssl_connection_for_reuse(response.ssl_sock, host)
-import connect_manager
+from connect_manager import https_manager
-                   "connected_link":"%d,%d" % (len(connect_manager.https_manager.conn_pool.pool), len(direct_connect_manager.direct_connect_manager.new_conn_pool.pool)),
+                   "connected_link":"%d,%d" % (len(https_manager.new_conn_pool.pool), len(https_manager.gae_conn_pool.pool)),
-        data = connect_manager.https_manager.conn_pool.to_string()
+        data = https_manager.gae_conn_pool.to_string()
-            logging.exception("e:%r header: %s: %s ", e, keyword, value)
+            logging.warn("send_header e:%r header: %s: %s ", e, keyword, value)
-                data = response.read(8192)
+                try:
-            if response.app_status == 405: #Method not allowed
+            if response.app_status == 403 or response.app_status == 405: #Method not allowed
-    serving_pacfile = default_pacfile
+def get_serving_pacfile():
-        global serving_pacfile, user_pacfile
+
-
+
-        with open(serving_pacfile, 'rb') as fp:
+        with open(get_serving_pacfile(), 'rb') as fp:
-            autoproxy_content = base64.b64decode(content)
+            autoproxy_content = base64.b64decode(pac_content)
-        except:
+
-        if self.path.endswith('.pac?flush') or time.time() - os.path.getmtime(serving_pacfile) > config.PAC_EXPIRED:
+        if self.path.endswith('.pac?flush') or time.time() - os.path.getmtime(get_serving_pacfile()) > config.PAC_EXPIRED:
-        self.send_file(serving_pacfile, mimetype)
+        self.send_file(get_serving_pacfile(), mimetype)
-                    raise HoneypotError(' certficate is none')
+                    #raise HoneypotError(' certficate is none')
-                    connect_control.fall_into_honeypot()
+                    #google_ip.report_bad_ip(ssl_sock.ip)
-                    connect_control.fall_into_honeypot()
+                    #google_ip.report_bad_ip(ssl_sock.ip)
-                    google_ip.report_bad_ip(ip)
+                    google_ip.report_bad_ip(ssl_sock.ip)
-                    google_ip.report_bad_ip(ip)
+                    google_ip.report_bad_ip(ssl_sock.ip)
-                #ssl_sock.set_tlsext_host_name(server_hostname)
+                ssl_sock.set_tlsext_host_name(server_hostname)
-                    google_ip.report_bad_ip(ip)
+                    google_ip.report_bad_ip(ssl_sock.ip)
-                    google_ip.report_bad_ip(ip)
+                    google_ip.report_bad_ip(ssl_sock.ip)
-    password = ''
+class User_special(object):
-    proxy_passwd = ""
+        self.proxy_enable = "0"
-    def __init__(self):
+        self.scan_ip_thread_num = 0
-        CONFIG = ConfigParser.ConfigParser()
+
-                CONFIG.read(CONFIG_USER_FILENAME)
+                self.USER_CONFIG.read(CONFIG_USER_FILENAME)
-                self.password = CONFIG.get('gae', 'password')
+                self.user_special.host_appengine_mode = self.USER_CONFIG.get('hosts', 'appengine.google.com')
-                self.host_appengine_mode = CONFIG.get('hosts', 'appengine.google.com')
+                self.user_special.ip_connect_interval = config.CONFIG.getint('google_ip', 'ip_connect_interval')
-                self.ip_connect_interval = CONFIG.getint('google_ip', 'ip_connect_interval')
+                self.user_special.scan_ip_thread_num = config.CONFIG.getint('google_ip', 'max_check_ip_thread_num')
-            self.proxy_passwd = CONFIG.get('proxy', 'passwd')
+            self.user_special.proxy_enable = self.USER_CONFIG.get('proxy', 'enable')
-            if self.appid != "":
+            if self.user_special.appid != "":
-                f.write("password = %s\n\n" % self.password)
+                f.write("appid = %s\n" % self.user_special.appid)
-            if self.host_appengine_mode != "gae":
+            f.write("enable = %s\n" % self.user_special.proxy_enable)
-                f.write(".google.com = %s\n\n" % self.host_appengine_mode)
+                f.write("appengine.google.com = %s\n" % self.user_special.host_appengine_mode)
-                f.write("ip_connect_interval = %d\n\n" % int(self.ip_connect_interval))
+            if int(self.user_special.scan_ip_thread_num) != self.DEFAULT_CONFIG.getint('google_ip', 'max_check_ip_thread_num'):
-                data = json.dumps(user_config, default=lambda o: o.__dict__)
+                data = json.dumps(user_config.user_special, default=lambda o: o.__dict__)
-                user_config.ip_connect_interval = self.postvars['ip_connect_interval'][0]
+                user_config.user_special.appid = self.postvars['appid'][0]
-        for i in range(10):
+        for i in range(100):
-                if i < 9:
+                if i < 99:
-                    time.sleep(1)
+                    time.sleep(i)
-bad_ip_file = os.path.abspath( os.path.join(config.DATA_PATH, "bad_ip.txt"))
+bad_ip_file = os.path.abspath( os.path.join(config.DATA_PATH, "bad_ip2.txt"))
-    bad_ip_pool = {}
+    bad_ip_pool = set()
-                        if len(str_l) != 2:
+                        if line == "\n":
-                        self.bad_ip_pool[mask] = ip
+                        self.bad_ip_pool.add(ip)
-                    fd.write("%s %s\n" % (mask, ip))
+                for ip in self.bad_ip_pool:
-        self.bad_ip_pool[ip_mask] = ip_str
+        self.bad_ip_pool.add(ip_str)
-        if ip_mask in self.bad_ip_pool:
+        if ip_str in self.bad_ip_pool:
-            return
+            return return_fail_message(wfile)
-            return
+            return return_fail_message(wfile)
-    gltd = random.choice(['org', 'com', 'net', 'gov', 'cn'])
+    gltd = random.choice(['org', 'com', 'net', 'gov'])
-                    if self.conn_pool.qsize() > max(1, len(appid_manager.working_appid_list)/5):
+                    if self.conn_pool.qsize() > max(1, len(appid_manager.working_appid_list)/2):
-                #self.create_more_connection()
+                self.create_more_connection()
-                #ssl_sock.set_tlsext_host_name(server_hostname)
+                ssl_sock.set_tlsext_host_name(server_hostname)
-
+    #connect_allow_time = time.time() + block_delay
-            connect_allow_time = time.time() + (60 * 5)
+        if time.time() - connect_fail_time > 60:
-            autoproxy_content = base64.b64decode(opener.open(config.PAC_GFWLIST).read())
+            try:
-        update_content = opener.open(req_url).read()
+        try:
-            data = '{"res":"success"}'
+                if google_ip.add_ip(ip, 100, "google.com", "gws"):
-
+import ip_utils
-            return self.req_importip_handler()
+        elif path.startswith("/importip"):
-                #
+            addresses = ip_list.split('|')
-            pass #ææ¶æ²¡å
+            data = '{"res":"'
-            pass #ææ¶æ²¡å
+            ip_list = self.postvars['ipList'][0]
-                    my_stdout.write("Retry max time, failed.\n\n" ))
+                    my_stdout.write("Retry max time, failed.\n\n" )
-    def check(self, callback=None, check_ca=False, close_ssl=True):
+    def check(self, callback=None, check_ca=True, close_ssl=True):
-                    raise socket.error(' certficate is none')
+                    raise HoneypotError(' certficate is none')
-                    raise socket.error(' certficate is issued by %r, not Google' % ( issuer_commonname))
+                    raise HoneypotError(' certficate is issued by %r, not Google' % ( issuer_commonname))
-
+        except HoneypotError as e:
-connect_fail_time = 0
+import connect_control
-        if time.time() < connect_allow_time:
+        if not connect_control.allow_connect():
-                    connect_allow_time = time.time() + (60 * 5)
+                    connect_control.fall_into_honeypot()
-                    connect_allow_time = time.time() + (60 * 5)
+                    connect_control.fall_into_honeypot()
-            connect_fail_time = 0
+            connect_control.report_connect_success()
-                    connect_allow_time = time.time() + (60 * 5)
+            connect_control.report_connect_fail()
-                elif time.time() < connect_allow_time:
+                elif not connect_control.allow_connect():
-from connect_manager import connect_allow_time, connect_fail_time
+import connect_control
-        while self.thread_num < target_thread_num and self.new_conn_pool.qsize() < self.connection_pool_min_num:
+        while self.thread_num < target_thread_num and self.new_conn_pool.qsize() < self.connection_pool_min_num and connect_control.allow_connect():
-            return False
+        if not connect_control.allow_connect():
-                    connect_allow_time = time.time() + (60 * 5)
+                    connect_control.fall_into_honeypot()
-                    connect_allow_time = time.time() + (60 * 5)
+                    connect_control.fall_into_honeypot()
-            connect_fail_time = 0
+            connect_control.report_connect_success()
-
+            connect_control.report_connect_fail()
-                elif time.time() < connect_allow_time:
+                elif not connect_control.allow_connect():
-from connect_manager import connect_allow_time
+from connect_control import connect_allow_time, connect_fail_time
-
+import socket
-
+            if not connect_control.allow_connect():
-                if self.check_ip(ip_str):
+                if self.is_bad_ip(ip_str):
-
+
-
+from connect_control import connect_allow_time, connect_fail_time
-
+import time
-                   "ip_connect_interval":config.CONFIG.getint("google_ip", "ip_connect_interval")
+                   "ip_connect_interval":config.CONFIG.getint("google_ip", "ip_connect_interval"),
-            else:
+            elif v == "direct":
-
+                #ssl_sock.set_tlsext_host_name(server_hostname)
-
+            if connect_fail_time == 0:
-    return 'www.%s.%s' % (word, gltd)
+from connect_manager import Connect_pool, random_hostname
-                ssl_sock.set_tlsext_host_name(server_hostname)
+                #ssl_sock.set_tlsext_host_name(server_hostname)
-            logging.debug("create_ssl update ip:%s time:%d", ip, handshake_time)
+            logging.debug("direct create_ssl update ip:%s time:%d", ip, handshake_time)
-            logging.debug("create_ssl %s fail:%s cost:%d h:%d", ip, e, time_cost * 1000, handshake_time)
+            logging.debug("direct_create_ssl %s fail:%s cost:%d h:%d", ip, e, time_cost * 1000, handshake_time)
-                logging.debug("create ssl timeout fail.")
+                logging.debug("create ssl for direct timeout.")
-
+from gae_handler import generate_message_html, send_response
-    return string.Template(MESSAGE_TEMPLATE).substitute(title=title, banner=banner, detail=detail)
+    elif keyword == "Alternate-Protocol":
-        headers['Connection'] = 'close'
+        #logging.debug("Head1 %s: %s", keyword, value)
-    wfile.write(body)
+    global connect_allow_time
-            html = generate_message_html('504 GoAgent Proxy Time out', u'å¢å¤ªé«äºï¼ç¿»ä¸è¿å»ï¼åä¼æ¯ä¸ä¼åæ¥ï¼')
+        if time.time() - time_request > 30 or time.time() < connect_allow_time:
-            logging.exception('direct_handler.handler %r %s , retry...', e, url)
+            logging.exception('direct_handler.handler %r %s %s , retry...', e, host, url)
-            logging.info("DIRECT t:%d %d %s", (time.time()-time_request)*1000, response.status, url)
+            logging.info("DIRECT t:%d %d %s %s", (time.time()-time_request)*1000, response.status, host, url)
-            logging.info("DIRECT chucked t:%d s:%d %d %s", (time.time()-time_request)*1000, length, response.status, url)
+            logging.info("DIRECT chucked t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
-                logging.warn("read timeout t:%d len:%d left:%d %s", (time.time()-time_request)*1000, length, (end-start), url)
+                logging.warn("read timeout t:%d len:%d left:%d %s %s", (time.time()-time_request)*1000, length, (end-start), host, url)
-                        logging.warn('direct_handler send to browser return %r %r', e_b, url)
+                        logging.warn('direct_handler send to browser return %r %s %r', e_b, host, url)
-                        logging.warn('direct_handler send to browser return %r %r', e_b, url)
+                        logging.warn('direct_handler send to browser return %r %s %r', e_b, host, url)
-                logging.info("DIRECT t:%d s:%d %d %s", (time.time()-time_request)*1000, length, response.status, url)
+                logging.info("DIRECT t:%d s:%d %d %s %s", (time.time()-time_request)*1000, length, response.status, host, url)
-            logging.exception("direct_handler err:%r %s ", e, url)
+            logging.warn("direct_handler err:%r %s %s", e, host, url)
-            logging.exception("direct_handler except:%r %s", e, url)
+            logging.exception("direct_handler except:%r %s %s", e, host, url)
-        logging.exception("direct_handler except:%r %s", e, url)
+        logging.exception("direct_handler except:%r %s %s", e, host, url)
-
+        if os.path.isfile(bad_ip_file):
-        #ip_mask = ip_utils.ip_num_to_string(ip_bin)
+        self.save_ip_list(force=True)
-        if host in config.HOSTS_FWD:
+        if host in config.HOSTS_FWD or host in config.HOSTS_DIRECT:
-        if host.endswith(config.HOSTS_FWD_ENDSWITH):
+        if host.endswith(config.HOSTS_FWD_ENDSWITH) or host.endswith(config.HOSTS_DIRECT_ENDSWITH):
-        if host in config.HOSTS_FWD:
+        if host in config.HOSTS_DIRECT:
-        if host.endswith(config.HOSTS_FWD_ENDSWITH):
+        if host.endswith(config.HOSTS_DIRECT_ENDSWITH):
-            return self.do_CONNECT_AGENT()
+
-    return "%s.appspot.com" % word
+    #return "%s.appspot.com" % word
-    def create_ssl_connection(self):
+    def create_ssl_connection(self, host=''):
-                if conn_time * 1000 < 300:
+                if conn_time * 1000 < 400:
-            html = generate_message_html('504 GoAgent Proxy Time out', u'GoAgentä»£çå¤çè¶æ¶ï¼è¯·æ¥çæ¥å¿ï¼')
+            html = generate_message_html('504 GoAgent Proxy Time out', u'å¢å¤ªé«äºï¼ç¿»ä¸è¿å»ï¼åä¼æ¯ä¸ä¼åæ¥ï¼')
-
+import direct_handler
-        """handle CONNECT cmmand, socket forward or deploy a fake cert"""
+        """handle CONNECT command, socket forward or deploy a fake cert"""
-                   "connected_link":"%d" % len(connect_manager.https_manager.conn_pool.pool),
+                   "connected_link":"%d,%d" % (len(connect_manager.https_manager.conn_pool.pool), len(direct_connect_manager.direct_connect_manager.new_conn_pool.pool)),
-    check = Check_frame(ip_str)
+    check = Check_frame(ip_str, check_cert=False)
-    test_multi_thread_search_ip()
+    test("64.233.163.117")
-    keep_alive = False
+    keep_alive = config.CONFIG.getint("connect_manager", "https_keep_alive") #0
-        self.timeout = 1.5
+        self.timeout = 2
-        self.keep_alive = True
+
-                self.create_more_connection()
+                #self.create_more_connection()
-            #    ssl_sock.set_tlsext_host_name(server_hostname)
+            server_hostname = random_hostname()
-            logging.debug("create_ssl %s fail:%s c:%d h:%d", ip, e, connect_time, handshake_time)
+            time_cost = time.time() - time_begin
-            logging.exception("gae_exception:%s %r", e, url)
+            logging.warn("gae_exception:%r %s", e, url)
-default_good_ip_fie = os.path.join(current_path, "good_ip.txt")
+default_good_ip_file = os.path.join(current_path, "good_ip.txt")
-            file_path = default_good_ip_fie
+            file_path = default_good_ip_file
-
+def test_mask():
-    test_random()
+    #test_random()
-# if GFW block some of the ip, direct connect to these domain if fail.
+# if GFW block some of the ip, direct connect to these domain if fail.test_mask
-            target_menu = 'status'
+            if config.get(['modules', 'goagent', 'auto_start'], 0) == 1:
-            data = '{ "check_update": "%d", "popup_webui": %d, "auto_start": %d, "php_enable": %d }' %\
+            data = '{ "check_update": "%d", "popup_webui": %d, "auto_start": %d, "php_enable": %d, "goagent_enable": %d }' %\
-                    , config.get(["modules", "php_proxy", "auto_start"], 0))
+                    , config.get(["modules", "php_proxy", "auto_start"], 0)
-                    my_stdout.write("Retry max time, failed.\n\n" % (i + 1))
+                    my_stdout.write("Retry max time, failed.\n\n" ))
-        if module not in ["launcher", "php_proxy"]:
+        if module not in ["launcher"]:
-    #eval("conf = {}")
+    #eval("conf = {}")
-            if config.get(["modules", module, "auto_start"], 1) != 1:
+            if module != "launcher" and config.get(["modules", module, "auto_start"], 0) != 1:
-            set(["modules", module, "auto_start"], 1)
+        if module not in ["launcher", "php_proxy"]:
-        return False
+    except Exception as e:
-            logging.warn("gae_exception:%s %r", e, url)
+            logging.exception("gae_exception:%s %r", e, url)
-                module_menus[module] = module_menu
+            if config.get(["modules", module, "auto_start"], 1) != 1:
-                lines[i] = "__password__ = '" + rc4_password + "'\n"
+                lines[i] = "__password__ = '%s'\n" % rc4_password
-            module_menus[module] = module_menu
+            if config.get(["modules", module, "auto_start"], 1) == 1:
-                    data = '{"res":"fail, php_enable:%s"}' % auto_start
+                    data = '{"res":"fail, php_enable:%s"}' % php_enable
-                    RemoteContralServerHandler.deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd, rc4_passwd], stdout = subprocess.PIPE)
+                    RemoteContralServerHandler.deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd, rc4_passwd], stdout=subprocess.PIPE)
-def getpass_getpass(prompt = 'Password:', stream = None):
+def getpass_getpass(prompt='Password:', stream=None):
-    socket.create_connection(('127.0.0.1', 8087), timeout = 1).close()
+    socket.create_connection(('127.0.0.1', 8087), timeout=1).close()
-                result = appcfg.AppCfgApp(['appcfg', 'rollback', dirname], password_input_fn = getpass_getpass, raw_input_fn = my_input, error_fh = my_stdout).Run()
+                result = appcfg.AppCfgApp(['appcfg', 'rollback', dirname], password_input_fn=getpass_getpass, raw_input_fn = my_input, error_fh = my_stdout).Run()
-                result = appcfg.AppCfgApp(['appcfg', 'update', dirname], password_input_fn = getpass_getpass, raw_input_fn = my_input, error_fh = my_stdout).Run()
+                result = appcfg.AppCfgApp(['appcfg', 'update', dirname], password_input_fn=getpass_getpass, raw_input_fn = my_input, error_fh = my_stdout).Run()
-def edit_gae_py(rc4_password):
+def update_rc4_password(rc4_password):
-    edit_gae_py(rc4_password)
+    update_rc4_password(rc4_password)
-    edit_gae_py('')
+    update_rc4_password('')
-        my_stdout.write("Usage: uploader.py <appids> <email> [password]\r\n")
+        my_stdout.write("Usage: uploader.py <appids> <email> [password] [rc4_password]\r\n")
-            data = '{ "check_update": "%d", "popup_webui": %d, "auto_start": %d }' %\
+            data = '{ "check_update": "%d", "popup_webui": %d, "auto_start": %d, "php_enable": %d }' %\
-                    , config.get(["modules", "launcher", "auto_start"], 0))
+                    , config.get(["modules", "launcher", "auto_start"], 0)
-def getpass_getpass(prompt='Password:', stream=None):
+def getpass_getpass(prompt = 'Password:', stream = None):
-    socket.create_connection(('127.0.0.1', 8087), timeout=1).close()
+    socket.create_connection(('127.0.0.1', 8087), timeout = 1).close()
-                result =  appcfg.AppCfgApp(['appcfg', 'rollback', dirname], password_input_fn=getpass_getpass, raw_input_fn=my_input, error_fh=my_stdout).Run()
+                result = appcfg.AppCfgApp(['appcfg', 'rollback', dirname], password_input_fn = getpass_getpass, raw_input_fn = my_input, error_fh = my_stdout).Run()
-                result =  appcfg.AppCfgApp(['appcfg', 'update', dirname], password_input_fn=getpass_getpass, raw_input_fn=my_input, error_fh=my_stdout).Run()
+                result = appcfg.AppCfgApp(['appcfg', 'update', dirname], password_input_fn = getpass_getpass, raw_input_fn = my_input, error_fh = my_stdout).Run()
-    gae_path = os.path.join(code_path, "gae", "gae.py")
+    gae_file_name = os.path.join(code_path, "gae", "gae.py")
-        gae_obj.close()
+        with open(gae_file_name, 'r') as fgae:
-        gae_obj.close()
+        with open(gae_file_name, 'w') as fgae:
-                    RemoteContralServerHandler.deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd], stdout=subprocess.PIPE)
+                    RemoteContralServerHandler.deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd, rc4_passwd], stdout = subprocess.PIPE)
-def uploads(appids, email, password):
+"""èªå¨ä¿®æ¹gae.pyä¸­çRC4å¯ç å­æ®µ"""
-    if len(sys.argv) <3:
+    if len(sys.argv) < 3:
-    if len(sys.argv) == 4:
+
-    uploads(appids, email, password)
+    if len(sys.argv) >= 5:
-                    gae_path = os.path.join(root_path, "server", "gae", "gae.py")
+                    gae_path = os.path.join(root_path, "goagent", config.__version__, "server", "gae", "gae.py")
-                    out_obj.close()
+                    gae_path = os.path.join(root_path, "server", "gae", "gae.py")
-    ca_vendor = 'PHP_proxy'
+    ca_vendor = 'PHP_proxy' #TODO: here should be XX-Net
-        subj.commonName = '%s XX-Net' % CertUtil.ca_vendor
+        subj.commonName = '%s XX-Net' % CertUtil.ca_vendor #TODO: here should be PHP_proxy
-        commonname = "PHP_proxy XX-Net - PHP_proxy"
+        commonname = "PHP_proxy XX-Net - PHP_proxy" #TODO: here should be PHP_proxy - XX-Net
-    #test_cmd()
+
-from cert_util import CertUtil
+    if common.CONTROL_ENABLE:
-        p.start()
+        CertUtil.init_ca()
-    ca_vendor = 'GoAgent'
+    ca_vendor = 'GoAgent' #TODO: here should be XX-Net
-        subj.commonName = '%s XX-Net' % CertUtil.ca_vendor
+        subj.commonName = '%s XX-Net' % CertUtil.ca_vendor #TODO: here should be GoAgent
-        commonname = "GoAgent XX-Net"
+        commonname = "GoAgent XX-Net" #TODO: need check again
-        commonname = "GoAgent XX-Net - GoAgent"
+        commonname = "GoAgent XX-Net - GoAgent" #TODO: here should be GoAgent - XX-Net
-    print out[0]
+
-    #test_cmd()
+
-        yield format_response(403, {'Content-Type': 'text/html; charset=utf-8'}, message_html('403 Hosts Deny', 'Hosts Deny(%r)' % netloc, detail='å±ç¨appidå ä¸ºèµæºæéï¼éå¶è§çè§é¢åæä»¶ä¸è½½ç­æ¶èèµæºè¿å¤çè®¿é®ï¼è¯·ä½¿ç¨èªå·±çappid <a href=" https://github.com/XX-net/XX-Net/wiki/Register-Google-appid" target="_blank">å¸®å©</a> '))
+        yield format_response(403, {'Content-Type': 'text/html; charset=utf-8'}, message_html('403 Hosts Deny', 'Hosts Deny(%r)' % netloc, detail='å¬ç¨appidå ä¸ºèµæºæéï¼éå¶è§çè§é¢åæä»¶ä¸è½½ç­æ¶èèµæºè¿å¤çè®¿é®ï¼è¯·ä½¿ç¨èªå·±çappid <a href=" https://github.com/XX-net/XX-Net/wiki/Register-Google-appid" target="_blank">å¸®å©</a> '))
-import collections
+import logging
-        # load ../../../data/goagent/config.ini
+        # load ../../../data/goagent/config.ini, set by web_ui
-
+        # load ../../../data/goagent/manual.ini, set by manual
-        buffer[last_no] = '<font color="%s">%s</font>' % (html_color, string)
+        buffer[last_no] = string
-            if host.endswith(".google.com") or host.endswith(config.HOSTS_FWD_ENDSWITH):
+            if host.endswith(".google.com") or host.endswith(config.HOSTS_FWD_ENDSWITH) or host.endswith(config.HOSTS_GAE_ENDSWITH):
-        self.send_response('application/json', data)
+        self.send_response('text/html', data)
-                user_config.password = self.postvars['passwd'][0]
+                user_config.password = self.postvars['password'][0]
-                self.send_response('application/json', data)
+                self.send_response('text/html', data)
-        self.send_response('application/json', data)
+        self.send_response('text/html', data)
-        self.send_response('application/json', data)
+        self.send_response('text/html', data)
-                    my_stdout.write("Retry again.\n\n")
+                    my_stdout.write("Retry %d time...\n\n" % (i + 1))
-            self.send_response('application/json', '{"status":"success"}')
+            self.send_response('text/html', '{"status":"success"}')
-        self.send_response('application/json', data)
+        self.send_response('text/html', data)
-        self.send_response("application/json", data)
+        self.send_response("text/html", data)
-                        (u"å¨å±PACæºè½ä»£çåæ¢", None, self.on_enable_pac),
+                        (u"å¨å±éè¿GoAgentä»£ç", None, self.on_enable_proxy),
-        buffer[last_no] = '<font color="%s">%s</font>' % (html_color, string)
+        buffer[last_no] = string
-                self.send_response('application/json', data)
+                self.send_response('text/html', data)
-        self.send_response('application/json', data)
+        self.send_response('text/html', data)
-        #('en_GB', 'cp1252'), en_US,
+        if hasattr(self, "lang_code"):
-    max_timeout = 20
+    max_timeout = 60
-                ctime, sock = self.tcp_connection_cache.get(timeout=0.3)
+                ctime, sock = self.tcp_connection_cache.get(timeout=0.2)
-            if 'X-Head-Content-Length' in response_headers:
+        if 'X-Head-Content-Length' in response_headers:
-                del response_headers['X-Head-Content-Length']
+            del response_headers['X-Head-Content-Length']
-                self.report_connect_fail(ip_str, force_remove=True)
+                self.report_connect_fail(ip_str)
-            if not "127.0.0.1" in netloc:
+            if not netloc.startswith("127.0.0.1") and not netloc.startswitch("localhost"):
-            if not "127.0.0.1" in netloc:
+            if not netloc.startswith("127.0.0.1") and not netloc.startswitch("localhost"):
-                    appid = self.postvars['appid'][0]
+
-            if not "127.0.0.1" in netloc:
+            if not netloc.startswith("127.0.0.1") and not netloc.startswitch("localhost"):
-            if not "127.0.0.1" in netloc:
+            if not netloc.startswith("127.0.0.1") and not netloc.startswitch("localhost"):
-      return 1
+      raise e
-    self.reason = args["Error"]
+    self.app_reason = args["Error"]
-        if e.reason == "BadAuthentication":
+        if hasattr(e, 'app_reason') and e.app_reason == "BadAuthentication":
-        if e.reason == "CaptchaRequired":
+        if hasattr(e, 'app_reason') and e.app_reason == "CaptchaRequired":
-        if e.reason == "NotVerified":
+        if hasattr(e, 'app_reason') and e.app_reason == "NotVerified":
-        if e.reason == "TermsNotAgreed":
+        if hasattr(e, 'app_reason') and e.app_reason == "TermsNotAgreed":
-        if e.reason == "AccountDeleted":
+        if hasattr(e, 'app_reason') and e.app_reason == "AccountDeleted":
-        if e.reason == "AccountDisabled":
+        if hasattr(e, 'app_reason') and e.app_reason == "AccountDisabled":
-        if e.reason == "ServiceDisabled":
+        if hasattr(e, 'app_reason') and e.app_reason == "ServiceDisabled":
-        if e.reason == "ServiceUnavailable":
+        if hasattr(e, 'app_reason') and e.app_reason == "ServiceUnavailable":
-        self.fd.write(time_string + message)
+        out_msg = time_string + message
-    filename = os.path.join(dirname, 'app.yaml')
+    app_yaml_file = os.path.join(dirname, 'app.yaml')
-    with open(filename, 'wb') as fp:
+    with open(app_yaml_file, 'wb') as fp:
-        pass
+        for i in range(10):
-
+def clean_cookie_file():
-        upload(appid, email, password)
+def uploads(appids, email, password):
-        pass
+        for appid in appids.split('|'):
-        logo_filename = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'python.png')
+        logo_filename = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'web_ui', 'favicon.ico')
-        icon_path = os.path.join(current_path, "Python.ico")
+        icon_path = os.path.join(current_path, "web_ui", "favicon.ico")
-        icon_path = os.path.join(os.path.dirname(__file__), "python.ico")
+        icon_path = os.path.join(os.path.dirname(__file__), "web_ui", "favicon.ico")
-#                        ("æ£æ¥", None, self.on_check_update),
+        import locale
-    print RemoteContralServerHandler.xxnet_version()
+    print RemoteContralServerHandler.xxnet_version()
-    print RemoteContralServerHandler.xxnet_version()
+    print RemoteContralServerHandler.xxnet_version()
-            start += send_size
+            sended = sock.send(payload[start:start+send_size])
-        if len(body) > 65535:
+        if len(body) > 10 * 1024 * 1024:
-    def __init__(self, ip):
+    def __init__(self, ip, check_cert=True):
-        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1", ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
+        self.check_cert = check_cert
-                if not issuer_commonname.startswith('Google'):
+                if self.check_cert and not issuer_commonname.startswith('Google'):
-                logging.info("CN:%s", ssl_cert.cn)
+                logging.info("%s CN:%s", self.ip, ssl_cert.cn)
-                ip_int = ip_range.random_get_ip()
+                ip_int = ip_range.get_ip()
-        for i in range(50):
+        for i in range(20):
-    check_all_exist_ip()
+    #test("194.78.99.84")
-        sock.send(request_data.encode() + payload)
+        sock.send(request_data.encode())
-    for i in xrange(max_retry):
+    while True:
-            return
+        if not force_remove:
-            if time.time() - fail_time < 1:
+            if not force_remove and time.time() - fail_time < 1:
-                self.try_remove_thread()
+                if not force_remove:
-    CertUtil.init_ca()
+        elif path == "/is_ready":
-    #config.load()
+
-    time.sleep(10)
+    time.sleep(1)
-        req.sign(key, 'sha1')
+        req.sign(key, CertUtil.ca_digest)
-        ca.sign(key, 'sha1')
+        ca.sign(key, CertUtil.ca_digest)
-        req.sign(pkey, 'sha1')
+        req.sign(pkey, CertUtil.ca_digest)
-        cert.sign(key, 'sha1')
+        cert.sign(key, CertUtil.ca_digest)
-        req.sign(key, 'sha1')
+        req.sign(key, CertUtil.ca_digest)
-        ca.sign(key, 'sha1')
+        ca.sign(key, CertUtil.ca_digest)
-        req.sign(pkey, 'sha1')
+        req.sign(pkey, CertUtil.ca_digest)
-        cert.sign(key, 'sha1')
+        cert.sign(key, CertUtil.ca_digest)
-        logging.exception("_request:%r", e)
+        logging.warn("_request:%r", e)
-                logging.debug("fetch gae status:%d url:%s", response.app_status, url)
+                logging.debug("fetch gae status:%s url:%s", response.app_status, url)
-        if not config.get(["update", "check_update"]):
+        if not config.get(["update", "check_update"], 1):
-                payload = self.rfile.read(int(request_headers.get('Content-Length', 0)))
+                payload_len = int(request_headers.get('Content-Length', 0))
-
+    gae_support_methods = tuple(["GET", "POST", "HEAD", "PUT", "DELETE", "PATCH"])
-        raise
+        config.PROXY_ENABLE = 0
-        self.wfile.write("HTTP/1.1 %d\r\n" % self.response.status)
+        self.wfile.write("HTTP/1.1 200 OK\r\n")
-            logging.debug("Head %s: %s", key.title(), value)
+            #logging.debug("Head %s: %s", key.title(), value)
-                self.wfile.write(data)
+                ret = self.wfile.write(data)
-                logging.info('RangeFetch client closed(%s). %s', e, self.url)
+                logging.warn('RangeFetch client closed(%s). %s', e, self.url)
-            if time < fastest_time:
+            if time < fastest_time or not fastest_sock:
-        p.start()
+        if self.keep_alive:
-        sock.settimeout(30)
+        sock.settimeout(90)
-        logging.warn("_request bad status line")
+    except httplib.BadStatusLine as e:
-        wfile.write("HTTP/1.1 %d\r\n" % response.status)
+        wfile.write("HTTP/1.1 %d %s\r\n" % (response.status, response.reason))
-    def check(self, callback=None, check_ca=False):
+        self.timeout = 5
-        openssl_context = SSLConnection.context_builder(ssl_version="TLSv1", ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
+    def connect_ssl(self, ip):
-            ssl_sock = connect_ssl(self.ip)
+            ssl_sock = self.connect_ssl(self.ip)
-            logging.debug("Check %s IOError:%s", self.ip, e)
+            logging.warn("Check %s IOError:%s", self.ip, e)
-            if ssl_sock:
+            if ssl_sock and close_ssl:
-    logging.info("test %s app %s", ip_str, result)
+    logging.info("test_with_app %s app %s", ip_str, result)
-            test_with_app(ip_str)
+            #test_with_app(ip_str)
-    test("127.0.0.1")
+    #test("127.0.0.1")
-
+    check_all_exist_ip()
-            request_data = 'HEAD /_gh/ HTTP/1.1\r\nHost: %s\r\n\r\n' % host
+    def head_request(self, ssl_sock):
-                ssl_sock.settimeout(2)
+        # public appid don't keep alive, for quota limit.
-                response = httplib.HTTPResponse(ssl_sock, buffering=True)
+        #logging.debug("head request %s", host)
-                response.begin()
+        request_data = 'HEAD /_gh/ HTTP/1.1\r\nHost: %s\r\n\r\n' % host
-            finally:
+        response = None
-        while True:
+    def keep_alive_thread(self):
-                sock_list = self.conn_pool.get_need_keep_alive()
+                sock_list = self.conn_pool.get_need_keep_alive(maxtime=200)
-                    inactive_time = time.time() -ssl_sock.last_use_time
+                    #inactive_time = time.time() -ssl_sock.last_use_time
-                    if head_request(ssl_sock):
+                    if self.head_request(ssl_sock):
-                logging.exception("keep alive except:%r", e)
+                logging.warn("keep alive except:%r", e)
-    print sock
+    #sock = forwork_manager.create_connection()
-                logging.warn("google_ip.runJob fail:%s", e)
+                logging.exception("google_ip.runJob fail:%s", e)
-            id_2 = random.randint(0, ip_range[1] - ip_range[0] - 1)
+            #logging.debug("random.randint %d - %d", ip_range[0], ip_range[1])
-            end = prefix + "." + end
+        num_regions = strline.split(".")
-
+        self.host_appengine_mode = "gae"
-                user_config.host_appengine_mode = self.postvars['host_appengine'][0]
+                user_config.host_appengine_mode = self.postvars['host_appengine_mode'][0]
-    host_appengine = "gae"
+    host_appengine_mode = "gae"
-                self.host_appengine = CONFIG.get('hosts', 'appengine.google.com')
+                self.host_appengine_mode = CONFIG.get('hosts', 'appengine.google.com')
-            if self.host_appengine != "gae":
+            if self.host_appengine_mode != "gae":
-                f.write("appengine.google.com = %s\n\n" % self.host_appengine)
+                f.write("appengine.google.com = %s\n\n" % self.host_appengine_mode)
-            if self.ip_connect_interval != 10:
+            if self.ip_connect_interval != "":
-                   "pac_url":config.pac_url}
+                   "pac_url":config.pac_url,
-                user_config.host_appengine = self.postvars['host_appengine'][0]
+                user_config.host_appengine_mode = self.postvars['host_appengine'][0]
-        version = config.config["modules"][module]["current_version"]
+        version = config.get(["modules", module, "current_version"], "")
-    config.config["modules"]["launcher"]["current_version"] = new_config["modules"]["launcher"]["current_version"]
+    #TODO: fix bug
-    if current_path != config.config["update"]["last_path"] or node_id != config.config["update"]["node_id"]:
+    if current_path != config.get(["update", "last_path"]):
-    config.config["modules"][module]["current_version"] = str(new_version)
+    config.set(["modules", module, "current_version"], str(new_version))
-    config.config["modules"][module]["ignore_version"] = str(new_version)
+    config.set(["modules", module, "ignore_version"], str(new_version))
-        if not config.config["update"]["check_update"]:
+        if not config.get(["update", "check_update"]):
-                current_version = config.config["modules"][module]["current_version"]
+                current_version = config.get(["modules", module, "current_version"])
-        config.config["update"]["last_path"] = current_path
+    if current_path != config.get(["update", "last_path"], ""):
-    config.config["update"]["uuid"] = xx_net_uuid
+    config.set(["update", "uuid"], xx_net_uuid)
-    xx_net_uuid = config.config["update"]["uuid"]
+    xx_net_uuid = config.get(["update", "uuid"])
-        if host in config.HOSTS_GAE or host.endswith(config.HOSTS_GAE_ENDSWITH):
+        if host in config.HOSTS_GAE:
-        if host in config.HOSTS_FWD or host.endswith(config.HOSTS_FWD_ENDSWITH):
+        if host in config.HOSTS_FWD:
-        else:
+
-        if host in config.HOSTS_GAE or host.endswith(config.HOSTS_GAE_ENDSWITH):
+        if host in config.HOSTS_GAE:
-        if host in config.HOSTS_FWD or host.endswith(config.HOSTS_FWD_ENDSWITH):
+        if host.endswith(config.HOSTS_FWD_ENDSWITH):
-            logging.debug("head request %s", host)
+            #logging.debug("head request %s", host)
-                    logging.debug("inactive_time:%d", inactive_time)
+                    #logging.debug("inactive_time:%d", inactive_time)
-                    config.config["update"]["check_update"] = int(check_update)
+                    config.set(["update", "check_update"], int(check_update))
-    #kwargs['maxsize'] = config.AUTORANGE_MAXSIZE
+    kwargs['maxsize'] = config.AUTORANGE_MAXSIZE
-            logging.exception('gae_handler.handler %s %r, retry...', url, e)
+            logging.exception('gae_handler.handler %r %s , retry...', e, url)
-__version__ = '3.3.0'
+__version__ = '3.3.1'
-            response = urlfetch.fetch(url, body, fetchmethod, headers, allow_truncated=False, follow_redirects=False, deadline=timeout, validate_certificate=validate_certificate)
+            response = urlfetch.fetch(url, body, fetchmethod, headers, allow_truncated=True, follow_redirects=False, deadline=timeout, validate_certificate=validate_certificate)
-            #self.wfile.write("%s: %s\r\n" % (key, value))
+        for key in response_headers:
-        self.ip_connect_interval = 10
+        self.ip_connect_interval = 3
-
+from appids_manager import appid_manager
-
+        p = threading.Thread(target = self.keep_alive_thread)
-                time_handshaked = time.time()
+    def _create_ssl_connection(self, ip_port):
-                ssl_sock.host = ''
+        connect_time = 0
-                        raise socket.error(' certficate is none')
+            return ssl_sock
-                verify_SSL_certificate_issuer(ssl_sock)
+            if ssl_sock:
-                google_ip.report_connect_fail(ip)
+    def connect_thread(self):
-                time.sleep(0.3)
+                    ssl_sock.last_use_time = time.time()
-            if time.time() - ssl_sock.last_use_time < 210: # gws ssl connection can keep for 230s after created
+            if time.time() - ssl_sock.last_use_time < 225: # gws ssl connection can keep for 230s after created
-            create_more_connection()
+            self.create_more_connection()
-reload(sys).setdefaultencoding('UTF-8')
+#reload(sys).setdefaultencoding('UTF-8')
-    sys.path.append(win32_lib)
+    linux_lib = os.path.abspath( os.path.join(python_path, 'lib', 'linux'))
-except (ImportError, SystemError):
+except (ImportError, SystemError) as e:
-        info += 'GoAgent Version    : %s (python/%s gevent/%s pyopenssl/%s)\n' % (__version__, sys.version[:5], gevent.__version__, OpenSSL.__version__)
+        info += 'PHP proxy Version    : %s (python/%s gevent/%s pyopenssl/%s)\n' % (__version__, sys.version[:5], gevent.__version__, OpenSSL.__version__)
-        config = yaml.load(file(config_path, 'r'))
+        config = yaml.load(open(config_path, 'r'))
-    except yaml.YAMLError, exc:
+    except yaml.YAMLError as exc:
-    osx_lib = os.path.join(python_path, 'lib', 'osx')
+    osx_lib = os.path.join(python_path, 'lib', 'darwin')
-class Mac_tray(rumps.App):
+from PyObjCTools import AppHelper
-        self.menu = ["Config", "Enable Proxy", "Disable Proxy", "Reset", "Quit"]
+        image = NSImage.alloc().initByReferencingFile_(icon_path)
-        webbrowser.open_new("http://127.0.0.1:8085/")
+        # Let it highlight upon clicking
-    def on_reset(self, _):
+        # Hide dock icon
-        module_init.start_all_auto()
+        NSApp.terminate_(self)
-    def on_enable_proxy(self, _):
+    def enableProxy_(self, _):
-    def on_disable_proxy(self, _):
+    def disableProxy_(self, _):
-        return res
+class Mac_tray():
-           callback(data)
+        msg = unicode(msg)
-        self.run()
+# Note: the following code can't run in class
-    sys_tray.serve_forever()
+    serve_forever()
-    extra_lib = "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python"
+    extra_lib = "/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/PyObjc"
-        from mac_tray import sys_tray
+        import mac_tray as sys_tray
-                          'Cache-Control'])
+                          'Cache-Control'
-    kwargs['maxsize'] = config.AUTORANGE_MAXSIZE
+    #kwargs['maxsize'] = config.AUTORANGE_MAXSIZE
-            if key.title() == 'Transfer-Encoding':
+            key = key.title()
-            if key.title() in skip_headers:
+            if key in skip_headers:
-            #wfile.write("%s: %s\r\n" % (key.title(), value))
+            response_headers[key] = value
-            #logging.debug("Head- %s: %s", key.title(), value)
+            #logging.debug("Head- %s: %s", key, value)
-__version__ = '3.2.0'
+__version__ = '3.3.0'
-    config = {"modules": {}, "update":{"check_update":1, "last_path":"", "uuid":""}}
+def recheck_module_path():
-        current_version = config["modules"][module]["current_version"]
+        current_version = get(["modules", module, "current_version"], "_")
-        config["modules"][module]["current_version"] = current_version
+        set(["modules", module, "current_version"], current_version)
-    #print yaml.dump(config)
+
-            #logging.debug("report_connect_fail network fail recently")
+            logging.debug("report_connect_fail network fail recently")
-            #logging.debug("report_connect_fail network fail")
+            logging.debug("report_connect_fail network fail")
-                        ("restart goagent", None, self.on_restart_goagent))
+        menu_options = ((u"Config", None, self.on_show),
-        self.set_register(self.INTERNET_SETTINGS, 'ProxyEnable', 4, 0)
+        self.set_register(self.INTERNET_SETTINGS, 'ProxyEnable', 4, 0) # disable goagent proxy
-                f.write("appengine.google.com = %s\n" % self.host_appengine)
+                f.write("appengine.google.com = %s\n\n" % self.host_appengine)
-              os.path.abspath( os.path.join(root_path, "launcher", "start.py"))
+    run_cmd = "\"" + os.path.abspath( os.path.join(root_path, "python27", "1.0", "pythonw.exe")) + "\" \"" +\
-            f.write("passwd = %s\n" % self.proxy_passwd)
+            f.write("passwd = %s\n\n" % self.proxy_passwd)
-            logging.debug("Head1 %s: %s", keyword, cookie)
+            #logging.debug("Head1 %s: %s", keyword, cookie)
-        logging.debug("Head1 %s: %s", keyword, value)
+        #logging.debug("Head1 %s: %s", keyword, value)
-        logging.debug("Head1 %s: %s", keyword, value)
+        #logging.debug("Head1 %s: %s", keyword, value)
-        logging.debug("Send %s: %s", k, v)
+    #for k, v in headers.items():
-            logging.debug("Head- %s: %s", key.title(), value)
+            #logging.debug("Head- %s: %s", key.title(), value)
-        module_init.start()
+        module_init.stop("goagent")
-        self.HOSTS_POSTFIX_ENDSWITH = tuple(self.HOSTS_POSTFIX_MAP)
+        fwd_endswith = []
-config.load()
+config.load()
-        if host in config.HOSTS_MAP or host.endswith(config.HOSTS_POSTFIX_ENDSWITH):
+
-        if host == "appengine.google.com":
+        if host in config.HOSTS_GAE or host.endswith(config.HOSTS_GAE_ENDSWITH):
-        if host in config.HOSTS_MAP or host.endswith(config.HOSTS_POSTFIX_ENDSWITH):
+        if host in config.HOSTS_FWD or host.endswith(config.HOSTS_FWD_ENDSWITH):
-    from mac_tray import sys_tray
+    try:
-    wfile.write("HTTP/1.0 %d\r\n" % status)
+    wfile.write("HTTP/1.1 %d\r\n" % status)
-        wfile.write("%s: %s\r\n" % (key, value))
+        #wfile.write("%s: %s\r\n" % (key, value))
-            wfile.write("%s: %s\r\n" % (key, value))
+            if key.title() in skip_headers:
-            logging.warn("gae_handler %s %r", url, e)
+            logging.warn("gae_handler err:%r %s ", e, url)
-            logging.exception("gae_handler %s except:%r", url, e)
+            logging.exception("gae_handler except:%r %s", e, url)
-        logging.exception("gae_handler %s except:%r", url, e)
+        logging.exception("gae_handler except:%r %s", e, url)
-            self.wfile.write("%s: %s\r\n" % (key, value))
+            #self.wfile.write("%s: %s\r\n" % (key, value))
-# Caller:  gae_urlfetch
+
-                payload = zpayload
+def inflate(data):
-        headers['Content-Length'] = str(len(payload))
+        headers['Content-Length'] = str(len(body))
-    metadata += ''.join('%s:%s\n' % (k.title(), v) for k, v in headers.items() if k not in skip_headers)
+    #kwargs['options'] =
-    request_headers['Content-Length'] = str(len(payload))
+    payload = deflate(payload)
-    response = request(payload, request_headers)
+    body = '%s%s%s' % (struct.pack('!h', len(payload)), payload, body)
-    #logging.debug("request time:%d status:%d url:%s ", int(cost_time * 1000), response.status, url)
+    response = request(request_headers, body)
-    if response.status != 200:
+    response.app_status = response.status
-    if len(data) < 4:
+
-        response.fp = io.BytesIO(b'connection aborted. too short leadtype data=' + data)
+        response.fp = io.BytesIO(b'connection aborted. too short lead byte data=' + data)
-    response.status, headers_length = struct.unpack('!hh', data)
+    headers_length, = struct.unpack('!h', data)
-        response.fp = io.BytesIO(message)
+        logging.warn("fetch too short header need:%d get:%d %s", headers_length, len(data), url)
-    response.msg = httplib.HTTPMessage(io.BytesIO(zlib.decompress(data, -zlib.MAX_WBITS)))
+    raw_response_line, headers_data = inflate(data).split('\r\n', 1)
-
+            if response.app_status != 200:
-
+        #logging.debug("CA key:%s", key)
-            # clean old site certs
+            logging.info("no CA file exist")
-        self.max_timeout = 5
+        self.max_timeout = 15
-        self.connection_pool_num = config.CONFIG.getint("connect_manager", "https_connection_pool") #20/30
+        self.connection_pool_max_num = config.CONFIG.getint("connect_manager", "https_connection_pool_max") #20/30
-        while self.conn_pool.qsize() > self.connection_pool_num:
+        while self.conn_pool.qsize() > self.connection_pool_max_num:
-                ssl_sock.close()
+            if t < 200:
-                while self.conn_pool.qsize() < self.connection_pool_num:
+                while self.conn_pool.qsize() < self.connection_pool_min_num:
-            while self.thread_num < target_thread_num and self.conn_pool.qsize() < self.connection_pool_num:
+            target_thread_num = min(self.max_thread_num, (self.connection_pool_min_num - self.conn_pool.qsize()))
-        if conn_num < self.connection_pool_num:
+        if conn_num < self.connection_pool_min_num:
-
+#!/usr/bin/env python
-        
+
-        self._refresh_icon()
+        self._refresh_icon(recreate=True)
-    def _refresh_icon(self):
+    def _refresh_icon(self, recreate=False):
-        if self._notify_id:
+        if self._notify_id and not recreate:
-        self._refresh_icon()
+        self._refresh_icon(recreate=True)
-    modules = ["goagent", "launcher"]
+    modules = ["goagent", "launcher", "php_proxy"]
-    if not launcher_version or not os.path.isdir(launcher_path):
+    if not launcher_version or not os.path.isdir(os.path.join(current_path, launcher_version)):
-        p = subprocess.call(["Wscript.exe", "create_shortcut.js"], shell=False)
+        p = subprocess.call(["Wscript.exe", "//E:JScript", "create_shortcut.js"], shell=False)
-                return
+            while ip_num > max_good_ip_num:
-            ip_str = self.gws_ip_list[ip_num - 1]
+                ip_str = self.gws_ip_list[ip_num - 1]
-            del self.ip_dict[ip_str]
+                if 'gws' in server and ip_str in self.gws_ip_list:
-                self.gws_ip_list.remove(ip_str)
+                ip_num -= 1
-    if not launcher_version:
+    launcher_path = os.path.join(current_path, launcher_version)  
-    import sys
+import os
-    current_path = os.path.dirname(os.path.abspath(__file__))
+
-        super(Mac_tray, self).__init__("XX-Net", title="XX-Net", icon="Python.ico", quit_button=None)
+        icon_path = os.path.join(current_path, "Python.ico")
-
+    config["modules"]["launcher"]["auto_start"] = 0 # this means auto start on system login
-    test("216.58.220.86", 10) #gws
+    #test("216.58.220.86", 10) #gws
-    test("64.233.189.166")
+    test("127.0.0.1")
-
+default_good_ip_fie = os.path.join(current_path, "good_ip.txt")
-            return
+        if os.path.isfile(good_ip_file):
-        with open(good_ip_file, "r") as fd:
+        with open(file_path, "r") as fd:
-import os, sys
+import os
-data_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'data', 'launcher', 'config.yaml'))
+root_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir))
-    except yaml.YAMLError, exc:
+    except Exception as  exc:
-load()
+
-    load()
+    if os.path.isfile(data_path):
-    print yaml.dump(config)
+    #print yaml.dump(config)
-
+import subprocess
-launcher_version = config["modules"]["launcher"]["current_version"]
+def get_launcher_version_from_config():
-import subprocess
+    try:
-
+    create_data_path()
-                #verify_SSL_certificate_issuer(ssl_sock)
+                verify_SSL_certificate_issuer(ssl_sock)
-        openssl_context = SSLConnection.context_builder(ssl_version="TLSv1") #, ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
+        openssl_context = SSLConnection.context_builder(ssl_version="TLSv1", ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
-        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1") #, ca_certs=g_cacertfile)
+        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1", ca_certs=g_cacertfile)
-        logging.warn("check_update except:%s", e)
+        logging.exception("check_update except:%s", e)
-    if current_path != config.config["update"]["last_path"]: # or node_id != config.config["update"]["node_id"]:
+    if current_path != config.config["update"]["last_path"]:
-    if config.config["update"]["uuid"] == '':
+    if not config.get(["update", "uuid"]):
-
+import os
-        os.chdir(os.path.dirname(os.path.abspath(__file__)))
+
-        fd = open("ip_range.txt", "r")
+        logging.info("load ip range file:%s", self.range_file)
-            self.search_more_google_ip()
+        #if not self.is_ip_enough():
-            logging.info("remove_slowest_ip:%s", ip_str)
+            handshake_time = property['handshake_time']
-            info += 'Pac File           : file://%s\n' % os.path.join(os.path.dirname(os.path.abspath(__file__)), self.PAC_FILE).replace('\\', '/')
+            #info += 'Pac File           : file://%s\n' % os.path.join(self.DATA_PATH, self.PAC_FILE)
-        with open(filename, 'rb') as fp:
+        with open(serving_pacfile, 'rb') as fp:
-            with open(filename, 'wb') as fp:
+            with open(user_pacfile, 'wb') as fp:
-            logging.info('%r successfully updated', filename)
+            logging.info('%r successfully updated', user_pacfile)
-            return
+        global serving_pacfile, user_pacfile
-                data += b'\r\n'
+                data += b'\r\n This is the Pac server, not proxy port, use 8087 as proxy port.'
-            logging.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
+            return
-    ncount = 0
+    searching_thread_count = 0
-            logging.error("set_ip err:%s", e)
+            logging.exception("set_ip err:%s", e)
-        while not self.is_ip_enough():
+        while True: #not self.is_ip_enough() and self.searching_thread_count < 2:
-        self.ncount -= 1
+        self.searching_thread_count -= 1
-        while self.ncount < max_check_ip_thread_num:
+        while self.searching_thread_count < max_check_ip_thread_num:
-            self.ncount += 1
+            self.searching_thread_count += 1
-                #return
+                #self.conn_pool.put( (ssl_sock.handshake_time, ssl_sock) )
-    if current_path != config.config["update"]["last_path"] or node_id != config.config["update"]["node_id"]:
+    if current_path != config.config["update"]["last_path"]: # or node_id != config.config["update"]["node_id"]:
-            logging.info("path changed in same machine")
+        if sys.platform == "win32" and platform.release() == "XP":
-        return True
+import subprocess
-        subj.commonName = '%s CA' % CertUtil.ca_vendor
+        subj.commonName = '%s XX-Net' % CertUtil.ca_vendor
-        commonname = "GoAgent CA - GoAgent"
+        commonname = "GoAgent XX-Net - GoAgent"
-            os.system(('security find-certificate -a -c "%s" | grep "%s" >/dev/null || security add-trusted-cert -d -r trustRoot -k "/Library/Keychains/System.keychain" "%s"' % (commonname, commonname, certfile.decode('utf-8'))).encode('utf-8'))
+            CertUtil.import_mac_ca(commonname, certfile)
-        conn = httplib.HTTPSConnection("github.com", 443)
+        conn = httplib.HTTPSConnection("www.baidu.com", 443, timeout=3)
-    #test("208.117.224.103", 10) #gws
+    test("216.58.220.86", 10) #gws
-    #test("218.176.242.24")
+    test("64.233.189.166")
-    check_all_exist_ip()
+    #check_all_exist_ip()
-        self.timeout = 3
+        self.timeout = 1.5
-        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1")
+        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1") #, ca_certs=g_cacertfile)
-                return
+                self.conn_pool.put( (ssl_sock.handshake_time, ssl_sock) )
-                time.sleep(0.1)
+                time.sleep(0.3)
-            if time.time() - ssl_sock.last_use_time < 230: # gws ssl connection can keep for 230s after created
+            if time.time() - ssl_sock.last_use_time < 210: # gws ssl connection can keep for 230s after created
-        def _create_connection(ip_port):
+        def _create_connection(ip_port, delay=0):
-
+elif sys.platform == "darwin":
-                    html = generate_message_html('Please deploy new server', message)
+                    html = generate_message_html('Please deploy your new server', message)
-                    logging.info('GAE %s %s status:%s', self.command, self.path, response.status)
+                    logging.warn('GAE %s %s status:%s', self.command, self.path, response.status)
-                    data = response.read(8192)
+                    data = response.read(8192) #TODO: loop read until timeout or except.
-                        ret = self.wfile.write(data)
+                    if send_to_broswer:
-                        #response.close()
+                        https_manager.save_ssl_connection_for_reuse(response.ssl_sock)
-        yield format_response(403, {'Content-Type': 'text/html; charset=utf-8'}, message_html('403 Hosts Deny', 'Hosts Deny(%r)' % netloc, detail='url=%r' % url))
+        yield format_response(403, {'Content-Type': 'text/html; charset=utf-8'}, message_html('403 Hosts Deny', 'Hosts Deny(%r)' % netloc, detail='å±ç¨appidå ä¸ºèµæºæéï¼éå¶è§çè§é¢åæä»¶ä¸è½½ç­æ¶èèµæºè¿å¤çè®¿é®ï¼è¯·ä½¿ç¨èªå·±çappid <a href=" https://github.com/XX-net/XX-Net/wiki/Register-Google-appid" target="_blank">å¸®å©</a> '))
-        yield message_html('403 Hosts Deny', 'Hosts Deny(%r)' % netloc, detail='url=%r' % url)
+        yield message_html('403 Hosts Deny', 'Hosts Deny(%r)' % netloc, detail='å±ç¨appidå ä¸ºèµæºæéï¼éå¶è§çè§é¢åæä»¶ä¸è½½ç­æ¶èèµæºè¿å¤çè®¿é®ï¼è¯·ä½¿ç¨èªå·±çappid <a href=" https://github.com/XX-net/XX-Net/wiki/Register-Google-appid" target="_blank">å¸®å©</a> ')
-      response = self.opener.open(req)
+      response = self.opener.open(req, data=None, timeout=50)
-        self.fd.write(message)
+        self.fd.write(time_string + message)
-        os.remove(appengine_rpc.HttpRpcServer.DEFAULT_COOKIE_FILE_PATH)
+    try:
-    main()
+elif sys.platform == 'darwin':
-    test()
+    test()
-        self.menu = ["Config", "Reset", "Quit"]
+        self.menu = ["Config", "Enable Proxy", "Disable Proxy", "Reset", "Quit"]
-        module_init.start()
+        module_init.stop_all()
-        self.connection_pool_num = 20
+        self.max_thread_num = config.CONFIG.getint("connect_manager", "https_max_connect_thread") #10
-                time.sleep(0.5)
+                time.sleep(0.1)
-            if time.time() - ssl_sock.last_use_time < 210: # gws ssl connection can keep for 230s after created
+            if time.time() - ssl_sock.last_use_time < 230: # gws ssl connection can keep for 230s after created
-    max_thread_num = 10
+    max_thread_num = config.CONFIG.getint("connect_manager", "forward_max_connect_thread") #10
-
+# get value from config:
-                if time.time() - get_time < 10:
+                if time.time() - get_time < ip_connect_interval:
-import random
+from appids_manager import appid_manager
-
+from appids_manager import appid_manager
-                   "gae_appid":config.GAE_APPIDS,
+                   "gae_appid":"|".join(config.GAE_APPIDS),
-            file_path = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, 'data', 'downloads', filename))
+
-    import os
+
-        get_uuid() # update node_id and uuid
+        if node_id != config.config["update"]["node_id"]:
-            notify_install_tcpz_for_winXp()
+def need_new_uuid():
-    node_id = uuid.getnode()
+    if need_new_uuid():
-    return uuid
+    xx_net_uuid = config.config["update"]["uuid"]
-    main()
+#!/usr/bin/env python
-        cmd_line = 'certutil -L -d %s |grep "GoAgent" ||certutil -d %s -D -n "%s" ' % (firefox_config_path, firefox_config_path, common_name)
+        cmd_line = 'certutil -L -d %s |grep "GoAgent" &&certutil -d %s -D -n "%s" ' % (firefox_config_path, firefox_config_path, common_name)
-        cmd_line = 'certutil -L -d sql:$HOME/.pki/nssdb |grep "GoAgent" ||certutil -d sql:$HOME/.pki/nssdb -D -n "%s" ' % ( common_name)
+        cmd_line = 'certutil -L -d sql:$HOME/.pki/nssdb |grep "GoAgent" && certutil -d sql:$HOME/.pki/nssdb -D -n "%s" ' % ( common_name)
-                        self.wfile.write(b'HTTP/1.0 502\r\nContent-Type: text/html\r\n\r\n' + html.encode('utf-8'))
+                        html = generate_message_html('404 No usable Appid Exists', u'æ²¡æå¯ç¨appidäºï¼è¯·éç½®å¯ç¨çappid')
-                        self.wfile.write(b'HTTP/1.0 502\r\nContent-Type: text/html\r\n\r\n' + html.encode('utf-8'))
+                        html = generate_message_html('503 No usable Appid Exists', u'appidæµéä¸è¶³ï¼è¯·å¢å appid')
-        socks.set_default_proxy(proxy_type, config.PROXY_HOST, config.PROXY_PORT, config.PROXY_USER, config.PROXY_PASSWD)
+        logging.error("proxy type %s unknown, disable proxy", config.PROXY_TYPE)
-        default_socket = socket.socket
+        logging.debug("patch socks")
-    test("208.117.224.103", 10) #gws
+    #print network_is_ok()
-# bad case is 1300ms and more.
+# bad case is 1300ms and more.
-        logging.warn("proxy type %s unknown, disable proxy", config.PROXY_TYPE)
+        logging.error("proxy type %s unknown, disable proxy", config.PROXY_TYPE)
-        #socket.socket = socks.socksocket
+    socks.set_default_proxy(proxy_type, config.PROXY_HOST, config.PROXY_PORT, config.PROXY_USER, config.PROXY_PASSWD)
-    thread_num = 0
+
-            while self.thread_num < target_thread_num:
+            while self.thread_num < target_thread_num and self.conn_pool.qsize() < self.connection_pool_num:
-    def create_connection(self, port=443, sock_life=5):
+    def create_connection(self, host="", port=443, sock_life=5):
-                break
+        if host != "appengine.google.com":
-                ip = google_ip.get_gws_ip()
+                if host == "appengine.google.com":
-from goagent.local.google_ip import ip_utils
+import ip_utils
-62.197.198.193-251
+1.179.248.0-1.179.248.255
-94.40.70.0-63
+64.18.0.0-64.18.15.255
-        pr.print_stats(sort="cumtime")
+        pr.print_stats()
-        sock.sendall(request_data.encode() + payload)
+        sock.send(request_data.encode() + payload)
-        sock.sendall(request_data)
+        sock.send(request_data)
-            sock.sendall(data)
+            sock.send(data)
-                    self.wfile.write(data)
+                    data_len = len(data)
-        remote = forwork_manager.create_connection(port=port, sock_life=connected_in_s)
+        remote = forwork_manager.create_connection(host=host, port=port, sock_life=connected_in_s)
-        logging.debug('FWD %s %s:%d closed', self.command, host, port)
+        logging.debug('FWD %s %s:%d with closed', self.command, host, port)
-        logging.debug ('GoAgent Web_control %s "%s %s ', self.address_string(), self.command, self.path)
+
-        elif path == '/deploy':
+        else:
-           timeout=None,
+           timeout=20,
-            relate_path = os.path.sep.join(sep_path[5:])
+            home_path_depth = len(python_path.split(os.path.sep))
-        res = ctypes.windll.user32.MessageBoxW(None, msg, title, 1)
+        #res = ctypes.windll.user32.MessageBoxW(None, msg, title, 1)
-            return
+        #if res == 2:
-    ctypes.windll.user32.MessageBoxW(None, u"you need patch tcpip.sys using tcp-z", u"Patch XP needed", 0)
+    ctypes.windll.user32.MessageBoxW(None, u"è¯·ä½¿ç¨tcp-zå¯¹ tcpip.sys æè¡¥ä¸ï¼è§£å³é¾æ¥å¹¶åéå¶ï¼", u"Patch XP needed", 0)
-    time.sleep(4)
+    time.sleep(10)
-        self.PROXY_PORT = self.CONFIG.getint('proxy', 'port')
+        self.PROXY_PORT = self.CONFIG.get('proxy', 'port')
-            info += '%s Proxy    : %s:%d\n' % (self.PROXY_TYPE, self.PROXY_HOST, self.PROXY_PORT)
+            info += '%s Proxy    : %s:%s\n' % (self.PROXY_TYPE, self.PROXY_HOST, self.PROXY_PORT)
-    proxy_port = "0"
+    proxy_port = ""
-sys.path.append(noarch_lib)
+if __name__ == "__main__":
-    sys.path.append(linux_lib)
+    if sys.platform == "win32":
-    def dump_ca():
+    def generate_ca_file():
-        return 0
+    def import_windows_ca(common_name, certfile):
-    def remove_ca(name):
+    def remove_windows_ca(name):
-        return 0
+        try:
-                any(os.remove(x) for x in glob.glob(certdir+'/*.crt')+glob.glob(certdir+'/.*.crt'))
+    def file_is_same(file1, file2):
-        with open(capath, 'rb') as fp:
+                CertUtil.remove_windows_ca('%s CA' % CertUtil.ca_vendor)
-        certfiles = glob.glob(certdir+'/*.crt')+glob.glob(certdir+'/.*.crt')
+
-        return
+        CertUtil.import_ca(CertUtil.ca_keyfile)
-            return config_path
+def test_del_ca():
-    #print get_linux_firefox_path()
+    CertUtil.init_ca()
-    max_timeout = 10
+    max_timeout = 20
-                ctime, sock = self.tcp_connection_cache.get(timeout=0.4)
+                ctime, sock = self.tcp_connection_cache.get(timeout=0.3)
-        string = '%s - [%s]LOG_EXCEPT: %s\n' % (time.ctime()[4:-5], level, fmt % args)
+    except Exception as e:
-        control_server = LocalProxyServer((config.CONTROL_IP, config.CONTROL_PORT), web_control.RemoveContralServerHandler)
+        control_server = LocalProxyServer((config.CONTROL_IP, config.CONTROL_PORT), web_control.RemoteContralServerHandler)
-    CertUtil.check_ca()
+    CertUtil.init_ca()
-
+os.environ['HTTPS_PROXY'] = ''
-    opener = urllib2.build_opener()
+    proxy_handler = urllib2.ProxyHandler({})
-class RemoveContralServerHandler(BaseHTTPServer.BaseHTTPRequestHandler):
+class RemoteContralServerHandler(BaseHTTPServer.BaseHTTPRequestHandler):
-            logging.warn('Control Req %s "%s %s ', self.address_string(), self.command, self.path)
+            logging.warn('Control Req %s %s %s ', self.address_string(), self.command, self.path)
-        logging.debug ('HTTP %s "%s %s ', self.address_string(), self.command, self.path)
+        logging.debug ('GoAgent web_control %s %s %s ', self.address_string(), self.command, self.path)
-
+    @staticmethod
-        self.send_response('text/plain', data)
+        self.send_response('application/json', data)
-                self.send_response('text/plain', data)
+                self.send_response('application/json', data)
-        self.send_response('text/plain', data)
+        self.send_response('application/json', data)
-                data = '{"res":"fail", "error":"%s"}' % e
+            if RemoteContralServerHandler.deploy_proc and RemoteContralServerHandler.deploy_proc.poll() == None:
-            if os.path.isfile(log_path):
+            if self.deploy_proc and os.path.isfile(log_path):
-            data = content
+            status = 'init'
-        self.send_response(mimetype, data)
+        self.send_response('application/json', data)
-            if i < 2:
+            if i < 9:
-    os.remove(filename)
+        os.remove(filename)
-        sys.exit(-1)
+        if appid == "":
-goagent.main()
+
-        config.load()
+        #config.load()
-    config.load()
+    #config.load()
-    config.load()
+    #config.load()
-import logging
+import atexit
-    config.load()
+    #config.load()
-        config.load()
+        #config.load()
-    config.load()
+    #config.load()
-import os, re, sys
+import os, sys
-import operator
+import urllib2
-        config.load()
+        #config.load()
-        logging.debug ('HTTP %s "%s %s ', self.address_string(), self.command, self.path)
+        logging.debug ('launcher web_control %s "%s %s ', self.address_string(), self.command, self.path)
-            config.load()
+            #config.load()
-                #self.wfile.write(data)
+            tme = (datetime.datetime.today()+datetime.timedelta(minutes=330)).strftime('%H:%M:%S-%a/%d/%b/%Y')
-        data = index_content % (menu_content, right_content)
+        data = (index_content.decode('utf-8') % (menu_content, right_content.decode('utf-8') )).encode('utf-8')
-        #self.wfile.write(data)
+        self.send_response("application/json", data)
-subprocess.call([sys.executable, start_script], shell=False)
+
-    ca_keyfile = 'CA.crt'
+    ca_keyfile = os.path.join(data_path, 'CA.crt')
-    ca_certdir = 'certs'
+    ca_certdir = os.path.join(data_path, 'certs')
-        commonname = os.path.splitext(os.path.basename(certfile))[0]
+        #commonname = os.path.splitext(os.path.basename(certfile))[0]
-                return os.system(cmd_line)
+                # certutil -L -d sql:$HOME/.pki/nssdb | grep "%s" ||  % commonname,
-        self.connection_pool_num = 30
+        self.max_thread_num = 10
-            if t < 300:
+
-            ssl_sock.close()
+            else:
-    max_timeout = 5
+    timeout = 1
-                time.sleep(delay)
+        def _create_connection(ip_port):
-                queobj.put(sock)
+                self.tcp_connection_cache.put((time.time(), sock))
-                queobj.put(e)
+            finally:
-            pass
+        while True:
-            for i in range(3):
+        while time.time() - start_time < self.max_timeout:
-                    logging.warning("no gws ip.")
+                    logging.error("no gws ip.")
-                    return result
+                addr = (ip, port)
-        except NetWorkIOError as e:
+        except Exception as e:
-                raise
+                logging.exception("forward except:%s.", e)
-            logging.debug("forward closed.")
+
-max_check_ip_thread_num = 10
+max_check_ip_thread_num = 5
-            return
+        if not force:
-                    jsrule = PacUtil.adblock2pac(adblock_content, 'FindProxyForURLByAdblock', blackhole, default)
+                jsrule = PacUtil.adblock2pac(adblock_content, 'FindProxyForURLByAdblock', blackhole, default)
-                jsrule = PacUtil.autoproxy2pac(autoproxy_content, 'FindProxyForURLByAutoProxy', autoproxy, default)
+            jsrule = PacUtil.autoproxy2pac(autoproxy_content, 'FindProxyForURLByAutoProxy', autoproxy, default)
-        response.fp = io.BytesIO(b'connection aborted. too short headers data=' + data)
+        if data.startswith('ent æå¡ç«¯å·²ç»å¨'):
-        thread.start_new_thread(self.__fetchlet, (range_queue, data_queue, 0))
+        #thread.start_new_thread(self.__fetchlet, (range_queue, data_queue, 0))
-                thread.start_new_thread(self.__fetchlet, (range_queue, data_queue, cur_threads * config.AUTORANGE_MAXSIZE))
+                #thread.start_new_thread(self.__fetchlet, (range_queue, data_queue, cur_threads * config.AUTORANGE_MAXSIZE))
-        data = self.connection.recv(1024)
+        try:
-                    # goagent upload to appengine is slow, it need more 'fresh' connection.
+        remote = forwork_manager.create_connection(port=port, sock_life=connected_in_s)
-            forwork_manager.forward_socket(self.connection, remote, bufsize=self.bufsize)
+        try:
-
+import platform
-        import platform,sys
+
-    for i in range(3):
+    for i in range(10):
-else:
+elif sys.platform == 'linux' or sys.platform == 'linux2':
-        #socket.socket = socks.socksocket
+        socks.set_default_proxy(proxy_type, config.PROXY_HOST, config.PROXY_PORT, config.PROXY_USER, config.PROXY_PASSWD)
-        socks.set_default_proxy(proxy_type, config.PROXY_HOST, config.PROXY_PORT)
+        socks.set_default_proxy(proxy_type, config.PROXY_HOST, config.PROXY_PORT, config.PROXY_USER, config.PROXY_PASSWD)
-#      Sui Feng          <suifeng.me@qq.com>
+
-sys.path.append(noarch_lib)
+    proxy_user = ""
-    log_fd.flush()
+    try:
-    if config.get(["web_ui", "popup_webui"], 1) == 1:
+    if config.get(["modules", "launcher", "popup_webui"], 1) == 1:
-sys.path.append(noarch_lib)
+import autorun
-            data = '{ "check_update": "%d", "popup_webui": %d }' % (config.get(["update", "check_update"], 1), config.get(["web_ui", "popup_webui"], 1) )
+            data = '{ "check_update": "%d", "popup_webui": %d, "auto_start": %d }' %\
-                    config.set(["web_ui", "popup_webui"], popup_webui)
+                    config.set(["modules", "launcher", "popup_webui"], popup_webui)
-        #self.wfile.write(data)
+        self.send_response('application/json', data)
-import socks
+if config.PROXY_ENABLE:
-    logging.warn("proxy type %s unknown, disable proxy", config.PROXY_TYPE)
+    if config.PROXY_TYPE == "HTTP":
-    socket.socket = socks.socksocket
+    if config.PROXY_ENABLE:
-                sock = socket.socket(socket.AF_INET)
+                if config.PROXY_ENABLE:
-            #logging.debug("Check_appengine %s IOError:%s", ip, e)
+            logging.debug("Check %s IOError:%s", self.ip, e)
-
+def test_multi_thread_search_ip():
-    socket.socket = socks.socksocket
+    noarch_lib = os.path.abspath( os.path.join(python_path, 'lib', 'noarch'))
-                sock = socket.socket(socket.AF_INET if ':' not in ip_port[0] else socket.AF_INET6)
+                if config.PROXY_ENABLE:
-                sock = socket.socket(socket.AF_INET if ':' not in ip else socket.AF_INET6)
+                if config.PROXY_ENABLE:
-            except (socket.error, OSError) as e:
+            except Exception as e:
-        while time.time() - start_time < self.max_timeout:
+        #while time.time() - start_time < self.max_timeout:
-                if not isinstance(result, (socket.error, OSError)):
+                if not isinstance(result, (socket.error, OSError, IOError)):
-    test_pool_speed()
+    #test_pool_speed()
-                logging.info("load ip: %s time:%d domain:%s server:%s", ip_str, handshake_time, domain, server)
+                #logging.info("load ip: %s time:%d domain:%s server:%s", ip_str, handshake_time, domain, server)
-            pass
+        if check_ip.network_is_ok():
-                        remote.sendall(data)
+                        remote.send(data) #TODO: check this
-
+#!/usr/bin/env python
-
+else:
-
+                else:
-
+from config import config
-if __name__ == "__main__"  and False:
+if __name__ == "__main__":
-    #test("203.165.14.230", 10) #gws
+    test("208.117.224.103", 10) #gws
-
+from config import config
-                            'TLS_EMPTY_RENEGOTIATION_INFO_SCSV']
+
-        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1") #, ca_certs=g_cacertfile) #, cipher_suites=ssl_ciphers)
+        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1")
-
+                ssl_sock.host = ''
-    max_timeout = 3
+    timeout = 3
-                start_time = time.time()
+                sock.settimeout(self.timeout)
-        for j in range(self.max_retry):
+        start_time = time.time()
-        logging.warning('create_connection fail.')
+        logging.warning('create tcp connection fail.')
-#!/usr/bin/env python
+#!/usr/bin/env python2
-        return target(*args, **kwargs)
+        try:
-import traceback
+    def get_appid(self):
-            self.working_appid = None
+            return None
-        return self.working_appid
+            return random.choice(self.working_appid_list)
-
+skip_headers = frozenset(['Vary',
-def gae_urlfetch(method, url, headers, payload, app_server, **kwargs):
+        except Exception as e:
-    skip_headers = https_manager.skip_headers
+
-    #cost_time = stop_time - start_time
+    response = request(payload, request_headers)
-    def __init__(self, wfile, response, method, url, headers, payload, fetchservers, password, maxsize=0, bufsize=0, waitsize=0, threads=0):
+    def __init__(self, wfile, response, method, url, headers, payload, maxsize=0, bufsize=0, waitsize=0, threads=0):
-                        response = gae_urlfetch(self.command, self.url, headers, self.payload, fetchserver, password=self.password)
+                        response = gae_urlfetch(self.command, self.url, headers, self.payload)
-                response = gae_urlfetch(self.command, self.path, request_headers, payload, app_server, **kwargs)
+                response = gae_urlfetch(self.command, self.path, request_headers, payload, **kwargs)
-                        logging.warning('GET %s no response', self.path)
+                        logging.warning('GET no response %s ', self.path)
-                    appid_manager.report_not_exist(appid)
+                    logging.warning('APPID %r not exists, remove it.', response.ssl_sock.appid)
-                    appid_manager.report_out_of_quota(appid)
+                    logging.warning('APPID %r out of Quota, remove it.', response.ssl_sock.appid)
-                        rangefetch = RangeFetch(self.wfile, response, self.command, self.path, self.headers, payload, fetchservers, config.GAE_PASSWORD, maxsize=config.AUTORANGE_MAXSIZE, bufsize=config.AUTORANGE_BUFSIZE, waitsize=config.AUTORANGE_WAITSIZE, threads=config.AUTORANGE_THREADS)
+                        rangefetch = RangeFetch(self.wfile, response, self.command, self.path, self.headers, payload, maxsize=config.AUTORANGE_MAXSIZE, bufsize=config.AUTORANGE_BUFSIZE, waitsize=config.AUTORANGE_WAITSIZE, threads=config.AUTORANGE_THREADS)
-                        logging.warn('GAEProxyHandler.do_METHOD_AGENT timed out, url=%r, content_length=%r, try again', self.path, content_length)
+                        logging.warn('GAEProxyHandler.do_METHOD_AGENT timed out, content_length=%r, url=%r, try again', content_length, self.path)
-                    logging.warn('GAEProxyHandler.do_METHOD_AGENT url=%r return %r, abort.', self.path, e)
+                    logging.warn('GAEProxyHandler.do_METHOD_AGENT return %r, abort. url=%r ', e, self.path)
-from httplib2 import Http
+import urllib2
-        self.CONFIG = ConfigParser.ConfigParser()
+    proxy_enable = "0"
-        self.CONFIG_USER_FILENAME = os.path.abspath( os.path.join(root_path, 'data', 'goagent', 'config.ini'))
+    def __init__(self):
-        self.password = ''
+        ConfigParser.RawConfigParser.OPTCRE = re.compile(r'(?P<option>[^=\s][^=]*)\s*(?P<vi>[=])\s*(?P<value>.*)$')
-                self.CONFIG.read(self.CONFIG_USER_FILENAME)
+            if os.path.isfile(CONFIG_USER_FILENAME):
-            self.password = self.CONFIG.get('gae', 'password')
+            try:
-            logging.exception("User_config.load except:%s", e)
+            logging.warn("User_config.load except:%s", e)
-    def save(self, appid, password):
+    def save(self):
-            if appid != "":
+            f = open(CONFIG_USER_FILENAME, 'w')
-                f.write("password = %s\n" % password)
+                f.write("appid = %s\n" % self.appid)
-            logging.warn("launcher.config save user config fail:%s", self.CONFIG_USER_FILENAME)
+            logging.warn("launcher.config save user config fail:%s", CONFIG_USER_FILENAME)
-    return content
+    opener = urllib2.build_opener()
-        mimetype = 'text/plain'
+    def get_launcher_version(self):
-                data = '{ "appid": "%s", "passwd": "%s" }' % (user_config.appid, user_config.password)
+                data = json.dumps(user_config, default=lambda o: o.__dict__)
-                user_config.save(appid=appid, password=passwd)
+                user_config.appid = self.postvars['appid'][0]
-#!/usr/bin/env python
+#!/usr/bin/env python2
-import remote_control
+import web_control
-        control_server = LocalProxyServer((config.CONTROL_IP, config.CONTROL_PORT), remote_control.RemoveContralServerHandler)
+        control_server = LocalProxyServer((config.CONTROL_IP, config.CONTROL_PORT), web_control.RemoveContralServerHandler)
-                    logging.info('GAEProxyHandler.do_METHOD_AGENT url=%r return %r, abort.', self.path, e)
+                    logging.warn('GAEProxyHandler.do_METHOD_AGENT url=%r return %r, abort.', self.path, e)
-                    traceback.print_exc()
+                    #traceback.print_exc()
-class RemoveContralServerHandler(BaseHTTPServer.BaseHTTPRequestHandler):
+current_path = os.path.dirname(os.path.abspath(__file__))
-            self.wfile.write(data)
+            self.send_response(mimetype, data)
-                self.wfile.write(data)
+                self.send_response(mimetype, data)
-        self.wfile.write(data)
+        self.send_response(mimetype, data)
-        data = ''
+        self.send_response('text/plain', data)
-        self.wfile.write(data)
+        self.send_response(mimetype, data)
-        self.wfile.write(data)
+        self.send_response(mimetype, data)
-        self.wfile.write(data)
+        self.send_response(mimetype, data)
-
+    except Exception as e:
-        pass
+    except Exception as e:
-import ConfigParser, io, os, re, sys
+import os, re, sys
-import subprocess
+import operator
-user_config = User_config()
+module_menus = {}
-        self.wfile.write(b'HTTP/1.1 403\r\nConnection: close\r\n\r\n')
+    def load_module_menus(self):
-            self.postvars = {}
+        module_menus = sorted(module_menus.iteritems(), key=lambda (k,v): (v['menu_sort_id']))
-            logging.info('%s "%s %s HTTP/1.1" 404 -', self.address_string(), self.command, self.path)
+    def address_string(self):
-            if filename.endswith('.js'):
+        url_path = urlparse.urlparse(self.path).path
-            elif filename.endswith('.css'):
+            elif file_path.endswith('.css'):
-            elif filename.endswith('.html'):
+            elif file_path.endswith('.html'):
-            elif filename.endswith('.jpg'):
+            elif file_path.endswith('.jpg'):
-            elif filename.endswith('.png'):
+            elif file_path.endswith('.png'):
-        elif path == '/quit':
+            self.send_file(file_path, mimetype)
-                self.wfile.write(data)
+                self.send_response(mimetype, data)
-    def req_goagent_config_handler(self):
+    def req_index_handler(self):
-                    user_config.clean()
+        try:
-            data = content
+                    active = ''
-        self.wfile.write(data)
+        data = index_content % (menu_content, right_content)
-        self.wfile.write(data)
+        self.send_response(mimetype, data)
-                logging.warn("load_ip line:%s err:%s", line, e)
+                logging.exception("load_ip line:%s err:%s", line, e)
-__reset_color = lambda: None
+
-        __reset_color = lambda: SetConsoleTextAttribute(GetStdHandle(-11), 0x07)
+        set_console_color = lambda color: SetConsoleTextAttribute(GetStdHandle(-11), color)
-        __reset_color = lambda: sys.stderr.write('\033[0m')
+        err_color = '\033[31m'
-def log(level, fmt, *args, **kwargs):
+def log(level, console_color, html_color, fmt, *args, **kwargs):
-    buffer_lock.release()
+    try:
-    string = '%s - [%s] %s\n' % (level, time.ctime()[4:-5], fmt % args)
+    string = '%s - [%s] %s\n' % (time.ctime()[4:-5], level, fmt % args)
-                return os.system('certutil -L -d sql:$HOME/.pki/nssdb | grep "%s" || certutil -d sql:$HOME/.pki/nssdb -A -t "C,," -n "%s" -i "%s"' % (commonname, commonname, certfile))
+                cmd_line = 'certutil -L -d sql:$HOME/.pki/nssdb | grep "%s" || certutil -d sql:$HOME/.pki/nssdb -A -t "C,," -n "%s" -i "%s"' % (commonname, commonname, certfile)
-if __name__ == "__main__":
+if __name__ == "__main__"  and False:
-        openssl_context = SSLConnection.context_builder(ssl_version="TLSv1", ca_certs=g_cacertfile)
+        openssl_context = SSLConnection.context_builder(ssl_version="TLSv1") #, ca_certs=g_cacertfile) # check cacert cost too many cpu, 100 check thread cost 60%.
-    test('210.139.253.39', 100)
+    #test('208.117.224.213', 10)
-        self.CONFIG_USER_FILENAME = os.path.abspath( os.path.join(current_path, os.pardir, os.pardir, os.pardir, 'data', 'goagent', 'config.ini'))
+        self.CONFIG_USER_FILENAME = os.path.abspath( os.path.join(self.DATA_PATH, 'config.ini'))
-
+import traceback
-class Connect_pool(object):
+class Connect_pool():
-        self.pool_lock.acquire()
+        handshake_time, sock = item
-            self.pool[sock] = speed
+            self.pool[sock] = handshake_time
-            self.pool_lock.release()
+            self.not_empty.release()
-    def get(self, block=True):
+    def get(self, block=True, timeout=None):
-            else:
+                    return None
-        return self.get(False)
+        return self.get(block=False)
-        self.min_connection_num = 20
+        self.max_thread_num = 40
-        ssl_ciphers = [x for x in self.ssl_ciphers if random.random() > 0.5]
+        #ssl_ciphers = [x for x in self.ssl_ciphers if random.random() > 0.5]
-        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1", ca_certs=g_cacertfile, cipher_suites=ssl_ciphers)
+        self.openssl_context = SSLConnection.context_builder(ssl_version="TLSv1") #, ca_certs=g_cacertfile) #, cipher_suites=ssl_ciphers)
-                return
+        while self.conn_pool.qsize() > self.connection_pool_num:
-                #logging.debug("create_ssl update ip:%s time:%d", ip, handshake_time)
+                logging.debug("create_ssl update ip:%s time:%d", ip, handshake_time)
-                def check_ssl_cert(ssl_sock):
+
-                check_ssl_cert(ssl_sock)
+                #verify_SSL_certificate_issuer(ssl_sock)
-                #logging.debug("create_ssl %s fail:%s", ip, e)
+                logging.debug("create_ssl %s fail:%s c:%d h:%d", ip, e, connect_time, handshake_time)
-                while self.conn_pool.qsize() < self.min_connection_num:
+                while self.conn_pool.qsize() < self.connection_pool_num:
-            while self.thread_num < self.max_thread_num:
+            target_thread_num = min(self.max_thread_num, (self.connection_pool_num - self.conn_pool.qsize()))
-            except:
+            ret = self.conn_pool.get_nowait()
-                #logging.debug("ssl_pool.get:%s handshake:%d", ssl_sock.ip, handshake_time)
+                logging.debug("ssl_pool.get:%s handshake:%d", ssl_sock.ip, handshake_time)
-        if conn_num < self.min_connection_num:
+        logging.debug("ssl conn_num:%d", conn_num)
-                handshake_time, ssl_sock = self.conn_pool.get()
+            ret = self.conn_pool.get(True, self.max_timeout)
-                logging.error("get ssl_pool err:%s", e)
+            else:
-            raise TypeError('http_util.request(payload) must be a string or buffer, not %r' % type(payload))
+            raise TypeError('_request(payload) must be a string or buffer, not %r' % type(payload))
-    tcp_connection_cache = collections.defaultdict(Queue.PriorityQueue)
+    max_timeout = 3
-        def _create_connection(ip_port, timeout, queobj, delay=0):
+    def create_connection(self, sock_life=5):
-                sock.settimeout(timeout or self.max_timeout)
+                sock.settimeout(self.max_timeout)
-                logging.info("create_tcp update ip:%s time:%d", ip, conn_time * 2000)
+                #logging.info("create_tcp update ip:%s time:%d", ip, conn_time * 2000)
-                logging.debug("tcp conn %s fail", ip)
+                conn_time = int((time.time() - start_time) * 1000)
-                logging.info("create_ssl report fail ip:%s", ip)
+                #logging.info("create_tcp report fail ip:%s", ip)
-                        sock.close()
+                    self.tcp_connection_cache.put((time.time(), sock))
-                pass
+        try:
-        logging.warning('create_connection to %s fail.', addrs)
+        for j in range(self.max_retry):
-# base on checkgoogleip 'moonshawdo@gmail.com'
+# based on checkgoogleip 'moonshawdo@gmail.com'
-timeout = 5000 # 5 second
+max_check_ip_thread_num = 10
-    remove_ip_thread_lock = threading.Lock()
+    remove_ip_thread_num_lock = threading.Lock()
-    last_sort_time_for_gws = 0
+    last_sort_time_for_gws = 0  # keep status for avoid wast too many cpu
-    network_fail_time = 0
+    network_fail_time = 0 # keep status for avoid retry too frequently
-        if len(self.gws_ip_list) >= min_good_ip_num:
+        if len(self.gws_ip_list) >= max_good_ip_num:
-        with open("good_ip.txt", "r") as fd:
+        if not os.path.isfile(good_ip_file):
-        logging.info("load google iplist num: %d, gws num:%d", len(self.ip_dict), len(self.gws_ip_list))
+        logging.info("load google ip_list num:%d, gws num:%d", len(self.ip_dict), len(self.gws_ip_list))
-        p.start()
+        if False:
-        if self.iplist_need_save == 0:
+    def save_ip_list(self, force=False):
-            with open("good_ip.txt", "w") as fd:
+            with open(good_ip_file, "w") as fd:
-            #self._clean_bad_ip_if_we_have_too_many_ips
+        except Exception as e:
-                fastest_num = min(ip_num-1, 40)
+                ip_num = len(self.gws_ip_list)
-                ip_str = self.gws_ip_list[index]
+                #index = get_random_pr(fastest_num)
-                if time.time() - fail_time < 10:
+                if time.time() - fail_time < 300:
-                #logging.debug("add ip:%s duplicated", ip_str)
+                self.ip_dict[ip_str]['history'].append([time.time(), handshake_time])
-            self.ip_dict[ip_str] = {'handshake_time':handshake_time, 'domain':domain, 'server':server, 'timeout':0, "history":"%d,"% handshake_time, "fail_time":0}
+
-                # this ip will no return back to first good ip list until all become bad
+                # Case: some good ip, average handshake time is 300ms
-                    self.ip_dict[ip_str]['handshake_time'] = org_time + 200
+                if handshake_time - org_time > 100:
-                #self.ip_dict[ip_str]['history'] += "%d," % handshake_time
+                self.ip_dict[ip_str]['history'].append([time.time(), handshake_time])
-            logging.error("set_ip input")
+    def report_connect_fail(self, ip_str, force_remove=False):
-        ip_removed = False
+        #ip_removed = False
-            #self.ip_dict[ip_str]['history'] += "fail,"
+            self.ip_dict[ip_str]['history'].append([time.time(), "fail"])
-                ip_removed = True
+            if force_remove or self.ip_dict[ip_str]['timeout'] >= 2:
-            self.try_sort_ip_by_handshake_time(force=True)
+        #
-        self.remove_ip_thread_lock.acquire()
+        self.remove_ip_thread_num_lock.acquire()
-        self.remove_ip_thread_lock.release()
+        self.remove_ip_thread_num_lock.release()
-                    return
+                if not self.network_is_ok():
-            self.remove_ip_thread_lock.acquire()
+            self.remove_ip_thread_num_lock.acquire()
-            self.remove_ip_thread_lock.release()
+            self.remove_ip_thread_num_lock.release()
-        while True:# not self.is_ip_enough():
+        while not self.is_ip_enough():
-            return
+                if not self.network_is_ok():
-                    logging.warning('APPID %r out of auota, remove it.', appid)
+                    logging.warning('APPID %r out of Quota, remove it.', appid)
-                    logging.info('"GAE t:%d %s %s HTTP/1.1" status:%s len:%s', time_cost, self.command, self.path, response.status, response.getheader('Content-Length', '-'))
+                    data_size = response.getheader('Content-Length', '0')
-                    logging.exception('GAEProxyHandler.do_METHOD_AGENT %r return %r, errno: %d ', self.path, e, int(e.args[0])) #IOError(9, 'Bad file descriptor')
+                    logging.exception('GAEProxyHandler.do_METHOD_AGENT %r return %r', self.path, e) #IOError(9, 'Bad file descriptor'), int(e.args[0])
-                    remote.sendall(data)
+                if remote is not None:
-
+import connect_manager
-            data += "%s \t %d \t %d \t %s\r\n" % (ip, handshake_time, timeout, history)
+            t0 = 0
-            module_init.module_init.stop_all()
+            module_init.stop_all()
-            data = '{"res":"success"}'
+                if os.path.isfile(log_path):
-lib_path = os.path.join(code_path, "google_appengine.zip")
+lib_path = os.path.join(code_path, "lib")
-    capath = os.path.join(os.path.dirname(os.path.abspath(__file__)), CertUtil.ca_keyfile)
+    #capath = os.path.join(os.path.dirname(os.path.abspath(__file__)), CertUtil.ca_keyfile)
-    test('208.117.224.213', 10)
+    test('210.139.253.39', 100)
-        self.min_connection_num = 30
+        self.max_thread_num = 20
-        self.conn_pool = Queue.Queue()
+        self.conn_pool = Connect_pool() #Queue.PriorityQueue()
-        self.conn_pool.put( (time.time(), socket) )
+    def save_ssl_connection_for_reuse(self, ssl_sock):
-
+                #logging.debug("create_ssl update ip:%s time:%d", ip, handshake_time)
-                logging.debug("create_ssl %s fail:%s", ip, e)
+                #logging.debug("create_ssl %s fail:%s", ip, e)
-                    logging.debug("create ssl conn %s", ip_str)
+                    #logging.debug("create ssl conn %s", ip_str)
-                        self.conn_pool.put((time.time(), ssl_sock))
+                        ssl_sock.last_use_time = time.time()
-                ctime, sock = self.conn_pool.get_nowait()
+                handshake_time, ssl_sock = self.conn_pool.get_nowait()
-                sock = None
+                ssl_sock = None
-            if time.time() - ctime < 210: # gws ssl connection can keep for 230s after created
+            if time.time() - ssl_sock.last_use_time < 210: # gws ssl connection can keep for 230s after created
-                sock.close()
+                ssl_sock.close()
-        logging.debug("ssl conn_num:%d", conn_num)
+        #logging.debug("ssl conn_num:%d", conn_num)
-            return sock
+        if ssl_sock:
-                return sock
+                handshake_time, ssl_sock = self.conn_pool.get()
-                logging.warning("get ssl_pool err:%s", e)
+                logging.error("get ssl_pool err:%s", e)
-        def _create_connection(ip_port, timeout, queobj):
+        def _create_connection(ip_port, timeout, queobj, delay=0):
-
+                logging.debug("tcp conn %s fail", ip)
-                if time.time() - ctime < 5:
+                if time.time() - ctime < sock_life:
-            thread.start_new_thread(_create_connection, (addr, timeout, queobj))
+            thread.start_new_thread(_create_connection, (addr, timeout, queobj, delay))
-                    logging.warning('create_connection to %s return %r, try again.', addrs, result)
+        logging.warning('create_connection to %s fail.', addrs)
-max_good_ip_num = 100
+max_check_ip_thread_num = 30
-    ip_dict = {} # ip_str => { 'handshake_time'=>?, 'domain'=>, 'server'=>?}
+    ip_dict = {} # ip_str => { 'handshake_time'=>?, 'domain'=>, 'server'=>?, 'timeout'=>}
-            for line in fd.readlines():
+        with open("good_ip.txt", "r") as fd:
-                #logging.info("load ip: %s time:%d domain:%s server:%s", ip_str, handshake_time, domain, server)
+                logging.info("load ip: %s time:%d domain:%s server:%s", ip_str, handshake_time, domain, server)
-            logging.info("load google iplist num: %d, gws num:%d", len(self.ip_dict), len(self.gws_ip_list))
+                #logging.debug("load_ip ip:%s time:%d", ip_str, handshake_time)
-            logging.warn("load google good ip fail: %s", e)
+        logging.info("load google iplist num: %d, gws num:%d", len(self.ip_dict), len(self.gws_ip_list))
-            logging.error("set_ip input")
+            logging.error("add_ip input")
-            self.ip_dict[ip_str] = {'handshake_time':handshake_time, 'domain':domain, 'server':server}
+            self.ip_dict[ip_str] = {'handshake_time':handshake_time, 'domain':domain, 'server':server, 'timeout':0, "history":"%d,"% handshake_time, "fail_time":0}
-            self.report_connect_fail(ip_str)
+        if handshake_time < 5: # this is impossible
-                self.ip_dict[ip_str]['handshake_time'] = handshake_time
+
-            logging.error("set_ip err:%s", e)
+            logging.error("update_ip err:%s", e)
-    def report_connect_fail(self, ip_str):
+    def report_connect_fail(self, ip_str, force_remove=False):
-
+        ip_removed = False
-            self.ip_dict[ip_str]['handshake_time'] = handshake_time
+            fail_time = self.ip_dict[ip_str]["fail_time"]
-            if handshake_time >= timeout:
+            if force_remove or self.ip_dict[ip_str]['timeout'] >= 5:
-        self.try_sort_ip_by_handshake_time(force=True)
+        if ip_removed or True:
-                    self.network_fail_time = time.time()
+                    logging.warn("network is unreachable. check your network connection.")
-            self.ip_lock.release()
+            conn = httplib.HTTPSConnection("github.com", 443)
-        self.try_sort_ip_by_handshake_time()
+        for i in range(ip_num-1, min_good_ip_num, -1):
-                return None
+            if property['handshake_time'] < 600:
-                 result.domain, result.server_type, result.handshake_time, len(self.gws_ip_list))
+            #logging.info("add  %s  CN:%s  type:%s  time:%d  gws:%d ", ip_str,
-        while not self.is_ip_enough():
+        while True:# not self.is_ip_enough():
-                self.report_connect_fail(ip_str)
+            result = check_ip.test_gws(ip_str)
-google_ip = Check_ip()
+if __name__ != "__main__":
-if __name__ == '__main__':
+def test_network():
-                appid = self.appid
+                appid = appid_manager.get_appid()
-                    self.appid = appid_manager.get_appid()
+                    appid = appid_manager.get_appid()
-                    if not self.appid:
+                    if not appid:
-                    self.appid = appid_manager.get_appid()
+                    appid = appid_manager.get_appid()
-                    if not self.appid:
+                    if not appid:
-                    logging.info('"GAE %s %s HTTP/1.1" status:%s len:%s', self.command, self.path, response.status, response.getheader('Content-Length', '-'))
+                    logging.info('"GAE t:%d %s %s HTTP/1.1" status:%s len:%s', time_cost, self.command, self.path, response.status, response.getheader('Content-Length', '-'))
-                if host == "appengine.google.com":
+                if host == "appengine.google.com" or host == "www.google.com":
-                    logging.error('http_util.create_connection((host=%r, port=%r)) timeout', host, port, )
+                    logging.warn('http_util.create_connection((host=%r, port=%r)) timeout', host, port, )
-    main()
+    test2()
-
+import config
-    webbrowser.open("http://127.0.0.1:8085/")
+    config.load()
-            data = '{ "check_update": "%d" }' % (config.config["update"]["check_update"])
+            data = '{ "check_update": "%d", "popup_webui": %d }' % (config.get(["update", "check_update"], 1), config.get(["web_ui", "popup_webui"], 1) )
-            if reqs['check_update'] :
+            if 'check_update' in reqs:
-            return ""
+
-        if commonname.count('.') >= 2 and [len(x) for x in reversed(commonname.split('.'))] > [2, 4]:
+    def get_cert(commonname, sans=(), full_name=False):
-class Common(object):
+class Config(object):
-    def __init__(self):
+
-        self.CONNECT_POSTFIX_ENDSWITH = tuple(self.CONNECT_POSTFIX_MAP)
+        #self.CONNECT_HOSTS_MAP = collections.OrderedDict((k, v) for k, v in self.CONFIG.items('hosts') if ':' in k and not k.startswith('.'))
-        self.HTTP_FORCEHTTPS = set(self.CONFIG.get('http', 'forcehttps').split('|'))
+        self.AUTORANGE_HOSTS = self.CONFIG.get('autorange', 'hosts').split('|')
-        self.pac_url = 'http://%s:%d/%s\n' % (self.PAC_IP, self.PAC_PORT, self.PAC_FILE)
+        self.keep_run = True
-common = Common()
+
-
+import time
-class ip_range(object):
+class Ip_range(object):
-    test()
+    test_random()
-common = Common()
+from config import config
-        blackhole = '%s:%s' % (listen_ip, common.PAC_PORT)
+        listen_ip = config.LISTEN_IP
-                logging.info('%r downloaded, try convert it with adblock2pac', common.PAC_ADBLOCK)
+            if config.PAC_ADBLOCK:
-                logging.info('%r downloaded and parsed', common.PAC_ADBLOCK)
+                logging.info('%r downloaded and parsed', config.PAC_ADBLOCK)
-            logging.info('%r downloaded, try convert it with autoproxy2pac', common.PAC_GFWLIST)
+            logging.info('try download %r to update_pacfile(%r)', config.PAC_GFWLIST, filename)
-            logging.info('%r downloaded and parsed', common.PAC_GFWLIST)
+            logging.info('%r downloaded and parsed', config.PAC_GFWLIST)
-    pacfile = os.path.join(os.path.dirname(os.path.abspath(__file__)), common.PAC_FILE)
+    pacfile = os.path.join(os.path.dirname(os.path.abspath(__file__)), config.PAC_FILE)
-            elif time.time() - os.path.getmtime(self.pacfile) > common.PAC_EXPIRED:
+            elif time.time() - os.path.getmtime(self.pacfile) > config.PAC_EXPIRED:
-common = Common()
+from config import config
-        if not common.LISTEN_VISIBLE:
+        ctypes.windll.kernel32.SetConsoleTitleW(u'GoAgent v%s' % config.__version__)
-        if common.LOVE_ENABLE and random.randint(1, 100) <= 5:
+        if config.LOVE_ENABLE and random.randint(1, 100) <= 5:
-            ctypes.windll.kernel32.SetConsoleTitleW('%s %s' % (title.value, random.choice(common.LOVE_TIP)))
+            ctypes.windll.kernel32.SetConsoleTitleW('%s %s' % (title.value, random.choice(config.LOVE_TIP)))
-        logging.critical('please edit %s to add your appid to [gae] !', common.CONFIG_FILENAME)
+    if config.GAE_APPIDS[0] == 'goagent':
-        url = 'http://%s:%d/%s' % (pac_ip, common.PAC_PORT, common.PAC_FILE)
+    if config.PAC_ENABLE:
-    logging.basicConfig(level=logging.DEBUG if common.LISTEN_DEBUGINFO else logging.INFO, format='%(levelname)s - %(asctime)s %(message)s', datefmt='[%b %d %H:%M:%S]')
+    logging.basicConfig(level=logging.DEBUG if config.LISTEN_DEBUGINFO else logging.INFO, format='%(levelname)s - %(asctime)s %(message)s', datefmt='[%b %d %H:%M:%S]')
-    logging.info(common.info())
+    logging.info(config.info())
-        server = LocalProxyServer((common.PAC_IP, common.PAC_PORT), pac_server.PACServerHandler)
+    if config.PAC_ENABLE:
-        control_server = LocalProxyServer((common.CONTROL_IP, common.CONTROL_PORT), remote_control.RemoveContralServerHandler)
+    if config.CONTROL_ENABLE:
-    server = LocalProxyServer((common.LISTEN_IP, common.LISTEN_PORT), proxy_handler.GAEProxyHandler)
+    server = LocalProxyServer((config.LISTEN_IP, config.LISTEN_PORT), proxy_handler.GAEProxyHandler)
-        time.sleep(100000)
+    while config.keep_run:
-from http_util import HTTPUtil
+from cert_util import CertUtil
-http_util = HTTPUtil()
+from config import config
-def message_html(title, banner, detail=''):
+def generate_message_html(title, banner, detail=''):
-    # deflate = lambda x:zlib.compress(x)[2:-4]
+def gae_urlfetch(method, url, headers, payload, app_server, **kwargs):
-    skip_headers = http_util.skip_headers
+    skip_headers = https_manager.skip_headers
-    response = http_util.request(request_method, fetchserver, payload, request_headers, crlf=need_crlf, connection_cache_key=connection_cache_key)
+    response = https_manager.request(request_method, app_server, payload, request_headers)
-                thread.start_new_thread(self.__fetchlet, (range_queue, data_queue, cur_threads * common.AUTORANGE_MAXSIZE))
+            while cur_threads < self.threads and time.time() - t0 > cur_threads * config.AUTORANGE_MAXSIZE / 1048576:
-                        response = self.urlfetch(self.command, self.url, headers, self.payload, fetchserver, password=self.password)
+                        response = gae_urlfetch(self.command, self.url, headers, self.payload, fetchserver, password=self.password)
-    urlfetch = staticmethod(gae_urlfetch)
+
-                return self.wfile.write(('HTTP/1.1 301\r\nLocation: %s\r\n\r\n' % self.path.replace('http://', 'https://', 1)).encode())
+        if host in config.HOSTS_MAP or host.endswith(config.HOSTS_POSTFIX_ENDSWITH):
-            return self.do_METHOD_AGENT()
+            return self.do_AGENT()
-    def do_METHOD_AGENT(self):
+    # Called by do_METHOD and do_CONNECT_AGENT
-        special_range = (any(x(host) for x in common.AUTORANGE_HOSTS_MATCH) or path.endswith(common.AUTORANGE_ENDSWITH)) and not path.endswith(common.AUTORANGE_NOENDSWITH)
+        special_range = (any(x(host) for x in config.AUTORANGE_HOSTS_MATCH) or path.endswith(config.AUTORANGE_ENDSWITH)) and not path.endswith(config.AUTORANGE_NOENDSWITH)
-            request_headers['Range'] = 'bytes=%d-%d' % (start, start+common.AUTORANGE_MAXSIZE-1)
+            request_headers['Range'] = 'bytes=%d-%d' % (start, start+config.AUTORANGE_MAXSIZE-1)
-            request_headers['Range'] = 'bytes=%d-%d' % (start, start+common.AUTORANGE_MAXSIZE-1)
+            request_headers['Range'] = 'bytes=%d-%d' % (start, start+config.AUTORANGE_MAXSIZE-1)
-        for retry in range(common.FETCHMAX_LOCAL):
+
-                #if common.GAE_VALIDATE:
+                if config.GAE_PASSWORD:
-                response = self.urlfetch(self.command, self.path, request_headers, payload, fetchserver, **kwargs)
+
-                        html = message_html('502 URLFetch failed', 'Local URLFetch %r failed' % self.path, str(errors))
+                    if retry >= config.FETCHMAX_LOCAL-1:
-                        html = message_html('404 Appid Not Exists', 'Appid %r Not Exists' % appid, 'appid %r not exist, please edit your proxy.ini' % appid)
+                    logging.warning('APPID %r not exists, remove it.', appid)
-                        continue
+                    logging.warning('APPID %r out of auota, remove it.', appid)
-                    continue
+                        continue
-                    # logging.warning('retry: %s' % retry)
+
-                if response.app_status != 200 and retry == common.FETCHMAX_LOCAL-1:
+
-                        rangefetch = RangeFetch(self.wfile, response, self.command, self.path, self.headers, payload, fetchservers, common.GAE_PASSWORD, maxsize=common.AUTORANGE_MAXSIZE, bufsize=common.AUTORANGE_BUFSIZE, waitsize=common.AUTORANGE_WAITSIZE, threads=common.AUTORANGE_THREADS)
+                        # 206 means "Partial Content"
-                while 1:
+
-                        response.close()
+                        https_manager.save_ssl_connection_for_reuse(response.ssl_sock) #, response.connection_cache_key)
-                if e.args[0] in (errno.ECONNABORTED, errno.EPIPE):
+                if e.args[0] in (errno.ECONNABORTED, errno.EPIPE, errno.ECONNRESET):
-                    logging.exception('GAEProxyHandler.do_METHOD_AGENT %r return %r, try again', self.path, e)
+                    logging.exception('GAEProxyHandler.do_METHOD_AGENT %r return %r, errno: %d ', self.path, e, int(e.args[0])) #IOError(9, 'Bad file descriptor')
-        elif host in common.HOSTS_MAP or host.endswith(common.HOSTS_POSTFIX_ENDSWITH):
+        if host in config.HOSTS_MAP or host.endswith(config.HOSTS_POSTFIX_ENDSWITH):
-        for i in range(5):
+
-                hostname = hostname or host
+                if host == "appengine.google.com":
-                remote = http_util.create_connection((host, port), timeout, cache_key=connection_cache_key)
+                    connected_in_s = 10
-                    logging.error('http_util.create_connection((host=%r, port=%r), %r) timeout', host, port, timeout)
+                    logging.error('http_util.create_connection((host=%r, port=%r)) timeout', host, port, )
-
+            forwork_manager.forward_socket(self.connection, remote, bufsize=self.bufsize)
-        logging.info('CONNECT_AGENT %s %s:%d ', self.command, host, port)
+        logging.info('GAE %s %s:%d ', self.command, host, port)
-                logging.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s', self.connection, e, self.path)
+                logging.exception('ssl.wrap_socket(self.connection=%r) failed: %s path:%s, errno:%s', self.connection, e, self.path, e.args[0])
-            logging.info('ssl error: %s', e)
+
-            self.do_METHOD()
+            if self.path[0] == '/' and host:
-from common import common
+from config import config
-        gws_ip_num = len(common.google_ip.gws_ip_list)
+        gws_ip_num = len(google_ip.gws_ip_list)
-                   "pac_url":common.pac_url}
+                   "goagent_version": config.__version__,
-__hostsdeny__ = ()  # __hostsdeny__ = ('.youtube.com', '.youku.com')
+__hostsdeny__ = ()
-        my_stdout.write("upload  fail: %s\n\n" % e)
+    for i in range(3):
-
+def uploads(appids, email, password):
-            deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd], stdout=subprocess.PIPE)
+            self.deploy_proc = subprocess.Popen([sys.executable, script_path, appid, email, passwd], stdout=subprocess.PIPE)
-                self.send_file(log_path, "txt")
+                with open(log_path, "r") as f:
-                pass
+                content = ""
-
+# coding:utf-8
-
+# coding:utf-8
-        p = subprocess.call(["cmd", "/c", "create_shortcut.js"], shell=False)
+        p = subprocess.call(["Wscript.exe", "create_shortcut.js"], shell=False)
-            self.get_more_proxy_ip()
+            self.search_more_google_ip()
-                self.iplist_need_save = 0
+            self.iplist_need_save = 0
-                self.remove_ip(ip_str)
+        try:
-            #    return
+            else:
-            self.ip_dict[ip_str] = {'conn_time':conn_time, 'domain':domain, 'server':server}
+                self.ip_dict[ip_str] = {'conn_time':conn_time, 'domain':domain, 'server':server}
-                self.gws_ip_list.append(ip_str)
+                if domain not in self.domain_ip_list:
-        del self.ip_dict[ip_str]
+            property = self.ip_dict[ip_str]
-            try:
+            if 'gws' in server and ip_str in self.gws_ip_list:
-                pass
+        except Exception as e:
-            self.get_more_proxy_ip()
+            self.search_more_google_ip()
-            self.last_sort_time = time.time()
+        ret_list = []
-            return []
+            if len(self.gws_ip_list) == 0:
-        self.ip_lock.release()
+            i = 0
-            pass
+            logging.warn("get_domain_batch_ip %s fail:%s", domain, e)
-                pass
+                logging.warn("google_ip.runJum fail:%s", e)
-    def get_more_proxy_ip(self):
+    def search_more_google_ip(self):
-            if timeout == 1:
+            if timeout == 1 or domain == None:
-                self.set_ip(ip_str, conn_time, domain, server)
+                self.set_ip(ip_str, conn_time)
-    check.get_more_proxy_ip()
+    check.search_more_google_ip()
-            return
+            return None
-            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, certfile=certfile, server_side=True, ssl_version=ssl.PROTOCOL_TLSv1)
+            ssl_sock = ssl.wrap_socket(self.connection, keyfile=certfile, certfile=certfile, server_side=True)
